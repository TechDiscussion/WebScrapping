[
{"website": "Cap-Gemini", "title": "Rafael: A Developers Story\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Rafael-Developers-Story/", "abstract": " As the global population live in angst about humanity’s future, there are industries across the world that are being stretched to the peak of their abilities in ways never seen before. The healthcare sector is one of those industries. It is reasonable to say that the capabilities of healthcare systems across the world were the most crucially needed systems this year. Without a sound healthcare system in a global pandemic, you severely limit the capabilities and services that you can provide. This is both a physical truth with regards to the ability to scale the amount of hospital beds available during a spike of A&amp;E admissions, and it is also a technological truth with regards to having the technology available to efficiently test, treat and track patients. Of course, like many global crises many people suffer from both physical and emotional damage, there are always the visionaries that come together to provide aid. In this instance a COVID-19 task force was assembled consisting of 18 engineers in  Capgemini Bordeaux  with backing from the health support team at the  University Hospital of Bordeaux  to bring you Rafael. . Unlike the typical blog post you’re used to reading from me, this one will be taking on a slightly different form. I will be providing an interview with one of the lead engineers behind Rafael – a COVID-19 tracking application. We will be discussing Rafael and its development methodologies, technology inside of today’s healthcare systems and the potential impact COVID-19 will have on the responsibility technology plays in future healthcare systems. . Rafael is a COVID-19 tracking application that has been created by Capgemini Bordeaux with the help of the healthcare support team at the University Hospital of Bordeaux. It focuses on identifying potential COVID-19 hosts with additional functionality enabling a follow up of suspected/confirmed patients. This allows for the quick response by physicians to minimise the possibility of a further spread. . The below image describes the context in a bit more detail alongside the final application’s solution flow. .   . When a user opens Rafael, they are met with a screen where they fill with their information and symptoms for further evaluation. If the user is identified to have serious symptoms then they will be swiftly contacted by a physician. .   . For a physician, there will be many potential patients that will need care. The following Crisis Center Dashboard allows physicians to see patients’ states sorted by the seriousness of their symptoms. This allows for physicians to be aware of those with the most serious conditions and symptoms so they can act faster to care to them. .   . In this troubling time, you and your team have stepped up to the plate and assembled a COVID-19 task force, but do you mind telling us a bit about who you are, what you do and what brought you to Capgemini? .  “Sure, my name is Mohamed Akdim and I’m a 29-year-old Lead Software Engineer and Junior Architect at the healthcare department of Capgemini Bordeaux. My journey at Capgemini began on 2014 as an intern. The diversity and innovation of the projects were key parts in me joining the Capgemini team. In fact, I had the opportunity to work with skilful teams on multiple business domains including Transports, Public services and the Wine industry. I had the chance to use one of the more popular Cloud Providers Microsoft Azure and had a chance to learn Reactive programming and AI.”  .  Reactive programming  can be quite a learning curve to those coming from the more popular imperative paradigm – I can imagine it came with its challenges alongside learning Cloud engineering and AI? .  “It did, all these challenges were very instructive to my career and kept me seeking for new ones. It was in-fact the same mindset that eventually led me to work on one of the most challenging projects: the build of Rafael Platform - a part of a bigger program called  “Ange Gardien” ’ which is a collaboration between the University Hospital of Bordeaux, multiple healthcare actors in the region and Capgemini.”  . Coming from personal experience, I can attest that my life has changed dramatically since the beginning of 2020 as I am now working from home, and I’ve even spent significantly less money – which I’m never going to complain about. Have you also experienced a drastic change in your life? If so, what is different now than before? .  “The impact of the COVID-19 pandemic is huge on one’s life. Everything has changed during the pandemic with the period of confinement. It was a tough experience to stay at home, not hanging with friends and colleagues, and being away from family. Although, the fact that we were directly involved in the outbreak with the development of Rafael was very helpful in some way, kind of being on duty. In fact, the French president Mr Macron has declared war upon the virus. Thus, we were participating as a team in this national effort by facilitating the support and management of COVID-19 patients. This gave us the strength to work hard and to fulfil the development of the app in a very efficient way.”  . That’s very interesting how you feel like it is like being on duty. I do admit, I find it very intriguing how differently world leaders have faced this pandemic. Some have followed science and nothing else, some have carried on as normal, and as you stated, some have declared war. It’s especially interesting with regards to the political landscape too as over the years when something happens on a global scale, USA are normally one of the self-appointed leaders (the Ebola Virus pandemic) however with the COVID-19 pandemic it seems that nations across the globe have taken their own path, and what’s most interesting is the cultures reactions to said paths. One type of reaction that we’ve seen many times over in multiple different nations around the world is the rise of the innovators. Many individuals like yourself have taken up the initiative to help create a solution that aids in the handling of the virus. With relation to this, how do you feel about the impact such solutions/apps have had towards COVID-19 and how do you feel being a part of that impact? .  “I believe the app had a significant impact in the region. More than 2000 COVID-19 patients were able to be managed via the app. Each patient had a personalised follow-up by healthcare professionals. About 10,000 physicians were registered in the app. Each physician was able to monitor the health state of their patients remotely, therefore reducing the risk of contamination between patients and diminishing the spread of the outbreak. The enablement of flagging and alerting mechanisms has helped the health support team to take quick decisions and act effectively.   Obviously, I’m very proud of what we’ve done so far, and I look forward for the upcoming challenges.”  . It sounds like you’ve really hit a chord with the healthcare support team with so many individuals being registered to the app. Are there any patient/health worker anecdotes you care to share with us? .  “The health support team members at the University Hospital of Bordeaux were very satisfied by the application Rafael, as they could easily manage more than 200 patients daily. ‘The patient management workflow is very intuitive and facilitates greatly our work.’ said one of the physicians at a debriefing meeting.”  . I’ve always learned that no matter how good you feel your product is, if nobody wants to use it, it is a bad product. Rafael however seems to have really resonated with those who use it which is the most important factor. If people like using it, you’re onto a winner. Not to mention it looks great from a user interface perspective. . Moving onto apps in general with regards to their impact on healthcare, what are the more recent impressions you get from the healthcare sector with respects to using apps? .  “I think that healthcare apps are reshaping the picture around patient management. It improves patient experience, especially with regard to accessing health information and making doctor/patient communication more convenient and straightforward, ensuring transparency in medical charge, and ameliorating short-term outcomes. Patient’s background medical data is easily accessible, thus helps to improve the efficiency of the diagnosis. In addition to that, the emergence of mobile health apps has helped to overcome geographical and organisational barriers to improve healthcare delivery to everyone.”  . I completely agree in that it improves the patient experience. I also find a particular point very interesting that was made by Yuval Noah Harari in his book 21 Lessons for the 21st Century that with the rise of big data and AI, it’s not hard to imagine a future where we have an increased presence of AI robots in healthcare. He then goes on to make the point that it will be much easier reduce fatality rates worldwide that are due to out of date medical knowledge. Normally it can take quite a bit of time for crucial and ground-breaking up-to-date medical information to make its way from the Harvard University Lab it was discovered in, to a poverty-stricken rural village in Ethiopia. Whereas with the help of big data and AI, these ground-breaking medical updates can be pushed to all robots around the world within seconds – improving the integration of updated medical knowledge exponentially. Thus, allowing all medical practitioners around the world to share a single source of regularly updated truth. With this example in mind, how do you feel technologically will evolve in the future to help people? Does Harari’s example make you more optimistic that a utopian future is possible, or do you see a more dystopian horizon? .  “I strongly believe that we’re only scratching the surface. For example, teleconsulting will be done more easily and widely. The precision of medical diagnosis is improving continuously. Of course, IOT’s latest outstanding progress is helping to overcome some technical barriers, leading to more connected objects with more precise measurements. Last but not least, the melding of AI and Big Data has a tremendous potential in the healthcare industry. Practitioners and especially patients will have a set of tools that can guide them to take quick and critical saving life decisions. Healthcare technology has a bright future for sure.”  . I completely agree, very valid points you put forward, I too am optimistic about the future but at the same time I am certainly not forgetting that the potential chance of dystopia is still possible. . Bringing it closer to our beloved area of software engineering, it is true that when we think about medicine and healthcare, traditionally we think of doctors and nurses – not software engineers. What other industries or systems do you think engineers might get involved in, especially as we learn to connect remotely? .  “Of course, some jobs simply can’t be done at home. But the pandemic is accelerating the trend, possibly for the long term and for larger opportunities. For example, many innovative companies are using Virtual Reality to develop digital offices that mimic real-world office spaces. This scenario makes it much easier to allow certain professions to be conducted remotely, such as Virtual Operations Managers, Market/Financial Managers, and of course Medical and Health Management Professions.”  . That’s a very interesting thought, I’ve written a few posts about the  impacts that COVID-19  had on remote working and  how I feel the government should incentivise it  and one of the downsides of remote working I mentioned was the obvious lack of social contact which is inescapable. I was actually entirely convinced that we had no solution to this yet, but the idea of a Virtual Reality office where employees are able to connect into to feel more involved socially is absolutely genius. I’d definitely be interested to see how that technology evolves in the near future where remote working is still at the forefront of everyone’s minds. . Moving on to how the Rafael was developed, do you mind going into how long it took you and your team to get from ideation to delivery? .  “It took us 5 days to deliver the MVP of the application Rafael COVID-19. Afterwards, we continued to enrich the application by providing multiple evolutions and support.”  . Wow! 5 days is incredible. Especially with balancing the pressure of the ever-growing dynamic of COVID-19. Capgemini has really stepped up with regards to placing ourselves on the front-lines of getting solutions out to the world. Just a few weeks back we also had another Capgemini team work closely with HMRC in the United Kingdom to  release the furlough portal . Something which has been undeniably foundational in the reasons for why citizens of the UK have been able to afford basic necessities. . Was there much support/backing from your project? .  “We had overwhelming support from every actor inside the project. Thanks to our decision makers, we quickly had access to every resource needed in order to work effectively, including both human or technical resources. Everyone was fully committed to conclude the mission successfully.”  . You mentioned having quick access to technical resources, were there any colleagues that helped with technical/personal support? .  “Yes definitely! We were a task force COVID-19 team. I had the chance to work with professional, skilful and fully committed teammates. It’s thanks to them that the application was successfully delivered with high quality standards in a very short amount of time. I would like to take this opportunity to thank every actor inside the project for the great collaboration and commitment shown during this period.”  . Lastly, it wouldn’t be an engineering blog if we didn’t talk a bit about the technical aspects of Rafael. My first question is based around security. As you’re probably aware, in a world where surveillance and data breaches are a top concern for many people, the topic of security when it comes to health systems is incredibly important to people - for the obvious reasons. In many cases, not being completely in control of the security of a system can be a matter between life and death. Do you mind sharing any security best practises you followed that ensured that the app was secure and reliable and safe from malicious attack? .  “Privacy and security in healthcare apps are both major concerns in our work and we take it very seriously. Of course, one of the crucial points is to follow up the  OWASP recommendations . We must ensure that every developer had security mindset since the beginning. This is done by making multiple security supports and formations available to everyone, in parallel to continuous code reviews and security mobs. The authentication/authorisation parts are implemented with the most proven standards such as OpenID Connect and OAuth 2.0. In addition to this, we do use double authentication to guarantee a more secured platform. On the technology stack level, we make sure to use the most secured and updated frameworks with the latest security patches, without forgetting data encryption on each level: data exchange, HTTPS, SSL/TLS etc”  . With just five days between ideation and delivery, I can imagine with such a pressure you must have been met with some technical challenges when designing, developing and delivering Rafael. .  “One of the biggest challenges was to use the reactive programming model in the development of the app. This model is built around data streams and consists on a non-blocking, event-driven applications that scale with a small number of threads with backpressure as a key ingredient. The main purpose is to ensure high availability under intensive workloads. The learning curve is a bit tricky, but the advantages of this model are huge with great potential.”  . One of the core principles we follow in our team at Capgemini to get up and running and delivering at a fast pace is to have a CI/CD platform and pipeline setup for your applications as early as possible. Was this the same on Rafael? Also, what other software methodologies did you follow in order to best deliver the app? .  “Here at the healthcare department in Bordeaux, our projects are fully Agile. We use Scrum in order to improve the productivity and the quality of the application with small feedback loops. This method allows us to be highly reactive by reducing time to market. This was a crucial point in the development of Rafael Covid-19 app during the outbreak. Besides, we rely on Capgemini’s Production line to ensure CI/CD. We follow DevOps principles in order to shorten the development life cycle and provide continuous delivery. This is made possible thanks to our highly competent DevOps team.”  . As you mentioned previously with regards to the purposes of using of Reactive programming, high scalability and high availability under intensive workloads is absolutely crucial for healthcare systems. The healthcare systems are not like media streaming sites where if the website is down for one hour, nobody dies. In healthcare systems, being highly scalable and available are absolutely fundamental to human life and are on the same level of importance as security. Now, as healthcare apps are by nature mandated to be reliable and available, how did you ensure that the apps were highly scalable and highly available under intensive workloads? .  “Rafael is based on micro services architecture which is deployed on the Microsoft Azure Cloud. This ensures a highly scalable, performant and resilient application. Each microservice is able to scale with the entire system without suffering from performance problems. Our platform implements the common patterns in distributed systems including configuration management, service discovery, circuit breakers, intelligent routing, event bus, distributed sessions, etc. All these aspects combined with the reactive programming model guarantee a high scalability and availability levels of the platform.”  . It’s really something amazing that your team managed to deliver an app with such important functionality within five days and it’s even more impressive that you managed to implement very modern paradigms on such short notice, at such great pressure in a landscape where every waking day could bring something completely different. Many companies are still debating whether they should bite the bullet and break up their monolith to microservices. So, I personally would like to congratulate you guys on what you’ve achieved in the time period and thank you for talking with me. . I’m not often a fan of film credits but I feel in this case it’s important and recognition goes a long way in our line of work, with that in mind, here is a list of the team’s names and their roles within the development of Rafael and once again, thank you for reading we hope you enjoyed. . Benjamin Richard - Chief Information Security Officer . Cendrine Fillioux - Engagement Manager . Ulysse Moutard - Delivery Executive . Alexandre Ripart - Project Manager . Valentin Arbez - Product Owner . Selma Menja - Product Owner . Pierre Laprade - Scrum Master . Matthieu Ruelle - Scrum Master . Eric Gauchery - Cloud Architect . Olivier Bethery - Architect . Xavier Sicard - Architect . Laurent Duhart - Tech Lead (Mobile) . Mohamed Akdim - Tech Lead (Back-End) . Thomas Joubert - Tech Lead Shared Services . Maxime Pomier - Developer . Maud Si Mansour - Front-End Developer . Alexis Jonot - Back-End Developer . Nicolas Saucisse - Web Developer . Alexandre Blin - UX Integration . Théo Pauliat - UX/UI ", "date": "2020-07-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The Engineering Collective\n  ", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/engineering/the-engineering-collective/", "abstract": " Starting out on a journey to learn something new can be both daunting and challenging. . Where do you begin? How do you recognize the good materials from the bad? How do you know you’re on the right path? . These are all questions we’ve asked ourselves along the way when trying to learn or understand something new. We often find ourselves using our favourite search engine to begin the learning journey, but what if we didn’t have to go in blindly, and could have reference to a host of learning materials, experience and best practices in one central place. . That is precisely how the engineering collective was born. As engineers, we are always looking for consistent approaches when building things, and we always strive to follow best practice when doing so. . We don’t reinvent the wheel when building software, so why do it when creating a learning path? . The engineering collective was born out of a need to collate shared experiences and knowledge from engineers who’ve  been there and got the t-shirt  so to speak. It’s a collection of  Roadmaps , put together to help guide people on their learning journeys. . I’ve always found learning about new technologies or engineering practices and principles really interesting. You’re not just adding more skills to your CV or profile, you’re learning new and interesting skills to help make you a better, more rounded engineer. Sometimes however, it can be tricky to know where to start; which resources are best, which courses are most worthwhile, who are the subject matter experts in the team, the list goes on. I’ve always believed strongly in knowledge sharing and helping to up-skill more junior developers within the teams I’ve worked in. We all had to start somewhere, and I’ve been lucky throughout my career so far to work with some really awesome people, all of whom have been more than happy to share their knowledge along the way. . The engineering collective was designed and built in an attempt to answer some of those questions and become the place to start when learning something new. It doesn’t have to be technical, the idea is to collate a collection of resources and roadmaps, that could be anything from a Java quest, to speaking at a conference. . The idea of using roadmaps was to give a visual  journey  through the learning path, trying to put together a list of skills and experience in a logical order, to help showcase the  best  approach to learning something new. Sometimes there may be pre-requisites to a learning path, in other cases there won’t be. . Roadmaps alone aren’t the answer however. It’s great seeing a visual path through a particular learning journey, for example learning Java, to see that learning the fundamentals about the JVM or the syntax and language features comes before learning about Design Patterns. Where this collective stands out, is in the accompanying notes and resources that have been put together by those who’ve contributed. Each  Quest  has several key, core sections which accompany the visual roadmap; . Resources - links to blog posts, courses, videos, tutorials . Certificates / Badges - links to certifications or badges to showcase learning progress . Engineering Suggestions - notes from engineers who’ve been on this learning journey, things they found helpful or not so helpful . Together, the roadmap and the accompanying notes, really help shape the collective and give it an edge over other similar frameworks and guides. . Even though it’s still in its infancy, I believe this collective can be a really useful tool for us as a group of engineers with a huge variety of skills and experience, to help up-skill the engineers of the future. Through our skills and experiences, we can help shape the way other engineers learn and progress through their own learning journeys, and in turn create a  truly  collective, open source, community driven set of learning paths. I feel we have a responsibility as engineers to share our knowledge as much as possible and to help pave the way for future generations. . The collective was inspired by the shared knowledge and experiences between engineers, as a result we’d love for you to contribute to the collective, whether it’s an update to existing content, a new piece of content in the form of a new Quest, or just new ideas on how the collective could be improved further. . Please see  contributing  section of the project for more details. . You can see more at  the engineering collective  ", "date": "2020-09-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Elasticsearch: Introduction\n  ", "author": ["Kamar Ali"], "link": "https://capgemini.github.io/development/elasticsearch-introduction/", "abstract": " Elasticsearch is one of those technologies I have always heard about but never had the opportunity to get hands on with. Apart from making an educated guess I didn’t really know what it was, or what it really provides us. . I thought I’d be a little more productive during the COVID-19 lockdown, and due to my curiosity I decided to do some research on the technology. So these next few posts are going to document my findings and hopefully you can follow along if you’re looking to take a dive into the world of Elasticsearch and the entire ELK stack. . For more reading around the technologies in the ELK stack: .  Elasticsearch  .  Logstash  .  Kibana  . So, what is Elasticsearch? Straight from the horse’s mouth: . Elasticsearch is a distributed, open source search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. Known for its simple REST APIs, distributed nature, speed, and scalability. . In layman’s terms it’s Google for your data, it allows you to search and analyse huge amounts of data in near real-time and returns answers in milliseconds. It achieves this by searching on an index of your data. It’s a powerful open source tool built on  Apache Lucene  which provides a tonne of advantages. . It’s multilingual as it allows us to perform and combine various kinds of searches irrespective of the data type . Data can also be returned in whichever form is required . It performs in near real time, meaning, we can analyse billions of records in seconds . Can manage huge indexes . It’s scalable up to thousands of servers . Can accommodate petabytes of data (that’s a lot) . Schema free, meaning documents with any structure can be stored . Provides a RESTful API for your searching needs . AWS have a dedicated service for elastic search! . It’s been adopted by some HUGE brands, including:              Dell                      Utilises Elasticsearch to support e-commerce searches on the dell.com website                           The Guardian                      Used to gain visibility in real time on how readers are interacting with content on different platforms (website, social media). This allows The Guardian to better tailor their articles to users             Also used to monitor site activity allowing the development teams to keep the site running smoothly                           Docker                      Powers search functionality on docker website for a constantly growing database                             . Dell                      Utilises Elasticsearch to support e-commerce searches on the dell.com website                   . Utilises Elasticsearch to support e-commerce searches on the dell.com website . The Guardian                      Used to gain visibility in real time on how readers are interacting with content on different platforms (website, social media). This allows The Guardian to better tailor their articles to users             Also used to monitor site activity allowing the development teams to keep the site running smoothly                   . Used to gain visibility in real time on how readers are interacting with content on different platforms (website, social media). This allows The Guardian to better tailor their articles to users . Also used to monitor site activity allowing the development teams to keep the site running smoothly . Docker                      Powers search functionality on docker website for a constantly growing database                   . Powers search functionality on docker website for a constantly growing database . Node: A single instance of Elasticsearch . Cluster: A group of nodes . Document: JSON document containing a collection of fields. Every document belongs to a type and is stored inside an index. These are identified with a unique identifier called a UID . Type: This is a class of similar documents, for example if we were storing music albums . Index: A collection of different documents. Works like the index in a book . Shard: Indexes can be horizontally divided into shards . Replicas: We can create replicas of our indexes and shards which increases the availability of our cluster .  There are guides for installing it on Linux, Mac and Windows; you can also use docker.  . Once you have installed Elasticsearch, be sure to start it and use the following command to verify the service is running. (I have my Elasticsearch service running locally) . You should see a response containing details on your Elasticsearch service like the following: . Now that we have our Elasticsearch service up and running we can get to work storing our first document. . For the purposes of this demonstration we will be storing fruit and veg in Elasticsearch, which are both a type of food. Elastic acts like a REST API so we can use either POST (to add new data) or PUT (to update existing data). . We’ll first focus on POSTING data, and in particular we will be adding an apple, which is a type of fruit to our ‘food’ index. To do this we will be sending a POST request to:  /food/fruit  with a body like the following: . Before sending the request lets quickly break down the URL. We’re sending a request to  /food/fruit  and this is composed of two major parts of Elasticsearch. . The first part is the index, think of this as a ‘database’ in elastic search, which contains a mapping of multiple types, so this ‘food’ database will contain all our different foods. . The second part of our URL is the type, this pretty much represents a class of similar documents; so when we are talking about food we can split them into a range of different types such as: . Fruit . Veg . Meat . Poultry . Dessert . We can even split into cuisines              Italian         Indian         Mediterranean         Mexican           . Italian . Indian . Mediterranean . Mexican . Below you can see the entire request to create our first document which represents an Apple. This will be indexed in food and is of type fruit: . And here is the response we received, as you can see from  _id  this has been generated for us. . If needed we can also specify your own IDs. To demonstrate this we will create a banana document in Elasticsearch with the ID  12345B . Below is the request we will send. Notice the ID has been appended to the URL so now we have  /food/fruit/12345B . . And below is the result, notice how the  _ID  field now contains the value we specified. . Using PUT to update our documents is also a powerful tool and can help you on your way. For the sake of this demo we will be looking to update our apple from ‘red’ to ‘green’. To do this we will need to generate a put request with our updated body and specifying the ID of our document so elastic knows which document to update. . Small problem, I don’t remember the ID of the apple document I stored. The solution here is to use the simple search feature Elasticsearch provides to find our document and the data we need. . We will be using the  _search  URL and in particular we will be looking for our apple using the following:  _search?q=apple . This will search on all fields to see if we have any matches (more on searching a little later). . Below is the composed search: . And here is the result (as you can see we have got a hit and we can grab the value from  _id ): . I will cover search a little later, however now that we have our ID we can compose the following request, and update our apples colour from red to green. As you can see we have put the ID in the URL, and in the body we have specified we want to update the colour to green. . Here is the response we get, as you can see Elasticsearch has alerted us that the document has been marked as updated! . Finally if we do our search again we will retrieve a green apple: . The first basic search is to collect a single item by its ID. If we remember our Banana with the ID of 12345B, we can compose a basic GET request with the following URL:  /food/fruit/12345B . . And here is the result: . All the fields in the response (above) which begin with an underscore are metadata fields, the  _source  object is where we will find the document we indexed. . Another example of searching which was touched on earlier is using the  _search  endpoint. We will be doing another basic query looking for the word  world . The way the basic URI search works is by scanning all the fields of all the documents for the specified string, which doesn’t make it very efficient! . Here is our basic GET request with the search looking for the word  world . . And here are the results, as you can see both descriptions for our apple and banana contain the string  world  so both documents are returned! . There are also some other pieces of data which you get alongside your ‘hits’, these can be helpful when you are trying to be more efficient with your searches. As a quick overview: . Took – The time in milliseconds the search took . Timed_out – Did the search timeout? . Shards – The number of shards searched . Hits - The actual result . For more efficient and specific searches we can use the  QueryDSL  (which I will cover in more depth in a future post). This is a request body search which allows us to get a lot more advanced with our searching. Below is a query we have composed to search the colour field for the phrase “yellow”. . And as you can see below our banana document has been returned. Note that search is case insensitive, as we searched for  Yellow  despite the document containing the string  yellow . . The final subject this post on basic elastic concepts is going to look at is deleting data, which is surprisingly easy. All you need to do is send a DELETE request ensuring you specify the ID of the document (much like the POST to create data). . For this example we are going to delete the banana document. To do this we will send a DELETE request to  /food/fruit/12345B . . As you can see in the returned body the banana document has been deleted. . Just to double check we will do our previous search for our yellow coloured fruits using the QueryDSL. As you can see no results are returned, so the document has been successfully deleted from Elasticsearch. . This was a simple look into the world of Elasticsearch to get to grips with the tool, gain a little understanding and get our feet wet. Next I will be looking into QueryDSL and how we can really leverage the power Elasticsearch gives us to compose some advanced searches and take a deeper dive, I will also be exploring Kibana and Logstash and how we can use these other tools in the ELK stack and tie all of this together. ", "date": "2020-10-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "5 Things I wish I knew when I became a software developer\n", "author": ["Jedd Hopkins"], "link": "https://capgemini.github.io/learning/5-things-I-wish-I-knew-when-I-became-a-software-developer/", "abstract": " March 2018. Having recently finished a two-year contract collecting map data with a large search engine provider, I found myself at a point in my life where I had a real chance to pursue a new career. Having absolutely no background in coding at all, I decided to look up fast track ways to get into the industry without a computer science degree. . This is how I found  Makers Academy . They painted a perfect picture… . Join the 3-month bootcamp, get hired, work your dream job. It almost sounded too good to be true. . I decided to apply through their apprenticeship scheme which resulted in a connection with Capgemini. They then gave me the opportunity for a new career. I finished my Makers bootcamp in April 2019 and now have 1.5 years of experience under my belt as a Java developer. . I am so grateful for the opportunity I received and I want to share some things with upcoming junior developers. These are tips that I still use today, but that I wish I knew just a little bit sooner in my developer life. . I know the scenario; you finish work for the day and doing more coding may be the last thing on your mind. But don’t let that deter you. A personal project can be anything you want, which means it will most likely be something you will enjoy building because it aligns with your interests. This could range from small and simple to multi-layered and involved. . You want to build a little calculator that runs in the terminal?…Nice! . A weather app which uses a weather API to display information on a front-end interface?…I’m listening. . A huge multi-microservice program which really pushes the boundaries of your knowledge and skill?…Be my guest! . All personal projects, no matter the size, give you scope to improve on what you have learnt at work, or start fresh and learn something totally new - and hey, if you enjoy what you’re building, 9 times out of 10 you will find the time and come back to it. For the other one time, the sheer fascination that your family members will have for your project (especially if they are in no way techy like mine!) may coerce you back to the keyboard. . Nobody wants you to fail, and every single developer you will meet once started out their journey in the exact same position that you are in right now, as a wide-eyed junior developer.  This is something I really came to terms with at Makers. They put it into perspective by telling me that even the best developer doesn’t know everything. I may have some information that a developer with 15 years’ experience may need. This was instilled in their ethos and passed onto the students. As a cohort we would all help each other because we knew that the next day, we would need that same help with something ourselves. This concept is explained brilliantly in this article titled  When It’s Clever to Admit That You’re Not Feeling Clever  written by Andrew Harmel-Law. . This culture can also be found at Capgemini and the sooner you become comfortable with asking for help, the sooner it can be your cushion of hope when you are stuck. . Let’s face it, we all like it when someone asks us for help. It makes us feel like the person asking thinks we are smart enough to know the answer! Asking a fellow junior developer will comfort them as they will have more confidence in their own knowledge if they have the answer and asking a senior developer will show that you are willing to learn and contribute to the team. Also, you never know, by asking someone for help and  explaining your problem , you may find that you knew the answer all along. . I remember my first day on my first project at Capgemini. I had my notepad and I said to myself “I’m going to write down every technology, every keyword that I don’t understand so that I can get up to speed”. (A+ for effort! F for realism!) . By the end of the day I couldn’t see the beginning of my list. . It was way too much for me to feasibly go and research and take in. So, I tried to break it down as best I could. By focusing on the tools and technologies that I was going to be using near enough everyday it gave me a much more solid foundation to build from. . Personally, if I work on something for a few days and just get the bare basic knowledge, then move onto something totally different with the same cycle repeating, I find that I actually don’t learn a lot about anything. It’s only when you get your hands dirty and get involved that you truly see how something can work and how you can utilise it. . This way you’re taking the time for new information to be absorbed before you can soak up something else. . What is  impostor syndrome ? Some of you may not have heard of it, I’m not sure I did until a few months into my coding journey. It is a sense of feeling that you are a fake, not good enough at your job and that others around are doing better than you. Failure to recognise achievements, instead focusing on mistakes and shortcomings, fuel this fire. . But why might it never go away? Well, technology is constantly evolving. New languages and tools make it easy for a developer to feel a sense of inadequacy or that they are back to square one. It can, however, be easy to accept and manage. . Keep track of your achievements and take time to look back on how far you have come, not just in the last year but even in the last month or so. Recognising that you have felt this way before and looking back to remember what you did to overcome it can be useful. . We always tend to underestimate ourselves, but acknowledging that you learn a lot more than you give yourself credit for and realising that your role requires constant learning will give you a head start in accepting impostor syndrome. . This one is last, but it is probably more relevant now than all the others put together. . Due to the current national situation in 2020, a lot of us are now working from home. I’ve gone from an office location in London with a team of 15+, to a home-office location in the West Midlands with me sat at a desk alone with my computer. Staying connected with my project team and keeping channels of communication open have become crucial. As we have inevitably become distanced from one another, we have made changes in our day-to-day to try and talk more often. . Three of the main changes we implemented were: . A designated Microsoft Teams channel/call that can be used by anyone when they have a problem, all team members can drop in and out as they wish. . Turning video on when on conference calls . Daily catch up at the end of the day. This discussion is open to anything and everything, work related or not. . I know the movie industry makes coders out to be introverted, just sitting in a dark room, hacking away. But for me, programming is a social activity. I wouldn’t be able to do this job without the help of others around me. . The changes my project team have made to try and maintain the social aspect to work has meant that throughout lockdown we have continued to deliver as much as we did whilst in the London office, if not more. . More importantly, it has meant that we have all remained a close team and no one has been left isolated. ", "date": "2020-11-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "An Introduction to Apache NiFi, Use Cases and Best Practices\n", "author": ["Jonathan Smith"], "link": "https://capgemini.github.io/development/introduction-nifi-best-practices/", "abstract": " Apache NiFi is a visual data flow based system which performs data routing, transformation and system mediation logic on data between sources or endpoints. . NiFi was developed originally by the US National Security Agency. It was eventually made open source and released under the Apache Foundation in 2014. This has brought several advantages to the software, creating a large community user base providing support and regular updates. . I have found NiFi to be a particularly useful and rapid solution to apply to problems involving a flow of data from one or more endpoints to another that require some routing and manipulation or validation along the way. Most use cases require little to no coding, as all functionality is provided by the many bundled processors. . Each processor performs a specific given task. It is also possible to write your own processor using Java, or use one of the many written by the NiFi community. . NiFi supports a number of different endpoints including, but not limited to: .  File Based, Mongo, Oracle, HDFS, AMQP, JMS, FTP, SFTP, KAFKA, HTTP(S) Rest, AWS S3, AWS SNS, AWS SQS  . Apache NiFi can be set up by following the steps for your  operating system  . After following a few steps and navigating your browser to the default port 8080, you should be presented with the NiFi User Interface, which looks like the following: .   . This is a canvas where processors can be dragged and dropped and can be connected via flows to other processors and endpoints. .   . A processor in NiFi performs a discrete task and has one or more inputs or outputs. A freshly installed NiFi comes with a plethora of processors to choose from. We will use a few of these in the simple example flow later on in the post. . Other than inputs and outputs, each processor has settings, schedules and properties, which are unique to each processor and allow changing its behaviour. . An example of a processor may be  GetSQL  or  PostHTTP  -  Processors are clearly named to describe the exact operation they perform. .   . A flow is what connects one processor to another processor, port or funnel, and works in one direction. A flow can be created by dragging a line on a canvas from one object to another. . Each flow has its own internal queue. This queue allows buffering to occur when the downstream process is slower, paused or stopped for whatever reason. Flow queue contents are stored as files on the NiFi server storage, so this needs to be taken into consideration when designing a flow. The queues parameters can be tweaked to allow a finite amount before either deletion, or back-pressure is applied. This post won’t go into either of these advanced topics but I may cover these at a later point. . Usually what happens when we are using a processor, is that each processor has been designed to have one input and more than one output, covering at least one success state and at least one failure state so data can be routed onwards appropriately. . A funnel is a conduit aid that allows many flows to converge into a single flow, which helps to ensure a clear layout when lots of flows all have the same fate. .   . Groups, or Processor Groups encapsulate a flow of processors into a single object on the canvas, to group any related bits of functionality to tidy away into a named processor group. This processor group can then connect to other processor groups, processors or funnels like a standard processor. . To delve deeper into the processing group, double click to reveal its flow. Processor groups can be used effectively when dealing with large, complex flows that have several discrete functions, utilising a processing group for each function. . Processor groups use ports to terminate the internal flows and allow them to be accessed outside the process group. A processor group typically has at least one input and one output port. . A NiFi flow transports data from the start to the end of the flow in chunked file content known in NiFi as Flow Files which, by default uses the storage media where NiFi has been installed as a mechanism. . A Flow File consists of Flow File Content and Flow File Attributes. Flow File Content is the entire content of the data of the file, for example the contents of a text file. The Flow File Attributes are metadata attached to each flow file, represented in key value pairs. These attributes are sometimes set by processors themselves, to indicate certain properties or status of a flowfile, but can also be manipulated by the flow designer themselves (This is covered later). . Each flow file can be visually traced on the NiFi UI through the displayed counts as it transits through the flow. We can even look at the content and attributes of a flow file itself when it resides in a flow queue. Right-clicking a flow queue and selecting a flow file presents the file attribute values and content. .     . Let’s have a go at creating a flow to solve a (very simple) problem. I have a dataset of files in a directory that is populated from some external system. I want these files sorted and placed into different folders based on their content. . These files could really be any format but I have picked files containing JSON file content. This is just random data I have created. .  5fa80170fa67ed319b684985.json  .  5fa80170a8ff51d6d8a09a69.json  .  5fa8017059de621e415058c2.json  .  5fa80170bb4d9e4b5bec5594.json  . I want these files to be placed into two different directories, current and savings based on their  accountType . The first thing we need to do is to add a processor which will read the files in a directory and turn them into flow files. I have used the  GetFile  processor to do this. Drag the processor symbol in NiFi to add this processor. Once added to the canvas, set the processor properties to similar values to scan an input directory for JSON files: .     . Now that we have the input stage of our flow defined, we need to be able to read the flow file content to be able to decide which direct the files need to be written to. There are a few ways of doing this, but I am using an EvaluateJSONPath processor to read the file content and extract the  accountType . .   . This processor supports addition of custom properties to be able to read custom defined JSON parameters to extract. Add the property name and value above to extract the account Type JSON value from the file into a Flow File Attribute called  accountType . The “$.accountType” Is JSONPATH syntax for grabbing a key called  accountType  from the root JSON node. . Next we need to be able to route our logic based on the  accountType  flow attribute. To do this we need to use the  RouteOnAttributeProcessor . Add this to the canvas and add the following properties: .   . The values for the additional properties make use of something called NiFi expression language, which we will touch upon later, but this is a way of routing to the two different routes based on the setting of  accountType . . This routing logic is one of the common design patterns within NiFi. I’ll probably look at some more common design patterns in the next iteration of this post series. . Now that the routing logic is present, we need to add a way of writing the flow files to a folder on the storage. To do this we use a  PutFile  processor. . Set the properties similar to the below. Note that we will need two of these, one for savings and one for the current account type.   . All we need to do now is connect the processors up with flows. Simply drag a line from each processor to the next to create a flow for each success and failure state. . It is good practice to use funnels as a way to extract out terminal flow queues from the flow. E.g. any failures from individual processors and the final success state from the end of the flow. . In the end, your flow should look something like this. If there are no errors a stop symbol will be displayed next to each processor (if you get a warning, hover over the symbol and remediate the issue that is stated). .   . That’s it! A routing system for files in about 10 minutes! To test your flow out you can start each individual processor by right-clicking and selecting start, or by use of a shortcut by right-clicking the canvas and selecting start, which will start all processors in the current canvas. . The flow should route the files to the correct directories as seen in the UI: .   . Check the directories to see that the files have been sorted. . In the examples above, we edited the processor properties to add values directly to the processors, such as the file paths, JSON keys to extract et cetera. Is this always the best approach? What if we had a similar file path that is used in more than one processor? . We can use variables as the values for properties and set this variable elsewhere once which will affect all processors that use the variable. Any processor properties that support the NiFi expression language (You can always check by hovering over the question mark icon next to the property). . We are going to change our example flow to abstract out part of the common path we use for our input and output processors. Currently they are set to something like  C:\\Users\\jrsmi\\Documents\\nifi-test-input,C:\\Users\\jrsmi\\Documents\\nifi-output-savings, C:\\Users\\jrsmi\\Documents\\nifi-output-current . . We can extract out the base path as it is the same for all of the processors. . Right-click on the canvas and select variables. Add a new variable basePath and set it similar to the following: .   . Now go into the  GetFile  and  PutFile  processors and replace the preceding part of the  filepath  string with the new variable(note that you will need to stop the processor first before making any changes) . All variables in NiFi are represented using the syntax  ${variableName} . The processor properties should look like the following: .   . That’s it! Try playing your NiFi flow again and check it still works. . Previously we talked about processors automatically setting Flow File attributes based on a processing operation performed on a Flow File. Flow file attributes can also be set manually by using the  UpdateAttribute  Processor as part of a flow. This is useful in a number of cases, one of which we can demonstrate by selectively setting the filename in our example flow based on which account route the flow goes down. . Add an  UpdateAttribute  processor and add a new filename property to the process as per below. .   . Now add this processor in between the  RouteOnAttribute  and  Putfile  processor for the current account. . Change the output directory property of the  PutFile  processor to simply use  ${filename} . The flow will look like the below diagram. .   You have now demonstrated the use of setting flow attributes dynamically. . This can be very powerful with certain flow logic, particularly when determining where a flow file originates from and what should happen to it. . The NiFI expression language is a powerful way of adding logic and manipulation into processors in a pseudocode fashion. We have used the NiFi Expression Language a few times already to perform variable substitution and check whether an attribute contains a certain value to perform routing. . NiFi expression language is primarily written in the values of processor properties. The NiFi expression guide can be found at  NiFi Expression Language Guide  or by right-clicking any processor and selecting the “Usage” option. . A few common examples of function that can be performed using the NiFi Expression Language are: String Manipulation, Maths Operations, If Then Else…. DateTime operations and encoding. Some of these functions will be explored in the next parts of this guide. . This is by no means a list of strict commands you must follow, as your use cases may be different and warrant a slightly different approach, but more of a guidance to ensure some good practices are met when designing flows. . I will be adding to this section with more good practices and guidelines to follow. Please feel free to suggest any good practices you follow. .       The layout of a flow should be modelled on gravity. Gravity pulls objects down towards the earth and flow files should generally follow this approach where successful flow files are pulled down towards the bottom.     . The layout of a flow should be modelled on gravity. Gravity pulls objects down towards the earth and flow files should generally follow this approach where successful flow files are pulled down towards the bottom. .       When a logic decision is made, for example route on attribute or some other place the flow splits, the flow should move horizontally, right or left, before continuing on its downward path as seen in the example below.     . When a logic decision is made, for example route on attribute or some other place the flow splits, the flow should move horizontally, right or left, before continuing on its downward path as seen in the example below. .       Error handling and failures should also be handled horizontally where possible, at the same level as the processor which produces the failure.     . Error handling and failures should also be handled horizontally where possible, at the same level as the processor which produces the failure. .   . All processors should be renamed from their defaults to give a good description of what they are doing. E.g.  RouteOnAttribute  renamed to  WhichAccount ,  PutFile  to  PutSavingsAccount  and be unique from any other processor. This doesn’t just help visually to distinguish the processor, but also when reading the log file to pinpoint an error when something goes wrong. . I have seen multiple ways of handling flow file errors - some good, some bad. Here are some of my recommendations to follow. . Avoid auto-terminating flows, always add queues for each flow condition so error conditions can be spotted quickly . Don’t be tempted to group flow conditions together like in the diagram above with failure and unmatched (this was done for simplicity) as it becomes difficult to distinguish which flow condition resulted in the file on that queue . For a similar reason, do not be tempted to do what I see often and route all failures from processors into a single failure point. Users of the flow should be able to see by the use of queue counts how many files have resulted from each processor flow status. . Utilise these wherever possible to group common functionality. For example we could make our example flow contain Input, Sorting and Output processing groups, which would encapsulate the related logic . Don’t make processing groups any more than two levels deep, it becomes very easy to get lost and a chore to drill down into N levels of abstraction . Use variables when and wherever possible to control all flow parameters from a single place. The top level is usually a good place for variables that have an effect on all processors and processor groups, otherwise it is good to have variables at the processor group level. For example when you have two output stages that will have different variable values. . In Part 2, we will look at securing the NiFi Control UI using certificates and access control roles and groups, posting and reading from REST endpoints, the NiFi registry, additional design patterns, managing source control changes to the NiFi flows and Clustering/High Availability. ", "date": "2020-12-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Distributed Tracing with OpenTelemetry & Jaeger\n", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Distributed-Tracing-with-OpenTelemetry-And-Jaeger/", "abstract": " Scientific inquiry starts with observation. The more one can see, the more one can investigate - Martin Chalfie . Imagine if I was to approach you during your lunch and tell you that there has been an increase in customer raised issues detailing the occurrence of errors whilst processing online transactions, what would be the first question you ask? Which service? What page are they getting the error? Depending on your system and knowledge of said system, you may have completely different questions to ask. Back when monoliths were the standard practise, normally, things were slightly easier to track with regards to the root cause. This was because you only really had to look in one place – a big place, granted – but one place none the less. . Since microservices have become the standard practise nowadays (even in cases when they shouldn’t be), companies have been bending over backwards trying to adopt them. The naive assumption was made that upon adoption, many problems disappear the second you turn off the monolith switch. The experienced among us know this is far from the truth. In fact, in many cases, companies have just traded one problem in for another. Sure, monoliths could be horrible to work with, but if there was a problem, you always knew where to look. Since the widespread adoption of microservices, companies, for the most part, have certainly experienced the benefits that they bring, but have done so at the cost of a more complicated architecture. . The problem domain has gone from “wrestling with the big beast” to “managing these little demons”, and part of being able to effectively manage something is being able to see it. By this, I don’t mean seeing it in a deployment diagram – or even a  Kiali  dashboard of your  Istio  service mesh. I’m talking specifically about being able to see your services interact with each other during the request cycle – this is called distributed tracing. . There are three main pillars to observability: . Tracing . Metrics . Logging . In this post, I will be touching on distributed tracing, one of the main pillars of observability. Distributed tracing provides you with the insights into the full lifecycle of requests in your system. If a user logged onto your website, depending on the complexity of your system architecture, they may hit a few services along the way. Each request that they make, will be traced through the system. Having a strong observability platform will allow you to gain insights to the details of the trace and even uncover issues with your system that you were never able to see. Where are the performance bottle necks? Was there an error, if so, where did it start from? All of these are questions in which, when answered, allow you to improve your systems user experience. . Upon preparation of the working example for this post, I initially started using  OpenTracing . It was at the half-way point of my Java service when I thought to myself, why am I still using OpenTracing? Especially since it’s merged with  OpenCensus  to become  OpenTelemetry ? And now you probably ask, what do all of these words mean? .  OpenTracing  – a  CNCF  project, now an incubating project – was/is a vendor-agnostic standardised API that allowed engineers to instrument traces throughout their code-base. It allowed for the creation of instrumentation libraries that would essentially wrap around application code in order to record and report trace information. .  OpenCensus  – a Google project – is a set of libraries that allow you to collect application metrics and distributed traces in real-time. Similar to OpenTracing, it required the engineer to instrument the API calls into their code with the additional benefit of capturing metric data. . The problem engineers had with the two options above is deciding which one to use. Should they use OpenTracing for tracing and OpenCensus for metrics? Or should they use OpenCensus for both tracing and metrics? This is where OpenTelemetry came in. . OpenTelemetry (OTEL) was formed by the merging of OpenTracing and OpenCensus. Currently a CNCF sandbox project and its second most active in terms of contributions – Kubernetes being the first –, OTEL since its inception aimed to offer a single set of APIs and libraries that standardise how you collect and transfer telemetry data. . OTEL not only aimed to simplify the choice, but it also allows for cross-platform capability with SDKs being written in several different languages. Its architecture and SDKs allow for companies to develop their own instrumentation libraries and analyse the trace information with supported platforms. We will look at this in a bit more detail during the example below where we use OpenTelemetry to create and collect the telemetry data whilst exporting it to a  Jaeger  backend to visualise them. . Developed at Uber and now a CNCF graduated project, Jaeger is a distributed tracing platform that was inspired by  Dapper  and  Zipkin . Jaeger allows for the visualisation of request traces through a set of services that exist in a system domain. This functionality is incredibly powerful as it allows engineers to quickly pinpoint exact points of failures in what could be a complex and intricate architecture. . Without talking too much, let’s get into an example. Before I go any further, I feel that I have to make it known that the purpose of this post is to shine a light on the power and capabilities of OpenTelemetry, Jaeger and distributed tracing. If you’ve been reading my blog posts previously you know that I tend to write a step by step walkthrough on what I’m doing. For this post, I will be assuming that you have a good knowledge of Docker and microservice APIs so that if you wanted to run through what I’ve done you could check out the GitHub repository and run it yourself without me walking you through it. So now that the disclaimer is out there, let’s get started. . For the purpose of this post, I have created a basic employee management system that allows for the creation and retrieval of employees and their details. The repository  is hosted on GitHub  and contains multiple different services and databases that each have their own simple function to perform. The below architecture diagram outlines the different components. .   .  Front-end service : Simple front-end service written in VueJS .  NodeJS API : Simple NodeJS Express API that serves the data for the front-end and communicates with the three backend services .  Java Backend API : goes to a PostgreSQL database in order to collect the associated salary grade of the employee’s official title .  Go Backend API : writes/retrieves to/from the employee Mongo database .  Python Backend API : retrieves the salary amounts for a specific grade from a MySQL database . Each of the languages that were chosen all have auto-instrumentation libraries. What this means is that I didn’t have to write any code in order for the trace information to be collected and sent to the backend. This is especially great for companies who may not have the necessary expertise or time in order to create a comprehensive tracing framework that is specifically tailored to their requirements and business domain. Due to the fast adoption and progression of OpenTelemetry, there are many auto-instrumentation libraries that aim to do all the manual lifting for the users of their frameworks. An example of this, is that for Java there is an auto-instrumentation  .jar  file that you can attach as an agent to the JVM inside the container that automatically records the information you need and sends it to the place you want to send it. For NodeJS there are built-in extensions for the Express library so all of the requests in and out from your API endpoints are automatically traced and collected and sent to the analysis tool of your choice. In some languages like Rust or even Golang, the level of auto-instrumentation can be limited depending on its popularity as a language. From my basic observations, the Java and JavaScript instrumentation libraries seem to be the most supported and extended. I did try to make a full example of all of the currently supported languages by OpenTelemetry, but due to most of them requiring more heavy-lifting than I originally planned to do, I thought JavaScript, Java, Golang and Python were enough. . To run the stack, I will bring all services up by using the docker-compose file in the  jaeger-distributed-tracing-demo  parent repository. It has all services and databases as git submodules for easier management and maintenance. If you are running this yourself, you will need to build the  java-service  just so the Dockerfile can use the JAR file. . Note: if you are running this yourself, the first run will take a bit of time due to the downloading of the base images and the building of the service images. . Once all of the services and databases are up and running, there should be two URLs that should now be available. The employee management front-end, and the jaeger front end. . Navigating to  localhost:8080  takes us to the employee management service front-end where a few text boxes and buttons exist to allow us to create and get employees. .   . Let’s create an employee called  Joe Bloggs  and let’s give him the title of  Senior Software Engineer . .   . Now, in the background, the front-end service sent a  POST /create-employee  request to the NodeJS API which has in-turn sent a  GET /salary-grade/{title}  request to the  java-service  that gets the salary grade (in this case A3) for the title  Senior Software Engineer  and returns it to the NodeJS API which subsequently uses it in a  GET /salary-amount-for-grade  request to the Python service which gets the minimum and maximum salary amounts for the salary grade from the database which in this case is 40000-49999. When the Python API returns both the minimum and maximum salary amounts back to the NodeJS API, it will calculate a random number between the two and then sends all employee details to the  go-service  via a  POST /employee  request that inserts all data in the MongoDB employee database. . Paired with the architecture diagram, the above explanation may be somewhat easy to understand. However, the architecture diagram does not illustrate the request flow that was followed exactly. This is where Jaeger comes in. . As part of the OpenTelemetry export functionality, all trace data can be sent to a direct analysis tool like Jaeger or  Zipkin  or it allows you to send it to a proxy (that could be an OpenTelemetry collector attached as a side-car to the service container) which further exports all data to an analysis tool of choice. Additionally, if you are using OpenTelemetry for metrics, all data can be exported to a metric analysis tool of choice such as  Prometheus . Due to the simple local setup I’ve got, I export all data directly to Jaeger using the jaeger-exporter functionality offered by the relevant libraries and SDK. An example of this can be found in the  go-service  . Enough of the what’s happening in the background, let’s see some request traces. . If I navigate to the  localhost:16686  URL, I get sent to the Jaeger front-end. .   . Now as you probably can see, there are some services already populated in the drop down at the top left. If there are traces exported to Jaeger, it will add the service to the list. Now, since it’s already got the  nodejs-service  already populated, let’s ignore the rest of the options and click  Find Traces . . Here, we’re shown two traces - the top one being what we’re are interested in. .   . Once clicked, the entire trace is displayed on-screen. .   . If you are wondering why the front-end service is not on the trace as it was essentially where the trace should have started, the reason for this is because although VueJS is supported by OpenTelemetry via its  “web” package in the “opentelemetry-js” repository , currently the JavaScript jaeger-export libraries (I am using it in the NodeJS API service) is not supported  in the browser . This means that the VueJS front-end service, although it creates OpenTelemetry data, cannot export it to Jaeger - which is why we cannot see the front-end service in Jaeger. . If I was to use Golang for the front-end there would have been no problems, however, I wanted to learn some VueJS and treated this example system as a means to do exactly that. . Now, let’s look at the trace, we can see that the request was sent to the NodeJS API and the subsequent calls to the other back-end services are also listed in the Gantt chart. We have a good idea of what requests and which services, took the longest to respond to their caller, additionally, we can see the details of the endpoints that were hit for each service. .   . Let’s expand the  go-service mongodb.query  and see what we get. .   . So, not only are we able to see the exact service endpoints that got hit during the request, we are now able to see the exact  db.statement  that was sent into the MongoDB. This functionality is offered to us out the box by the  otelmongo  auto-instrumentation library for the  Golang implementation of OpenTelemetry . PROTIP: For secure sectors like Government systems, you may want to avoid displaying database queries via any front-end due for the obvious security reasons. . If you have the example system up and running locally, try playing around with the panels in Jaeger, you will also see that the PostgreSQL query that was executed by the  java-service  is also visible, just like the MySQL query that was executed by the  python-service . . Now that we have created the employee  Joe , let’s try retrieving him. . On the front-end, when we created our employee, a message was displayed returning the employee ID. When we use it in the get employee section of the page, we will see the following details that were saved to the MongoDB Employee database. .   . There are some additional bits of information that we can see there that we did not enter into the original employee creation step, these details were what was returned by the java and python services, alongside some additional timestamps that are added to the record by default by MongoDB. . Now let’s look at the trace in Jaeger. If we go to the screen and just click the  Jaeger UI  button at the top left we get sent back to the home screen. This time, let’s search for the  go-service  in the drop-down box and click  Find Traces . .   . Now, the  GET /get-employee  query is there as the first entry and it displays the  nodejs-service  service and the  go-service . If you’re wondering why it shows all services, even though we chose a service that is lower down in the request call stack. This is because they are all registered under the same trace-id. In this case, the trace-id is  1bf3249  and if you were to enter that into the search bar at the top left, you would get given this same screen. . How this works more specifically is the  trace-id  is the id of the overall trace that has the series of events inside it. Each service creates one or more Spans that are each a unit of work in Jaeger. A trace is essentially a collection of Spans (has to have a minimum of one Span). Services can be programmed to create many spans, in our example, all of the auto-instrumentation libraries that we are using essentially create the Spans for us. All of this data gets exported to Jaeger by the exporter libraries in each of our services, and Jaeger essentially puts it all-together for you to visualise. . So, in this case, because the  go-service  and the  nodejs-service  are essentially reporting Spans to the same trace, this is why we see both services when click the  go-service . We aren’t saying, show me all of the  go-service  Spans, we are asking Jaeger to show us all traces that the  go-service  reported spans too. .   . Similar to the create employee trace, we have a Gantt chart displaying the timeline of the request trace. We can see that the request went from the  nodejs-service  to the  go-service  and we can also see the endpoints that it hit along the way - as well as the  mongodb.query  that was executed against the Mongo database. . Now, I know that although this looks nice to visualise and is very cool to see. The question “Why do we need all this?” may still be asked. Other than being able to see the request flow through the system, and the timings of them - which could actually give you an idea of where the typical bottle necks are for performance - it actually tells you where the issues started. Thinking back to the first question I asked in this post, imagine you’re on lunch and get given a load of user raised issues, all reporting errors. Where do you start? Is a service down? Is the database broken? Is there an outage? Well, let’s replicate that scenario and see how distributed tracing helps us investigate the issue. . So, let’s say, for example, the  go-service  was to be “down” due to some service crash. Let’s also assume that there are no recovery protocols in place to restart the service when this happens. What happens if we were to try and create another employee when the  go-service  is down? To replicate the service being down, I will simply stop the docker container and rerun a create request using the name  Mike Bloggs  with the title  Lead Software Engineer . . So, on the front-end, when I clicked  Add Employee  nothing happened. As the user, I don’t know why, no error got returned through the screen. So, my natural instinct is to click it again. And again. And again. But still nothing. .   . Eventually, I get an error back. .   . Now sure, it took too long because of timeouts, an easy fix, just add it to the backlog. However, let’s say one of your services didn’t have retry logic or timeout handling. You would experience the same thing. The chances are, a user wouldn’t hang around long enough to wait for the error to be returned. They would either leave the website and never return or they may be kind enough to file a bug to let you know about the problem. . This is where the power of distributed tracing comes into play. If we refresh the Jaeger UI and click  nodejs-service  in the drop down and click  Find Traces , now we have the following. .   . Technically, when someone files a bug, you would normally have some information about the screen they were on which allows you to narrow things down to the specific service. For us, we know that the problem happened on the front-end, but since we haven’t got the VueJS Jaeger exporter functionality currently, we go to the next service down in the stack that has got the functionality - the  nodejs-service . This is why we selected the  nodejs-service  in the drop down and not the  go-service  - although that’s where the problem is (which you wouldn’t actually know in a real scenario). . Looking at the Jaeger UI we can see that there were 2 errors. Let’s click this trace to see more information. .   . We can see that at the bottom the  nodejs-service   POST /employee  request failed. Let’s click this to see more information as we still don’t know why. .   . That’s more like it. We can see that the  POST request  http.url http://jaeger-tracing-go-service:8091/employee  failed with an error saying  ECONNREFUSED . With this information we’ve narrowed the problem area down to a specific service. Is the service down? Or is the URL we are sending the request to wrong? Due to the tracing data we now have, we can investigate further. As Martin Chalfie  describes   “the more one can see, the more one can investigate” . . This is the power of distributed tracing. Giving you the insights to the interactions between your systems components and domains. There are additional features in Jaeger that allows you to visualise the exported data in different ways. If you’re running my example system, feel free to have a play around. . Now, building on top of the above. Let’s say there is a problem with the database (but act like you don’t know it yet). If I stop the PostgreSQL database that the  java-service  uses and rerun the same create employee request on the front-end, what do we think will happen? . Well, when the error eventually gets thrown to the front-end, let’s refresh the Jaeger UI whilst clicking  Find Traces  on the  nodejs-service . .   . Oh wow! There are now 15 errors? Let’s click the trace to find out more. .   . That’s a lot of errors coming from one service. But any experienced engineer could tell you that there is obviously some retry logic being used in the Java service (offered out of the box by Spring) and due to the problem still occurring after the 10th try, it eventually fails and reports back to the caller - which in this case is the  nodejs-service . . Let’s click the first error entry for the  java-service . .   . Interesting. We are getting a JDBC connection error. Any java engineers know that JDBC connection errors are database connection issues. . Let’s investigate further by clicking lower down in the  java-service  errors. .   . Bingo! Based on just the one panel being expanded, we can see from the java stack trace there was a connection issue whilst dealing with the PostgreSQL database. This now gives an engineer more information on where the problem area is and will swiftly lead to the discovery of the root cause. . Something I don’t want you to miss, is the  Logs  section. If, as part of your code, you are logging any output, it will be displayed in Jaeger as part of the data of the Span. Also, I hope you recognise that we can also see that the  go-service  and  python-service  spans are not in the trace. This is simply because the  java-service  is the first back-end service that the  nodejs-service  calls, so an error there means no calling of either the  python-service  or  go-service . . Lastly, a quick observation from me and my experience of dealing with some of the languages for this demonstration, it seems that the Java auto-instrumentation libraries are far more comprehensive than the rest. However, due to the rapid feature additions and contributions made to OpenTelemetry monthly, big strides will be made in all other languages and their frameworks over the next few years. . Although this was a very simple demo application, it’s easy to see the benefits of what distributed tracing brings. In fact, one thing I can say for certain, was when I was testing the front-end, and I wasn’t getting the response that I expected, it was actually much easier to just look into the Jaeger traces to find where the problem was occurring - as opposed to just looking at the  docker-compose  logs. . For very complicated systems and architectures, distributed tracing becomes pretty much invaluable. With the auto-instrumentation capabilities that come out the box with the most popular frameworks and languages make it incredibly easy to get up and running. It took me around about a full week of accumulated hours to get a very simple employee management system in a working state where I could use it for this post - that is for the different languages and frameworks.  Normally for companies who only really have one or two main languages, it should be relatively easy to adopt and trial proof-of-concept work with OpenTelemetry &amp; Jaeger/Zipkin. Depending on your architectural maturity and platform capabilities, you may be able to get a full working distributed tracing capability within your clusters - all thanks to the auto-instrumentation that is offered by OpenTelemetry. If, however, you have a bit more money and time to spend on developing an effective observability platform, you may even want to implement manual instrumentation that more effectively suits your business needs. . I hope you’ve learnt something in this post, if you have any questions, feel free to contact me on the socials. If you have any issues with the example employee management system, feel free to raise an issue on the  GitHub repository  and I’ll get back to you as soon as I can. ", "date": "2020-12-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Externalizing Spring Boot Config with Kubernetes\n  ", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/engineering/externalising-spring-boot-config-with-kubernetes/", "abstract": " Spring Boot and Kubernetes go hand in hand when building modern microservices.  Spring Boot  is a widely used JVM-based framework which allows you to build out stand-alone, production grade applications.  Kubernetes  is an open source container management and orchestration system, which makes it quick and easy to deploy and manage those production grade applications. . I’ve recently been working on a project where we’re writing Spring Boot microservices, which are being deployed into a Kubernetes cluster. My previous experience has primarily been with  Docker Swarm  up until this project, so it’s been an interesting switch with different challenges. One of those challenges was how to externalize the Spring Boot configuration within the Kubernetes cluster, I’d thought it would be similar to Docker Swarm, but there were several differences I found along the way. . In Docker Swarm, you  deploy services via a stack file , this is very similar to docker-compose for those familiar with that approach. You can define environment variables in the stacks files which are then passed to the running containers. . In Kubernetes, everything is a resource, and you  manage those resources  through manifest files. Similarly to Docker Swarm, you can  define environment variables  for your Kubernetes Pods and Deployments within those manifest files. Defining environment variables within a manifest file is really useful, as it allows you to abstract configuration values away from the application you’re building. However, it can become difficult when you have a lot of configuration that you want to abstract, on the one hand, you want to abstract that configuration to separate the concerns between the application and the resource management, on the other, you don’t want the resource definition (manifest file in Kubernetes) becoming overly verbose and unreadable. . Enter ConfigMaps. .  ConfigMaps  in Kubernetes are a great way of abstracting your configuration values away from your application, and also decoupling your configuration from your image, which ensures your application is more portable. . A great usage of ConfigMaps is to externalize your Spring Boot configuration away from your application. If we’re building applications in the right way, most production grade services will have gone through various environments before finally being deployed into production. Within each environment, resources are likely to be different, even from the most basic aspects such as datasource endpoints. One of the best things about building containerised applications, is it gives you the ability to ensure your application operates consistently in each environment. One key enabler of this is to externalize your configuration. That way, the image you build doesn’t change throughout each environment, only the configuration that gets injected into it. . By using a ConfigMap, you ensure you keep your application image the same in every environment, you abstract your application configuration away from the application itself and you have a specific, separated resource to manage that configuration. . A good example (I find) of externalized configuration is for a datasource. I’m sure many of you reading this have, at some point, had multiple configuration entries in your application for your datasource in each environment, I know I have. From experience, I find this bad practice for several reasons; it bloats your application configuration in your service, it ties your configuration to your service code and it means you have to update that application code (and therefore your image) for each environment you want to deploy into. . An alternative approach, and an approach I’ve taken on my current project, is to utilise Kubernetes ConfigMaps to hold the configuration for our Spring Boot services. . To show the approach I’ve taken, I’ll run through an example of configuring a datasource for a Spring Boot application and externalizing that configuration with a Kubernetes ConfigMap. . Let’s say we have a simple Spring Boot application, which connects to a MySQL (other databases are available) instance. We have the following configuration in our  application.yml  file: . When the application is running, the above configuration will allow us to connect to a  local  MySQL instance, great! . Now let’s say we know we’ve got at least 2 other environments we’ll be deploying to before production. We could update our configuration as follows: . The above is fine, it’ll likely work and get the job done. However, what happens if the endpoint changes in dev? Or the username gets changed in pre-prod? We’d need to make application code changes, re-build the image and then re-deploy to each environment. Another major flaw to this approach is that you’re keeping sensitive data such as usernames and passwords in your application code, which is a major security issue for production grade applications. . An alternative approach would be to use  Spring Boot externalized configuration  using property place-holders to abstract the actual property values away from the application code, like so: . This approach gives you far greater flexibility; you don’t have to change application code each time your configuration changes, the same image can be promoted through each environment and you abstract your configuration values away from your application code. . Now that we’ve created the configuration file for our application, we want to build it as a Docker image. This is really simple, but you can make it as sophisticated or complex as you like, depending on the requirements you have for the image you’re building. . In this example, I’ve used Maven to build the application, so assuming we’ve run a  mvn clean install  prior to building the image, we have the built jar file ready to go. . We can create a very simple Dockerfile: . The above Dockerfile uses the  openjdk:11-slim  base image, copies the jar file we created earlier from the  mvn clean install  and runs a Docker  CMD  to execute that jar file. . Now we can run a docker build to create the above image ready for use: . As mentioned previously, Kubernetes has a specific resource type for managing config resources: the ConfigMap. In order for us to abstract our datasource configuration from above in the Kubernetes space, we can create a ConfigMap with the desired values. It’s worth noting there are several different ways this can be achieved, in this example though, I’ve used the  spring application json  method. . There are multiple ways of creating a ConfigMap in Kubernetes, most commonly using  kubectl . In this example we create the ConfigMap from a file called  dev-configmap.yaml  with the following contents: . In the file above, we create a resource kind of  ConfigMap  called  spring-config  - the name is important as we’ll need to know this in order to refer to it later. Every ConfigMap has a  data  section which can contain anything you like, in our example, it contains a JSON entry with a key of  dev-config.json . . In order to create this ConfigMap in our Kubernetes cluster, we can run the following command to  apply  the file: . By this point, we have our application ready to accept injected configuration properties and a Docker image created from that application. We also have a ConfigMap deployed into our Kubernetes cluster, so now we can bring that all together by deploying a  Pod  into our cluster, that uses our newly built image and ConfigMap. . Kubernetes Pods are the smallest deployable units in Kubernetes. We’re going to create a very simple Pod definition ( example-pod.yaml ) that deploys our image and uses our ConfigMap as an environment variable for the running container. . Okay, so what exactly is the above Pod definition doing I hear you ask. Firstly, we know it’s a Pod because of the  Kind  - which is Pod. Secondly, we’ve given it a  name  of config-demo, this will be the name of the Pod in the Kubernetes cluster. Next, we define a  spec , which is the Pod specification, it defines everything needed about this Pod to Kubernetes. We define a  single  container, which uses the image we built earlier ( config-demo:latest ) and defines some key information such as ports to be used and the environment ( env ) variables to be used. . We’re going to focus on the  env  definition. As you can see from the Pod definition above, we’ve defined 2 variables, firstly the  SPRING_PROFILE  which has a value of  dev . This indicates to the running Spring Boot application which active profile to use. As we defined several configuration entries in our  application.yml  file, it’s important we set this value so that Spring knows which config set to pick up. Secondly, we define a  SPRING_APPLICATION_JSON  variable, which is referencing an entry from our ConfigMap we created earlier. . Focussing on the  SPRING_APPLICATION_JSON  variable, defining an  env  variable this way in the Pod, allows us to pull data from a ConfigMap running in the cluster via the  configMapKeyRef  value type. What this is doing, is looking in our Kubernetes cluster for a ConfigMap called  spring-config  (that’s why the naming is important, these have to be the same!) and specifically within that map, looking for a  key  called  dev-config.json . . Furthermore, this combination of ConfigMap name and key, will pick up the following entry from our ConfigMap: . When we run this Pod, the configuration values from the json structure above will get injected into our running Spring Boot service! . Now with all of the above in place, we have everything we need to create the new Pod and see if it’s all working as expected. . As a simple check, I’ve created a controller in the Spring Boot service, that when invoked, will just print out those configuration values we’ve passed to the application. . First off, let’s create the Pod using the kubectl apply method: . Now we can check the Pod is running by using some other kubectl commands (I always do this to sanity check what I’ve created): . The output  should  contain an entry for the newly created Pod: . We can also check our ConfigMap is present . Which should show us something similar to . So our Pod is running, our ConfigMap is present, we should now be able to invoke the service to see if everything has worked as expected. In this scenario, we’ll need to use  Kubernetes port-forwarding  as we don’t have a running  Kubernetes Service  in front of our Pod. In order to do this, we simply run the following command: . Which just tells Kubernetes to  port-forward  the Pod  config-demo  which is running on container port  8080  to port  8080  running on my machine. . Now we can open up a browser (or using your favourite REST client) and hit our endpoint  localhost:8080/hello  . When doing so, we should see (or get a response of)  Hello World!  . This is great, as we know the application is running, but we want to see if our configuration has been injected properly from our ConfigMap. We can now check the logs for the running pod with the following command: . All being well, we should see the following output at the end of the logs . Ta-daaa! It’s all working as expected, our application has picked up the  dev-config.json  entry from our ConfigMap and injected it into our running Spring Boot application! . So there we have a simple, but useful example of how Kubernetes ConfigMaps can be used to externalize Spring Boot configuration. That’s not the end of the story though, and you likely have some questions. . One question I definitely have is, what about sensitive information (such as usernames &amp; passwords)? . ConfigMaps are great at abstracting configuration away from your application code. Ensuring you have a flexible, maintainable, isolated pattern for storing and updating application configuration. However, by themselves they’re only part of the solution. For storing and utilising more sensitive information within Kubernetes, you have  Secrets , but that’s another post for another day… . If you’ve gotten this far, great stuff! I hope you’ve learnt something from this post and found the content and example project useful. If you want to see the project I’ve used to accompany this post, it can be found  in the example project repository . ", "date": "2021-01-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Securing Spring Boot Config with Kubernetes\n  ", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/engineering/securing-spring-boot-config-with-kubernetes/", "abstract": " In a  previous blog post  I wrote about how  Spring Boot  and  Kubernetes  are widely used together when building modern microservices. This post is a natural sequel to the aforementioned post, so it’s worth reading that first if you haven’t already done so. . Whilst I covered a good example of how Kubernetes ConfigMaps can be utilised in order to externalise application configuration, I raised some questions about security and how to store and use sensitive information in Spring Boot applications. . As discussed in the datasource example in my last post, externalizing application config with ConfigMaps has lots of benefits. On the other hand, whilst ConfigMaps provide flexibility and aid with separation of concerns, they don’t give a lot of security. ConfigMaps are great for non-secure configuration, but when security is paramount, such as for datasource credentials, you have Kubernetes Secrets. .  Kubernetes Secrets  let you store and manage sensitive data. They are great for storing credentials, tokens, keys etc. A word of caution though,  Secrets by themselves won’t necessarily solve all of your security concerns. Secrets are stored as unencrypted, base64-encoded strings - which by themselves aren’t very secure. It’s important to secure your Kubernetes cluster as well as the data within it, such as  enabling encryption at rest  and  enabling RBAC rules . There are other, more mature approaches to using Kubernetes Secrets, such as  Kamus  or  Sealed Secrets , but we’re not going into those here, as we’re focussing on using Kubernetes Secrets as a standalone resource. . For the purposes of this post, I’ll be looking at how Kubernetes Secrets can be used to store sensitive information, such as datasource credentials. . We’d previously stored the datasource configuration in a ConfigMap. This time we’re going to create a Secret in Kubernetes and pass the values to our application via  environment variables  from our Pod. . There are several ways to create Secrets; via yaml files or using  kubectl . I find it useful to create Secrets using  kubectl  initially, outputting the result of the command to a yaml file for later usage. To create the Secret we can run the following command: . The above command creates us a Secret named  datasource-credentials , from the literal values  root  for username and  password  for password. Notice the  generic  parameter as well, this is the default Secret type in Kubernetes and refers to an  Opaque Secret . If we wanted to get the output from creating a Secret without actually creating the resource in the cluster, we could add the following two parameters onto the end of the command: . These parameters tell  kubectl  to output the results of the command in yaml format and not to apply to changes to our Kubernetes environment, this way we can copy the results into a yaml file for later use. The full command would look like this: . Now we can check the Secret has been created successfully: . Which should show us an entry like so: . Now we have our Secret created within our Kubernetes environment, we can use it within our application and Pod configuration. . The changes to the Spring Boot application are minimal, and involve changing the reference values within the  application.yml  file. Previously the application configuration for the datasource looked like this: . Now we just change it so that the username and password properties can be referenced from environment variables: . The environment variables are set within the Kubernetes Pod manifest, the values for which are pulled from the Secret we created earlier. Our Pod manifest needs some additional entries to create those environment variables: . The key changes from the above are these two  env  entries: . The above approach is to  use Secrets as environment variables  within the Pod. We create two environment variables, one for  DB_USERNAME  and one for  DB_PASSWORD , each of these gets assigned the values  username  and  password  from the  datasource-credentials  secret respectively. It’s important to note the naming of each environment variable matches the values defined in the Spring Boot  application.yml  file, otherwise no value would be passed to the running application. . Now with the changes in place to use Kubernetes Secrets for our datasource credentials, we can apply the Pod changes and hopefully see the application pick them up. . First off, let’s re-create the Pod using the  kubectl  apply method: . Now we can check the Pod is running by using some other kubectl commands (I always do this to sense check what I’ve created): . The output  should  contain an entry for the newly created Pod: . So our Pod is running, we should now be able to invoke the service to see if everything has worked as expected. In this scenario, we’ll use  Kubernetes port-forwarding  again as we don’t have a running  Kubernetes Service  in front of our Pod. In order to do this, we simply run the following command: . Now we can open up a browser (or using your favourite REST client) and hit our endpoint  localhost:8080/hello  . When doing so, we should see (or get a response of)  Hello World!  . This is great, as we know the application is running, but we want to see if our configuration has been injected properly using the environment variables created from our new Secret. We can now check the logs for the running pod with the following command: . All being well, we should see the following output at the end of the logs . Great stuff! It’s all working as expected, our application has picked up the  dev-config.json  entry from our ConfigMap, which we created previously and is also picking up the  username  and  password  values from our newly created Secret. . So there we have a simple, but useful example of how Kubernetes Secrets can be used to externalize secure Spring Boot configuration. Whilst by themselves Secrets don’t provide a completely secure solution, there are recommended approaches to ensure your Kubernetes cluster itself is secured as well, which makes leveraging Secrets a more secure approach. Following secure patterns and principles within your Kubernetes cluster, such as  Zero Trust Architecture , helps ensure you maintain higher levels of trust and security, which in turn makes using resources like Secrets, inherently more secure. . Kubernetes provides several resources that we can leverage in order to externalize our application configuration securely. The benefits of externalizing application configuration are numerous, and with Kubernetes becoming ever-more popular as an orchestration tool, having tried and tested patterns and principles to utilise those resources is more important than ever. . I hope you’ve learnt something from this post and found the content and example project useful. If you want to see the project I’ve used to accompany this post, it can be found  in the example project repository . ", "date": "2021-02-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Elasticsearch: Deeper Dive\n  ", "author": ["Kamar Ali"], "link": "https://capgemini.github.io/development/elasticsearch-deeper-dive/", "abstract": " In  my previous post  we looked at getting started with Elasticsearch, covering some basic concepts and getting some hands on. . In this article I want to expand on that, taking a deeper dive and covering the following: . Importing large amounts of data . Trimming results . Paging results . Scoring . QueryDSL . Before getting started make sure you have Elasticsearch installed and running, details of which can be found in the  official Elasticsearch documentation.  . To import large amounts of data into Elasticsearch we will be using the bulk API, which allows us to index a lot of data with a single API call. . Before doing this we will need some large datasets to import. I used an  online JSON generator  which can be used to generate datasets required for the work we will be doing.  On the left hand side of the panel you have the generator configuration, I will be using the following configuration, which will generate us 3 JSON objects with the specified fields (you can paste this in). . When ready click ‘Generate’ at the top of the screen. This will display your randomly generated JSON documents on the right hand side of the screen. . Before running the bulk import, we will need to make a few tweaks to your JSON documents for the import process to function correctly. . Before downloading the data, ensure you have selected ‘Compact’ as the bulk import uses new line characters to determine the end of the file. . Once the data has been downloaded, we will need to remove the square brackets at the beginning and end of the document so we only have our individual JSON objects. . We’ll need to ensure that there is a new line character at the end of each JSON document. . Finally, ensure that you’re adding an ‘action line’ before each document. This simply tells elastic to add an ID to the document, and in particular we will be telling elastic to create an ID for us. . An example of what my JSON document ready for bulk import should look like. You’re welcome to copy this and save it as a document called data.json. . Once we have prepared the data, we’ll be using curl to hit the bulk import API.  As the data generated is based on people, we need to create an index type that makes sense, so I decided to model this is as a university course students list: a ‘computer_science’ index with type ‘students’. . In this example the JSON data file I have created is named ‘data.json’ . Once you’ve executed the command the console will output data similar to what is shown below: . We can now search on our stored students data. The following command will query for all students who have blue eyes: . And the result of the query is as follows: . Say we’ve got thousands of results and we’re not interested in the actual data just yet, we can trim the results to remove the  _source  of our hits. . With the result looking a lot cleaner: . We can continue to trim this down and limit hits using the size parameter.  An example of where we may want to do this, is if we just want to know the total number of students returned from our query. . And here’s the output, as you can see, the objects aren’t returned in the hits array. . I only have three documents stored in elastic, however we can still demonstrate paging results. . Limits can be added to the results as demonstrated previously, using the ‘size’ parameter.  So if we just want the first result we can use ‘size=1’ and this will get us the first result (from 0). We aren’t sorting yet, so elastic is returning these in an arbitrary order. . This will fetch us the first document: . If we want to get the next document we can use the ‘from’ parameter in conjunction with the ‘size’ parameter. This will tell us to grab x documents from position y. To get our second result we would use ‘from=1’. . As you can see we now have our second student: . If we try to go beyond this (we only have two students with blue eyes) we will get a blank result: . And the result: . You may have noticed a score field is returned with each document returned by queries in Elasticsearch.  This is the way in which Elasticsearch signals to us how the results rank in terms of relevance to our query, including field matches and any additional configuration we may have used.  The score itself is calculated using the  Lucene Practical Scoring Function  . QueryDSL (Domain Specific Language) is a framework we can use for more specific and efficient searches by providing our criteria in the request body as JSON. There are two types of query clauses: . Leaf query clause: Looks for a certain value in a particular field . Compound query clause: A combination of leaf queries and other compound queries . The most basic query is the  match_all  query, which will return everything. . And on running this we get the following result, as you can see all three of our students are returned: . We can also use the  match  query to repeat our previously used query for students with blue eyes: . And as you can see our two students with blue eyes are returned: . If we want to see how many females with blue eyes are in the class we will need to use  a  boolean query .  This matches documents based on boolean combinations of other queries. We will be using the  must  clause which basically means that the query clause parameter MUST appear in matching documents. . To achieve what we did in the previous query and collect all students with blue eyes using a boolean query, we would do the following: . With the same result: . Now we can either stack another match clause to get our females with blue eyes, or use the filter clause (which doesn’t affect the scoring of the document). . Two stacked match clauses in a must works exactly like the logical operator  AND . So in the below query we’re saying  eyeColour = blue AND gender = female . . With the following result: (note that the score for this query: 1.4508327 is different to the previous 0.4700036) . And now using the  filter  clause: . As you can see in the results below, the score is the equivalent of just searching for blue eyes: 0.4700036 . The next useful query type is  wildcard query .  Simply put this allows us to query using a wildcard pattern  *  which is a placeholder that matches zero or more characters. . The following query will allow us to search on the phone field for anyone who has the 911 area code, with anything coming before and after, allowing us to effectively search on this field. . As you can see from the result, the relevant student is returned. . Finally I wanted to cover  should  which works like the logical operator OR.  Similarly to the  match  (AND) clause earlier, we can stack multiple  should  clauses to create our OR queries. . For this example we will be expanding on our wildcard query and looking for students from two area codes: 911 or 929.  Note that we have replaced  must  with should and have both our wildcard queries wrapped up in this clause. . And below is the result, as you can see both our students from these area codes have been returned! . As you can see there is a lot to uncover with Elasticsearch, it’s a powerful tool with lots of use and is something every developer should be at least slightly familiar with.  This article hasn’t even scratched the surface of what we can do, however I hope it has given you an understanding of what can be accomplished and a great foundation to continue within the world of Elasticsearch. ", "date": "2021-02-26T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "User Privacy and Data Use in iOS 14\n  ", "author": ["Riccardo Freschi"], "link": "https://capgemini.github.io/engineering/user-privacy-and-data-use-in-ios-14/", "abstract": " Digital Advertising is a form of marketing which uses the Internet to deliver promotional messages to consumers. . In the Digital Advertising ecosystem there are 3 main actors: . consumers, who are the recipients of the messages . advertisers, who are the entities willing to spread a specific message about their service or product . publishers, who own the space to display an advertiser’s message. . Consumers are all of us, people who might be interested in a product, hence the target of the value proposition. . Advertisers can be anything from physical people to brick and mortar shops to gaming companies or big corporations, etc. . Publishers can be website owners interested in selling an area of a webpage to display an advertisement, or they can be social networks, search engines or any other entity with a Web presence. Publishers can also act as advertisers and vice versa: think of a website which hosts ad space and at the same time promotes its service or product on other platforms. . In between publishers and advertisers there are a number of second level entities which make the ad space fulfilment possible. They jointly form the so-called advertising technology (AdTech) stack. Demand Side Platforms (DSPs), Supply Side Platforms (SSPs), Ad Exchanges and Ad Networks form the core of the AdTech stack. . SSPs are platforms enabling publishers to manage, sell and optimize their available inventory. On the opposite side of the spectrum we find DSPs. DSPs allow advertisers working at brands and ad agencies to buy inventory on an impression-by-impression basis from SSPs. . Ad Exchanges are the actual digital marketplaces in between DSPs and SSPs where the purchase of a given ad space happens, typically via real-time bidding (RTB) auctions. . Ad Networks are brokers of inventory and also generally placed between DSPs and SSPs. .   . The criteria governing the buying and selling of advertising space are not limited to price comparison: advertisers are interested in optimising their investment, which means spending the least for the maximum chance of converting a prospect into a customer. . Such conversion is much more likely if the profile of the buyer persona and the profile of the candidate consumer match. The buyer persona describes the customer archetype. The candidate consumer is also called the “target”. And “targeted advertising” is the first controversial actor of our story. . Targeted advertising is the technique of directing the messaging towards an audience with certain traits, based on the product the advertiser is promoting. These traits can be anything from demographic, to income, to personality, to lifestyle, etc., gathered via many different means, altogether going by the name of “tracking”. . By itself, targeted advertising would not be so bad: doesn’t everyone prefer to be bothered with propositions of products more relevant to their interests rather than not? . The problem is tracking. . Technically, tracking refers to the act of collecting user or device data from a website or a mobile application (commonly referred to as an “app”) and linking it with other data collected from other companies’ apps, websites, or offline properties. . Tracking also refers to sharing the collected data with Data Brokers, which are companies whose primary business is collecting personal information about consumers from a variety of sources and aggregating, analysing, and sharing that information, or information derived from it. Data Brokers are the source of consumers’ profile information, which forms the foundation of advertising digital auctions: whenever a consumer visits a webpage hosting an ad, before presenting that same ad, a number of events take place in the AdTech stack, leading to a series of bids to purchase the ad space; such bids are based on the profile of the consumer provided by Data Brokers. It is fair to say though, that not all ad space is sold within the full AdTech stack and hence involving profile information sourced from Data Brokers: there are environments where the stack is squashed into a single platform, think of e.g. the usual suspects Google and Facebook, where advertisers can acquire space directly from them, who in such a case play the role of publishers, SSP, Ad Network… basically the full stack. It is interesting to note that even incumbents like Facebook rely on Data Brokers for profiling: in  a report from Forbes  . Facebook argues that it must buy this data because that is simply how advertising is done today and that companies want to use the same marketing selectors across every platform. . Coming back to tracking,  according to Apple , examples of tracking include: . Displaying targeted advertisements in your app based on user data collected from apps and websites owned by other companies. . Sharing device location data or email lists with a data broker. . Sharing a list of emails, advertising IDs, or other IDs with a third-party advertising network that uses that information to retarget those users in other developers’ apps or to find similar users. . a third-party SDK in your app that combines user data from your app with user data from other developers’ apps to target advertising or measure advertising efficiency, even if you don’t use the SDK for these purposes. For example, using a login SDK that repurposes the data it collects from your app to enable targeted advertising in other developers’ apps. The following situations are not considered tracking: . When the data is linked solely on the end-user’s device and is not sent off the device in a way that can identify the end-user or device. . When the data broker uses the data shared with them solely for fraud detection or prevention or security purposes, and solely on your behalf. . SDKs (Software Development Kits) are third party software components embedded in apps, which implement a large variety of pieces of functionality. Because they’re useful and generally easy to use, SDKs are embedded in lots of the published apps. A comprehensive list and description of the most used SDKs is maintained by  MightySignal . . A number of studies from accredited government institutions and news media has brought to the public attention that: .  University of Oxford : . A very large number of apps embed third party SDKs, which form networks that link activity across multiple apps to a single user, and also link to their activities on other devices or mediums like the web. This enables construction of detailed profiles about individuals, which could include inferences about shopping habits, socio-economic class or likely political opinions. .  Journal of Economic Literature : . consumers’ ability to make informed decisions about their privacy is severely hindered because consumers are often in a position of imperfect or asymmetric information regarding when their data is collected, for what purposes, and with what consequences. .  New York Times , reporting on location tracking: . [the data reviewed] originated from a location data company, one of dozens quietly collecting precise movements using software slipped onto mobile phone apps. […] The companies that collect all this information on your movements justify their business on the basis of three claims: People consent to be tracked, the data is anonymous and the data is secure. None of those claims hold up, based on the file we’ve obtained and our review of company practices. Yes, the location data contains billions of data points with no identifiable information like names or email addresses. But it’s child’s play to connect real names to the dots that appear on the maps. […] Describing location data as anonymous is “a completely false claim” that has been debunked in multiple studies, Paul Ohm, a law professor and privacy researcher at the Georgetown University Law Center, told us. “Really precise, longitudinal geolocation information is absolutely impossible to anonymize.” […] If you have an S.D.K. that’s frequently collecting location data, it is more than likely being resold across the industry,” said Nick Hall, chief executive of the data marketplace company VenPath. […] If a private company is legally collecting location data, they’re free to spread it or share it however they want,” said Calli Schroeder, a lawyer for the privacy and data protection company VeraSafe. .  University of Oxford : . […] most apps [959,000 apps from the US and UK Google Play stores] contain third party tracking, and the distribution of trackers is long-tailed with several highly dominant trackers accounting for a large portion of the coverage. […] the median number of tracker hosts included in the bytecode of an app was 10. 90.4% of apps included at least one, and 17.9% more than twenty. .  The Consumer Council of Norway , following an investigation: . 20 months after the GDPR has come into effect, consumers are still pervasively tracked and profiled online and have no way of knowing which entities process their data and how to stop them. . The adtech industry is operating with out of control data sharing and processing, despite that should limit most, if not all, of the practices identified throughout this report. . The digital marketing and adtech industry has to make comprehensive changes in order to comply with European regulation, and to ensure that they respect consumers’ fundamental rights and freedoms. .  Forbes : . In the world of Data Brokers, you have no idea who all has bought, acquired or harvested information about you, what they do with it, who they provide it to, whether it is right or wrong or how much money is being made on your digital identity. Nor do you have the right to demand that they delete their profile on you. . Consequently, government regulators have been taking action by sanctioning those found in breach and by setting rules on personal information handling. . Some of the most notable penalty examples are: .  Facebook : . Facebook, Inc. will pay a record-breaking $5 billion penalty [… for] deceiving users about their ability to control the privacy of their personal information. .  Twitter : . the FTC [alleged Twitter] used phone numbers and email addresses that were given to the company for safety and security purposes for targeted advertising between 2013 and 2019. .  Google : . On 21 January 2019, the CNIL’s restricted committee imposed a financial penalty of 50 Million euros against the company Google LLC, in accordance with the General Data Protection Regulation (GDPR), for lack of transparency, inadequate information and lack of valid consent regarding the ads personalization. . As far as the introduction of data protection regulations is concerned, two of the most prominent examples are the California Consumer Privacy Act (CCPA) and the European General Data Protection Regulation (GDPR). . CCPA is based on four  founding principles , which state that consumers have: . The right to know about the personal information a business collects about them and how it is used and shared; . The right to delete personal information collected from them (with some exceptions); . The right to opt-out of the sale of their personal information; and . The right to non-discrimination for exercising their CCPA rights. . Similarly, GDPR sets out  seven key principles , which lie at the heart of its general data protection regime: . Lawfulness, fairness and transparency . Purpose limitation . Data minimisation . Accuracy . Storage limitation . Integrity and confidentiality (security) . Accountability . The underlying theme can then be summarised as: . Reduce consumer data collection to the strictly necessary . Be transparent on why you need it and on what you do with it . Be careful of how you handle it, you are accountable for that . Those are basically the same principles adopted by Apple in its approach to privacy. . Over the years, Apple has distinguished itself for paying special attention to making their products more privacy friendly and helping keeping users’ identity and data more secure. . On Safari, the main feature introduced to hinder tracking is Intelligent Tracking Prevention (ITP), an initiative started by Apple in 2017. . In the browser context, tracking is achieved historically by third party components (similar to the SDKs described above) dropping small bites of information called cookies in the browser of the consumer. Frequently those cookies carry just one piece of information: a constant identifier assigned to the consumer (or better her/his browser); the rest of the profile is stored in a server which the component communicates directly or indirectly with, via Data Brokers, to incrementally add information, while the consumer visits other sites also embedding the same component. . Back to ITP: Safari started first with blocking third-party cookies, then later on, tightened the noose around client side first-party cookies too, by putting a 7-day expiration date on them. This change was added to iOS 12.2 and Safari 12.1 on macOS High Sierra and Mojave. . First-party cookies are cookies added by the website the consumer is visiting. Placing tracking information in first-party cookies is a workaround recently developed (for good or bad) by some companies to overcome the limitations caused by the blockage of third-party cookies ( e.g. Facebook ). . More recently, with version 14, released in September 2020, Apple incremented ITP with a “Privacy Report”, listing all trackers Safari detected during the consumer’s site visits. . On the mobile side, Apple introduced the first advertising-related features in 2012, with iOS 6. . First it removed the Unique Device Identifier (UDID), a constant identifier associated to the device, previously always available to apps and playing a role similar to the tracking cookie in the browser context. . Second, it put in place another device identifier, named Identifier for Advertisers (IDFA), which is a string commonly represented by numbers and letters (technically a 128-bit value, called a UUID). Differently from UDID, the IDFA can be made unavailable to apps if the third new feature is switched on by the user: a feature called Limit Ad Tracking (LAT). . When LAT is enabled, the user’s IDFA is zeroed out (i.e., the value is replaced with zeros) when accessed by apps, hence hiding the device identity. . In reality, prior to iOS 10, the IDFA was still passed, even if a user had enabled LAT, but was accompanied with a request not to use the IDFA. Many companies decided not to honour this request, so Apple decided to zero out the IDFA from iOS 10 onwards. . More recently Apple enabled the users to opt-out of Location-Based Apple Ads: with opt-out disabled, if a user granted the App Store or Apple News access to her/his device location, Apple’s advertising platform would use the current location of the device to provide with geographically targeted ads on the App Store and on Apple News apps. .   . iOS 13 brought with it an update to location data controls. .   . Firstly, users were periodically shown messages informing them of certain apps that were using their location data in the background (i.e., when not actually using the app in question). .   . Secondly, Apple changed the options available when users were presented with the popup to choose whether an app could use their location data: the original options were updated from “Always, Never, and While using” to “Allow While Using App, Allow Once, and Don’t Allow”. .   . Other privacy-related additions to iOS 13 were the permission to use Bluetooth and the permission to read Contacts’ notes: before, apps could access those functionalities freely, while with the new OS, the user was prompted to approve. . That leads us to the present day and to the controverted protagonist of the present perspective: iOS 14 (14.5 in its latest incarnation, Beta released on February the 4th, 2021). . With iOS 14, Apple delivers a number of new  privacy-related features : .   . The most debated ones are the two at the top of the list above: “Privacy information on the App Store” and “App tracking controls and transparency”. . “Privacy information on the App Store” states that in order to submit new apps and app updates, application publishers must provide information about their privacy practices in App Store Connect. If their apps use third-party code, such as advertising or analytics SDK’s, they also need to describe what data the third-party code collects, how the data may be used, and whether the data is used to track users, unless the captured data meets all of the criteria for optional disclosure listed below: . The data is not used for tracking, which means that the data is not linked with Third-Party Data for advertising or advertising measurement or shared with a data broker. . The data is not used for Third-Party Advertising, for the app publisher’s Advertising or Marketing purposes, or for Other Purposes . Collection of the data occurs only in infrequent cases that are not part of the app’s primary functionality, and which are optional for the user. . The data is provided by the user via the app’s interface, it is clear to the user what data is collected, the user’s name or account name is prominently displayed in the submission form alongside the other data elements being submitted, and the user affirmatively chooses to provide the data for collection each time. . Some data related to apps in the Regulated Financial Services and Health Research can optionally disclose the collected data, provided some extra criteria are met. . In its guidance, Apple also provides definitions for the different types of data, such as “Email Address” and definitions for data use purposes, such as “Third-Party Advertising”, to help app publishers understand what kind of data falls within which policy. . For every type of captured data, Apple requires app publishers to identify if it is linked to the user’s identity (via their account, device, or other details) either by the app publishers themselves or by their partners. Data collected from an app is considered linked to the user’s identity, unless privacy protections are put in place before collection, to anonymize it, such as stripping data of any direct identifiers (e.g., user ID or name) before collection. Additionally, after collection, data must not be linked back to the user’s identity, either directly or tied to other datasets that enable it to be linked indirectly. . On the second privacy update, “App tracking controls and transparency”: app publishers need to receive the user’s permission through the AppTrackingTransparency framework to . access the device’s IDFA or . (in general) track them. .  This is the big change : if previously, like seen above, the IDFA was zeroed in case of LAT enabled only, now with iOS 14 (14.5 exactly) it is always so, unless the app receives first the user approval by requesting it via the AppTrackingTransparency framework. Such a request results in the user being presented a popup and prompted to grant the app access to the IDFA. (The popup can be customised with a purpose string to add more information about why the app needs to access the identifier.) .  The change will specifically affect the ad targeting side of the ecosystem, in all its declinations: segmentation, retargeting, lookalike audiences, exclusion targeting, etc.  . Today in fact, a large number of advertising platforms relies on the IDFA, e.g. the  Google Mobile Ads SDK : . The Mobile Ads SDK for iOS utilizes Apple’s advertising identifier . The change goes actually even further than fencing access to the IDFA: from Apple’s FAQ section we understand that the following practices might result in App Store rejection: . gating functionalities or incentivising the user to grant tracking permission . using another identifier (e.g., a hashed email address or hashed phone number), unless permission is granted through the AppTrackingTransparency framework . fingerprinting or using signals from the device to try identifying the device or a user . tracking performed by an integrated third-party SDK, even in case of single sign-on (SSO) SDK . It is evident then, that any attempt of unexplicitly granted tracking would not be tolerated by Apple and that the app publisher is deemed fully responsible for the code running in her/his app, even for the code running in embedded SDK’s, produced by third parties. . Content providers owning multiple apps and willing to apply analytics across them, have the option to use another ID, the ID for Vendors (IDFV), without obligation to request user’s permission via the AppTrackingTransparency framework. Again though, only in case the IDFV is not combined with other data to track a user across apps and websites owned by other companies. In that case, permission needs still to be granted via the AppTrackingTransparency. . A piece of functionality which is also affected by the privacy changes is “attribution”: whenever an app is installed as a consequence of the user tapping on an advertisement on another app, a common practice today is to leverage the IDFA to detect which ad on which device resulted in the conversion and hence measure the effectiveness of the advertising campaign. Apple guidelines recommend adopting to the SKAdNetwork framework instead, which the AppTrackingTransparency grant is not required for. . In another note Apple announces the  upcoming support for Private Click Measurement , that facilitates advertising networks measuring the effectiveness of advertisement clicks within iOS or iPadOS apps that navigate to a website. This might be a welcomed change by the advertising business, considering that the IDFA is not available on the browser, hence tracing the conversion from an ad tapped on mobile and directing to a web page was hard before but possible now. . We have gone through an overview of the main different entities operating in Digital Advertising and how targeted advertising in particular heavily leverage tracking to build up consumer profiles, used to compute likelihood of ad conversions. . We have seen that said profiles are frequently built up by capturing and combining data in opaque ways, which raised concerns amongst public opinion and governments, leading to the introduction of regulations such as GDPR. . In the last part, I presented some of Apple’s efforts to support user privacy in both Safari and iOS, opposing tracking in a similar way as other personal data laws around the world attempt doing. I paid special attention to the latest privacy changes introduced with iOS 14, describing the impact on IDFA usage and App Store publishing process for mobile apps. . I believe that complications for those willing to keep pursuing tracking behind the scenes will be considerable, though possible technical workarounds might be found. . As for those willing to operate in the clear, my recommendation is to apply the updates described in the previous sections. . In summary:  treat users with consent, if tracking is needed ask for permission, disclose transparently what will happen to the consumers’ data and why consumers should agree on making it available, give users easy ways to update and change their preferences, keep control of where the acquired data flows, go even further and adopt anonymization where possible and delete data when not needed anymore . ", "date": "2021-03-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Using AWS S3 and Hugo to Create and Host a Static Website\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Using-S3-and-Hugo-to-Create-Hosting-Static-Website/", "abstract": " As a software engineer and consultant, I have been responsible for designing, developing and deploying custom websites and systems for a range of private and public sector companies and agencies. With most of my time being spent on other people’s sites and systems, I’ve never actually created any of my own. Today that changes. I have finally decided to use all of the knowledge I’ve gathered over the years to create my own website with the additional decision of letting you guys in on it. In a series of future blog posts, I will create and deploy a simple website to AWS using a variety of tools. With each blog post I will either improve or add functionality to the site and its CI/CD processes whilst letting you guys in on all the action. . To begin creating the site, I will need to choose a static site generator. After a few Googles, I came across  Hugo . As a fan of Golang (which Hugo is written in) and Hugo’s speed with site generation, I concluded that it would be a great choice to create my site with. Next I need to choose a hosting service. There are many ways to host a static site online. Some popular choices are  GitHub pages ,  Netlify ,  Firebase  and S3, however, I need one that won’t take any additional registration or learning and one that is minimal hassle. This means that Netlify and Firebase are out of the equation as I have no experience in either and I don’t have an account with Netlify. GitHub pages is an extremely popular choice due to its simplicity and cost (it’s free). In fact it’s what this very same Capgemini Engineering Blog uses, however, with Hugo, it’s a bit more work to get up and running, as detailed here on the  Hugo site . So now the options have been filtered down, I’m left with S3. S3 is incredibly easy to use and setting up a static hosting site only takes a few steps. I also, as a Certified AWS Developer, have experience with S3 and the AWS platform, so this was the most obvious option. . So now I have chosen my tools, with Hugo being the site generator and S3 being the hosting service, I can start creating. . With Hugo installed, the first thing I want to do is choose a  Hugo theme.  As I’m not particularly interested in spending hours and hours creating an accessible and stylistically sugary site, I can use a theme that has already had said time spent on it. Additionally, in my case, where I just want to get a basic site up and running and available to the world, I want to avoid copious amounts of time being spent on padding and margin issues. After a quick browse on the Hugo themes site, I’ve found that  hugo-coder  has caught my eye due to its simplicity, minimalism and clean look. Let’s use that and get to work. . To create the site on my laptop, I can immediately take advantage of Hugo’s functionality by using it to create the site. By running  hugo new site chrisjburns.com  Hugo automatically generates a skeleton site for me to start from. Next I installed the  hugo-coder  theme by navigating into the site folder and running  git submodule add https://github.com/luizdepra/hugo-coder.git themes/hugo-coder . After the theme has been installed, I can use an example  config.toml  already provided and when I run  hugo server -D  (-D, –buildDrafts) the site is served to me instantly with the following outputted to the terminal: . When I go to the URL:  http://localhost:1313/  I see the following… .   . Look at that, I’ve instantly got a site created. Thank the Universe for Hugo themes as this would have taken me hours to do from scratch. . However, in all its glory, there are some minor problems. One being that my name is not John Doe and I’m also not a Magician – so let’s change some of this. I will additionally remove some icons, the menu at the top and the title at the top left as currently I just want to setup a basic front page as a base to get my site out to the world. . After some chopping and changing of the  config.toml  file, I have now got the following: .   . Nothing fancy, just something basic. For reference, in the  config.toml  file I have only really commented out the menu items at the bottom: . As well as only including my specific social URLs for the icons: . I have additionally commented out the title at the top as I don’t really want the title in my page, at least not yet. . Lastly, as shown above, I have commented out the  theme = coder  line which basically points Hugo to the  hugo-coder  theme that I added as a git module. The reason for this is because I don’t really want to have to strictly adhere to the way the styling and colours look in the theme, and I want to personalise it in some sense. An example of this would be the fonts that I have already changed at the bottom. All I’ve done is removed the dependency of relying on a theme to be existent, and by doing this, I copied over all of the theme settings, stylings and layouts into the  assets  and  layouts  folders. Now these settings, styles and layouts are completely free for me to modify to my own taste. For a beginner user, this may take a bit more time to do as it requires some knowledge about Hugo and front-end web development. . Now I have a basic version of the site, I want to generate it. Generating the site is as simple as one command;  hugo . When run, Hugo will render the site and its contents into a directory in root called  public/ . This directory will be what is deployed/uploaded onto S3. . When creating a static website for S3, it already provides functionality to do this. . First, I created a public bucket that I aim to use for my site. It is always recommended to use the domain that you wish to use for your site. In my case it shall be  chrisjburns.com . .   . Now I have the bucket, I can upload my site. I do this uploading the rendered  public/  directory. Next, I enable static hosting by going to to the  Properties  tab in the bucket settings and choosing  Static website hosting  whilst entering the following settings: .   . Make sure to make a note of the endpoint URL as I will refer back to it in a few moments. When I click  Save  the following is shown to state that the static hosting is enabled. .   . Now I have enabled the static hosting, I need to give the public read access in order for them to see the website. . To do this I created a bucket policy that gives anyone read access to the content in the  chrisjburns.com/  bucket. The policy looks like the following: .   . Now when I go to the Endpoint URL that I mentioned a few moments ago we should see the site in all its basic glory. .   . This is the most basic way of setting up a static site in S3. Obviously, sites with programmed functionality behind them won’t really fit this model that well due to the restrictions on only static content being allowed - although  JAMStack  is pushing the boundaries on this very restriction. However, for a site that has just static content then it can work wonders and is amazingly cheap. If at any point you wanted to modify something on the site, just make the changes that you want and the regenerate the site by running  hugo  and re-upload the  public/  directory into the bucket. . With all its pros, there are some cons. First among these that I’m not using a domain name. Instead I’m using an S3 endpoint URL, which works fine as we’ve seen, but nobody wants to enter this to get to your site. Additionally, not only is this a very basic site, but it is also an insecure one. Currently the site is served over http and not https which means that all information being sent over the internet is unencrypted. Additionally, this bucket exists in the eu-west-2 region (London), this means that if someone in Japan (shout out to my Japanese readers) wants to view this site then they will have to load all of the content from London. This, as you can imagine, is terribly inefficient as depending on the content of the site and the size of the images, things may take a few seconds longer. To get over this, a content delivery network (CDN) would be needed. The benefits to using a CDN is that there will be edge locations that are spread around the world that all cache the content of the site so if someone in Japan makes a request, they are just making a request to their nearest edge location, instead of making a request to London. This speeds up the loading of the site and in fact reduces the costs involved as the requests do not need to go to S3 everytime as the CDN will have all of the content already cached in its edge locations - therefore reducing the load on S3. . However, this is beyond the scope of this post. Stay tuned, as I will be covering the upgrading of this very basic site to use a CDN in the next blog post. I have also got some more exciting content coming up in future posts around registering a domain and introducing CI/CD into the mix for our site to get rid of the manual work. So, stay tuned and in the meantime, find me on the socials. ", "date": "2020-03-02T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Promoting on Success: A Safe and Reliable Strategy of Promoting Serverless Applications\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Promote-On-Success/", "abstract": " Traditionally, there have been many ways of releasing applications. This relates to both the different test environments and the production environment itself. However, with serverless applications, traditional strategies may not transfer so well due to the characteristics of those architectures. For applications that run and live on containers that themselves run inside clusters, there can be many different releasing strategies that can apply. For this post we will solely focus on a serverless releasing strategy that follows a ‘promoting on success’ model. . It is worth noting that this is more focused on the AWS Cloud platform, however, feel free to adapt certain aspects to different providers where it applies. Additionally, this article will not make any assumptions on CI/CD tool choices. We will only be staying at a medium/high level with regard to the AWS components and the pipeline flow. . Picture the scene, we’ve all been there; a demonstration you’re leading is due to start in a few minutes and some engineers on the team in efforts to finish their ticket, merge some last minute changes into master before it kicks off and for some reason luck isn’t on your side today as consequently the pipeline notifies you that the newly deployed version of the application has just failed its post-deploy tests and is unstable. If you’re lucky, the pipeline will tell you – giving you a few minutes to quickly run out the door and leave the demonstration to someone else, however, some aren’t so lucky to have the luxury of being told by the pipeline and will in fact go on with the demonstration to the inevitable ‘NullPointerException’ (or any runtime exception equivalent) error when running a simple scenario. Now you have to face the music and tell all of the stakeholders at the review that it won’t happen again and you promise to show them next time – when in reality, it probably will happen again as it already happened once, especially due to it being a human error. . There has been many ways in which engineers have attempted to maintain stable builds in their environments, some of the most popular ones nowadays include  Blue Green deployments  and  Canary releases . Although both are normally more used to support production releases, they can also be used in non-production environments. However, the entire idea behind them is to route certain portions of traffic to the new version of the application and to roll back if a specific threshold of issues arises. This is what makes these strategies effective and secure. They are great at reducing and handling the risk for when / if anything goes wrong – this is great for container-based application deployments as well as serverless deployments. That being said, although both strategies work well, depending on the stack and environment setup, they can be quite complex and tiresome to setup. For serverless architecture, you could argue that the effort of a Canary deployment strategy isn’t really warranted in environments that aren’t related to production. . This is where the strategy ‘promote on success’ comes in or as others know it, Blue Green. Instead of the traditional deploying, verifying and then rolling back, we instead deploy, verify and promote. With this flow we ensure that the running environment itself is never down due to a rollback. It also gives you more of an idea of what versions of the code made it to the environment. . The Blue Green &amp; Canary strategies typically involve the same steps; deploying the application, verifying it works as expected and then switching the traffic over to the new version. Some would argue the only real difference is that with Canary deployments you redirect a portion of the real user traffic to the new version whilst also pointing portions of real user traffic to the old version whereas with Blue Green it is more of a big bang effect where you switch all traffic to the new version at the same time once verification is completed. . For serverless deployments, it is up to engineers themselves with regard to which one they want to use for their production deployments, however I think it is fairly safe to assume you wouldn’t really want to consider using the Canary releasing strategy to your test environments mainly due to the amount of overhead you would create with the additional point of not needing to use real user traffic as it’s not production. However, the Blue Green strategy does seem more sensible with regard to our non-production environments. . If you think of when a person is promoted, they are promoted because they have proven that they can do the job – in the most cases. The promoting on success strategy is no different. We only promote the application version if it has proven that it has passed all of the assurances beforehand. The Blue Green deployment strategy offers the same assurances. One of the main benefits is the separation between application versions. The reason I have called it ‘promoting on success’ is to really give it that “exactly what it says on the tin” feel as Blue / Green doesn’t really give you much information from its name. Upon deployment of a Lambda, AWS creates a new version (completely immutable snapshot of the code) of the code each time that you publish the function. To read more on Lambda versions, the  AWS documentation  has some great guides. By default, IaC frameworks like  Serverless  will automatically publish a new version of the Lambda every time you deploy your Lambda to AWS. The benefit with using versions results in there being a track history of amendments of the Lambda that allows you to retrospectively fallback to if things where to go wrong. You would do this by using versions and aliases.  Aliases  are another piece of functionality within AWS Lambda that is extremely useful when dealing with Lambda versions. On top of that, for the ‘promote on success’ strategy, aliases are one of the key components. They are essentially pointers to specific versions of a Lambda. As mentioned above, in an example where there is an alias called ‘prod’ and it points to a newly published version of 45 of a Lambda, and for some reason people are reporting new bugs, you can quickly ‘roll back’ by pointing the ‘prod’ alias to version 44 until all bugs have been fixed - which would then be within version 46. Although that sounds controlled and smooth, this is exactly what the ‘promote on success’ strategy is there to avoid. At no point should you need to roll back to a previous version. If you’re tests are thorough enough, the version that gets promoted into the environment, should not break anything. . To implement the ‘promote on success’ you follow the following steps, the order in which you do them will matter to a degree due to dependencies, however staying at a high level, this is what it includes: . Deploy the new version of the Lambda . Deploy API Gateway that always points to the $LATEST version of the Lambda e.g. API Gateway URL: http://www.api.example.com/application-name/latest . Run test suites against the URL of the $LATEST API Gateway – this should run against the newly deployed version of the Lambda . If the tests pass, make use of the Lambda aliases to promote the $LATEST version of the Lambda (this can be the environment type) e.g. preprod . Deploy another API Gateway that always points to the environment alias of the Lambda e.g http://www.api.example.com/application-name/preprod . Following the above steps, you will achieve a ‘promote on success’ strategy for deployment. What we are ensuring by following this is: . There is always an endpoint that directly exposes the latest version of the application Lambda at any given time. . There is always an endpoint that directly exposes the latest stable version of a specific application Lambda as the API Gateway endpoint is pointing to the alias itself. For example, the preprod endpoint will always be pointing to the most stable preprod version of the application Lambda. . By promoting the Lambda versions on test success, we ensure that whatever gets promoted through the pipeline lifecycle has passed the quality assurances we have set in place – whatever they may be. This is an important point as the entire strategy relies on the fact that there has been an investment into the thoroughness and effectiveness of the testing. After all, this is what defines your “stable” environment. . As described above, you now have a pipeline that deploys the application Lambda to specified environments only on test success, ensuring a more stable environment that will not fall over during demos. This removes the traditional rollback on failure approach where your adding additional steps that run if things go wrong, which of course, can go wrong themselves. . It’s worth mentioning that when you create and deploy both of the API Gateways, there will be no need to redeploy them – unless of course they change. The new version of the Lambda will be the only component that gets updated with each pipeline run. How you integrate the above steps into your pipeline is up to you, you can do this all by  CloudFormation  or by bash scripts – whichever suits your approved technology stack. But I have found that this works pretty well in my experience as you can even tie in the promoted versions to release note automation. Additionally, with this simple approach, you get a choice of what type of testing goes on in the verification stage, therefore increasing the stability and security of the successful promotion. . Not everything is perfect, it’s worth mentioning that there is a possible drawback to this approach and that is race conditions. If you have an application that is constantly being contributed to due to hundreds of commits every hour – depending on the complexity of the solution – you will suffer race conditions. More specifically, if 2 or more pipelines are running at the same time, then the latest version of the Lambda may not be the version of the lambda that the specific pipeline is concerned with due to another concurrent pipeline having just deployed a new version. I advise you to be wary in these situations. However, although there are CI tools now that allow you to block parallel builds to stop these racing conditions from occurring, I would still say it is wise to be wary. . As always, any questions find me on the socials. ", "date": "2020-02-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Quarkus meets Liquibase\n  ", "author": ["Andrej Petras"], "link": "https://capgemini.github.io/development/Quarkus-meets-Liquibase/", "abstract": " Migrating an application to a new framework is mostly painful and challenging. We started to migrate our existing Jakarta EE applications which have been using Liquibase to manage the database changes.  Quarkus  has native support for  Flyway  which is another tool to manage the database changes. One option was to migrate the  Liquibase  change log file to Flyway SQL. Liquibase is also widely used in open source community, and has some advantages that you may find useful in daily DevOps processes. We chose to stay with Liquibase and minimize the changes in the projects. Native support for Liquibase would be highly favorable to avoid  another Java JDK dependency in the Docker environment. So we started to create Liquibase extension for Quarkus. . Quarkus Kubernetes native Java stack comes with an extendable framework. There is a very nice guide on Quarkus  page how to  write your own extension . We are already using our own extensions for logs,  Java Persistence API (JPA) and testing specific to our projects and needs. But much more exciting is to write an extension that would become a part of the Quarkus and would contribute to the open source community. I believe we are not the only ones who have been using the Liquibase framework to manage database changes in recent  years in projects. With the help of  Quarkus  and  Liquibase  community, we designed and implemented the Liquibase extension which is now  part of Quarkus framework. In this article we are going to create an example Quarkus Application which will show  how to use this Liquibase extension. . In our example we will use Maven to manage the project. First, we will create simple REST + JPA application. For this we can use the Quarkus maven plugin: . The project structure is created and we can add our JPA model for Event. . The REST controller for the Event resource will have methods for  create ,  findByGuid  and  findAll . . In the next step we need to set up the database connection in our  application.properties . . For the local development we will use the PostgreSQL Docker image to run the database server. . The newly created database is still empty. Now we can add the Liquibase extension to our project. Just add this Maven dependency: . For our database changes we need to put the  changeLog.xml  file in the  src/main/resources/db/  directory.  In the changeLog we can use the includes and the other formats yaml, json and sql are also supported.  You can create the file by hand or also use a generator to evaluate the recent changes of the entity model. . To update our database schema automatically at the start of the application add this property  quarkus.liquibase.migrate-at-start   in the  application.properties  and set its value to  true . This will start the migration during the starting of the application. . Now we can start the application in development mode .   . From the logs we can see that Liquibase runs the database update. In the database we will have new table  and we can start to create objects. Let’s create event in the database. . Output: . We can load the data through this command . Output: . We have data in the table, now we are going to add new fields. First we need to update the Java class and add field and getter and setter. . Next step would be to add the changeSet in our  changeLog.xml . . One of Quarkus benefits is that we do not need to restart the application just to make a call to get our object through the rest interface and Quarkus will do the job for us and run the latest version of our application with database update. . Output: .   . From the output, we can see that the model has a new field and also the database table has been updated. Another useful configuration  are drop all tables before migration, disable Liquibase validation and changeLog file path. . Also, you can use Liquibase directly in the application. We need to inject LiquibaseFactory and create the Liquibase object. . For example we can create the REST controller to return all run changes. . To get all executed changes from Liquibase we need to call this URL: . Then we will get output like this: . Now Quarkus provides first class support for using Liquibase as will be explained in the Liquibase guide.  All this configuration can be found in the  Quarkus Liquibase guide  . We have also use cases where we need to run Liquibase separately from the microservice. One option is to install Liquibase  directly but in the case we have native application we want to avoid a JVM installation. For this case, we created  the Liquibase native build (currently only for PostgreSQL database and Linux) which can be downloaded from   1000kit-liquibase  . This Liquibase binary is also provided as a base Docker image which you can use in your docker image.  1000kit/liquibase  Docker image as build image. The source code of the example is in the github repository  1000kit/quarkus-liquibase-example  where we need to first build the native application. . Then we can build the Docker image with native Liquibase inside and copy the Liquibase changelog files  also in the Docker image. . The  CMD  command of the Docker image will run Liquibase before the application and the Liquibase configuration is using the standard Quarkus environment variables for the database connection. For testing, use the  docker-compose.yml  which is in the git repository. First start the database and the application. . The database connection for the application and for Liquibase is configured in the  docker-compose.yml  file. . The environment variable  QUARKUS_LIQUIBASE_MIGRATE_AT_START  is setup to  false  and it will disable running Liquibase second time by the Quarkus Liquibase extension. The output of the application will look like this: . I hope that this extension will be useful for other teams - if you’re building Quarkus applications that work with Liquibase,  please try it out, and if you use it on your project, I’d love to hear about it. ", "date": "2020-03-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Ten Steps towards Cloud Native\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/cloud/cloud-native-steps/", "abstract": " Here in the Open Source Cloud Engineering (OSCE) division of Capgemini Custom Software Development, cloud native development is what we do. What we struggle with is explaining to people exactly what that is. Cloud native is a buzzword which is thrown around a lot, but what does it mean for a team of developers and how does it help their clients? . As a company hosting bespoke or “ COTS ” software, simply moving that software to servers hosted by a cloud provider is not necessarily going to be the most cost-effective solution. Cloud-hosted servers are usually more expensive, because the cost accounts for the engineering teams who install and manage the hardware and the software teams who manage the apps allowing you access. Cloud vendors will offer different cost incentives to their clients to use their hardware and software offerings in a manner which is beneficial to the vendors. For example, if you want an entire dedicated Linux server with 2 sockets and 48 physical cores, but you’re not using it all day / every day then it’s a wasted resource for the cloud vendor. If you agree to let the vendor use the server when you’re not using it and add extra peripherals to it, and the vendor can then charge another client for that processing power, it’s more efficient. There’s a lot of creative billing in this space - for instance  AWS Spot Instances , where you effectively bid for processing power that holds different values at different times based on demand. . To us, cloud native applications are those which are written for maximum efficiency when deployed with a specific cloud provider. In this sense, by “efficiency” I mean cost efficiency, what is the lowest cost for running your application logic in a stable and scaleable way - but under the covers that probably implies maximum processing efficiency too.  We also consider efficiency in the development pipeline - do you have test environments sitting there racking up costs when you aren’t using them? What about the servers where your pipeline code runs, do you tear that down when not in use? All these considerations tie into cloud native development. . By this definition, the ultimate cloud native application is a function-as-a-service, or “serverless”, and indeed if you are starting a new project or adding peripheral functionality to an existing architecture, you should consider function-as-a-service first as potentially the cheapest and least time-to-market for your functionality. There are reasons why you  wouldn’t  choose function-as-a-service, which I will go into shortly, the most commonly stated of which is that you don’t want your business logic to only run on one cloud provider. You may wish to run it on-premise too, you may not want to re-architect to a specific provider’s serverless solution framework/languages, you may be wary of vendor lock-in. And of course a serverless function is just that - a single function that must be orchestrated and its inputs and outputs must be managed, so it’s not a full application in itself. . There is also an argument that containers are kind of serverless depending on how they’re deployed - OK technically the thing in your container may have a lot of server-ish features, but you don’t really care about how they run when you hand them over to a platform such as Google’s  GKE , Azure  AKS , or Amazon’s  EKS  So where does that leave our definition? How about: .  Cloud native applications are written for maximum efficiency when deployed to the cloud, whilst remaining flexible and portable  . It’s starting to get a bit long isn’t it! . OK here’s the sales pitch - but we’re very proud of it. OSCE have a couple of offerings in the cloud native space. One is our Cloud Native Maturity Assessment, which helps companies to judge where they are on the route to maximising cloud efficiency, and the other is our default, open-source cloud native development platform. . Many aspects of cloud development are overlooked when people think about moving their estates to the cloud. Here are our Ten Steps to checking whether your business is getting the most out of moving to the cloud. . Question One - how are you actually going to get your application out into your shiny new cloud space? You need to go back to basics and look at your delivery pipeline before you’re ready for the cloud. Once you relinquish control of your deployment environment, you have to make sure it is reproducible automatically at every step. A build server is a good place to start in automating your deployment process. You need to be repeatably creating and testing an immutable artifact that can be deployed onto your cloud platform. But where does that build server itself run? Can you stick that in the cloud too? . So your build server has built this great Docker image. Now what? How do you get it out to the cloud? Many companies will have complex firewalls to prevent exactly this sort of access, so you’ll need to start thinking about how to manage the process. Work from the principle of minimum access too, for instance your deployment pipeline only needs read access to your repository. And make sure every access is audited. If your artifact repository is itself in the cloud, you’ll need a separate set of credentials for your build server to access it and write to it - you need to start thinking about having key vaults to store these credentials and figuring out how to safely pass these around. . Before you throw your estate to the internet wolves, think of how you’re going to apply patches and fixes to it. There’s no longer a short-cut whereby you can log onto the machine and switch out a JAR file or something equally terrible, and you don’t want to have to bring down the whole cloud environment for an emergency change to one of your core Docker images. Think about your patching process now, and save a lot of pain tomorrow. On the plus side, running parallel environments and switching between them suddenly got a whole lot easier, so this is one of the best routes to take. . Let’s think about this. Is it really a prerequisite of cloud native? I think so. Sure, if you had a “ big ball of mud ” type architecture you could simply rent a server big enough and host it in the cloud, but the cost savings from elasticity wouldn’t be there. If your architecture is deployed in a modular fashion and you can horizontally scale the pieces with the most load up and down as needed, there’s your saving. . There are many layers of abstraction when moving applications from on-premise to the cloud. We’ve mentioned the simplest step of Infrastructure-as-a-Service, and the furthest step of serverless, but how do we decide where in between to draw the line? OSCE have chosen Kubernetes-as-a-service as our abstraction layer. We build and test immutable  Docker  containers, and deploy these to a cloud  Kubernetes  service. Why? Well - I put this question to our team and the following gems came back: .  Container Orchestration  Just to point out, we need container orchestration software. The real advantage of microservices is that they are deployed separately - all this stuff about the code being modular is not new, even before object-orentation our functions “did one thing and did it well”. The separate deployment aspect means you can scale horizontally, you can restart failed instances, you can kill off long-running instances, manage IP addresses. You get reliability and efficiency - but you need to automate this process. And the market for automating container lifespans is narrow - Kubernetes is the clear choice. .  Speed to market  People are familiar with setting up an environment where you can run Docker containers locally, and if that’s your deployment unit it’s fast to transfer it to your cloud Kubernetes environments. You can run pretty much anything in a Docker image, but if you want your old Cobalt code to be serverless you’ll have to rewrite it in another language. .  Empowerment of Developers  It’s not too bad for developers to manage Docker containers. There’s a familiar language for defining the container, it’s OS concepts that we know, there are a lot of templates to get started with. Kubernetes is another matter - more networking skills are required to configure a Kubernetes cluster that devs may not have. It fits nicely with the the desire for a developer to be self-sufficient; “Build apps, not platforms”. (thanks  Kevin Rudland  for that one) .  Considerable cost savings  Kubernetes managed services have bonuses such as you don’t need to run your own Kubernetes master nodes, the service includes this, so you just pay for your workers. And enough of the underlying infrastructure is obfuscated from you to allow cloud vendors to provide efficient platforms. . The slight down-side of Kubernetes-as-a-Service is that you may feel locked in to the specific Kubernetes service provider - you can limit this lock-in by sticking to core Kubernetes where possible so that moving isn’t too difficult. Or, to steal another direct quote from the team, “avoiding lock-in for its own sake is counter-productive. You’re left choosing from lowest common denominator services and lose all the benefits of new, extremely useful offerings. Porting from one provider to another very rarely happens, but wishing you could use a few of the dozens of great vendor proprietary services happens regularly”. There is Google’s open-source interim layer,  KNative , which our team roundly embrace. KNative runs on Kubernetes and allows you to seamlessly build and deploy Docker containers to Kubernetes without knowing where the underlying Kubernetes pods actually are - are they in cloud A, cloud B or on-premise or on my dev machine? Doesn’t matter! . Application supportability in the Cloud is a really big jump. You can’t just log onto the box and get the output files from a process, or see what’s hanging. You need to think, in advance, of all the things that could go wrong and plan for how you would know they have gone wrong in the cloud. Fortunately the cloud providers help a lot, with some really great logging APIs available, but you have to architect to ensure you are using them correctly. A good first step is to choose an internet-accessible log repository, set up a secure way to read and write to it, and send ALL your logs there. You’ll also need a robust framework for creating correlation IDs so that you can trace function calls between services. Then you can look into the cloud-native offerings of your chosen platform and see where savings can be made. . Wait, what? I thought we got rid of the infrastructure? Well, yes you did, but you still want to control and define your environment. This is where apps like Terraform and Helm come into play. You can define your cloud-provider-specific infrastructure definition, and spin it up and take it down again whenever you like - including your tooling platforms. . There is an implicit number 0 on this list - security. As soon as you’re outside your business LAN, on the internet, you’re instantly at risk of attack. Vulnerability scans and intrusion detection software are suddenly vital, and should be run as part of your build pipeline. As a first level you can install and run software such as  Snyk  yourself, either on-premise or in the cloud, but for cost-saving consider moving to the as-a-service model. . I love this term. Really what it means is, now that you’ve got your infra-as-code in place, you can go around ripping things down with gay abandon whenever they’re not being used, in complete confidence that you can spin them up again whenever needed. This is great for things like the build pipeline, where you will have an awful lot of software that runs and uses a lot of processing power, but only occasionally. Great for UAT, load testing, integration testing too. We love ephemeral! . Now that we’re flying in the cloud, we can start to get into the details of really maximising performance. Why bother with the overhead of HTTP between microservices? And do we need JSON - maybe we can switch to binary APIs between layers? Now that we are cloud native, we can start to focus on really streamlining our architecture. Spread those wings! . I mentioned at the start that I’d talk about why the Open Source Cloud Engineering team choose not to head in at the serverless level of abstraction for our default cloud native architecture, even for a greenfield project. The main reason is that we are looking at the big picture, and a serverless function is not an architecture in itself. For a start, think about  state . Serverless functions are stateless, yet to be modular we must have multiple functions interacting together and managing data between themselves. So we already need some kind of framework to co-ordinate function calls and parameters - we probably need some kind of queue or event stream, and some kind of data store, and some complex rules to manage when each function is triggered based on which data or event. . You might have to significantly adapt your architecture to incorporate all serverless functions, and you’re at risk of losing the purpose of that architecture in the process. There are frameworks to manage state between serverless functions, such as  AWS step functions , and you can read about  our experiences with the Serverless Framework , although they are usually very cloud-vendor-specific. There are very specific constraints around serverless functions too - such as a fixed runtime of  n  seconds per execution, only certain languages supported which may not match the skillsets of your development teams, and quite a steep learning curve for configuration. For instance, who here knew that by default an AWS lambda function will retry 10,000 times under certain failure conditions?! Lambda architectures are still quite bleeding edge and interesting problems such as this can come and bite you when you’re least expecting it. . There is also the very rational fear that businesses have of being tied to a single cloud provider. There is the concern that the vendor may cease to operate, or dramatically change its cost model. Conversely, what if my application deployment process is based around using AWS Elastic Beanstalk, and I suddenly discover that Amazon are in fact my company’s biggest competitor? How can you trust Amazon not to gain insight from the architectures and applications you’re deploying? You can imagine how often that continues to happen in the modern business landscape! Companies want to maintain a certain level of control and autonomy over their applications and are often uncomfortable about close ties to a specific cloud vendor.  Despite all this, Serverless functions can always be considered first, particularly for small new tasks that “hang off” your main architecture, and a recent  DataDog survey  suggests that this is exactly what people do. The user group with the highest take-up of serverless is companies that deploy containers, so if they already have the containers, I’m guessing that the serverless functions themselves are peripheral functionality - helping to log/manage/alert/monitor the main application which runs on the containers. . The “cold-start” problem is often cited as an issue with serverless functions but I do feel we are able to solve this now. The gist of the problem is that in an efficient architecture, the function’s runtime environment is only spun up when it’s needed and is then destroyed, and especially for languages like Java with a JVM to launch this spin-up can be noticeably slow. Common workarounds include reducing the minimum number of functions running, so you are more likely to have one function up and running if it’s handling the majority of requests. Or pay a tiny bit more and have one instance staying “hot” for longer periods to take initial load while the others spin up. Also making sure you pick the right language for the job helps - it’s much faster to spin up a Node runtime than Java, for example, although check out our  earlier post  for more information on how to decrease your Java application footprint. . So in summary, containers are cloud-native and can effectively be serverless deployments and for now, we’re happy with that. Consider function-as-a-service but with an open mind. In the words of  Bret McGowen  when he spoke at  Serverless Days Cardiff , our clients want “Portability using industry standards and Open Source”. And whether you’re developing from scratch or moving an existing architecture, containerised applications deployed to Kubernetes-as-a-service gives you just that, fast. ", "date": "2020-03-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Using AWS CloudFront as a CDN for an S3 Static Site\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Using-AWS-CloudFront-as-a-CDN-for-an-S3-Static-Site/", "abstract": " In my last post  Using AWS S3 and Hugo to Create and Host a Static Website  we looked at creating a static site in AWS S3 using Hugo. As previously mentioned, there are some slight disadvantages with hosting a static site on S3, one of these being that it is all served insecurely from the London region in AWS. Therefore, if someone in Japan was to view my site, there is going to be a slightly longer delay in response times with regard to loading the page. In this blog post, we will look into resolving this issue using  AWS CloudFront , which is a content delivery network ( CDN ) that effectively provides high availability and performance by distributing website content spatially relative to end users. . In times where online content is viewed internationally and fast loading times are pretty much a necessity, the ability to serve your online content within seconds becomes a priority. This is echoed even more by  Google’s research  that states that 53% of users of a mobile site will actually leave if a page takes longer than three seconds to load. For companies that have servers in the UK and users in the US, if you have an image/video heavy home page, your user could easily be waiting a few seconds; which can actually deter them and send them away. This is where CDNs step in. AWS CloudFront is Amazon’s CDN offering that is able to distribute and deliver content to users all over the world at fast times using its edge locations and regional edge caches. . Without going into too much depth, it is important to understand what CloudFront’s edge locations and regional edge caches are and how they will fit into the bigger picture with regard to my site. . Regional edge caches are CloudFront locations deployed globally and are spatially relative and at close proximity to your viewers. They are located between the origin server and the POPs (points of presence), which are global edge locations that serve content directly to your viewers. In my case, S3 is the origin server but in general it is the location where the original source content is stored i.e. S3, EC2 etc. . In the context of my website, let’s go back to my Japanese readers. Wherever they are in Japan, when they make a request to my website, DNS routes their request to the nearest (nearest is best in terms of latency) POP that can deliver my website content to them. During this request the POP will check that it has the content cached, if so, it will deliver it to the user. In the case that the content is not cached then the POPs go to the nearest regional edge cache to fetch the content. . Some POPs may not have certain content because as this content becomes less popular, it may be removed by individual POPs to make room for the more popular content. This is why POPs go to the regional edge caches; they have a larger cache than an individual POP so content remains in the cache longer. This helps keep more of your content closer to your viewers thus reducing the need for CloudFront to go back to your origin server; consequently, improving overall performance for viewers. . Back to our example, in the regional edge cache location, CloudFront again checks its cache for the requested content of my Japanese reader. If the content is in the cache, CloudFront forwards it to the POP that requested it and as soon as the first byte arrives from regional edge cache location to the POP, CloudFront begins to forward the content to my user. CloudFront also adds the content to the POP cache for the next time someone requests it in Japan (or any surrounding locations). . The great thing about regional edge caches and POPs is that all this is done under the hood of CloudFront, we do not have to configure this or manage it in any way. . Below is a picture of the current edge (POPs) and regional edge cache locations. As this picture constantly changes,  Amazon updates an AWS features page  to notify readers of any new locations. .   . In the previous blog post, I walked through the creation of my site, the uploading of it to S3 and the configuration of the static site hosting. Using the S3 endpoint URL I was able to get to the site and see it in all its basic glory. There was however, no CDN. So enough of the theory and throwbacks, let me walk you through how I created my site’s CloudFront distribution. . To start, I created a CloudFront “Web Distribution” and specified the S3 Endpoint as the “Origin Domain Name” and made sure that in the “Viewer Protocol Policy”, I selected “Redirect HTTP to HTTPS” just to make it more secure and to force insecure requests to become secure. I left the caching refresh to the default number (86400 seconds = 24 hours). .   . An additional comment is that CloudFront by default automatically provides us with an SSL certificate which allows for HTTPS - which is used for secure communication between the client and server. One last point is that I made sure to add  index.html  as my “Default Root Object” as this is my homepage and it is the page that I want users to see when they visit the default root URL. .   . Here is the SSL Certificate provided by CloudFront: .   . There we have it, after some simple steps I am now using CloudFront in order to effectively and securely serve my website to those around the world. Furthermore, I have now reduced the load on the origin S3 bucket due to the advantages of using CloudFront edge location caching. Not to mention the additional benefit of speeding up response times for my users. . With each blog post there has been improvements made to my site making it more available and usable. I’m not implementing massive architectural changes, instead I’m leveraging Amazon’s more popular service offerings in order to deliver a more effective and accessible site. These improvements aren’t complicated and are easy for any junior engineer to follow. The most challenging aspect so far in this entire process has been the getting to know Hugo – which again was made more simpler with the use of a Hugo theme. . The only issue I have now is that although users can get to my site easily no matter the location, they have to use the CloudFront URL. Which doesn’t exactly flow off the tip of the tongue as much as a custom domain name. Realising this, the next step for me is to register my own domain and connect it up to my CloudFront distribution – which I will do in the next post. So, stay tuned. ", "date": "2020-04-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Cypress, isn’t that a popular holiday location?\n", "author": ["Paul Monk"], "link": "https://capgemini.github.io/testing/cypress-testing-framework/", "abstract": " Cyprus is a nice sunny island and is the third most popular of the Mediterranean islands. However, as I can’t go to Cyprus right now due to COVID-19, I have had to settle for something that sounds similar instead, that is  Cypress  😉 .   . Cypress is not a sunny island, but it is a relatively new end-to-end testing framework that is creating fireworks in the testing world. It has been dubbed a   Selenium  killer , and given my experience of it I must agree with that statement. I’m sure most people have experienced flaky and unreliable UI tests, and if you haven’t then you have been very lucky! Cypress goes a long way towards solving the issues Selenium and other testing frameworks have. . Do you want easy-to-write, reliable and fast automated tests? If the answer is yes then Cypress is the tool for you! . Selenium is a well known testing tool that has typically been the go-to for automated UI testing. It interacts with the browser using a web driver which allows Selenium to interact with the browser like it was a user. See the diagram below: .   . This architecture allows Selenium to easily support multiple browsers, but it also creates a large amount of complexity. Due to the unpredictability of how browsers load a webpage Selenium tests end up being very unreliable. To counteract this, you have to add in the dreaded  wait , to pause the test until you can be sure the browser has loaded all the webpage elements you need, eventually these waits end up everywhere! This leads to tests that are slow and unreliable. . Cypress in comparison has none of the above complexity. Many other frameworks just use Selenium underneath, but Cypress has been written from scratch. All the tests are written in JavaScript, and these run directly in the browser, so say goodbye to the web drivers. As such Cypress can see exactly what is happening in the Browser, so it is more successful in knowing when a webpage has loaded. Additionally, this also makes it much faster. Cypress tests happen in a blur of activity, as it can interact with elements as soon as they have loaded. Cypress doesn’t have any of the restrictions that the Selenium architecture entails. Behind the scenes Cypress is designed like this: .   . There are lot more features over just reliable tests as well. It has a great UI for developing and running the tests, as well as a headless mode for use with continuous integration tools. By default it takes screenshots and videos of all your tests, so you can easily debug any issues. There are hundreds of plugins offering additional functionality, including support for REST/API integration testing and Cucumber/Gherkin integration. Cypress even bypasses CORS for you! All of this turns Cypress into a full end-to-end testing framework, not just a UI testing tool. . We spent a day trying out various testing frameworks when looking for an alternative to our unreliable Selenium tests. This involved writing 5 tests for  happy path  cases of a basic search page (much like Google). See our results below: .  Languages supported : JavaScript   Ease of setup : 10/10   Development speed : 9/10   Test execution time : 10s (7 tests)   Cucumber/Gherkin Support : Yes . Cypress is extremely easy to setup and use. The team developing the tests wrote an additional 2 tests, and still finished about an hour before everyone else. The tests were easy to read, fast to run, and reliable. .  Languages supported : Has a bespoke keyword test syntax, with additional libraries written in Python or Java   Ease of setup : 5/10   Development speed : 7/10   Test execution time : 60s   Cucumber/Gherkin Support : No (own test syntax) . Robot was a little difficult to setup due to the custom syntax and the team having issues configuring the settings correctly. Once up and running however the tests were fast to write, and when used in the correct way the syntax is very effective. Due to the slow setup this team only just finished their 5 tests in time, and the tests were slow to run. .  Languages supported : Has its own cucumber like test syntax, with Libraries written in Java, and support for executing JavaScript functions   Ease of setup : 8/10   Development speed : 6/10   Test execution time : 55s   Cucumber/Gherkin Support : No (own test syntax) . Karate is a good framework, and the only other framework we tested that is written from scratch like Cypress. The setup was quick, with this team finishing the writing of their tests only a little after the Cypress team. The syntax took a little bit of time to learn and the tests ended up looking a little complex. Our assessment is that a tester without programming experience would struggle to write tests. The tests were also fairly slow to run. .  Languages supported : Java, Python, Ruby, C#, JavaScript, Perl, PHP   Ease of setup : 7/10   Development speed : 5/10   Test execution time : 60s   Cucumber/Gherkin Support : Yes . Selenium has been the go-to testing framework for years and it is very comprehensive. There is lots of documentation which made the setup fairly smooth. However the tests are fairly verbose and it is fiddly to get the timings correct. Overall selenium did not fair well in our tests against the other frameworks. .  Languages supported : Ruby (also a Python version)   Ease of setup : 2/10   Development speed : 5/10   Test execution time : 20s (only 2 tests)   Cucumber/Gherkin Support : Yes . Despite having Ruby experience, when starting from scratch the framework was difficult to install and setup, hence this team only managed to write 2 tests. The tests written were quite clean and ran reliably, however the complexity of the setup discounted this framework for us. . While Cypress faired well against the other frameworks, there are a few disadvantages that should be considered for a fair comparison: . Tests must be written in JavaScript, no other languages are supported . Lack of browser support, it only supports Chromium browsers and has beta support for Firefox . Framework is still fairly new, meaning plugins for integration with CI tools can be limited . However overall for me the benefits have outweighed the minor disadvantages. The biggest selling point of Cypress is the reliability, it is so nice to write a test framework where you can see the green tick on your CI builds over and over again! . Being a NodeJS library Cypress is super easy to get started with. The JavaScript syntax is based off  Mocha  so it is easy to read and already familiar to a lot of developers. . Launching the test runner has conveniently created the directory structure Cypress needs, so just add a  test.spec.js  file under the cypress/integration folder. Cypress will immediately display it in the list of integration tests. . In your  test.spec.js  file enter the following: . The above should be fairly self-explanatory due to Cypress’s simple syntax. However to summarise the above runs a google search for  Cypress  and verifies  cypress.io  appears in the search results. . Click on the  test.spec.js  test at the bottom of your Cypress dashboard to run the test. This should something like the screenshot below, which a nice green bar on the left showing the test passed! .   . You can click on each step that ran to see a screenshot of each step. Additionally, you can re-run the test by pressing the reload button, see if you can beat my fastest run of 2.22 seconds (as shown in the screenshot) 😉 . The example shows how simple it is to get up and running with Cypress, so hopefully you are now ready to try out Cypress yourself, and finally write some reliable end-to-end tests! There are many other example tests included with the default Cypress installation, as well as lots of online resources. For further reading see  the various guides on the Cypress website . ", "date": "2020-05-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Zero to CKA in 2 Weeks\n  ", "author": ["Matt Antley"], "link": "https://capgemini.github.io/kubernetes/Zero-to-CKA-in-2-Weeks/", "abstract": " Prior to the 2 weeks leading up to my CKA exam, I had no Kubernetes knowledge to speak of. I didn’t know what a Kubelet was, how to create a Pod, nor did I know what the Control Plane did. . Fast forward 2 weeks and I actually understand most of these concepts. I also know how to setup a Pod and other Kubernetes Objects as well as some of what the Control Plane is doing. On top of this, I passed the  Certified Kubernetes Administrator (CKA)  with a score of 84% on my first attempt. . This article will contain some thoughts on the Certified Kubernetes Administrator exam and how I managed to pass it after 2 weeks with no prior knowledge of Kubernetes. .   . The first week was spent primarily learning some of the theory of Kubernetes. Learning what a Pod and other Objects are rather than how to set them up and use them. I wanted to slowly expose myself to Kubernetes instead of diving head first and getting overwhelmed by it all which is why I opted to focus on the theory in the beginning. . I started with  A Cloud Guru’s Kubernetes Deep Dive course by Nigel Poulton , this course goes into enough detail to give you a broad idea of what Kubernetes is, how to use it and what it can do without scaring you off. These videos also include some demos that you can follow along with if you’d like. I knew my 2nd week learning Kubernetes would contain plenty of lab based exercises so at this point I wanted to focus on the theory as opposed to the practical. . I extended the theory based learning by reading about Kubernetes Objects, Services and a little on the Control Plane. A lot of this was extracted from the  Kubernetes documentation  which I recommend you become familiar with if you thinking of learning Kubernetes and especially if you’re planning on taking the CKA exam. . Throughout the week I made notes of what I’d learnt using  Nuclino , detailing what a Pod was, how it can be used and how it differed from other Objects like Deployments and DaemonSets. If making notes didn’t help me in taking on certain pieces of information then I drew diagrams that allowed me to understand some concepts with greater ease (Services like NodePort and ClusterIP come to mind for this). Over the week I had built up a fairly comprehensive document that I could refer to if I needed a refresher on anything I had covered. . For the 2nd week I had enrolled onto a CKA course hosted internally by Capgemini which was put on as part of getting more employees to learn about Kubernetes and get themselves certified with the CKA exam. .   . The agenda for the week was fairly straightforward, days 1–3 were spent covering core concepts, Kubernetes Objects, Services and carrying out labs (hands-on creation of these components). A lot of the areas covered in these first few days would feature heavily on the exam and become the majority of the marks that made up the exam. Day 3 also included a mock exam with a variety of exam-like questions which was used to gauge the classes’ current confidence levels with the exam coming up in 2 days. . Day 4 of the course covered some of the smaller sections of the exam like Logs, Top and Config Maps but most of the day was used to introduce components that would be used for more real world systems and wouldn’t feature in the exam. This included components like Ingress Controllers, Resource Limits and some Pod Security basics. . Day 5 was reserved for some exam revision in the morning which consisted of a light Q&amp;A to test our knowledge and for us to ask last minute questions if needed. We were then given some quick-fire setup questions to work through to warm ourselves up before the exam. For example, setting up a single Pod with the name ‘hello-world’ and a namespace of ‘cka-training’ which were a good pre-exam warmup.  . Looking back to the first day of the course, creating a Pod took 30-40 minutes to complete as a group, this was by going either the declarative (YAML) or imperative (kubectl) route. However by the end of the week the majority of participants could do this within a few minutes. For me this was down to the repetition in which we did these tasks and continuous use of the kubectl CLI tool to build a lot of the Object creation into muscle memory. . The exam is 3 hours long and consists of 24 questions in total. It is 100% hands-on and doesn’t contain any multiple choice questions, this can either work for or against you depending on how comfortable you are with a terminal. As for me I’m used to sitting in a terminal, using vim as my text editor and typing away Linux commands for most of the day, because of this I was more comfortable than some would be in this situation. . As you go through the exam you’ll notice that each question displays the percentage of how much it’s worth in the marking criteria and it’s worth noting these are you go through. A virtual notepad is available which I used to make notes of how much each question was worth and if I felt I had answered it to a satisfactory level. If not I simply wrote “Come back to” if I felt I was spending too much time on a particular question or if I didn’t know how to answer it off the top of my head. . 3 hours seems like a lot when you look at it but the time will quickly deplete as you go through the exam, make good use of your time and if you get stuck or you are unsure make a note of the question in the Notepad and come back to it later. I managed to answer 21/24 questions to a level where I was satisfied that I had the right answer (Although I’ll never be sure) this was with 30 minutes to spare at the end. It allowed me to go back through the remaining 3 questions and work at them a little more to try and obtain some more marks from them. . My number one suggestion is that you should try to avoid using Google while working towards the CKA exam, stick to the Kubernetes documentation as that’s pretty much all that will be available to you in the exam. Gaining some familiarity for the docs will pay off for the exam as well as any additional work you end up doing with Kubernetes. The Kubernetes documentation is extremely good. In most examples you can find exactly what you need to answer an exam question (Some tweaks may be required) but instead of tearing your hair out, you should head straight to the documentation in order to prevent wasting time on your exam. I’m really just trying to hammer home how important the documentation is to you in the exam, hopefully you got that! . Despite spending 8 hours a day in the 2nd week on the CKA course I still put in an additional 2–3 hours a night into shoring up my notes, playing around with kubectl and doing a bit of reading of the documentation to fill in any gaps in knowledge I had at any point. I think this additional time paid off in the end as it helped me build up my confidence with Kubernetes and especially with kubectl. . Using the imperative way of creating resources (kubectl) but adding a few extra parameters to have the command generate the YAML for you to go the declarative route is extremely beneficial. It allows you to double check what you’re doing against what the question is asking for as well as giving you a good base if you do need to edit anything. It’s a win-win.  . The arguments you need for this are: . For example: . This will create a file called  nginx.yaml  containing the YAML needed to create a Deployment called nginx with the nginx image. From there you can edit the YAML and simply apply it! . Lastly, If you’re going to take the CKA exam then I wish you the best of luck! I definitely preferred this hands-on exam approach as opposed to the multi-choice-a-thon exam format like some other exams have you do, but maybe that’s just me. ", "date": "2020-05-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Ask In The Channel\n  ", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/culture/ask-in-the-channel/", "abstract": " Our team have been using instant messaging for a long time. There are a lot of us, working in different parts of the world, and chat can really help us to stay in touch and build connections. We were early adopters of Slack, having previously dabbled with IRC and Skype, and there’s a fairly healthy chat culture among the team. However, recently I’ve noticed something that feels to me like an anti-pattern: too many private messages. . Slack provides statistics on how many messages are sent, and what proportion are in private channels, public channels, and direct messages. When the number of direct messages is higher, that feels like a red flag to me that the team isn’t comfortable sharing with each other. . When someone is unsure of something, or needs help, they’ll often send a private message to ask. Fair enough, you might think. That’s what instant messaging is for, isn’t it? Enabling quick communication between colleagues. The problem is that most of the time, other people are likely to have the same question. . It’s often junior team members asking the question, and I wonder why they’re asking one to one. If they were to ask in our devs channel, there would be a lot more people who might know the answer, and the chances of one of them having the time, energy, and motivation to help would be much higher. Also, it’s quite likely that other members of the team would benefit from seeing the answer to the question. . This might just be a grumpy old man getting fed up with answering the same question again, but it fits with my beliefs about open source, and  the value of transparency . I may be echoing  something that other people have already said , but I think that it bears repeating. . By asking privately, we’re behaving as if not knowing is something to be ashamed of. It’s only natural that we don’t like to admit our ignorance - there can be a sense of shame in needing to ask for help. But to me, there’s something empowering about  feeling secure enough to be able to admit my ignorance , and if we can share knowledge, we all benefit. . Until recent events forced change, we weren’t a fully distributed team - we were a team of hubs, with the majority in either the London, Mumbai or Bangalore office, plus a few outliers like me who mostly work from home, and only come into the office from time to time. . In that scenario, it’s easy for the remote team members to be second class citizens, as the office-based people inevitably have face-to-face conversations, perhaps at lunch or while getting a cup of coffee. Now that we’re all remote, that has changed, but as  Dirkjan Bussink has said , when we work remotely we need to be more mindful about how we communicate, and more intentional in cultivating bonds between members of the team. . Recently COVID-19 has brought about an increase in the number of people working remotely, but we need to remember that this isn’t normal remote work - it’s  working at home during a crisis , and  it’s difficult . In these challenging times, we need to be even more mindful of the fact that we don’t get a full picture of what’s going on at the other end of a chat message. . Chat seems to have become the default method of communication for remote work, but we need to understand when to use asynchronous communication methods (like chat), and when we need the higher bandwidth of synchronous methods. . As with any piece of software, it’s important to think about how we use chat, and to make sure that it’s a tool that we control, rather than allowing it to control us. Rather than permanently having a chat window open, bombarding us with distractions, it’s helpful to open it when we’re ready to communicate, and close it when we need to focus. Similarly, we can turn off incoming messages in Outlook using the “Work offline” setting - this is great, as long as you remember to go back online now and then. . There are  problems with instant messaging , but I think that chat brings a lot of value to both distributed and co-located teams, and it’s unlikely to go away any time soon. Besides, chat is a relatively new invention, and humans are still learning and evolving  the etiquette of instant messaging . It’s important to strike a balance between under-sharing and spamming - another antipattern in instant messaging is  excessive use of @here and @channel . But if we can create conditions of psychological safety in our teams, where we’re more comfortable displaying our weaknesses, then we can grow stronger together. ", "date": "2020-05-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Using Route 53 to Create a New Domain for a Static Site\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Using-Route-53-to-Create-a-New-Domain-for-Static-Site/", "abstract": " Over the years there have been countless websites that offer free hosting to users, with the limitation that you use their specific URLs. As shown in previous blog posts where I  create a static site and host it on S3  and then  use CloudFront as a CDN , S3 and CloudFront are no exception to this. Both services have their own flavour of URLs that are not user friendly in the slightest therefore making it more challenging to direct someone to. Conveniently for me, AWS have their own domain name system (DNS) called Route 53 that allows you to register a custom domain name that you can choose yourself. . For my site I use CloudFront for my CDN in order to deliver the content around the world and if you followed my last blog post, you know it’s completely public and also secure via HTTPS. The only issue is that I am forced to use the CloudFront URL i.e.  su3jcsi39s.cloudfront.net . This isn’t exactly a catchy URL that someone will remember when I tell them – providing I can even remember it myself. To get over these inconveniences, you would use a custom domain name. As I have used AWS for all of my hosting needs so far, it made sense to stay within the AWS platform and use the AWS DNS system –  Route 53 . . First, I needed to come up with a catchy domain name that people can remember. If you’ve been reading my previous blog posts, you can probably guess what I went with. That’s right: chrisjburns.com. .   . For an annual payment of $12.00 I was in possession of my own domain name that I’ll be sure to remember. I haven’t bought my first house yet – but I can imagine this is what it feels like when you finally take a step onto the ladder – only in this case it’s a step onto the ladder of acquiring my own “home” in the world of the internet. . After my domain name was acquired, I applied for my own SSL/TLS certificate. This is so users of my site can verify that I am indeed the owner and that all communication will be secure and encrypted. It also increases my Google rankings and improves conversion rates whilst building users trust. . Again, I was in luck as AWS has their own Certificate Manager called, wait for it,  AWS Certificate Manager . . As I had no Certificates with Amazon and have never applied for one before, I had to follow the documentation. However, summarised, I applied for a “Public Certificate” and added the domain  chrisjburns.com  and sub-domains  *.chrisjburns.com . The second is to ensure that if I ever wanted to create any sub-domains to my site – possibly a blog? Then when I create the sub-domain  blog.chrisjburns.com , then the certificate will also apply to the new sub-domain. . After some DNS validation, which was a slightly more convoluted and tricky task as  documented here , I had finished the certificate application and all I had to do was wait. Within an hour I had the result and it had been approved and issued to me. .   . Once all of the individual components were ready, it was time to connect them all together. All I had to do was hook my new certificate and domain name up to my CloudFront distribution. . First things first, I modified the CloudFront distribution that I created as part of the previous blog. After going into the distribution’s “Distribution Settings”, I edited the “General” page so that the distribution used my Custom SSL certificate and CNAMEs. .   . The last step was to route all inbound queries to “chrisjburns.com” to the CloudFront distribution. Again, in classic AWS style, this was simple. . Going into the Route 53 Hosted Zone that I had for my new domain, I created a new record set with type “A – IPv4 Address” and an Alias with the target being the CloudFront distribution URL (this was also a drop-down list to choose from). .   . Once the new record set was created I hung around and waited for Route 53 to do its thing with regards to setting up the routings. After a few minutes I was able to see my new domain name point to the CloudFront distribution. .   . There you have it, a completely static site hosted on S3 and served by CloudFront with domain routings handled by Route 53. . With my site being available to use under its new domain name there is one last point to make. Due to the sites infrastructure being serverless, I do not pay for any monthly hosting fees. Instead, I pay each time the site is used (due to the low price of each request, I more than likely won’t be paying anything for my site’s use). This makes it vulnerable to DDoS attacks – or in my case, denial of wallet attack (DDoW). However, due to the highly scalable and highly available services that I am using, denial of service is unlikely. What is more likely is someone running a program that bombards my site for hours which in turn sends my billing costs through the roof. . To counter this, there is managed DDoS protection provided by  AWS Shield  that is integrated with both Route 53 and CloudFront. This means if said attacks were to happen, I don’t have to worry. . With the final solution architecture looking like the below, I have a fully available and scalable website that is accessible to the world via my new domain. .   . I can start adding new features to my site whenever I want as the underlying infrastructure is all in place. In the future, I won’t do any blog posts talking about minor changes to the site as it would essentially just consist of me amending/adding a few new pages and reuploading to S3. However, if I were to make any updates with the process or architecture to the site, then I’ll be sure to let you guys know in a new post. My next blog will probably be around creating an automated CI/CD pipeline so I can commit and push to a Git repository that in turn triggers a pipeline that automatically deploys my new site to S3. But for now, enjoy the content and if you have any questions, find me on the socials. ", "date": "2020-05-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Using GitHub Actions and Hugo Deploy to Deploy a Static Site to AWS\n", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Using-GitHub-Actions-and-Hugo-Deploy-to-Deploy-to-AWS/", "abstract": " Owning a website can be fun. Rather than thinking of it as being a bunch of words followed by a “.com”, instead think of it like owning a home in the world of the internet. Your own personal space that you can customise to your heart’s content. Depending how you’ve set up your website, when adding or updating any content, in the beginning there can be the manual task of deploying your new content to your website. When updating  my site  with new content, I had to make the changes locally, render them and then upload the newly rendered content to S3. This task only took around 10 seconds, but I’m an engineer and I like automation. . In previous blog posts I documented my journey of  creating a static site using Hugo and hosting it on S3  whilst then  utilizing CloudFront  in order to evenly deliver my content around the world. Finally I  registered my own domain using Route 53  that served as the front door to my website. . Echoing the above, once all of the infrastructure was set up, there was still a hint of manual work needed when amending the site. This was bad, at least in my eyes. My next goal was to remove as much of the manual work around the site as I could in order to allow me more time to create new content. . Fortunately for me, Hugo has the  deploy  functionality. This allows you to upload your site directly to a Google Cloud Storage (GCS) bucket, an AWS S3 bucket and/or an Azure Storage Bucket. Due to my storage bucket of choice being S3, I used the S3 deploy functionality. . Looking back, it was incredibly easy to automate the task of uploading my site onto S3. All I had to do was modify my  config.toml  with some deployment specific configuration and run  hugo deploy . Not to forget having the  AWS credentials and profile configured  in order to integrate with AWS. . The  config.toml  deployment configuration looked like the following. .  deployment.targets  describe the name of the deployment as well as the URL of the bucket that the site will be uploaded to .  cloudFrontDistributionID  is the ID of the CloudFront distribution you use to deliver your site. If you do not use CloudFront, this can be removed as  hugo deploy  uses it to perform cache invalidations . The  deployment.matchers  describe the certain behaviour and caching policies for specific file types . After configuring the  config.toml  with the deployment configuration, I ran  hugo  to render the sites assets into the  public/  directory and then ran  hugo deploy --force --maxDeletes -1 –invalidateCDN  to deploy the site to S3. . Here’s a breakdown of the deploy command: .  --force  - Forces any files that may not have changed to be reuploaded (in case of diff problems) .  --maxDeletes -1  - Sets the maximum number of files to delete, or  -1  to disable (default 256) .  --invalidateCDN  - Invalidates the CloudFront cache for my site. This means all cached content that is held in any of the CloudFront edge locations across the world will be removed ready for the retrieval and caching of new content. . Here was the output of the deploy: . After about 30 seconds, once all of the CloudFront caches had been invalidated, I could navigate to my site and see the newly added/updated content. . As shown above, I reduced the effort of deploying my site down to one command. Instead of rendering my site’s assets and then manually uploading the content to S3, I could now do all this simply by rendering the assets and using the Hugo deploy functionality. On top of this, it also invalidated CloudFront caches which was previously a manual task. Although I only had to run two commands to render and deploy my site, there was one more step to add in order to make this entire process fully automated - GitHub Actions. .  GitHub Actions  as described on their website: “Automate your workflow from idea to production”. Since my website source code is checked into GitHub, this was great for me as it allowed me to easily automate my sites deployments every time I commit any changes - for free. . To enable GitHub Actions, I  setup a workflow  in the ‘Actions’ tab of my site’s repository. For my workflow, I wanted to be able to build and deploy my site for every merge to master. To do this, I added the below at the top of my  main.yml  in the  .github/workflow/  directory. . This will make sure that on every push to master a workflow build is kicked off. . Next, I added a build job that would contain the steps of building and deploying my site. These should look familiar with what I previously had to run manually. . This would checkout the repository inside of the workflow and also use the  peaceiris/actions-hugo@v2  featured action that installs Hugo inside of the workflow in order for it to perform builds and deploys. . Next I added the build step that would handle the rendering of my Hugo site’s assets. . Lastly, I added the deploy step with the command that I used previously to deploy. . As you can see, I use the AWS credentials in order to deploy. This makes sure that the workflow has the correct permissions in order to deploy the site to S3. The credentials are provided using  GitHub secrets , again, very easy to add. . After this, there was a fully working workflow that was triggered anytime it detected a change on master. Whether it be a direct push, or a merge request from a feature branch, any time it detects a push to master, it will kick off the pipeline. . With the workflow set up and the building and deploying of the site being fully operational. There was just one thing I had to tidy up. If you’re familiar with AWS or any other cloud provider, you will more than likely be aware of the  principle of least privilege . For those who aren’t well versed in what it entails, the principle of least privilege is a mantra that dictates that a user or resource only has the permissions it needs in order to perform its job function. At no point should it have more permissions than it needs. . For my pipeline, initially I had provided my personal AWS credentials that had admin access to all of the services within my account. This was bad. At no point should GitHub Actions have more permissions than it needs to be able to deploy my site. Knowing this, I created a new  IAM User  with its own set of AWS credentials, that I then used to replace my personal credentials in the pipeline. . For the new pipeline user, I created a policy that would include the permissions needed in order to deploy my site. After some investigation, I narrowed down the permissions that  hugo deploy  needs at a minimum in order to deploy from the pipeline. The following is the policy that was attached to the pipeline user, that is used to perform the deployment of the site – following the principle of least privilege. . As shown above in the policy, we are giving the pipeline permissions to update and add objects to the S3 bucket as well as adding permissions that allow for the creation of cache invalidations in CloudFront. . Once the policy was attached to the pipeline user, and the pipeline user AWS credentials were provided as secrets to the pipeline, I performed another push to master to ensure everything worked as expected and within a few seconds, I had all green from the pipeline with an updated site. .   . In this blog post I have detailed the steps I took in order to setup a fully functioning CI/CD pipeline for my site in GitHub Actions. The pipeline renders and deploys my site’s assets to S3 automatically. From what used to be manual tasks, GitHub Actions and Hugo Deploy have now taken these tasks off my hands leaving me – the engineer – more time to develop new features for my site without having to worry about the manual duties in between. . Hope you enjoyed this series of blog posts, any questions, you can contact me on the social networks listed in  my bio . ", "date": "2020-06-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Out of the Comfort Zone and into the Classroom\n", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/learning/out-of-the-comfort-zone-into-the-classroom/", "abstract": " Last year I decided to run a React ( a frontend JavaScript Technology for building user interfaces ) course within the account I’m working on. This year, I decided to do the same thing again, except to a wider audience with much better planning and structure. . I’ve finally got round to writing something down about my experiences over the past year or so. Hopefully it’s useful for anyone thinking whether or not to run a course/series themselves. . I don’t like speaking in front of people. Fact. . I generally get very nervous, and often I prefer to be the person in the back quietly listening in, as opposed to being at the front leading the conversation. . So why did I decide to run a course? A few reasons: . I enjoy frontend development, and React in particular, and I wanted to see if others would be interested as well . We wanted to try and build a capability within the account in this area, and I know a bit about React as a technology and wanted to try and share that knowledge . I thought it would be a good idea to get out of my comfort zone . With enough ‘solid’ reasons behind my thinking, I decided to just go for it, what’s the worst that could happen, right? . Mid 2018 I decided to start running the first ‘Learn React’ series. I’d put together a chunk of content, which aimed to give a basic grounding in React, split down in weekly bitesize pieces. The turnout was fairly small at first; over the course of several months, we probably averaged less than 10 attendees. . Being the first time I’d ever ran a course/series, I had no experience to draw on regarding how to run things; should I live code everything? Read through slides? Show video content? . I had lots of ideas of how to do things, so in the end I just mixed it up and tried them all - which worked to a degree, but I often found myself going off-topic and talking around concepts I hadn’t planned in the session content. Sometimes I’d be talking around how to render multiple components from a map function (like a todo list for example) and end up explaining how the JavaScript  map function  works. This was a common trend throughout the series and a definite learning point from my perspective - next time, keep things simple and on point. If you touch on areas outside of the technology you’re focusing on, provide a link or brief overview, don’t let it detract from the content you’re trying to deliver. There were several challenges I found such as this throughout the delivery of the course. . One of the major struggles I found, as I alluded to above, was choosing the right level of detail to go to when trying to cover a certain subject. I’d be explaining a topic such as React state and how  setting state is asynchronous , and suddenly find myself going down a rabbit hole about the asynchronous nature of JavaScript. At first I didn’t realise that I was actually adding more confusion to the conversation, rather than keeping it simple. . Sometimes it’s good, and much needed, to explain how key aspects of the JavaScript language are key to how React works, however I found it important to keep those explanations to a high level, so as not to cause confusion or detract from the main points. . I guess if you’re normally cool, calm and collected, live coding probably isn’t an issue for you. However, if you’re anything like me, and usually hold none of the aforementioned traits whilst in front of an audience, live coding can be tough. . I learnt this the hard way. Last year when I first started running the course, I was live coding quite a lot. It wasn’t necessarily the typing of the code that was an issue. I’m convinced that live coding in front of an audience is a recipe for turning a perfectly working laptop into a cataclysm of failures. Suddenly your IDE stops working, the WiFi dies, you’re trying to zoom in for those in the back and end up closing the IDE altogether. Before you know it, writing a routine  Hello World  app has turned into solving the  Travelling Salesman Problem  in front of an audience under tight time constrictions. Less than ideal. . Live coding also takes time, often more time than you realise when you’re also trying to explain a topic for the first time. You quickly run out of time and find you’ve only half explained something, and the half you did explain was all over the place because you were trying to build it at the same time! . Just don’t. . Don’t convince yourself that  you’ve got this  or  it’s only explaining X, that’s easy . . There were a few times where I did exactly the above, and as soon as I was in front of people, that idea of  you’ve got this  quickly turned to  you should have prepped for this . . Reason being (for me) was two fold. Firstly, you’re potentially teaching a technology or aspect of a technology to people for the first time, they’re going to take on board what you say - so it’s only right that you prepare and deliver that content as correctly and factually as you can. You’ve made the effort to teach the course, do it justice. . Secondly, explaining a concept casually to a friend is a lot different to standing in front of a (hopefully) captivated audience. You can quickly forget something or go off point if you’ve not prepared anything, it also gives you that safety net if you do  freeze  or forget something you’ve gone over a hundred times before. . After the first run of the series last year, I did come across a lot of challenges, but I also learnt a lot as well. . You don’t have to go overboard, but doing a little prep work pays dividends in the long run. This year for the course, I actually created a  sample application  first, split up into topical chunks in different Git branches. This was a nice way of splitting the content up and allowed me to cover certain aspects more easily. I could still live code to showcase particular things, but it gave me that safety net of ‘here’s one I did earlier’. . Again, similar to above, don’t try and wing it. You’ll only do yourself and the audience a disservice if you do. That little bit of extra effort into preparing for each session definitely makes a big difference to both your experience of teaching, and the audiences experience of learning. . The level of detail you need to go too will depend on your audience. For me, the audience was very new to React, front end development and JavaScript, so my explanations of certain aspects of React differed between sessions. Ultimately, I found it was about striking the right balance between just enough extra detail to support the React content, but not too much that it would overload people or add more confusion than clarity. . I  actually  enjoyed running the series (and still do) much more than I thought I would. . As I mentioned at the start, standing up in front of an audience is definitely not my thing and pushes me out of my comfort zone quite a bit. However, the process of running the course(s) has benefited me in several ways; . I know a lot more now about React than I did at the start, because I wanted to ensure I was crystal clear on the aspects I was teaching . I’ve gained more confidence now in speaking up in front of people, and sharing the things I’ve learnt . I was able to share my knowledge with a lot of new people, introducing them to new technologies and helping them up-skill . If you’re anything like me, and the idea of standing up in front of people and teaching a course scares you, you’d be surprised at how much you’ll learn from it. Not only from the content that you’re going through, but also about yourself. . One of the great things about our team and Capgemini in general is you get the freedom to try new things. Knowledge sharing and teaching new content is great, all of us will have been that knowledge sponge throughout our careers, so it’s nice to give back, and even better when your supported in doing so. . So if you’re sitting there thinking you’d like to run a learning series, but you’re not sure how it will pan out. Go for it, you might surprise yourself. ", "date": "2019-04-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The bigger picture and the smaller details\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/agile/bigger-picture-smaller-details/", "abstract": " Many people, myself included, say that  sprints are the wrong metaphor  for software development. When we think about the need to maintain a steady, sustainable pace, and the importance of  avoiding burnout , the idea of  the sprint can give us the wrong idea . But maybe it makes more sense if we think about how real sprints work, and about all of Scrum’s ceremonies, not just the cargo cult of the stand-up. . When Usain Bolt finishes a race, he doesn’t just start again straight away. He does his victory pose, smiles for the cameras, does some warming down, and has a few days rest. When it’s time for the next race, he spends time preparing himself, both mentally and physically, for the challenge ahead. He does everything he can to ensure that he’s in the best possible state before his next sprint starts, so that he’s able to function at maximum capacity to achieve a good result. . As software development teams, all too often we ignore the importance of these activities between sprints. In particular, the retrospective is frequently undervalued. It’s easy for managers to see the retrospective as a time-consuming navel-gazing session, where the team doesn’t deliver any business value or burn any story points. If the team is under pressure to deliver faster, it’s probably the first thing that gets cancelled. . In our  software engineering team manifesto , we laid out some points for how a team should operate, including the idea that we “focus on the details, and on the bigger picture”. Maintaining a healthy balance between these two ways of seeing can be a challenge, and it’s one of the things that people often find difficult when moving from an individual contributor role to  take on leadership responsibilities . As developers we’re used to getting deep down into the details of an issue, but as leaders, shaping the direction of a product, we need to take more of an overview. If you’re a technical lead it can be very difficult to switch between these two contexts, especially if you don’t take time to reflect. If the team is working flat-out, it’s easy to fall into the trap of focusing solely on the small details - things like burndown rates or technical implementation challenges - at the expense of considering the bigger picture of the project’s direction of travel. It’s important to schedule time for both modes of thinking, and retrospectives are an ideal opportunity for thinking about the bigger picture. . Another analogy sometimes used for software development is  climbing a mountain . When you’re climbing uphill, you have to keep looking at your feet, watching out for uneven ground to make sure you don’t trip up. It’s only when you stop and look around that you get a chance to enjoy the view, and understand the whole point of the endeavour. If you keep plodding on without taking a break, it’s very easy to get disheartened. . That’s what retrospectives are for: a chance to take stock of where we are and how we got there. They offer a moment to pause, take a breath and think about where we’re going next, and whether we’re going the right way. They’re an important opportunity to  celebrate what we’ve achieved, and recognise the hard work we’ve put in . . So let’s remember to take a break from time to time, appreciate our colleagues, understand our achievements and our challenges, and think about where we go from here. ", "date": "2019-04-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Introduction to Quarkus: Supersonic Subatomic Java\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Introduction-to-Quarkus-Supersonic-Subatomic-Java/", "abstract": " Due to the constant evolution of different languages and frameworks in the tech industry, developers are able to develop and deploy apps with faster speeds and lower footprint on the underlying systems in which they are deployed to. This simultaneously increases the need for faster deployments with lower footprint which motivates engineers to make the next best thing in regard to this. This can be one of the leading arguments when deciding which language or framework to use. Is it lightweight? How fast does it run? What is its footprint on the system as a whole? Although there are many engineers who let their bias kick in when choosing a language or framework, you should always choose – within reason – what better suits the current scope of work you are anticipating. One of the reasons Java may not be considered over other technologies when dealing with API microservices is its footprint. Cue Quarkus.. .  Quarkus  is  “A Kubernetes Native Java stack tailored for  GraalVM  &amp;  OpenJDK HotSpot , crafted from the best of breed Java libraries and standards”.  Quarkus aims to make Java a leading platform in Kubernetes and serverless environments by also simultaneously offering developers a cohesive reactive and imperative model to  address a wider range of application architectures.  . Quarkus tailors your application for GraalVM and HotSpot and because of this, you can achieve amazingly fast boot times (by using a technique called compile time boot), incredibly low resident set size (RSS) memory, near instant scale up and high-density memory utilization in container orchestration platforms. . Quarkus also claims that when it comes to the amount of time it takes to boot up the application and deliver its first response it is in the milliseconds, as shown below in comparison with other stacks. . From the outset, Quarkus has been designed around a container first philosophy meaning that it is optimised for low memory usage and fast start-up. It does this by enabling the following: . First class support for Graal/SubstrateVM . Build time metadata processing . Reduction in reflection usage . Native image pre boot . It also makes developers lives a lot easier by allowing: . Unified configuration . Zero config, live reload in the blink of an eye . Streamlined code for the 80% common usages, flexible for the 20% . No hassle native executable generation . Wired on a standard backbone, it brings the best breed of  libraries and standards  you love and use. These standards include CDI, JAX-RS, ORM, JPA and many more and instead of a whole application server the applications are run in an optimized runtime either via a Java runtime, native executable or a native image – something we will dive into later on. . Enough of the high-level talk, lets dive into some code and real examples. . If you navigate to the Quarkus site, they offer guides on how to correctly setup and configure your system ready to create a Quarkus application as well as how to create your first Quarkus application. For this blog post – whilst skipping the setup – we will align most of our focus on creating the application itself. So, let’s get started. . The easiest way to create a Quarkus project is to use the Quarkus plugin for Maven by running the following inside of a terminal: . This will use the Quarkus plugin in order to generate a skeleton project with a basic  /hello  endpoint along with a landing page, some tests, a Dockerfile for both native and jvm modes and a config file. This acts as a skeleton so you can build on top of it and tailor it towards your requirements. Once the project has been generated let’s try compiling it using the  quarkus:dev  profile:  mvn compile quarkus:dev  . As shown by the logs above, we can see that after it had finished downloading all dependencies, it both compiled and started in 0.866s. For a first metric that isn’t bad at all, especially when compared to a basic SpringBoot starting time of normally a few seconds – which excludes the stripping away a lot of unwanted config (which you wouldn’t want to have to do if it was a small project). . As it’s started, we can visit the landing page at  localhost:8080  or even hit the basic  /hello  endpoint with  localhost:8080/hello . . So, we have established so far that both Quarkus compilation and start up is pretty quick but let’s dive into the development mode they boast about. Since we have run the app in  quarkus:dev  mode, we are essentially already doing this. Development mode allows hot deployments with background compilation which means when you make changes to any Java, resource or configuration files and re-hit the application, your new changes will immediately take effect – no downtime. Re-hitting the application essentially triggers a scan of the workspace and if there are any changes detected by Quarkus then the affected files are recompiled, and the application is redeployed, and your new request is then handled by the recompiled code.  Additionally, in dev mode, it also listens on port  5005  for a debugger, however you can disable this by running it with  -Ddebug=false . . Hot deployments you say, tell me more! Ok, so, let’s alter the code, so instead of the basic  /hello  endpoint returning “hello”, it returns “hello, this is a new compile”. We can do this by just modifying the return String in the  GreetingResource.java  file: . Now, as stated above, these changes will be automatically compiled and redeployed when we hit the endpoint. Let’s give it a try by hitting  localhost:8080/hello  again in our browser or REST client. . Wallahhh!!! We now see the new text – which proves that the change was indeed recompiled and redeployed. What do we see in the logs? . Well, that’s pretty impressive. As you can see from the logs above, our changes were detected in the  GreetingResource.java  file which triggered Quarkus to stop in 0.002s, further resulting in a recompilation of the changes and redeployment – all in 0.409s. It literally took Quarkus less than half a second to detect the changes made, stop the application, recompile and restart and perform the request. That is essentially live development as no human would be able to distinguish a Quarkus hot deployment any differently from a normal request response time. Normally, you would have to stop the app, recompile it and then restart. That would take normally more than a few seconds most of the time. Quarkus impressively does all this in under half a second – the time of a normal request response from a running API. . Also, one positive – or possibly negative – point on Quarkus’ hot deployments is that the flow of development isn’t interrupted. This is due to no more waiting for rebuilds and restarts of the application which in turn means no more coffee breaks or social media updates. I mean come on, we have all been guilty of checking our phones whilst waiting for a rebuild to complete to only then come back to work completely unfocused and having to take a few minutes to get back in the flow we were in mentally before we stopped. Well, this can be seen as a positive for Quarkus hot deployments as they allow for a constant flow of development by reducing the chance of distraction whilst at the same time increasing productivity. However, some people might see that as a negative as they won’t be able to keep up to date with the matrix of social media, additionally, some may even reduce their coffee intake – but is that really a bad thing? . Now it’s worth mentioning that there are other tools that you can use in order to achieve hot deployments that simulate live development. Tools such as  OpenLiberty  (minimal server runtime) in combination with  WAD  could also be used to simulate live deployments, however Quarkus does offer this out of the box so there is no assembling of different tools needed. . Dependency injection (DI) in Java is thought to be one of the crucial aspects and needs for an application – especially in microservices. DI allows for the reduction in coupling in your application making it more malleable and easier to test. This is one of the core reasons for why Spring – Java’s most popular framework – was initially developed. Nowadays a high percentage of Java apps are written with Spring as they have evolved by taking over many areas of Java development, but a criticism of Spring from developers is that sometimes it’s too big for what you need – a fair criticism some might say? In Quarkus, DI stems from ArC which is a CDI-based DI solution made specifically for Quarkus architecture. ArC additionally comes as a dependency of  quarkus-resteasy  so we already have it in our project. . Let’s do an example by using ArC. First we are going to create a  GreetingService.java  that has a  greeting  method which accepts a  String  argument for  name . The functionality of this method simply greets the user by their name: . The greeting service above, will now be injected into  GreetingResource.java  which we’ll call by passing it a  name  parameter in order for it to return a greeting response. . Now, whilst our application is still running, lets hit the new endpoint to make use of those hot deployments. . There we have it, I have been consequently greeted, and here are our logs when we hit the new endpoint: . Pretty neat, now we have an endpoint with a service as an injected dependency which we have made use of to greet us by our name. Pretty simple stuff, to find out more information about Quarkus DI  visit the Quarkus guides . . A wise man once said, “for every hour spent in development, spend 5 in testing”. When we generated the project initially using the Quarkus plugin, it also generated a few tests for us. Quarkus supports both JUnit 4 and 5 but for this we will use JUnit 5. In the GreetingResourceTest.java we have a few new things. . Initially the first thing we can spot is  @QuarkusTest . This is the Quarkus test runner, it essentially instructs JUnit to start the application before the tests. Let’s look at the first test: . Using  RestAssured  we can make use of the BDD style testing. Let’s run  mvn test  to kick off the tests and see what the logs output. . Not bad, pretty easy stuff. For more information on testing in Quarkus just  visit the official website  where it has information of injections, mock support and native executable testing. Although they may not be the popular frameworks and libraries that are most commonly used (Mockito etc), Quarkus has already shown promise with filling those gaps, so I wouldn’t be surprised if they didn’t include support for your favourite libraries and frameworks in the future – providing they don’t lose sight of its unique selling points of being super-fast. . Java applications are normally compiled and packaged into  .jar  files. With Quarkus it’s the same thing with some minor differences. With Quarkus the application is packaged using the package goal (mvn package) and it produces 2 jar files: .  getting-started-1.0-SNAPSHOT.jar :  containing the classes and resources of the projects, this is the standard artifact produced by the Maven build. .  getting-started-1.0-SNAPSHOT-runner.jar : being an executable jar. Be aware that it’s not an Uber-jar as the dependencies are copied into the  target/lib  directory. . When the two  .jar  files have been created, you can run the application without Maven by using the executable jar produced by the package goal by running the following command:  java -jar target/getting-started-1.0-SNAPSHOT-runner.jar  . 0.506s start up time, not bad. This executable  .jar  can be run locally if you want to run your application without having to start it up in dev mode. However, it is worth noting, in this era, with containerisation being a common principle when dealing with microservices – you may just want to run it in your favourite container platform instead. Additionally, if you did want to run the executable  .jar  inside a container you would also have to copy its dependencies from the  target/lib  folder. . Something to take into consideration, if you follow the common practice of today, images commonly have things baked into them that don’t really concern the running of the application e.g. Maven included in Java based images. This is all well and good but in efforts to reduce the size of the image and footprint on the system, what you ideally want is one executable that has everything it needs to run – kind of like a native executable. This avoids the need to copy  .jar  files into the image themselves with also removing the need to have a JVM. What? Did I just say you can run Java without installing a JVM into an image? I did, let’s see how. . As seen above, when you want to run your application, you would have to use Java in order to run the  .jar  files, or if you wanted to run the executable  .jar  you would have to also make sure it can access its dependencies as well as having a JVM installed and running. Running just the  .jar  is what companies previously did, currently still do and may even do for the foreseeable future, with or without containerisation. Copying the  .jar  file into a container and running it is still a followed method to this day by many – however, it requires an underlying JVM installed inside of the container (which normally comes with the java image chosen). What we want is to create an executable that has our application and everything it needs to be able to run – including the JVM. So, let’s produce a native executable for our application by running  native  profile specified in the project:  mvn package -Pnative  . Now in your target directory you will see another file called:  ./target/getting-started-1.0-SNAPSHOT-runner  . This is the native executable, it not only has an improved start-up time of the application whilst producing minimal disk footprint but included in this executable is everything the application needs to run – including the JVM, which has been shrunk to be just enough to run the application. The size of the native executable is around 19M whilst the default packaged  .jar  file is around 6.1k, however when you factor in all the things needed in order to run the default  .jar  file, the footprint in regards to resources quickly adds up to a number that eventually surpasses the executable. . Let’s run the executable by running the following and see what happens:  ./target/getting-started-1.0-SNAPSHOT-runner  . From the logs we can see that the application started instantly (0.007s), I don’t think my finger even got 5 cm away from the keyboard before it was already started. When compared to the default  .jar  being run via Java which took 0.506s to start, the speeds I must say, definitely favour the executable. . However, by default, the native executable is made specifically for your operating system (Linux, macOS, Windows etc). Due to the container not using the same executable format as the one produced by your operating system, we will instruct the Maven build to produce an executable from inside a container. When we run the following command, an executable will be produced that is a 64bit Linux executable so depending on your OS you may not be able to run it, however this is fine as we will be copying it into the Docker container. Run the following command to create the 64bit Linux executable:  mvn package -Pnative -Dnative-image.docker-build=true  . Just to note if we try and run this new executable (I’m on MacOS) we get the following error which as explain above is expected  zsh: exec format error: ./target/getting-started-1.0-SNAPSHOT-runner  . As explained above, now we have a Linux executable, what we now want is to bake this into an image which we can use to create our container. . Nowadays, companies that are taking on new development work or performing their routine infrastructure redesign would normally move towards a microservice architecture. Within the microservice world,  .jar  files themselves aren’t really used in the same way as before. Previously the application was packaged up into a  .jar  file (or .war file) and was executed on the host operating system – whether it be a developer’s system, CI server or a production server.  Nowadays, those same  .jar  files are instead baked into images containing a JVM which containerisation platforms would use in order to deploy the application. These same images are subsequently passed on through the pipeline to what would be its final destination; a container orchestration platform in a production environment. . As mentioned above, with Quarkus, we don’t need to worry about jar files anymore and instead we can create executables that includes the application and everything it needs in order to run. All we need is a native image in order to create our container and that’s it. So, enough talk, let’s create our native image. . Initially, the project generated at the start already has a Dockerfile that we can use to create our image. If we go to  src/main/docker  we can see  Dockerfile.native  that contains the following: . I’m not going to go through and explain what each line does as I am making an assumption that you’re somewhat familiar with Docker and Dockerfile’s but in summary, the image is based on a minimal Fedora distribution and it copies the executable we created earlier into the image and then runs it. Now following the traditional Docker image building process, we can use the  Dockerfile  above to generate our Docker image by running the following:  docker build -f src/main/docker/Dockerfile.native -t quarkus/getting-started .  . Now the image has been created on our machine at 125MB we can now run the image in a container using the following command:  docker run -i --rm -p 8080:8080 quarkus/getting-started  . Shown by the logs above, the container with the running application has started in 0.005 seconds. Compared to the average Java container start up, that is ridiculously fast as now we have a containerised Java application fully running and ready to handle requests. . With regard to footprint, I believe we have achieved very impressive metrics as the overall impact on resources have now been cut down by a large percentage due to the container itself only running a barebones minimum distribution alongside an executable that not only has the application and its dependencies but also the JVM. The size of the image, however, is 125MB and is still rather big, after all, it is Quarkus we are talking about. Let’s see if we can shave a few megabytes off the image size. . Although everything so far has been very impressive, the image size is still a bit too big. We can take this a step further by using distroless images that take even less space on the underlying system. Distroless images only contain the application and its runtime dependencies – no package managers, shells or any other programs found on a standard Linux distribution. . I won’t go through the ins and outs of creating distroless images as it is a subject in itself, however, to create a distroless image in our case we can replace the contents of the  Dockerfile.native  with the following: . This uses a technique called multi-stage building and so when we run the following command again to build the image based on the updated Dockerfile:  docker build -f src/main/docker/Dockerfile.native -t quarkus/getting-started .  . We now get an updated image created on our system and this time the size is 36.9MB – almost 100MB smaller. Now what happens if we run it? . Looking at the logs above, although there’s no real improvement in start-up time, by using a distroless image we managed to shave off almost 100MB from the image size - resulting in less disk space being used. Not too bad when you consider the fact that the  openjdk:8-jre-alpine  image is 84MB in size and also happens to be one of the most commonly used Java images today. . Additionally, we already know the container starts up almost instantly, but it is also worth noting that its memory usage is 1.848MiB. To give some context, a standard containerised SpringBoot application with a standard packaged jar file using the  openjdk:8-jre-alpine  image runs at around 350MiB. . Realistically, in the world of containerisation and container orchestration, the image is the artifact that is not only used to run the application but to create the container. As shown above, the image created by Quarkus not only starts at ridiculous speeds but overall has less of a footprint on the system. Although the image size can be reduced even further by using distroless images, this is a step further in the direction of footprint reduction that the majority of people still do not do. However, combined with a native executable as shown above, the numbers are astounding. That being said, what we have shown in this article is the frontier of the Java world in regard to creating, running, and deploying applications via containerisation. Like most things in this industry it will take companies a few years before they evolve to a point where these things are adopted – which may not be a bad thing for them business wise as all the creases and holes may have been ironed out and filled by that time – as after all, if you are on the front line, you are most likely to get shot and hurt ;). . As this was just the unearthing of Quarkus as a whole, there are many different capabilities it has and continually keeps adding and if you want more of an idea and guide on what these could be head over to the  Quarkus  site. . With more current generation developers and companies choosing serverless frameworks for their applications, speed and footprint is one thing that has to be considered. Quarkus definitely provides an efficient solution for running Java in this new world as it was designed with these in mind. In fact, one of the unique selling points for Quarkus - and in fact running Java applications natively – is the extremely short start-up time. As shown above, everything operates in a matter of milliseconds, from building, starting and deploying to hot deployments, you’re never sat waiting for something to happen, everything is instantaneous. This in itself can be seen as a game changer, especially nowadays since companies are starting to up their expectations of wanting to be able to setup and tear down applications and services in the blink of an eye, all whilst having a minimal footprint on the underlying systems. . Still though, throughout its history and still to this day, one of the biggest limitations in the Java world is performance as the JVM needs a huge amount of time and resources to start up. In its defence, its runtime has mostly been optimized for throughput in long-running processes but with current companies demanding that applications should aim to start-up almost instantly it’s simply not enough to start a JVM in the normal way anymore. This is somewhat solved with the creation and use of images and containers as it removes the need of having to start up full JVMs therefore resulting in less resource strain on the underlying systems. . Lastly, I have to agree with Sebastian Daschner from DZone / JavaZone  when he says  that whilst all this is interesting, we shouldn’t just forget that most enterprises are still running, and probably will continue running, their workload for a longer period of time with no plans to change anytime soon. However, as he describes, the approach of getting rid of most of the “dynamics” at runtime also has a positive impact on the overall resource consumption and is certainly promising and a step in the right direction. ", "date": "2019-05-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "There isn't a module for that already?\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/module-already/", "abstract": " Sometimes clients ask for the wrong thing. Sometimes developers build the wrong thing, because they didn’t ask the right questions. If you’re solving the wrong problem, it doesn’t matter how elegant your solution is. . One of the most important services that we as developers and consultants can provide is being able to help guide our clients to what they need, rather than simply giving them what they want. Sometimes those two things are aligned, but more often than not, figuring out the right thing to build takes some discovering. . Why don’t wants and needs match? It might be because the client hasn’t spent enough time thinking about the question, or because they haven’t approached it from the right angle. If that’s the case, we can help them to do that, either by asking the right questions or by acting as their  rubber duck , providing a sounding board for their ideas. Alternatively, it might be because, as a marketing or content specialist, they lack sufficient awareness of the potential technological solutions to the question, and we can offer that. . Once you’ve properly understood the problem, you can start to look for a solution. In this article, I’ll talk about some examples of problems like this that we’ve recently helped clients to solve, and how those solutions led us to contribute two new Drupal modules. . Sometimes the problems are specific to the client, and the solutions need to be bespoke. Other times the problems are more general, and there’s already a solution. One of the great things about open source is that somebody out there has probably faced the same problem before, and if you’re lucky, they’ve shared their solution. . In general, I’d prefer to avoid writing custom code, for the same  reasons that we aren’t rolling our own CMS . There are currently  over 43,000 contributed modules available for Drupal , some of which solve similar problems, so sometimes the difficult part is deciding which of the alternatives to choose. . Sometimes there isn’t already a solution, or the solution isn’t quite right for your needs. Whenever that’s the case, and the problem is a generic one,  we aim to open source  the solutions that we build. Sometimes it’s surprising that there isn’t already a module available. Recently on my current project we came across two problems that felt like they should have been solved a long time ago, very generic issues for people editing content for the web - exactly the sort of thing that you’d expect someone in the Drupal community to have already built. . One area that sometimes causes friction between clients and vendors is around  estimates . Unless you understand the underlying technology, it isn’t always obvious why some things are easy and others are hard. .    XKCD -tasks  . Even experienced developers sometimes fail to grasp this - here’s a recent example where I did exactly that. . We’re building a site in Drupal 8, making heavy use of the  Paragraphs module . When adding a webform to a paragraph field, there’s a select list with all forms on the site, sorted alphabetically. To improve usability for the content editors, the client was asking for the list to be sorted by date, most recently created first. Still thinking in Drupal 6 and 7 mode, I thought it would be easy. Use a view for selection, order the view by date created, job done - probably no more than half an hour’s work. Except that in Drupal 8, webforms are no longer nodes - they’re configuration entities, so there is no creation date to order by. What I’d assumed would be trivial would in fact require major custom development, the cost of which wouldn’t be justified by the business value of the feature. But there’s almost always another way to do things, which won’t be as time-consuming, and while it might not be what the client asked for, it’s often close enough for what they need. . In the example above, what the content editors really wanted was an easy way to find the relevant piece of content. The creation date seemed like the most obvious way to find it. If you jump to a solution before considering the problem, you can waste it going down blind alleys. I spent a while digging around in the code and the database before I realised sorting the list wouldn’t be feasible. By enabling the  Chosen module , we made the list searchable - not what the client had asked for, but it gave them what they needed, and provided a more general solution to help with other long select lists. As is so often the case, it was five minutes of development work, once I’d spent hours going down a blind alley. . This is a really good example of why it’s so important to validate your assumptions before committing to anything, and why we should  value customer collaboration over contract negotiation  - for developers and end users to be able to have open conversations is enormously valuable to a smooth relationship, and it enables the team to deliver a more usable system. . One area where junior developers sometimes struggle is in gauging the appropriate level of specificity to use in solving a problem.  Appropriate specificity  is particularly relevant when working with CSS, but also in terms of development work more generally. Should we be building something bespoke to solve this particular problem, or should we be thinking about it as one instance of a more generic problem? As I mentioned earlier, unless your problem is specific to your client’s business, somebody has probably already solved it. . With a little careful thought, a problem that originally seemed specific may actually be general. For example, try to avoid building CMS components for one-off pieces of a design. If we make our CMS components more flexible, it makes the system more useful for content editors, and may even mean that the next requirement can be addressed without any extra development effort. . Sometimes there can be a sense that requirements are immutable, handed down from on high, carved into stone tablets. Because a client has asked for something, it becomes a commandment, rather than an item on a wish list. Requirements should always be questioned The further the distance between clients and developers, the harder it can be to ask questions. Distance isn’t necessarily geographical - with good remote collaboration, and open lines of communication, developers in different time zones can build a healthy client relationship. Building that relationship enables developers to ask more questions and find out what the client really needs, and it also helps them to be able to push back and say no. . It can be tempting to imagine that the digital is infinitely malleable; that because we’re working with the virtual, anything is possible. When clients ask “can we do X?, I usually answer that it’s possible, but the more relevant question is whether it’s feasible. . Just as  the web has a grain , most technologies have a certain way of working, and it’s better to work with your framework rather than against it. Developers, designers and clients should  work together to understand what’s simple and what’s complicated  within the constraints. Is the extra complexity worth it, or would it be better to simplify things and deliver value quicker? . Sometimes that can feel like good cop, bad cop, where the designers offer the world, and  developers say no . But the point isn’t that I don’t want to do the work, or that I want to charge clients more money. It’s that I would much rather deliver quick wins by using existing solutions, rather than having developers spend time on tasks that don’t bring business value, like banging their heads against the wall trying to bend a framework to match a “requirement” that nobody actually needs. It’s better for everyone if developers are able to work on more interesting things. . As an example of an issue where a little technical knowledge went a long way, we were looking at enabling client-side sorting of tables. Sometimes those tables would include dates. We found  an appropriate module , and helped to get the Drupal 8 version working, but date formats can be tricky. What is readable to a human in one cultural context isn’t necessarily easy for another, or for a computer, so it’s useful to add some semantic markup to provide  the relevant machine-readable data . . Drupal has pretty good facilities for managing date and time formats, so surely there must be a module already that allows editors to insert dates into body text? Apparently not, so I built  CKEditor Datetime . . With some helpful tips from the community on  Drupal Slack , I found some CKEditor examples, and then started plumbing it in to Drupal. Once I’d got that side of things sorted, I got  some help from the plugin maintainer  to get the actual sorting sorted. A really nice example of open source communities in action. . Another challenge that was troubling our client’s content team was knowing what their images would look like when they’re rendered. Drupal helpfully generates image derivatives at different sizes, but when the different styles have different aspect ratios, it’s important to be able to see what an image will look like in different contexts. This is especially important if you’re using responsive images, where the same source image might be presented at multiple sizes depending on the size of the browser window. . To help content editors preview the different versions of an image, we built the  Image Styles Display  module. It alters the media entity page to show a preview of every image style in the site, along with a summary of the effects of that image style. If there are a lot of image styles, that might be overwhelming, and if the aspect ratio is the same as the original, there isn’t much value in seeing the preview, so each preview is collapsible, using the  summary/details element , and a configuration form controls which styles are expanded by default. A fairly simple idea, and a fairly simple module to build, so I was surprised that it didn’t already exist. . I hope that these modules will be useful for you in your projects - please give them a try: .  CKEditor Datetime  .  Image Styles Display  . If you have any suggestions for improvement, please let me know using the issue queues. ", "date": "2019-05-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Devoxx 2019 Review\n  ", "author": ["Matt Smith"], "link": "https://capgemini.github.io/learning/Devoxx-2019-Review/", "abstract": " Another Devoxx has come and gone and it’s time to reflect on what was seen and loved!  There were loads of amazing talks this year: Some new technologies, friendly faces and some wildly futuristic technology that makes it seem like we’re living in a cool Cyberpunk world - though, not a Blade Runner-type one. .     . Now this is what living in the future feels like! Nick guided us through his journey of using  DNA  (yes, those double-helix strands that are in our cells) to store data.  Unfortunately, we’re not talking about superfast SSD-like speeds, but this  could  be a long-term storage solution that the world is starting to really need. . Before seeing this talk, I was taking  Graal  to be this weird native-Java thing that Oracle was pushing, possibly to gain PR points or just make money. HOWEVER, after coming away from this talk, Graal is now near the top of my list on things that I wanted to go implement in my Java projects. The sheer speed and performance improvements displayed here were beyond cool! . Quarkus was something on the edge of my tech perception (though certainly not the case for my colleague Chris Burns who wrote a  fantastic article on this very blog  a couple of weeks ago!).  Watch this talk to hear about a very cool piece of tech that is going to help make your applications and containers function at hyper-speed: . Our very own Capgemini engineers, Joao &amp; Clive, gave a really interesting talk about a Hackathon a few members of the Open Source Cloud Engineering (OSCE) team, here at Capgemini, did a little while ago.  In this, Joao and Clive dive deep into how they leveraged Platform.IO, AWS IoT, Serverless, Graal, Pulumi and a handful of ESP-32 micro-controllers to create a greenhouse full of technology that also sits inline with Capgemini’s values on  Sustainability &amp; Corporate Social Responsibility . . (You can even see me arrive late @ 13:42 ). . OH BOY, did I love this talk. Kevin’s enthusiasm around this bit of technology is joyous to watch (and the tech itself is darn cool too!).   Testcontainers  is basically a library that you can import into your project so that you can easily use Docker containers in your integration tests. That’s right, no more Mocks, Stubs, In-memory Databases that you have to keep stable and fight with - instead, just write a little code that boots the software you need into a container, before you run your tests.  I’ve never been more excited to write tests! . Andrew is one of  our alumni , and it’s awesome to see him continue to thrive. Here he talks about some of the lessons he learned when trying to do, what he dubs as, “Organisation Refactoring”.  It’s super interesting to watch, and definitely gave everyone who watched it something to take away: . Sarah is another one of Capgemini’s finest and gave this great talk on her experimentations with Alexa and AI for Sentiment-Analysis to create a personality test, based on the  OEJTS  - a community version of the Myers-Briggs personality test.  Look out for me @ 14:00  as I volunteer to actually take a short test! . I went along to this talk with my colleague  Clive , who recommended that I go witness the amazing Ted Neward give a talk.  I didn’t really need much persuasion as Go is a language I’ve been meaning to get my hands dirty with for a long time but I’ve been too busy to do so.  Well, this  was  the perfect talk for me as Ted does a great job of breaking down the complexities and subtleties of Go, even for  a JavaScript developer like me  😂. . I didn’t actually go to this one in-person, but my colleague Sam Taylor did and thought it was excellent.  As someone who is very fond of ripping out the buzzwords and toxic jargon from our development practices, I have to say: This talk is  really  quite good. . This one is an easy sell: If you’re an Intellij user and you want to be a super-speedy developer who knows all the cool tricks to getting the most out of your favourite Jetbrains IDE, you HAVE to watch this talk. Trisha is a great speaker and just gives more evidence as to why Jetbrains are doing some really cool things. . Inspired by the film  Se7en , Luis Weir (ANOTHER Capgemini Alumni) gives a really interesting and insightful talk into what you really need to avoid doing when designing an API. A must watch for any of you budding Architects out there! . How could I not mention our booth?  We were a tremendous success this year and garnered a lot of attention (not just from our cool t-shirts, beer and ice cream machine!). . As featured  above in Clive &amp; Joao’s talk about using Serverless to improve your Horticulture , we also had our IoT Greenhouse which a few colleagues of mine in the Open Source Cloud Engineering (OSCE) Team built at a Hackathon. It was great to see people interacting with the IoT devices, such as applying heat to the heat sensor to watch the Greenhouse window be opened by one of the actuators. .   . We even had some senior Capgemini folks turn up in the form of  Martin Scott  (Director of our Custom Software Development (CSD) business unit), on Thursday, and  Paul Margetts  (UK Managing Director), on Friday.  We had some great chats with them and they got to see the real impact that Capgemini is having with the engineering community - something we only want to further improve upon! .   . So thanks to everyone who came to our booth to meet us, and special thanks to everyone on the team who turned up and made great (eco-friendly) footprint at Devoxx 2019. .     . I won a drone! :3 .   ", "date": "2019-05-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Fast and Free AWS Web Deployment Tutorial\n  ", "author": ["Pip Turner"], "link": "https://capgemini.github.io/learning/fast-and-free-aws-web-deployment-tutorial/", "abstract": " I wanted to learn how to use Amazon Web Services (AWS) so I worked through a few online courses and decided that it was time to get practical. However, I was wary of getting charged accidentally while using AWS services. . To help others who are looking to do the same I thought it would be nice to have a simple guide describing how to do something in AWS that will show a bit of its free functionality and should take no more than an hour. . Using  AWS Elastic Beanstalk  you are going to deploy a simple web application that you will be able to access over the internet. Specifically, this is a Spring MVC Java application  that allows users to create an account, login, logout and displays a random quote on the homepage once a user is logged in. .   . This application uses a MySql database to store user credentials and a set of quotes. The schema of the database is shown below. You are going to setup this MySql database on AWS and build the database using MySql Workbench. .   .       Sql Workbench - This can be downloaded from here:  Download MySql Workbench      . Sql Workbench - This can be downloaded from here:  Download MySql Workbench  .       A free tier AWS account - You can sign up for one here:  AWS Free Tier Account      . A free tier AWS account - You can sign up for one here:  AWS Free Tier Account  .       An application that will allow you to unpack a  .zip  file. There are a number of free applications available that will do this such as  7-Zip      . An application that will allow you to unpack a  .zip  file. There are a number of free applications available that will do this such as  7-Zip  .       Optionally: An integrated development environment of your choice to inspect the web application code (I used  IntelliJ ).     . Optionally: An integrated development environment of your choice to inspect the web application code (I used  IntelliJ ). . I have created a  GitHub Repository  with all the things you will need. . Download this as a  .zip  and unpack it to a sensible location on your computer. . In these folders is all the code required to alter the web application should you choose to do so. . There are two folders that you will definitely need. The  ReadyToDeployWar  folder and the  DbSetup  folder. . I have packaged the web application in a  .war  file in the  ReadyToDeployWar  folder. . I have also saved the SQL database creation scripts that you will need to build the database in the  DbSetup  folder. . Once you have an AWS free tier account navigate to  https://aws.amazon.com  and click on the “Sign in to the console” button in the top right corner. If this button says “Create an account” instead, don’t worry, just click on “My Account” and then “AWS Management Console” in the dropdown menu. .   . Enter your login credentials and you will be redirected to the management console. Type in Elastic Beanstalk in the “Find Services” search bar and select the option that comes up. .   . This will open the AWS Elastic Beanstalk welcome page. Click on the “Get started” button on this page. .   .  Elastic Beanstalk  is an Amazon service that will setup an entire environment for you in the Amazon cloud, based on your own configuration. The key components we will be concerned with are the EC2 instance, the database and the security groups. The EC2 instance is the computing capacity where your app will actually be deployed. We will be setting up a Tomcat EC2 instance. We will also be setting up a MySql database and configuring security groups to allow your computer to connect to that database. . You will be directed to the “Create a web app” page where we are going to have to start adding some configuration details. First add in the application name, here I have chosen “SpringMvcOnAws”. Don’t worry about filling in the application tags section underneath as this is optional. Next choose the “Tomcat” platform from the drop down box. We are going to upload our  .war  file with our packaged web application so select the “Upload your code” choice and click on the “Upload” button. .   . Select the “Local file” option and click on the “Choose File” button. You will then have to locate the  SpringMvcOnAws.war  on your own computer. This will be in the  ReadyToDeployWar  folder wherever you unpacked the zip file mentioned in “the web application code” section above. Select that file and give it a version label. I chose to call it “springmvconaws-1.0.0”. Then click on the “Upload” button. .   . This will return you to the “Create a web app” page and you may be tempted to click on the “Create application” button but don’t as we still need to add a little more configuration, specifically for the database. Instead click on the “Configure more options button”. .   . This will take you to the Configure page. Now scroll down and click on “Modify” under the database section. .   . This should open up a “Modify database” page where we will need to add additional configuration. Most of the fields should be filled in for you as it automatically sets it to the free tier database configuration. Double check this by comparing the values with the image below. . You will need to add a Username and Password for your database. I chose “Administrator” and a unique, secure password for mine but I recommend you choose your own. Make it something memorable as you will need these later. Also change the Retention to “Delete” using the dropdown as this will make sure you don’t retain anything when you later want to destroy the environment. Once this is all done, click on the “Save” button. .   . You will be directed back to the configuration page. You will notice that in the Configuration presets at the top it has gone from “Low cost (Free Tier eligible)” to “Custom configuration”. Now this made me think I was going to be charged but don’t worry I wasn’t. . This is optional, so feel free to skip this step, but, it is worth noting that if you actually want to be able to access the Tomcat EC2 instance where your application will be deployed, you will have to do some additional configuration. Specifically you will need to click on the “Modify” button in the security section and you will need to add a key pair. You will first need to set up that key pair. To do this follow this  Amazon guide to key pairs . .   . We are now ready for Elastic Beanstalk to set up our environment for us, so just click the “Create app” button at the bottom of the configuration page. .   . Elastic Beanstalk will now begin setting up your environment. .   . This may take some time (approx 10 to 15 minutes) so go for a coffee and hopefully it will be ready to go when you come back. . Once the environment is setup you will be redirected to the environment dashboard. We are going to need to do some additional configuration so we can access and set up the database. Click on “Configuration” on the left hand side to go back to the environment configuration page. .   . Scroll down to the database section again and click on the link in that section. .   . This will take you to a page that shows the database instances setup under your account. There should only be one database here so click on the ID link of that database. .   . An overview page will be displayed listing all the details of that database instance. Note the endpoint and port details stated on the left. We will need these later to connect to the database, so make a note of them. . Look at the security group rules section at the bottom. We are going to need to change the inbound rule to allow your computer to connect to the database. Select the link in this section that relates to the security group with inbound listed on the right. .   . This will take you to a security groups overview page. You should have one security group listed. Select the inbound tab at the bottom and click on the “Edit” button as we need to add a rule. .   . You should now see an inbound rules window. Click on add rule and select “MySql/Aurora” in the type drop down. This should automatically put the protocol as TCP and the port range as 3306. In the Source section you then need to select “My IP” in the drop down menu. . This allows your computer (or the computer at that IP) to connect to the database via TCP at port 3306. You can choose to add a description or you can go ahead and just click on “Save”. .   . We should now be able to connect to that database so open up MySql workbench on your computer. Click on the database menu and select “Connect to a database”. .   . Now do you remember those database connection details we saw earlier? We are going to need to add them in here. Add the hostname and port that you saw on your database details page and then add the username and password that you used to configure the database at the start. Once complete click on the “OK” button. .   . You should see your AWS database open up in MySql Workbench. We now need to add a schema to this database so that the web application can store its user credentials and quotes somewhere. This is where you are going to need the sql script from the zip file you unpacked. You should find this SpringDemoDbCreate.sql file in the  DbSetup  folder. Open this file in text editor or IDE of your choice and copy all of the contents. .   . Go back to MySql Workbench and copy it into the Sql file section on the right. Make sure you have nothing highlighted in that section then click the run sql script button. You should see the script running as it produces output in the action output window. Once that has finished click on the refresh button next to the schemas section and you should see the springdemodb schema appear. Feel free to explore the tables in this schema if you want to. .   . Your database is now set up and ready to use. . Congratulations! you now have our web app running and the database setup. . Problem is your web application doesn’t currently know where your new database is. So we need to set up some properties to tell it where to look and how to connect. . Let’s go back to the configuration page for your environment. You are now going to click on the “Modify” button in the Software section. .   . At the bottom of this page you will see the environment properties section. I have set up the web application to look for specific properties that you can add here. .   . Add the following environment properties in. You will need to insert your own database url, username and password in. . Then click the “Apply” button. .   . The environment will now update. You will be redirected to the environment dashboard where you will see the message at the top saying it is updating. .   . Once the environment is ready you will see a green tick on the environment dashboard. This means you are ready to go. . Click on the link at the top of the page that will take you to your publicly accessible web application. .   . This should open up a login page for the web application. You will need to create an account to get into the homepage so click on the “Create an account” link. .   . I recommend using fake credentials here as the web application is not secure. The username has to be in email format so put in “name@domain.com” and the password has to be over 8 characters so let’s just put in “password” in both the password and confirm password section. Click on the “Submit” button when you are ready. .   . Bingo! You should now see the homepage with the random quote displayed. .   . It should have also stored your credentials in the database so you should be able to log out and log back in again using those credentials. . You could try and expand on the web application and then upload the new version to AWS. This is very easy to do in AWS. Simply go back to the environment dashboard and select the “Upload and Deploy” button in the middle of the page. Find your new  .war , give it a version name and click “Deploy”. . You can also tear down the entire entire environment. To do this go back to the environment dashboard and click on the “Actions” button  and click on “Terminate Environment”. . Alternatively you could try and deploy the same application using Microsoft Azure and make a blog post about it. . Good luck and happy developing! ", "date": "2019-06-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The top three challenges in securing Public Sector digital services\n", "author": ["Dan Harrison"], "link": "https://capgemini.github.io/devsecops/Digital-Services-Public-Sector-DevSecOps/", "abstract": " When we talk to our clients, it’s often understandable that they are worried about the fact that, somehow, their ability to secure digital products and services cannot keep pace with the speed at which they are built. In a  previous blog post , we discussed how new DevOps tools and methodologies accelerate the creation of new features and updates to applications. . However, traditional approaches to security can no longer keep pace due to a variety of challenges, resulting in increased exposure to cyber risk and a decrease in the speed of delivery. The public sector faces a unique set of challenges due to its organisational setup and the nature of its work. . Recently, I had the opportunity to speak with a senior security expert working in the UK public sector, in order to understand which security challenges, in his opinion, top the list when it comes to delivering new digital services for government - here’s a summary of our discussion. . According to him, the digital transformation of the public sector based on government’s  “Digital by Default”  strategy aims to boost public services such as healthcare, pensions, universal credit, and law enforcement with digital technologies to make them more accessible and operationally efficient. . The problem he focused on was that such digital services are also high-value targets to a variety of would-be attackers for two main reasons: . Some of the services are considered critical national infrastructure. If they were not available for any period of time, there would be a high chance of social unrest making them ideal targets for activists and state-sponsored hackers. . The volume of sensitive citizen data managed by the services could be exploited for financial gains by organised criminal gangs, disgruntled civil servants, and connected third parties. . The second big challenge is that siloed security functions do not integrate well with more digital functions. . Furthermore, security functions with an old school approach to working are still the norm and digital departments can find it difficult to collaborate. For example, the security function mandates heavy documentation and vetting processes which often fly in the face of the agile principle of “working software over comprehensive documentation”. . Another challenge is that some security managers are not familiar with the latest technology and methodologies, which makes it difficult for them to assess risks and make recommendations. . Public sector organisations are still not placing big bets on cloud-based delivery models, despite having plenty of options available from tech giants such as Amazon, Google and Microsoft. . For DevOps, the cloud is at its core, as it enables scaling the infrastructure up or down in a matter of seconds, as and when needed. However, according to our security expert, the security function in a typical public sector organisation operates a list of approved services and tech stack elements that restrict DevOps teams from using some cloud-based technologies. . This practice, he says, gives the department control to reduce exposure to security risks. However, the challenge is that such a list is not updated frequently and is sometimes updated by the people who don’t understand the technology. As a result, there is a reduction in agility because teams struggle to deliver iterative improvements quickly across the stack. . Technology is only part of the solution though. It is clear from interviewing our security expert that many challenges arise from cultural differences and a lack of education. With this in mind, organisations should focus on the following core principles to fully achieve a DevSecOps approach to security: . Educate your workforce . Automate your processes . Monitor your applications and security level progress . You can find out how secure your DevOps is by filling out our  online assessment . It’s free, anonymous, and allows you to benchmark your maturity against other organisations and industries. ", "date": "2019-07-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Lead Developer London 2019 Roundup\n", "author": ["Nikki Algar"], "link": "https://capgemini.github.io/learning/Lead-Dev-London-Conference-2019/", "abstract": " Feeling very fortunate and full of expectation, I attended the London Lead Developer 2019 conference which was held at the iconic Barbican Centre in June. This event is focussed on technical leadership and it was my first time attending. It turned out to be the biggest Lead Developer conference to date as there were almost 1400 attendees and 28 speakers across the two days. .   . The organisers of the event clearly made a huge effort to take care of the details and ensure a smoothly running, truly inclusive and diverse event. On registration, attendees were presented with a choice of lanyard: a standard one; a yellow one if they preferred not to appear in any photos; and a rainbow lanyard which neatly coincided with the event being held during Pride month! Everybody had the option to personalise their badge holders with stickers to indicate their communication and pronoun preferences. . To further cater for individual needs, there was a creche, a prayer room, a quiet room and an alcohol-free lounge. Not to mention the well-labelled vegetarian, vegan and gluten-free food options during meal and snack times. All the talks and commentary were enhanced by live captioning appearing on large screens to each side of the stage. . As a sponsor, Capgemini had a stand and we were giving away  #RootedInEngineering  T-shirts and ice creams. Both of these proved extremely popular throughout the two day event. The backdrop of our stand was emblazoned with  Capgemini’s Corporate Social Responsibility highlights  and we drew interest with the greenhouse  IoT  demo (built during a recent hackathon). Our stall was rather eye-catching with its green and leafy motifs – some visitors commented that it’s not usually a colour or theme associated with a digital consultancy and this proved to be a talking point and an opportunity to discuss our sustainability values and how we ‘grow developers’. .     . Being a single track conference, all the talks took place in the main auditorium - I liked this because I knew I wouldn’t miss any! The talks varied in length from between 10 and 30 minutes each, presented by a host of high calibre international speakers. I was very encouraged that most of the speakers were women. In between the back to back talks, there were some breaks and a chance to refuel or browse the sponsor stands. Speaking to a few other attendees, I discovered that many had travelled from across Europe to attend. . Some common themes that ran through many of the talks were: . AI . Inclusive hiring . Building and developing effectively performing teams at speed and at scale . The importance of team health and personal wellbeing. . In my opinion, the vast majority of the talks and supporting slides were excellent and certainly measured up to my high expectations of this event. I would find it too hard to pick an absolute favourite and will watch most of them again to soak up the incredible insights and advice they offer. If you’d like to do the same, the full line up is available to view on this  Youtube Playlist of all the Lead Developer 2019 Talks.  . And finally, some of the book recommendations I noted were: .   Drive : The Surprising Truth About What Motivates Us  ( by Daniel H. Pink ) . Most of us believe that the best way to motivate ourselves and others is with external rewards like money—the carrot-and-stick approach. That’s a mistake, Daniel H. Pink says in, Drive: The Surprising Truth About What Motivates Us, his provocative and persuasive new book. The secret to high performance and satisfaction—at work, at school, and at home—is the deeply human need to direct our own lives, to learn and create new things, and to do better by ourselves and our world. .   Mindset : How You Can Fulfil Your Potential  ( by Carol Dweck ) . In a growth mindset, people believe that their most basic abilities can be developed through dedication and hard work—brains and talent are just the starting point. This view creates a love of learning and a resilience that is essential for great accomplishment. Virtually all great people have had these qualities. .  Making Work Visible : Exposing Time Theft to Optimize Work &amp; Flow ( by Dominica DeGrandis ) .  Accelerate : The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations ( by Nicole Forsgren, Jez Humble and Gene Kim ) ", "date": "2019-07-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Rematch: Redux Without the Bloat\n", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/development/rematch-redux-without-the-bloat/", "abstract": " State management. Whilst being a critical aspect of writing frontend applications, it’s an ever-changing landscape. You might be reading this and thinking  what even is state, and why do I need it?  Fundamentally, at a high level, state management is exactly what it says on the tin: a way of managing state. In reference to writing apps, it’s about how you manage your application state. To this effect, I want to look at a particular library for managing state that I’ve come across recently that has made writing React apps with state fun again:  Rematch . .  Redux  is great. I’ve been using Redux for a number of years and its clean, simple API has meant state management in my React apps has been nicely abstracted. However, the more apps I build, the more I find myself going through the same rigorous setup process before I can get started; create and setup my store, create and setup my root reducer, loosely build my actions and a separate file for my action types. I even built a  Yeoman generator  to scaffold a project with create-react-app and Redux to save me time and effort. . It’s important to point out, you  might not even need Redux , or any state management library for that matter -  React can handle state management for you . . Following best practices, when using Redux with React, often you’ll end up with a directory structure which looks something like this, for example: . As for the code, the following examples show what each of the core JavaScript files look like, when writing a React app with Redux: .  store.js  .  rootReducer.js  .  index.js  . Whilst the above doesn’t look like much - and you’d be right in thinking that - it quickly grows as you start to write your reducers and actions. .  todoActions.js  .  todoReducer.js  . And then connecting these up in your container: . The above is a  very  simple example of a todo app, which for now just adds a todo. I haven’t been overly verbose in explaining the intricacies of how Redux is setup and bound to React, but more information can be found  here in the Redux docs , and for  React specifically . . The point is that whilst Redux is a brilliant framework for managing application state, it’s begun to feel a bit bloated with regards to the amount of boilerplate code you end up writing and maintaining, if it wasn’t for  Redux devtools , it might be difficult to see how everything lines up and works together, once your app grows significantly, depending on how you’ve built it. . As I mentioned above, Redux  is  a brilliant framework for managing state. It would just be nice to have a library which gives you all the best practices of Redux, without the boilerplate. That’s where  Rematch  comes in. It’s effectively a wrapper around Redux, giving you all the Redux goodness, without the rigorous setup process. Let’s take the todo example above and look at it implemented in Rematch. .  index.js  . Making a call to Rematch’s  init()  function, initializes you a Redux store under the hood, and it gives you all the freedom to  add as many different plugins as you need . You don’t need to write an explicit store file which pulls together your enhancers, middleware, devtools and reducers. .  models.js  . The  models.js  file is central to the whole state management process, in relation to Redux, it holds: state, reducers, async actions and action creators. Being able to declare a single function which handles all of the above is really refreshing, you don’t need to create separate files for reducers, actions and action types - instead you can declare your code models and let Rematch handle the rest. You also don’t need to keep writing switch statements for controlling the flow of your action types, instead you just write nice clean functions. . Hooking it up to containers: .  TodoContainer.js  . At first glance it’s not too dissimilar from the standard Redux example. However, at a closer look, there are several differences; no need to import or use  bindActionCreators , no explicit imports or usage of actions, everything is just handled and wired up for you by Rematch. There’s less going on, less to worry about and more importantly, as your applications grow, less to keep a hold on and less complexity which could get out of hand. . Until you start using a new framework in anger, it can be difficult to give it a proper evaluation just by reading the docs (or indeed this blog post). However, if you are a fan of Redux, you don’t use any state management framework currently and are looking at choosing one, or you just want to play around with an awesome framework, I’d highly encourage you to try Rematch out. I’ve only really been using it for small apps so far to see how it compares overall with Redux, and I feel I’m much more productive, writing less boilerplate and still getting solid apps as a result of using Rematch instead of plain Redux. On the other hand, there’s still more I’ve got to learn about the framework; understanding the test support - in theory it should be as easy as testing Redux apps - is really important from both a testing perspective and developer experience. There’s also the question of scalability, and how easy it is to handle the models as your application grows significantly, and whether  effects  - which use core JavaScript language features - are better than  thunks . . Furthermore, another question in my mind, which will be answered through more usage of Rematch, is its performance.  Redux’s performance  has been tweaked over time, and generally scales pretty well as the number of reducers and actions increase. However, when you start to write more and more  thunks , you increase the amount of library code being invoked to perform asynchronous actions. Rematch on the other hand, uses  effects  to handle async actions, which use the ES7  async/await syntax , which is just baked in JavaScript functionality, and has no reliance on another framework - which itself is a big positive. . It’s still early days, and I’ve not used Rematch enough to find any significant downsides to the framework as of yet. So far though, I really like the framework, and because it’s just a nice, simple wrapper around Redux, you know it’s got a solid base, and has been built on tried and tested foundations. ", "date": "2019-07-19T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Introduction to Serverless Computing on AWS\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/development/Introduction-to-Serverless-Computing-on-AWS/", "abstract": " Back in November 2014, AWS completely changed the paradigm of modern Cloud computing by releasing a service called  AWS Lambda . Consequently, this changed the future product road map offerings for a lot of technology companies around the world that have since followed in Amazon’s footsteps  by releasing their own Serverless technologies  into the ecosystem. Moreover, leading companies like Microsoft and Google – who both join Amazon in being the top three Cloud providers today – have attempted to chase Amazon’s spot in being the leading Serverless platform provider but it seems that due to an already large developer community and plethora of use cases, AWS still dominate the Serverless landscape. . As this article will be more centred around using AWS Serverless technologies, I won’t be going into detail about what  Serverless  is. However, in a nutshell, Serverless is an execution model where Cloud providers allow you to dynamically use their resources in order to run your code – with the key point being that you are only billed for what resources are used in order to complete an executable run of your code. Although they are used interchangeably, it is important to point out that Serverless and  FaaS  are  not the same thing . The difference being that, Serverless is more of a system approach whereas FaaS is more in line with running event (often ephemeral) driven code statelessly on a third party vendor platform (AWS Lambda). Unlike with Virtual Machines, where the application is continuously running waiting for requests, with Serverless, nothing is running until the code is triggered, and when the code is triggered, however long it took to finish its execution, you are only billed for its duration. In AWS you are only  billed  when your Lambda functions are invoked and they are billed both per request and duration taken to complete request. Amounts include examples such as $0.0000002 per request and as low as $0.000000208 per 100ms. This is attractive to many companies as it can reduce costs significantly whilst also drastically cutting the extent of traditional infrastructure setup and configuration. This therefore minimises the amount of moving parts that an organisation has to keep track of as AWS guarantees that all this will be handled by them with high rates of availability within their  SLAs  of each service. Additionally, due to the cut in the amount of moving parts, this can decrease the time to production which appeals to many organisations for the obvious reasons. . Traditionally, to do Serverless work on AWS, there has always been the option of being able to go into the console (AWS Website UI) and set up what you need. For example, in the console you could manually setup the Lambda function with a  DynamoDB  backend and expose the Lambda with an  API Gateway  so it is accessible externally. However, in this new world where  Infrastructure as Code (IaC)  is the preferred approach when setting up and configuring Cloud infrastructure, you also want to keep this approach with Serverless infrastructure. . This is where  AWS CloudFormation (CF)  came in. CF is AWS’ own IaC offering that allows you to set up AWS resources in the Cloud via either YAML or JSON. CF is available at no extra cost and you only pay for the resources you create – which is dependent on the pricing of those resources. To create a Lambda Function in AWS using CF we could follow the AWS Documentation online that details how to not only set them up but we could also set up a Lambda Function that can talk to a DynamoDB with an API Gateway in front of it that altogether allows a public user to call the Serverless function which retrieves information from the database whilst outputting its response. We can do all that in CF – however, it isn’t an easy task to an individual fresh to CF and although CF is just YAML or JSON (depends on user preference), the actual discovering and understanding of the common resources types within CF can be quite daunting initially. This is where Serverless comes in. . No, you are not reading that wrong, those are the same words you saw above, however, we are now asking a different question – the difference being the word ‘Serverless’. Confused yet? Let me help. It is very important to note that there is the Serverless concept and the  Serverless framework . The Serverless concept as said above is a cloud execution model, however, there also exists something called the Serverless framework – that is just like  Terraform (TF)  and CF in that it is just another IaC tool. For ease of reading I will reference the Serverless concept as ‘Serverless’ and the Serverless Framework as ‘SF’. Although SF is Infrastructure as Code like Terraform and CloudFormation, it is important to note that it’s actually more similar to TF in the sense that it’s Cloud provider agnostic whereas CF is AWS specific. TF, some could argue, is much more comprehensive in what it offers as it allows you to create all kinds of resources in the Cloud, whereas SF is more fine-tuned for setting up Serverless infrastructure and its shared resources. I would even argue it is much easier to set up Serverless infrastructure with SF than it is with TF – from experience. However,  Yan Cui has shown  that you can actually use SF and TF together in order to take advantage of the best features of both – those being TF’s extensiveness and SF’s simplicity around deploying Serverless infrastructure. . As Yan also mentions, SF is the most popular deployment framework for Serverless applications as it offers a suitable abstraction over CF and some of the best practises out of the box. Something I don’t disagree with in the slightest. Additionally, even if SF doesn’t offer their own implementation around simply deploying shared resources you can always just resort to using CF syntax in YAML as SF actually boils all of its templates into CF syntax anyways. This allows for a lot of flexibility, especially if you want to stick to one framework. Lastly, SF also enables you to test your functions, something we will explore a little below. I make this point because with TF, it is mainly infrastructure only and does not offer you the ability to test what you have created. . Now, enough of the background, let’s get creating some Serverless-ness… . So, with the background above setting the foundations of understanding what Serverless is, let’s dive into some examples. As we’ve discussed, using CF to create your Serverless infrastructure can be a daunting task for someone fresh to the world of Serverless, so bearing this in mind we are going to use the Serverless Framework instead. . I will skip all the pre-requisites of  setting up AWS CLI , configuring AWS Access Keys and  installing the Serverless Framework with NPM  as those are well documented online. So, let’s jump right in. Once the SF has been installed, create a folder for our code and inside it all we need is two files, a  handler.js  file and a  serverless.yml . The purpose of these are the following: .  handler.js  is the file that includes the code in which will get uploaded and invoked in AWS Lambda. .  serverless.yml  is the file that SF looks at for instructions on what resources to deploy and what configuration to deploy with them. . In the  handler.js  file, include the following code: . Here we see a simple exported function called  helloWorld  that accepts  event  and  context . These are passed into the function when the Lambda is called. The   event   will contain data about the resource or request that triggered the function and the   context   object will contain information about the invocation, function and execution environment. There is also a  callback  argument that we can accept, however  callback  is only accepted for non-async functions and since we are declaring an async function, we don’t need it. Additionally, in non-async functions you can call the  callback  function to send a response and it typically takes two arguments, error and response. For async functions you return a response, error or promise to the runtime instead.  For our code, we are declaring an  async  function that returns a response that contains just an OK status code of 200 and a body that just outputs a message. Not much to it. . In the  serverless.yml  file, include the following code: . We have the service name line at the top that tells SF that this is the name of the service. Next we have described details about the Cloud provider (for us is  aws ), our runtime which is  NodeJS  as well as our region:  eu-west-2  (London). Below that is the  functions  section. This is what SF will read in order to create and deploy to AWS Lambda. For us, it just declares that we want a function named  helloWorld  and that its  handler  is the  helloWorld  function we created in the  handler  file. . Now let’s deploy to AWS by running the following ( -v  is to give verbose output): . SF will now deploy our code to AWS according to what we specified in  serverless.yml  whilst also performing some other bits like setting up IAM Roles and LogGroups. You will see in the terminal that it prints what resources it is creating as well as their success. Once completed it will print information about our Serverless function and its resources in the terminal. I won’t show you the full output I have as it includes account numbers. But what I can show you is the following: . As we can see, the service name is what we specified as well as the region and under the  functions  section we can see it has created our  helloWorld  function. Ignore the other details for the time being as you can research them more later on – stage’s I would recommend looking into if you’re looking to get into the SF as it is a very nice way of being able to group your deployments into stages e.g. dev stage, test stage, prod stage etc. . Now, I know what you’re thinking. It’s worked but what have we actually done? So, let’s navigate into the AWS Console within the London region and go to the Lambda Service and click Functions. We should be able to see our  helloWorld  function and when we click it, if everything went well we should be able to see something like the following: .   . Here we see the  helloWorld  AWS Lambda function. In the designer view you should be able to see all of the resources that it is connected to but as we have only created something simple, you will not see much inside here. You can see the  Amazon CloudWatch  Logs resource attached to the function and that was the LogGroup bit that I mentioned earlier on that SF does as a default. This is very useful to us as we don’t have to worry about setting up a LogGroup in our  serverless.yml  file as SF automatically does it for us. You can even have a look at the CloudWatch resource and have a play around and see what you can see there – I always encourage putting this article on pause and just having a play around in the AWS Console as this is where you learn. Currently, there won’t be anything in the logs as we haven’t invoked the function yet. So, let’s do that. . In the top right hand corner you will see the ‘Test’ button, click that and select ‘Create new test event’ (is probably already selected as a default as there are no test events yet), in the event template select ‘helloWorld’ (again, it’s probably already selected) and then in the ‘Event name’ we want to choose the function name in the code, which in our case is ‘helloWorld’. Additionally, in the JSON below just put a body (just the beginning and end braces) like so: .   . When your test event looks like the above, then click ‘Create’ and then click the ‘Test’ button again. This time it will run the test event we configured by invoking the  helloWorld  Lambda function whilst showing the results like below: .   . Hooray!! We can now see our message has been output to the screen. It also gives us a bit more information about the ‘Request ID’, billed duration and the duration it took to invoke the lambda and respond. Speaking about the duration, we can see it was 2.83ms, what that means is the Lambda existed for only 2.83ms and immediately after it finished outputting the message to the screen it terminated. We can even go and have a look in the CloudWatch looks and see what logs it has output. To do this, we go back to the Lambda screen we saw before when we first clicked on the function and this time, near the top left and select ‘Monitoring’. .   . This should take us to a screen that shows some nice fancy graphs and metrics about the Lambda and any information about its invocations. What we are interested in is the ‘View logs in CloudWatch’ option to the right-hand side of the screen, so select that and a new window will open with the option of being able to view the logs for our Lambda function. Have a look around and see if you can match up the ‘Test’ events with the log entries, hint.. use the ‘Request ID’. . Once you’ve had a play around with CloudWatch and seeing what it shows, you’re probably thinking, well, that’s all good and well but I don’t want to click a ‘Test’ button to invoke my Lambda function and you’re absolutely right – you wouldn’t want to do that. So, let’s instead use SF to invoke our Lambda from our local terminal. To do this we run the following  serverless invoke --function helloWorld  . What this will do is use SF to manually invoke the Lambda under the function name  helloWorld  that we specified in the  serverless.yml  file. . Here is our response: . You can also navigate to CloudWatch and see the logs for that invocation. Pretty impressive right? Just to recap so far, we have: . Defined the function code that gets run when invoked which only outputs a message response . Deployed the function to AWS Lambda . Created a test event that invokes the function . Viewed the logs that get generated in CloudWatch as well as some metrics about the invocations . Used SF to invoke the AWS Lambda from our local system . Now although we have created a Lambda, deployed it and invoked it, what we really want is to deploy our Lambda under a publicly available API endpoint that we can hit using a REST Client or a browser. Currently this is not possible as the Lambda is not exposed publicly. The test event is just a test that is run against the Lambda from within the console, and the SF invoke was only possible because it uses your AWS Access keys in order to invoke the function. In order to expose our Lambda publicly, we need an API Gateway. . We can either create the API Gateway through the console using the designer, or we can define it in the  serverless.yml  and let SF create it when we run a  deploy . Since we are using SF to do all the manual lifting, let’s stick to that theme. . To create an API Gateway it is very simple, we just add the following lines to our  helloWorld  function in the  serverless.yml  file. . The  events  section simply exposes the  helloWorld  function under the specified conditions. In our case, we are telling SF to create an  GET HTTP  Event that has a specified  path . That is all we need to do. Now let’s rerun the serverless deploy command above. You will notice it may take longer to deploy this time because it is creating the API Gateway and configuring it with permissions for the Lambda Function. Once it has finished, you will notice some different output in the logs. This time some sections will have data in them (endpoints etc). What happens if we copy and paste the endpoint URL into a browser? Let’s try! .   . Would you look at that, there is our invoked response from our public Lambda. What do we see in the Lambda console when we refresh the page? .   . Well, there’s now an additional resource in the designer window. We now have an API Gateway attached. Now would be good time to pause this article and just go into the CloudWatch logs and see if you can see the logs being populated when you hit the URL again via the browser or a REST Client. Additionally, I would also advise going into the API Gateway service in AWS and just clicking around to see what you can see in efforts to become more familiar with it. . Remember before when we mentioned that using CF to do all this would be quite daunting, well let’s go have a look how much code we would have had to have written to get all this to work. . Let’s go to AWS CloudFormation by selecting ‘CloudFormation’ in the ‘Services’ section of the menu at the top left of the Console. .   . Here we can see we have a  serverless-hello-world-dev  stack. Remember earlier, I said that SF creates an abstraction over CF? Well at a lower level it actually just takes the  serverless.yml  files and converts them to a CF template as show above. So, when you do a  serverless deploy  you can actually just come to the CF stack and see what it has created as well as view additional information like event statuses, outputs etc. . One thing we will do is look inside the template. To do this we click the ‘Template’ tab in the ‘serverless-hello-world-dev’ CF window. Here you will see the CF template file in JSON form. This is what the SF generates at the lower level. This is what can be seen as the daunting part, imagine having to write all of that JSON just to do what we have done so far, which it’s worth noting, isn’t very complicated. Now you can start to appreciate why SF is popular. Due to it being simple to use, it also hides a lot of this stuff away from you, but not to the degree in which you can never see it – all you have to do is come into the CF Template window and view the raw CF JSON or YAML. . Something even more cool is hidden in here, just click the ‘View in Designer’ button. .   . This is a little hidden gem that I think a lot of people aren’t aware exists. Let me tell you what you are looking at here. Not only are you looking at a real-time view of what your current Serverless infrastructure looks like in a nice and clear diagram, but you are also looking at a ‘Designer’ which essentially allows you to create or update your infrastructure. To do this, you would select the resource types on the left-hand side and drag them to the diagram. You can then connect them up to other services and even configure them more specifically in the code view below in either JSON or YAML as it is essentially a code version of what is on the design view. You can even select a resource within the designer view, and it will automatically take you to the CF Syntax code that it translates too. This was something that stood out to me as a pretty nice feature as it allows for a kind of  architecture as code  approach where you can design what you want your architecture to look like and then use the IaC below in your CI/CD pipelines. . Now, that’s it for the time being, in order to remove our Serverless infrastructure we can also use SF by running the following command in our terminal:  serverless remove  . This will remove all of the infrastructure we set up over the course of this article. To check everything has been removed, just refresh the CF window and you shouldn’t see the stack anymore. You can even check the Lambda window too. Man, I love IaC. . This article was not created in order to fill the theoretical gaps of doing Serverless computing on AWS neither the theoretical specifics about AWS Lambda and how they work at a lower level. If you wanted to read up on such topics, I would start with what a  AWS Lambda  is, what an Lambda  execution context  is, as well as just scouring the web for general content around developing Serverless applications on AWS. This information can be found online, there are plenty of resources on these topics. What I wanted this article to do was to show an individual how easy it was to set up a basic Serverless infrastructure in AWS using SF as well as the AWS Console in order to interact with what we created. Additionally we managed to create a AWS Serverless Function using AWS Lambda, we managed to attach a logging monitor and an API Gateway that exposes our Lambda to the public (try sending the exposed endpoint URL to someone and ask them what they see) as well as taking a basic look into CloudFormation and how it can be used in order to not only create and amend infrastructure but to view it in more detail. . All in all we have only covered the tip of the iceberg with what Serverless computing is about and keep a look out on this blog where I will be posting much more content around using Lambdas with DynamoDB, S3 as well as being able to perform Canary Deployments on our Lambdas using Aliases and Traffic Weightings. Another nice feature is being able to perform authorisation on entry to your Lambda using API Keys in API Gateway and Lambda Authorisers (formerly known as Custom Authorisers) that are additional Lambdas that you can create that enables you to perform more in-depth authorisation logic that you want to share across your infrastructure. . But as said above, that’s it for now, and until next time, see ya. ", "date": "2019-07-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Designing Cross Platform Mobile Applications with Xamarin\n  ", "author": ["James Heywood"], "link": "https://capgemini.github.io/.net/designing-mobile-cross-platform-applications-with-xamarin/", "abstract": " Xamarin provides a common development experience for creating cross platform mobile applications. The aim of this post is to highlight the design techniques available and options Xamarin provides for maximising code reuse, thus ensuring cleaner code and increased productivity. The anatomy of native Xamarin Android and iOS applications will be initially discussed and used as a starting point for exploring these concepts. .  Xamarin  is a Microsoft owned software company that provides cross platform implementations of the .NET Framework that target Android, iOS and Windows devices. With increased support in Visual Studio it provides a common development experience for .NET developers to create cross platform applications. The most common usage is to develop applications targeting Android and iOS mobile devices, thus these applications will be this blog post’s primary focus. . Xamarin facilitates the development of Android and iOS applications by providing the  Xamarin.iOS  and  Mono.Android  libraries, as shown in  Figure 1 . These libraries are  built on top of the Mono .NET framework  and bridge the gap between the application and the platform specific APIs. . The following is an explanation of how native Android and iOS Xamarin applications are structured including the different components and their relationships. .  Figure 2  outlines the project structure and the architecture of a Xamarin Android application. An Android application is a group of activities, navigable using intents, that provide the code that runs the application. The entry point of the application is the activity whose  MainLauncher  property is set to true, which is the  MainActivity.cs  by default. Activities that provide a view have an associated layout template that is made up of view controls. Activities and view controls reference the following resources: . layouts – view templates loaded by activities. . drawables - icons, images etc. . values – centralised location for string values. . menus – templates for menu structures. . The  Resource.designer.cs  class provides an index of identifiers for all the resources in the application. This class is referenced by activities and view controls to create an instance for use in the given context. .  Figure 3  shows the project structure and the architecture of a Xamarin iOS application. The application is made up of several view controller classes and associated views, collectively known as scenes, that are loaded into the main application window. View controllers are grouped into storyboards with each storyboard having an initial view controller. Views are made up of a view controls used for display or user interaction. Navigation between the view controllers is handled via segues. . The entry point for the application is the  main.cs  class that instantiates the specified  AppDelegate.cs  class, which loads the initial view controller of the default storyboard set in the  Info.plist  configuration file. Resources such as images, videos etc are referenced from the  Resources  and  Assets.xeassets  folders by view controllers and view controls. The  AppDelegate.cs  class includes delegates that handle application events and the view controllers handle the lifecycle for a given view. . Although the above two applications target different platforms, their architectures have many similarities. They are both event-driven with actions performed by delegates wired up to both application and UI events. The display in both cases is driven by views interacting with code behind classes, which for Android is layouts and activities, and for iOS is views and view controllers. These similarities continue as view controls are added to views to provide content display and user interaction. . Xamarin takes advantage of these similarities to provide a common UI development experience. While this blog will explore this a lot more, it suffices to now concentrate on how code can be shared between both Android and iOS applications. . There are a couple of methods that allow code to be reused across projects in Xamarin: Shared Projects and Class Libraries.  Figure 4  shows the available options to refactor common code out of the application layer of the platform specific projects. It also illustrates that the extracted code can be consumed by any .NET applications, thus further increasing the scope of its use. . A  Shared Project  differs from class libraries as the code is copied and included in each application assembly during compilation, thus no separate assembly is created. This non-separation of outputs does have its disadvantages, for example sharing the code beyond the scope of the solution becomes problematic, and unit testing components in isolation from the application is not possible. . A  PCL (Portable Class Library)  is a type of class library introduced to address the issues highlighted with Shared Projects.  Figure 5  shows an example the available platforms that can be selected, and illustrates that the use of PCL’s is not restricted to just Xamarin mobile applications. The compiled assembly can be referenced by other projects, however there are still drawbacks as the target platforms supported need to be selected on creation. Consequently, only a cross section of the APIs across the selected base libraries are available for use, limiting the available scope. . The latest, and recommended, method to share code is using  .NET Standard  libraries which provide a wide ranging and consistent API with full compatibility across the latest versions of the .NET framework, .NET Core and Xamarin, thus alleviating the limitations of the PCL approach. . In Xamarin the concept of code sharing can be taken even further by implementing the MVVM pattern. This will be discussed next. . Xamarin, as previously discussed, is based on an event-based architecture in which methods handling UI and business logic would traditionally reside in code behind files.  Figure 6  highlights how the MVVM pattern moves the responsibility of handling view data and events into view model classes and away from activities and view controllers. The view model classes then reside in shared code, which is referenced by the application projects, leaving the activities and view controllers with the platform specific responsibilities. The connection between the view and view model components is handled via data binding which ensures properties and methods on the view model are wired up to the view controls. Apart from code reusability. another advantage of data binding is that the view reflects the view model’s state through two-way binding. Consider the example of a price calculator, updating the gross amount view will feed through to the view model, causing the net value to be recalculated, which will automatically reflect on the view. .  Figure 6  does highlight one problem with the MVVM approach that needs to be addressed. The example shows the application saving a devices location which will require retrieval of its GPS coordinates. The code to handle saving now resides in the shared code but the platform specific libraries that handle retrieving the devices location are referenced by the application projects. How can the view model pattern be implemented to handle platform specific functionality? . Shared projects provide a mechanism for including platform specific code by using conditional compilation. Android and iOS projects have default compilation symbols configured which can be used to include platform specific code. Shared projects are different to class libraries as they act more like extensions to the projects that reference them, thus they have access to the libraries referenced by those projects. The compilation symbols can be used to implement platform specific code in the same area, which is then pulled into the specific project during compilation. . This is not ideal as the code does not follow good code design principles, for example there is no separation of concerns, thus adding complexity. .  Figure 7  highlights a more elegant, and preferred, approach that follows  SOLID principles , in particular the Dependency Inversion Priniciple. The Shared Code includes an  ILocation  interface that is referenced by the view model, and by the application projects. The application projects implement their own specific version of the  ILocation  interface in the form of a  Location  class. An  Inversion of Control (IoC)  container, which is configured in the application projects, is used to implement the Dependency Inversion Priniciple by injecting its own version of the  Location  class into the view model, thus at runtime the platform specific  Location  class will be used to retrieve the devices location. . There are several MVVM frameworks that are available for Xamarin development.  MVVMCross  and  MVVMLite  are commonly used examples, which also provide IoC container support, but there are others available that may better suit your needs. . The previous sections have concentrated on code reuse, next let’s consider the UI. . Previous sections have outlined how code sharing can be achieved between platform specific applications but Xamarin also provides an approach for sharing the UI components. Xamarin.Forms takes advantage of the commonality between the architectures of native Android and iOS applications.  Figure 8  shows the project structure of a Xamarin.Forms project in Visual Studio and the relation between the components involved. . An Android and iOS project is created, as before, along with an additional third project which contains the common UI components. A Xamarin.Forms project uses XAML mark-up for creating views and accompanying code behind pages for handling behaviour. Xamarin.Forms also provides view controls that can be referenced both in the XAML and the code behind for creating the user experience. The application class,  app.xaml.cs , is the entry point that loads the initial page for the application and contains delegates for handling application level events. . The entry points of the Android and iOS projects,  MainActivity.cs  and  AppDelgate.cs , are configured to load the common Xamarin.Forms  App  class. Once the app is initialised, platform specific renderers translate the pages into activities or view controllers, and the views and view controls into their Android, or iOS, counterparts at runtime, thus providing the native application experience. . As Xamarin.Forms is based on the .NET framework and XAML it can also be used to create the UI experience of applications targeting Windows devices. Apart from reusability, another advantage of using Xamarin.Forms over native Xamarin application development is that MVVM support is built in out of the box through simple refactoring of the XAML mark-up and associated view models. . However, although Xamarin.Forms will handle most requirements there may be limitations when developing against specific platforms. Xamarin.Forms does provide extension through custom renderers but this may potentially create greater complexity in your application which can often be better implemented using the native approach. . Hopefully this post has provided a high-level understanding of how to approach the development of cross platform Xamarin applications. I mentioned earlier how .NET Standard libraries are now the recommended approach for implementing common code and how implementing the MVVM pattern, in conjunction with good design practices, can increase code reuse even further. Finally,  l discussed how reuse can be extended to the UI layer by using Xamarin.Forms to build common UI components and highlighting that this should be sufficient in most cases unless requirements steer towards a more richer native UI experience. . The techniques and options covered show how the responsibility of the native application projects can be limited to managing only platform specific functionality and resources leading to better separation of concerns, and consequently higher code reuse and productivity. ", "date": "2018-08-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Do Repeat Yourself - returning to the Lead Developer conference\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/lead-dev-conference-2018/", "abstract": " After  such a positive experience at the Lead Developer conference last year , as soon as I saw this year’s announcement, I saved the date and started writing the email to get my company to pay for the ticket. Thankfully, my boss didn’t need much persuasion, so I was able to join 1100 others at the Barbican Centre for two days of talks at  The Lead Developer London 2018 . . It’s a conference that really seems to care about doing things the right way to help attendees get the most out of it, whether that’s by maintaining a code of conduct, keeping us well fed and watered, running speaker office hours sessions, or organising mindfulness sessions to help us recharge. . There were a few recurring themes, and as with last year, one of the ideas that came up repeatedly was the difficulty of making the transition from individual contributor to leader, and the importance of realising that it’s not just a promotion but a career switch. As Dan Persa put it, “the skillset required to be a successful lead is not a natural evolution of the skillset required to be an effective developer”. For example, the Don’t Repeat Yourself principle is much less useful when dealing with people than it is when dealing with computers - people often need repetition in order to get the message. Jenny Duckett also highlighted the value of communicating goals over and over again to help the team to have a clear focus. . Similarly, Menno van Slooten talked about his difficult experience of becoming a lead, and made the point that as developers, we like the sense of accomplishment that comes from building things and solving problems, and when we become leads, it’s easy to overlook the value that we can bring by using our influence to help improve the lives of the people we work with, even if it’s just by going to a meeting so that they don’t have to. . A subject that came up in more than one talk was  the value of “psychological safety” , the value of trusting your team members, and the importance of avoiding a blame culture. As with last year, Nickolas Means spun an intriguing tale full of nerdy details, to set us up for a few key lessons. Chief among them for me was the idea that we should “assume positive intent” - a team shouldn’t have to earn trust. An important part of the role of a leader is to create a healthy environment for the team to flourish, and to allow them to use their skills - as Alicia Liu put it, “a gardener doesn’t tell plants how to grow”. . Alicia Liu and Christian McCarrick both talked about the value of scheduling time to focus on tasks - or as Alicia put it, scheduling time to worry. Christian’s point about keeping track of ‘dark matter tasks’ by having a ticket for everything really resonated with me, as someone who spends a lot of time in JIRA. . Another theme that struck me was the importance of listening, and allowing other people space to talk, whether that’s in one-to-ones or larger meetings. Both Melinda Seckington, in her session on Goal-Setting Workshops for Managers, and Kevin Goldsmith, in his talk about Using Agile Techniques to Build a More Inclusive Team, provided practical examples of how leaders can help their teams to be more explicit and effective in their communications. . Being more intentional in communication was also one of the subjects of Dirkjan Bussink’s talk on distributed teams. His team at GitHub are spread around the world, and being widely distributed brings a range of communication challenges. He talked about the value of chat (and video chat) as a “virtual water cooler”, and how it can help us to remember that there are real human beings on the other end of the pull requests and chat messages. . While technology can help, meeting in person is ideal to foster team bonds and have the kind of discussions that are only really possible when you get the team together and use the higher bandwidth of face-to-face communication. . Chat is a different medium to real conversation, and as Dirkjan said, it’s valuable to talk in Slack channels, rather than private conversations - it “encourages collaboration and people can jump in and help out, and it also promotes transparency”. This echoed one of the points I was trying to make when I visited colleagues in Mumbai last year, with  my presentation on asynchronous and synchronous communication . . All of the talks were worth listening to, but there were some that particularly stood out for me. . Clare Sudbery drew on her previous experience as a teacher to talk about how to enhance the skills of developers, building a culture of everyone learning together. She highlighted the importance of teaching people in a way that doesn’t put them off learning by making them feel stupid. As Clare mentioned, teaching (often in the form of pair programming) is enormously valuable as a learning tool for the teacher - being able to explain your complex knowledge to someone who doesn’t get it yet helps you to consolidate your knowledge. . Clare wasn’t the first (or the last) person at the conference to mention mob programming, but she did begin to persuade me that it might not be a completely ridiculous idea. . A point made by Clare that probably sums up my whole conference experience is that “it’s empowering to realise that other people have gaps in their knowledge too” - none of us are perfect, including those people we look up to. It reminded me of an idea that my former colleague (and Clare’s current colleague) Andrew Harmel-Law explored in a previous blog post here -  the importance of admitting when you don’t know the answer . . Most of the talks focussed on so-called ‘soft skills’, but some were a little more technical. Alice Goldfuss helped cut through the hype with some sound advice about containers, while Marek Rogala gave a pirate-themed introduction to functional programming. . Adrian Howard was really preaching to the converted with me when he talked about the need to focus on business value, rather than story points. . Alexandra Hill spoke engagingly on a subject close to my own heart - code reviews. It’s too easy for them to become a source of conflict within teams, and she discussed some strategies for mitigating this, not least of which is giving positive feedback. . Just as it was last year, the Lead Dev was an excellent conference, that gave me  fresh ideas and renewed enthusiasm . Someone I spoke to in one of the breaks suggested that some of the talks didn’t tell him anything new, but in a way that’s partly why I like the conference so much. I like hearing clearly expressed versions of ideas that aren’t yet fully formed in my own mind. Perhaps some of it is ‘just common sense’, but it’s often easy for common sense ideas to get lost among the noise. . It’s reassuring to know that more experienced leaders face some of the same challenges that I do, and to realise that I’m not alone in finding some things difficult. As Menno van Slooten put it, “talking to them, and knowing that they also had these problems made me feel a little bit less alone, and made me feel a little bit better about myself”. In a way, it feels almost more like a support group for lead developers than a conference. The people on stage are there to share their experiences, those experiences are familiar to most of the attendees, and the talks help us to figure out our personal coping strategies. . Now that  the videos of the talks are up on YouTube , I’ll be revisiting some of the talks, and I hope to be back there again next year. ", "date": "2018-08-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Why you should try React higher-order components now\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/frontend/react-HoCs/", "abstract": " I am in my final year at university: part of the  Capgemini Degree Apprenticeship . This year requires a large software project to be delivered and one component of that is a  React  frontend. Through it I’ve been experimenting and discovered a love for  higher-order components  (HOCs). I believe one of the reasons I’ve been able to, in the later stages of the project, churn out screens quickly as it comes closer to the deadline is the use of HOCs. . A higher-order component is simply a function which takes a component as a parameter and returns another component. . An example is helpful here. Consider this HOC which takes a component to render if the data has been loaded. If the data is still loading it renders the loading spinner: . Now, I can simply use this whenever I have a container which loads data, composing it with an “Data Fetcher” HOC (which fetches the data and defines whether it’s still loading) and applies it to a  PresentationalComponent  which just displays the props passed to it by the HOCs: . Happy days. No more checking if the query is still loading and returning the loading spinner in every container that fetches data. . I believe understanding this really speeds up development, by giving quick access to common things you do in your components. . Once I fully moved to using HOCs I was able to remove React and JSX from my  container components . In the below example I have an Auth Service which provides a getProfile method to get the profile of the logged in user. If profile is populated it returns some JSX with the profile in its props. When refactored to use a custom  withUserProfile  HOC the class below: . Can become: . Subsequently, any component which also needs a user profile (which is a lot of them), can also benefit from having the  userProfile  prop automatically filled with user data. I think this provides great separation. JSX is only used for the presentational components which is what it’s intended for. Data fetching, mapping and injecting is done in Container components solely with the use of composition and higher-order components. I can then also use  withProps  or  mapProps  to keep the interface between the container component and presentational component free of framework details e.g. I can map from props.data.deliveries (which is where Apollo GraphQL puts the fetched data) to props.deliveries and the presentational component is none the wiser where it received its data from. . In the above example you may also want to re-render on user logout or timeout and so the HOC can subscribe to updates from the Auth Service and that subscription code can be written once, as opposed to in every container component that required a user profile. . The last code snippet uses a HOC  mapProps  from a library called  Recompose . I’ve found these small HOCs very helpful. They do one, small thing well and describe my intentions better than using JSX e.g.  &lt;Profile profile={userProfile}/&gt;  is better described by  mapProps((props) =&gt; ({profile: props.userProfile})) . They also provide extra benefits e.g. if you use  withHandlers  you can be sure a new function will not be created on each render. This is instead of using arrow functions in JSX, which would create a new function every time. Take a read of the  Recompose docs  and try using it, even if you don’t like it in the end it will expand your mind as to what’s possible with HOCs. . I’ve always been told to “prefer composition over inheritance”, and I usually listen to this old adage. Typically my classes end up having a constructor parameter which is an object which fulfills some interface which I invoke in one of the class methods later: . HOCs however, are a new form of composability for me. Any number of HOCs can be composed together. This is not limited to the constructor parameters you define in your class (or however you inject your composed functionality). The composed functionality does not have to conform to a particular interface, i.e. have particular methods, because it’s not called by the class or component being enhanced. The best bit is that the component does not know it had been enhanced with a HOC: the above class, in contrast, is obviously composed with a  transformer . . I’ve always thought this sort of composability should be available to replace programming which relies heavily on inheritance such as Java Android. With the advent of  Kotlin as a first class Android language , hopefully this will become the case. . In my project I also needed to do authorisation. So, if a user had a particular role they could see and navigate to some parts of the UI but not others. I researched how others have done it and found  an article which used a HOC  to do this. I initially tried this way, thought the use of currying was good but that it could be made better. The pattern I used isn’t strictly a HOC: it uses the fact that any children of the component specified in JSX are put in props.children and it “enhances” those child components by conditionally rendering them based on the user’s role. I thought was worth showing to add to the mix of ways to do authorisation: . Which is then used like this: . Take this as an example of how to take advantage of the spirit of HOCs but not necessarily follow the definition to the letter. Using the method above resulted in less code and seemed more usable to me. It goes to show that the myriad of ways you can do conditional rendering demonstrates how React embraces functional paradigms and really allows the developer to be expressive in their own way. . This post doesn’t go into the details of HOCs but it should be an encouragement to explore them further if you haven’t: here’s  a good starting place . Use them to separate concerns between presentational and container components, create framework agnostic interfaces and abstract out routines you find yourself writing a lot e.g. loading, validation or error checking. ", "date": "2018-10-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "It's OK Not to be Agile\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/agile/ok-not-to-be-agile/", "abstract": " I love messaging platforms.  Slack  has made a real difference to the way we can work with remote teams and keep our disparate engineers sharing thoughts and mindsets. We’re also experimenting with  Teams , which is proving great, but all these tools sure do make it easy to become distracted from the day job! . This week’s distraction at Capgemini Engineering HQ revolves around a number of  new takes on Agile  which are being posted to explain why “Agile does not Scale”, and how this or that new and exciting version of Agile can change things. The big beef for me with these articles is that they do not define the problem domain before launching into a solution. So let’s start with the problem! . I know we do this a lot, but no harm in going back to basics and redefining what we mean by Agile first of all. At Capgemini, when we say “Agile”, we refer to ways of working and methodologies including Scrum, XP, Kanban, DSDM, Lean, possibly including DevOps - which are facilitators to us adhering to the  Agile Manifesto . These ways of working enable our customers to better understand how technology can help them, and, in a timely way, produce and improve useful business applications. . Drilling down to what it means to be Agile according to the Manifesto, a big part of this is to enable a team to be autonomous, so that they can deliver software iterations faster. These iterations can then be evaluated and improved. Why faster? Well, they don’t need to wait for some other team / person to approve their decisions. So much time is spent waiting for various forms of signoff; if you can get the decision-maker in your agile team you can move forward so much quicker. This is great for small business units or small companies, where you can easily identify this decision-maker and get their commitment to a project. It is, however, something which does not scale easily. Imagine if the project is something along the lines of: “bring new technology and modern ways of working to the justice system”. This spins out across multiple projects, sites, funding streams, technologies, and may or may not involve software development - how on earth can you maintain autonomy in that kind of environment? And, in fact, should you even be trying? It seems to me that in some scenarios, talking about the project as Agile is really not the right approach. This doesn’t mean in any way that I’m changing my stance as an Agile Evangelist, but more that we should use it in the right place. Agile is a software development methodology; if you’re not developing software - ie you’re not that far along the digital reform path, or you’re installing a custom software package which needs configuring rather than developing, then maybe an agile methodology is not right for you. And redefining your approach as “new Agile” whilst moving away from the Agile manifesto is certainly not helpful. . There are a couple of project types to discuss here: . This is an area I’ve been working in for the last couple of years. Agile doesn’t really provide benefit here for very interesting reasons - the development phase is really tiny, basically a few minutes linking bits of “pipe” together if you’re using a product such as  Mulesoft ,  IBM API Connect , or even Camel (especially the exciting new  Camel K ). With these platforms, it can take significantly longer to write your Jira task than to actually complete the development phase! . Secondly, there is the fact that an integration project has at least two extra parties (if not more) involved, the source (API caller(s)) and the target(s). These parties are very often different companies, or at least very separate divisions within a company. So again, the decision making process is more complex than the build process and a specific stakeholder / product owner is hard to identify. What if an API caller wants a certain field to be present in their response, which does exist in the target system but is only provided in a convoluted way? Or if it exists in another system? Does the integration layer accommodate this change? Does the target system change for it? Who decides? Who provides the test data / test cases? This multi-party uncertainty often results in the formation of a RACI matrix - cue Agile purists berating the death of their  creativity , possibly rightly so - for here, what benefit does the integration team have in being Agile? We are always going to need at least two product owners (source, target) and then of course some kind of mediator when they inevitably disagree… and hence we are unlikely to be able to have all three decision-makers in our sprint team. . Our recent project’s take on Agile for Integration involved, again, going back to the manifesto, and taking the pieces that really helped. In this case, “Individuals and Interactions over Processes and Tools”, “Customer Collaboration over Contract Negotiation”. Despite being split-site and multi-party, we strove to build up a relationship across the integration by getting people together and talking. We were open and honest about the speed at which we could develop on our integration platform, we were open to building disposable pieces. We didn’t wait for requirements documents to be signed off before we built an integration, we were iterative; we built a best-effort flow and then we tested it with the end systems to see whether it provided what they needed. We had standups and also scrum-of-scrum style meetings. This flushed out issues much quicker than if we’d have worked in a more waterfall pattern and forced our parties to try and fully document their API requirements. So yes, we had a RACI matrix and no, we weren’t autonomous; no, I wouldn’t call it Agile, but I feel we were still being true to our roots! . Here is a second scenario where Agile is misused, and one with which you are probably familar: .  Programme director:  Where is the project plan? .  Project manager:  We don’t have one, we’re Agile. .  Programme director:  What is this team doing? .  Project manager:  They’re configuring the Employee Benefits system. .  Programme director:  And when will that be live? .  Project manager:  When we get the data feed from the new payroll system. .  Programme director:  Well, when do the employees start using the new payroll system? .  Project manager:  We’ll train them two weeks before the system goes live .  Programme director:  And when is that? .  Project manager:  I can’t tell you, as we only plan 2 weeks ahead . The point here is that in large-scale technology programmes, in particular  ERP  system installations or similar, there is tight coupling between disparate workstreams. The teams can only be autonomous to a certain extent, and can only move so far, before they are blocked by other workstreams. These dependencies are built in to the software package being installed, and the development team have little flexibility to adjust them. Without a long-term project plan forecasting and mapping interdependencies, (which implies rigid requirements), this kind of project can’t succeed. Frameworks like  SAFe  are designed to give programmes the courage to call themselves Agile, to use an Agile methodology; but how far from the manifesto are they moving? Too far to call themselves Agile? Probably. . Our take on Agile at Scale within Capgemini Engineering is manifold. We endorse and promote SAFe, partly as a way to reassure our clients that (a) Agile at Scale is possible and (b) a lot of scholarly thinking went into creating the SAFe framework. When we are on the ground, there are a couple of proven foundations that we would encourage. We love the Scrum of Scrums whereby a daily standup, containing a member of each sprint team, is held to communicate important updates. Then there are some architectural paradigms which really help: . We have long been  advocates of DDD  and it really does help to allow autonomy at a sprint team level if you have put in the effort to identify and reduce dependencies earlier on. How does DDD help? Well, by encouraging a team to define bounded contexts, to get the development and architecture teams closely involved with the business so that they get a proper understanding of the business domains, and getting a focus on continuous integration so that conflicts are identified and resolved constantly - rather than in a big, painful bang at irregular intervals. The focus on defining a Ubiquitous Language; a common, business-wide dictionary of terms, is also essential for cross-communication when developing at scale. . A microservice architecture also helps to grant autonomy to the teams building those service(s). It’s a great way to divide up development ito pieces that are, as our blogger  Gayathri  puts it beautifully, “people-sized”. Using interfaces as the contracts between microservices, and understanding  who owns the interface definition  (clue: it’s not the implementor) can help get the right product owner, able to make the right decisions, in place to autonomise the microservice dev teams. And microservices should also by definition be independently deployable, so you won’t tread on any toes at a platform/environment level either. . We always advocate an early focus on code deployment mechanisms when working at scale. For a start, it’s often where big bottlenecks can be found. You can streamline deployment down to minutes, but if you have to wait 2 months for a heavy CAB process to grind out a release date, is there any point?! This is often something that gets overlooked by companies who wish to be agile and work at scale, so we point it out as early as possible. And again, it’s fine if you wish to stick to your CAB schedule, but don’t label your project as agile. . Having a standard continuous delivery process across a large-scale project can really help disparate teams to undestand how each other works. A single Jira instance can easily scale, although it may well require a dedicated and skilled resource to make sure it remains legible! And it’s OK to have multiple source control systems, repositories, dashboards and build automation tools, as long as they follow the same basic rules (branching techniques, scheduling i.e. “build on commit”, number and purpose of environments, red dashboard = bad etc). . It’s easy to struggle to work in an agile way when working at scale. And it’s great to find ways to get large-scale projects moving forwards successfully, but we need to be careful with our use of language and terms. If “Agile” simply comes to mean “Software Development with Sticky Notes”, we will start to lose the understanding of why we wanted to change the way we develop software in the first place. So, it’s fine not to be agile, it’s great to shout about your ways of working, but just don’t feel the need to say that you are, don’t use the term. . Agile is a recognition that, in software development,  things will change . We won’t get it right first time because nobody knows what “right” is at the start, and we need to develop ways of working to embrace and absorb that change by granting development teams the freedom to decide how to cope with it. Any references to “fully define”, “controlled schedule” cannot by definition be agile ways of working. Be wary! ", "date": "2018-10-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Basic Accessibility for Web Applications\n", "author": ["Liam Giles"], "link": "https://capgemini.github.io/accessibility/Basic-Accessibility-for-Web-Applications/", "abstract": " As frontend developers, it’s not enough to know how to build the applications, you also need to know how to build applications for everyone to be able to use. . With the number of web users continuously growing, it’s important that as developers we do our best to reduce alienation of individuals that can occur if we don’t develop our applications in a manner that is accessible and usable by as many people as possible. . Accessibility could be tied with unit tests to a developer. To some, it’s an afterthought. Something that we can just quickly do at the end of the development cycle and no one will know. Yet, when you reach the end of the development cycle, suddenly you have this mass of untested and often, untestable code. So you’re faced with the decision of just ignoring tests or refactoring and making it testable. . Accessibility is no different. That’s why it’s important to have a consideration and a strategy for accessibility during the development phase of the application. You don’t want to find yourself nearing completion of a project when the accessibility audit happens, only to find that there’s a huge number of issues that need to be resolved, with each change bringing the potential to introduce regressions, as markup and implementation may need changing in order to facilitate these accessibility requirements. . When we don’t follow best practice for accessibility we alienate and prevent users from being able to interact and use the applications that we have developed. From a business perspective, that can mean a loss of potential sales due to restricting the amount of people that can use the application. Ultimately it’s the users that suffer, as they are unable to effectively use applications and services that they may be required to. . Just looking at UK statistics, the  UK Government Family Resources Survey  has reported that 1 in 5 people have reported a disability, in raw numbers that’s 13.9 million individuals, a rise from 11.9 million in 2013/14.  These numbers include individuals with mobility, visual, and auditory disabilities. With such an increase just in the UK, there is a higher demand for accessible applications and websites. For some disabilities, access to the web is done through assistive technologies, and it’s important we take these into consideration whilst we build and develop our applications. . Accessibility is a broad topic and it’s important that applications and websites are thoroughly audited for any major accessibility short falls, but it’s important to tackle the problem at the source, which is during the design and development phase. I want to focus on the development phase in this post. This list is in no means a complete fix, but it’s a step in the correct direction to encourage developers to be more considerate about the people who will be using the application. Here are some initial things that developers could take on board which would begin to tackle the issue. . Good CSS can make bad markup invisible to the average website visitor. However, no amount of styling will make bad markup more meaningful to a computerized visitor such as a search engine web crawler, browser translation tools, or assistive technologies such as screen readers.    html.com   . This quote is an important one to have in consideration when building web applications. For the average user, they won’t be able to tell the difference between use of non-semantic tags such as  &lt;span&gt;  to display a label and using the correct  &lt;label&gt;  tag, but for those who use assistive technologies, such as a screen reader, the  &lt;span&gt;  tag conveys no meaning or purpose, where as the  &lt;label&gt;  tag offers various attributes that build meaning that the screen reader can understand and relay. . A major starting point in accessibility, is ensuring that the HTML that you are writing for your web applications is as semantic as possible. For example, using  the  &lt;button&gt;  tag instead of a  &lt;div&gt;  for a button, not only makes the markup clearer for developers, but also introduces a bunch of built in accessibility features for the button. . With HTML5, we also have access to different layout tags, this helps define areas of the page, without having a tonne of nested  &lt;div&gt;  tags to create the layout. Tags such as  &lt;article&gt;  and  &lt;nav&gt;  provide clear descriptions of their regions, for example, a  &lt;nav&gt;  would contain navigation, a  &lt;main&gt;  would be used once to declare the focal content of the page. . Accessible Rich Internet Applications (ARIA), are a set of attributes that are available to provide better accessibility to web content and applications.  When developing applications, it’s not always possible to use a semantic markup, for example, if you were to create and display a chart in HTML, there isn’t a tag that would fit. Using the correct ARIA tags, you can transform this markup to be useful to assistive technologies. . For example, if you’re using SVG, you could add an ARIA tag in order to portray to a screen reader what a user can visually see, by doing the following:   &lt;g aria-label=\"Item 1: 40%, 108\"&gt;&lt;path&gt;&lt;/path&gt;&lt;/g&gt; . This will tell the assistive technologies the information about the section of the chart (in this example, the item name, its percentage and raw value). . ARIA tags can also be used to describe the application to users of assistive technologies, tags such as  aria-hidden  or  aria-expanded  describe the state of the application, or using tags such as  aria-describedby  to provide assistive technologies with the correct content that goes with another piece of content on the page. When developing an interface, it’s always useful to ask yourself, if this needs and if this could be enhanced through the use of relevant ARIA tags . Roles are attributes that can be added to an element, for example, you could have a  &lt;li&gt;  tag, that is sitting inside a menu, adding a role to make it  &lt;li role=\"menuitem\"&gt;  will show that this element is inteded to be a menu item. Attaching roles onto any element will provide assistive technologies with information on how to handle a specific element. .  Further reading for more information on roles and supported ARIA tags.  . At  UpFrontConf this year , there was an insightful talk on accessibility given by  Laura Carvajal , which discussed the concept of keyboard testing during the development stage of an application.  The process is quite simple, it involves unplugging a mouse from the device so you’re forced to navigate and use the application using only the keyboard. This encourages thought into page layout and flow during the development phase, to ensure a concise and natural feel for those who rely on a keyboard to interact with the application. . Other should-do’s for developers to help keyboard users include; adding  :focus  styles in order to convey feedback to those using keyboards for when they are focusing elements that are tied to actions inside the applications, for example, if you have an element that would have a  :hover  state, then the  :focus  state should mimic the feedback that the  :hover  state conveys. In addition to this, custom elements that are created for the applications that are intended to be interacted with, should have a  tabindex  property, this will ensure that the element has the ability to be focused via a keyboard. . With the immense and ever-growing amount of content thats available on the web, it’s important that this content can be accessed by as many people too. One of the major types of content consumed on the web is video. Just looking at YouTube,  over 1.9 billion users visit YouTube every month globally, and every day, over a billion hours of content is consumed by users.  That is a massive amount of content, just on one platform. . If you are creating or displaying video on your application or website, adding captions should be of top priority. Platforms like YouTube offer tools directly to add them to your video, but it’s also extremely easy to add captions if you are showing video that is self hosted and embedded via  &lt;video&gt; tags. . First, you need to create a webVTT ( .vtt ) file, a basic webVTT file looks a bit like the following: . In order to get these into your video, you should upload them with your video, and then add them to your video markup like this: . Following this simple step will allow your video to be accessible to those who are hard of hearing or deaf, you can also style captions in order to adapt them to the style of your application, should you wish. For more information on webVTT, check out the  w3 specification.  . There is so much more to delve into for accessibility, and these four topics I’ve mentioned can be explored with so much more depth, but what I want readers to take away from this post is a mindset that will start to question how their applications are developed so that they can make improvements that incorporate thoughts and considerations for accessibility, and using the above aspects as a starting point. The web is a fantastic and connected place to share knowledge, entertainment and so much more, let’s ensure that this content is available to as many as possible. ", "date": "2018-11-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "A framework for progressively decoupled Drupal\n  ", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/spalp/", "abstract": " A lot of people have been jumping on the headless CMS bandwagon over the past few years, but I’ve never been entirely convinced. Maybe it’s partly because I don’t want to give up on the  sunk costs  of what I’ve learned about Drupal theming, and partly because I’m proud to be a  boring developer , but I haven’t been fully sold on the benefits of decoupling. . On our current project, we’ve continued to take an approach that Dries Buytaert has described as  “progressively decoupled Drupal” . Drupal handles routing, navigation, access control, and page rendering, while rich interactive functionality is provided by a JavaScript application sitting on top of the Drupal page. In the past, we’d taken a similar approach, with AngularJS applications on top of Drupal 6 or 7, getting their configuration from  Drupal.settings , and for this project we decided to use React on top of Drupal 8. . There are a lot of advantages to this approach, in my view. There are several discrete interactive applications on the site, but the bulk of the site is static content, so it definitely makes sense for that content to be rendered by the server rather than constructed in the browser. This brings a lot of value in terms of  accessibility ,  search engine optimisation , and  performance . . A decoupled system is almost inevitably  more complex , with more potential points of failure. . The application can be developed independently of the CMS, so specialist JavaScript developers can work without needing to worry about having a local Drupal build process. . If at some later date, the client decides to move away from Drupal, or at the point where we upgrade to Drupal 9, the applications aren’t so tightly coupled, so the effort of moving them should be smaller. . Having made the decision to use this architecture, we wanted a consistent framework for managing application configuration, to make sure we wouldn’t need to keep reinventing the wheel for every application, and to keep things easy for the content team to manage. . The client’s content team want to be able to control all of the text within the application (across multiple languages), and be able to preview changes before putting them live. . There didn’t seem to be an established approach for this, so we’ve built a module for it. . As we’ve previously mentioned, the team at Capgemini are strongly committed to supporting the  open source  communities whose work we depend on, and we try to contribute back whenever we can, whether that’s patches to fix bugs and add new features, or creating new modules to fill gaps where nothing appropriate already exists. For instance, a recent client requirement to promote their native applications led us to build the  App Banners module . . Aiming to make our modules open source wherever possible helps us to think in systems, considering the specific requirements of this client as an example of a range of other potential use cases. This helps to future-proof our code, because it’s more likely that evolving requirements can be met by a configuration change, rather than needing a code change. . So, guided by these principles, I’m very pleased to announce the  Single Page Application Landing Page module for Drupal 8 , or to use the terrible acronym that it has unfortunately but inevitably acquired, SPALP. . On its own, the module doesn’t do much other than provide an App Landing Page content type. Each application needs its own module to declare a dependency on SPALP, define a library, and include its configuration as JSON (with associated schema). When a module which does that is installed, SPALP takes care of creating a landing page node for it, and importing the initial configuration onto the node. When that node is viewed, SPALP adds the library, and a link to an endpoint serving the JSON configuration. . Deciding how to store the app configuration and make all the text editable was one of the main questions, and we ended up answering it in a slightly “un-Drupally” way. . On our old Drupal 6 projects, the text was stored in a separate ‘Messages’ node type. This was a bit unwieldy, and it was always quite tricky to figure out what was the right node to edit. . For our Drupal 7 projects, we used the translation interface, even on a monolingual site, where we translated from English to British English. It seemed like a great idea to the development team, but the content editors always found it unintuitive, struggling to find the right string to edit, especially for common strings like button labels. It also didn’t allow the content team to preview changes to the app text. . We wanted to maintain everything related to the application in one place, in order to keep things simpler for developers and content editors. This, along with the need to manage revisions of the app configuration, led us down the route of using a single node to manage each application. . This approach makes it easy to integrate the applications with any of the good stuff that Drupal provides, whether that’s managing meta tags, translation, revisions, or something else that we haven’t thought of. . The SPALP module also provides event dispatchers to allow configuration to be altered. For instance, we set different API endpoints in test environments. . Another nice feature is that in the node edit form, the JSON object is converted into a usable set of form fields using the  JSON forms library . This generic approach means that we don’t need to spend time copying boilerplate Form API code to build configuration forms when we build a new application - instead the developers working on the JavaScript code write their configuration as JSON in a way that makes sense for their application, and generate a schema from that. When new configuration items need to be added, we only need to update the JSON and the schema. . Each application only needs a very simple Drupal module to define its library, so we’re able to build the React code independently, and bring it into Drupal as a Composer dependency. . The repository includes a small example module to show how to implement these patterns, and hopefully other teams will be able to use it on other projects. . As with any project, it’s not complete. So far we’ve only built one application following this approach, and it seems to be working pretty well. Among the items in the issue queue is better integration with configuration management system, so that we can make it clear if a setting has been overridden for the current environment. . I hope that this module will be useful for other teams - if you’re building JavaScript applications that work with Drupal, please try it out, and if you use it on your project, I’d love to hear about it. Also, if you spot any problems, or have any ideas for improvements, please get in touch via the  issue queue . ", "date": "2018-12-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Project Management Methodology  – Probably a necessary evil for large Agile IT Projects\n", "author": ["Imran Khan"], "link": "https://capgemini.github.io/agile/project-management-methodology-agile-projects/", "abstract": " For me, who has been practicing the  Agile  in the IT industry for much of my career, earlier project management methodologies appeared as a big evil monster which try to control everything. After recently being certified as a  PRINCE2 Practitioner , I felt like sharing current understanding and experience in a light read with the engineering community which may benefit large enterprise level Agile IT projects. . When looking into PRINCE2, the very first challenge was that it all appeared like geared towards  Waterfall  and a lot focused on control, for example its principle about  Manage by Exception . The other challenge was, some really confusing terms like  Configuration Item  which can easily be termed as an artefact, basically anything (document, software etc which requires a version and status). Another example of confusing term was  Product . In PRINCE2, Product can be of two types i.e.  Management  or  Specialist . In essence  Management Products  are really documents created to support project like  Project Plan  and  Specialist Products  are parts of the final Product and would be handed over to the customer like software etc, though it can be documents as well. . Initially, there was a time when the dominant thought was that PRINCE2 may not be suitable for an Agile IT project. However, after much experience and analysing its potential application to some previous experiences, a light bulb was lit up that a tailored version  (Tailor to the Environment)  of PRINCE2 may be a necessary evil for the large enterprise Agile IT projects. . Let’s start with some of my experiences about what a typical large Agile IT projects lacks, for example lacking a coherent structure for the whole of the project. It is not to say that projects have no structure at all but more to say that it misses the coherent element and an end to end view of the project. Following are some specific areas: . i.\tWhen talking about enterprise level IT projects, Agile does not really discuss all the roles and responsibilities needed for a project. While  Scrum  is a good approach to structure a delivery team, it discusses less about large enterprise project team. For example, if the  Scrum Master  is not really responsible for the delivery then who is really responsible for the delivery of the artefact? Who to go to when there is an issue? . ii.\tOn some projects, teams appear a bit distracted and appear disjointed towards the main product and end up spending time on less valuable artefacts or trying to  fix a non-existent problem . Sometimes it leads to duplication of efforts or other times creating some unnecessary artefacts which are considered reusable, but  no one really reuses them . In some cases, it leads to release nightmare as dependencies are less understood at an earlier stage. A number of companies and projects try to use the mechanism of Scrum of Scrums or something similar, but does it really work? . iii.\tWhile inherently Agile is supposed to deliver software as needed by the business but occasionally on long-running Agile projects, the technical delivery may be very good, but business may appear less interested in using it. One of the examples is of a large public sector project which was delivered on time and budget but never went live. . iv.\tAgile also seems to miss some other cool bits related to project management. For example, there is a principle about reflecting at regular intervals to become more effective which is typically done as  retrospective ceremony  but there is little or no attention to the lessons from previous projects and how learning can be applied to the future projects. . v.\tDocumentation and reporting progress are also challenging on an Agile project. Agile is heavily influenced with the  XP-Extreme Programming . I am an advocate of XP but probably not at the cost of the project or post project life cycle of the application. No documentation may appear very compelling to some during the project but may not look so compelling when looking at the end to end life cycle of the product. There can be situations where you have just code with a sea of fragmented wiki pages. For example, on one project we just had compiled Java code with even no source. Did it really help the next project or was it used more as a kind of mechanism to lock the client so no one else knows how it works and let the client suffer should they decide to go with a different provider? . PRINCE2 seems to appear as a good methodology to manage above mentioned gaps in on a large Agile IT project. For example, it provides a solid structure for the end to end life cycle of a project which helps not only the management team, but it can also help the development and delivery teams to see the relationship with other components and see the non-technical bigger picture. I think PRINCE2 can nicely fill above mentioned gaps: . i.\t Define Roles and Responsibilities  and  Manage by Exception  principles and related concepts offers well-defined roles and responsibilities which can help to structure project teams relatively better. For example, for a large Agile IT project, a Scrum Master can facilitate a delivery team and a  Project Manager  can act as a  Team Manager . It would allow, the team to focus on the development, a Scrum Master to facilitate them, a Project Manager to take the responsibility of delivery and an  Executive  to take the responsibility of the overall  Business Case . It would put less strain on the Project Manager to facilitate technical aspects as one may not be technical at all. On the other hand, less strain on the Scrum Master as one does not have to be responsible for the delivery but purely facilitation (A true Scrum Master which I have rarely seen in the practical life). This also nicely address the challenge of whom to go to when there is a problem. Please note these are roles and not the person. One person may fill different roles, though some roles cannot be combined. . ii.\t The  Focus on the Product  principle and related concepts like  Project Product Description, Product Breakdown Structure, Product Description and Product Flow  allows keeping team focused on what is needed. It also allows seeing how these teams contribute to the overall Business Case for the project. Even an Executive can see the need of any team and how long it is needed for from a very high level. There is absolutely no harm in using  Jira  or similar tools to create the same structure but in this instance links are clearer, priorities are more organized, and team has more structured view of the end  Product  and its link to the Business Case. By all means, it does not suggest that you should describe all details up front before beginning of the project as it would just kill Agile, however you have an earlier structure in place and a  Product Description  can be matured at even the  Work Package  level. This also helps to map Product for the release cycle and see what is needed for what stage. . iii.\t Continued Business Justification  and  Manage by Stages  principles can really help with the projects like large public sector project mentioned above which never went live. While teams Focus on Product and delivery, at a higher level management keep evaluating the business justification at each stage. Should a Product fail to justify its value to the business at any stage, it is relatively easier to identify associated risk and allocated people and resources. It helps management to justify its continuity or premature termination. In other words, Products are only continued when it makes sense to continue not only at the micro (team level) but also at the macro (corporate management level). . iv.\t Learn from Experience  is a core principle of PRINCE2. It mandates and encourages to not only actively capture lesson in the  Lesson Log  and reflect or refer them in the  End of Project Report  but also apply lessons from previous projects at early stages of the project during the  Starting Up a Project  process. . v.\tPRINCE2 allows mature structure around mandatory documentations like  Risk Register ,  Quality Register ,  Check Point Reports  and in some cases provides a template to start with. It would also help to sort out pain of management reporting. The structure of Project Product Description, Product Breakdown Structure, Product Description and Product Flow, their related Work Package structure and stages allows management to assess the project progress more rigorously. As mentioned earlier, there is absolutely no harm to use tools like Jira for such structure. It is more about structure and reporting rather than what tool you use. Jira can be configured in a way which produces the statistics and reports needed by the management like Check Point Reports. It would allow Project Manager and teams to focus on the delivery rather than wasting much precious time on reporting. . In summary, I see project structure is a necessary evil for the large enterprise Agile IT projects. PRINCE2 appears as a good methodology to provide such structure, though it should be tailored (Tailor to the Environment) to suit the project as needed while maintaining the integrity using its 7 core principles. ", "date": "2019-01-24T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "How to update data models in Drupal 8\n  ", "author": ["Tijani Nasser"], "link": "https://capgemini.github.io/drupal/How-to-update-data-model-on-drupal-8/", "abstract": " In this article we will see how to update data models in Drupal 8, how to make the difference between model updating and content updating, how to create default content, and finally, the procedure to adopt for successful deployments to avoid surprises in a continuous integration/delivery Drupal cycle. . Before we start, I would encourage you to read  the documentation of the hook  hook_update_N()   and to take into account all the possible impacts before writing an update. . Updating the database (executing hook updates and/or importing the configuration) is a very problematic task during a Drupal 8 deployment process, because the updating actions order of structure and data is not well defined in Drupal, and can pose several problems if not completely controlled. . It is important to differentiate between a contributed module to be published on drupal.org aimed at a wide audience, and a custom Drupal project (a set of Drupal contrib/custom modules) designed to provide a bespoke solution in response to a client’s needs. In a contributed module it is rare to have a real need to create instances of configuration/content entities, on the other hand deploying a custom Drupal project makes updating data models more complicated. In the following sections we will list all possible types of updates in Drupal 8. . The Field module allows us to add fields to bundles, we must make difference between the data structure that will be stored in the field (the static  schema()  method) and all the settings of the field and its storage that will be stored as a configuration. All the dependencies related to the configuration of the field are stored in the  field_config  configuration entity and all the dependencies related to the storage of the field are stored in the  field_storage_config  configuration entity. Base fields are stored by default in the entity’s base table.   . Configurable fields are the fields that can be added via the UI and attached to a bundle, which can be exported and deployed. Base fields are not managed by the  field_storage_config  configuration entities and  field_config . . To update the entity definition or its components definitions (field defintions for example if the entity is fieldable) we can implement  hook_update_N() . In this hook don’t use the APIs that require a full Drupal bootstrap (e.g. database with CRUD actions, services, …), to do this type of update safely we can use the methods proposed by the contract  EntityDefinitionUpdateManagerInterface  (e.g. updating the entity keys, updating a basic field definition common to all bundles, …) . To be able to update existing data entities or data fields in the case of a fieldable entity following a modification of a definition we can implement  hook_post_update_NAME() . In this hook you can use all the APIs you need to update your entities. . To update the schema of a simple, complex configuration (a configuration entity) or a schema defined in a  hook_schema()  hook, we can implement  hook_update_N() . . In a custom Drupal project we are often led to create custom content types or bundles of custom entities (something we do not normally do in a contributed module, and we rarely do it in an installation profile), a site building action allows us to create this type of elements which will be exported afterwards in yml files and then deployed in production using Drupal configuration manager. . A bundle definition is a configuration entity that defines the global schema, we can implement  hook_update_N()  to update the model in this case as I mentioned earlier. Bundles are instances that persist as a Drupal configuration and follow the same schema. To update the bundles, updated configurations must be exported using the configuration manager to be able to import them into production later. Several problems can arise: . If we add a field to a bundle, and want to create content during the deployment for this field, using the current workflow (  drush updatedb   -&gt;   drush config-import  ) this action is not trivial, and the hook  hook_post_update_NAME()  can’t be used since it’s executed before the configuration import. . The same problem can arise if we want to update fields of bundles that have existing data, the hook  hook_post_update_NAME()  which is designed to update the existing contents or entities will run before the configuration is imported. What is the solution for this problem? (We will look at a solution for this problem later in this article.) . Importing default content for a site is an action which is not well documented in Drupal, in a profile installation often this import is done in the  hook_install()  hook because always the data content have not a complex structure with levels of nested references, in some cases we can use  the default content module . Overall in a module we can’t create content in a  hook_install()  hook, simply because when installing a module the integrity of the configuration is still not imported. . In a recent project i used the   drush php-script   command to execute import scripts after the (  drush updatedb   -&gt;   drush config-import  ) but this command is not always available during deployment process. The first idea that comes to mind is to subscribe to the event that is triggered after the import of the configurations to be able to create the contents that will be available for the site editors, but the use of an event is not a nice developer experience hence the introduction of  a new hook  hook_post_config_import_NAME()   that will run once after the database updates and configuration import. Another hook  hook_pre_config_import_NAME()  has also been introduced to fix performance issues. . To achieve a successful Drupal deployment in continuous integration/delivery cycles using Drush, the most generic workflow that I’ve found at the moment while waiting for a deployment API in core is as follows : .   drush updatedb                 hook_update_N()  : To update the definition of an entity and its components          hook_post_update_N()  : To update entities when you made an entity definition modification (entity keys, base fields, …)           .  hook_update_N()  : To update the definition of an entity and its components .  hook_post_update_N()  : To update entities when you made an entity definition modification (entity keys, base fields, …) .  hook_pre_config_import_NAME()  : CRUD operations (e.g. creating terms that will be taken as default values when importing configuration in the next step) .   drush config-import   : Importing the configuration (e.g. new bundle field, creation of a new bundle, image styles, image crops, …) .  hook_post_config_import_NAME() : CRUD operations (e.g. creating contents, updating existing contents, …) . This approach works well for us, and I hope it will be useful for you. If you’ve got any suggestions for improvements, please let me know via the comments. ", "date": "2019-02-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "The Conservation of Complexity in Software\n  ", "author": ["Nigel Hamer"], "link": "https://capgemini.github.io/architecture/The-Conservation-of-Complexity-in-Software-Architecture/", "abstract": " In physics, the law of  conservation of energy  states that the total  energy  of an isolated system remains constant—it is said to be conserved over time.  Energy  can neither be created nor destroyed; rather, it transforms from one form to another. In IT I postulate that there is another similar law that we all should get familiar with: . The   conservation of complexity   states that the total   complexity   of an isolated  business / IT  system remains constant – it is said to be conserved over the lifetime of the solution. Complexity can neither be created nor destroyed; rather, it transforms from one form to another. . If energy is the ability to do work, or the ability to move or elicit change, then complexity (in this context) is the resistance to change in a system. This resistance causes the effort and time to implement a change to increase which in turn increases the cost of sustaining and enhancing a system. How do I come to that conclusion? . The following diagrams model the way in which system architecture has developed over the last few decades. Each diagram represents the same generic business problem solved through differing architectural styles. Monolithic systems have given way to client/server or tiered designs which have in turn fallen out of favour to be replaced by service architectures whether the  SOA  or the Microservices kind. . Each progression above can be classed as simpler than its predecessor. Components and then services have been introduced as a means to compartmentalise business logic in order for it be reused or replaced altogether. The drive to Microservices takes this further. It is the manifestation of the  Single Responsibility Principle  at architectural level. Building a service that does exactly one thing well is much easier than trying to weave that code into a monolith. There are no distractions and it is straightforward to articulate the acceptance criteria or what done looks like. However, one service does not make a solution, so what is the impact to the overall system complexity? . Layering on inter-component or inter-service interaction on to the client/server and Microservices models above highlights that the complexity has shifted to the networks and communication channels between the components that make up the system. It seems that the client and server components are complex to build but simple to connect, whereas Microservices are simple(r) to build but more complex to interconnect. . Building communication networks between components adds another layer of complexity. Nodes in the network need wiring together and managing. Security becomes more of a concern as the traffic travelling on the network needs protecting. More network hardware is introduced, and someone has to manage it. It may be easier to test each component individually but how do you know that the system in its entirety is working? When things go wrong how do you pinpoint the cause? . The complexity has simply been relocated from the software and code into the network and solution management. . People have roles to play in complexity, after all in many software architectures, people are a fundamental part of the system. . In the early days of automating business processes using computers, the software often played the part of a glorified filing cabinet. Records are accessed in the system, they are reviewed, changed if necessary and then pushed back. The users and the software are working together to achieve some business activity, often with users holding relatively complex processes in their heads. Sometimes the people using the system act as an integration layer. Data is read from one system and keyed in manually to another. . As activities are automated and the burden on people is reduced the complexity moves into the system. New user interface styles are designed and built so users can be more efficient. Workflow systems are introduced that allow humans and systems to communicate more effectively. . In essence complexity has been moved into the software to make life easier for the users. The complexity of the complete system has not changed. . Even in a world where DevOps is gaining popularity, it is still typical for software to be born in a large-scale delivery project, at the end of which it is transitioned into support where it is run until it is no longer required. In my experience these large delivery projects are where large-scale investment takes place and where attention is focused. However even the best plans need to be changed and scope is often reduced. Business functionality is prioritised over operational requirements and before you know it the software is in support, but it is complex to operate. . The complexity of the system has not changed. We have simplified the delivery timeline, but all the complexity has moved to the support team. More people are needed to run the system, more telemetry is required to understand what is going on and the solution is more expensive to operate. . As people responsible for the successful delivery of software we need to be aware of the consequences of our choices. Our jobs can be pressurised and it’s natural to try to make life simpler but the choices we make can have a wide impact. When we consider the complexity of the entire system, we can assert whether our simplifications are positive. We might be making life worse for the people who have to run our software or for the people using our software, or even our future selfs when we are called back to fix our software. ", "date": "2019-03-22T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "What is Code Coverage and Why It Should Not Lead Development\n  ", "author": ["Chris Burns"], "link": "https://capgemini.github.io/testing/What-Is-Code-Coverage-and-Why-It-Should-Not-Lead-Development/", "abstract": " Within the world of software development and delivery there are not only thousands of tools, frameworks and principles, but there are also many terms that have no concrete definition that tend to fluctuate depending on who you’re talking to. In my experience, it’s critical that everyone is on the same page when it comes to definitions – whether they agree or not. One of the hardest challenges about solving a problem or trying to implement a solution is the initial discussion of how it will be done, and this can be made even harder if the definitions of the terms being discussed are understood differently by everyone involved. For easier reading of this article, I will outline concrete definitions of included terms - it does not matter if you agree or disagree with these definitions, the key factor is that you understand the principles this article outlines. . Code coverage is essentially what it sounds like - the amount of code that is covered in execution by a single test or collection of tests. As the word “tests” is an umbrella term for so many different types, for this article when the word “test” is used, it will refer specifically to  unit tests . This is because with unit tests you get quick feedback and it is very easy to identify what lines of code are covered, but when you start venturing further up the  testing pyramid  you will find it not only harder to see the lines covered but your feedback isn’t as fast. Having said that, the overarching principle that this article puts forward can also be somewhat applied to the different kinds of tests higher up the pyramid. . So, why do people care about code coverage? In a nutshell, because they feel it offers an assurance on the reliability of the code. In other words, if you know a line of code has been covered by a test, you may feel that you can safely assume that the code isn’t doing something quite naughty like logging request details and sending them to a private server somewhere in Switzerland. Although you would think and hope someone would spot that during a review, you would even more so hope that a test has been written to verify that behaviour is not expected or wanted. I mean how many times has a developer said  “Ahhh I didn’t even see that, oops” . Even in a scenario like logging request details and sending them to Switzerland, you wouldn’t want a mistake like that being made by a developer because when your company stock has fallen and customers are abandoning ship, the last thing you want to hear from a developer is  “oops, I must have missed my morning coffee haha” . . Also, slightly off topic, but this is another reason why Test Driven Development (TDD) is a popular testing approach, it makes you write the tests first as it follows a kind of  “I expect it to do this, so let’s write the code to make it do just that”  approach rather than a  “so the code does this, how do I write the tests to make them pass”  approach. The key thing to note is the tests and code are to be refactored and changed to evolve together rather than the old approach which is that the code cannot change, but the tests must be hacked to pass as  “the code works fine when I test it manually” . Additionally, the expectation of what you want the code to do and what it actually does are very tightly coupled throughout its development so it removes the chances of compromising test and code quality as after all, you wouldn’t normally go back to a Product owner and say  “my code doesn’t work with your requirements, so I’ve decided to change the requirements to fit the code” . Funny, but it’s kind of the same thing. Now that being said, code coverage is certainly something you should strive for when writing tests, because as said above you’d ideally want code verification – however, only striving for it may mean you are missing something crucial; whether the code meets the functional requirements. This can often be lost when writing tests, you get so caught up in trying to get a line covered that you forget that this line isn’t related to the scenario you are writing the test for. This is where the crucial point of this post is centred around. Code coverage does not equal test coverage. In fact, let me repeat and possibly rephrase.  HIGH CODE COVERAGE DOES NOT EQUAL HIGH TEST COVERAGE  . . As said in the abstract, there are normally many definitions and understandings people have relating to terms within the industry. So, in efforts to get everyone on the same page, lets ditch all what we know and say the following. Test coverage and code coverage are not the same. Test coverage can be measured by the following: . Mapping requirements to actual test cases . The status of the test cases . Code coverage (Includes code that directly or indirectly implements a requirement) . As you see test coverage and code coverage aren’t competing things and are often confused but it isn’t an either-or question, in fact they aren’t even synonymous. Code coverage exists under the test coverage umbrella. For an effective testing strategy, you need to primarily strive for test coverage which will include code coverage as one of its components. If you aim for a more complete test coverage, naturally, the code coverage will follow suit. However, according to the requirements. It’s a good rule of thumb to write tests that not only cover the code, but verifies them against the expected and wanted behaviour (requirements). This with the addition of testing each branch of code (branch coverage) should ensure that there is no stone left unturned in regards to the code paths. This then avoids the act of writing tests to cover code just for the sake of covering code and making the metrics look good. It is worth noting, I am not here to dictate how you structure your unit tests i.e. testing functions or features, how you arrange your unit tests is up to you. What I am suggesting is that in your tests, make sure that you are at least ticking off the other two criteria under the test coverage umbrella. This can be assured by verifying or asserting the codes behaviour against requirements and verifying that the tests pass. As a sub-comment, if you follow this approach, the assertions and verification will naturally occur as its one of the only ways you can verify that the code is doing what it is supposed to do in accordance to the requirements. . Let’s look at an example in somewhat pseudo code, let’s say you have a service that retrieves data from a database and depending on the content type flag, returns data in a JSON format or an xml format. Here is the code. . Now the code in general seems straight forward, there is a method called  retrieveData  that depending on its passed parameter, calls two different methods called  getJSONDataFromDatabase  and  getXMLDataFromDatabase . Now for the time being, the inner workings of those methods do not matter because we are only focused on the testing of the  retrieveData  function. . So now let’s declare two tests,  testDataIsJSON  and  testDataIsXML : . Now developers worth their salt could instantly pick up the problems with these tests but let’s just all look at it through the eyes of a beginner for a moment. These tests both test the  retrieveData  function and do so to 100% coverage. Try it, run the equivalent in your favourite language and you’ll find that 100% of the  retrieveData  code is covered. Now if we were to take the mentality of  “if it’s covered, we are good” , then we could commit this and feel safe because all tests pass and all code is covered, right? Not exactly. Remember when we said the inner workings of  getJSONDataFromDatabase  and  getXMLDataFromDatabase  didn’t matter? Well, they may not matter to the developer who would write those tests, but they sure as hell matter to the person who’s got to fix the bug in the code because the tests didn’t take into consideration that those two child methods may return null. Let’s say someone was to do some code changes to one of the child methods ( getJSONDataFromDatabase  and  getXMLDataFromDatabase ) and accidentally introduced a bug that  always made the methods return null? I mean, technically, according to the code coverage metrics, the code is covered and the tests pass, so there’s no problem, right? This is exactly why I said code coverage should not lead development, because now, if someone was to introduce a null bug, the tests would still pass correctly, and coverage also wouldn’t drop, resulting in a false sense of reassurance that nothing has gone wrong. This is also an example of  assertion free testing . . Another problem with this is that these tests do not act as a regression net. As mentioned above, if someone introduced a bug that broke production code, your tests would not fail therefore giving the sense that nothing bad has happened. There is where test coverage comes in. If you look at these tests, a scenario has been covered - to some degree, but where is the verification? You have no way of knowing if the tests test whether data is returned at all, and if it is returned, is it the correct data type. A valid test would be something like  “when data from a database is requested in JSON format, validate there is a response and it is the response you expected” . Now if we were to write valid tests with verification, we would do something like: . Now with that code the new assertions correctly verify the code is performing the way we want it to perform, the tests pass, and the code is covered, therefore meaning we have full test coverage (all three components of the tests coverage criteria has been met).  So, the crucial thing learnt here is code coverage does not automatically fulfil all test coverage criteria, as we have seen, it is possible that 100% code coverage can be attained without having any verification against requirements. However, with the second test, with 100% test coverage, 100% of the code is covered too. Now that was a simple example of how code coverage can give false security.  I know there are developers reading this thinking  “who would commit such atrocity” , but I can say, if an application has a lot of code, when things get messy and very complex, if correct testing principles have not been applied at the start, it becomes very hard to track current test coverage and the simple example above will present itself in a more complex way and it won’t be until you stay late on a Friday night that you finally realise that the tests don’t cover any valid scenarios further resulting in a decline in confidence in your testing suite. I have also seen in the past a test class that was hundreds of lines long, that had tests that were describing what they were testing – and not only were they not fully testing what they were saying, but they were actually testing other bits of functionality that other tests said they were testing, so it became some sort of a spaghetti codified test class that covered a high 90s percent of the class under test but had a very low number of real scenarios covered with verification. Scary stuff. . Please don’t walk away from this article thinking that I am trying to dictate to you about how you should write good unit tests, the overarching principle I am trying to convey here is to simply approach testing slightly differently. This can be achieved by trying to avoid aiming solely for high code coverage metrics and instead trying to aim more for high test coverage metrics by covering all 3 of its components. The examples I have laid out in this post are very simple scenarios where code coverage can be misleading whilst in parallel test nothing. For a more detailed breakdown of how code coverage metrics can be misused, please read  Brian Marick’s paper on this exact topic . My additional advice would be that, in future, if you ask someone if they have written the unit tests (assuming TDD wasn’t followed) and they reply with  “Yep, all tests have been written and 100% of the code is covered”  reply with  “is the test coverage 100%?”  if they look back at you with a confused face, this may reveal a bigger problem that needs to be addressed because the real kicker is that if you haven’t got 100% code coverage, you can live with that because more often than not it is just the somewhat harmless conditional statements that prevent 100% code coverage – some would even argue that aiming for 100% may indicate that someone is writing the tests to make the numbers look good and not actually thinking what they are doing. However, if you have even missed or not properly ticked off one of the three critical components of the test coverage criteria listed above, you are committing code that: . Is not tested and verified against the requirements . Isn’t successfully passing the tests . Hasn’t been covered by any tests . I hope that this article has presented a hard and solid proposition as to why code coverage  shouldn’t lead development and instead has presented a safer and more reassuring approach on how you can use the test coverage metric to help verify your solutions implementation. Any questions or queries please feel free to contact me on any of the socials. ", "date": "2019-04-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Musings of a female Software Engineer on International Women's Day\n", "author": ["Gayathri Thiyagarajan"], "link": "https://capgemini.github.io/engineering/musings_of_a_female_engineer_on_IWD/", "abstract": " On International Women’s day, I pause to think about the changes that has come about since the whole Active Inclusion and Diversity drive kicked off, not just within Capgemini but in the tech industry as a whole. . I wonder how it all came about, who had the Eureka moment and realised we need more diversity in the technology industry. But, it is not as dramatic as it sounds. Turns out, bad publicity from an  investigation into diversity  at workplace in Silicon Valley seems to have propelled the whole thing off. Of course, the rest of the world woke up to it much later and jumped on the band wagon. . Whatever be the reason, the effects and side-effects have been very interesting. For me personally, some have been encouraging, some alarming and others downright funny. . Historically, Engineering was never perceived as a profession for women but after the IT boom late last century and the innovation explosion of this decade, it is fast becoming a popular profession for women to take up. Casting my mind back, in the past 10 years, I have rarely worked in a project with more than 2 women engineers in a team. Certainly, the fact that there are fewer women in engineering seems to have gained enough attention and motivated many organisations to change it. . Diversity and Active Inclusion are like two sides of a coin. When you have a diverse workforce, how do you make them feel included? Diversity might be a good motivation for hiring from different pools of candidates, but without Inclusion, you can’t retain them. Inclusion is tricky because there are almost no ways to measure it. Check out  our engineering team’s Diversity Manifesto . . But, it’s not just an organisation’s initiatives that must drive these policies. Every individual’s action has an impact on the dynamics of the diverse and inclusive workplace we envision. . As a senior engineer, I morally feel obliged to contribute to this just as anyone else. When I am usually offered an opportunity to speak at a meet-up or a conference with an explicit statement to say that they need more women speakers, it makes me wonder. By taking this up, how am I helping the cause? I began to see myself as a catalyst or a stimulus for a better future and as long as I felt qualified to give a talk, that was good enough for me. Hopefully in few years, we would have achieved enough balance that we don’t have to hunt for women speakers any more. But I draw a line between that and being asked to just stand in a stall at an unrelated conference to create an illusion of having women in a team. . Inclusion is a different beast altogether. Signs for lack of Inclusion are very subtle and sometimes unconscious. Sometime people try to sound inclusive almost as an afterthought but it’s just downright funny. Imagine in a meeting, where there is a single woman in a room full of men and some one makes a statement - “Suppose the defendant had committed a burglary and does a run, what is the process for bringing him to the court …  or her to the court  ”, after a well intended nod in the direction of the woman. It’s really hard not to laugh. This is not inclusion. Instead of consciously using gender specific pronouns in a generic statement to imply inclusivity, we should do more to realise and avoid the unconscious bias we have towards people similar to us. . Nowadays, there are more WomenInTech meet-ups, events and conferences - places to meet women with similar interests, hear their inspiring stories and journeys and build a network. Not just at a macro level, we should have such networks within each organisation, build support structure and promote Diversity from within. There have been some great initiatives to bring women back after a career break, plenty of groups and academies aimed at training women in latest technologies and many more. There is definitely more happening  to promote female role models to follow in this industry. . I believe there is more momentum around these issues at the moment than a decade ago. It’s important to be mindful that every one of us has the power to reinforce or undermine the cause by our actions. ", "date": "2018-03-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Starting out in the world of IoT\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/starting-IoT/", "abstract": " IoT was a very daunting place for me when I started. After building an  IoT scale , I’m now slightly more knowledgeable. It’s been one of the most interesting bits of learning I’ve done for a while because you really get to understand how things work. . Lets start with the basics. I was using an  Arduino  board with an  Intel Grove Commercial kit . Arduinos are simple boards. They can be programmed in C and C++ with a very easy to use  online code editor  which compiles your sketch (a file with your custom code) and flashes your Arduino once compiled. Flashing is the process of writing your compiled code to the Arduino’s memory. Flashing overwrites the last flashed binary i.e. you can only have one piece of software on the Arduino.  Grove  is a plug and play system of sensors which can be attached to a base shield. Intel supports these sensors out of the box. These sensors communicate with the Arduino via digital and analog pins. . The Online Arduino Platform and editor now has  support for Intel Platforms  (e.g. an  Intel NUC  or  Intel Up Squared ). This allows you to write and upload code to your Arduino without plugging it in to your computer i.e. through “the cloud”. It even allows you to see what’s running and, shock, allows you to run multiple sketches at one time. How does it do this black magic? Your code is actually running on the Intel platform your Arduino is plugged into, not on the Arduino itself. The Intel Platform flashes the Arduino with  Firmata  when it starts up (hence why Intel recommends you plug your Arduino into the Platform before it boots). Firmata is a protocol for communicating with the IO on the Arduino and so the multiple sketches running on the Platform actually just communicate with Arduino through the Firmata protocol. Flashing the Arduino with Firmata essentially makes it a Firmata server and your Intel Platform is a Firmata client. . Firmata allows high level IoT libraries such as  Johnny Five  and the  Node Red  nodes to execute on a computer but interact with a connected Arduino. . The drawback of this is that the Firmata protocol has not implemented everything. It has support for a lot of whats needed - digital and analog reading and writing, serial communication,  I2C  (which LCD screens use) etc. but some protocols aren’t yet implemented - e.g.  SPI . This means you can’t use the Intel Platform to run code for a  HX711  (an analog to digital converter and amplifier) on a connected Arduino. In order to run something like the HX711 with an Intel Platform, the HX711 code has to run directly on the Arduino, avoid being flashed with Firmata (this requires either disabling the imraa service on the Intel Computer or pluging in the Arduino after the Intel machine has booted) and then using the Intel Platform as a simple gateway to the internet (rather than it running the Arduino sketches). This is because the Arduino can either run Firmata or it can run your compiled code from the Arduino web editor, not both. Firmata is required to use  Grove sensor plugins  or Johnny Five, so be aware you can’t use a mix of custom code and these high level libraries. This means you either only use sensors which are compatible with Firmata and therefore programmable in Node Red and Johnny Five or you get your hands dirty and write some c++. . While I was in the process of figuring out why the load cell of my IoT scale wasn’t “just working”, I learnt some things about physical debugging with a multimeter. This may seem obvious to some people but I’ll spell it out here to be explicit and because it’s interesting. Sensors are usually just changing electricity. They’re actually brilliantly simple and clever. For example a potentiometer (essentially a knob) takes in some voltage and outputs some voltage. How much voltage is output is down to the how much the potentiometer is turned - turning the potentiometer is just increasing the resistance in the circuit. Code that interacts with the pot is simply measuring how much voltage is returned. Amazing. This equates to: .  analogRead(pin)  is an Arduino function which reads from an analog pin and returns a number from 0-255.  A0  is an Arduino constant which corresponds to the 0th analog pin. As another example a load cell takes voltage in and outputs a very small amount of voltage based on how much the load cell is bent or compressed by using a  Wheatstone Bridge  pattern of resistors. Genius in its simplicity. The millivolts of output are amplified (by something like a HX711) and fed back into the Arduino. . All this means is that if you want to know if it’s physically working, you can measure voltages, resistance and continuity at different points in the circuit. But remember: measure voltage while the Arduino and sensor are connected to power, and measure continuity and resistance when it’s disconnected. Continuity and resistance checking injects an amount of voltage into the system which could overload the board or sensor. . Another interesting debugging technique is to  cat  the output from the device if it writes to a serial port with  Serial.println(\"something\"); . On a Mac, devices are attached to a file in  /dev/  on the filesystem (my Arduino was attached as  /dev/tty.usbmodem1421 ) and you can see what’s coming over by catting it:  cat /dev/tty.usbmodem1421 . . Connected flows of data can easily be created by using  Node Red , a graphical integrator that allows nodes, which input data, output data or do both, to be linked together. To communicate with the Arduino I used the serial node which takes text from the usb communication. This can then be wired to an  MQTT  broker, transformed by functions or even trigger a transaction on a  Hyperledger Composer  network. The Intel Gateway comes with Node Red installed to manage the Grove sensors so this can be configured to read from the packaged sensors and write to new endpoints. . Experimenting with IoT has been a steep learning curve but what I’ve learnt has really intrigued me. Firstly, how clever some of the low level hardware we use everyday is: buttons, potentiometers, the underlying protocols for how they communicate. Secondly I’ve been amazed at the reliability of the Arduino, it’s worked and booted flawlessly every time. I’d recommend exploring making some IoT sensors to anyone, hopefully this post will demystify some things and make the learning curve less steep. .  Intel IoT Gateway Documentation  .  A thorough walkthrough using the Intel Developer Kit with Azure  .  Johnny Five  .  MRAA the low level library used to interact with in-outs on boards  .  UPM, the library that uses MRAA, to give nicer high level APIs in multiple languages  ", "date": "2018-03-16T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Serverless, and the challenges using it\n  ", "author": ["Jastinder Singh Atwal"], "link": "https://capgemini.github.io/.net/serverless-and-using-it/", "abstract": " Recently I have been taking a look at serverless computing, trying to go beyond the headlines of why serverless is a good thing. Those headlines are something like “create your first serverless application in minutes”. While this isn’t untrue, there is more to consider when using a serverless architecture. For one, there is a server somewhere. It’s just that you don’t have to worry about it. All you worry about is your code, and your credit card. Your software is out there and it will scale as long as you can pay for it. . It’s worth noting that when you “go” serverless it’s a very different way to develop applications. This type of approach is very tightly coupled to the cloud. While this might seem like an obvious statement, there are some implications. This post isn’t about describing what serverless is. Instead, here I am noting down some of my initial observations of using serverless and what some of those implications are. The serverless application that I refer to throughout this post was developed as an  Azure Function , but  AWS Lambda  is another option. . So one benefit of serverless is the time taken to code up and make your software available. And it really does take minutes. A bit of code, a couple of clicks and your software is up and available on the internet. At this point you are being charged on a consumption model, so you only get billed for the compute you consume. Also, because it’s so quick to create, if it doesn’t work as you expect (for whatever reason) it’s easy to re-create. . But for me it’s about more than just the development. It’s setting up a automated, repeatable process that gives me assurance what I have done is correct and the ability to push changes across development, testing and production environments. So while I agree the coding is easy, I found the testing and some of the deployment to be a little tricky (more on this to follow). And while it took me a half hour to create the Function itself, it took me a week to look at options and come up with a solution that met my testing and deployment requirements. To make matters worse sometimes there is very little documentation to support what you are trying to achieve. Or the documentation is out of date because it’s a bit of a moving target. . The point is this - some of these things are very new. And that means you may need an innovative approach to achieve what you need. In the end that may cost you time. . Let’s put this into context. My Function App has two RESTful routes, and dependencies in the form of a NoSQL data store and a Queue. The Queue is used when calling one of those routes, so a message is published for some long background processing to happen without blocking the original call.  The code was taken from another post on Azure Storage for Serverless . What’s very nice is that the Azure Function does the hard work for me so that the interface into the storage and queue are passed in as input parameters to my method: . Here I am getting  Table  and  Queue  without doing anything. But when it comes to testing, some of these interface are not easy to mock ( Table ), and others can’t be mocked at all ( Queue ). In other words, I am in the Cloud, I have dependencies in the Cloud, some I can mock and some I can’t. But I still need the rigor that I would normally put around the software that I develop. This becomes even more important if you need to change or even re-create your microservice and then test it with a known baseline to ensure functionality doesn’t regress. . For me, I was really limited as to what I could do for the unit testing and while I did do a little, I got much better code coverage with my functional testing. This might not always be the case. You may have a simple serverless function and it might be quite easy to unit test it (is it just a  static  method). But I feel my example plays out a realistic scenario and it took a bit of thinking to come to a solution that worked for me. . So, how does functional testing work with Serverless?  There is some documentation as to how a function can be tested . But when it comes to how you can do this in an automated way, there doesn’t seem to be much in the form of guidance. So I set myself the following goals; I wanted to have a automated Dev Ops process to deliver my software and I wanted to have a continuous integration and continuous deployment process where functional tests could serve as a gate prior to promoting to other environments. I came up with: .   . This is a different model to conventional software testing because in that world everything probably exists on the build server. But here, I am putting my software out into a representative environment and then seeing how it behaves. In fact as a developer this proves to be quite useful because my feedback cycles are much tighter. I can identify and fix quicker than having to wait for a tester to feedback. . There is also the argument that if you can achieve good test coverage in a representative runtime environment with representative data, then why mock and unit test at all? The lesson learnt is that although I know that unit testing can be used for functionality in isolation, functional testing seems more natural and a much better fit for serverless. . Serverless and DevOps are made for each other. Lets take a look at how the serverless function is deployed. Note that this was achieved using  Visual Studio Team Services Release Management , but I have also heard that  Octopus deploy  is very good although I haven’t used it personally. .   . So the process removes what was there before (which is optional), creates a clean environment and then deploys the software to it. I have four environments; .   . To expand on development: .   . As well as deleting what was there prior to deployment, I as the developer take responsibility for the automated functional tests as part of the release. I like the way I can recreate from scratch and test in a clean environment. If it succeeds then the release system will automatically move forward and deploy to test. I don’t remove the test environment because testers may need to go in and inspect what is there. . How long does the development release take? In total ~ 4 minutes. That’s not that bad (even though I do try and aim for between 2-3 minutes). Most of the time is taken deleting the environment (~ 1 minute 30 seconds) and creating the environment (~ 1 minute 20 seconds). The deployment and testing takes ~ 1 minute (which is about right for my test environment where things are not recreated). This is a relative measure, as the components and complexity of your environment increase, so will your CI/CD time. But then this is an argument to not bundle too much in and keep resource groups simple (a benefit of the microservice). . One of the most frustrating things about this process is that intermittently the CI/CD process fails, for no reason other than it being in the cloud. Or, it takes a long time to complete (for example, a recent development release took ~ 14 minutes). It seems that these issues are mostly around the deletion and creation of the resource groups. But I like a consistent CI/CD process because I know that if it fails, it’s because of my code. Yes, over time I have learnt to identify what is the build and deployment system and what is me, but it still means I have to kick off another build. Again this all adds up in the form of time taken on failed builds. Also factor in collaborative working, where another developer comes to check in code and finds the build is broken, they spend some time investigating what could be wrong, kicks off another build and it just works! .  Its worth pointing out that Frameworks like Serverless provide an additional layer of abstraction for managing serverless applications . I haven’t used it myself but it could be worth investigating if this type of framework could be used to help out with deployment. . Serverless has some great benefits, one of which is that it accelerates application development. Functional testing is well suited to this architecture and the Dev Ops process works well. Your CI/CD system can create whole environments to meet your requirements. It’s worth going beyond just the application development effort for different types of serverless applications and cloud providers to consider things like how will you test the application, or what you will need to deploy. . The  source code  associated with this is available including the  ARM template  that is used to create the release environment. If you have any questions around the Visual Studio Team Services build process raise it here or create issue in the repository and I will be happy to help. ", "date": "2018-03-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Roslyn-Based .NET Code Analyser\n  ", "author": ["Ekhor Asemota", "Kriss Sulikowski"], "link": "https://capgemini.github.io/.net/roslyn-based-net-code-analyser/", "abstract": " While it is relatively easy to write code, it is not so easy to write high quality maintainable code. In this post, we introduce a technology -  Roslyn , which enables .NET software engineering teams to implement automated code reviews based on the skills which they already have. . The typical modern software engineering team is comprised of people with diverse background and experiences and as such, there is a need to implement some quality control mechanism which aims to ensure that the code developed by the team, is fit for purpose. One technique often used to achieve this is code review. Wikipedia provides the following definition for code review: .  Code review is systematic examination (sometimes referred to as peer review) of computer source code. It is intended to find mistakes overlooked in software development, improving the overall quality of software. Reviews are done in various forms such as pair programming, informal walkthroughs, and formal inspections     Wikipedia   . Within the Capgemini engineering blog, these two posts,  Better Learning Through Code Reviews  and  What to look for in a code review , provide more information about code reviews. . While automated code reviews generally result in more reliability when compared to manual code reviews, they often introduce licence costs, significant customisation efforts and lack of project specific context. . With the introduction of Roslyn-based .NET code analyzers, it is now quite straight forward for any .NET developer to develop code analyzers which are able to target specific projects. In addition, such an analyzer will be enforced in real time within most standard .NET integrated development environment (IDE) such as Visual Studio. Therefore, code issues are detected as the code is being written by developers which results in minimizing the cost for potential rework. . Roslyn is the code name for the .NET compiler as a service offering and prior to Roslyn, source code compilers operated as black boxes which accepts source code as input. Upon compilation, either a binary is produced for successful compilation or error(s) when the compilation fails. This is however different with Roslyn. . In addition to the general source code compilation performed by compilers, Roslyn provides “hook points” - events, through which subscribers can participate in the compilation process. For example, an event may be when Roslyn encounters a constructor or a method or even a variable within the source code.  Through such events, a subscriber which is bespoke code, otherwise referred to as a code analyzer, can obtain rich information about the input source code – full access to the entire syntax tree. . With this contextual information about the source code, it is possible for the analyzer to enforce specific rules on how code should be constructed. In addition, an analyzer can if necessary fail the entire source code compilation process and send relevant information back to the compiler which is subsequently displayed as part of the compiler log information. . Consequently, specialised analyzers can be developed to process very specific portion of a source code. For example, it is possible to develop an analyzer which fails the compilation process when any method has more than say 13 parameters or when a method has more than 100 lines of executable code. . Roslyn-based code analyzers are .NET libraries which can be developed in both C# and VB.NET. They can either be packaged as  Visual Studio extensions  or as  NuGet packages  which require no installation and apply only to the C# projects which depends on them. . This blog post on  Getting Started with Roslyn Analyzers  provides detailed information about Roslyn analyzers. . To develop a Roslyn code analyzer, the .NET Compiler Platform SDK must be installed on the development machine. This process is simplified if Visual Studio is the development IDE. . Open Visual Studio 2015/2017 -&gt; Select Tools &gt; Extensions and Updates. In the Extensions and Updates dialog box, select Online on the left, and then in the search box, type .NET Compiler Platform SDK. . This resource on  Getting Started with Roslyn Analyzers  provides more information on how to develop .NET code analyzers. . Once the SDK is installed, it adds a syntax visualizer window to Visual studio. This window is very useful when analysing code syntax tree. The Syntax Visualizer can be displayed via View &gt; Other Windows &gt; Syntax visualizer and the figure below shows a screen shot of this window. .   . Each code analyzer using the Roslyn SDK must inherit the base class  Microsoft.CodeAnalysis.Diagnostics.DiagnosticAnalyzer  and implement two fundamental steps: . Write a method that will perform the code analysis over a given syntax node . Register the action at the analyzer’s start-up so that the analyzer can respond to compiler events. . The registration is done within the Initialize method by invoking the  RegisterSyntaxNodeAction  method of the  AnalysisContext  parameter.  RegisterSyntaxNodeAction  requires a delegate method and a syntax type that will trigger the delegate method. For example, if RegisterSyntaxNodeAction has a second parameter of  SyntaxKind.ConstructorDeclaration , the delegate passed in as the first parameter will be triggered every time a class constructor is encountered by Roslyn. . The code sample below registers a  AnalyzeMethodDeclaration  which will be triggered whenever Roslyn encounters any method. . And the implementation of the delegate is shown below: . Once the code analyzers are implemented and tested, the project produces an analyzer .dll output. This output is then packaged into both a NuGet and a VSIX extensions. . The NuGet Package (.nupkg file) will add the analyzer to your target project as a project-local analyzer which participates in the project builds. This option is particularly suitable to a continuous integration workflow as there is no need to install any software. The package moves with your .NET project and a NuGet restore is all that is required before a build to ensure that the package is available to the project. . The VSIX extension (.vsix file) will install the analyzer library as an extension to Microsoft Visual Studio. As such, the analyzers are subsequently available to all projects opened with this instance of Visual Studio IDE. . Create a local NuGet feed . Copy the .nukpg file into that folder . Right-click on the project node in Solution Explorer and choose Mange NuGet Packages . Select the NuGet feed you created on the left . Choose your analyzer from the list and click install . To maintain a high-quality code base which ultimately delivers value within software projects, code reviews are an essential part of the development workflow. With the introduction of Roslyn-based code analyzers, the effort required to develop automated code reviews have been significantly reduced. In addition, these code analyzers have full access to the syntax tree and as such rich contextual information about the source code. . Furthermore, Roslyn analyzers can be deployed through NuGet which aligns very well to the continuous integration workflow. We have in the post presented a high-level overview for Roslyn analyzers. In subsequent posts, we intend to present detailed implementation of analyzers based on this technology that are currently been used within the Capgemini Microsoft business unit team to ensure a high level of code quality and consistency. ", "date": "2018-04-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A look at Cloud-Native Apps on Azure\n  ", "author": ["Ian Crow"], "link": "https://capgemini.github.io/cloud/cloud-native-apps-on-azure/", "abstract": " Organisations are using software as a key differentiator and source of competitive advantage. Whilst we often think about technology-led companies, such as Netflix and Uber, it is transforming all types of organisation. The cloud supports this transformation, and we are seeing big shifts to development of applications on cloud platforms. Whilst cost reduction may be a big factor in the move, it is the speed and flexibility provided by cloud services that companies can use to accelerate business velocity and growth. . The term  Cloud-Native App  is used to describe an application that is built for the cloud on the cloud. In this blog post we will talk about some of the characteristics of cloud-native applications and explore a framework for building them in the Microsoft Azure cloud. . By responding to change quickly companies can gain competitive advantage; building new environments in hours not weeks, releasing new features continuously and creating new business opportunities through innovation. Software systems should adapt easily to business demands. But how? . The term  Antifragile  is used to refer to software systems that deal with the disorder of change better. Cloud-native applications embrace the “antifragility” that the cloud has to offer, making extensive use of Platform as a Service (PaaS) managed services. In addition cloud-native applications are designed and built using architecture styles that better accommodate change. Among the characteristics of cloud-native apps are: .  Microservice-based  to significantly increase the overall agility and maintainability of applications with loosely-coupled, autonomous services that facilitate change. .  Containerized  to quickly move, enhance and scale applications with independent containers enabling modular development and deployment. .  Operate on managed platforms  to reduce cost of operation through efficient use of resources with high levels of resilience and scalability; and benefit from features such as auto-scaling to allow the system to change to demands. .  DevOps  to remove organizational and cultural inhibitors in a move toward continuous delivery and end-to-end automation for the application lifecycle; and reduce the risk of introducing new software into production even allowing for experimentation. . So now we hopefully have an idea of what cloud-native is and means to us, let’s look at what the Azure cloud offers to build cloud-native applications. . Azure cloud provides a range of compute services as shown in Figure 1. A key difference in these services is the level of control that can be exercised over the environment. Offset this against the effort required to design, set-up and operate it, and make applications resilient, scalable and supported. By giving over control of managing the platform to the cloud providers, we can meet the characteristics we set out for cloud-native apps and focus effort on the real business value of developing software. .  Virtual Machines  - An Infrastructure as a Service (IaaS) offering that provides maximum control over the hosting environment and support for legacy workloads. Consumers are responsible for operational activities such as server patching and monitoring. .  Virtual Machine Scale Sets  - Provides services on top of Virtual Machines where you need to deploy large numbers of identical servers with load balancing and auto-scale, reducing some operational overhead. .  Containers  - Provides traditional container orchestrators (Docker, Kubernetes, Mesos) as well as Microsoft’s Service Fabric for building managed microservices applications that are resilient and scalable, and support Linux and Windows platforms. .  App Services  - A fully managed and scalable Platform as a Service (PaaS) offering for Web, Mobile and API applications, which removes a lot of the management overhead, yet provides flexibility with support for multiple platforms (Windows/Linux) and languages (.NET, Node.js, PHP, Java, Python). .  Serverless  - Provides an on-demand and scalable execution model for coded functions in multiple programming languages, so that you pay only for the time the code is executing, from the point at which it is triggered to completion. . A detailed comparison of these compute services is beyond the scope of this article, but it should be apparent that our characteristics for cloud-native apps can be best met by Container, App Services and Serverless compute. They are managed services that support the ability to build microservice-based architectures in a containerized deployment model with support for DevOps tooling and automation. Whilst IaaS could be made to support these characteristics, a lot more effort has to be made in the set up and management, reducing our ability to quickly respond to change. . We should also distinguish between some of the Container offerings and the App Service and Serverless ones. The underlying infrastructure is less abstracted away with Container services and we still have some management overhead, for example patching the servers the containers run on. They do however have the advantage of being more portable across different cloud platforms if that is a concern for you. . In addition to the compute services already described, Azure offers a range of other managed services to enable development of end-to-end applications. .  Storage  - Managed storage for logs, document and media files (e.g. Blobs and SMB File services). .  Data  - Relational and NoSQL databases, caching and search services (e.g. SQL Azure, Cosmos DB, Redis Cache, Azure Search). .  Messaging  - Queues and subscriptions (e.g. Azure Service Bus). .  Security  - Authentication services (e.g. Azure AD). .  Network  - Traffic management, content delivery, load balancing and network virtual appliances. . Specialised or more advanced apps can make use of additional services for Artificial Intelligence, Analytics and Event driven architectures (such as IoT) and integration for hybrid scenarios. . Putting all of this together, figure 2 shows a framework for building cloud-native applications on Azure. Other cloud platforms do provide similar offerings. If you are not using Azure, hopefully this article has given you some things to look out for on your cloud platform to build cloud-native apps that respond to change and enable you to work at the pace your business demands. ", "date": "2018-04-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A design review checklist for non‑designers\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/development/design-review-checklist/", "abstract": " The world of web design and  development has changed enormously over the course of my career so far , and mostly for the better. In some ways, building websites has become much easier. In others, it’s become far more complex. While browser support for for web standards has improved, the range of device sizes out there makes aiming for pixel perfection a fool’s errand. Thankfully, it’s no longer such a common aim - there’s a more widespread acceptance, largely since the responsive design revolution, that websites won’t look the same in every browser. . In some ways, though, things haven’t changed that much. Sketch may have largely replaced Photoshop, and design deliverables are more likely to be Invision prototypes, but the process is often similar. The industry may have largely adopted agile, at least  superficially , but projects can sometimes feel like a series of mini-waterfalls, with a set of gates to pass through. Things will generally go something like this: After some research, a designer creates a mock-up, and shares it with the client (perhaps sharing some of the thinking that has gone into it). The client gives feedback, which may or may not be based on something more scientific than “I like it” or “I don’t like it”. Then the designer takes it back for a round of revisions, which is then presented to the client. This continues for a few iterations until the design is signed off, and then the developers build it. Once the developers think it’s ready, it’s tested, compared to the designs, and any discrepancies are logged as bugs to be fixed. . In an ideal world, this sign-off from the product owner would be an ongoing collaborative activity, based on looking at a web page, rather than a mock-up. Designing in the browser would be quick and easy enough that there’d be no need to throw designs over a wall, and the line between designer and developer would become more blurred, perhaps even disappear. For small projects, or minor changes, that can often be the case, but on large complex projects, there’s still usually a handover from designers to developers, and the design deliverables are often held up as the target for the developers to hit. In that series of waterfalls, there’s a danger that the designs will be viewed as stone tablets handed down from on high, so it’s vital that those designs are reviewed thoroughly before they’re signed off. . It’s always good to get another pair of eyes to sense-check your work - that’s why  I’m so keen on code reviews , and the same principle applies to design work. It’s easy to be so focused on one aspect of what you’re doing that you miss something else, and modern web design has so many facets to bear in mind that it’s difficult for one person to think of everything. . Perhaps the biggest reason for developers to review designs is to ensure that it will be technically feasible to build whatever it is that’s being designed. In the past, a big part of this might have been about persuading the design team not to use quite so many gradients, drop shadows and rounded corners, or at least to accept that  they wouldn’t be present in all browsers . These days, as with so many aspects of our jobs, things have evolved, and developers need to think of the bigger picture beyond the code. Developers may also have valuable insights into how best to build the designs into a working system. For instance, if you’re building with a particular framework, it may give you some components out of the box that might meet the needs of the client. If the design can incorporate these existing components, rather than needing everything to be custom-built, the project is likely to proceed much more efficiently. . We’re often building new functionality on existing websites, and our clients may work with multiple design agencies, which means that the person designing the new elements may not have been involved in previous phases, and may not be aware of the decisions that have been made in the past, or the discussions that informed them. Ideally there’s some kind of style guide to help ensure consistency, but design decisions from the past shouldn’t be set in stone. If the new designer is going off-piste, either through oversight or deliberate choice, it’s important to make sure that the decision is given due consideration, based on sufficient knowledge of those past choices. Often the code is the best source of information about past decisions. . Even on “greenfield” projects, it’s good to have developers involved as early as possible, partly because they might be able to suggest minor changes that can lead to major efficiencies, and partly because developers tend to approach things with a fairly modular way of thinking. . At Capgemini, developers collaborate closely with our colleagues from the user experience design team. On my current project, I’m lucky to be able to sit with our design team,  so we can talk things through very easily every step of the way. On other projects, the design work might be done by an external agency, or perhaps even in another country. One of the most important factors in the success of these projects has been getting developers and designers talking to each other as early as possible. . In my experience, the closer the working relationship between designers and developers, the more likely the project is to be successful - we’ve had much better results when we’ve been able to co-locate the teams and remove barriers to communication. It’s important to avoid a “them and us” scenario, where designers come up with ambitious ideas, and  developers are always saying no . . The reality is that there will always be changes as the project moves forwards. That’s the whole point of agile. During the project, and after go-live, we’ll discover reasons why a design will need to be tweaked, perhaps for technical feasibility, perhaps because we’ve realised that it would improve user experience. While a digital product may never be truly finished, it’s a lot easier to make changes when you’re still looking at prototypes and mockups than once you’ve built a large system.  So if a developer has objections to a design, or suggestions for improvements, the earlier you raise them the better. All too often, I’ve realised that a design needs changes only after it’s been presented to the client for sign-off. . With that in mind, here’s an attempt at a checklist of things for developers to think about when reviewing proposed designs. . If (as is so often the case) we’re building on top of something that already exists, we need to make sure that our new features follow any styles and conventions already established in the existing product. Even if we’re starting from scratch, it’s important to ensure that there’s internal consistency within our designs. . Are shared components, such as menus and branding, the same across all screens or pages? If there are different designs for different breakpoints or platforms, is there consistency between them? . I’ve bought into the ethos of  style guides, component libraries and design systems , but it’s natural that people want to see what those components will look like in context, so it can be difficult to escape from full page comps. The more that the designers have approached things from a component-based way of thinking, the more likely it is that the design will be consistent, but there’s always the possibility of typos or other errors. . Even if there’s not a component library, with large organisations, you’ll probably be working within a set of brand guidelines. If so, do the designs follow them? If not, is this project an opportunity to start building a style guide? . Have we missed out anything important? Or has anything crept into one of the designs, perhaps a leftover from a previous iteration? This shouldn’t generally be a deliberate choice:  “If content is relevant to users on one device, it should also be available on other devices and not hidden away” . . For instance, if there’s a login form with a “forgotten password” link, does the link appear before or after the form submit button? Whatever the ‘right’ answer is from a user experience point of view, it usually makes sense to keep the same answer throughout the project. . If there’s a grid system in use, does the new design follow it? Do the image sizes and aspect ratios match what’s there already? Are the aspect ratios the same at different breakpoints, or will there be a need to art direct responsive images? If so, does that cause extra complexity? . Hopefully you’re working with a set of brand guidelines, where the colour palette has been explicitly defined, and you have a list to reference. If not, perhaps the next best thing is a Sass variable list from the current codebase. Wherever your colours are defined, it makes sense to refer back to a list, and consider whether additions to that list are justified. . How many font sizes, weights, and variants are you using? Do you really need all of them? Is there a reason that the heading on this page is 1px bigger than an apparently equivalent heading on that page?  Again, ideally this would follow an existing style guide, but it’s the kind of area where it’s easy for inconsistencies to creep in, especially if multiple designers are working on a project, unless someone has taken the time up front to define the ground rules. . Where developers are often guilty of sacrificing end users’ needs to their own convenience, designers can sometimes focus too closely on the aesthetics of their work, and risk neglecting inclusivity. . This is a big subject, and I won’t be able to do it justice here, but a few things to keep an eye out for include: .  colour contrast  .  text size  .  form labels  .  excessive motion  . For some designers, it might be tempting to think that the words don’t matter in mock-ups, but  designs shouldn’t use Lorem ipsum . The words used should be consistent with the current site, consistent between designs, and consistent with the organisation’s tone of voice. They should also be inclusive, avoiding jargon or unnecessarily complex vocabulary. They should also be checked for spelling and grammar - sloppy mistakes may seem trivial, but they can make a terrible impression with clients that can damage the project. . What happens when a section has longer or shorter text than the designer has used? Will the CMS automatically resize images to the desired size, or can editors control the size of the image that is displayed? In short, is there a danger that editors (or other users) could break the design? While it may or may not be feasible to train content editors to follow guidelines about content (or even set hard limits in the CMS), end users will always do their own thing, and the design should be able to adapt to them. For example, if your system shows the logged-in user’s name in an effort to be more friendly, you’d better be confident that the design can cope with long names, and as Karen McGrane puts it,  “truncation is not a content strategy” . . A big reason for having developers (both front end and back end) involved as early as possible in the design process is to be able to apply knowledge of the internals of the system. For example, I recently worked on a project where the designer wanted to include a profile picture in the user details page. It looked great, but the system didn’t have a facility to add images to user profiles, and building it would have been too complex, given the intended project timelines. On another project, we wanted to show users the contact details of their account manager, but there wasn’t any feasible way that the website could get access to that information. . This is another enormous and complex subject area, and one that will only become more important with the introduction of GDPR. For instance, if you’re creating a marketing sign-up form, you’ll need to consider  how to appropriately confirm consent . . CSS is far more powerful than it used to be, and improved browser support for modern features means that we can create much more adventurous and ambitious designs than was possible a few years ago, without needing to resort to some of the hacks that used to be widespread. Having said that, we need to be careful not to forget that  “performance is user experience” , as Lara Hogan put it so effectively. . These early conversations are the time to think about setting a  performance budget  - once the client has fallen in love with the high-resolution fullscreen images, it’s too late. . While images usually have the biggest impact on performance, adding more fonts (or variants of the same font) is a very quick way of slowing down page load. How many fonts does your design actually  need ? Sometimes  embracing constraints  can help designers to do their best work. . I’ve only briefly touched on a few points here, and some of them are big enough to merit whole books, but hopefully this list will be useful to you in your projects. I’m sure that there are review points I haven’t thought of. After all, that’s the whole point of reviewing things. So if you have any suggestions, please let me know via Twitter or the comments section. ", "date": "2018-04-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Building for Alexa at Devoxx UK\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/development/voices-for-women/", "abstract": " For our Platinum sponsorship of Devoxx UK this year, we had a theme to align with our recruitment drive of “it takes all types to make a team”. One attraction that we had at our stall was an application taking you through the  Open Extended Jungian Type Scales  - an open-source equivalent of the Myers Briggs personality test. The project has 32 questions and an algorithm to condense your personality type into four scaled letters - are you Introverted or Extroverted, Sensing or iNtuitive, Thinking or Feeling, Judging or Perceiving. . We then roughly grouped the resultant 16 personality types into some common technology roles: . DevOps . Server Side Development . Front End Development . Technology Scout . Agile Coach . We had talented character designer  Finbar Coyle  draw up some cartoons of each role and had them made into stickers and T shirts to hand out.   This was a fun game, although I did offend one of our most senior devs by classifying him as a front-end developer! I thought it would be a good addition to build the quiz for  Alexa , so that “she” could walk you through the questions. . Alexa would make an excellent psychologist with her calm and measured tones – although, only the English and German speaking worlds. (alternatives are  Google Assistant  which supports nine languages,  Cortana  eight and  Siri  an impressive 24). Amazon has published its reasons for  choosing a female voice  for Alexa – basically driven by customer feedback across large trial groups. I wonder what kind of an effect this could have on the next generation of programmers? Will there be a kind of implied perception that women are naturally good at maths and logic because of it? I already compare poorly to Siri in my childrens’ eyes (“Siri knows the square root of 2 to six decimal places, Mum, why don’t you?”). Or will it create a more negative impact of women being the do-ers and not the thinkers? I hope some university research departments are braced to investigate. . I’d been to one of Amazon’s excellent  Dev Days , so I felt ready to build. The  documentation  and development environment for using the Alexa skills kit to create your own Alexa skill (A “skill” is the name of an Alexa command interpreter) are excellent – there is a browser-based IDE or you can write your own code and configurations and upload them to AWS, and plenty of tutorials and templates to get you started - although, in the way of many new languages and libraries, it does move very quickly and the demo you built last month will probably need updating now… . As expected with new domains, there is quite a lot of terminology to learn. The  Alexa Skills Kit  (ASK) allows you to build the “interaction model” for your skill – the part which defines the format and flow of the conversation your skill will create via Alexa. This is defined using a JSON format. Then there is the skill service - the back end where the computation related to the skill takes place. The easiest back end to adopt is an AWS lambda function, particularly as it removes the need for an SSL certificate. You can just provide the function UID to the Alexa skill and use Amazon’s OAuth tokens to secure.  Then there is the  Alexa Voice Service , (AVS) which represents the capture of the sound made by your voice, and the decision of which skill to direct it to. Devices such as  Amazon Echo  and  Sonos One  contain implementations of Alexa Voice Services with an always-on microphone listening for the keyword, or you can install a  software version  of Alexa Voice Services on your PC and trigger it to listen using a button click (I used a Java implementation with great success). . For myself as a back-end developer, the development of the interaction model is by far the harder to understand. Once the parameters from your user are processed by AVS and arrive at the back-end Lambda function, it’s easy to use the Alexa SDKs and implement any custom business logic. SDKs for AWS lambda are available in Java, Python, C# and Javascript (NodeJS).  The front end interaction model, as I mentioned, is  defined in JSON  and consists of three parts – a required language model object, an optional dialog object and an optional array of prompts. Let’s start with the language model as the most interesting piece. . The language model has an invocation name. which is how the skill will be triggered – it’s basically the bit that Alexa will listen for and use to route incoming voice requests to the relevant skill. So, if you name your skill “analyse me”, enable it for your device via your Amazon account, and then stated “Alexa, analyse me”, your skill would be launched by Alexa Voice Services. . Next in the language model are the  intents , presumably so-called because they are the pieces where you interpret what the user intends to do… Within an intent, you list out the various phrases that a user might attempt in order to interact at this stage of your skill flow. There are a number of built-in intents, for instance an  Amazon.HelpIntent , triggered when the user says “help” or similar, an  Amazon.YesIntent  triggered when the user says “Yes”,  Amazon.StopIntent  for when the user screams “ALEXA FOR GOD’S SAKE STOP!”.  The interaction model does not define the logic that is executed for each intent, that is done in the back-end service. It simply defines the intent with a name and lists the possible ways that the user can trigger it. This was the point for me where the usual “shattering of the magical illusion” of technology occurred. . We (developers) all know that technology isn’t magic, and that at the end of the day you have to build everything yourself and APIs/tools/frameworks simply help us along, but with something new and shiny like AVS there’s always the optimistic moment when you think “Ooooh, how clever IS this thing?”. . Of course, the answer is – very clever, but not THAT clever. When you say to Alexa, “turn the lights on”, and you get the same response as when you say, “switch on the lights”, it’s not because AVS contains super amazing linguistic skills which understand that these two phrases mean the same thing. It’s because a meticulous programmer has entered all the possible permutations of things you could say to turn on the lights into their intent model. This implies that, in order to avoid entering the entire thesaurus into your intent model, you need to be a bit clever with how you structure the conversation flow with Alexa. You need to move away from open ended questions such as, “What would you like to do?” and instead aim for the simplest responses possible, for instance, “Would you like me to turn the lights on?” Voice interface design is  already  a broad and fascinating discipline. . Another surprise for me was how difficult it is to collect an open phrase from the user. The parameters within a sentence that Alexa needs to collect are known as  slots . When creating your own intent, you add some sample “utterances”, using curly bracket notation to define the slots, and the type of the data within those slots. So for instance, in one of the Alexa sample skills which teaches you about US geography, the sample sentence to trigger the intent is  “Tell me about {CityName}” . The type of the  CityName  slot is set to the predefined  AMAZON.US_CITY , so that Alexa knows it is expecting a value from a predefined list. There are a bewildering array of  predefined slot types , across geography, entertainment, weather, dates, and on and on, and you are expected to find what you are looking for in the library. . The built-in intents are, of course, finite. Consider a skill that asks the user’s name. There is a predefined  AMAZON.US_FIRST_NAME , but it may not cover everything you require as you might want this slot to include nicknames, shortened names, non-English names, and so on - it ought to be able to have ANY value. Alexa has the concept of a  literal  to support this, currently  being replaced by the concept of a custom slot , but it’s an interestingly restricted concept. For each literal you have to define an example value. So I tried an AskNameIntent with a model like this: . This works fine if I say “My name is Ermintrude”, and it works fine if I say “Sarah”, but if I just say “Ermintrude”, the skill does not pick up the data. The reasons behind this appear to be fairly complex, to do with the way Alexa’s algorithm determines when a user has started and completed a sentence. Just noise which does not directly map data in the intent is apparently unacceptable. It does have the added by-product of making Alexa skills more data-safe and secure; it’s nigh on impossible for a custom skill to “spy” on you and submit your conversation into the cloud. There are further limits to the Literal/Custom Slot format which prevent skills being able to do things like store dictated notes. There is an 8 to 10 second hard cut-off when AVS listens for input, so even using the most carefully structured literal wildcard intent your musings will be cut off at the knees! . Once you’ve got to grips with intents and slots, you’ve basically got the hang of the interaction model. The next part of the model schema involves defining any custom types you may need - ones outside of Alexa’s built in library. For instance, for my psychometric test skill, in order to capture the answers I need to create custom types for every possible answer. So in my server-side code Alexa asks the question: . My Interaction Model has to define types that will match the user’s response. So I have a list of potential utterances with a custom slot type {Response}: . And then I define the Response type: . Again you can see how much non-technical thinking goes in to just one of the 50 questions in the psychometric test. This is going to be one big JSON file. . Finally we move on to the back end, and the Lambda function which will receive the information from my skill. Again, there are lots of  well-documented code samples  on  Github  to get you started developing the skill service and linking it to your interaction model. The one area that I found difficult to understand was how the concept of state could be used in between voice interactions to create a conversation flow. . For my psychometric test skill, I wanted to generate a value from 0 to 5 for each answer. To turn from a boolean (A or B) to this 1 to 5 scale, I decided to have Alexa ask the user how sure they were of their answer after each question, and then judge the positivity of their response. For instance, the flow might go like this: . (Q1) Alexa: “Do you get worn out by parties, or do they fire you up?”  (A1) User: “I get worn out by parties”.  (Q2) Alexa: “Would you always say that?”  (A2) User: “Well no, maybe I’m just tired today”. . Alexa “Okay. Now, next question… “ . And so on. I wanted to use state to let the Lambda function know whether it was listening for the question answer (A1) or the answer to the follow-up assertion (A2). So, I define three states in my function. . I then use the  Alexa.CreateStateHandler()  method to group logic based on state. . There is an  official API to define dialog flow  if your dialog is more complex, but they are the same concept of state underneath. States do help make the code easier to read and follow. . In conclusion, this is a new set of skills a developer will need to add to their bow to work with AVS or, indeed, any voice interface. In the same way that web design required graphical skills, voice interface design is a completely new subject. Skills that move steadily away from mathematics and logic into the worlds of English and other modern languages, and the psychology of speech and conversation. I’m hoping that this is another way in which women will be lured into software engineering, as English and psychology are famously female-dominated degrees. Fingers crossed! . If you’d like a copy of my Alexa skill, I haven’t published it on Amazon but the code is on my  github repository . Here you will find the JSON file containing the interaction model and the Javascript file containing the lambda function logic. Happy scripting! ", "date": "2018-05-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Service Fabric Cluster Creation\n  ", "author": ["Jay Barden"], "link": "https://capgemini.github.io/development/service-fabric-cluster-creation/", "abstract": " There are many ways to create a Service Fabric cluster and what follows, is by no means, the only one. . A few posts have been written elsewhere on the internet (for example  Create Service Fabric clusters from Visual Studio ) and whilst they are accurate, I felt they were lacking detail in some areas, so here’s my take on creating a Secure Service Fabric cluster. As I will cover in a related post on my observations whilst working with Service Fabric (coming soon), I am a firm believer in scripting the creation of everything possible and I fully admit that this post does not achieve that. However, if you want an easy way to get started, then the approach described here is probably the best option. . I could simply say: “Because it is secure”, but that doesn’t really answer the question. . The management of the Service Fabric cluster is performed via Service Fabric Explorer which supports both Certificate and Azure Active Directory authentication (more information can be found at  Service Fabric Security ) with the certificate being the simpler, default option. . Now, it is possible to create an insecure cluster (no, it doesn’t have personality issues!) that anyone can access over the internet and make changes to, but even for a test cluster, it is recommended that you always build a secure cluster. . Whilst a secure cluster can be created in a few steps – either via the Portal or using ARM (Azure Resource Manager) templates, this post focuses on the latest option available directly in Visual Studio 2017 15.6 and higher. . For the following steps, I created a Service Fabric Application called SFApplication1 with a Stateless ASP.Net Core API Service called Web1 with no authentication. Basically, I chose the defaults where applicable as they are also the simplest to test – which we will do at the end of this post. If you prefer to create a different Service-Type, that is, of course, absolutely fine. Whatever service-type you choose, please make sure you can test it works once deployed to ensure the cluster has been created correctly. . Ensure you are logged in to Visual Studio with an account that can access the target subscription. . There are several ways to publish the Service Fabric Application, below are the two simplest options: . From the  Build  menu, select  Publish Web1 : .   Or by right-clicking the Service Fabric Application (the one with the pretty icon partially obscured below): .   . Either method will display the main  Publish  screen: .   . As highlighted above, in the Connection Endpoint, you can select  Create New Cluster . Doing so will launch the following screen: .   . As I am in the UK, I set the Location to  UK South  but left the other options as the defaults. Specifying a 3-node cluster will automatically denote it as a Test cluster in Azure - i.e. a cluster that is not suitable for Production use and has certain restrictions - for full details, please see the  Service Fabric cluster capacity  documentation. . You can either click  Next  to navigate to the Certificate tab or simply click the Certificate tab. Either will present the following screen: .   . As part of the wizard, a certificate will automatically be created in an equally automatically created Azure KeyVault – no need to do anything 😊 The  Import certificate  checkbox will import the certificate once created so you can immediately manage the cluster via the Service Fabric Explorer. The certificate password is not optional, I recommend you use a strong password even if the cluster is only intended for testing (as it is here). . Clicking  Next  or selecting the VM Detail tab will display the following: .   . At this point, you may notice that the  Next  button has changed to  Create  and is enabled. Don’t be fooled in to thinking you can skip the remaining options: .   . The current template does not list all the possible VM sizes but does cover the basics (and, the size can be changed later via the Portal / PowerShell / Azure CLI): .   . The Advanced tab has the following options: .   . As shown, ports 80 and 83 are enabled by default – I usually remove 83 (as I avoid using non-standard ports for publically accessible APIs - I am not sure why Microsoft even include Port 83!) but you can set the ports to whatever you require. The  Application Insights key  is more interesting; it not only allows you to specify the key (no surprise there) but it will automatically send the cluster messages (such as  Application Starting ) to the specified Application Insights instance. You will still need to send your application-specific messages to Application Insights if you want them to show there, but this does simplify monitoring the overall cluster health quite considerably. . I have entered my Application Insights key and clicked  Create . After 1-2 seconds (where Visual Studio performs a series of validation checks), a confirmation screen will be displayed: .   . Clicking  OK  will return you to Visual Studio and how the output window with the status messages: .   . The mistake I made the first time was thinking that the Application would also be deployed – this is not the case. A second Publish will be required as we will see later. In hindsight, it is clear that we have deviated from the initial Publish route as we are creating a cluster, but that did not occur to me the first time! . The Resource group will be created, and you can  monitor  the creation of the cluster from either Visual Studio or the Azure Portal (although). I’ve italicized the  monitor  as the Azure Portal will only show the items created, not any pending items such as the actual VMs: .   . From the Properties option (1), we can see the Provisioning Status (2) was successful and that the nodes are being created (3). . For this test, the cluster took approximately 15 minutes to create which is far from ideal but the screenshot below shows the successful completion message (if you are wondering how / why the completion message is green, may I recommend  VSColorOutput  which is an excellent Visual Studio plug-in). .   . The Portal shows: .   . The Cluster state shows  Baseline upgrade  and this means we are good to go. OK, it really means the cluster is performing some internal upgrade tasks, but it is fully functional for our purposes so we can continue. . As with the Cluster Creation, you can use the same Publish option you used earlier - i.e. either from the  Build  menu or by right-clicking the Application Project. The original Publish screen will be displayed again: .   . You may need to  &lt;Refresh&gt;  to see the newly created cluster. . As the application does not exist, we do not need to select the  Upgrade the Application  or change any of the other defaults. Click  Publish  and you will be returned to Visual Studio. Below is a capture of the  Output  window just after clicking  Publish . .   . Once completed, the Visual Studio Output window will look like the one below: .   . And the Service Fabric Explorer will look like: .   . Did you notice the issue I  deliberately  created? No? Neither did I as I was doing the screenshots! I didn’t change the WebAPI to use port 80 or specify the port WebAPI uses in the Cluster Creation (on the  Advanced  tab). . As it stands,  Postman  - an excellent tool for testing APIs - shows: .   . To resolve this, I have two choices: . Add the port to the cluster configuration or . Update the Service Manifest (shown below): .   . As mentioned in several posts, I am lazy by nature, so I will change the Service Manifest, but this also improves the accessibility as the port will become an industry standard port – a win-win to me! . Below is the default ServiceManifest (the pink squiggles are from the  Visual Studio Spell Checker  plugin showing that even Microsoft can make mistakes!  Directoy …) with the Port highlighted – your port number will probably be different. .   . Once changed to 80, I can redeploy: . As the steps are the same, I won’t repeat them all here. The important differences are: . The  Upgrade the Application  must be ticked (#1 below) as, by default, Visual Studio will deploy the application as though it were a new application and the deployment will fail - even if the manifest has been updated (#2 below): .   . We also need to update the Manifest Versions by clicking the  Manifest Versions  button (#2 above). This is required so that Service Fabric knows the version is different (the  how  it is different does not matter as we will see below) or a  Same version  error will occur. . The Edit Versions screen is shown below – I have expanded the Web1Pkg but other than that, it is  as is . .   . Whilst the above versions are using the traditional Semantic versioning, you don’t have to – all that matters to Service Fabric is that the new version is different from the current version. Below, I have continued with the Semantic versioning and updated the Config version to 1.0.1 – as the  Automatically update application and service versions  is ticked, and as the example is using sematic versioning, the Application Version is also increased to 1.0.1 automatically (yep, I’m lazy!) .   . Clicking  Save  will return you to the previous screen where we can simply  Publish , and you will again return to Visual Studio. Below is a snapshot of the  Output  window: .   . After a few minutes, the application upgrade will be complete, in the meantime, you can also monitor via Service Fabric Explorer: .   . Clicking on the highlighted box above will show the upgrade domain status: .   . Once complete, Service Fabric Explorer will return to the earlier appearance: .   . Within Visual Studio, the  Output  window will show: .   . More importantly,  Postman  will get a result: .   . Creating a cluster from Visual Studio is a reasonably painless experience which, as a bonus, follows the current Microsoft  Best Practices  without you having to perform several manual steps. . Thank you to Mark Cunningham for reviewing this post, but most importantly, thank you for taking the time to read this post too - I hope you found it useful. ", "date": "2018-06-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Combining OAuth and JWT to gain performance improvements\n", "author": ["Tosin Ogunrinde"], "link": "https://capgemini.github.io/architecture/combining-oauth-and-jwt-to-gain-performance-improvements/", "abstract": " For many years Simple Object Access Protocol (SOAP) was the standard approach for communicating with remote services, often via HTTP. The landscape has changed significantly in recent years with the increase in the adoption of Representational State Transfer (REST) APIs. There are still a number of use cases that suit SOAP, for example where stateful operations are required. . One of the six guiding architectural principles of REST is statelessness. Every API request from the client to a server must contain all the necessary information necessary to serve the request. The server maintains neither state nor context. How do we then authorise access to protected REST APIs? Say hello to  OAuth  and  JSON Web Token (JWT) . OAuth and JWT are two of the most widely used token frameworks or standards for authorising access to REST APIs. In this blog post I consider how both OAuth and JWT can be combined to gain performance improvements. . OAuth enables an application to obtain limited access to an HTTP service. While JWT is a compact, URL-safe means of representing claims to be transferred between two parties. OAuth has a number of grant types. So whenever I refer to OAuth in this blog post, I am referring to the  OAuth 2.0 Resource Owner Password Credentials Grant type .  A user is required to authenticate or login to obtain a token. A typical authentication flow is shown in the sequence diagram below. .   . A user will enter their username and password via a client (which could be a mobile device or PC), and at the end of the authentication process the user will be supplied with a token. The client will then include the token with every subsequent API request to a resource server (like the User server).  To compare what the authorisation flow for both OAuth and JWT will look like, let’s consider an example where we make an API request to  GET the authenticated user . . The OAuth flow to  GET the authenticated user whose ID is 123  will typically look like the sequence diagram below. .   . While, the JWT flow to  GET the authenticated user whose ID is 123  will typically look like the sequence diagram below. .   . The JWT implementation is less chatty and more performant compared to OAuth. This is because JWT enables a resource server to verify the token locally. In its compact form, JWT consist of three parts: the header, payload and signature.  The signature is the result of signing the base64Url encoded header and the base64Url encoded payload with a key. The resource server uses the signature to verify that the token has not been tampered with. . The JWT payload contains the claims. The claims are statements about an entity (typically, the user excluding private information of course) and additional metadata like the expiry time. JWT however has a drawback in that once it has been issued it will allow its holder to gain access to a resource server until the expiry time is lapsed. It looks like we need a way to revoke JWTs. So let’s go back to OAuth. . OAuth is chattier compared to JWT. This is because OAuth requires the Auth server to verify the validity of the token and the Auth server in turn relies on the information it has stored in a database to make this judgement. OAuth however does have an advantage over JWT in that tokens can be easily revoked. This is particularly a good feature if instant access revocation is desired. The basic OAuth token response is shown below. . The relevant attributes are described in the table below. . As shown in the example OAuth flow above, the client will include the access token in every API request to a resource server. The client will then make use of the refresh token to obtain a new access token. How can we benefit from the inherent performance advantages associated with JWT and the limited access capability provided by OAuth? By issuing an OAuth token with JWT in both access token and refresh tokens as depicted below. . The client will include the short-lived JWT for every call to the resource server, and will make use of the long-lived JWT to obtain a new access token. The short-lived JWT is validated locally. We do however need to keep a record or blacklist of the revoked refresh tokens till they expire. This blacklist will be checked only when the client wishes to refresh the OAuth token. A new access token will not be granted if the refresh token is found in the blacklist.  The blacklist should ideally contain refresh tokens associated with users who have logged out and users whose account have been disabled. Although the access token is not immediately revoked, it is meant to be a short-lived token. . Finally, there may be scenarios where this behaviour is not desired but ultimately, it depends on the requirements of the system. This approach enables the resource server to validate the OAuth access token locally and only requires interaction with the Auth server when we need to get a new OAuth access token. It is this reduction in the interaction with the Auth server that gives us the performance improvements. ", "date": "2018-07-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How fast are your React‑ions\n  ", "author": ["Paul Monk"], "link": "https://capgemini.github.io/react/how-fast-are-your-react-ions/", "abstract": " Since its conception by Facebook in 2013,  React  has quickly become one of the most popular libraries for building web-based user interfaces. React uses JavaScript to create a dynamic web application normally rendered directly on the client side device, similar to  Angular  or  Vue.js . However, unlike other frameworks where the HTML and JavaScript are split into separate files, React uses JSX to include HTML-like structures inside JavaScript code. Its design encourages the use of small reusable components so it scales well when building large applications. . As React is a library it is already a lot more flexible than other JavaScript frameworks available, this is a good thing but does mean there are a lot more development decisions to make! It also offers excellent performance out of the box but must be used in the correct way to maintain this performance when building more complex applications. . React has been designed with speed in mind, and each update brings additional performance improvements. . Firstly React uses state and one-way binding. The state of a component is a fancy name for the data that it displays on the web page. In React the state is the single point of truth for a component, as the state controls what data the HTML renders, and can only be updated using the  setState  method. This is the concept of one-way binding where the state is used to automatically update the HTML, but changes to the HTML don’t automatically update the state. In contrast in two-way binding the data and HTML are linked, so an update to one will cause the other to be automatically updated. This was a big performance issue with Angular version 1 ( AngularJS ), where there were too many listeners watching for changes within the HTML, meaning the web page would grind to a halt everytime a text box was updated. One-way binding can mean a little more code has to be written as  onChange  events must be fired from the HTML, which cause the state to be updated, which then causes the HTML to update. This may seem a little complicated but it means the only listeners watching for changes are the ones you define. So page updates only occur when you want them to, making your application very fast and efficient! . Secondly it is optimised to only update sections of the page when they are affected by state changes. Just because the user enters data into a text box doesn’t mean the whole page has to be updated. As React has the concept of splitting the page up into components, it means individual components can be updated without affecting the rest of the page. One issue with this however is that it can become difficult to update all the related components if data changes, but global data stores like  Flux  and  Redux  have the solution to this. They introduce the concept of a global state where data can be shared between unrelated components, and updates are fired to the appropriate components when the global state changes. . Thirdly React has the concept of a virtual DOM. Updating the web page through the browser can be slow even for small changes (particularly in Internet Explorer and mobile browsers). This problem is solved by holding a copy of the web page in memory. This means that React can update the virtual DOM and calculate what changes the browser actually needs to make, before giving these specific instructions to the browser. So instead of leaving the browser to work things out React can provide a “patch” which will just update specific parts of the page. Another advantage to the virtual DOM concept is improved testability. React can essentially play the role of a browser so you can now run proper unit and integration tests against an application without using a browser, making the UI tests faster and far more reliable! . There is a lot of competition in the JavaScript UI world with many different frameworks and libraries, and performance is often used to compare them all. React is one of the most popular UI libraries, however Angular by Google is also very widely used, and Vue.js is a new framework that is quickly becoming popular with UI developers. . When a web page is loaded for the first time the browser must download all the relevant HTML and JavaScript code, so the more it has to download the longer the user has to wait. This means that smaller JavaScript libraries will result in faster page loading times. Unlike Angular and Vue.js, React is classed as a library not a framework which means it has less features included in its core code. This is a good thing as it means you only bring in features as you need them, instead of having loads of features which your aren’t using. So comparing the JavaScript file sizes when they have been gzipped: as of version 16 the React library with React DOM is 31.8kB, Angular is more bloated at 111kB but Vue.js is the smallest at just 20.9kB. . React and Vue.js both make use of the virtual DOM to speed up page updates, Angular doesn’t have a concept of a virtual DOM but it does still optimise the updating of the webpage by only updating the parts that have changed. . Ultimately in terms of performance all three are very similar, traditionally Angular has struggled with performance but as of version 5 it is very slightly faster than Vue.js and React overall according to this  3rd party benchmark . Of course if you want pure performance then there are other libraries like  Inferno , however they have less community support than the more well known solutions. Ultimately plain JavaScript is always going to be the fastest performance wise but it takes a lot of effort and expertise to develop a fully dynamic web application without an additional library or two! . So React is designed to be fast and efficient, but it is also easy to kill the performance of your application by having badly designed component trees and causing unnecessary re-renders of the web page. . An important piece of React knowledge to remember is that updating a top level component will cause all the child components nested within it to update. This can be extremely powerful when making a single page web application, however not knowing this can also be the cause of some major performance issues. It is important that your component tree is designed to minimise the amount higher level parent components are updated. . For example if you have a search page, which may consist of an input box and a list of results, the following component design is not ideal: . Having the table of results nested within the input box component means that every time the input box is updated e.g. when the user types a letter, the entire table of results would also be automatically updated. . To fix this issue consider this alternative component design: . Here the table of results is at the same level as the input box, meaning the input box component can be updated without affecting the table of results. . This example may seem trivial, but thinking about the design of the component hierarchy in advance can lead to massive performance increases in a large application. . Sometimes it is necessary to update the high level parent components. However this doesn’t mean all the child components need to be updated. The  shouldComponentUpdate  method can be used to decide whether a component’s HTML needs to be updated or not by returning a boolean value. In the standard React  Component  class this method just returns true, meaning that the component will always update when its parent component updates or its state changes. However React also provides a  PureComponent  class. This provides a better implementation of the  shouldComponentUpdate  method that shallowly compares every object/variable in the component’s state for changes, and if nothing has changed it will return false preventing an unnecessary update. If this still isn’t good enough you can provide your own implementation for the  shouldComponentUpdate  method, however it is advisable not to deeply compare the values of nested objects or loop over every element in an array as this can be computationally expensive. Instead of deeply comparing objects/arrays for changes the immutable design pattern should be used, where a change to an object/array causes it to be copied to a new object/array. Libraries like  Immutable.js  can assist with this, but again if the data for your application is being updated frequently these operations can be more expensive than just re-rendering the HTML. . The  render  method of a React component should be used to describe its HTML structure, and nothing more. As this method is called frequently it is not the place to initialise variables or perform complex calculations. . The  render  method should rely on the component’s state to provide it with all the values it needs, so instead of performing calculations inside the  render  method do them when updating the state of the component instead. . Objects should not be cloned or initialised within the  render  method. These jobs should be done in the component constructor method, so all the objects the component needs are created the first time it is rendered and aren’t recreated every time it re-renders. . Finally try not to write inner functions, or arrow functions inside the  render  method. Everytime the component is re-rendered these functions will be recreated. Not only does this waste computation time but it can also cause child components to needlessly re-render. Examine the following two code examples: . Both examples make use of the  onChange  callback method of a React Input text box to update the component’s state as the user types. Example 1 uses a Javascript ES6 arrow function to create an anonymous function inside the  render  method, example 2 creates a named function bound to the MyForm class. To look at, example 1 is much tidier, however the anonymous arrow function will be recreated each time the user types a letter in the text box, it will also get recreated any time the component is re-rendered. Additionally the Input component will compare its previous state to its new one before it decides if it should update or not, however as the anonymous function will be new each time, the Input component always gets re-rendered whether the text inside it has changed or not. In contrast with example 2 the  handleChange  function is only ever created once and won’t force the Input component to update if the text inside hasn’t changed. . Redux provides an application with a global state which can be used to share variables between unrelated components. This is a very useful feature that can also help improve the performance of an application. . Firstly Redux gives you more options when designing the component hierarchy of an application. It means that components can share data with each other without having to be explicitly connected by a parent component. Two unrelated components can then be updated through a Redux state change without affecting any other components within the application. . Secondly Redux itself has many performance optimisations. Components can register their interest in specific variables within the global state, and Redux will only update those components when the variables they are interested in change. Bear in mind objects/arrays are shallowly compared, so a component that registers interest in an object/array may be re-rendered by Redux even if the data hasn’t been changed. . However overusing Redux can negatively impact performance. If a lot of components register interest in a variable, then they will all be updated when that variable is changed. It is advisable to keep variables scoped to the state of a specific component unless they really need to be shared across the entire application. Redux can also increase the complexity of the code, by introducing more files and “black box” actions, making bugs/performance issues difficult to pinpoint. . By default React does all of its work on the client-side machine. This can help take the load off the server and make for a more responsive client-side experience. However for slower client machines and mobile devices it can increase the initial loading time of the web page. To solve this problem React introduces the concept of rendering the initial web page server-side, and handing control over to the client for subsequent page updates. The  ReactDOMServer  library can be used to implement server-side rendering. This improves the users experience as not only does the page load very quickly, it also responds quickly when they make changes, and uses less computational power on their device potentially extending battery life on mobile devices. Additionally as of React version 16 server-side rendering has been made up to  3.8x faster . . As of version 15.4.0 a new performance timeline was introduced. This allows you to see when components get mounted, updated and unmounted, and how long each operation is taking. This tool currently only works in Edge, Chrome or IE. See the ReactJS site for  instructions on using this tool in Chrome  . . Another useful tool is the  why-did-you-update  library. This can be plugged into your application and detects when a component re-renders unnecessarily. It will print out logs to the console when a component updates even if its state didn’t change. This can be useful to pinpoint places where you should be using a  PureComponent  instead of a plain  Component  or implement your own version of  shouldComponentUpdate . . If you want to build a maintainable, reliable and fast web application then React is a good way to go. It has been built with performance in mind and is suitably fast for most use cases without the need for any performance optimisations. However without knowing some of the theory behind React it can be very easy to build slow and inefficient applications. . If your application is a little sluggish then there are many steps you can take to speed things up. However the easiest way to build a performant React application is to get the initial design of the component hierarchy correct so that you never experience any performance issues. ", "date": "2018-05-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Thoughts on SkillsMatter Meetup: Plugin Architecture\n  ", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/design/clean-architecture/", "abstract": " I attended a  SkillsMatter meetup  the other day where  Uncle Bob (Robert C. Martin)  explained some key features of clean architecture. It was a new talk and the content was taken from his new book  “Clean Architecture” . It was an insightful evening and a good reminder to be more purposeful in designing and structuring components - especially since writing more JavaScript recently I’ve found it much easier to not follow good design patterns. Below are some of the highlights of the talk which stuck out to me. . In this talk Bob advocates the use of plugin architecture and making use of good modularisation to enable swapping out of components whenever requirements change. He used the example of  ReSharper  plugging in to  Visual Studio  (VS). VS defines the interface for plugins and ReSharper implements that. ReSharper depends on VS - ReSharper knows about VS but VS doesn’t know about ReSharper. . This can then be applied to any application - it can have a core which defines the interface and plugins which extend and add extra functionality. Plugin architecture is a powerful way to keep components separated and allow for extension and changes in the future but the core of the application has to be designed specifically to allow plugins. . Bob made the distinction between UI and database access code being low level and the business use cases and business entities being high level. The low level components should depend on the high level components, resulting in the high level components being independent of how the data is shown or how it is stored. . It was good to be reminded that databases are low level. So often I think of the database as tied to the business entities because I use an ORM (Object Relational Mapper) and the database tables are mapped 1-to-1 to the entities. . He also laboured that objects and data structures are different. Objects are a collection of methods which operate on some data. That data is hidden and we don’t know where it comes from. By contrast, data structures simply hold data rather have functions which operate on it. By this definition ORMs (Object Relational Mappers) don’t exist - it should be “data structure relational mappers”. Either way, ORMs are fine to use but they’re low level, they should be part of the database plugin and they shouldn’t be tied to the business entities. . One key point Bob made, which I thought very pertinent, was that we have a tendency to marry the frameworks we use. The framework authors don’t care about whether you have good architecture, they care about you using the framework. They haven’t promised you anything but you have promised to use their classes and annotations all throughout your code. We have committed to them but they haven’t committed to us - a one way relationship. . Is it possible to abstract away the use of a framework rather than using it throughout the code - can it be made as a plugin to serve a purpose? I would suggest that framework use should be kept out of the business logic and should be moved to a lower level component. . You need to be aware of the direction of your dependencies, and also be aware that you can change the direction if they start flowing in the wrong direction. . Because there is this distinction between low level components (UI, data access) and high level components (business entities), with low level components plugging in to and extending the functionality of the high level, the low level always depends on high level, i.e. the UI should depend on the objects and interfaces provided by the business rules and entities. This is shown on the diagram at the top of this  blog post by Bob from 2012 . The outer layers depend inwards and the real important components are the ones towards the centre: they can be coded without any external dependencies or mocks because they are what the rest of the application depend on. . When designing an application he suggested making a high level drawing of the components and dependencies between them but not going into too much detail. He then encouraged continuous refactoring to make sure dependencies keep on flowing in the right direction. If they don’t, take control of the dependency directions and put an interface between the components so that lower levels always depend on higher. . Bob made a good argument for separating business logic from the way data is shown and stored. This is a challenge: is your business logic clearly separated from your framework, the way it’s shown to the user and the way it’s stored? Can you point to a package and say, “those are all of the use cases and business entities and there’s no SQL or HTML in there”? . He suggested that the business entities, the business use cases, the UI and the database access layer should each be a separate JAR/DLL or package. This means you can swap them out easy, deploy them independently (so you don’t have to redeploy the whole application) and you can develop on them independently, allowing more teams to work on the same application. . I took away a lot to think on from this talk so I’ll end with some good questions we should ask to make sure our architecture is clean. . Is your business logic clearly defined and separated from the rest of the application code? . Can you swap out your UI without any change to the rest of the application? . Could you swap out your framework for another one that provides the same functionality? . Is your application structured in a way that can provide a new way of presenting the data e.g. chat bot, by just deploying (plugging in) a new package, or would you have to change the whole application? ", "date": "2017-11-03T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Knowledge Is Dead, Long Live Learning\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/knowledge-funerals/", "abstract": " There’s a certain inescapable truth that people who work with technology need to face. As time goes by, the knowledge we’ve gained almost inevitably becomes obsolete. If we specialise in something, how do we deal with the fact that our specialism, which may even have been cutting edge technology that we were pioneering, eventually becomes a legacy system? As  Ellen Ullman  put it, “The corollary of constant change is ignorance … we computer experts barely know what we are doing.” . Front end developers are very familiar with this feeling, confronted so frequently with the  dizzying pace of change  in the world of JavaScript frameworks. Once upon a time, I was very proud of my ability to make CSS layouts work in IE7. Now all those tricks and hacks are little more than worthless trivia, perhaps less valuable than actual trivia. At least knowing who scored the winner in the 1973 FA Cup final might help in a pub quiz - I can’t imagine that being able to prefix properties with an asterisk will ever come in handy, but it’s taking up storage space in my brain. Now that CSS grid is becoming widespread, everything I’ve learned about floats (and even flexbox) is becoming less and less useful. There are even some people (although I’m not one of them) who would say that  CSS itself no longer has value . Similarly, jQuery is already on its way to joining YUI and MooTools in the graveyard of things I used to know about, and experienced members of the Drupal community have recently been coming to terms with the fact that in order for the technology to progress, we’ll have to unlearn some of our old ways. . It isn’t just true for technology. London taxi drivers are finding that their hard-earned  Knowledge  is being made obsolete by satnav, and before too long, the skill of driving will itself have gone the way of basket weaving or being able to pilot a horse-drawn buggy - something that might still be interesting for the enthusiast, but isn’t relevant to most people’s lives. . Confronted with the unpleasant reality that our hard-learned skills are becoming outdated, what’s the appropriate response? Do we follow the example of the  Luddites  and rage against the evolution of the machines? It’s easy to fall victim to the  sunk costs fallacy , and ego provides a strong temptation to hang on to our guru status, even if we’re experts in an area that is no longer useful. If you’re a big fish in a shrinking pond, you may need to be careful that your pond doesn’t dry up entirely. Besides,  do you really want to work on legacy systems ? Having said that, if your legacy system is still mission-critical somewhere, and migrating away would be a big job, there’s good money to be made - just ask  the people working on COBOL . . I think there’s a healthier way of looking at this. With the internet acting as a repository of knowledge, and calculators at our fingertips, education is evolving. There’s no longer much value in memorising times tables, or knowing the date of the battle of Culloden. As my colleague  Sarah Saunders  has written, you’re never too old to learn, but the value of learning things is greater than the value of the facts or skills that we learn - the meta-skill of learning is the really useful thing. Then again, I would say that,  having done a philosophy degree . . For example, the time and effort I put into learning French and German at school doesn’t currently seem like a worthwhile investment, when I think about how frequently I use those languages. But I would never say that it was a waste of time. When I lived in Tokyo, having learned other languages definitely helped when it came to learning Japanese. Then again, these days I don’t often spend any time in Japan or with Japanese people, so the current value of that effort seems low. But do I regret spending that effort? Not a bit. It helped me to make the most of my life in Japan, and besides, it was interesting. . Two of the most compelling conference talks I’ve heard in the last few years touched on this theme from different directions.  Andrew Clarke  and  Patrick Yua  both emphasised the importance of focussing on the underlying principles, rather than chasing whatever the current new hotness might be. Designers and developers can learn something from  Yves Saint Laurent : “Fashions fade, style is eternal”. . We need to recognise that things will always keep changing. Perhaps we could help ourselves to acknowledge the impermanence of our skills by saying some kind of ceremonial goodbye to them. I have an absurd vision of a Viking funeral, where a blazing longboat sails away full of old O’Reilly books. We may not need to go that far, but we do need to remind ourselves that what we’ve learned has served us well, even if that knowledge is no longer directly applicable. A knowledge funeral could be an opportunity to mourn for the passing of a skill into obsolescence, and to celebrate the value of learning. ", "date": "2017-11-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Learning Gulp.js\n", "author": ["Leon Hassan"], "link": "https://capgemini.github.io/engineering/learning-gulpjs/", "abstract": " Gulp.js is a task runner that is used to automate tasks such as compiling all your style sheets into a single file, uglifying your JavaScript and so much more. This post is going to introduce you to the world of task runners through Gulp and how you can leverage task runners to make your work better and your workflow more agile. .   .  Gulp  is a task runner, that means it runs tasks. Tasks that you would usually run on a regular basis from the command line or even run using a bash script, task runners handle all that for you. For instance you can use a task runner to lint your JavaScript files every time you change them, it’ll then let you know if you’ve done anything un-lint-worthy. . The main difference between Gulp and other task runners like  Webpack  and  Grunt , is that Gulp focuses on code. You don’t need to spend time setting up your config like with Grunt, you just get dug-in straight away writing JavaScript. I feel like that’s the best part of Gulp, you’re essentially writing a generic streaming module in JavaScript and this is because Gulp is a streaming build system. It uses Node.js streams rather than reading/writing files and storing them temporarily, this makes Gulp really fast! . One of the most common uses of task runners is to concatenate, minify and uglify files. For our example, we will look at a basic setup for compiling Sass stylesheets to a single CSS file, then we’ll minify the result. . Your Gulp file ( gulpfile.js ) should sit on the top-level of your project directory, like your  package.json  and  .gitignore . The first thing you’ll need to add to the file is: . To install Gulp you should run the following in the route directory of your project (you can use the  -g  flag to install globally): . This will install Gulp and add it as a dependency to your  package.json  (thanks  --save-dev  flag!), making it really easy to get this all up and running on other machines. You need to install any package you require in your Gulpfile. . Let’s get our first Gulpfile ready to watch for changes to our Sass files for changes then transpile and concatenate them. .  Note : You can install all of these packages at once using the command  npm install --save-dev gulp-sass gulp-rename gulp-concat gulp-minify-css . . This Gulpfile has two watchers bundled into its default task. . The first watcher will watch the SCSS directory (and it’s subdirectories) for changes in any file ending ending in  .scss , then it will run the  sass  task. The  sass  task will then concatenate and transpile the sass to CSS, name this output file  style.css  and place it in the css directory. . The second watcher is for the css directory, watching for changes in the  style.css  file. When there is a change it will concatenate the file (doesn’t do anything in this example), minify the file and then output the file to the directory  dist/css . . Run the Gulpfile by navigating to the root directory of your project and running the command  gulp . This will run  gulp default , which in our example is a task that runs  gulp watch . .   . Now you have a minified CSS file built from all your SCSS files! . I use the  BrowserSync  package to update my website’s resources in real-time, without the need to refresh. An added benefit of using BrowserSync is that it automatically runs a local version of your website, including an external link. Super useful for testing on different devices without a metric ton of setup! . The first thing you need to do is add the dependency to your Gulpfile (and install the package, adding it to your  package.json !) by adding the following line:  let browserSync = require('browser-sync').create(); . . Next we need to instantiate our local server when we kick-off our Gulpfile by adding the following to our default task, before our watchers: . Finally we need to make sure that when we change our resources, the change is streamed to BrowserSync. To do this we need to add a line to our  css  task (after we set the file destination), the following line will signify to BrowserSync that there’s been a change in a file:  .pipe(browserSync.stream()); . . Now our Gulpfile should look like this: . Now when we run  gulp , not only do we get a local server running the site (and an external link!). We also get live reloads/updates when we change our SCSS files. .   . Now that you’ve got your own nifty little pipeline, try to concatenate, minify and uglify your JavaScript using what you’ve learned. Leave a comment or tweet me if you get stuck or manage to do something cool! ", "date": "2017-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "ffconf 2017 - Conference review\n", "author": ["Simon Conway"], "link": "https://capgemini.github.io/engineering/ffconf-conference-review/", "abstract": "   .  ffconf  is a JavaScript conference which is held in Brighton each year. This post provides a brief summary of ffconf 2017 (held on 9 and 10 November) and links to the talks posted by the organisers. . The day offered a good balance of future-of-the-web philosophy and technical detail. ffconf is fairly small - the 8 sessions are run back to back in a single cinema auditorium - which gives the event an intimate and friendly vibe. The event is held in  The Duke of York’s Picturehouse , a quirky cinema which lays claim to being the oldest in Britain and, in 2012, was also voted the best. .    Image By Hassocks5489 at en.wikipedia (Transferred from en.wikipedia) [Public domain], via Wikimedia Commons  . The organisation of the event was excellent… we were treated to coffees, pastries, cakes and ice cream during the day. Signing in at the start was a breeze. Staff were friendly and happy to help. There was plenty of opportunity to talk to the speakers who were hanging around the event all day and were happy to share their views. . The ffconf talks are repeated on each day of the two day conference. So attendees have a bit of flexibility to choose the day that suits them. This year, on 8 November there were also workshop sessions on Angular and React. (Which I did not attend.) . For me, this was enjoyable day with plenty of useful content. Here are some highlights from the day (My favourites listed first): .  Addy  is a Google engineer and he has spent the last couple of years focussed on how to get progressive web apps to run really quickly on mobile devices. . This was a fantastic session packed with practical hints, tips, tools and resources to improve time to interactive on mobile. . Well worth watching for anyone building mobile first progressive web apps. . A light hearted, entertaining and humorous session with two clear parts: .  Part 1   Bruce  introduced Stylable his component styling project.  (stylable.io)  . Stylable is a CSS pre-processor that allows you to write style rules in CSS syntax, with some extensions that adhere to the spirit of CSS. It provides the following features: . Allows component level styling while maintaining well understood CSS features (declarative, familiar, static and fast) . Allows CSS macros with JavaScript that can be used at build time. . Language validation and code completion. . Errors are reported at build time so there is no need to run the app to find out that CSS has silently failed . Stylable is in Beta and they are currently working towards rolling it out into production at Wix, the website hosting platform. This means it will underpin the styling for 10 million websites, which is a pretty good test set. .  Part 2   Bruce  then talked a bit of web philosophy; What is the web? What forces are shaping the web? Why is important for developers to engage in this process? How can we get involved? . An interesting and thought provoking session. . A well presented talk on web standards and web components; what they are, what they are not and why they matter. .  Monica  works for the  Polymer Project , which creates a library and apps to allow users to apply web components to create custom elements and facilitates separating your app into components. . Monica covered what makes a good web standard, what makes a poor web standard, what is a polyfill and how does this lead to development of new browser features. . Then she briefly described the web component specifications that were introduced in 2012 and ran through how these have developed over the last 5 years. In 2017 the web component spec has now been adopted by all browsers so it sounds like they might really take off in 2018. . There is a lot of buzz around web components in the JavaScript community at the moment so, for me, this was a useful talk to improve my understanding. .  Blaine  works in cyber security and was the lead developer in the  OAuth project . In previous lives he was a lead architect at Twitter and Yahoo. . Blaine is as interested in usability as ensuring top level security. In fact, he points out that as soon as usability is compromised, the workarounds that users take are often the cause of the biggest security issues. . He takes a light hearted and humorous look at the failures of the current systems for logging in. He offers some suggestions about how login can be implemented to improve the user experience, leading to large improvements in click through rates. .  James  had a number of ideas around education in front end development and the importance of web standards. His key ideas are: . We should teach front end developers by doing rather than by teaching theory and people can develop a working knowledge of coding quickly. (this was the thinking behind a successful book he wrote) . Tooling is here to stay and is an important part of the web. Tooling configuration is tricky because tools need to work for everyone so need to be highly configurable. We should to include tooling in our web standards and give tooling a seat at the table when deciding web standards. . Accessibility is important. Reducing the size of the apps will make apps in regions with low internet speeds. . A small number of companies effectively own the web and this is not good. . His ideas seemed solid and he was passionate about how important it will be for the future of the web get these things right. . The end of his talk had the slightly downbeat message that there are now so many well-funded, vested interests controlling the direction of the web that James struggled to see how he could even get his ideas on the agenda for discussion in a meaningful way. (I think he was looking to recruit supporters at the event.) . Memory management is pretty much handled automatically by JavaScript. But how does it work? . A back to basics lesson in how garbage collection works in JavaScript. this is useful for people (like me) that have only worked with more recent front end tools and have never had to manage memory manually and would like to know the basics of how it works. .  Jenn Schiffer  is a developer at  Fog Creek Software , as well as a pixel artist and philosopher. . Firstly she ran through the latest Fog Creek project,  Glitch . This is a community that helps create apps in a simple way. By providing app examples, a code editor, instant hosting and deployment Glitch aims to allow anyone to build a web app. . Then she discussed a wide range of philosophical ideas including expression, inclusion, bias, intellectual property theft and accountability in the tech industry. She paints a picture of the tech industry heading towards a crisis and has some advice about how we should all act as developers. . In summary I enjoyed ffconf! The speakers were engaging, the content was useful and relevant to me as a front end developer and the organisers did a great job of hosting and making everyone feel welcome. They’ve also done well with the quality of the video produced from the event. ", "date": "2017-12-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Seeking Frontend Developers\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/frontend/hiring-front-end/", "abstract": " Customers, whether consumer, business, employee, partner, citizen or stakeholder, are demanding the same high grade user experience they experience from the likes of Facebook, Google and Netflix, in the applications and interactions they have with organisations every day. . In the last few years we have seen an explosion in the demand for bespoke-built, high-quality, interactive user interfaces. At Capgemini, we see this manifested as increased demand for frontend development skills,  specifically modern JavaScript frameworks such as React, Angular, Vue or Ember. Alongside this, the predilection for “JavaScript Everywhere” has resulted in us developing with  React Native for cross-platform mobile apps , Node.js for server-side and lightweight APIs, and the  Serverless framework  for deploying Functions-as-a-Service among numerous others I won’t list here. .   . Additionally, we’re continually looking at ways to improve the quality, efficiency and reliability of our applications, through applying good development practices like Test Driven Development (TDD) or Behaviour Driven Development (BDD), build automation  tools like Webpack and Gulp, as well as embracing best practices in accessibility and usability. . As a result of all this, our Frontend team in the UK is growing, and we need experienced Senior Frontend Developers to lead our development teams across a variety of clients and projects. . We’re actively hiring for this role, which you can read more about on our recruitment page for  Senior Frontend Developers . . We often don’t say enough about the fantastic things we do and how we do them, maybe over-delivering on our  seven shared values - including Modesty , and also through respecting the wishes of our customers in what we share. We often say that Capgemini is probably the best company you probably never heard of. What do we think is so good, and what do we offer for people who join our teams? . We invest a lot into our staff development and careers progression. Internal and external training is available from a wide range of sources and looking at either technical or “soft” skills . We also attend many different conferences and talks throughout the year with many of our developers being  speakers at these events  . If there are specific technologies that you want to focus on there is the opportunity for you become a champion for them within the business . All our teams contribute to the the Capgemini community by sharing vast amounts of knowledge, best practice and code across our teams . We have team members with a broad range of experience who are always willing to help each other, you only need to ask the community and they’ll help to quickly find the right solution . We are big contributors to open source and we encourage our staff to  contribute to the wider communities  . We have access to some of the best technology and tools currently on the market . Through our partnerships and alliances relationships we have great access to marketing leading solutions as they are released . We are always looking for opportunities to improve the way in which we work, whether it’s a new tool, the latest technology or an emerging methodology . We work closely with our clients to help them understand new technologies, running workshops for them and creating proof of concepts to demonstrate how these technologies can be applied to solve business challenges . We believe that innovation is the future, this is why we encourage our teams to find new ways in which to solve our clients problems . We also have specialist areas within the business which are  dedicated to innovation . When not on project you get the opportunity to work within these areas . We foster an environment in which new ideas can be proposed and used within our clients projects. We believe in creating the best solution for the clients, not just the one that everyone else can do . The Senior Frontend Developer role sits within our Software Engineering community and so you can get a good idea of what we’re looking for successful candidates to bring to the role by reading our  Grade Ladder for Lead Software Engineers . . You can also read about our culture and  How We Work  right here on the Engineering blog. . A flavour of the kinds of projects our Frontend team have worked on this year include: . A web application to visualise fuel quality data at different ports around the world, to enable planning of shipping routes. (React, Google Maps APIs, Node.js, Azure, CosmosDB) . A mobile application for a large public sector department to act as a site concierge and notify hosts when guests arrive for meetings and provide facilities information (React Native, Google Firebase, iBeacons, Node.js) . A web application mapping restaurant purchasing trends for a large consumer goods provider to enable proactive sales and supply strategies (React, Carto.js) . A parcel tracking solution for a large delivery organisation (Angular) . Visualising data from oil and gas pipeline inspections, to enable analysis and prediction of potential pipe damage (React, D3.js, Node.js, AWS Lambda) . A showroom app for a large car manufacturer helping customers better match their desired specification to available stock to reduce waiting times for receiving a new model (React, WS02) . We asked our existing team two questions - why did you join Capgemini, and what have you gained since joining? .     . If you want to know more, or want to have the opportunity to hear a bit of what it’s like working at Capgemini, feel free to contact me on  Twitter  or  LinkedIn , or get in touch with us through our  recruitment pages . ", "date": "2017-12-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Microsoft, .NET and not (necessarily) Windows\n  ", "author": ["Jastinder Singh Atwal"], "link": "https://capgemini.github.io/.net/core-x-platform/", "abstract": " Recently I looked into developing a cross platform .NET Core application. Up to a few years ago a .NET application was developed with the intention to run it on Windows. Not anymore. The landscape for .NET has changed dramatically over the past two years. And that’s probably because the landscape for Microsoft has changed as well. . I understand there may an audience who says “with our programming language we’ve been able to do cross platform forever”. Bear with me. You need to understand that for a developer using Microsoft .NET this is a big change. I went from being someone who found developing on ‘not Windows’ daunting, to being a developer who now embraces these platforms. For me, I have had to adapt but now have the flexibility to move in different directions based on my particular requirements and constraints at the time. I think open source, cross platform gives you flexibility where you need it most - at the point when you are code complete and need to move forward. Its great to know I have the option to run my code on an operating system of choice. And .NET Core will run most places on premise and anywhere on the cloud. It runs on Amazon ECS just as it can run in Azure, or in a Docker container that runs in Kubernetes or Openshift, or whatever. . Microsoft has been good at allowing developers to be productive relatively quickly, through their frameworks and tools, and that hasn’t changed. Below I demonstrate how to create a cross platform application with a short walk through and the tools you need. But there is more to .NET Core than just cross platform. It allows you to create .NET applications that are faster, cheaper and easier to maintain. Let’s touch on each of those areas. Note that the following applies to .NET Core and not the  .NET Core on the .NET Framework . . .NET Core has made gains in  performance  by looking at and improving the low level internals of the fundamental things that computer programs do. There is a message that ASP.NET Core is x many times faster than ASP.NET (x changes depending on who you speak to), but that is a relative measure. If ASP.NET wasn’t that quick in the first place, and ASP.NET Core is better, then it’s probably still nowhere near some of the high performance open source web servers that exist. But, why is high performance code important? . Well, let’s look at the cloud as an example. For those of us who have looked at developing Microsoft application designed for the cloud, a key design principle is designing the application to scale horizontally. So you have developed an application according to those design principals and now your application needs to scale. So Microsoft tells you to scale out, but what’s the impact of that? Well, cost probably. Scaling horizontally may mean more compute time, which maybe means a higher bill at the end of the month. If I choose not to use a Microsoft web server, would my throughput be greater and therefore my costs lower? . If we look at the  road map  it seems Microsoft have realized this and are putting effort in to do something about it. The important statement is  “optimize .NET much more for scenarios in which inefficiencies are directly tied to your monthly billing” . I also believe performance gains are important for Azure because .NET Core will be the framework that underpins Microsoft’s PaaS and server-less offers. If everything in the cloud is a price war, then the framework becomes more attractive to potential customers if it costs less to run. . Add to the mix the contribution the community have been making around performance improvements. An example is the  pull request  submitted by the Age of Ascent. To summarize, reading and parsing bytes off a socket buffer with a web request is relatively expensive. This pull request took months to complete. That story behind it is important, people working together and working toward a common goal. Some of the best things about open source. The new  Span&lt;&gt; and Memory&lt;&gt;  constructs are specifically designed for high performance parsing of data streams and reducing heap allocations. These constructs will probably be introduced at some point next year. . Finally, the intention must be to have much higher density per host, whether that host is physical, virtual or through the use of some form of containerization. .NET Core achieves that. A key driver for moving to the cloud has always been to bring down costs and it’s only correct that .NET Core already supports a variety of deployment scenarios with a view to making it a more attractive framework for increasing density and therefore reducing costs. . I started to think about my experience of starting on a new project. Let’s say I start and Application A is already out there, doing its job well and for a while. I come along and develop the new Application B. Application B has all the bells and whistles so uses the latest version of .NET. Application A was developed a while ago, perfectly happy with an older version of .NET. When it comes to deploy it, I have two options. I can co-host both and so upgrade Application A to use the new framework. That means more time and cost (regression test, support impact, etc.). Or, maybe I will ask for another server. That’s low risk, and both Application A &amp; B are happy. Virtualization has become an easy option as far as the application development is concerned but provisioning an on premise server may take a little while. And there is also the small thing of two under-utilized servers. . Well, a key concept in .NET Core (not the .NET Framework) is side-by-side. It means you can host as many .NET Core applications as you need on a single server, all targeting different .NET Core runtimes with zero impact in terms of regression testing and support. This is more important in a multi-tenant architecture where those other applications may not be yours. . Below is a short walk through to create a new cross platform .NET Core Web Api. You will need to download and install the  NET Core SDK  and  Visual Studio Code  (probably in that order). You will also need  OmniSharp  which is the engine that powers C# in Visual Studio Code, but you dont need to download that explictly, VS Code will do that for you (more on that later). I have tested the following on a Mac, Ubuntu 16.04 and Windows. These steps are the same across all three and that highlights an important point - a consistent tooling experience across all platforms. . Once the .NET Core SDK has been installed, open a terminal window (or PowerShell). Note that if you are using PowerShell replace the ‘&amp;&amp;’ with a ‘;’ to chain up commands. . Create a new folder and cd into it: .  mkdir corexplatform &amp;&amp; cd corexplatform  . Create a new .NET Core solution: .  dotnet new sln  . Create a new src folder and cd into it: .  mkdir src &amp;&amp; cd src  . Create a new Web Api and give it a name CoreXPlatform.API: .  dotnet new webapi -n  CoreXPlatform.API  . Add the new Web Api to your solution: .  dotnet sln ../CoreXPlatform.sln add ./CoreXPlatform.API/CoreXPlatform.API.csproj  . Create a new sub folder called test at the solution level: .  cd .. &amp;&amp; mkdir test &amp;&amp; cd test  . Create a XUnit test project called CoreXPlatform.API.Tests: .  dotnet new xunit -n CoreXPlatform.API.Tests  . Add the test project to the solution: .  dotnet sln ../CoreXPlatform.sln add ./CoreXPlatform.API.Tests/CoreXPlatform.API.Tests.csproj  . Reference the project you want to test in the Test project: .  dotnet add ./CoreXPlatform.API.Tests/CoreXPlatform.API.Tests.csproj reference ../src/CoreXPlatform.API/CoreXPlatform.API.csproj  . You now have a project that you can begin to develop in Visual Studio Code. Now open Visual Studio Code and  File -&gt; Open Folder  the  corexplatform  folder. If this is a new install VS Code should recognize you are trying to work with C# project and prompt you to install a C# Extension. This is OmniSharp that I mentioned earlier. If you dont get the prompt, on the left hand-side panel, the bottom icon is for extensions. Search for the C# extension and install it (VS Code will prompt you to reload the editor). Its also worth taking a look through the many, many extensions supported by VS Code. . VS Code will take a little while to download what it needs. Bear with it, its worth the wait. If you look at the Output window (Ctrl+Shift+U) you can see VS Code performing the download. An example on my Windows 10 machine: .   . You will now want to build and run the application. VS Code should provide a prompt to install the dependencies you need: .   . Click yes and you will notice a new  .vscode  folder has been added to the root with two files;  tasks.json  that is used to build the application and  launch.json  that is used to run the application. If you hit Ctrl+Alt+B VS Code will proceed to use  tasks.json  and prompt you to build your Web Api. To debug, hit F5 and VS Code should start the application. Once the application is running you can test the Web Api locally in a bash shell using  curl -iv http://localhost:5000/api/values  or in PowerShell using  Invoke-WebRequest http://localhost:5000/api/values . . So that provides a minimal solution that you can begin to start to work with. A Git  repository  has been created using the commands above. It provides some reference material in case you have any problems setting this up. There are also the  launch configurations  for both Windows and Linux under the .vscode folder. It also has some other highlights including a cross platform build script using  Cake , build definitions for  Windows  and  Linux  and multi-stage  Windows  and  Linux  docker builds that allows you to optimize the size of the Docker container hosting your cross platform application. . At this point it’s worth pointing out that there are also some quite useful tools that will aid the development of cross platform applications using .NET Core. Two of these are Windows Subsystem for Linux (WSL) and API Analyzer. . Windows Subsystem for Linux (or WSL) is useful if you want to use Windows 10 as a single development environment to build, test and run your cross platform .NET Core applications. WSL gives you an opportunity to get feedback early on how your application will behave and also the opportunity to debug if your development and target environments are different. Once WSL is  installed , you will also need to install the  .NET Core SDK  for Linux. Its worth pointing that WSL is independent from the Windows 10 host, and therefore doesn’t share tools. In fact, you should not edit Linux files using Windows Tools as it can cause  problems . Below shows both .NET Core SDK’s on a single Windows 10 host: .   . There are some simple  instructions  as to how to set-up VS Code to either launch your application to debug on WSL or attach when it is running. . The .NET API Analyzer is a NuGet package that you can add to your application that will tell you if there are any compatibility issues when running your code between Windows and Linux. The Analyzer will throw up warnings at compile time if it thinks you may have problems later on at runtime. The following example demonstrates how to add the .NET API Analyzer to an application. . Create a new console application: .  dotnet new console  . Add the API Analyzer: .  dotnet add package Microsoft.DotNet.Analyzers.Compatibility --version 0.1.2-alpha  . Open the  Program.cs  and add some Windows specific code: . Build the application and you should see the warnings telling you things will not go well on Linux and Mac: . .NET Core has three key selling points. Firstly, it allows developers to create applications and then have the flexibility to deploy them across multiple platforms. Secondly, .NET Core is being developed with performance in mind, and in addition to the general performance improvements there is a view to reduce the costs of hosting your .NET Core applications in the cloud. Finally, it supports a side-by-side model to reduce support implications of co-hosting applications and to give much greater density per host, again with a view to try and reduce costs. In addition to these, the tooling allows developers to be productive. The example above shows how a solution can be setup in 10 or so commands across platforms (Windows, Linux or Mac). ", "date": "2017-12-13T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "2017 in review - innovation, diversity, funerals and donkeys\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/blog/end-of-year-review/", "abstract": " Our engineering team blog has continued to grow this year, with 25 authors writing more than 40,000 words in articles across a range of topics.  We’ve welcomed 17 first-time authors to the fold, with colleagues from other countries joining the UK engineering community in contributing. We’ve also expanded into different technologies, with members of our Microsoft practice starting to get involved alongside our open source teams. . We’ve had over 115,000 views of our articles (an increase of nearly 10% on last year), with this piece on  REST and microservices  by Craig Williams showing an enduring popularity. With the continued hype surrounding Bitcoin and other crypto-currencies, it’s hardly surprising that Graham Taylor’s article on  building private Ethereum networks  has also been one of our most widely-read pieces. . Of this year’s new articles, the most popular has been Ant Broome’s piece  sharing our grade ladder  - another example of our commitment to open source, not just in terms of the software that we build, but also in the way we build it. . This year’s longest article, and the second most read of our newly published articles, was James Relph’s 3,020 words on  creating a portable Kubernetes cluster . . At the other end of the scale, Tosin Ogunrinde was the soul of brevity, needing only 140 words to announce his  open source library for building CQL statements in Apache Cassandra . . Some of this year’s articles have been focused on particular technical topics, such as Sophie Field writing about  multi-stage builds in Docker . Meanwhile, others have discussed more general areas, such as team culture, the psychology of  learning new skills in a fast-moving technology landscape ,  an efficient debugging mindset , and even  how to avoid becoming Buridan’s donkey . . Some of our writers have been looking forward, with our  Applied Innovation Exchange  helping to focus our minds on the future, while others have discussed  working with legacy systems . . We’ve also been active in the wider software engineering communities, either  presenting at a range of events , attending conferences like  ffconf ,  Lead Dev , and  QCon , or sponsoring events such as Devoxx and  DrupalCamp London . . Some of our articles have been from the point of view of experienced team members, such as Sarah Saunders on  learning React as an experienced Java developer , while others have been written by members of our junior talent programmes. Graduates, apprentices, and even summer interns have contributed to the blog, with Leon Hassan writing about his  first SAP UI5 application , Henry White writing about  exploring Meteor , and Nagaraj Govindaraj and Rushil Soni discussing how they used  VoiceOps to manage cloud infrastructure in AWS . . Sadly, this year we said goodbye to  Andrew Harmel-Law , who had been one of the main driving forces in getting this blog up and running, and has left the company to pursue new challenges. He will be missed, but we’ll try to carry on following the example he set. One of the last contributions he made at Capgemini was one that I think will continue to shape the way we work in years to come - he was one of the main authors of our collective  response to the “anti-diversity manifesto” controversy . The wider company is also committed to  active inclusion , and this sentiment is echoed forcefully by our team in a way that might make more sense to software engineers. . Since  the blog started in September 2014 , more than 50 members of our team have written over 150 articles. What will 2018 bring? I wouldn’t like to risk making predictions in terms of technology trends, but I’m certain that the Capgemini engineering team will continue to share our opinions and thoughts on what we learn in the months to come. ", "date": "2017-12-15T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Encrypting configuration in Apache Karaf\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/java-cryptography-in-karaf/", "abstract": " It’s very important to encrypt passwords or other sensitive information in configuration files. This stops attackers gaining easy access to sensitive information, limiting the way they can harm your system. This has been made easy in an  OSGi  environment with  Jasypt  due to the features and classes it provides. . Any work to do with security starts with an awareness of the attack vectors that are being mitigated by your work because it will influence how you implement the changes and choose between trade-offs. This example will minimise the damage an attacker can do if they gain access (read-only) to the configuration files. . In this example I’ll be working with  Apache Karaf  to provide OSGi and  Apache Aries Blueprint  to provide dependency injection. . To enable Blueprint to decrypt properties you need to add a property parser to your blueprint xml: . Which requires a new namespace in the same file: . All of which requires the jasypt-encryption feature to be installed in your Karaf container. This feature will export the packages and classes needed by your Blueprint and will provide the necessary namespace handler above. The above snippet also includes a variable for the  passwordEnvName  allowing the name of the property which holds your master password to be changed per application or per environment (it’s not good to have the same master password between environments). . Jasypt provides  a number of classes  to configure how properties should be decrypted: . Simple - password is a value passed to the class i.e. it’s in your configuration file, defeating the point of encrypting properties in the first place . Environment - this means that you configure the key of a operating system property or JVM (Java Virtual Machine) property and this class will retrieve the value and keep it in memory for use in decrypting the passwords. Bear in mind, the risk of the master password being visible on the system e.g. in a process command and the need for applications to automatically restart and therefore always have access to the master password. . Web - the application waits until the master password is entered in a web UI by someone. . The last two are the most secure as it means your master password is now no longer in configuration too. Let me be clear:  doing work to encrypt properties is only useful as long as your master password is safe.  . The Oracle JDK does not ship with the strongest encryption algorithms (due to US export laws): only MD5 or SHA1 and  DES (Data Encryption Standard)  are included which are all considered insecure now. The  Java Cryptography Extension pack  can be downloaded to add to the JDK but there is another easier way.  Bouncy Castle  provides a wide range of algorithms which are much more secure than those packaged with Java, including other SHA and, critically,  AES (Advanced Encryption Standard)  based algorithms. I recommend using AES, please do not use MD5, SHA1 or DES based encryption. . There are a few ways to configure Bouncy Castle: . The first (and easiest) way is below. Using it means that just the JVM fired up by your application has Bouncy Castle as a provider. Place this in a method which is run when the application is started up so it is run just once: . It also requires a Maven dependency: . The second way is to add to the Java installation so that any created JVM will have Bouncy Castle as a provider. This requires putting the Bouncy Castle jar in the $JAVA_HOME/jre/lib/ext directory and adding a security provider line to the java.security file. This is more involved and inflexible. . Once Bouncy Castle is available Jasypt needs to be told to use it. This is done by setting the provider name on the Jasypt Config class e.g.  &lt;property name=\"providerName\" value=\"BC\" /&gt;  ( BC  is the String constant name of the Bouncy Castle provider). Now feel free to configure the Config class with any of the wide and varied  encryption algorithms available to you . . The mention of encryption might make other developers in your team groan as they realise that it’s going to be harder to make sure components are connected and configured correctly. When encrypting configuration you should try not to make other developers have to work harder once you’ve completed your work. One way of doing this is using  Docker  , as a standalone executable which has the necessary components packaged up, to encrypt and decrypt text in one line. This tool is completely separate to the runtime of the application we’ve just been modifying. Docker is a convenient platform to run this standalone tool to encrypt and decrypt Strings with a password. For example: . Once the process finishes the container exits and the password entered as a parameter is no longer stored anywhere - except possibly your bash history (you can prevent this by preceding the command with a space). . So instead of each developer setting up Java locally to have Bouncy Castle as a provider, downloading the Jasypt jars and then running the encrypt command, all they have to do is copy and paste. Feel free to use this docker container to make encrypting and decrypting passwords easier. To decrypt, use the decrypt.sh script in the same directory with the same parameters (using encrypted text as input). You can also list the available algorithms: . When using Blueprint you don’t explicitly write bundle activators so there isn’t an application boot method to register Bouncy Castle using the method  Security.addProvider(new BouncyCastleProvider()) . Thankfully,  JBoss Fuse  (a distribution of Karaf with some extras) makes this easy since it’s already registered by default as a Java Cryptography Extension provider. The only problem I had in setting up this example is the Jasypt Configuration classes being found correctly. I use the Apache Felix Maven bundle plugin to generate bundles as part of the build (Felix is the core OSGi implementation which Karaf uses and wraps with some extra features). A bundle is simply a Jar with a MANIFEST.MF file in the root directory which specifies the imported and exported packages. This plugin generated a manifest that specified that my bundle needed the Jasypt package version 1.9 to 2.0 whereas the deployed version was a Redhat specific version (with a textual Redhat identifier which OSGi couldn’t understand). When there are unresolved dependencies in the OSGi container I find it helpful to unzip the bundles and find the required dependencies, in the import-package, in the MANIFEST.MF. In this case Karaf required adding a specific version in the bundle plugin Import-Package instruction like so: . Where jasypt.version is filtered by Maven into the specific version of Jasypt bundled with JBoss Fuse. . To test this, I used  Stefan Birkner’s System Rules  to set OS variables which my Blueprint test could use. This library provides  JUnit rules  which allow you to set a range of variables and expectations on system dependencies. My use of the library used the  EnvironmentVariables  rule and overrode the setup method to set the variable and add Bouncy Castle: . It’s not hard to encrypt properties using the tools available and so it should be standard practice when configuring applications. Make sure you look for other information that may be sensitive: API keys or passphrases, and encrypt those too. Keep the master password safe and use a strong encryption algorithm. ", "date": "2018-01-19T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Non functional requirements and Blockchain\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/blockchain/blockchain-ux/", "abstract": " Here in the  Applied Innovation Exchange  we get to be part of some pretty excellent events. One notable example is  “Capgemini Week of Innovation Networks”  (CWIN) which happened in November 2017 on the 8th floor of our Holborn office. As part of this 5 start-ups were invited to pitch their businesses to the attendees, set up stands, demo their application or work and be involved with a round table/panel question time. . I loved hearing about the innovations and the way they were using technology to solve old problems such as payroll, inaccessible banks for the developing work and privacy in big data. I had a great conversation with one start-up about the challenges of rolling out a  Blockchain  platform, which got me thinking: what’s the most difficult thing about putting some Blockchain in production? .  A quick example  shows that a Blockchain implementation is possible in one page of javascript. Indeed theres an  explosion of different Blockchains  using a variety of different consensus algorithms, with varying levels of  byzantine fault tolerance  and throughput.  Hyperledger  and other well supported open source projects are getting better and better developer tools to improve the developer experience and to make it quicker to implement new features. So developing Blockchain applications is not in itself difficult. . I’ve found that through using  Hyperledger Composer  in my final year project at university that modelling the business domain, creating smart contracts and generally developing new features is actually very easy. I’m sure others using  Truffle Framework  for  Ethereum  would agree, the developer experience is getting better and better. . The difficult stuff is everything around that. Non-functional requirements. . One particular barrier to Blockchain development is designing the user experience. The question is, how much of Blockchain, as the underlying technology, should you reveal to your users? . Should people know their actions on the app are interacting with a Blockchain somewhere? Should they know about public and private keys? What if people don’t realise their private keys should be kept private? Should they know about tokens and that they need them to be able to interact with the application (write something to the Blockchain)? . These questions are important because people may be trusting your Blockchain application with their money. The value of assets on the Blockchain are usually backed by the value of a cryptocurrency coin, given value by an  initial coin offering  (ICO). If user’s assets are linked to the value of that coin then how do you inform them that their net worth is increasing and decreasing as the cryptocurrency value increases and decreases? If they use your platform are they now cryptocurrency investors? . It’s also important from a user experience perspective. Applications should be easy to use. That first impression is very important for a new user. Launching a platform and cluttering it with Blockchain related jargon is sure to put off the majority of users. But dumbing it down may reduce the “cool” factor or the visibility of the key benefits of Blockchain which differentiate your application from the non-Blockchain competition. . If your platform is tied to a Blockchain implementation such as Ethereum then it is also tied to the scalability of that underlying platform. Ethereum currently runs at  4 transactions a second , up to a theoretical maximum of 25 (based on proof of work). If the next Ethereum game took off, generating a lot of transactions like  crypto kitties did , would this affect your users? Would they be able to get their assets out immediately if they needed them? Users will want to know they’re not being tied down to anything and they can return to the non-digital world at a point of their choosing. Choosing the right underlying platform (or choosing to roll your own) is essential for this. . Projects like  Hyperledger Cello  are aiming to make Blockchain network administration easier. However, I found it was still difficult to set up a “production”  Hyperledger Fabric  cluster and there’s a lot of manual steps to get Hyperledger Composer running on it. It takes time to integrate these things and improve the DevOps experience, so until then, allocate a good amount of time to provisioning and automating cluster deployment. . There’s a tendency when selling the advantages of a Blockchain application to oversell the benefits to governance. Arguably the governance stakeholders (e.g. police, government, auditors) receive the most benefits from being able to track what everyone is doing and use it to regulate or check compliance. This is important but other stakeholders need incentive to join in with the network otherwise there will be no users and no transactions to regulate or check. . Then there’s the problem of persuading all the stakeholders to get involved so you can do business analysis and requirements gathering. The modelling and smart contract definitions require understanding of the business domains of all the different types of stakeholders. Answering the question of what should be stored in the Blockchain requires the views of a variety of stakeholders. . Lastly, who begins the Blockchain platform initiative and pays for the development of it? A key advantage of Blockchain is that it can enable business and collaboration between organisations which have never met. This will attract a lot of organisations during the lifetime of the platform but one organisation needs to take the initial step, see the value and build the platform for everyone else to use. . Blockchain is still finding its feet in being implemented in consumer facing applications. As application developers we need to design, as part of the customer experience, hiding the complexities of the implementation from people without putting them at risk. There should be a journey into learning Blockchain little by little, when the consumer wants to, in order to make sure they don’t wander blindly into a cryptocurrency faux pas. ", "date": "2018-02-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Debugging into a NuGet package\n  ", "author": ["Jay Barden"], "link": "https://capgemini.github.io/development/debugging-into-a-nuget-package/", "abstract": " Over the past year, I have been working on a programme of work that, initially, was intended to extend an existing Azure Service Fabric solution by adding a further 14 WebAPI endpoints. However, early in the project, we took the design decision to break the new APIs into their own solutions. . This decision was primarily based on start-up time of the application when debugging locally but presented an old foe - duplication of code. We had many hundreds of useful methods with thousands of lines of code and, whilst we could simply  copy-n-paste  the methods, that was clearly not a practical or sustainable solution - not to mention its total disregard for  Don’t Repeat Yourself . . The customer had an on-premise TFS Server which we already used for the build process so the solution was simple: NuGet the methods and use TFS to host the feeds. After all, we’d only have 3-5 packages, right? Wrong! Whilst we genuinely thought there would not be hundreds of packages (and we were right on that!), we have reached 21 packages and, along the way, encountered the obvious problem; debugging. OK, maybe it is not really an obvious problem, but given the title of this blog, I hope it comes as no surprise! . This post isn’t intended to give a deep, exhaustive explanation of debugging - there is a brief overview in a moment - the intent here is to help solve the frustrating moment when you try to debug into a method only to find Visual Studio steps over the code (as it is in an included NuGet package rather than when you accidentally hit  F10  instead of  F11  - we’ve all done it!) and continues on its merry way. Often this won’t be an issue as the defect you are hunting down will be elsewhere, but what about when it’s not? What about when the code containing the bug is in a NuGet package you’ve created? . The screen-shots that follow have been taken from a scaffolded Console Application with no meaningful changes - I used the .Net Framework 4.6.1 but, that was for simplicity and I mention for reference only - any supported language will work; VB, .NetCore, etc. The screen-shots are from Visual Studio 2017 Enterprise, but NuGet Manager has been available in Visual Studio since Visual Studio 2010 and is available in all editions including Community. . Below is a very simple Main method. As you can see, it will create a new Contact and then write the details to the screen. . The output will look like the example below: .   . I am sure the above output will not surprise you. However, below is the output when the Contact Id is not set: .   . Now, in an ideal world, the exception would tell us which parameter was out of range and what that expected range was. Whilst this is a very basic error to locate (as the class below will reveal in a moment), in a real-world example, we may have to go through many classes before we reach the source of the error. . For this example, we’d probably set a breakpoint on line 9 (as with everything in Visual Studio, there are many ways to do this - selecting the line and clicking “F9”, clicking in the left most column etc.) but we’d end up with a breakpoint: .   . Debugging from here is as simple as clicking “F5” (if you prefer the menu option: Debug &gt; Start Debugging). For this simple scenario, a few clicks of “F11” to step-into the code would locate our issue. . But wait, the class is in a NuGet package, not our solution… . When the class is inside a NuGet package and we “F11” at our breakpoint on  var contact = new Contact { Id = 1, Name = \"Jay\" };  we come down to line 11, rather than step into the offending class (when we  F11  during execution of  Console.WriteLine($\"Id: {contact.Id}, Name: {contact.Name}\"); ), we are basically back to where we started: .   . Below are the steps on resolving this issue. . The first step required is to create a local NuGet folder to host the debug NuGet packages. The word local here does not have to be taken literally – a network share or even a file share on Azure (AWS / etc.) will work. You can even create your own internal NuGet server if you want but that is outside of the scope of this article. For the remainder of this blog, we’ll assume the C: drive of the developer’s machine will be used. . The name of the folder is not important but, for simplicity, make sure the name is descriptive enough to be remembered:  c:\\Local NuGet Feed  is a sensible name,  c:\\My stuff  is probably not. . The screen-shots below are from Visual Studio 2017 but, other than potential location changes for some of the options, the steps should work for VS2010 and upwards. . Open Visual Studio and navigate to the NuGet options for the sources:  Tools &gt; Options &gt; NuGet Package Manager &gt; Package Sources  (or type  Package Manager  in the Quick Launch search box). All being well, a screen like the one below will be shown: .   . Click on the highlighted “ + ” button to add a new source location. The screen will update as shown (the blue-highlighted option is the newly created, default source): .   . In the lower half of the window, we can change the name and the source. As with the folder itself, I recommend being explicit with the name to avoid confusion later. Once you are happy with the new name, click the ellipses highlighted below to change the source. A screen similar to the following will be displayed (normally it will default to your local Documents folder, I have pre-selected my new folder): .   . Once you have clicked on the desired folder (do not enter the folder, just single-click it), click the highlighted  Select  button to return to the previous screen: .   . As you can see, the bottom half of the right-hand side is now correct but the blue highlighted source has not updated. To complete your update, click the  Update  button and then the  OK  button. The OK button does also update the screen but you will need to click it a second time to leave the screen completely. .   . Visual Studio will now include this feed whenever it is specifically selected or when you have selected  All  as shown below: .   . If you do not have a copy of the NuGet executable, please take a moment to download the latest version from  nuget.org . before continuing. . For simplicity, it’s recommended that you store the NuGet executable in your repository directory and add that directory to your Environment Variables path parameter. This isn’t 100% necessary but it does save typing! . The first step in building the NuGet package is to perform a Debug build of the solution you want to use as a NuGet package. For brevity, we will assume this has been done and you used a new version number. Updating the version number means you can  update  your solution - if you do not want to increase the version number, you will need to downgrade your solution before you can upgrade it to your  debug-enabled  version. You do not need to have a NuGet spec file anymore – the NuGet executable will use the  .csproj  file. You can use a  .nuspec  if you want / have one already but the examples are based on using a  .csproj  (to use the  .nuspec , simply specify it instead of the  .csproj  file). . Please note: if you use the .csproj file, the NuGet version is based off the values in the AssemblyInfo.cs. . Once built, open a Command Prompt / PowerShell prompt in the project folder (the examples use a Command Prompt as the colours are clearer – the process works for both prompts) and enter: .  nuget pack \"NuGet Example.csproj\" –symbols  (where  NuGet Example.csproj  is the correct name for your project) . A short-cut for the above is: .  nuget pack –symbols  . Both commands will perform the same way if no  .nuspec  file exists. The second command will prefer the  .nuspec  if it exists (although NuGet  may  change this in a later version). With no  .nuspec  file, the only difference is you are being explicit in the first example. Depending on the size of the project, the command should only take a few seconds and produce output like below: .   . If you forgot to build the solution, the above would be an error message saying no binaries could be found. Alternatively, if you see an old version number, this means that the solution does have the binaries, but has not been rebuilt since the version number was increased. . As the solution is using the  .csproj  file and I have not populated the Company or Description in the  AssemblyInfo.cs  file, there are two warnings in the above screenshot. This was done to demonstrate how good the NuGet executable is at validating everything, it was not a mistake! ;-) Your project folder should now include two  .nupkg  files: .   . Copy the two NuGet files created to your local NuGet feed folder. Whilst writing an internal version of this blog, colleagues said “we don’t need to copy both packages - we have to rename the  -symbols  and include only that file” but, I’ve not had the same experience - YMMV. If you are unlucky enough to find debugging into the NuGet package still doesn’t work for you, please try the  rename  trick. . I’ve cheated a little - I’ve already added the package to the demo solution and made it use the NuGet version of the highly useful  Contact  class. However, below are the details of how to update your solution assuming you already have a version of the package installed (which, if you are reading this, I think is a fair assumption). To facilitate this, I built a 1.0.1 version of the NuGet package - as the package is using the standard .csproj file, this was as simple as increasing the AssemblyVersion and AssemblyFileVersion in AssemblyInfo.cs to 1.0.1.0 (the final “0” - Revision - is ignored in the NuGet semantic versioning). . The last step is to update the target solution to use the new,  debug-enabled , NuGet package. This process is almost identical to the standard process - the only difference is the source. From the target Project / Solution, open NuGet Manager as shown below: .   . In the above, I have set the Package Source to  All  and changed to the  Updates  tab. As we saw earlier, the warnings generated by the NuGet pack command have been realised in the NuGet screen – the Description is, as it said, just “Description” and my username is showing as the author. . All that is left is to select the newer package (by ticking the box to the left of it - or, the  Select all packages ) and click the  Update  button to update the solution. . With the above in place, we can resume our debugging session. With the breakpoint still set on line 9, launching the console application via “F5” (or the  Start  button etc.), we can begin to step into the code (via “F11”) until we reach our destination. The Program class has not changed much - in fact, the only change is the additional  using  statement on line 4 - which brings in our NuGet package version of the  Contact  class: . As we step into our code, rather than being forced to step over the error and be faced with the less-than-helpful exception, we can now step into the NuGet package and into the class until we reach the actual issue: .   . The above shows that we are actually in the Contact class from the NuGet.Example namespace - which, of course, is in our NuGet package, not our solution. :-) . Now we’ve found the cause, we can either fix it or change our calling code. . If you, like the project I started this blog by referring to, have many NuGet packages, the chances are you will - sooner or later - end up with a hierarchy of dependent packages. You will, no doubt, be please to learn that the above works there too. The only things you have to do are: . Starting at the deepest point in your NuGet package hierarchy, create a debug version of that package - don’t forget to bump the version number to avoid downgrading then upgrading the next package in the chain . Update the next NuGet package up the chain to use the new debug version of the package - again, bump the version . Rinse-n-Repeat up the chain until you reach the top-most NuGet package . Update your solution to use the new chain of debug-enabled packages . Debug as normal . Debugging an issue in a NuGet package is, following the above steps, almost as straightforward as debugging directly through the class. No more headaches  guessing  the problem and potentially losing hours only to find you were wrong. . Whilst I have not had the opportunity to try it, I have been made aware (Thanks Chris Dickson!) of a potentially very useful site that resolves the source issue for public NuGet packages that you have not built:   SymbolSource . Let me know your experiences! . One final comment - assuming you followed the  bump the version  advice - don’t forget to either commit the updated NuGet packages to your build server or downgrade your application to the original versions before you commit it. Failure to do one of these will cause your Solution to fail to build on the build server (I know, it may seem unnecessary to say but, trust me, I’ve done - several times!) . Happy Debugging! ", "date": "2018-03-02T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Hacking Blockchain\n", "author": ["Sarah Saunders", "Craig Williams", "Dan Cotton"], "link": "https://capgemini.github.io/blockchain/hacking-blockchain/", "abstract": " Who can resist the opportunity to join a Hackathon? Certainly not the engineers in Capgemini’s  Applied Innovation Exchange (AIE) ; fast-turnaround cutting-edge product development is what we do. So when Capgemini Financial Services announced a global virtual IBM Hyperledger Hackathon, a team was thrown together. . The AIE is split across the UK, and for this project we had team members from the Aston AIE and the “virtual” AIE, those of us unlucky enough not to live in Birmingham or London! The virtual AIE had been focussing on  Smart Contracts  and Blockchain using  Hyperledger  for a while, so we had some of the relevant skills and a bit of background already. . We could choose from a number of use cases, or we could choose our own if backed by a client sponsor. Finding a use case for blockchain is possibly one of the hardest aspects of a blockchain project, so we went with one of our existing client ideas - which closely tied with the hackathon requirements anyway - and set out to build a smart contract system for the car lease industry. . As documented in   a previous article , it helps to follow some basic criteria around whether a use case is suitable for blockchain: . Does it have a database with shared write access? . Is there an element of trust? . Is functional consensus required? . For a car lease company, it’s possible to meet these criteria. There are several parties who may wish to input data into the car lease database - the client (policyholder) themselves, the insurance company, garages who carry out repair, sources to verify the value of a replacement car, possibly the police or other witnesses to accidents, third party claimants if an incident involves many people. These parties have different motives so there is a trust aspect, and consensus has to be reached on the outcome of a claim. . Good to go! We had a rough plan to produce a front-end for use by the policyholder and a Hyperledger smart contract with inputs from the various parties. We split the work so that the team who were sat together in Aston could focus on the front end and Node JS APIs, and the remote team could focus on Hyperledger. . For our minimum viable product we chose the use case whereby a policy holder has had an accident involving no other vehicles, and has caused damage to the car of sufficient magnitude that it’s cheaper to write off the car than to fix it, and so the claimant will receive a payout. This involves just a few actors and data sources - the claimant themselves, the garage who report on the value of the damage, a car valuation service and possibly IoT data sent directly from sensors on the car. . For a stretch case we wanted push notifications to allow the claimant to be notified of changes to their claim status without having to log on to the application. . For a further stretch case we considered two drivers involved in a crash, so a second claimant is thrown into the mix. . For the purposes of the hackathon, the plan was to mock up the third party data sources as RESTful APIs with mock data, and run specific use cases through the blockchain code. .   . The front end would be written using  Polymer , providing responsive views for mobile or desktop apps. Polymer was chosen as a result of its strong standards compliance, which allowed us to onboard new joiners with only very basic web experience very quickly, and its true encapsulation of components, which allowed us to more easily and efficiently divide our backlog into components without worrying about how others are implementing their code. In addition, the Polymer starter-kit provides a great place to start, instantly giving us a responsive, progressive web-app, with offline support out of the box. The central application would be written on a NodeJS platform, calling the Hyperledger RESTful APIs to pass data to the blockchain. NodeJS was chosen partly because the technology fits the chatty, multi-HTTP message architecture of calling multiple RESTful APIs across multiple devices, partly because we could make use of the Hyperledger Fabric API available for NodeJS, also because it allowed us to pair server-side JavaScript with a JavaScript client, in turn facilitating us manipulating JSON easily and in the same way on both sides of a request, and partly because we already had a starter-for-ten written in NodeJS from the IBM Bluemix car lease demo application and in a hackathon, you need every helping hand you can get… . The push notifications were built using  Firebase  and the  PolymerFire  series of Firebase enabled WebComponents. Using Firebase gave us an instant WebPush compatible server and the PolymerFire elements allowed us to simply drop 2 HTML elements and one small JS file (for instantiating Firebase messaging) into our page and start receiving notifications instantly without having to install an app. . Blockchain claims functions would be written in  Go , this was so that we could take advantage of the Hyperledger Go API. . In our system, the chaincode needs to obtain a car valuation in order to establish if a vehicle should be written-off.  In theory, the chaincode could make an http request to a third party car valuation service directly, but in practice this would not work.  This is because every peer within a Hyperledger Fabric system executes the same chaincode independently. Therefore, every action within the chaincode must be deterministic in nature, otherwise there could be a disagreement on the current state of the system.  If, for example, each peer made a direct REST call to a valuation service, there is no way to guarantee that the response to each would be the same, which could result in some peers considering a vehicle to be a write-off, whilst others do not. . To overcome this, we utilized the Oracle pattern.  An oracle is by definition “an agency of divine communication” and in the blockchain world the concept is generally used to describe an entity that fetches data on your behalf that you cannot, or don’t want to fetch yourself. . Our oracle service is implemented as an asynchronous REST endpoint in the Node JS server and works as follows: . Each Fabric peer reaches a state in which a car valuation is required, and sends a REST request to the oracle service.  This request contains all the required car information from our mock APIs and transferring that information into , along with the id of the blockchain transaction which triggered the invocation. . The service returns immediately with a 202 ACCEPTED response. . A car valuation is then obtained (via the third-party  Edmunds api ) on behalf of the chaincode . The oracle then triggers an invocation into the Fabric blockchain with the car valuation details.  Only one invocation is made, even though the endpoint may have been called my multiple peers independently. . The chaincode will only accept a car valuation from a specific oracle member, and if the origin certificate matches up correctly it will now have access to a car valuation and can make decisions regarding the write-off status of a vehicle based on this. . Ideally, in a production ready system, the oracle would collate a number of valuations from different sources and establish a median  price, rather than trusting the valuation from a single source.  We felt that this was not required for a proof-of-concept hackathon  implementation. . We used Docker containers to emulate four Hyperledger Fabric peers and a membership service; this allowed the development and IBM Bluemix cloud environments to be the same. For development, we used Vagrant to configure a VirtualBox and ran the Docker images there; on Bluemix you can upload your Docker image to the cloud.  Both the Node and Polymer applications run in CloudFoundry runtimes on IBM Bluemix PaaS. As neither framework has specific ties into their runtime, we felt it unnecessary to use separate containers for these two simple apps. . The team worked in an agile way, using Trello to capture agile user stories and break them down into assignable tasks. We had a remote daily standup and linked sprint demos into the AIE weekly demo sessions to give us something to aim for. We encourage traceability as being key in ensuring that our teams succeed when their size fluctuates, and as such we have 2-way traceability between our user stories/tasks and our Pull Requests on GitHub. These four essential components helped the project stay on track despite remote and fluctuating team members, it was a real success story to see. . First step was to take our user stories and create a JSON data model which could be used by each layer of the architecture. Starting out with a policy holder and a vehicle, we broke out the concepts of a policy and a claim, deciding the important attributes of each object. . Once we had our model and workflow, we could separate out the chaincode functions we would need. Hyperledger Go “objects” have three public method which are called by Hyperledger Fabric -  Init()  which is called once when a peer is started and sets up initial state,  Invoke()  which as the name implies can invoke changes to the blockchain - parameters to this method can define which blockchain changes are made, and  Query()  which is used to examine the current state of the blockchain. . The Node JS application was where the business logic of the system really resided. Here went the logic of which oracles should be called when, and Event Listeners were configured to listen for blockchain events . We used a pre-built Node + ExpressJS generator to initialize our server-side app, and started to expand it out by adding in the Hyperledger Fabric SDK. In order to document the APIs that we serve, we use a Swagger JSDoc node module in combination with a separate pre-existing Swagger UI instance. This allows us to have a single application that is able to request documentation for all of our running APIs, allowing us to more easily keep track of our API microservices. . The Hyperledger member service was used to define users; the Node JS application called the Hyperledger Fabric REST APIs with user information to authenticate and generate a token. This token could be used for a session to make Hyperledger Fabric API calls and define who was calling the chaincode. Within the chain code, authorisation compared whether the calling user was allowed to trigger each set of functionality. . We’ve all learned a lot on this project, and we have created some valuable reusable components. The Hyperledger / Bluemix platform is impressive, as is the ability to reproduce the environment with Vagrant and Docker for local development anywhere. We’ve found some (more) cool Javascript stacks and frameworks, and perfected our remote team working skills. So how did we do? The judges are apparently impressed with our entry, with the team being advised to apply for visas ready for the trip to the finals in India. Good luck everyone! ", "date": "2017-06-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Neo Engineer or Buridan’s Donkey\n", "author": ["Gayathri Thiyagarajan"], "link": "https://capgemini.github.io/engineering/becoming-a-neo-engineer/", "abstract": " “The Internet?  Is that thing still around?”    - Homer Simpson  . True story - Back when I started my IT career nearly 12 years ago, fresh out of college, little did I know that my whole career was being decided by an HR lottery. It worked this way, assume 40 grads were joining the company on a particular date, 10 were randomly allocated to the Java stream, 10 to Microsoft, 10 to Mainframe (I dodged a bullet there) and 10 to Testing. Of the 40, those allocated to Java, Microsoft even Testing made it through into having a decent career. But the poor guys and girls made to work on Mainframes purely randomly had a horrific experience trying to turn their career around. Most of them turned to Testing and some of them reinvented themselves as Java or Microsoft Engineers. . Of course, I have now learnt that my destiny need not be decided by HR or Resource Managers and the project they place me in. . In recent times, there have been so many advancements in technology and architecture such as Microservices, Event based systems, Cloud, DevOps, AI, Machine Learning, Blockchain, Big Data, Reactive programming to name a few that it is solely the onus of an Engineer to learn, evolve and carry themselves to the  future . I use the term “evolve” consciously because with the learning of each of these technologies you become someone who is better adapted to the future. . The time to market for a new technology is so short that it will just whizz past us while we are making decisions on which one to learn next. . Mainstream technologies mentioned in the first paragraph of this blog are now simply the means to take us to the future rather than the future themselves. They were the future once. Besides, new programming languages are  emerging  like mushrooms and there are lots of options to choose from like Python, Ruby, Go, Swift, JS, Scala, Haskell, Kotlin and the list goes on. For instance, you can pick any programming language which supports Reactive Streams to build a  Reactive system  – Java, Scala or ReactJS etc. But it is important to know and understand what Reactive Systems are and how they impact a Microservices architecture. In the DevOps world, things are even more fluid. But the same principle applies there and elsewhere such as Machine Learning, Big Data etc. . Now, this is where I explain the donkey. .  Buridan’s donkey  is a paradox in the conception of free will originally formulated by Aristotle but made famous by a French Philosopher Jean Buridan. It goes like this - a hungry donkey placed equidistant from two piles of hay will be unable to make a rational decision between the two piles and ultimately starve itself to death. . Pick a trend that tickles your fancy (for me it is now Machine Learning, it was Scala before) and learn it by applying. I would strongly recommend reading  Capgemini’s Technovision  which identifies the technological trends that are most likely to impact businesses in the near future. . Summing up - Don’t be a Buridan’s donkey paralysed by options but be ready when the future comes calling. ", "date": "2017-07-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "My First UI5 Application\n", "author": ["Leon Hassan"], "link": "https://capgemini.github.io/engineering/My-first-UI5-application/", "abstract": "   . In this post we will be covering some basic set-up of our development environment (including the SAP Web IDE), going over MVC architecture with particular regard to SAP applications and finally building your first SAP application. . The first thing you should do is sign up to  SAP Cloud Platform (SCP) , signing up to SCP will let you deploy and build applications in the cloud. By clicking  this link  you can sign up for your free account that gives you access to a suite of developer tools and essentially let’s you play around as much as you like with SAPUI5! . Now that you have a SCP account, you can access the  Sap Web IDE . You can get to the IDE by logging into the SCP website, you will be taken to the cockpit which should look like the screenshot below: .   . Now you can either download and run the SAP Web IDE locally, or continue to use it via SCP. If you are continuing without installation, skip this subsection. . The first thing you need to check is your version of Java, to do this open command shell in Windows or terminal in OS-X and run the command  java -version . . You must have version 1.7 or higher installed to proceed, if you do not meet this pre-requisite then please download Java from  this link . . Now that we’ve cleared the pre-requisites, we can download the IDE! . Proceed to the  SAP Development tools  webpage and download the relevant package for your environment. . Extract the zipped files to either somewhere on your harddrive e.g.  C:\\SAPWebIDE  on Windows, or to  /Applications/SAPWebIDE/  on OS-X. . Your workspace will be a folder inside of this directory called  serverworkspace , I would keep back-ups of this folder if you aren’t using source control! . To run the SAP Web IDE locally you should enter your install folder and enter the  eclipse  folder and run  orion.exe , this will run the IDE for you to access at the ip   localhost:8080   and you will be on your own SAP Web IDE! .  Note : For those that don’t know, localhost is analagous to the ip  127.0.0.1 , so you can also access the IDE from   http://127.0.0.1:8080  . . I will be using a local install of the Web IDE on my Windows laptop, throughout this tutorial (and likely this whole series). . When you (create an account and) log-in you will see the Orion dashboard. .   . However we aren’t at the Web IDE yet, we must now navigate to  http://localhost:8080/webide/index.html  and this will bring you to the Web IDE, as pictured below: .   . Navigate to the  SCP Cockpit  and you should see the Cockpit dashboard, as pictured below: .   . SAP applications follow the Model-View-Controller (MVC) architecture. MVC architecture is a design pattern that decouples the three major parts of any application, the model, view and controller. .   . As you can see in figure 1 above, this is the basic workflow of an MVC application. The user’s actions are routed to the  Controller , which in then manipulates the  Model . The Model updates the  View , which is what the user sees and interacts with. . The model is the business logic of the application, it manages the data, logic and rules of the application. The model is typically used to store data, which can be retrieved via the controller and displayed in the view. . A view can be any outputted representation of data, such as the front-end of your application or at a component level, a chart or diagram. The view will change according to changes in the model. . The controller is essentially the application router, it takes user input and sends the relevant commands to the model or view. . In a SAP project there are a number of folders and files that you will not recognise if you are not familiar with MVC. Let’s have a quick overview. .   . Within a given project, you have three core folders. What are they? You guessed it: Model, View and Controller! If we create a single page in our application, let’s call it  homepage , then there will be a model, view and controller for this  and  for every other page you want to create in your application. .  Note : An incredibly useful resource is the  SAPUI5 Explored , it provides a comprehensive list of UI5 entities as well as examples (including code) of how they are used. It’s just like a developer network for a language and I will be referencing it throughout this section. . Open up your IDE for developing the application, if you’re using the SAP Web IDE then we want to click ‘New Project from Template’. .   . This will open a dialog and you should select the tile that says ‘SAPUI5 Application’, this will give us a simple bare-bones application. .   . At this point you will enter your project name (whatever you like, I’d call it ‘playground’ or something similar) and click finish. . Now you have your bare bones project, if you click the run icon as pictured below. .   . Clicking this will build a basic run configuration that will run the application when it is selected and you click run. We’ll get into other run configurations at a later date. .   . The  index.html  is what instantiates the application, if you look in the file you can see in the header that not only are we loading a bunch of SAP scripts but we then run a script that starts the SAP shell and attaches it to the DOM. . To adjust some of the values for the title etc, we must edit our properties file which holds this information and is referenced whenever we want to print the title name. Which brings us to Internationalisation (the i18n folder)! . As mentioned earlier in the post, there are the MVC folders but there is also the i18n folder. This folder acts as the localisation for the application, within this folder is a property file which consists of key-value pairs. . For the purposes of this tutorial, this properties file will only hold the applications title and description in whichever language you choose. For a real application however, you could use this folder to hold property files for a variety of different languages. .  Note : To make sure that your application is properly localised, use i18n labels for form labels and the like. . For the purposes of this tutorial we won’t be editing the controller or model. We will only be editing the view to get used to importing UI5 controls. . Navigate to your view folder and open your view. If you followed the instructions properly when creating the project, this should be called  View1.view.xml . It should look like this: . In the View tag (from the mvc namespace, as defined in the very same tag), we define a number of important things such as our namespaces for MVC, HTML and basic SAP. We then we use a number of tags, all imported from the  sap.m  namespace, our default. Other namespaces we have set up for this view have an alias such as  html  or  mvc  and you must prefix their controls. . Let’s add another namespace for UI5’s Layout. In your initial  View  tag, add the following line:  xmlns:layout=\"sap.ui.layout\" . Now that we’ve got the namespace available to use, let’s add one of it’s controls to our view. . Add the above code into your  content  tag, save and run the application. .   . Now have a brief look through the SAPUI5 Explored site and find a control you’d like to use in your project. . Found one? Good! I’ll be using the  ‘Add Bookmark Button’  action control. . The first thing you need to do, is check which library the component comes from. This is the first line of the entity page on SAPUI5 Explored, for the component I have chosen this is:  sap.ushell.ui.footerbar.AddBookmarkButton . The library in this case is  ushell . . To access a component you have to import the corresponding library, so in your view file add the following code:  xmlns:ushell=\"sap.ushell\" . .  Note : An issue with UI5 is the inability to load a single component from within the framework, a hacky workaround is to take the JavaScript model of the component and import it as you would a custom control (which will be covered at a later date), but this will have issues where there are dependencies you are unaware of! . So we’ve mapped our chosen component library to a namespace, now to use the component we have to prefix it’s XML tag with the namespace we are calling it from. So in my case my tag would look like this: . To see which parameters should be included, you should look at the properties of the control and check a code example. You can view a code example by clicking on an example and then clicking the code button in the top right corner of the window. ", "date": "2017-08-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Capgemini UK Engineering's Response to the Google (Anti-)Diversity Manifesto\n", "author": ["Capgemini UK Engineering Team"], "link": "https://capgemini.github.io/engineering/Capgemini-Engineering-Diversity-Manifesto/", "abstract": " A lot has been written about the 10-page memo by former Google employee James Damore.  Following on from  the folks at MongoDB , we, the Capgemini UK Engineers, felt the need to share our thoughts on it with all our peers in the worldwide engineering community. . We’re not going to go into the science.  There have been  Quora posts  as well as  articles  which have responded in detail to each of the points raised in the memo in far more detail.  There’s little point adding to the voices in this area - there are others who know far more about the current state of the science of individual differences (from the perspective of evolutionary psychology and beyond) than any of us.  Suffice it to say, we agree with the conclusions of the above experts; that there are undeniably individual differences between everyone, and that in some areas there are individual differences between the sexes. However, to use any of these as a basis for a conclusion of the kind which states that one group in particular is on average more biologically suited to software development is simply wrong. . This said, we do however feel well-placed to comment on what makes a good software engineer.  We know from personal experience that more than anything else, the reason these extrapolations (from carefully-selected traits to a general superiority in software engineering) are flawed is because they fail to acknowledge the multi-faceted nature of this discipline within which we have made our careers.  And ironically, it is this very complex mix which makes diversity and inclusion more than simply a moral crusade or a nice to have. These focuses are imperatives for anyone who wants to engineer the best solutions and ship the best products. . How can we be so sure? . Let’s take a step back for a second and think about something as simple yet fundamental as Test Driven Development. When we do TDD we know that there are three distinct mindsets at play in every single line of code that we write. There is the thinking about what it needs to do (from the perspective of the consumer - the failing test, aka the Red). Then there is the thinking about how it needs to achieve it (the line of code to make the test pass, aka the Green). Finally there is the redesign, to make what you just wrote clean, well-designed, and structured. . That’s three mindsets just for a single line of code. . That’s when you’re at the keyboard alone. . Three distinct, different, equally hard, core things. . But wait; what if you’re pair programming? Or if you’re working as part of an Agile team? Or trying to understand the concepts that are in your user’s head but that they can’t quite get you to understand? . How many skills are at play then? . And we’ve not even started to think about planning, architecting, testing, debugging, deploying, monitoring, scaling, fixing, maintaining, and decommissioning your code. Or the code that one of your peers wrote. Or even something that someone you never met wrote, but that you depend upon to get your job done every. single. day. . How many skills are at play then? . This is our key point.  And one which we feel needs to make  indelibly  clear. While there are no objective measures of software quality, or productivity, or even value; there  are  things we do know. How do we know? From our hundreds of years of collective software delivery experience. . We know what the job entails. Sure, it entails abstract, hardcore, intellectual graft; but it also entails creativity, human interaction on a massive scale (with developer-peers, and non-developers alike). It requires understanding, compromise, dogged determination, self-motivation, and a willingness to surrender. It requires distance. It requires focus. It requires communication.  It requires planning and management. It requires guessing. It requires continuous learning. It requires humility. It requires a sense of fun. In short, it entails a lot. So much in fact, that no one individual has all these skills. Not even the ones who’ve been doing this a long time. . We know some other things. . We know that the ultimate results of combining all the above are good products and working solutions. We also know that the  only  way to get the best of all of these skills is via empowered teams; inclusive teams who trust each other and support each other; teams who collectively are far stronger than any individual’s strengths; teams who between them encompass not only a myriad of skills-mixes, but also a rainbow of viewpoints and thought patterns.  And the result?  They deploy their powers, and those of their peers, to greater, frequently unimaginable effect. . There is only one way to achieve this. That is when the working environment is a safe and inclusive place; a place where individuals can be the worst at something, or try something spectacular and fail; a place where individuals can express themselves, be themselves, disagree, and then come together with a better, alternative solution.  That is when the environment is one which supports people with these skills to work as teams, and to build amazing things together. .  This  is why diversity and inclusion are vitally important.  Diversity makes us all better, and inclusion ensures that everyone can contribute; and consequently the end results are the best they can be.  We have a lot of stuff to imagine and build in the coming years, and we’ll need all the help we can get - let’s not put up even more unnecessary barriers. ", "date": "2017-08-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Managing Cloud Infrastructure in Amazon Web Service using VoiceOps\n", "author": ["Nagaraj Govindaraj", "Rushil Soni"], "link": "https://capgemini.github.io/cloud/Managing-Cloud-Infrastructure-in-Amazon-Web-Service-using-VoiceOps/", "abstract": " Amazon Echo has steadily become the hottest smart home product on the market. Suddenly, every tech company wants to integrate its products with Amazon’s customizable virtual assistant,  Alexa . . Alexa is Amazon’s voice controlled service on many devices like Echo, Echo Dot, and Echo Show. Alexa provides capabilities or skills to create a more personalised experience. The integrations keep coming, at such pace it’s hard to know exactly which products/services will work with it. Alexa provides features which include: . Voice recognition system . Adaptable behaviour . Skill capabilities . Scalability in the cloud . Engaging experiences . We joined Capgemini as members of the first Summer Intern programme based out of Telford, UK. Shortly after our induction we had been given the task of creating cloud Infrastructure in AWS using an Alexa enabled device, admittedly this was an open-ended task which could be tackled in different ways, but we were trusted enough as young professionals to be given free rein on how we would tackle this task. . We relished the challenge at hand and saw it as a unique opportunity to learn some new technologies and services, which would otherwise not have been exposed to us, had we not undertaken this internship. . We spent our first week getting familiar with the new technologies and tools that are required to complete this project, in addition to collecting functional and non-functional requirements from stakeholders, allowing us to scope out the required work using a  Trello  board with a  Kanban  agile model. The Trello board allowed us to easily track and delegate work to each other, whilst providing a highly readable overview of the project. . An Alexa skill to create, delete and provide a status of CloudFormation stacks running in AWS. . Have appropriate invocations which are short and easy to call for each subsequent command to Alexa. . Make sure actions which would cost the business money or cause a lot of damage should be protected by authentication. . A help action which gives the capabilities of the skill. . Allow multi-user access from a single Alexa enabled device and have varying levels of access to capabilities for each user. .   . When deciding on how we would handle the integration of the Alexa Skill to AWS services, we settled on using AWS Lambda over other alternative solutions such as  Elastic Beanstalk ; due to its serverless platform and ease of implementation. . We developed the Lambda component of the skill concurrently in  NodeJS  and  Python  with both versions providing identical functionality, due to members of the team having varying skill sets and specialisation. . Detailed below is how the Alexa skill will integrate with the Lambda function. .  Alexa Skill               Utterances         Intent           . Utterances . Intent . AWS Lambda              Handler           . Handler .   .  AWS Lambda  .  Alexa Skill  .  AWS S3  .  AWS CloudFormation  .  AWS Simple Notification Service  (One Time Passcodes) . Agile is an iterative process so we developed an MVP (to be presented to stakeholders) for the Alexa skill which uses voice activated commands to create cloud infrastructure from CloudFormation template files, which can be either written in  JSON  or  YAML . When instantiated these templates create CloudFormation Stacks. . Basic integration of core components. . Created a basic CloudFormation template for testing. . Created a CloudFormation stack from a basic template. .   . These are generic actions which incur a negligible cost or are free (Allocated a monthly allowance of free access to services required to run actions). .   . Each action which requires authentication needs to prompt for the username and a passcode which can be elicited from the user in the form of a slot from an Alexa enabled device. These privileged actions (create/delete cloud infrastructure) have an authentication layer wrapped around them. We looked at various options such as QR codes and access credentials. A decision was made to use one time passwords (OTP) which would be sent to an authenticated users mobile device. The user speaks the code to Alexa, and Alexa validates that code is correct and either allows or rejects the action. . A decision was made not to make skill session based, as that would grant access to the skill for large periods of time, instead the user is required to follow the authentication model each time they want to execute a privileged action; this prevents bad actors from interjecting and running commands as the user. . The code can be found in our  GitHub repository . . The solution was developed with scalability and future development in mind, this was achieved by following a modular approach, which allows for modules to be interchanged easily should a better approach be found. . This section will represent the basic functions used for individual components in the development of the project. There are many functions involved to incorporate different features to Alexa with specific conversations. To make it easy, we have declared all the constant variables at the top of the script to change the values according to the requirement. We want to portray the main features in a sequential way dividing into appropriate snippets which follows as: . Dependencies and package library . CloudFormerCreateIntent . CloudFormerDeleteIntent . CloudFormerListTemplateIntent . CloudFormerHelpIntent . These are the main intents used in the script which will be discussed below in detail. .  1. Dependencies and package library:  CloudFormer requires that the dependencies outlined below in the code snippet be included for the skill to function correctly. The cloudformer-node package has been forked and tweaked (the forked repository can be found  here , all the other packages are used as it is. .  2. CloudFormerCreateIntent:  The create action requires elevated privileges which can only be obtained if the user is authorised by an administrator. This works by specifying the option number of the stack, which the user requests to be created. In the event that the user is authenticated the stack is created, otherwise an access denied message is returned by Alexa. .  3.\tCloudFormerDeleteIntent:  The delete action also requires elevated privileges; this works by specifying the option number of the stack which the user requests to be deleted. In the event that the user is authenticated the stack is deleted, otherwise an access denied message is returned by Alexa. .  4. CloudFormerListTemplateIntent:  The ListTemplate intent loops through the specified S3 Bucket in AWS and then Alexa outputs information about each CloudFormation Template (JSON and YAML) found within the bucket. Note, that the JavaScript Promise is used to ensure that the list of templates is returned for the intent to handle. .  5. CloudFormerHelpIntent:  The help intent has been designed so that it is similar to an expert system, as the majority of people would have prior experience using such a system; this allows for a user to intuitively follow through the intent to get whatever assistance they may require. . The other minor functions to have been included in the script are: . A count of the templates that exist in the S3 bucket. . A lists of existing stacks and their status in CloudFormation. . A global IAM role has been used with a custom policy with the minimum required permission on the AWS resources. . We will be demonstrating our Alexa VoiceOps solution at the  Digital and DevOps Meetup in Birmingham on the 14th of September . See you there! . We’ve thoroughly enjoyed the project and the journey it has taken us on, during our short stint here at Capgemini; it has provided us with an invaluable insight into the processes and culture which the company prides itself upon. We highly recommend this summer programme to students who are passionate about Technology and Consulting. We look forward to gaining similar opportunities and continuing our journey with Cloud-based development projects in the near future. . There are a few additional features which we would have liked to implement had we had some more time, these can be found below. . Billing alerts to individual users for running resources in AWS. . Using DynamoDB tables to store the authorisation keys which allows expiration after time limit. . An authenticated action which can add/remove users from the skill by modifying users and their phone number in the database. . DynamoDB table to keep track of stacks to prevent duplicate names. . Add user tag to created stacks. . We would personally like to thank our stakeholders and who were involved throughout the development of this project. We appreciate their consistent support and input. .  Developers:  Jordan Lindsey, Nagaraj Govindaraj, Rushil Soni .  DevOps Engineer Interns:  Nagaraj Govindaraj, Rushil Soni .  Robot Process Automation Intern:  Jordan Lindsey .  Digital Delivery Architect Intern:  Sara Jebril .  Scrum Masters:  Nagaraj Govindaraj, Rushil Soni .  Stakeholders:  Douglas Thomson, James Devaney, Jonathan Sugden, Les Frost, Paul Taylor ", "date": "2017-08-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Dotnet, Docker, DevOps\n  ", "author": ["Jastinder Singh Atwal"], "link": "https://capgemini.github.io/docker/dotnet-docker-devops/", "abstract": " Since starting on my journey down the Microsoft Open Source road, one of the things I have been introduced to is Docker. Increasingly, I have looked for opportunities where I can use Docker when developing my .NET applications. DevOps has always seemed like a good fit. . I like the consistency Docker gives me. I can develop, build, test and publish my applications in a consistent environment. It also means transitioning my .NET Application across environments should be relatively frictionless (if I choose to deploy as a Docker container). . As background, as a NET developer, I now have  two options  when I start to develop my application; .NET Core or .NET Framework. With the .NET Framework, it’s mature and been around for ages. With .NET Core, it’s an open source, cross platform, high performance framework and has made huge progress over the past 2-3 years. For me .NET Core is future for .NET, especially when we are thinking about the cloud. . Digressing slightly - which would I choose? An important consideration when making the choice is the risk I am taking on if I choose to deliver using something new like .NET Core. It’s not that I wouldn’t use NET Core, but I know from experience that the risk is greater. If I choose the mature .NET Framework then I know what I am getting. Ideally, what I need is the  best of both  worlds – the assurance that I get of using a framework that I have used for years and something that is future proofed. . Here I would like to focus on a gap (coming back to Docker). The DevOps story around Docker and .NET Core is pretty good. There is already support to  build  and  run  .NET Core applications in Docker. You can also  run  NET Framework applications in Docker. But if I want to use Docker to build, test and publish both .NET Framework and NET Core applications? . To achieve this, I created a  Dockerfile  layered on top of the  Windows Server 2016 Server Core  image. To compliment this image, I installed the latest  MsBuild tools ,  .NET Core ,  various .NET Frameworks  and  NuGet . These are some of the tools I might expect to be installed on a server I want to use to build a .NET Application. . But instead of a server, I have the tools I need in a container. And this container can be used by the build system of choice (as long as it has docker installed). I get consistency across my build server(s). I have chosen the tools above, but another team may choose different tools to meet their own requirements. I could extend it to include my build agent. Docker offers much greater density across physical/virtual host, whether its on-premise or cloud. For this example, it offers a model where all build agents across the organization are hosted on a single machine with a view to reduce provisioning costs. . At this point it’s worth taking time to mention the build system that has been used to achieve this. The Git repository is built using  Cake . I’m not going to talk in too much detail about what Cake is, but  Cross-Platform DevOps for .NET Core  is a good read if you are interested to find out more. To summarize, it allows me to specify my build script in C#, that can be exercised by the build system of choice. It means that this repository is built on both Linux using  Travis  and Windows using  AppVeyor . It’s an example of a single repository hosting all docker files regardless of if they target Windows (as is the case with this one) or Linux. A docker image is only built if the sub-folder that contains the Dockerfile or associated files change (see below for the link to the GitHub repository). The final image is pushed by the build system to the Docker registry under  capgemini/net-build . . So, how can this image be used? The  capgemini/net-build  image is built to run as a Windows Container. It can be used with either  Docker for Windows  on Windows 10 (switching to Windows containers), or  Windows Containers on Windows Server . The commands here were tested on Windows 10, build version &gt;= 14372 and  Server 2016 running on Azure  sized at Standard_D1_V2. Take the following command (that can be used during continuous integration) to publish a Windows Classic .NET Framework application: .  docker run -t -v \"$(Get-Location):C:/app:rw\" --rm --workdir C:/app capgemini/net-build:1.0 powershell.exe \"msbuild /p:Configuration=Release /p:PublishDir=./output/ /t:Publish\"  . To break this down,  docker run  runs the container.  -v \"$(Get-Location):C:/app:rw\"  mounts the current directory in the container.  capgemini/net-build:1.0  is the build image.  msbuild /p:Configuration=Release /p:PublishDir=./output/ /t:Publish\"  is the powershell command that gets executed in the container to run the build and generate the output to a folder called  output  that you can find in the same folder as the application being built.  --rm  removes the container when the build is complete and it exits. . Let’s look at another example. Let’s say that you have a newer .NET application that targets both .NET Core and .NET Framework. In your project file (.csproj) you may have the target frameworks set-up as  &lt;TargetFrameworks&gt;netcoreapp2.0;net47&lt;/TargetFrameworks&gt; .  netcoreapp2.0  is .NET Core and  net47  is .NET Framework. . If you want to publish the output from the .NET Framework application, you can run the following command at the root of your solution: .  docker run -t -v \"$(Get-Location):C:/app:rw\" --rm --workdir C:/app/src/MyApplication/ capgemini/net-build:1.0 powershell.exe \"dotnet publish -f net47 -c Release -o ../../frameworkoutput\"  . The subtle difference with this command is that the powershell command is run in a sub-folder src/MyApplication/ through setting the  --workdir , after which you should find the build output in the folder  frameworkoutput  at the root (the same as your mount point). That folder is probably considered as your build system artefact. . To publish the .NET Core variant: .  docker run -t -v \"$(Get-Location):C:/app:rw\" --rm --workdir C:/app/src/MyApplication/ capgemini/net-build:1.0 powershell.exe \"dotnet publish -f netcoreapp2.0 -c Release -o ../../coreoutput\"  . In this case  coreoutput  is picked up as the build artefact. These examples show how to capture the build output, but you would probably use  Multi-stage builds in Docker  if you are adding your build output directly to an optimized Docker image ready for staging and production environments. . To test an application, the following command can be run (to the test .NET Core variant): .  docker run -t -v \"$(Get-Location):C:/app:rw\" --rm --workdir C:/app/tests/MyTests/ capgemini/net-build:1.0 powershell.exe \"dotnet test -f netcoreapp2.0 -c Release --logger trx%3bLogFileName=../../../TestResults.xml\"  . You should now find an xml test results file that can be parsed by the build system of choice to find out how your tests went. . The  Capgemini/dockerfiles  git repository contains the source code, including the build script used by Cake. . The docker image described here is more DevOps focused and can be used to build, test and publish applications developed to run on .NET Core and .NET Framework. The docker image is built using Cake on both AppVeyor (for Windows) and Travis (for Linux). It demonstrates that when using Docker to build applications, its quite easy to transition the approach across build systems because the same rules apply within the container regardless of what kicks off the build process. ", "date": "2017-10-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Design Thinking: Cutting through the fluff\n", "author": ["Leon Hassan"], "link": "https://capgemini.github.io/engineering/design-thinking/", "abstract": " Design thinking: What is it and why do I care about it? Design thinking is a framework for finding ideal solutions to complex problems, through observation, empathy and experimentation centered around humans. . You might think that this is all rather fluffy and without real content, but I’m here to show you otherwise. . The following post is a collection of my thoughts following a Capgemini design thinking workshop run by  Treehouse Innovation  and is part of a wider initiative within Capgemini to promote design thinking across the business. . Design thinking always aims to find the most desirable solution, questions about feasibility and viability are left until the end of the process. .   . Inspire . Ideate . Implement . These three phases form the double diamond design thinking methodology. There are many other methodologies, however they are all relatively similar. .   . A fundamental concept of design thinking is the “converge-diverge” thought process. Design thinking teams should by nature be a diverse group, with many different disciplines. This is because it will give a greater variety of perceived and observed problems, as well as possible solutions. .  Converging  is working together to come to a common understanding whilst  diverging  is work undertaken on your own and then taken back to the group. As you can see in the diagram above, the two diamonds represent the rough converge-diverge pattern, by phase of the process. . In the Inspire phase, we do three things: Define the challenge, generate observations and distill them to insights. . Defining the challenge can be quite difficult, you have to be careful in so many ways to create an effective solution. . A good challenge statement will convey the issue you are trying to solve without baking a solution into the statement itself and without being explicitly measurable. . Observation is the most important step in this phase because this is where you will be interacting with the people who should benefit from your solution. . Three tools that we can use to gain observations are: Desktop Research, Field Observations and Interviews. .  Note : We must be very careful not to project our thoughts onto the observations, only comment on what you have actually observed and been told. Observations should be uncoloured by our own bias and specialist knowledge. . Desktop Research is simply research done from your desk. Looking up stuff on the internet, reading books and journals. Desktop Research is usually the first port of call when preparing for interviews and field research because it is quick and easy, it gives you almost instant access to a number of resources which will help you to phrase your questions for further research. . Desktop Research allows you to look into what people are doing in your competitors and other industries quickly, it should not be exhaustive or particularly deep. Desktop Research should inspire your decisions and preparation for further research. . In field observations it’s important to leave all preconceptions at the door! Assume a beginners mindset and be genuinely curious. Take  lots  of pictures, both of the people at work and the environment they work in, keep your eyes open for shortcuts and workarounds. Why are things done a certain way, how do people circumvent the ‘norm’? . The following are a few field observation methods: .  Fly on the wall : Go and observe people where they live, work, shop and play. Think like a detective, take pictures and note down anything that piques your curiosity. .  Shadowing : Ask a members of the usergroup if you can shadow them. Request that they show  and  tell you how they go about their tasks. .  Role play : Physically put yourselves in the shoes of your usergroup and see what you can see from this perspective. .  Photo journals : Ask members of the usergroup if they can capture photos from their lives, related to your topic that they believe are important or interesting. If possible, ask them to annotate the reason for each photo or meet with them and allow them to walk you through the photos. .  Similar situations : Beyond these direct observations, you can observe similar situations in other industries and markets to see how they tackled the problem and if you can extrapolate that knowledge and relate it to your own challenge. . It’s always good practice to enter an interview with a few questions in mind, but don’t be afraid to allow the interviewee to lead the conversation down other routes as this often gives you a keen insight into their character and values. It’s also very helpful to always enter an interview with two people, one to lead the conversation and another to take notes. There are a number of things to keep in mind when organising your interviewees and while interviewing them: .  Extreme users : These users have strong opinions whether positive or negative, getting extreme opinions will give you the inspiration to accurately frame the problem. e.g. amateurs and professionals, people who love or hate your topic. .  Context : Whenever possible you should be communicating with users in their relevant context, put yourself in the user’s shoes and allow them to  show  you what they mean. .  Friends and family : Don’t underestimate the power of your existing network, your personal connections and extended network often provide safe and easy information. .  Remote : It’s not always possible to be face-to-face with your interviewee, get them on video chat and give them some questions in advance so that they can consider their answers. . Share all of your observations with the team and then independently come up with insights. Observations are fact, pure and simple. However in gathering these observations we gain a certain level of understanding the “why” behind the observation, the motivation behind it. . Insights should focus on what you believe the motivation behind observations are, they can be drawn from a single observation or many. . An insight should  not  be how to fix something, or a possible solution. An insight should be an opening to this, a part of your challenge statement that you could focus on to solve the issue. I find it useful to start insights with “How Might We …?” (A.K.A HMW), for instance: HMW make junior developers feel more comfortable sharing their thoughts and ideas? . There is no baked solution in the insight and it is open-ended, allowing everyone to try and find their own solution to this. . Empathy maps are a fantastic tool to accurately represent a user’s experience and help you to understand their needs. .   . You map out the information they’re giving you against this map and it helps you to identify the user’s values and true issues, articulated and hidden. It can also be helpful to notice if you haven’t covered any areas when filling in the map. . To get truly insightful information, you should always ask about best and worst experiences because these situations evoke emotion. Following this emotional response ask why they feel that way, find out what makes the user tick. A final question to really drill down and find what the root of the problem was, is to ask ‘what’ and ‘how’ it happened, the events that led to this situation. . To recap, a good general line of questions is: . What is your best/worst experience . Why was it your best/worst experience . What led to this? How did it come to this? . Share insights across the team and then group them, it’s very likely a few of you will have come up with similar insights. . Once you’ve grouped similar insights together, talk through each of the groups and maybe spend a few minutes talking about each insight in the group. . Now everyone should pick their top 3 insights (using dot stickers is really helpful!) and see which of the insights come out on top. .  Note : It’s important at this point to be able to get behind other peoples ideas and it will become more important as we move into the brainstorming. . Now that we have our top insights, we can start to come up with ideas on how to action the insights. . Defer judgement. . Encourage wild ideas. . Build on other’s ideas (especially the ones you don’t like!). . Stay focused on the topic. . One conversation at a time. . Be visual, a drawing is worth a thousand words. . Go for quantity, just keep putting more ideas in. . Following these rules makes for a very productive brainstorm. Making sure that everyone is heard and not judged leads to many varied ideas, which is exactly what we want. Lack of discussion helps to keep the brainstorm at breakneck pace: announce your idea, explain it and then get back to creating more ideas. .  Note : If you’re struggling to come up with ideas, use a ‘defibrillator’. Add a constraint that makes you approach the problem differently, you can get method cards that do just this from places such as  IDEO  and many others. . Now that we have our insight and our HMW, why do we want to do this and what is stopping us? These two questions will help you further distill the information we have and lead us closer to a solution! You can summarise this by creating new HMW’s. . Asking ‘Why?’ helps us to abstract our HMW, to see it in the bigger picture, however asking ‘What is stopping us?’ helps us to narrow down our problem. . Armed with our HMWs, plan small-scale cheap experiments to see if you are going down the right track. A good rule of thumb is to limit yourself to £100 spending power for the experiment. . Experiments should be prove to some extent, a single piece of your functionality. For instance, let’s say we’ve decided we want to set up “Uber for Bins”. A good experiment would be to give one street access to a prototype, allow them to book a timeslot to have their bins emptied. All you need is a man and some kind of van, easily within budget and it will give you a good idea if people would or would not use the service. . After performing your first experiment, you can start to prototype your solution based on your findings and just iterate this cycle with each new experiment. . Once you have a suitably mature prototype, with a few good experiments and findings to support your choices then it’s about time to start pitching. . If you enjoyed reading about design thinking then I implore you to have a look at the  Change by Design  book, written by the CEO of  IDEO . The design thinking process, and many of the tools in this article are a product of IDEO’s relentless push to do better. . This is a pretty rough-and-tumble introduction to design thinking, if you have questions about anything please drop a comment below. Likewise if you get to apply this, please drop a comment about your experience of how it went. ", "date": "2017-09-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "To SQL or not to SQL\n  ", "author": ["Paul Monk"], "link": "https://capgemini.github.io/design/sql-vs-nosql/", "abstract": " A common question that many developers and architects struggle to answer is which technology stacks should I use for my project, and more importantly why? This decision is particularly important when it comes to picking a database, as databases are often the backbone of an application. They affect which other technologies are used and the overall performance of the application. There are many articles that describe the advantages of different database software, and every company likes to advertise their database as the best. But there are circumstances when different types of database are better than others. . SQL databases have become the standard database for most applications. There are clear standards (both ANSI and ISO) that SQL databases must meet, and there is extensive support for them, both on a community and enterprise level. The main strength of an SQL database is its relational data model, where referential integrity between data can be guaranteed. . The SQL language is well known, with many functions and a concise syntax. Simple commands can be learnt quickly, however it takes a lot of work to master the language and database design. . Tables can hold millions of rows of data, and still maintain fast performance. SQL databases are optimised to hold and search through large amounts of data, and indexes can be used to speed up search and retrieval of rows. . The query language is powerful enough to carry out several commands in one query. Queries can be nested inside each other, and in Oracle databases SQL is extended by  PLSQL , adding an aspect of procedural programming to aid in building queries and carrying out complex queries with many logical steps. . Having been around for many years, and being widely used in many enterprise applications, there is a wide range of community and enterprise support for SQL databases. There are also many different databases that are of an enterprise standard. . With support for indexes and lots of other optimisations, SQL databases are the fastest for carrying out searches of data over a single table. Performance can drop off when searching across multiple tables, so a good design is important to maintain performance. . Due to foreign-keys and the maintaining of referential integrity between tables, SQL databases can retrieve data across several tables quickly. The SQL language also has good support for this using JOINS. However as stated above a good design is important to maintain performance, as well as optimised queries. . As the majority of SQL databases are very mature and run on a single server, it means that they are highly available and data is always consistent as there is no need to synchronise data over many servers. . Data migration is always an issue with SQL databases. Every time you want to add a new column, or delete one every row of the table is affected. If you start updating foreign keys then maintaining the referential integrity of data can be a nightmare. This typically results in large migration scripts to adjust data every time the data model changes. . Many languages have support for converting data from an SQL database into Objects that can be used programmatically. However configuration can often be fiddly, the phrase “it’s easy when you know how” really does apply here! . The architecture of SQL databases mean that they can generally only be run on one server, so expensive hardware is needed to cope with large amounts of data and high demand. . As SQL databases generally run on one server if that server goes down then the database can’t be accessed. There are workarounds for this such as having multiple servers listed in the connection strings, but this isn’t as flexible as having multiple servers running for the same database instance. . Here are the most popular types of SQL databases. As well as the common pros and cons that apply for all SQL databases, each of these have their own strengths and weaknesses. . Oracle is the most popular enterprise standard database, it has a large number of users and so has extensive support if you are willing to pay for it. There is a free version (express), but the features are limited so it is only suitable for small applications. It is designed to hold large amounts of data, has support for PLSQL, has additional commands like tablespace, synonyms and packages, and has several backup mechanisms. It has wide support across many major programming languages. This is the SQL database to use for large enterprise applications that hold lots of data, and need a fast reliable database. . MySQL is a partly open source database engine, with MySQL server edition being closed source, both are managed by Oracle. It again has a large user base and a large amount of enterprise support. It also has extensive community support due to its open source nature.  It is lightweight and fast to set up, and still has almost all of the features the Oracle database does, except for the support of PLSQL. It also has wide support across many programming languages. One downside of MySQL is that performance can drop off when storing large amounts of data. This should be used as a good free alternative to Oracle, for smaller applications that need a fast setup and don’t need a lot of computational resources. . MariaDB is a fork of the MySQL database, it was created when Oracle took over the development of MySQL in 2010. It has many of the features of MySQL and is fully compatible with MySQL drivers and libraries, so programming language support is high. It has speed improvements over MySQL, and so scales better to large amounts of data. It has a lot of community support as it has been growing in popularity over the past few years, but its enterprise support is not up to the standard of Oracle’s. It is however beginning to diverge from MySQL so may not always be compatible with it in the future. MariaDB should be used as a slightly faster alternative to MySQL, and is in the spirit of true open source! . PostgreSQL is another open source database. It has some specific use cases, and so in general isn’t as fast as other SQL databases. However it excels at joining large numbers of tables, and searching through large amounts of data. It also has support for PLSQL, unlike MySQL and MariaDB. PostgreSQL should be used for complex data models, where lots of tables are present and queries are complex. It can be used as an open source alternative to oracle when PLSQL support is needed. . NoSQL databases in their varying forms have gained traction over the past few years, as their community support has increased. NoSQL has become a phrase used to describe a wide range of different data storage technologies. There are no official standards for NoSQL so there are many different implementations each with their own strengths and weaknesses. In general the main strength of NoSQL is its flexible data model, where columns can be added/removed from Collections (tables) ad-hoc, without the need for any data migration. . Unlike SQL data is kept in flexible collections instead of tables. This means that “columns” can be added or removed to a single object without affecting any of the other objects in the table. This means that no migration scripts are needed when the data model changes. . NoSQL is designed to run across multiple servers, and be multi-threaded. The data across all servers may not necessarily be up to date. But it is easier and cheaper to add more servers to improve performance, instead of buying expensive hardware, this also means there is no single point of failure. There are many examples of using Raspberry Pis to create MongoDB clusters. . Large objects can be stored without the need for massive tables like in SQL. This makes NoSQL very efficient for the reading and writing of large amounts of data in large single chunks. . NoSQL doesn’t have all the constraints that SQL has, so it is very fast for reading/writing to single collections. Simple queries can be used to search for data in a collection, and many databases support indexing to further improve speed of retrieval. . As NoSQL databases can run across multiple servers when setup they naturally have a high availability, and aren’t dependent on just one server staying up so they have great fault tolerance should a server fail or have to be taken down for maintenance. . There is no checking of data between collections in NoSQL. There is no concept of a foreign key either. This helps make the data model flexible but can cause discrepancies in the data, and can make it slow to search for data across different collections. . Many NoSQL databases are open source and haven’t been widely used for enterprise applications, however some of the most popular databases do offer good support networks. As there is such a wide range of NoSQL databases performance and reliability also differ greatly between them. . NoSQL databases use their own bespoke query languages, often in varying forms of Javascript as there query language. It is easy to learn initially but queries can get large and complex quickly, with a lot of curly brackets! There are also big variations in query languages between different platforms, so just because you know one doesn’t mean you can use any NoSQL database. . Due to the lack of referential integrity and absence of foreign keys, getting data from different collections requires a full search of each collection. This can make linking data together slow when the data model is complex. . Running across multiple servers causes issues with data consistency. If running the database across several servers it will take some time to propagate updates to all of them, meaning sometimes you can get different data from different servers. Some NoSQL databases (like MongoDB) cope with this better than others. . Here are the most popular NoSQL databases, each of these is a different take on the NoSQL technology so their ideal uses cases vary greatly. . MongoDB is the most popular and well known NoSQL database. It stores its data as JSON documents, inside collections which are akin to tables in SQL. As such it has a wide level of community and enterprise support. It is very flexible with objects being stored as JSON, which can be easily converted and used programmatically without the need for complex configuration. It is fast, scales well. Data consistency when running over several servers can be maintained by using the “red concern” option, which ensures that data held by the “majority” of the server is returned instead of the local copy which may be out of date. It does have a limit on the size of a single document which is 16mb, however this can be overcome by using the GridFS API. There is good support for many different programming languages, and it naturally fits in with any Javascript program. MongoDB should be used if you need a fast database that is very flexible and can store almost anything successfully. . Neo4j is a Graph database written in Java, its design works around many of the limitations that hold other NoSQL databases back. It stores data as key-value pairs (nodes) which are all linked together in a graph like structure. For example a person may be a node, they may be related to an address which is another node, there may also be another person which is also related to the same address. These relationships between nodes help maintain a certain amount of referential integrity. Graph databases offer excellent performance when the data model is complex with a lot of links between different objects, in this sense graph databases outdo most other SQL and NoSQL databases while still maintaining a flexible data model. Both community and enterprise support is available for Neo4j. The main use case is for heavily linked data, where data would be spread across several tables in an SQL database.  However it will be slower than other databases for data models just requiring a single table in SQL. . Cassandra is a Column database by Apache, its query language and data model are the most similar to SQL’s out of the 4 listed NoSQL databases. It does have slight differences however, as the data model is row-orientated instead of column-orientated. This means that each row can have different columns, and don’t have to contain all the columns that other rows have. It can be used to store large amounts of data, and is particularly fast at writing data to the database. Being so similar to SQL the learning curve for Cassandra is less steep than other NoSQL databases, when converting from SQL. However it does lack scalability when data models are complex and data is spread over multiple tables. It has excellent community support, but enterprise support is only available through third party sources. It should be used to store large amounts of data in a single table, and where high fault tolerance is needed. . The Oracle NoSQL database stores its data as key-value pairs, but it also has support for SQL style tables and JSON documents. The Key-value pairs are stored in a similar way to Neo4J, however Oracle has the concept of a “Master” key (e.g. a unique id) which links all the “Child” keys together. There are both commercial and open source versions available. Being Oracle it has excellent enterprise level support available. It can store large amounts of data and process it quickly, it is best at storing simple data models without relationships. It can cope with a large number of read/write requests. However it doesn’t support transactions like a standard Oracle SQL database. The ideal use case for Oracle NoSQL is an application with a simple data model and a high volume of traffic such as network monitoring. . There isn’t always one database that’s perfect for the job, and even within the SQL and NoSQL categories there are vast differences between the databases. However in general the following rules should be considered when choosing the database for your next project: . There is logically related data which can be identified up-front and is unlikely to change . Data integrity is essential . Consistency of data is more important than partition tolerance . You want standards-based proven technology that developers are used to working with . Commercial applications with good enterprise support is preferred . There are unrelated, indeterminate or evolving data requirements . Simpler or looser project objectives, perfect for Agile projects where you are able to start coding immediately . Speed and scalability is important . Partition tolerance is more important than consistency . Open source technology with good community support is preferred ", "date": "2017-10-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Ways to Skin a Cat\n  ", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/frontend/react-for-java-devs/", "abstract": " I’m too old to be a front-end developer. Talk of  frameworks  and  js-library bingo  makes me feel like a WWII veteran at a segway convention. But then I went to  Devoxx US  and heard Ben Ilegbodu’s talk, ‘ Navigating the React solar system ’, and I thought — it sounds like this stuff is actually getting there. The front end world now looks and sounds much more like the Java world - the same patterns are being used, the same issues are being addressed in similar ways. So when there was a project floating around to migrate an Angular 1 application to a more up-to-date JavaScript framework (don’t ask me why…) I decided to take it on. So, how do you learn React? First thing to know is that React is not a JavaScript framework in itself, so you will need a supporting structure for your application.  . Ben recommended Facebook’s  Flux  design pattern in his talk, so I decided to take a look at  Redux , a “predictable state container” for JavaScript apps, which evolved the ideas of Flux but avoids its complexity. To get started, I followed  Wes Bos ’s enjoyable “ Learn Redux ” video tutorial series. On video 1, I realised I was missing some fundamentals, and I’d need to learn  ES6  first. Otherwise, when faced with a line of code like this: .  submit = values =&gt; return (&lt;div&gt;{ …items.slice(0,1), [values.id]: doSomething(state[values.id], values)}&lt;/div&gt;)  . even a Java 9 programmer might become a little disorientated!  . ES6 is short for ECMAScript version 6, sometimes known as Harmony. ECMAScript is a scripting-language specification created to standardise JavaScript across multiple independent implementations (other implementations including JScript and ActionScript but JavaScript being the best-known implementation). Browsers are not yet ES6 compatible, so currently you need to include a compile step (or “transpile step”, as these futuristic front-end developer beings like to call it) to translate to ES5 and get your application ready for use. I don’t think this is a big deal, though, as it gives you the opportunity to automate code quality checks using the transpiler and tools like JSLint. I found  this post by Eric Elliott  just the right level I needed to get me up to scratch for Learn Redux. . As soon as you mention you are using Redux, a ton of people will ask you  why . For me, my application was a sort of wizard form which builds up a JSON object based on various decisions the user makes whilst navigating the form, so the implementation of state as a single plain object was incredibly useful. The state can also be easily part-populated at the start via a server call, which is exactly what I needed, and the “time travel” is very useful in testing and proving the app. I’d say that thanks to Redux the new app implementation is much more robust, and the  Redux Dev Tools  plugin for the browser really helped development (although to be fair, the  React Dev Tools  was even more useful). . The simple basis of React/Redux applications is that your data is in a store, and it is  immutable . Actions on the website trigger reducers, and reducers return a new store state based on the action type. If, like me, you’re historically a Java developer who has been through the hell of multithreaded programming, you’ll understand the beauty of this. If state is immutable, you don’t need to worry about how multiple threads will change it — it doesn’t change. Phew! And the functional, event-driven design of the actions carrying out the updates is very neat. An action simply takes some parameters and fires an event of a given name, with the parameters contained in a JSON object. These events are then picked up by reducer functions, which update the state. The wiring is just a couple of lines of code - you define a mapStateToProps() function which says which pieces of state are updated by which reducers, then you define a mapDispatchToProps() method which links in all your action code to the dispatcher that will dispatch the actions, then you connect it all together by calling - yep, you guessed it, the connect() function. Lovely. . So, you’ve learned something new, you’ve justified your decision, time to write an app! Of course there are plenty of ways to start — from scratch, by copying the Learn Redux code, or — my favourite — check for a template. That way, you should get a working application out of the box with all the necessary libraries for running and basic testing, plus build and deployment scripts via npm, and all you need do is apply your design to it. I found  this handy blank template  for React Redux. Then I opened it. It bore ABSOLUTELY NO RESEMBLANCE to the code I’d seen in my training course. . React Router what? Four?? No nested routes? What’s a React Form? Why am I getting error messages that my components should extend React.Component? Argh. . A quick google took me to this great video by the React Training people, and got me up to scratch with the newer routing concepts. Routing is completely separate from the app, so you can build your app first and add the navigation in later. As I’m building a web app, and not (ie) a NodeJS native JavaScript app, I use react-router-dom library. In the tutorial I watched, it was necessary to create a browserHistory object and pass it to the router to store history, now there is a new BrowserRouter class which does that for you. To link somewhere, use the Link class:  &lt;Link to={'/url'}/&gt; . You can pass in parameters to the URL using  ${param}  — for instance .  &lt;Link to={'/page/${page.id}'}/&gt;  . Then, in your main app, wrap your components in a   elements and then include   elements which include the path and the component (or render method) that this route should generate if triggered. For example,   .  &lt;Route exact={true} path=”/” component={Home}/&gt;  . When to use component and when to use render? Well, I found that if you want to pass propeties down to a component, you can’t do it by passing the properties to the Router element. So say I had a sub-component called Comments that needed access to the Comments action, I couldn’t do  &lt;Route path=”/comment” comment={this.props.comment} component={Comments}/&gt;  because this would make the comment parameter accessible to the Route only, and not the Comment component. So I had to create a method like this: .  const MyComment = props =&gt; { return (&lt;Comments comment={this.props.comment} {…props} /&gt; ); }  . and then use render rather than component in the Router element .  &lt;Router path=”/comment” render={MyComment}/&gt;  . Then, from within my Comments component, I can call  this.props.comment(text)  to access the comment action. There’s some more ES6 here — the spread operator (…) which means “put everything from this object in here” and is really handy for recreating state, and the lack of brackets around the props param — if there’s only one parameter to your function you don’t need to wrap it in brackets. . The first step in building a React app with Redux is to design the shape of your store. In my case, I was just building up one JSON object to send to a server, so I could simply lift and shift the existing JSON into the store structure. It can be hard to find somewhere to document this store shape, as the actions and reducers only usually provide part of the overall store shape, and unless you have an initial state from the server there might be nowhere to visualise the whole thing. The README.md might be a good place to paste in the whole JSON object, just for reference, but then of course you have to be sure to update it. A unit test might be a better place. To actually define your store in your application code, you need to use the createStore() method in the redux library. This takes a combination of all your reducers (see below) as a parameter. You can use the redux method combineReducers() to link them all together. .  const store = createStore(combineReducers({reducer1, reducer2, …. });  . As mentioned, the Redux store is updated by a set of actions. So, once you have your store, you can create a method for each type of action that can update it, and specify the values that the action will need to pass to the reducer. For instance, adding a comment: .  export function addComment(postId, author, comment) {       return {          type: ‘ADD_COMMENT’,          postId,          author,          comment       } }  . There’s some ES6 in here, if your object key is the same name as its value, you don’t need to write both. So .  { postId }  . is short for .  { postId: postId }  . Saves loads of typing whilst maintaining readability. This is all you need for your action. You’ll have a reducer to act on each type, take the parameters and update the state. . Remember the reducer doesn’t alter state, it returns a new state. The reducer method takes the state object and the action (as returned by your action method) as parameters, and returns a new state object. So if your state was an array of comments, your comments reducer might look like this: . Note we’ve added a default empty state here if none provided. It’s common to have an initial state, possibly loaded from a server, as a const variable and pass this in to your reducer, for instance: .  function whatever(state=initialState, action)  . Somewhere you’ll have some kind of main component which can do things like: . Include the header and footers . Include the Router (see React Router 4 above) . So, let’s write this next. As mentioned, to create a component you can now write a class that extends React.Component and just provide the  render()  method to return your HTML and elements. So a very simple component would look like this: . For styling, I decided to use SCSS because I could then lift and shift the styles from the existing app. My build pipeline compiled the SCSS to CSS, so I could just include .  import ‘./Main.css’  . into my component to style the divs. In the component, you have to use  className=&lt;class&gt;  instead of the keyword class= , as it’s like a property (`el.className`) and not the string-typed attribute `(el.setAttribute(‘class’))`. Same for the browser keyword for — in React you have to use htmlFor.  . Because we do TDD, right? Ahem. So, here are the tests we ALREADY WROTE. The good thing about React / Redux / React Forms / React Router is that a lot of the logic is tested for you. You only really need to test the specifics of your methods and components.  The template I used had enzyme built in for the testing, and this appears to wrap Jasmine so the format was easy to understand, if you wanted the chai expect format you’d have to import that separately. I did need to add in some further libraries but more on that later. For your actions, you really only want to test that you’ve passed in the right parameters to run the reducer, so you can create an expected action (e.g. .  expect(actions.specifyUser(postId, author, comment)).toEqual(expectedAction)  . For components, a simple test is whether it renders. You can use the ReactDom render method to test this: . Note I’ve had to give a Provider here to pass the store to the context of the Main component, you can mock a store using redux-mock-store library’s configureStore() method. . To wire up a React Redux app, you need two methods — mapStateToProps and mapDispatchToProps, and then call connect() on them.  . “actions” above are all the methods you wrote as actions — so for instance  .  import * as actions from ‘./myactiondirectory/myactionjsfile’;  . and bindActionCreators() is impoted from ‘redux’ and binds all the methods together. A well named method… This will allow all your action methods to be visible in your component properties, and it will decide which pieces of state need to notify which reducers when they change. This means that your reducers need only fire and switch through a subset of your state — so in the above example, the posts reducer wouldn’t fire on a comment update. Now we need to call connect. .  connect(mapStateToProps, mapDispatchToProps)(Main)  . You can see here I’ve called the function twice, once with our state and dispatch methods, and then once with the component we want to wrap (in this case, Main.js) A standard logical breakdown is to put mapStateToProps, mapDispatchToProps and the connect method in a separate component file — for instance App.js. So we have: . Then, in your index file, you’d just call ReactDOM.render(  , document.getElementById(‘root’)) to begin the tree. To pass properties and components down the Redux component chain is to wrap a component in a Provider element — I did this a root level. .  &lt;Provider store={store}&gt;&lt;App/&gt;&lt;/Provider&gt;  . This allows the store to be visible to the Main component as a property on the context. But after that, you might want to be more selective about which bits you pass down by passing specific properties into the elements, e.g. in Main.js I could say: . instead of having to wrap the   element in a provider.  . Next learning curve was  Redux Forms  — very useful for adding data into the store from HTML form fields, also for validating form entries. I had a lot of “if this selected then display that” type form logic, and a lot of validation on the fields, so this seemed to make sense. Redux Forms differs slightly from the React component classes I’d been writing, in that I had to provide a method with an object parameter which returned some HTML to render, and then create a form using the reduxForm() method like this: . where ‘comments’ defined the name of my rendered HTML form Then, in the code to be rendered, I use the   element to define my inputs. You can either use the default rendering:  .  &lt;Field name=”addAuthor” component=”input” type=”radio” value=”Yes”/&gt;  . Or, you could specify your own render function to add labels etc: .  &lt;Field className=”&lt;cssClassName&gt;” component={renderField} label=”Comment” hint=”Add your comment here” type=”text” validate={required}/&gt;  . and then renderField is defined like this: . Note the use of classNames() method from the JavaScript ‘classnames’ library to allow flexible multiple classes to be passed into an element. This way I can add the error class when needed. Also note the use of conditional rendering using {variable &amp;&amp;  }. This way, for instance, the hint is only rendered if you provide one, and the error is only rendered if the form field has been incorrectly set. Cool, huh.  . The final cool thing in the Redux Forms definition is the validate parameter, which takes a function to call to validate the field. A validator might look like this: . When it came to testing the validators, I’d initially defined them in my form component class, and was having a hell of a time trying to get a unit test to set the field with an incorrect value and check that the rendered HTML contained the error class on the span element, for example. After a while and a bit of research, I decided that I hadn’t written the logic of displaying the error on validation fail, so I shouldn’t be trying to test it. I should be just testing the required method itself. This gave me a bit of a headache as in order to access it I’d have to export it from the module, and exporting just for unit test purposes doesn’t sound right. In the end I moved all my validation methods to a different file, wrote individual tests for them like this: . and then imported the validators into my form component. Feels much better! ", "date": "2017-09-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Embracing the Legacy\n", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/legacy/embracing-the-legacy/", "abstract": " “Wait what! You’re still using that version of X?!” . We’ve all been there. You’re on a new project and the first thing you’re made painfully aware of is that it is a ‘legacy’ project. .  Legacy … . That single word is often enough to send many a developer into panic mode, followed by hot sweats and the bitter taste of disappointment. Out of reach slides the dream of a nice ‘green field’ project, where you can design, innovate and build beautiful fresh  new  code. In turn, leading to a much fuller development experience, and being safe in the knowledge that you’re going to come away with some nice shiny new skills. . I think the comic below sums up how it can feel to work on legacy projects sometimes… .   .  Source  . Okay okay, I’m painting a pretty bleak picture here, but it’s true! As developers we often get disheartened ( not quite to the point of hot sweats though I’d hope …), when we find ourselves on a legacy project that seems frighteningly out-of-date. What’s more, we find ourselves working on some form of legacy code base more often than we do on brand new ‘ green field ’ systems. In addition, this isn’t likely to change anytime soon, as  legacy systems still play a large role in the enterprise workplace . The thing is, legacy code isn’t going anywhere, but neither is the disappointment of coming across badly written code that’s ages old, and the drab realisation that we’re not getting to use the latest shiny software stack. However, it’s not all doom and gloom, there are ways of making working on legacy code a bit less painful and a bit more enjoyable! . It’s time to  Embrace the Legacy … . I know what you’re thinking; . ‘How can you embrace legacy, it’s old and boring!’ . Yes, legacy code can be  very  old and legacy systems can be somewhat boring to work with at times, there aren’t many developers who will argue that fact. However, embracing legacy projects isn’t about trying to remain emotionally stable whilst you try to make it through to the end of the project. It’s about embracing the concept and mindset that allows us as developers to utilise our skills and best practices, in order to leave any legacy project we might work on, much better than we found it. . So how do we do it? . I believe there are several important aspects to ‘Embracing the Legacy’ and I know there are many more that could be added to this list, but these are the ones I believe can really have an impact on both developer and project when it comes to working on legacy systems. . From personal experience this is a biggie. I joined a project recently and got landed with a role that involved the transformation of an old legacy system. Did I get slightly disappointed when I knew how old the system was and the technology stack it utilised? Yes. However, I looked at the technology stack, the requirements of the Proof of Concept ( PoC ) we were tasked with, and thought to myself; ‘I could instil some change here for the better’. One key aspect of the PoC was to move away from proprietary software and go open source. .  Open Source … . Those two words immediately open up the door for change and the chance to bring in some new technologies that could not only mean fresh skills, but also a more up-to-date tech stack to work with. Moreover, the first win I had was to move away from using Rational ClearCase for our version control system ( I did tell you it was an old legacy system …), and start using Git. . Win number 2 came shortly after, as part of the transformation work ( moving from Weblogic 10 to JBoss EAP 7 ) I posed the idea and migration plan for moving from an old Ant build system to Maven – already thinking ahead into automated builds and deployments – and that was taken on board and adopted. So already, within a few weeks I’d gone from the initial ‘legacy’ reaction of disappointment, to ‘this could actually shape up to be quite a good opportunity’. . More to the point, when we’re working on legacy code or projects it’s easy to get caught up in the initial disappointment of what can  seem  like the prospect of a bleak project, but it doesn’t have to be that way. .   Small wins count  . . Even if you can modernise and introduce change into small pieces of a legacy project, it’s already one more thing that you will have improved on and left in a better place than you found it. Clock those up and before you know it, you’ll have made an ‘old, boring’ project, into something a little more ‘interesting and modern’. . One of the biggest gripes I have with working on legacy projects is the mentality of ‘copy and paste’. Whenever I come across duplication in the code I always think to myself it didn’t start off this way, somewhere down the line someone fell into the copy and paste trap. What’s more, it’s an easy trap to fall into. You’re tasked with writing a new function, but it’s the same as something that already exists in the codebase, so you just think;  ‘it’ll be easy to just copy this and I can refactor it later…’ . Only  later  never comes around and it’s one more piece of duplication to be added to the pile that is picked up by the code analytics software you’re using. .  You are using  code analytics software  aren’t you?  . Building code should  not  follow that age old mantra of; why do today what I can leave for tomorrow, it should be the opposite! In fact even if you spot code duplication and you haven’t introduced it yourself, treat it as your responsibility to refactor it! A good way to think about this is by following a principle very similar to the  boy scout rule ,  always check a module in cleaner than you found it . If you can’t refactor it at that moment in time, then add it to a  technical debt  page somewhere  visible . However, be careful; technical debt can easily build up if not addressed, so it’s important to keep it to a minimum if you have to add any at all. After all, as developers we want to  build technical  wealth  not technical  debt  ! . The one hope you have as a developer when working on legacy code is that there is a good testing foundation in place. In fact, coming from the previous point about refactoring, a lack of tests seriously hinders any refactoring work you want to do, as with legacy systems it’s hard to make changes without being safe in the knowledge that the existing tests will catch any problems you may create. Sometimes, when there are no tests - or at least very little - it becomes hard to justify to the business that actually spending time to build up a good regression test base will yield further rewards down the line, and the short term hit in project time is worth the long term gain going forward. . Furthermore, I was deployed on a project last year where I was tasked in designing and building a new regression testing framework. Firstly, I’d like to point out yes, it was another legacy project ( although not as old as some that I’ve worked on ), but the technology stack wasn’t super out-of-date – which made a nice change. There was a poor regression framework in place already, but it needed to be replaced, so we ended up running with a cucumber BDD style, functional testing framework, utilising selenium web driver for driving the browser style tests. For me this was cool; a nice opportunity to design and innovate, work with some new technologies and learn some new skills. Remember those  small wins … . Moving swiftly on, we made the conscious decision to take the hit in project time to build up a good foundation of regression tests that would give us much more value in the long term, compared to the time ‘lost’ on building it in the first place. Moreover, we were able to tie these tests into our Jenkins builds which lead to eventually having nightly builds and deployments, followed by a complete run of our regression suite on a fresh database build. In turn, this meant that each morning we were able to routinely check the test runs, safe in the knowledge that if any failed it was quick and easy to find the  culprit  reason ( no blame culture of course ). . It’s not only about building new test frameworks for legacy projects, or introducing new testing concepts such as functional browser tests for web applications. It’s also about rejuvenating existing tests; maybe refactoring test code, updating the version of the test framework and updating the test code to reflect the new features available. What’s more, it’s important to realise that just because the system is legacy, it doesn’t mean you have to put up with bad tests, or a lack of tests. There are always ways to improve coverage, or introduce new testing methods in order to add further value to the project – and make it that bit nicer to work with. . Legacy projects don’t have to mean legacy methodologies as well. Gone are the days of manual processes ruling the roost, it’s time to automate! . In the last few years,  DevOps  has really come into its own, and a part of DevOps is to automate stuff; automate builds, tests, processes and deployments. If nothing else, automating those manual projects saves time and effort. Time and effort that could be utilised on much more important things ( remember that technical debt list? ). Of course, it’s not just about automation, and if you’re following a DevOps based approach, there’s sometimes a  limit to how much automation you need . . Allowing time and planning in automation tasks can make a huge different to any project, but especially legacy projects where automation hasn’t been a priority before. Linking back to my earlier point about rejuvenating the tests, having  test automation in place is a real benefit  to all projects, but with legacy projects it can give you that extra assurance and safety net when doing all the other goods things like introducing new features and refactoring code. . Gone should be the days when manual builds and deployments rule legacy ways of working. It should be almost  mandatory  for projects to embrace automated builds and deployments nowadays, because there are a  host of benefits to doing so . Besides bringing a bit more life into an older, legacy project, the time it can save in the long term can be utilised to address of areas of the project in need of attention. More importantly, it allows us to do one of the things we love doing as engineers,  innovate ! Although DevOps might be considered a more modern practice in software projects, it can very much  be part of legacy projects too . . At this point you’re probably wondering what to take away from all this. I hope that when you were reading through this post, you noticed that the points I’ve raised don’t just apply to legacy projects. . This post could be about any style of software project, the reason for writing about legacy projects in particular, is that we can get so hung up on the word  legacy , we often forget that we’re still building software, we can still design and innovate, and yes, it  is  still possible to write beautiful code. . For me,  Embracing the Legacy  is a way of thinking and a way of working that allows us to move past the fact we might not be on the most interesting projects, utilising the most cutting edge technology. However, we can still bring our breadth of experience to the table as engineers, and use our skills and knowledge to transform legacy projects into something truly worth working on. ", "date": "2017-03-24T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Devoxx UK 2017 Giveaway!\n", "author": ["Shana Dacres"], "link": "https://capgemini.github.io/agile/devoxx-givaway/", "abstract": " Enjoy a free ticket to one of the most sought after developer’s conference of the year, Devoxx UK 2017, courtesy of Capgemini Engineering. . Following a hugely successful 2016 which saw us as  Gold level sponsors of Devoxx UK  and delivered two brilliant sessions on  Harnessing Domain Driven Design for Distributed Systems  and  NFR Testing and Tuning - A Scientific Approach , we’ve decided to do it again. But this time   we’re platinum baby!   . Capgemini is set to return to Devoxx UK for a very special conference experience.  @Gayathri  will be speaking again, this time discussing  Distributing Memory in Distributed Systems , and exploring the potential pitfalls and solutions to distributing memory across distributed systems. . Taking place at The Business Design Centre on the 11th and 12th of May 2017, your only way (apart from buying a late bird ticket) into the ultimate developer’s conference is to win a ticket! . With a ticket you will be able to wonder into sessions such as  Microservices for the Masses with Spring Boot, JHipster, and JWT ,  The Anatomy of Java Vulnerabilities ,  The reactive landscape , and  The DevOps Superpattern . It’s an immersive, not to be missed, multi-disciplinary experience inspired by developers for developers. . Our major theme this year is role models. So we would like to know who your role model is and why. Here is an example: . Happy #IWD2017. Every woman who breaks through the stereo type and proves #yeswecan is #MyRoleModel. My mum, dad, hubs and my boss #heforshe. . All you have to do to bag yourself a ticket is . Follow the  @CapgeminiUKppl  handle on twitter. . Tweet us  @CapgeminiUKppl  who your role model is and way using the hashtag  #BeYourOwnRoleModel  . Not on Twitter? You can always send an email with your full name to  technologyconferences.uk@capgemini.com  with the subject “Devoxx UK 2017” and tell us who your role model is and why.  Yep, it’s that easy! . You can find out more about Devoxx and take a peek at the vast range of  content being delivered here . Final tickets are currently on sale at £510 (inc. VAT). But if you fancy saving the cash and bagging yourself a FREE ticket, enter our competition and tweet us by  23:59 on the 1st  09:00 on the 8th of May 2017.  Just keep your eyes peeled in case you win and we get in touch…    Good luck!   . Capgemini reserves the right to amend the competition end date at any time. . If you are the lucky winner we will notify you via your twitter handle or the e-mail address supplied shortly after the competition end date. . The prize will not be transferable to another person. . No part of a prize is exchangeable for cash or any other prize. . Travel and accommodation is not included. . Competition ends  23:59 on the 1st of  09:00 on the 8th of May 2017. . We’re involved in tons of conferences / meet-ups and our people attended, speak and help organise them too ( Women of Silicon Roundabout ,  Lead Developer Conference ,  ContainerSched ,  FATJIL  –just to show off a bit).  We’re really keen on developing and expanding the knowledge of our people, it’s part of our culture and it’s who we are. If your interested in joining us  our eyes are always open for the best talent on the block . ", "date": "2017-03-31T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "QCon London 2017\n", "author": ["Rune Offerdal"], "link": "https://capgemini.github.io/culture/qcon-london-2017/", "abstract": " I had heard great stories about QCon. It’s the conference where practicing engineers find new trends and technologies and learn how to adopt them. Did the content match the inviting wrapping? This is my experience from  QCon London  2017 as a first time QCon participant. . The QCon London conference is a three day event. Each day starts off with an inspiring keynote, after which the conference splits into seven tracks that are focused on a particular topic. One of the unique selling points (USP) of QCon is that all tracks are curated and speakers join by invitation. This makes all presentations stand out as unique even within a track, and the track topics themselves seem to be chosen from coming trends. . As a participant you have complete freedom to select which presentations to follow. You may focus on a particular track or pick presentations freely to get something from several topics.  I tended to do the latter. . The tracks were chosen wisely and covered a lot of ground.  Here are some of the 21 track topics: . Architecting for failure . Dark code: The legacy/tech debt dilemma . Applied JavaScript - Atomic Applications and APIs . Engineering Culture @ . Architectures You’ve Always Wondered about . Fast &amp; Furious: Ad Serving, Finance, &amp; Performance . Modern Learning Systems . Containers - State Of The Art . Workhorse Languages, Not Called Java . Softskills: Essential Skills for Developers . Modern Distributed Architectures . Practical Cryptography &amp; Blockchains: Beyond the Hype . Observability Done Right: Automating Insight &amp; Software Telemetry . Security: Lessons Learned From Being Pwned . There are not many buzzwords in those titles, and that is in essence what QCon is about. It is less about hype and more about practical lessons and experiences.  Still the following topics were covered well: . DevOps . Internet of Things (IoT) . Blockchain . Containers . Microservices . Security . Agile . Cloud . Machine learning . Continuous Delivery . The refreshing focus on hands-on experience not only makes it interesting, but also useful. The speakers are engaging because they are engaged.  These are not sales people with all superlatives and no substance.  This is the real deal.  These are people that have fought the battles of adopting new tech and emerged on the other side with better solutions and experiences to share and learn from. . The concept of QCon is very good. The focus on curated content pays off. It is obviously a well-oiled conference machine behind it, but still there are issues that can be improved. . What is the thing about black on white bullet point PowerPoint presentations? A conference presentation isn’t and shouldn’t be a white paper. One would expect the track hosts to have tools for QA to assure a minimum of quality. Bullet point hell should be avoidable these days. The same goes for expert language. Few people share the same subset, and QCon has a diverse audience, despite the focused selection of topics. To be fair most presentations avoided these pitfalls, though, but the exceptions were obvious. We can all learn from the contrasting experiences. . There were also food, exhibition areas and social happenings. These were OK, but not the reasons to visit QCon London 2017.  Enough said. . Coming back from QCon, I think the most common question I have been asked is “What was new this year?” . “New” seems to matter, but I found few topics that were completely new.  Based on experiences from other conferences, I did however notice a focus shift from “Look! New and shiny!” to “These are our best practice experiences from adopting this new tech.” This practical approach seems to be another USP for QCon. Less spectacular and much more useful. . Still, I think the conference started with a bang through the keynote “ Security War Stories: The Battle For The Internet of Things ”. This was an engaging talk about how much is already connected to the Internet and the security risks associated with that. As engineers we all know the importance of security software and keeping up with security standards. What was considered Fort Knox level security five years ago may be as secure as leaving your jewelry under your beach towel today, so upgrades are mandatory. When more of the physical world gets online, the same security decay happens in things on the Internet. How will they keep up? The speaker Alasdair Allan warned about losing sleep over this topic. This is certainly a hot potato to handle for the future. . After three days of presentations I had a lot of new information. It was more about the “how” than the “what” of the new technology trends. Here are some of the things I felt inspired to do after QCon. . Distributed architectures and microservices form the basis for a world of continuous delivery of functional apps. Many app solutions are distributed already, but may still inherit some of the less desirable properties of the monolithic past. I am currently engaged in an ongoing transformation from legacy monoliths to distributed apps, and especially the presentation “ From Microliths to Microsystems ” shows how the transition to microsystems also means adopting a new set of best practices. . IoT is growing and growing fast. Soon everything with a function is connected. It is only a question of time before there is no clear line between IoT and the classic Internet. When your shoes are on the Internet and can carry software, there is no reason they can’t carry malware. Can a camera in your IoT-enabled glasses be hacked to pick up your pin codes when you enter them? . DevOps is the integration of development and operations with continuous delivery as the driver. As old monoliths are replaced with distributed microsystems, the number of servers to monitor and maintain rise exponentially. This is a monitoring and debugging disaster waiting to happen unless there is a focus on a unified way of monitoring operations, detecting errors and providing information to analyse it.  There are many pitfalls here.  There is the danger of information overload, while there is also a danger of not logging enough for analysis. In the presentation “ Avoiding Alerts Overload From Microservices ” Sarah Wells of the Financial Times made it pretty simple: “I only want alerts for things that require me to act” and “When something goes wrong, I’d like everything to be logged and available for analysis”. . Asynchronous vs synchronous communication between microservices was mentioned in several presentations, and it is one of the paradigm shifts that come from moving from a monolith to microservices. Thinking asynchronous communication as your default is new. Even if it makes a lot of sense, I personally need to adapt my mindset to embrace this. . The tools of the cloud and microservices are still emerging and maturing as new needs rise in the distributed world.  One example is container orchestration. As each microservice needs one or more microservers in containers deployed on different cloud providers, these containers with their servers need to be managed. A hot candidate is  Kubernetes , which is an open-source system for automating deployment, scaling, and management of containerized applications. The adoption of these concepts and tools is necessary to be able to connect the dots for a complete distributed systems architecture. . The western world embraces youth. New is often better, but in the names of the Minidisc, DVD-audio and Google Glass, new stuff sometimes fail or are short lived at best. Sometimes the first attempt fails, only to pave way for the successful next generation. Since many presentations in QCon are based on experience, they also often recognise how new ideas go together with established principles or is simply a way of reinventing existing ideas in a new context.  An example is how microservices are not a new idea, but as new technology has enabled distributed systems and cloud deployment, microservices has moved from idea to practical solution. Really good principles are long lived. Engineering great solutions means joining experience and wisdom with the new opportunities. This is not an easy task and a continuous learning process. . Even if completely new buzzwords were absent at QCon, I think the hottest buzzword (besides “innovation”) is still “ blockchain ”. The use case that everyone refers to is Bitcoin, but there are a lot of ideas out there on how to utilize blockchain technology. It is a technology with obvious USPs, but also with some weaknesses, like the increasing cost of mining new block hash codes. Transforming the blockchain ideas into workable software products and new ways to remedy the weaknesses of this technology are opportunities for innovation. My guess is that this is where new experiences will emerge in the next couple of years. If they do, we will find them on QCon. ", "date": "2017-04-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Capgemini Microsoft Dynamics 365 team at Extreme 365\n", "author": ["Ben Hosking"], "link": "https://capgemini.github.io/dynamics365/capgemini-dynamics-365-team-at-Extreme-365/", "abstract": "   . \"Invest in the future because that is where you are going to spend the rest of your life.\"   Habeeb Akande  . Capgemini Microsoft Dynamics 365 UK team attended Extreme 365 hosted in Lisbon, it's important to understand where Microsoft Dynamics 365 is heading to make sure we understand and use the latest features. . Extreme 365 is a conference with a mix of technical demos, business presentations and the tagline \" learn + connect = grow \" . Over 700 people attended the conference held in Lisbon, allowing people to attend sessions and meet people from the Microsoft Dynamics 365 community. . Extreme 365 features the road map of Microsoft Dynamics 365, great sessions and the chance to meet passionate people.  It was an opportunity for various Microsoft Dynamics 365 teams of Capgemini from around the world to meet and socialise. . The Capgemini Microsoft Dynamics 365 UK team is part of a bigger multinational Capgemini Dynamics team covering 10 countries who already meet regularly with Skype. The Extreme 365 conference offered an ideal opportunity for some of the Capgemini team to meet in person. It’s helpful to meet in person and put names to faces so when we are collaborating and sharing information on calls for projects and bids the interactions are more personal and meaningful. This year at Extreme 365 Capgemini had individuals from Norway, Holland, UK and Portugal attending the event. . The Capgemini Microsoft Dynamics 365 team proposed session “DevOps for Enterprise Scale Microsoft Dynamics 365 projects” was rejected because of the lack of Enterprise scale partners attending Extreme 365. . This was disappointing to the team because DevOps has benefits for all Microsoft Dynamics 365 projects and Microsoft is trying to push enterprise Microsoft Dynamics 365 projects.  Enterprise Microsoft Dynamics 365 projects is a growing area and there was nothing like it at Extreme 365. . Microsoft Dynamics 365 is evolving rapidly, teams need to understand what is coming up and areas they should focus their learning.  New functionality and services offers an opportunity to learn and lead for those who are prepared and ready. . Extreme 365 is a five-day event, the first 3 days are for Microsoft Dynamics 365 partners and the last 2 days are for customers. . Capgemini Microsoft Dynamics 365 team attended the first three days with the plan of learning new features, products and services in and around Microsoft Dynamics 365.  A key motivator for many attending is to build up their knowledge, profile and network. . The format of the partner event is to have a full day training session from the sponsors. The  ClickDimensions  advanced partner session was insightful and it was great to meet 11 time MVP  Matt Wittmann . .   .  Extreme 365 – Hosk notes on ClickDimension advanced partner training . .  Microsoft Dynamics 365 – Is Adobe Marketing the end of ClickDimensions? . . An exciting enhancement from ClickDimensions is the Enterprise Statistics warehouse .   . The Microsoft Dynamics 365 roadmap was highly anticipated, delivered by   Param Kahlon  - General Manager, Product Management, Microsoft Dynamics CRM.  The highlights of roadmap were: .  Microsoft Dynamics 365  comprises of many different products under its umbrella and comes in two editions: . Operations – Dynamics AX . Financial – Dynamics AX . Sales, Customer Service . Marketing – Adobe Marketing . Field services and project services . Sales – Dynamics CRM . Financials – Microsoft Dynamics NAV . Marketing – New marketing functionality coming soon . A cut down version of sales and marketing aimed at smaller companies.  Microsoft is planning to bring some marketing campaign automation. . The target market is companies who want out of the box of functionality with no complex customizations and quick setup. .  Microsoft purchased LinkedIn  last year and the Dynamics community has been waiting to see how Microsoft will integrate LinkedIn with Microsoft Dynamics 365. .   . LinkedIn will have embedded widgets and activities in Microsoft Dynamics 365 but there was no date when these enhancements will be available. . Microsoft will continue to enhance the existing Sales Navigator in 2017. . Microsoft continues to build functionality around Microsoft Dynamics 365, extending it with related services, functionality and intelligence.  This allows Microsoft to create services which can be used by any of its products and take off the load from Microsoft Dynamics 365 but make functionality available to Microsoft Dynamics 365. .  Customer insights  uses a powerful mix of Microsoft most successful and feature rich services. .   . The road map contained the most interesting information but disappointingly it highlighted existing functionality not new functionality.  The real value of attending Extreme 365 comes from the discussions between talented and passionate people that are triggered by the planned sessions and presentations. . It's important to know where Microsoft Dynamics 365 is heading as those who prepare today will be successful tomorrow. . More details can be found at  Extreme 2017 . ", "date": "2017-05-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A Portable Kubernetes Cluster\n", "author": ["James Relph"], "link": "https://capgemini.github.io/kubernetes/a-portable-kubernetes-cluster/", "abstract": "  Kubernetes  is rapidly becoming the de facto industry standard for container orchestration.  Initially a development of Google’s internal  Borg  orchestration software, Kubernetes provides a number of critical features including: . Service discovery . Container replication . Auto scaling and load balancing . Flexible and automated deployment options . A HTTP based API . The existence of a comprehensive API in particular has allowed Kubernetes to be managed by a number of different tools that allow the rapid build up of a cluster, often targeted towards a particular cloud provider or underlying infrastructure architecture. . The Public Sector Digital and DevOps Unit (PSDDO) is a new capability unit dedicated to enabling Capgemini to respond to the Governments Digital Transformation Agenda.  As part of the build up of the unit the PSDDO Digital Academy was run over the course of 3 months, the aim of which was to assure that all members were up to date on the latest technologies, skills and methodologies.  As part of the Academy process there were a number of practical weeks during which Developers, DevOps Engineers, Delivery Managers and Business Analysts were placed into agile teams.  We were then given specific goals and had to develop a number of microservices and an infrastructure on which to run them.  This was the perfect opportunity to develop an infrastructure from scratch using the tools most appropriate for the job, without legacy requirements. . Our developers wrote a web application consisting of two microservices (using AngularJS), for which we initially built an infrastructure on  Amazon Web Services’ EC2 Container Service  (AWS ECS).  This could be deployed rapidly, was easily scalable and could be configured - and configured quickly - with fairly complicated behaviour in terms of scaling both at the node (i.e. host  EC2 instance ) and micro-scale level (i.e. the individual  Docker  containers on each host).  While this was a capable production level system it did come with the normal caveats associated with AWS ECS: it was not portable to other cloud providers (or to local systems) and there are several “black box” infrastructure objects where detailed control is in the hands of Amazon.  The normal (and oft repeated) discussions apply as to whether those caveats are sufficiently offset by the speed and ease of use of the Amazon tools.  We were, however, fortunate to be in the position where after developing this initial architecture we could try an alternate approach and re-factor the infrastructure into a cloud-agnostic Kubernetes cluster. . We had a two week period to develop our Kubernetes infrastructure from scratch.  As we were hosting on AWS we decided to use kops to build our Kubernetes cluster.  This is, in effect, a shortcut to save time in building the cluster and was of minimal impact to our final architecture.  There are a number of tools that can achieve the same goal in similar ways - kops is currently best suited for quickly spinning up a cluster directly in AWS (although it can also output Terraform templates). . The  kops  tool is available for OS X and Linux, and has two dependencies -  kubectl  and the  AWS CLI .  The aws tool needs configuring with an AWS user who has appropriate IAM rights in AWS to create EC2 instances, Autoscaling groups, write into the desired DNS zone and create an  S3 bucket .  Once those tools are installed and the appropriate user created in AWS then building a cluster can be achieved with a simple command, in our case: . Our cluster was built in the Ireland AWS region, however this can be changed as appropriate (it is worth checking on the number of zones available in each region, and  the AWS tools available in your chosen region ).  The above command creates a cluster with 3 masters and 3 nodes - one in each availability zone.  Kubernetes uses clustered etcd for storing it’s state files, which uses the  Raft  consensus algorithm.  Raft requires a minimum of three members to provide failure tolerance (the quorum requirement is (n/2)+1) so production clusters should always start with a minimum of three masters and increase in steps of 2 as required.  Kubernetes also currently requires this failure tolerance for rolling updates of the cluster masters themselves.  The  $TEAMID  variable was passed in to allow rapid testing of different cluster builds.  One very useful feature of kops is the storing of the cluster state file in an S3 bucket.  This makes team working significantly easier as any team member can quickly download the cluster state file to their machine to enable them to operate directly on the cluster using kubectl: . We wrapped our cluster standup procedure in a bash script (later deployed via  Concourse  - our CI/CD platform) which performed a number of tasks to automate the entire cluster standup: . Build the cluster in AWS . Wait 320 seconds. . Create some persistent storage (EBS in AWS) for our mongoDB instances . Deploy our backing services (elasticsearch, mongoDB, fluentd, ingress controller etc.) . Deploy our microservices . Update the Security Groups in AWS for our ingress controllers . Create DNS entries in  Route53  for all our services . The 320 second wait required is to allow Route53 to update with the internal DNS names of the cluster masters and nodes - without this wait the subsequent steps 3-5 (which all use the kubectl tool) would fail.  The time chosen was based on average maximum values seen with a safety margin of around 20 seconds. . The full version of our script is available in our  GitHub repository  at ci/scripts/spin.sh, the system that runs that script requires the kops, kubectl and the AWS CLI as well as openssh and jq (some JSON parsing is required in updating Route53).  The repository has been sanitised but was built in a very short time so little refactoring or optimisation has been done, so use at your own risk!  Please read the README file in the repository which contains important information about the code.  The relative paths to files containing other example code included in this post are shown before each code block. .   . Our finished AWS architecture was relatively simple by the end, consisting of 6 EC2 instances, some EBS backing storage, an S3 bucket and some changes to Route53.  We deliberately chose not to use an AWS ELB - instead relying on internal load balancing using  nginx  instances.  There were a number of reasons for this - firstly this removed another AWS black-box and removed another AWS specific tool, which could be especially useful in situations where we could foresee cluster federation being used to deploy the cluster across two different cloud providers or between private and public clouds - potentially where IaaS type load balancers are not available (and/or not supported by the Kubernetes loadBalancer directive).  While S3 and Route53 are both specific AWS features those do not need to be local to the cluster (i.e. Route53 could still provide DNS services for a cluster hosted in Azure) and could be rapidly replaced with other programmatic DNS and blob storage. . Secondly this gave us significantly greater ingress control.  Kubernetes handles ingress control with nginx by re-writing the appropriate entries in the nginx config file whenever a service is moved, which is often quicker than an update to an ELB config.  Furthermore advanced configuration could be included in the ingress config (such as L7 based routing) and it gave us the option for replacing our chosen nginx containers with other ingress controllers for a different feature set (such as nginx+, vamp or traefik) without re-architecting our cluster.  The yaml for ingress specification is relatively straightforward, for instance to setup routing to one of our microservices (from ingress.yaml): . The replication controller setup for our nginx instances was a little more complicated, mainly due to the extra arguments that we were passing, including the health check URLs (from ingress_rc.yaml): . There is also a “default-backend” service (a very basic HTTP responder) that answers 404 pages not covered by the ingress controller and responds to a health check with a 200.  This handles unknown domains and is also a requirement if the cluster was moved to Google Cloud Platform. . A very small Docker container then runs a simple DNS script to keep Route53 updated with the appropriate node IPs for the Ingress Controller (again this needs the AWS CLI, kubectl and jq installed) and Route53 Health Checks are used to re-route requests away from failed nodes.  Route53 is updated via a JSON document passed to the AWS API, so we start with a JSON template for Route53 (ci/scripts/dns_update.json): . We then populate that template using a loop in our DNS script.  For speed and simplicity during testing we added all Kubernetes services to DNS, but this can be limited to simply the services that you require.  This example outputs all the services from Kubernetes and their associated ingress controller IPs, and then formats the output and builds the template for Route53 (from ci/scripts/spin.sh): . We believe this gives us a very similar capability to the use of an ELB, without the associated drawbacks. . Our microservices required a  mongoDB  cluster, which we split across all three nodes and provided with persistent storage.  In addition to that, and the microservices and ingress controllers, we built a significant number of other tools into our cluster to provide logging and metrics.  Mostly these tools were chosen due to their simple integration with Kubernetes.  We used  fluentd  to collect logs from our nodes and containers, which were shipped to  elasticsearch , which was accessed by a  Kibana  frontend.  On the metrics collection front we used  InfluxDB  to gather node metrics, and our developers also passed the microservice metrics to InfluxDB.   Grafana  then provides the graphing interface for the metrics. . What this meant is that despite only deploying two microservices each node was running a minimum of 4, but more likely an average of 7 containers, and as a result we had to initially up the size of our chosen EC2 instance (to a t2.large).  This was needed to provide the speed and memory to host all of our required services but we do feel this got us close to the features required for a production level cluster.  The layout below gives an overview of the general layout of our cluster (although Kubernetes itself is in charge of allocating each individual service). .   . Our microservices were created as Deployments in Kubernetes.  Deployments were added in Kubernetes last year (in version 1.2) and provide a declarative method for automating the update of Replica Sets and their associated pods, including being able to use schemes such as canary rollouts.  Creating a deployment is a painless process and we were able to specify the requirements for our microservice in a small yaml file, which also exposed the appropriate service ports.  An example for one of our microservices is shown below (from microservices.yaml): . In this example the microservice is deployed with 2 replicas, and port 8899 exposed as a service. . A Deployment rollout is triggered if, and only if, the Deployment’s pod template is changed.  In a production system this would be triggered potentially by image labels changing or other Deployment information being changed.  Given the short timescale and need for repeated testing of deployments we worked around this by adding an extra piece of metadata to the template whenever we wanted to force a deployment rollout (from ci/scripts/server_deploy.sh): . In this case this simply added the date as a metadata field to the pod template - so whenever we ran that command we could guarantee that the pod template would change and therefore trigger a rollout - which is particularly useful for testing. . One caveat with this approach is that it is not recommended to use the same image label for a new deployment.  This is because Kubernetes will cache the docker image, and if the label matches will not re-download the container (unless a pull policy of Always is specified).  Kubernetes will assume the Always pull policy if the  :latest  tag is specified, although this is not recommended for production as it makes deployment rollbacks difficult.  Kubernetes supports automated rollbacks on deployment issues so for production systems a proper versioning system should be used and deployments triggered by the version being updated in the appropriate template. . Not directly related to Kubernetes but worthy of a mention is  Concourse  - our CI/CD tool of choice.  Concourse merits significant discussion in and of itself; it is a modern, Docker-based CI/CD tool that is fast, scalable, pipeline based and (generally) snowflake free.  By the end of the task our Concourse pipelines could build our Kubernetes cluster from our infrastructure code stored in gitlab (along with tests) and update and deploy our microservices on any push, test them and perform a rolling deployment as outlined above.  We even had a separate pipeline that builds the Docker image that Concourse then uses in the creation of the pipeline - building kops, kubectl, jq and aws into a container using a Dockerfile which then could run our cluster standup script. . As a result our entire deployment can be started by standing up a single Concourse server in AWS, passing it a pipeline that includes links to our gitlab repository and our default variables, then starting the deployment pipeline.  Within 14 minutes Concourse will have stood up our entire cluster, including all services, and will be watching our microservice repositories for any new commits. . It’s hard to overstate the power and flexibility of Kubernetes and it’s certainly not a unique observation.  Within a day we had our initial cluster built within AWS and were able to deploy containers - despite no member of the team having prior experience with Kubernetes.  We then built a system that we consider close to production ready within a 2 week period - not everything is perfect and there are certainly areas that could be tidied up and refactored but, in general, the system works well as a highly available, highly scalable and highly portable cluster.  Using nginx as the ingress controller internal to the cluster also expands the options that are available to develop this further - both in terms of having more intelligent load balancing than is currently available with AWS ELBs and in terms of multi-cloud federated clusters. . Personally, I also believe that the process was incredibly useful in terms of reinforcing the benefits of agile working and modern DevOps techniques - not merely in terms of the techniques themselves but as a clear example in showing how the processes really can very quickly produce working systems that can be iterated and improved rapidly without compromising on quality. . Finally it’s worth highlighting how useful just being able to “play” with new technology is to engineers - being in a situation where a group of tools can really be pushed to see what they can do.  The practical weeks during the digital academy gave us a focussed goal with freedom to try out a variety of tools and this gave everyone involved a much better idea of the scope and limitations of those tools, and what they can be used to build.  The advantages of this kind of approach can be seen in the success of Capgemini’s  Applied Innovation Exchange  (AIE) which aims to solve client problems using the latest technologies and tools.   Malcolm Young  recently wrote a great article, “ A Blurry Look Into the Future ” which describes his visit to AIE’s London Lab. . We’ll be giving a demo of this cluster at the  Birmingham Digital and DevOps Meetup  on May 4th, so please come along, watch it in action and join in the discussions. . Developers: John Eccleston, Yen Kee Miu, Jon Pike . DevOps Engineers:  James Relph, Jack Doherty, Lee Clench, James Fegan &amp; Ian Galt . Scrum Master:  Anita Preece . Thanks also to Andy Eltis, Darren Rockell, Sam Tate and Andy Gibbs who worked on the initial iteration based on AWS ECS.  Thanks to Doug Thomson and James Devaney for setting the practical week challenges and nudging us towards Kubernetes and the associated toolchains. ", "date": "2017-03-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "CQL Statement Builder\n", "author": ["Tosin Ogunrinde"], "link": "https://capgemini.github.io/apache%20cassandra/cql-statement-builder/", "abstract": " Anyone who has worked with  Apache Cassandra  will not underestimate the importance of selecting the right  data model .  However, this could be a daunting task due to the  lack of a generic schema generator (in Java).   For example, each time you create a new class/model or your class/model structure changes, you’ll have to write or rewrite the  CQL  statements required to create or update the schemas in your  Apache Cassandra  cluster. . Automation is a core tenet of DevOps and so I have decided to automate this process. I have written a Java library that: . Generates the  CQL  statements required to create or update schemas in an  Apache Cassandra  cluster. . Builds a range of  CQL  statements from standard Java classes/models. . I have now decided to open source this library! So check out  cql-statement-builder  and let me know your thoughts. ", "date": "2017-05-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Will this Meteor hit?\n", "author": ["Henry White"], "link": "https://capgemini.github.io/development/will-this-Meteor-hit/", "abstract": " Over the past few years, the use of JavaScript as a full-stack web technology has increased through the use of libraries such as React and Angular. As a result, the open-source platform  Meteor  was created. This post looks at the potential of Meteor to change the landscape of website development as we know it today. . Meteor is a full-stack JavaScript platform for developing modern web and mobile applications. . Looking at the platform’s website, it claims to be the “fastest way to build JavaScript apps”. Being the logically-minded individual that I think I am, it appears to be rather difficult to prove/disprove that idea. Regardless, I have gathered that Meteor is currently marketed as the quickest platform to work with from inception to implementation for producing a full-stack JavaScript engineered web solution which can be deployed once for multiple devices (web/mobile). . Accomplish in 10 lines what would otherwise take 1000. . It is true that through the power of minification, the number of lines in a JavaScript file can be reduced by a signficant amount. With a full-stack JavaScript application, I can imagine this being quite effective. But maybe instead of that, they’re claiming that JavaScript is a more lightweight language than the likes of PHP and Java, which is indeed possible - but let’s not go into that. . Use the same code whether you’re developing for web, iOS, Android, or desktop. . This is very appealing and would obviously save a lot of time, but should we consider how many solutions actually require an application to be consistent across web and mobile? Would this feature be redundant for those clients seeking a mobile-enhanced solution? . Use popular frameworks and tools, right out-of-the-box. . As an open-source platform, Meteor does have a collaborative community base. As we’ll discuss later, the package system implemented in every Meteor installation provides a simple and effective way of incoporating modules, addons, plugins (or whatever else you would like to call them) into a site by using a simple CLI. I’m always in favour of not having to re-invent the wheel everytime I write code. . With Meteor being a full-stack JavaScript platform, one would imagine there is code to manage both front-end and back-end requests/responses. Now how is this achieved? Let’s take a look at the typical file structure a Meteor application must follow: .  client  - all code used by the client/browser/user .  server  - all code used by the server .  imports  - all other code/assets/files including the important  startup  (bootstrap) directory . I wouldn’t say that exactly - Meteor uses MongoDB to hold a persistent data layer which exists as long as the server is running. If you are unfamiliar with MongoDB, “collections” (tables) are used to hold “documents” (records). . Here’s where it get’s interesting, a collection can either be created for use on the server or use on the client. Client collections are used to store local data for yes you guessed it, the client! These collections act as cached versions of the server side collections. Server collections represent stored data in the MongoDB database and act the same as a standard table. . Meteor is built from the ground up on the Distributed Data Protocol (DDP) to allow data transfer in both directions. Building a Meteor app doesn’t require you to set up REST endpoints to serialize and send data. Instead you create publication endpoints that can push data from server to client. . By defining a publication in the  api  directory (see above), a list of documents stored in a collection is available for the client to subscribe to and receive real-time results. This allows Meteor applications to be reactive and adapt to ever changing data immediately. . Meteor officially supports three user interface (UI) rendering libraries, Blaze, React and Angular. .  Blaze  was created as part of Meteor when it launched in 2011, and hence is claimed to be most integrated among the three with  Meteor architecture. It uses a  handlebarsJS -like templating syntax with HTML known as  Spacebars . .  React  with Meteor can be achieved simply by adding the npm React dependency to the installation. This allows you to write React components in JSX as with any other React application. .  Angular  with Meteor is officially supported and there is even a separate dedicated community at  Angular-Meteor.com . Although it seems to be a lot more work (bootstrapping) to get both working together effectively than with React or Blaze. . As mentioned before, Meteor employs its own fantastic packaging system known as  AtmosphereJS , previously developed as an NPM package known as Meteorite ( you can read their full story here ). . Atmosphere packages are written specifically for Meteor and have several advantages over npm when used with Meteor. . As you can imagine, this makes Meteor a pretty powerful platform, and encourages the open-source community to collaborate and improve on the platform over time. . One can simply browse the  AtmosphereJS website  and use the Meteor CLI to install packages: . Meteor integrates with Cordova, a well-known Apache open source project, to build mobile apps from the same codebase you use to create regular web apps. With the Cordova integration in Meteor, you can take your existing app and run it on an iOS or Android device with a few simple commands. . Cordova wraps HTML/CSS and JS into a native container to target multiple platforms. As a web app in itself, it means the Meteor application can simply be wrapped to support mobile devices immediately. However, as mentioned before - what if the client wants to expand on the mobile version of an application? Fortunately, there are neat conditionals that can be used to identify the user’s device: . However, depending on how different the mobile version is desired to be, one could argue that keeping both together with countless device conditionals would be counter-intuitive. It may make more sense to split the code into two applications and use some funky routing - but that’s for another discussion. . It’s rather easy to find a wide range of resources, tutorials and even books for learning how to use Meteor effectively. It’s great to see that a fairly new platform already has such a large and passionate community. . Many Integrated Development Environments (IDE) have started to incorporate working with Meteor with new plugins and tools. You can see some of these below: . WebStorm 9  integrates with Meteor  including automatic recognition of Meteor projects, coding assistance, running and debugging apps. . There are several packages available in the Atom ecosystem to assist Meteor development.  Meteor API  offers autocomplete, code snippets, color-coded grammar and syntax highlighting, while  Meteor Helper  enables launching and killing Meteor within Atom. . Do you like reading? Then look no more, there’s as many as 4 books available for you to digest - if that takes your fancy: . Learn Meteor and build faster &amp; simpler web apps, as we teach you how to build a real-time Meteor app from scratch. . An online book about the Meteor JavaScript framework that helps beginning web developers build their first real-time web application with Meteor in a matter of hours. . Let us show you how to make the most of this amazing platform to impress users, reduce development time, and turn your ideas into code. . An easy to follow, step-by-step approach to learning how to build modern web applications with Meteor. . To conclude, I believe that Meteor will most certainly increase in popularity and is in a very good position to push JavaScript to start to replace other technologies for website development. Meteor is extendable, it has countless packages to choose from, or the opportunity to create your own. Meteor is fast, an application can be deployed for multiple platforms at one time. Meteor is efficient, using only JavaScript code the application can be minified to increase performance and improve user experience. No doubt, there are other platforms suitable for developing your application, but it is the fast nature of Meteor development that really stands out. . Try Meteor out  by installing it from their website  - it may just be a game changer. ", "date": "2017-06-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Learning to Lead, Learning to Listen\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/lead-dev-conference-2017/", "abstract": " On election day I went to Westminster. Not for political reasons, but to attend  the Lead Developer conference . I’d been meaning to go ever since hearing good things about it from my colleague Tom Phethean, who went to  the inaugural conference a couple of years ago . It seemed like it would be right up my street - for a while now I’ve been thinking about where my career goes next, as I follow the seemingly inevitable career path of writing less code and having more meetings, and trying to figure out how to focus more on  getting the best out of others . Also, Capgemini were  sponsoring , so I knew there’d be some familiar faces there. . Although I’d arrived early,  the queue was big , and growing, but in spite of that, I got a seat in time for the start of the first talk.  Patrick Kua from Thoughtworks  touched on some valuable points on the difference between developers and leads, not least the idea that leads need to encourage a focus on constant principles and help people to grow into roles that enable leaders to delegate. Patrick’s point about valuing principles over tools reminded me of  Andy Clarke’s talk about the modern designer’s canvas at Smashing Conference 2014  - while technology is always changing, and it can be hard to keep up with the pace, those basic underlying principles remain evergreen, and they’re the things we should invest our time in learning. . It was reassuring to hear someone as confident as Patrick talk about issues that I’ve often faced, such as the difficulty of stepping up into a lead role, or the fact that “there will never be enough time”. In the face of that, the  Eisenhower Matrix of importance and urgency  seems like a useful strategy. . Another interesting point Patrick made was about the importance of being aware of cross-cultural communication factors, such as those described by  Geert Hofstede’s cultural dimensions theory  - a theme which was developed further by Katherine Wu in her thought-provoking session on  Ask vs. Guess Culture Communication . Katherine described a helpful framework for thinking about cultural differences, although it’s a complex subject, and I wonder if a one-dimensional spectrum provides enough nuance. I know that I can often shift between the two styles, depending on the situation, and  #VeryBritishProblems  can arise within a single cultural context - there’s diversity between people from the same country, and adding different cultures into the mix gives us even more to think about. It’s definitely important to be aware of the cultural background that has shaped you and the people you’re talking to - perhaps the lesson is that we often need to be more explicit and intentional in our communication, and make sure that we don’t end up in a situation where different people are guessing differently - as Katherine mentioned, you should never assume that you know what other people are thinking. . Another speaker whose session really got me thinking was  Adrian Howard , who introduced another theme that kept coming up throughout the conference - the importance of listening and being respectful of your colleagues. As Anjuan Simmons put it later in his excellent talk about  Leadership Lessons from the Agile Manifesto , you should “preserve dignity at all costs - especially for people who don’t agree with you”. Another message that came through from a few speakers was that, as Adrian put it, “ the manager’s job is to build the organisation, not the product ”. I couldn’t agree more with Anjuan’s suggestion that “as tech leads, you’re going to build more than just software” - every team has a culture, and it’s important to be conscious in shaping that culture - this is what Andrew Harmel-Law and I were aiming for in our  software engineering team manifesto . . Adrian made another point that really resonated with me: as a leader, “ your job isn’t to have all the answers  - it’s to find out where the problems are and help people solve them themselves”. This is a message I’ve been doing my best to spread for a while, although not quite so succinctly. There was a lot in Adrian’s talk, and it’s instructive that he focussed on the importance of active listening, rather than just waiting for our turn to speak - an idea which came up again later in  Erika Carlson’s talk on giving feedback . . As is so often the case at conferences, I feel that I need to watch the talks again to process them - that’s partly the point of writing this blog post. There’s something about hearing someone speak that brings ideas to life far more effectively than the written word can - as my colleague  Phil Hardwick  discussed in a previous blog post, even the best ideas need some help to get lodged in our brains. Another speaker whose talk was full of good stuff was  Maria Gutierrez , explaining the importance of taking time to plan, and making learning part of your day - I like the idea of talking to your children about what you’ve learned that day, and I’ve started trying that with my daughter. . That was followed by a real change of direction, as Rob Allen gave a measured and thoughtful description of the  features of a good API  - after all the talk of humans and our feelings, it felt slightly odd to be talking about technology, although Rob did include the important message that APIs should provide “code for computers, messages for humans”. . In an entertaining, interesting, and hyper-caffeinated talk, Cate Huston of Automattic made a very strong case for  the value of a regular and predictable release cycle  - something I can vouch for from my own experience. She also pointed out that one sign of a healthy team is that “people will hold each other accountable and hold themselves accountable to the team”, and that “ there is nothing more toxic to a culture of accountability than blame ” - a point that I’d wholeheartedly endorse. . The final speaker for day one was Nickolas Means - a good storyteller, although he almost lost us by geeking out too deep into military aviation history in his tale of  The Original Skunk Works . It was worth staying with him for the payoff, a powerful but fairly simple message: teams succeed when they spend their time on the things that did matter, and sometimes you need hacks to get you where you need to go. I hadn’t come across  Kelly’s Rules  before, but they really rang true with me - to paraphrase a few in particular: . use a small number of good people . you need to trust your team to make good decisions . the team should always have a clear focus on the top priority . there should be no more process than is needed . mutual trust is vital . On that note, the organisers brought out the drinks, although I was keen to get home to ask my daughter about what she’d learned that day. . Day two started with Kevin Goldsmith talking about  the value of learning from failures , drawing on lessons from Clippy and Spotify (where there is even a  “fail wall” ) and drawing an important distinction between two meanings of “fail-safe” - failure avoidance (which is almost impossible to achieve), and minimising the impact of failure. He also echoed Cate Huston’s point about blame, advising us “ don’t punish failure, punish not learning from failure ”, and reminding us of the importance of doing retrospectives for everything, whether they were a failure or a success. . Birgitta Boeckeler was next, underlining once again the importance of creating a common understanding in your team - in this case through creating  just enough of the right kind of documentation . That common understanding can often be difficult to achieve on remote teams, as they incur “a tax on communication”, as Mathias Meyer from Travis CI discussed, but while it may be necessary to make more effort, “less oral communication means more accidental documentation”. He also made some interesting points about the influence of open source on company culture. While Capgemini are a big corporation, our team are rooted in and  committed to open source , and I’m certainly aware of how that has helped to shape us, particularly in areas such as  transparency . . On the subject of open source, Patrik Karisch introduced an  automated accessibility testing tool  - something I’ve been meaning to investigate for a long time, although I think there will always be value in real user testing. . Next we heard from Carly Robinson of Slack on  Mentoring Junior Engineers . I’ve been sold on Slack as a tool for a long time, and the more I hear about their company, the more I like them. Carly spoke emotively of the need to cultivate empathy, respect, and intellectual humility in order to build a positive culture of learning. I hope that we do reasonably well by our higher apprentices and graduates at Capgemini - this is an area we’re certainly keen to encourage. . After another good lunch came the politest and most perfectly timed evacuation of a building I’ve ever witnessed. Thankfully it was a false alarm, and we were able to get back inside in time for coffee before starting the afternoon’s talks with  Randall Koutnik from Netflix , discussing a couple of questions that keep coming up for us at Capgemini -  what senior means  and how to create a meaningful  career path for software engineers . He made an interesting distinction between solution implementers, problem solvers, and problem finders, and echoed Carly’s reminder of the importance of mentoring junior colleagues. . The values of respect, feedback and trust came up again in talks from  Crystal Huff  and  Jill Wetzler , who both made compelling arguments (from different perspectives) that the tech industry isn’t doing enough to promote diversity -  active inclusion is very important to us at Capgemini , although I think there’s still room for improvement. . The election didn’t have as much impact on the conference as I’d expected, although Jenny Duckett of GDS was unable to speak because of  purdah  (which I’m glad to know I wasn’t alone in needing to look up).  Sally Jenkinson  made a very able last-minute stand-in, making some salient points on the difficulty many of us face in coming to terms with our own prejudices and embracing a new identity as we evolve into leadership roles, juggling the tactical with the strategic. . Tom Booth made some interesting points about the importance of  a supportive relationship between teams  and the necessity of embracing change, before Lara Hogan rounded things off with some useful advice on  Leading by Speaking  - reminding us that being in the spotlight isn’t just about standing on a stage, that almost everyone is anxious about public speaking, and that practicing helps, as does  celebrating with a donut . . Overall, it was a great conference - well organised, with helpful friendly staff, decent food, and a very high standard of talks. One of my favourite things was the  live captioning  - it’s great to be able to look at a screen to see what you’ve just missed, and it was a real-world reminder that accessibility isn’t just for people with special needs, but about making things more usable for everyone. As with every good conference I’ve been to, I’ve come back with my mind buzzing with ideas, and an ever-growing reading list. I’d definitely recommend it, and I hope to be there again next year. ", "date": "2017-06-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Change your debugging mindset\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/debugging/", "abstract": " I learnt to code at a bootcamp organised by Capgemini and delivered by QA in the first 13 weeks of my apprenticeship. Undoubtedly the best thing I got out of that was a nugget of wisdom which has stayed with me for the whole of my short career and probably will for the rest my years. . Knowing it has saved hours and hours of my time. I’m much better at debugging than I would be otherwise and can find what’s wrong in a system much more efficiently. That wisdom is, “ Think of the most obvious thing that could be going wrong and it’s probably that ”. . I’ve found this to be true 90% of the time. Yes, sometimes you need to delve into the infrastructure, inspect the packets with tcpdump, watch the TCP connections occur, but most of the time it’s the simplest possible thing which is stopping your system from working. For example: . If your application is not working: is it running? Are you looking in the right place? Is it deployed? . If some functionality is not working: is it configured? . One of my colleagues, James Gee, put it another way which is just as helpful to think about, “ What are you assuming? ”. When you’re investigating why your test or application isn’t functioning as expected, what are you assuming? You may be assuming you know which port it’s running on, you may be assuming the database is set up, you may be assuming the application is connecting to the right database. . Question is, how can you know your assumptions are correct? . In approaching debugging I would advise anyone to make a list of the simplest possible things that could be going wrong and then check each one in order of how likely you think it is. Once you get good at this you can do this in your mind but starting out I would recommend writing it out as it helps your brain develop the process. Even if you think you know the database is setup, check it anyway, it’s an assumption and you don’t know whether (if it’s a shared database between the team) someone has rebuilt/reset it between now and the last time you used it. . This works well alongside the  rubber duck debugging  technique. If you’re asked to be someone’s rubber duck then I think your questions should be around “What are you assuming?”, “What’s the simplest, most likely reason for this not working?”, “How do you know?” and “Show me”. These questions are good for teaching without telling, so that team members work out the knowledge for themselves, therefore making it more memorable. . Answering these questions gets harder with a microservices architecture or even just a service oriented architecture. . “We replaced our monolith with micro services so that every outage could be more like a murder mystery.”  @honest_status  . The questions are the same in the “murder mystery” microservices scenario but the possible culprits are numerous. It require more knowledge of the system to debug efficiently and a keen understanding to prioritise what could be going wrong most frequently. . In conclusion, there’s only one message to this post but it’s saved me countless hours and it’ll save you hours too: “What’s the most obvious thing that could be going wrong? Because it’s probably that”. ", "date": "2017-06-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "DevOps - Why I need to know about it as an Engineer\n", "author": ["Imran Khan"], "link": "https://capgemini.github.io/devops/DevOpsAndMeEngineer/", "abstract": " I recently attended DevOps days at  Les Fontaines  (Capgemini University in Paris). The key reasons were to understand some details of this buzzword and how it may impact me as an engineer and my existing clients. Being an engineer, my expectation was that it would be some sort of tool set or process that will speed up the delivery through automation. . What I learnt was that it is not just about the tools and there is no cookbook yet. DevOps is not simply about buying or using a toolset. Actually a good engineer has to persuade the client not to jump to the toolset upfront for the false impression of buying into DevOps.  It is worth adding that Agile and DevOps are related concepts but not the same. A typical journey towards DevOps assumes an Agile mindset is in place. Equally, DevOps does not mean forgetting about compliance with standards such as  ITIL  etc.  Before sharing my understanding of DevOps definition and how it may impact me, it is worth noting why CIOs/CTOs are asking about DevOps. . Increased expectations of great personalised user experience is one of the key reasons that is driving the need for speed. As a consequence, every company has become a customer company, every company has turned into a software company and every company needs development speed and agility like Amazon or Google. Therefore not only do the engineers have to learn it, service providers (as a whole company) need to understand it and the customers have to make an informed decision to embrace it. . As  Andre Cichowlas, Capgemini’s head of Group Delivery , puts it, “new ways of delivery in the cloud are changing the game and DevOps appears as a best fit enabler to leverage this delivery model”. . Out of many success stories shared at the event, one was of a mission-critical enterprise level system. By adopting a DevOps culture, the delivery time was improved to 27 times faster and the team was reduced from 45 people to 15, most of whom were engineers. . DevOps is defined using multiple aspects, for example: . DevOps is about being agile to be closer to the client and being more responsive to their needs. . DevOps is about changing mindset, finding value,  removing waste , automating wherever possible. . “A cross-disciplinary community of practice dedicated to the study of building, evolving and operating rapidly-changing resilient systems at scale.”  Viktor Farcic – Docker captain &amp; DevOps author  . But the one aspect that stuck in my mind was that DevOps is about transforming organisations by combining the organisational team’s efforts to become more centric to the end customers, for example development and operation teams working together as one team with no barriers. Although people may use a different buzzword, please also allow me to extend collaboration by adding business (both client and service provider) teams working together with development and operations as one team. By one team I mean, absolutely no boundary restrictions between people i.e. no more throwing stuff over the wall. . There is no doubt that engineers are the key people of almost all organisations as “software is eating the world” -  Marc Andreesen  and “engineers will eat your organisation”  Gideon Zondervan - Capgemini . While clients are leveraging or planning to leverage Cloud (Public or Private) to enhance speed, efficiency &amp;  elasticity , engineers are required directly to work with the business to experiment with an app without long analysis, test and deploy it. If successful, then they have to support it while working on some new experiments. . To support such disruption models, engineers need to give up the traditional mindset of receiving elaborated specifications and leaving it to the QA team to identify any functional gaps or leaving the task of deployment and maintenance to the Ops team. Engineers are required to churn unstructured and un-elaborated requirements into scalable Digital Applications. Engineers have to leverage  intelligent automation drive  to translate automation technology into a business value. Equally, engineers have to embrace the mindset of an operations person in order to act like a Business Agility Team member. Without such knowledge, one may remain confined in the borders of the development. In essence, engineers are the analysts, testers and support team members and DevOps and cloud are supporting such model. . A concrete example is of a decent size UK based engineering firm (21,000 people), who handed over the contract of maintaining its intranet to a small company (six people) and of them only two people support the whole of their intranet (hardware, software, website, web apps, load balancing, fail over, high availability, monitoring etc) using AWS. Adding more to the surprise, they are not full time on this client, they are managing many more. . Another example is of a large public sector department who migrated to shared service infrastructure about 3 years ago. It needed two partners to work together with the client for more than four weeks to provision hardware, software and web apps, involving a number of analysts, developers, ops people and project managers in the exercise. When an engineer who worked on it was exposed to the DevOps mindset, with help of AWS, he was able to set up 70% of the whole environment, as it excluded deploying application etc, in about 3 hours. No analyst, no ops, no project manager and time was reduced from about 4 weeks to 3 hours. It is an example of least delays, least handover and fastest validation. . A more recent example is of a public sector client whose supplier is struggling with maintaining and extending an existing complex web application. Structured around more than 12 subunits, they requested service providers for a complete package (business consultants, project teams, operations teams and all environments) that revolves around business function. Such customer-centric proposal would have been very difficult without the whole team adopting to the DevOps mindset and would be even more challenging if engineers resist to land into this arena. . In terms of skill set, engineers have to move away from the I-beam profiles (for example Java developer, front end genius, Oracle expert) as it becomes either a bottleneck or overcapacity on that skill. Typical T-shape profiles (for example, Java specialist with some knowledge of other languages) for task sharing and productivity may soon become history. Engineers may need to start embracing π-shape profiles (TestDevs, DevTest, people with two badges), by removing critical specialist dependencies. Although almost impossible, the next probable step points towards moving to an ideal square profiles (multi-specialist). However it is likely to cost much more development and training time; not to mention that it would be too hard to stay up to date with all skill sets. . In terms of design and development, engineers have to leverage business feature driven development to align it more around the moving parts of the business to support agility. It may require leaning more towards adopting microservices in order to build, scale, and deploy smaller loosely coupled services by leveraging DevOps to deliver business value. For example, a client required the service provider to build a dashboard along with the web app to show how much money business is making or losing for each key component of the application. It would have been very difficult to achieve it without having a microservice architecture. . In terms of support, compliance will become a more important aspect for engineers as they have to understand it in order to comply with it as they are the developer as well as the operation. There is no separate ops team, and as a consequence, it cannot be delegated. From an automation perspective, DevOps would be how to make it more robust, for example by automating compliance aspects. We have to be innovative within the bounds of the compliance of the industry. . For engineers, DevOps starts with the mindset and what a DevOps engineer can bring into the organisation. Tools are only enabler of achieving the DevOps mindset and can vary for different settings like Confluence, Jira, Chef, Puppet, Jenkins, URlease etc. However, like anything else, you should not start your DevOps journey without a plan. One way to start is by thinking of how to achieve the following: . Work as one team with shared goals and responsibilities . Focus on delivering customer value and business goals . Build high trust, collaborative environment . Value communication, knowledge sharing . Drive towards smaller, more frequent releases . Think components, microservices end-to-end . Maximize flow, manage constraints . Enable fast feedback and foster experimentation . Standardize . Make it modular for re-use . Automate everything! . Systematize feedback . For companies, the journey starts with forming customer centric teams. Teams need to go through stages of being comfortable with agile ways of working to continuous integration and delivery to mastering cross concerning skills. It can start from being a project team that turns into a product team or value stream. Companies like Capgemini provide a framework for transforming to the DevOps that includes a Maturity assessment model and a road map. . In summary, DevOps adoption is about people, process and technology. Businesses need to adopt it to avoid being a historical brand while acknowledging that transformation may vary to different speeds depending on team, organisation and application maturity. . The key message for myself as an engineer is that as I have to be more open to the change by accepting it rather than sticking to a developer mindset who cares less about the pre and post implementations stages. Engineers have to be more involved in the end to end lifecycle. Engineers have to act as a member of the Business Agility Team that drive the change, implement it, brings it to the life and supports it. In other words, we are expected to discover and assess innovative technology and processes, support them through proof-of-concept and industrialise and harden them in the operating models. . Within Capgemini, this has already been started by training engineers to develop a π-shaped skill profile along with embracing a DevOps mindset. . As engineers, we have to embrace this new career framework and align personal ambitions accordingly to avoid eating the dust by being the disrupter and not the disrupted ones. ", "date": "2016-12-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Talking About How We Work\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/development/how-we-work-talk/", "abstract": " Following the previous  blog post about our software engineering team culture  that I wrote with my colleague  Andrew Harmel-Law , I spoke about the subject at the  January Drupal Show &amp; Tell  last night. . I’ve been meaning to speak at a meetup for a long time, and if I hadn’t done it last night, I’d probably be putting it as one of my objectives for the year. The trouble was, I could never think of what to say. But conversations turned to tweets turned to blog posts, and it felt right to talk about this subject, particularly given that one of the themes of the blog post is the importance of communication between people. . I’d been to the  Drupal Show &amp; Tell meetup  a couple of times before, and it’s a friendly group with some familiar faces, so when I saw the call for speakers, it seemed the ideal opportunity for my first venture into public speaking. . As I rode my bike through the snow to the meetup, I was a little worried that the attendance might be a little sparse, and my blocked nose wasn’t helping my confidence. After a few anxious moments where we thought there might be more speakers than people in the audience, more people arrived, and we got started, with interesting and thought-provoking talks from Anthony Seale and Nigel Milligan. . Finally, it was my turn, and despite losing my thread once or twice, I think it went fairly well for a first attempt. As I mentioned in the talk, one of the key points is about improving through iteration - I’ll be tweaking the talk and delivering a new version of it at one of our internal lightning talks sessions soon. .  My slides for the talk are available online , and I’ve embedded them below. Thanks to  Cameron &amp; Wilding  for organising the meetup, and to everyone who came along. . Your browser does not support iframes. Please visit  https://malcomio.github.io/presentations/how-we-work/#/  to view the presentation. ", "date": "2017-01-13T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Our Grade Ladder\n", "author": ["Ant Broome"], "link": "https://capgemini.github.io/culture/our-grade-ladder/", "abstract": " Last year, we identified a need to redefine the career framework for our software engineers within the UK engineering teams and started work on a Capgemini Software Engineering grade ladder. The grade ladder is our team’s self-produced documentation to enable everyone, both inside and outside our team, to understand our  ethos and values  and what’s expected of them. . We were inspired by a blog post from  Camille Fournier ’s  renttherunway.com  in which they shared their ladder (and they took inspiration from the original  Foursquare engineering  ladder made by  Harry Heymann ,  Jason Liszka  and  Andrew Hogue ). . We are sharing our latest evolution, licensed under the  creative commons Attribution-NonCommercial-ShareAlike 4.0 International licence (CC BY-NC-SA 4.0) . That means we too are cool with you taking it and making it your own, all we ask is that you don’t make money off it, and that you credit those who came before you, just as we have. . When documenting the grades our objectives were to: . be  actively inclusive  (to ensure that we not only attract and recruit a diverse group of people, but also retain and develop those individuals once they are part of  our team ) . be team-owned (it’s ours!) . be generic (it won’t be tied to technologies, so it is less likely to go out of date this way) . allow it to evolve (we can change it) . be versioned (we know which one we’re working against) . not be written specific to Capgemini (because many of the consumers will not know our internal jargon / processes) . be HR-friendly (so it aids everyone’s career progression, rather then confusing it) . be open and public (so everyone can see it and everyone can contribute to it) . We have found the ladder has aided us in several ways. When hiring it’s helped recruiters to understand what we expect from candidates. During interviews it assists us to see where someone would fit. When people join us the ladder provides a terms of reference for the expectations we have of you. Continuing through your career the ladder then assists forming the basis to understand how you can grow and demonstrate abilities to justify progression to more senior positions within the team. . We’ve published it on our  GitHub  and as a  GitBook  (read more on this below) where it’s available to read online or as a  PDF ,  Mobi  or  EPub . If this sounds like the kind of place you’d like to work then you can check out our latest opportunities  here . . Being engineers we like to explore new tools when a good opportunity arises. The decision to publish the grade ladder gave us such an opportunity to use  GitBook.com  and  Gradle composite builds . First up we created a simple Java app that takes our grade definitions as a CSV file and generates some markdown pages into a layout suitable for GitBooks. Using  Gradle  we’ve scripted our Java build and page generation tasks. Using the  Gradle node plugin  we then added tasks to test creating the PDF, EPub and Mobi outputs locally. When we’re happy with our changes we commit the generated pages to GitHub which then fires a webhook to GitBook which then acts as our build tool regenerating our output formats. ", "date": "2017-01-20T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "I Test, Therefore I Am\n", "author": ["Az Madujibeya"], "link": "https://capgemini.github.io/development/I-Test-Robot/", "abstract": " The three laws of TDD, as espoused by  Robert C. (Uncle Bob) Martin  state: . You are not allowed to write any production code unless it is to make a failing unit test pass. . You are not allowed to write any more of a unit test than is sufficient to fail; and compilation failures are failures. . You are not allowed to write any more production code than is sufficient to pass the one failing unit test. . These laws form the basis for  Test Driven Development , a design approach to writing clean code. . This topic is not new (Uncle Bob has been teaching it for a number of years) but it’s helpful to keep raising it.  It should always be present in the mind of all developers when they embark on any new project so they can ask the obvious questions: Can I use it effectively on this project? Will it be useful? Will it help improve productivity? Does the system being designed lend itself to TDD? . There are a myriad questions already posed about the usefulness of TDD to a project and they are quite pertinent and should be taken seriously so that you don’t waste time writing tests for something that needs no tests, or cannot be tested in a test first approach. . Using TDD should lead to a well developed system with unit tests which are very specific to the particular piece of functionality, and the corresponding production code being more generic. There should be little or no coupling between production code and unit tests so future modifications and updates to the code can be developed quickly and effectively, especially if this design approach is followed where possible. This helps in the overall design of the system and can lead to exposing flaws early that otherwise might not have appeared till late in the development cycle.  These are two benefits of a test first approach: maintainability and good design, which gives confidence that the system being developed functions as per its stated test objectives. . Often, developers jump right in and start writing code first leaving any unit tests till the end, but this can be counterproductive and could waste more time than is often thought is being saved by not following a test first approach, or any approach at all for that matter. . Some experienced developers choose not to use TDD and instead and prefer to approach the design of the system using  Domain Driven Design .  DDD is the process of being informed about the Domain before each cycle of writing any code!  Behaviour Driven Development  (BDD)  is an implementation of TDD and uses aspects of DDD.  It also lends itself to testing certain conditions for a user story or feature. In either case, using any approach, whether structured like TDD, or nothing at all is fine and having the flexibility to choose is a good thing, so long as the reason for for doing so is clear and the objective is kept foremost in one’s mind. . Knowing when to use TDD and doing it well can be a stumbling block to those not acquainted with the process initially as it takes some getting used to following the ‘red, green, refactor’ steps that protagonists of TDD espouse.  However, after the initial hurdle and some time following these small steps, because that is the only way to approach this, the benefits are apparent to anyone that does it for the first time. . Following the test first approach of TDD sounds easy but it isn’t.  Writing tests depends on how clearly defined any acceptance criteria is for a particular story, and if you do have this then definitely make use of it to define any test scenarios that need to be developed. Also, if you’re just beginning to learn TDD then pair programming is the best way to start, especially if you can do so with someone that has experience.  You can decide who writes the first test and accompanying code, then switch when you’re both happy with the result. . In my experience, pairing with a colleague who also knew the process was the best way of improving. We were able to take it in turns to define a test, run it to see failure, then implement code to make that test to pass.  We went through this process iteratively until we were complete, swapping between writing the test and the implementation so that when finished we knew that our implementation was what we wanted and that it was backed up by our successful test. We continued this process until the whole user story was completed and it was no surprise that running a build at the end of the process was successful, as our process had given us confidence along the way. . Failure to follow the three rules of TDD can lead to breaking existing code, as I found out to my cost on a recent user story.  I can hear Uncle Bob saying “suffer the consequences all who fail to follow the  red, green, refactor process ” and laughing maniacally.  However, as with any methodology you get out what you put in and even if the community shifts towards something newer or more shiny, that won’t mean that time has been wasted doing what’s currently the flavour of now!  I believe quite the opposite because when new things do crop up (and they always do), being able to adhere to the ‘methodology’ will be in no small part due to having followed a prior one. . Flexibility is always a good thing to employ when embarking on any new framework or methodology so bear that in mind when, like me, you’re an ardent supporter of TDD, and try to recognise the good and bad in any framework and adapt it to your own specific needs. . There’s much to take on board with TDD, but with anything new just take small steps and try to follow the process strictly.  Only once you’re confident and comfortable should you allow yourself the luxury of flexibility. With your knowledge it doesn’t matter that you hadn’t refactored a common field into a class variable before or after a certain point because you can defer certain things till later.  It also doesn’t matter that you start with your  setup()  method because more than likely you’ll need one at some point.  Just allow time to do the refactoring at some point, and try not to break what was already working and you’ll be just fine. .  Uncle Bob’s Three Rules  .  When TDD Does Not Work  .  Is Unit Testing or TDD Worthwhile  .  Test Driven Development  .  Don’t Complicate Things  ", "date": "2017-01-31T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "The Best of Capgemini Engineering Blog 2016\n", "author": ["Bobby Moss"], "link": "https://capgemini.github.io/blog/best-of-2016/", "abstract": " Hello! Are you new here? If so, welcome to the Capgemini Engineering blog. It’s written by and for our fellow engineers across the technology industry. . This site has gone from strength to strength since it was created in 2014, and it provides an impressive showcase for the talent, passion and expertise shared among our teams every day. The number of contributors and the range of subjects continues to grow, and this will enhance the value we’re giving back to the wider tech community in 2017. . Speaking of “our contributions”, let’s take a look at our readers’ favourite articles of the past year… .  Craig Williams  weighs up the pros and cons of different approaches to using microservices in the implementation of RESTful interfaces. This was by far and away our most popular article of 2016, having been read by 27,217 unique page views (the full hit count was 29,506). Microservices and containers were named as one of Gartner’s  top 10 infrastructure-impacting trends of the year , so it’s easy to see why Craig’s insightful post drew so much industry attention. . Deji Akali shares the fantastic  DrupalCamp  slides he presented across the United Kingdom in 2015. This great post can teach you how to take your own Drupal projects to the next level with custom fields in just 13 minutes! Deji starts from the bare basics and uses examples to help you put your new-found knowledge into practice. .  Phil Hardwick  explains how Apache’s  CXF  and  Camel  frameworks are a match made in heaven when it comes to creating scalable and testable Java web services. In little under five minutes you can master the basics, and if Phil’s post has whetted your appetite you can take the next step with  this reference example  and the helpful resources he’s linked to. .  Andrew Harmel-Law  shares his thoughts on the new-fangled (as of 2014) microservices trend emerging from in-vogue companies  like Netflix  and the wider Java community. As the ideology and technology behind it matures and is increasingly adopted across the industry, Andrew’s post continues to offer an honest and thought-provoking insight into what we gain and lose when we “containerise” new applications and web services. . Cam Parry describes his perfect development workflow for creating, testing and securing applications developed and deployed in  Docker  containers. In this well-researched piece Cam provides a  reference example  and also reviews a wide array of useful testing and scanning tools such as  Vegeta ,  Protractor ,  Clair  &amp;  Lynis . .  Graham Taylor  guides you through how to create a private blockchain on your own desktop for testing purposes. While  testrpc  might be your initial “go-to” solution, he explains how to leverage  Docker Compose  to craft a working setup that’s much closer to what you’d expect to deal with in production environments. . If you like this article you should also take a look at  Kubeform: Kubernetes Clusters in any Cloud , where Graham explains how to use a straightforward visual interface to create a cluster of app containers on almost any virtualization platform. .  Sanjay Kumar  takes you on a deep dive into the weird and wonderful world of integration testing with  Apache Camel ,  Hystrix  and  MongoDB . He starts by explaining how this differs from testing you might already be familiar with and walks you through how to put this knowledge into practice in your real world development setups. Whether you’re a rookie developer or a seasoned professional, you’re guaranteed to benefit from Sanjay’s expertise. .  Malcolm Young  describes the anatomy of a good code review and the practicalities around implementing current industry best practice. In particular he details the key things to look out for to ensure your code is secure, scalable, accessible and maintainable over the long term. . John Shaw explains how Little’s Law can be applied to software projects that use Kanban boards to better organise customer demand and workloads. In less than 10 minutes you can discover how to reduce queue times, resolve bottlenecks and boost throughput from your agile development sprints. .  Nick Walter  lends you his experience about some of the difficulties he and the teams he’s worked with have encountered while implementing microservice-driven applications in production environments. This post will help you get a handle on the most common pitfalls so you can mitigate them in your own projects. . This site drew over 106,000 hits in 2016, an increase of 35% on the previous year. All 99 blog articles collectively drew at least 78,930 unique page views (with a full hit count of 87,731) between them this year, and 49 of those were created in 2016. According to the most popular tags the team focussed primarily on  development  (21 posts),  agile  (8),  open source  (8) and  DevOps  (5). . Our posts average approximately 1,210 words of wisdom, with an estimated total across all articles of 108,885. The most in-depth post is  Sanjay Kumar ’s deep dive into  How to do NFR Testing (Non-Functional Testing)  with an impressive 3,667 words. . The most prolific contributor of the year was  Malcolm Young  with 8 new posts. He and the following 7 established regulars produced 25 new posts between them in 2016:  Tom Phethean ,  Andrew Harmel-Law ,  Alberto Garcia Lamela ,  Sarah Saunders ,  Nick Walter ,  Graham Taylor ,  Ant Broome . . We’d also like to thank the following 12 new authors who contributed posts for the first time in 2016. They collectively shared their expertise across 18 new articles:  Shana Dacres ,  Phil Hardwick ,  Mat Perry ,  Satvinder Hullait ,  Gayathri Thiyagarajan ,  Amir Aryanpour ,  Matt Davidson ,  Sanjay Kumar ,  Abhilash Nair ,  Richard Sheppard ,  Greg Wolverson  &amp;  Imran Khan . . After such a successful year we’re looking forward to expanding the number of writers and posts over the course of 2017. We’re also planning to include contributions from other teams across the Capgemini group, as we seek to share our collective experience across an even wider range of subjects. If you have any post suggestions or feedback, leave them in the comments below! ", "date": "2017-02-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Security of the Future\n", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/cybersecurity/security-of-the-future/", "abstract": " Cybersecurity was a huge focal point of 2016. With a new hacking scandal being highlighted in the news almost on a weekly basis, cybersecurity has become a  major issue for all digital companies . Looking at some of the more severe breaches of 2016, it seems no-one is safe, and that cybersecurity is an area which is only going to grow and become more significant in the years ahead. . It’s clear that firms are now stepping up their game in response to the numerous cybersecurity breaches and attacks in the last few years, and it’s not just the private sector which is picking up speed on this.  GCHQ has recently partnered with Wayra , among others, with the aim of finding new and innovative companies, who use novel techniques to solve real world problems, with the focus being on applying these solutions in a cybersecurity context. Of these start-ups involved in this ‘incubator’ style environment, I’ve picked out a few which are worth watching in the cybersecurity space in the year ahead. .  CounterCraft  .  StatusToday  .  Spherical Defense  . CounterCraft specialise in counter intelligence campaigns. They utilise deception techniques in order to detect, study and manipulate adversaries, with the idea of turning a passive security system into an active defence. They offer tools and models to clients who wish to build, deploy and maintain these ‘deception’ campaigns. Alongside this they actually interact with ‘adversaries’ in order to further aid the campaign and gather real world data. . In the real world, deception is often used to prevent attacks, or at least reduce the damage a hacker can do,  honeypots are prime examples of this . However, honeypots only keep the attacker busy for so long, whilst the owner(s) of the system under attack think up another way to secure their assets. The idea behind CounterCraft is not only to passively protect a system, but to actively engage in its defence with pre-programmed responses, transforming the system from hunted, to hunter. . Furthermore, deception campaigns could be very useful to the banking industry. If banks could deploy effective campaigns to divert hackers away from their critical systems, whilst also learning more about those attackers and responding with active defence measures, they would have much better prospects against potential breaches. . The concern I’d have with the idea of these deception campaigns, and the ability to enable an active defence through pre-programmed responses, is how would it cover the wide range of potential threats? Cyber attacks can come from both internal and external sources. For example, how would you deploy a deception campaign internally to give protection against social engineering? Furthermore, it would be interesting to see how many areas CounterCraft could apply their security models too. . StatusToday provide real-time analytical data on people. They utilise human knowledge, alongside a sophisticated AI learning platform to do so. Furthermore, by collecting large quantities of data, they are able to create more efficient and robust algorithms, which in turn create  more accurate data reports . They aim to provide more in depth, accurate information about people, and with  social engineering becoming a major threat  in the cybersecurity area, being able to accurately report on human behaviour would be a big step in preventing internal attacks. In addition, the fact that it  works in real-time , ensures it maximises the chances of finding threats in good time, and minimises any potential impact of a breach. . Moreover, StatusToday provides a very real prospect of preventing and dissolving potential social engineering attacks. With more and more breaches coming from insider attacks, having the ability to monitor and react to human behaviour in real time could be a big bonus to companies at risk of internal threats. . On the other hand, one issue that springs to mind with this sort of technology is data privacy. In a digital age where more and more data is being put out on the web, a lot of people are still concerned with data privacy, as there are  still numerous issues surrounding it . Whilst behaviour monitoring software could be an effective counter-measure to social engineering style threats, data privacy could become a problem, as it poses questions about the kind of data being held and analysed. . Spherical Defense is really interesting; their product is a web application firewall for structured data. However, there is more to it than that; they utilise deep learning alongside artificial intelligence to prevent attacks. Amongst their focus areas are; intrusion detection systems, neural networks, anomaly detection and API security. One of their target areas is the banking industry, which based on some of the large scale breaches last year, clearly have weaknesses in their security. Through the utilisation of deep learning, Spherical Defence are developing state-of-the-art intrusion detection systems for the banking APIs, which could be a huge boost in preventing large scale attacks in this area.  However, they’re not only focused on the banking sector, they’ve also identified areas for their product within defence, healthcare and the ever growing world of  IoT . . Again this is a product which could mean big things for the banking and finance industry. With some of the largest breaches in the last few years targeting banks, having the ability to deploy state-of-the-art defence mechanisms would strengthen these systems and give them a fighting chance against potential threats. . It’s clear that cybersecurity will be a major focus of 2017, with more and more organisations choosing to invest heavily in defence of their assets. Capgemini is also investing in this area, from combining automated collection with in-depth human analysis to identify advanced persistent threats to supporting cybersecurity (and other!) start-ups through its global  InnovatorsRace50  competition that offers $50k of equity free funding to a winning idea. . Hackers are becoming smarter, and security breaches more difficult to detect and defend against. It will take greater intelligence and perseverance to combat the cyber threat, and these new, innovative companies - whilst not the finished articles - are providing a platform to do just that. ", "date": "2017-01-27T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "DrupalCamp London experience\n", "author": ["Luis Rodriguez"], "link": "https://capgemini.github.io/drupal/drupalcamp-london-experience/", "abstract": " This weekend’s  DrupalCamp London  wasn’t my first Drupal event at all, I’ve been to 3 DrupalCon Europe, 4 DrupalCamp Dublin, and a few other DrupalCamps in Ireland and lots of meetups, but in this case I experienced a lot of ‘first times’ that I want to share. . This was the first time I’d attended a Drupal event representing a sponsor organisation, and as a result the way I experienced it was completely different. . Firstly, you focus more on your company’s goals, rather than your personal aims. In this case I was helping Capgemini UK to engage and recruit people for our  open positions . This allowed me to socialise more and try to connect with people. We also had T-shirts so it was easier to attract people if you have something free for them. I was also able to have conversations with other sponsors to see why did they sponsor the event, some were also recruiting, but most of them were selling their solutions to prospective clients, Drupal developers and agencies. . The best of this experience was the people I found in other companies and the attendees approaching us for a T-shirt or a job opportunity. . As a new joiner in the Capgemini UK Drupal team I attended this event when I wasn’t even a month old in the company, and I am glad I could attend this event at such short notice in my new position, I think this tells a lot about the focus on training and  career development  Capgemini has and how much they care about Drupal. . As a new employee of the company this event allowed me to meet more colleagues from different departments or teams and meet them in a non-working environment. Again the best of this experience was the people I met and the relations I made. . I joined Capgemini from Ireland, so I was also new to the London Drupal community, and the DrupalCamp gave me the opportunity to connect and create relationships with other members of the Drupal community. Of course they were busy organising this great event, but I was able to contact some of the members, and I have to say they were very friendly when I approached any of the crew or other local members attending the event. I am very happy to have met some friendly people and I am committed to help and volunteer my time in future events, so this was a very good starting point. And again the best were the people I met. . As I had other duties I couldn’t attend all sessions. But I was able to attend some sessions and the Keynotes, with special mention to the Saturday keynote from  Matt Glaman , it was very motivational and made me think anyone could evolve as a developer if they try and search the resources to get the knowledge. And the closing keynote from  Danese Cooper  was very inspirational as well about what Open Source is and what should be, and that we, the developers, have the power to make it happen. And we could also enjoy Malcom Young’s presentation about  Code Reviews . . Closing this article I would like to come back to the best part of the DrupalCamp for me this year, which was the people. They are always the best part of the social events. I was able to catch up with old friends from Ireland, engage with people considering a position at Capgemini and introduce myself to the London Drupal community, so overall I am very happy with this DrupalCamp London and I will be happy to return next year. In the meantime I will be attending some Drupal meetups and trying to get involve in the community, so don’t hesitate to contact me if you have any question or you need my help. ", "date": "2017-03-07T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "A blurry look into the future\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/innovation/blurry-look-future/", "abstract": " Along with some of my colleagues in the Digital Platforms team, I recently took a tour of the London lab of Capgemini’s  Applied Innovation Exchange . The AIE is a series of facilities around the world giving Capgemini and our clients a space to explore emerging technologies, with the aim of being able to go from initial experiments to large scale projects. This goes hand in hand with the workshops that take place in our  Accelerated Solutions Environment (ASE)  - we do love our three-letter acronyms. . The London lab opened in the summer of 2016, and specialises in artificial intelligence and applications relating to the financial sector.  Other labs focus on areas relating to the needs of their local market - for instance in Munich they’re largely working with the automotive industry. . The London AIE is an impressive place - it’s bright and open, with a feeling of space helped by the views over the rooftops from the 8th floor of our Holborn office. One wall is taken up with an enormous touchscreen display, and around the room are some of the experiments that the team (led by  Graham Taylor ) have been working on. . On the surface it might look like an opportunity for Graham and his team to play with shiny new toys, and I think more than a few of my colleagues are a little jealous. There can be a tendency for some engineers to be seduced by the latest technology trends, but there’s a real focus at the AIE on solving client problems, using their “Discover, Devise, Deploy, Sustain” framework. . I’m not generally an early adopter by nature, preferring to wait until the bugs have been ironed out. I don’t always want to jump on the hot new JavaScript framework, preferring to follow the manifesto of  the boring developer . I tend to be suspicious of a lot of claims made by salespeople on behalf of the latest new technologies - all too often it’s little more than buzzwords and vapourware, or  a solution looking for a problem . . I’ve been somewhat sceptical about  blockchain , and I haven’t really bought into the  internet of things . As my colleague Niall Merrigan makes clear  in this excellent talk , there are a lot of things that are connected to the internet that really shouldn’t be. I don’t want or need my kettle to be smart, and I’d often agree with the people who say that  “the S in IOT stands for security” . . Having said that, visiting the AIE was fascinating, and I feel like I’m starting to be won over. The important thing for me is that the technology needs to have practical applications in the real world, and that’s exactly what the AIE teams aim to do. Graham showcased some of the projects that the team have been working on, and when you see the demos and proof of concepts that they’ve built you really start to see the potential applications. . For instance, in one project, they’re working on image recognition to improve retail efficiency. Another prototype uses sensors on cars to detect crashes and communicate with insurance companies and mechanics using blockchain. In one corner of the AIE, there’s an Amazon Echo being used as the input device for some machine learning experiments. In another area, some plumbing has been set up, with sensors on pipes measuring water flow, ready to shut off the main valve if a leak is detected. The team are already working with clients from a range of sectors and prototyping real solutions to real problems - the next step is taking them to a bigger scale. . We also tried a virtual reality demo. As I strapped on the HTC Vive, I discovered that you can’t wear it on top of your glasses, so I was exploring a slightly blurry virtual environment. The accessibility considerations of VR are something I hadn’t given much thought to, least of all the idea that they might apply to me now. . The other aspect of VR that became clear to me from this experience was that it isn’t just about the person wearing the headset, as discussed on  a recent episode of the Shop Talk Show podcast . It’s easy to imagine VR as a way for people to isolate themselves from the real world, but we shouldn’t forget the social side of things. While the person inside the virtual world has one experience, the people in the same room can enjoy watching what they do - even if in my case it was mainly about my colleagues laughing at my clumsiness with the controllers. . Sometimes, a negative perception of big companies like Capgemini is that they can be lumbering behemoths, never working on bleeding edge technology because it takes so long to change direction. It’s true that if you’re not careful, delivery teams can end up being  too busy to innovate , but there is innovation going on at a whole range of projects. The AIE feels like the ideal opportunity to address that perception. . I think that very often the difficult part of being innovative isn’t the technology side - it’s having the ideas. As Graham mentioned to us, one of the aims of the AIE is to help make the mental connections between the business needs of our clients and possible technology solutions. Workshops with one client might spark ideas about how to help another client, and there can be a virtuous circle of creativity, with one demo acting as a catalyst to help find a solution to a different problem. . From a brief tour, my mind is already buzzing with ideas of how our projects might be able to leverage some of the lessons that we’re learning in the AIE. As well as the core team, colleagues from around the business are spending time working in the lab, so hopefully I’ll get the opportunity to spend more time at the AIE in the future. ", "date": "2017-03-21T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "How We Work\n", "author": ["Andrew Harmel-Law", "Malcolm Young"], "link": "https://capgemini.github.io/development/how-we-work/", "abstract": " Recently within the various Capgemini UK software engineering teams we’ve been looking at our approach to learning and development. Capgemini is a big corporation, and in the past there has been a focus on certification. Like many people in the industry, we’re currently not entirely convinced by that direction, and yet we very much want to help our colleagues grow as engineers - key to that is to develop an engineering mindset. . This consideration has made us take seriously the question “what does it mean to be a software engineer”?  Answers to such questions are important. So what are the values that we share, despite being a diverse group of engineers, and what do these mean in practice? . As Software Engineers we build things using technology. We build complex things, but we aim for simplicity. . We focus on the details, and on the bigger picture. . We know that software is almost never built in isolation, and we value the benefits that wide collaboration brings. . We continually adapt to the evolving and emerging requirements of our clients. . We care about quality, whether that means the security of the applications we build, the performance and user experience, or the things that the end user doesn’t see, like coding standards and maintainability. . We believe that automation is essential to making our work reliable, efficient and repeatable, so we keep it at the heart of everything we do. . We believe that the best way of learning is by doing, and by sharing what we’ve learned. . We value our colleagues as human beings and as engineers, in that order. . It’s all very well talking about grandiose concepts, but how does this translate to our everyday work? What are the ideas we put into practice to become more effective in our day-to-day work? . The fundamental element which runs through the team’s culture is one which was made famous by Netflix, that of “Freedom with Responsibility” (you can read their  entire “Culture” deck on Slideshare ). As colleagues we all work to ensure we have the right amount of autonomy, are enabled to work as effectively as possible, delivering valuable outcomes to our clients while staying within the boundaries of the contract, and learning and developing as we go. We like to think that this fits with  Capgemini’s corporate values , while making more sense to people like us. . This manifests itself in the following directives: . This area is key, but may also sometimes be the hardest. We are experts, but  we don’t know everything, and we’re not all the same . One of the most important elements of having responsibility is to know when to raise your hand and point out problems or things you don’t like or are not sure of. As an engineer you are expected to look out for these problems, process-glitches and knowledge gaps. Remember, by being both humble and courageous you are far more likely to learn quickly. Allow yourself to be mentored. Don’t be afraid to ask questions. . There is a flipside to this responsibility - when others who have the guts to admit they are uncomfortable, don’t make them feel bad about their admission. If we punish openness and honesty then we will all suffer in the long run. . Finally, bear in mind that this is not a license to simply be a  Cassandra figure . Try and get to the root of problems – find the owner, or the stakeholders. Additionally question if / when something needs to be fixed. Record it, so when spare time does come up it can be re-investigated and re-evaluated. . On many occasions you will be presented with a situation where the course of action is unclear - and as you progress this will become more frequent. As a member of the team you are expected to be able to  cope with this uncertainty , and still make wise decisions. Sometimes there’s too much ambiguity –  don’t ignore it, and don’t see it as a failure . . You are expected to deal with these situations by identifying root causes rather than treating symptoms, and smartly separating what needs to be done now from what can be done later. . Don’t just give the client what they want. They’re paying us for our expertise, and sometimes our job involves telling them that they’re wrong, or helping them to realise that there’s a better way of doing things. Trust in your own abilities. Ask lots of questions, and try to understand the reasons why decisions have been made. Learn when to disagree with those decisions, and when to go along with them even if you disagree. . Think about  the impact of your work , and don’t assume that it’s someone else’s job to care. The team shares responsibility for its work, and success or failure isn’t just measured by how many deadlines you meet. . As an engineer in the team you should always know what it is you are working on now, and what you are going to work on next. You should know what the pertinent requirements are (both  functional  and  non-functional ) and who owns them. You should never  gold-plate  or over-deliver, but neither should you turn a blind eye to gaps or potentially unmitigated risks. When such elements are identified they should be recorded and communicated with the requirement owner. If this individual is not identifiable, then you should make the project manager aware as soon as possible. . Generally the scope of work will be recorded as stories in JIRA. Make sure you know where your scope is recorded, and keep up to date with it as it changes. Ideally stories will have their interdependencies flagged as “links”. If not, make sure you are aware at least at a basic level how things are inter-related. This will help you know when you are straying beyond the scope of an individual story in your work. . As an engineer you will be involved in estimation at the very least during the  sprint planning ceremonies . These meetings happen at the start of every sprint, and you are expected to participate fully – disagreeing when you do not feel comfortable, asking questions to find out why others think differently from yourself, and always being ready to both fight your corner and change your mind. By taking part in estimation of a story or task you are committing to it. . Always bear in mind that  estimation is guessing , and as you learn the guessing gets better. Also bear in mind that estimates are based on  your  knowledge – both of the technical platform and the business domain. As you learn your estimates will change. They may go up or down. Don’t be afraid to communicate either change. . We have an extensive tool chain available to us, both installed on our personal machines and running on servers. Make sure you know what each of these tools are, what role(s) they play in the delivery of our software, and how they can help make your life easier. . When possible, when you have performed a task twice manually,  consider automating it . . Although it will be different from one project to another, there will be a standard workflow on every project. It’s there for a reason. Make sure you know what it is, and how to follow it. Then follow it. Always. It will mean people are less surprised when you try and communicate with them. If you see scope to improve the workflow by making a change, suggest it. . Likewise every team you work in will have a “definition of done” as well as coding standards for all our languages, and static analysis checks built into the standard build script templates. Ensure you know what each of these entail. Bear in mind that individual projects may change / evolve this standard. Make sure you apply the correct definition to the work you undertake. . We, the team, are constantly creating patterns to prevent wheel-reinvention. Before you build something new, check the relevant wikis; someone may have been there before you. . If they’ve not, or the pattern you find doesn’t quite fit, add more – either as an update, or a new pattern flavour, or a completely new pattern. Make sure that before you have a new pattern you’ve tried it out in more than one place. Otherwise it’s merely implementation. . As mentioned above, our backlogs are typically kept in JIRA, captured as Stories, Epics and Technical Tasks and owned and maintained by our Business Analyst colleagues and our clients. This is where the Sprints are planned, reports are generated and it is the home of the standard workflow. Consequently you have no excuse to not plan your work and track it. This shared record should never be more than 24 hours out of sync with reality. Be disciplined; aim for far less. . Everything you work on will be mapped out and have a planned delivery date against it. Keep this in mind as you work. Know how many Sprints there are left to go until the next release, and what work is planned for it. This information will allow you to know when it is time to flag missing requirements and scope creep, or to keep your focus. . Remember, the most important thing is the release of running tested code which has demonstrable business value. Anything else is just a side effect or a distraction. . The more we know, the easier all our lives become. We are all bright people in the team, with a lot to contribute. Consequently, you should try to share your knowledge as much as you can. The ideal places for this are on the wiki, in code, or on the engineering blog. Don’t be afraid of updating the wiki, or adding new sections or pages. Don’t be afraid to comment, and learn to use the power-user elements such as “@-naming”, etc. . However, make sure it doesn’t become a mess. Badly structured information, or duplicated information, or out-of-date information is probably worse than no information. Don’t be afraid to update, delete, or re-structure.  Don’t ask for permission  unless it seems like it will be needed. . We’re working in a creative-inventive market, not a safety-critical one. We’ll make mistakes. Don’t waste time trying to get it right first time – but make sure you make it better the second time, and iterate rapidly to get to that second pass as quickly as possible. Furthermore, don’t take this as a license to be sloppy. Look out for mistakes (both yours and those of others) and fix them quickly when you find them. Never make a code change without a JIRA ticket. . There is always scope to be better. Whenever you do something take time to look back and see what you would change next time. Ask others for feedback (and look all around you, not just your lead / project manager / reviewer). Perhaps the “next time” is right now.  Make the changes small and incremental. Be prepared to be wrong - some “improvements” will actually make things worse. . Experiment. . Failure is inevitable so be aware that you will at some stage fail. Don’t be afraid of this; and instead face it and learn all the lessons you can from it. . However, don’t be reckless.  Never be willfully destructive. . It is the aim of the team to have as little additional bureaucratic overhead as possible. Consequently there is little / no hierarchy. “What do you think?” will be a reply you will hear a lot when you ask questions. . This means that we are all responsible for looking after each other. This works well because in all areas there will be always be one or more of us who know more than everyone else (be it a business or technical area) regardless of seniority / time in grade / time on project / time in the company. In the circumstances where you do know more, try and share that knowledge / experience to develop others. . Beyond this, look for opportunities to lead – it may be on a specific project, a piece of common functionality, an area of the tool-chain requiring improvement, or in the adoption of a new technology or pattern. Or it may be something else. If you see something is lacking, add it. . We know that there are developers beyond our team, country and company who could benefit from what we have learned, captured and produced. The team encourages you capture information in a way that is easily reusable by all of us. . If you have time, take it to the next level (but remember, this takes time, and this may be personal time) and produce a blog post or slide deck and present it to an internal or external user group / community of interest / conference. . The best form of sharing is via executable code. All of this must be invented. As a member of the team, when you spot something which could be abstracted out to an industrialized, reusable component (either in the business code, framework code, or even in the toolchain that we use to build these components) log it. When you get a chance discuss it with another member of the team. . When you think you have something worth industrialising you can take the code and internally “open source” it via a private repository on github. Other team members will help you get this up and running and manage the community / contributions that hopefully result. When we get really happy with it we’ll help you really  open source  it. . One thing you must bear in mind when doing either of these – don’t break client confidentiality, or our contract with them. If in doubt, ask. . The team isn’t for life. You will be given the opportunity to develop yourself. It is our responsibility to do this by giving you great team members and challenging pieces of work. . But beyond this we’re not going to formalise your development - that’s up to you. We expect you to manage your own career path, learning from experience, observation, introspection, reading and discussion. But if you want to get help from the team in this, ask. . If this sounded like kind of place you’d like to work, we’re glad.  There’s a link at the bottom of this page to go to our job board. Alternatively you can contact either of us on Twitter.  We’d love to hear from you. ", "date": "2016-09-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Digital Developer Meetup, Birmingham, 22nd September, 2016\n", "author": ["Richard Sheppard"], "link": "https://capgemini.github.io/development/Digital-Developer-Meetup/", "abstract": " Capgemini UK is hosting an event for digital technologists and experts on the evening of Thursday 22nd September at  the Old Library, Custard Factory  in Birmingham’s creative quarter! Whether you’re a platform engineer, project manager, scrum master, technologist or tester, whatever your interest and experience, and wherever you’re heading in your career, Capgemini would be keen to see you at the event and hear your views! . There will be plenty of pizza and beer available for everyone who can join us! Featured talks and demonstrations with some interactive Q&amp;A include: . Richard’s talk will be about the use of  React  to deliver a range of mobile, offline-first apps to a major UK car manufacturer. The talk covers how Capgemini are transforming supplier relationships, why we are employing React over alternatives, and how we create a reusable app architecture. . Will be demonstrating  Drupal  and  Apache Solr  search which is implemented globally (~52 sites) on recruitment company Michael Page websites. A range of topics to be covered include: .  Vagrant  for local deployment of a development environment .  Phing  for automated site builds . Drupal multi-site installation . Multi-lingual Drupal sites . Why Solr? . Solr query altering within Drupal . And the life of a software engineer at Capgemini. . Indy from Capgemini’s Public Sector Digital business will be demonstrating the use of schedulers and service discovery to manage Docker containers. . Doug and Dave will do a joint demo on Auto-scaling windows Web Farm on AWS managed via Puppet. . Register via our  Eventbrite page . ", "date": "2016-09-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Considerations for a Drupal 8 upgrade\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/considering-drupal-8-upgrade/", "abstract": " If you’re migrating from a different CMS platform, the  advantages of Drupal 8  seem fairly clear. But what if you’re already on Drupal? There has been a lot of discussion in the Drupal community lately about upgrading to Drupal 8. When is the right time? Now that the contributed module landscape is  looking pretty healthy , there aren’t many cases where I’d recommend going with Drupal 7 for a new project. However, as I’ve previously discussed on this blog,  greenfield projects are fairly rare . . One of the strengths of an open source project like Drupal is the level of support from the community. Other people are testing your software, and helping to fix bugs that you might not have noticed. Drupal 7 will continue to be supported until Drupal 9 is released, which should be a while away yet. However, if your site is on Drupal 6, there are security implications of remaining on an  unsupported version , and it would be wise to make plans to upgrade sooner rather than later, even with the option of  long term support . While the level of support from the community will no longer be the same, sites built on older versions of Drupal won’t  suddenly stop working , and there are  still some Drupal 5 sites out there in the wild . . Most big systems could do with some refactoring. There’s always some code that people aren’t proud of, some decisions that were made under the pressure of a tight deadline, or just more modern ways of doing things. . An upgrade is a great opportunity to start with a blank piece of paper. Architectural decisions can be revisited, and Drupal 8’s improved APIs are ideal if you’re hoping to take a more  microservices-oriented approach , rather than ending up with another MySQL monolith. . Drupal’s  policy of backward incompatibility  means that while you’re upgrading the CMS, you have the chance to refactor and improve the existing custom codebase (but don’t be suckered in by  the tempting fallacy that you’ll be able to do a perfect refactoring ). . Don’t underestimate how big a job upgrading will be. At the very least, every custom module in the codebase will need to be  rewritten for Drupal 8 , and custom themes will need to be rebuilt using the  Twig templating system . In a few cases, this will be a relatively trivial job, but the changes in Drupal 8 may mean that some modules will need to be rebuilt from the ground up.  It isn’t just about development  - you’ll need to factor in the time it will take to define requirements, not to mention testing and deployment. If it’s a big project, you may also need to juggle the maintenance of the existing codebase for some time, while working on the new version. . The sites that we tend to deal with at Capgemini are big. We work with  large companies with complex requirements , a lot of third party integrations, and  high traffic . In other words, it’s not just your standard brochureware, so we tend to have a lot of custom modules. . Given the fact that an upgrade is non-trivial, the question has to be asked -  what business value will an upgrade bring?  If all you’re doing is replacing a Drupal 7 site with a similar Drupal 8 site, is it really a good idea to spend a lot of time and money to build something that is identical, as far as the average end user can tell? . If the development team is focused on upgrading, will there be any bandwidth for bug fixes and improvements? An upgrade will almost certainly be a big investment - maybe that time, energy and money would be better spent on new features or incremental improvements that will bring tangible business value and can be delivered relatively quickly. Besides, some of the improvements in Drupal 8 core, such as  improved authoring experience , are also available in the  Drupal 7 contrib ecosystem . . On the other hand, it might make more sense to get the upgrade done now, and build those improvements on top of Drupal 8, especially if your existing codebase needs some TLC. . Another option (which we’ve done in the past for an upgrade from Drupal 6 to 7) is to  incrementally upgrade the site , releasing parts of the new site as and when they’re ready. . The right approach depends on a range of factors, including how valuable your proposed improvements will be, how urgent they are, and  how long an upgrade will take , which depends on how complex the site is. . Having said all of that,  the reasons to upgrade to Drupal 8 are compelling . One big plus for Drupal 8 is the possibility of improved performance, especially for authenticated users, thanks to modern features like  BigPipe . The improved authoring experience, accessibility and multilingual features that Drupal 8 brings will be especially valuable for larger organisations. . Not only that, improving Developer Experience (DX) was a big part of the community initiatives in building Drupal 8. Adopting Symfony components, migrating code to object-oriented structures, improving the APIs and a brand new configuration management system are all designed to improve developer productivity and code quality - after the initial learning curve. These improvements will  encourage more of an engineering mindset , and drive modern development approaches. The net benefit will be more testable (and therefore more reliable) features, easier deployment and maintenance methods and increase speed of future change. . There is no one-size-fits-all answer. Your organisation will need to consider its own situation and needs. .  Where does upgrading the CMS version fit into the organisation’s wider digital roadmap?  Is there a site redesign on the cards any time soon? What improvements are you hoping to make? What functionality are you looking to add? Does your site’s existing content strategy meet your needs? Is the solution architecture fit for your current and future purposes, or would it make sense to think about  going headless ? . In summary, while an upgrade will be a big investment, it may well be one that is worth making, especially if you’re planning major changes to your site in the near future. . If the requirements for your upgrade project are “build us the same as what we’ve got already, but with more modern technology” then it’s probably not going to be worth doing. Don’t upgrade to Drupal 8 just because it’s new and shiny. However, if you’re looking further forward and planning to build a solid foundation for future improvements then an upgrade could be a very valuable investment. ", "date": "2016-09-19T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Tips and Thoughts on Prototyping with AWS Elastic Beanstalk\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/aws-elastic-beanstalk/", "abstract": " I’ve investigated a few Platform as a Service (PaaS) offerings as part of small projects or prototypes I’ve done recently. I’ve used  Openshift (version 2) ,  Heroku  and now  Amazon Web Services (AWS) . This is only a few of the available PaaSs, there are a lot out there I’ve still yet to try, but I think my recent use of AWS as my platform of choice has been the most successful so far. . As part of my  degree apprenticeship  with Capgemini and Aston University I was involved in a group project module, designing a system of components with a frontend, REST API and an email service for an application called “Moco”. Seeing the opportunity to learn and use some new tech I decided to use  Docker , so it could be deployed anywhere, and AWS as a platform enabling the whole team to access the application. Docker seemed to fit well as it allowed flexibility in what I deployed, isolation between services and the choice to use various languages or frameworks for each component. We ended up using ReactJS and Redux on the frontend, Laravel for a REST API and Spring Boot with Apache Camel for an email service. . I hadn’t used Docker at all at this point so went through their  tutorial  and got to grips with commands, the Docker client and the fact that I had a VM on my Mac to run containers (native Docker beta release wasn’t out at this point). . The biggest “ah ha” moment was understanding  volumes  and  links . Firstly, that volumes are pieces of disk (folders) from your host machine which are mounted into the containers - like a shared folder in VirtualBox. Secondly, that links enable containers to communicate and have a reference to one another through their container names (these names are put into the container’s hosts file and Docker sorts out the IP addresses). The benefit to using volumes is that Docker images don’t have to be built again if files in a mounted volume change. Volumes allow for your containers to be as abstract and generic as you would like. For example, you can bake the application code into an image if you don’t want to re-use it elsewhere, forcing you to create a new build each time, or you can mount the code as a volume and build the image totally generically, customising and configuring it with volumes. . Running multiple containers meant I got to grips with  Docker Compose , another command line client which manages a set of containers defined in a  docker-compose.yaml  file. Using this setup was a lot faster than using  MAMP , my usual setup, and a lot more flexible e.g. I could change the versions of PHP, Composer and PHPunit much easier and there was no installation necessary. Plus it was easy for others in the team to set up the application - I didn’t have to ask them to install PHP, Composer, MySQL, or any dependencies of the actual application. This made my instructions for running the application very simple (however, in reality getting others to set up Docker for the first time wasn’t totally smooth so, although it ran brilliantly once set up, it took some time). . When I first looked at using AWS I did so because I wanted to see what their ecosystem was like and because I wanted more flexibility than Openshift or Heroku - that and they had a good free tier! I had hoped that there would be less of a learning curve involved with using AWS too and it would be just like using the Docker commands I was getting so comfortable with. . Initially I was worried: I read blog posts about setting up EC2 containers, provisioning it to run Docker, creating security groups and other very low level stuff I didn’t want to stray into. I wanted to keep away from the infrastructure and stick to the abstractions of using a PaaS, after all, it was only a short project - I didn’t want to spend my time setting up security groups. I wondered whether AWS was only “Infrastructure as a Service” but then I found  Elastic Beanstalk (EB)  and the infrastructure abstractions I had been hoping for. . EB gives you a choice of applications to run: Docker (single or multi-container), PHP, Python, Node.js, Ruby, Tomcat and others. It only takes 2/3 pages of options and configuration to have a whole environment set up and ready to be deployed to. As part of the set up Elastic Beanstalk creates an AWS virtual machine, called an EC2 node, for your docker or other type of application to run on. Then, it provides a URL to access your application and creates an S3 bucket where your code is uploaded. It also has has an excellent  command line client  which was not too hard to set up with a secret key. To start using my newly provisioned multi-container Docker environment I had to transform my  docker-compose.yaml  into a  Dockerrun.aws.json  file. This wasn’t hard either but the inevitable debugging that came after took a long time. . Understanding what went wrong in a deployment was probably the hardest and most time consuming aspect of getting set up. There are a lot of logs and a lot of log files and so the first problem is finding what logs are relevant. I mostly spent my time logged into my EC2 instance via SSH and in the container logs  /var/log/docker  where each container logs out to a file based on its container ID. . I came up against a few things I had to work out and debug. Firstly, not every container is essential. I had a container named “application” which simply set up volumes which other containers could use. Setting this container as “essential”:true stalled the deployment and only when I changed it to be “essential”:false did all the containers start correctly. . In another container I had a Java process, a Spring Boot application, which ran fine with docker-compose but when deployed, maxed out the CPU on my EC2 node. It turns out it was using all the I/O CPU and this was because it needed more memory. I doubled it to 256MB and the CPU returned to normal. . I had initially wanted to use a submodule in my git repo for my Docker build files and any configuration files needed for the containers. In the end I realised it was going to be more effort that it was worth to deploy git submodules with Elastic Beanstalk. The deploy command in the EB command line client zips up the checked in code which doesn’t include submodules. To include them there would need to be some post-deploy steps to init the submodule and pull the latest and then continue with the rest of the deploy. I found it was easier to build the images I needed and push them to the Docker Registry, rather than building on the fly when deployed. With this set up, AWS pulls the images from the Docker registry and starts them. Any configuration files necessary, I put into a new sub project (“moco-config”) of my main project and mounted volumes from the folders of that sub project to each container. . Lastly code is uploaded to an S3 (AWS’s object/file store) bucket and then mounted to  /var/app/current  in the EC2 instance so all the volumes need to be sourced from there. You can see this in the volume definitions of my final  Dockerrun.aws.json  file: . After getting around the above problems I needed to be able to run composer install whenever things were pushed. This meant investigating Elastic Beanstalk’s post deploy options. As it turns out it’s not too difficult but it did require a lot of tweaking and checking logs each time to see if it worked correctly. This is the final file: . I called this post_deploy.config and placed in it a folder called  .ebextensions . This file allows you to do multiple things after a deploy has happened but I just needed the commands and files functionality. I used it to create a directory of hooks to be executed, create my environment config file and then created a file with a bash script in it to be put in the created directory. I decided to use the PHP executable within my Docker container to perform the various installations and configurations. This worked very well in the end and I was impressed with not having to install any executables on my EC2 VM. I could rely on the executables always being provided by the Docker containers meaning I could destroy and rebuild the VM anytime and it didn’t require any setup. . Another problem I ran into was running npm on a low memory container. It would crash consistently and because I didn’t want to go anywhere near my free usage tier limits I decided that checking in the built frontend bundle was an acceptable trade off so that the EB CLI deploy command could upload it with the rest of the code. . RDS is Amazon’s database provisioning and management service. I used it to create a MySQL database as the datastore in Moco. I chose to use RDS rather than a Docker container with MySQL in it because it takes some load off the EC2 instance, it’s managed by AWS and comes with upgrade-ability, backup-ability and standard security to protect it. This paid off because halfway through the project I needed to upgrade MySQL in order to use JSON columns and this was a very simple process - it just worked. . In order for Moco to use the RDS instance I used the environment variables automatically assigned by Elastic Beanstalk. I used five of the environment variables:  RDS_HOSTNAME, RDS_PORT, RDS_DB_NAME, RDS_USERNAME, RDS_PASSWORD . Ideally these would be written into the  .env  file (Laravel’s way of overriding configuration per environment) at deploy time enabling you to have multiple environments with the same Laravel database config file. .       Having not used containers much in the past I really felt the benefits of  automation . Whilst the automation here was very simple relative to most projects I was glad to not have to provision virtual machines or do any manual steps with each deploy. I only had to type  eb deploy  and the application would be down for a minute and then back up with the new changes. This was amazingly fast for me and helped me work faster and better with others in the team - any bugs or changes needed and they could be deployed in minutes.     . Having not used containers much in the past I really felt the benefits of  automation . Whilst the automation here was very simple relative to most projects I was glad to not have to provision virtual machines or do any manual steps with each deploy. I only had to type  eb deploy  and the application would be down for a minute and then back up with the new changes. This was amazingly fast for me and helped me work faster and better with others in the team - any bugs or changes needed and they could be deployed in minutes. .       Containerisation also meant I could add a new container whenever, for whatever purpose. In my case I needed a  swagger UI  and it had to be the very latest version so that it included security options in the UI. Once this was built as a Docker image I just had to add it to my AWS Docker file, configure it, and it was available - completely isolated, composable and built at a specific bleeding edge version. The best thing about this was I only had to do this once: when I destroyed my EC2 instance I didn’t have to SSH in, download Node, NPM, build swagger and run it - I just ran the container again.     . Containerisation also meant I could add a new container whenever, for whatever purpose. In my case I needed a  swagger UI  and it had to be the very latest version so that it included security options in the UI. Once this was built as a Docker image I just had to add it to my AWS Docker file, configure it, and it was available - completely isolated, composable and built at a specific bleeding edge version. The best thing about this was I only had to do this once: when I destroyed my EC2 instance I didn’t have to SSH in, download Node, NPM, build swagger and run it - I just ran the container again. .       I didn’t touch any of the power of Elastic Beanstalk but I understand it can make scaling and load balancing very easy and automatic.     . I didn’t touch any of the power of Elastic Beanstalk but I understand it can make scaling and load balancing very easy and automatic. .       It’s free! (as in beer). For 12 months you can run 1 small EC2 instance (1GB RAM) and 1 RDS instance. I also didn’t get anywhere near the S3 upload limit even though I was uploading new code quite often.     . It’s free! (as in beer). For 12 months you can run 1 small EC2 instance (1GB RAM) and 1 RDS instance. I also didn’t get anywhere near the S3 upload limit even though I was uploading new code quite often. .       Because I was using Docker I could test my containers locally with docker-compose and be confident they would work the same way in the cloud (given that the Dockerrun.aws.json is correct and aligned with the docker-compose.yaml). Containerisation meant I didn’t have any issues with missing PHP libraries (I’m looking at you MCrypt!) or differences between versions and I didn’t have to stick to the versions of PHP that AWS supports (I’ve had pain in the past with MAMP’s PHP version and system PHP version). Using containers felt easier and smoother than using the MAMP stack and it felt right that the configuration of the processes required to run the code was written and kept with the code. The containers I used came with PHPunit, Composer and other executables ready installed so I spent less time installing tools and more time using them.     . Because I was using Docker I could test my containers locally with docker-compose and be confident they would work the same way in the cloud (given that the Dockerrun.aws.json is correct and aligned with the docker-compose.yaml). Containerisation meant I didn’t have any issues with missing PHP libraries (I’m looking at you MCrypt!) or differences between versions and I didn’t have to stick to the versions of PHP that AWS supports (I’ve had pain in the past with MAMP’s PHP version and system PHP version). Using containers felt easier and smoother than using the MAMP stack and it felt right that the configuration of the processes required to run the code was written and kept with the code. The containers I used came with PHPunit, Composer and other executables ready installed so I spent less time installing tools and more time using them. . Working with Elastic Beanstalk was absolutely worth it, despite the numerous teething problems. Once set up, it allowed quick iteration on the product and enough configuration to be in control of the infrastructure without being too low-level. It reduced the time I spent interacting with infrastructure and meant I could focus on writing the application. Using it with Docker makes it even more powerful, extensible and maintainable. I can’t comment on using this in production but for prototyping I haven’t used any better Platform as a Service. Lastly, even if you don’t use AWS as a Platform, I would highly recommend trying replacing your *AMP stack with Docker just for easier development purposes and gaining familiarity with containers. ", "date": "2016-09-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Rearing Good Design Ideas in the Wild\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/presenting-ideas/", "abstract": " Imagine you’re in a design session for a new component. The conversation is going back and forth between developers and architects but as they talk you discover the feeling of uneasiness in the current plan. It seems inflexible and prone to far-reaching modifications if change comes. You put out your idea: “I think we should… (insert your master plan here)”. Some people pause but then discussion continues without giving your idea a second thought, leaving you to continue feeling uneasy. Was it a good idea? Was it a bad idea? Why did it not catch on? . I sometimes come across good ideas that gain no traction in a design discussion. I more often come across ideas in my own mind that I feel are better than the current solution but which I can’t articulate. It’s infuriating to have an idea formed which  feels  better but which can’t be substantiated by your current level of technical eloquence. . Part of the key to good design is being able to articulate “why”. You owe it to the team and your engineering practice to defend and pitch your ideas properly so that good ideas don’t get swept away. The truth is, it’s your responsibility to start a discussion and encourage conversation about ideas rather than simply throwing it out into the group and hoping someone catches on. . Defending an idea is essential to giving it a fighting chance of survival out in the design discussion wilderness. Without backing it up, people may not understand why it’s a good idea. They may have missed something you’ve recognised or, by explaining it,  you may realise you’ve missed something  in your considerations. Without backing it up, a good idea is not recognised for its merits and it’s useless to the team - it’s just another possibility, not a solution. . To give yourself the best chance of creating discussion, make sure you prepare. Initially this seems like overkill, but this is practice, and in time you’ll no longer need to prepare but will be able to evaluate designs on the spot. Question your thinking and play devil’s advocate with your own idea: what will be the first counter argument? . To start you could talk about the possible problems of continuing with the current design. This sets people up to hear a solution, which they otherwise may not have been ready for, especially if they didn’t believe there was a problem to begin with. . Make sure it’s clear why you think your solution solves the problem of the current design, otherwise why is it better than what’s currently there? If you’re suggesting some refactoring it’s good to give the other developers in your team a shared reason for the extra effort required. . Question your design and encourage others to question it. It might just be a bad idea and you need to be willing to find that out. However, simply the act of raising the issue could lead to other better solutions. It’s better to present the problem and any solution you have even if you’re not sure it’s the right one. . In the discussion that follows it’s essential to keep a few things at the forefront of your mind. Just as everyone else is respecting you by listening to your idea and giving it the once over, it’s good to listen to and find some appreciation of their counter-arguments. If someone doesn’t understand, appreciate the opportunity to improve your articulation of ideas to someone from a different background . If the team aren’t feeling the idea then don’t force it. It might be that the reason you’re giving doesn’t represent enough value in the minds of the team. . A common pitfall is to push your views because of historical reasons but no practical or functional reason to follow it: “it’s how it’s always been done”, or “it’s how I’ve seen others do it”. This is also known as  cargo cult programming , a practice where, because of inexperience or misunderstanding, code is added which serves no purpose. Allow others to question your actions and you might learn why it’s a good practice or that it’s simply tradition. The other pitfall is to push your views too hard and therefore endanger future design discussions - who wants to be involved in another awkward telling of what’s right and what’s wrong? It can also remove the shared purpose of the team. Pushing your own view promotes yourself rather than the team coming to the conclusion of a good design. Ultimately, a team working together and collectively owning a design is more important than any single design idea. . Lastly, here are a few practical tips: . When someone makes a decision, ask why - you get to understand their articulation and their ideas. Their reasons will usually be universally applicable good reasons for design decisions so you can use them to inspect your own ideas. . These are universally good reasons which can be used to examine your own ideas, if they fit it’s worth pointing that out to the team:              Is it good for isolating change?         Is it good for simplicity?         Does it increase reusability?           . Is it good for isolating change? . Is it good for simplicity? . Does it increase reusability? . These are universally recognised bad reasons for design or refactoring and therefore questions which you should ask of yourself:              Is it  gold plating ?          Are you going to need it ?           . Is it  gold plating ? .  Are you going to need it ? . When you have an idea, gather the team round for a quick informal discussion (no meeting invites necessary, just a white board will do) and present the problem and your solution. . As mentioned above, get a white board and draw your idea - it creates better discussion and makes it easier for anyone in the team to articulate their thinking because they can point at any part and the whole team will know what they are talking about. . Consider how you present your ideas. It’s important to back up your suggestions, but also be aware of the team interactions and the discussion happening when a team solutionises. Lastly, question your own way of working: Do you find people easily buy into your ideas? How do you get everyone on board? How do you get the best out of the team so they learn to create other ideas and build on your own through discussion? Fostering a culture of healthy debate and openness will always lead to better engineering and solutions. ", "date": "2016-10-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Fun with Drupal 8 configuration management\n", "author": ["Richard Sheppard"], "link": "https://capgemini.github.io/drupal/d8config-fun/", "abstract": " It has been a few years since I have had the opportunity to build a website from the absolute beginning. The most recent project I’m on continues in that vein, but it’s early enough for me to consider ripping it apart and starting all over again. The project is particularly interesting to me, as it’s my first opportunity to use Drupal 8 in earnest. As I’ve got an interest in  automating as much as possible , I want to gain a better understanding of the configuration management features which have been introduced in Drupal 8. . Tearing it apart and starting again wasn’t the first thing considered. Being an arrogant Drupal dev, I figured I could simply poke around the GUI and rely on some things I’d seen at Drupalcon and Drupal camps in the past couple of years to see me through. I thought I would find it easy to build a replicated environment so that any new developer could come along, do a  git clone ,  vagrant up , review a README.md file and/or wiki page and they’d be off and running. . Wrong. . This post outlines many of the things that I examined in the process of learning Drupal 8 while adopting a bit of humility. I’ve created a  sample project  with names changed to protect the innocent. Any comments are welcome. . The structure of the rest of this post is as follows: .  Setting up and orientation with Drupal VM  .  Build your prototype  .  Replicate and automate  .  The reason we’re gathered here today (a brief intermission)…  .  Packaging it all up and running with it.  .  Summary and conclusion  . I am a big fan of Jeff Geerling’s  Drupal VM  vagrant project, so I created a fork of it, and imaginatively called it  D8config VM . We will be building a Drupal site with the standard profile which we’ll use to rapidly build a basic prototype using the Drupal GUI - no coding chops necessary. The only contributed module added and enabled is the  Devel module  at the start, but we will change that quickly. . Here are the prerequisites if you do follow along: . familiarity with the command line; . familiarity with  Vagrant  and that it’s installed on your machine ( note : the tutorial requires Vagrant 1.8.1+); . as well as Vagrant 1.8.1+, you need to have Ansible 2.0.1+ and VirtualBox 5.0.20+ installed; . have installed the  Vagrant Auto-network plugin  with  vagrant plugin install vagrant-auto_network . This will help prevent collisions with other virtual networks that may exist on your computer; . have installed the  Vagrant::Hostsupdater  plugin with  vagrant plugin install vagrant-hostsupdater , which will manage the host’s  /etc/hosts  file by adding and removing hostname entries for you; . familiarity with git and  GitHub ; . if using Windows, you are comfortable with troubleshooting any issues you might come across, as it’s only been tested on a Mac; . familiarity with  Drush . . Here is how the D8config VM differs from Drupal VM: . the  config.yml  and  drupal.make.yml  files have been committed, unlike the normal Drupal VM repo; . the hostname and machine name have been changed to  d8config.dev  and  d8config  respectively; . to take advantage of the auto-network plugin,  vagrant_ip  is set to 0.0.0.0. The d8config machine will then have an IP address from 10.20.1.2 to 10.20.1.254; . the first synced folder is configured with a relative reference to the Vagrant file itself: . I’ll do the same with subsequent shared folders as we progress - it’s a useful way to keep the different repos together in one directory. At the end of the tutorial, you’ll have something like: .  top  . If you have used Drupal VM before but haven’t upgraded in a while, there are a load of new features. Here are just two to note: . Ansible roles are now installed locally (in  ./provisioning/roles ) during the first  vagrant up ; . PHP 7 is now an option to be installed. In fact, it’s installed by default. You can select 5.6 if you like by changing  php_version  in the  config.yml  file (see  PHP 5.6 on Drupal VM ). . Create a directory where you’re going to keep all of the project assets. . Clone the  D8config VM repo . Or, feel free to fork  D8config VM  and clone  your version  of the repo. . After successful provisioning you should be able to point your browser at  http://d8config.dev  and see your barebones Drupal 8 site. Username:  admin ; Password:  admin . . If you’ve used Drupal VM before, you will want to examine the changes in the latest version. From  Drupal VM tag 3.0.0  onwards, the requirements have changed: . Vagrant 1.8.1+ . Ansible 2.0.1+ . VirtualBox 5.0.20+ . One sure sign that you’ll need to upgrade is if you see this message when doing a  vagrant up : .  top  . In this section of the tutorial we’re going to start building our prototype. The brief is: . The site is a portfolio site for for a large multinational corporation’s internal use. But hopefully the content architecture is simple enough to keep in your head. The following node types need to be set up:  Case study ,  Client ,  Team member  (the subject matter expert), and  Technologies  used. Set up a vocabulary called  Country  for countries served and a second to called  Sector  classify the information which will contain tags such as  Government ,  Music industry ,  Manufacturing ,  Professional services , etc. Delete the existing default content types. You can then delete fields you know you won’t need to avoid confusion -  Comments  for example - which will then allow you to uninstall the comment module. And, as you might deduce by the modules I’ve selected, it’s to be a multilingual site. . Hopefully this should feel comfortable enough for you, if you are familiar with Drupal site building. There are enough specifics for clarity, yet it’s not too prescriptive that you feel someone is telling you how to do your job. Whereas in one context, you may hear a manager say  “don’t bring me problems, bring me solutions” , most engineers would rather say for themselves  “don’t bring me solutions, bring me problems” . I hope this brief does the latter. . Have a go at making the changes to your vanilla Drupal 8 site based on the brief. . Every ‘site building’ exercise with Drupal is a move further away from the configuration provided by the standard or minimal profiles. In our circumstance, we will enable these modules via the GUI: . Responsive Image . Syslog . Testing . BigPipe . Devel Generate (Devel was installed due to settings in  config.yml  in the d8config-vm repo) . Devel Kint . Devel Node Access . Web Profiler . Configuration Translation . Content Translation . Interface Translation . Language . I’ve also added a couple of contributed themes via Drush so the site will no longer look like the default site. . For more details on these themes, see  the Integrity theme  and  the Adminimal theme . As you might expect, I set Integrity as the default theme, and Adminimal as the admin theme via the GUI. . After switching themes, two blocks appeared in the wrong regions. I went to the  Block layout  page and moved the Footer menu block from the  main menu  region to the  footer first  region and the Powered by Drupal block from the  Main menu  to the  Sub footer  block. . Due to the multilingual implication, I went to the  Languages admin  page and added French. .  top  . At this stage you’ve made quite a lot of changes to a vanilla Drupal site. There are many reasons you should consider automating the building of this site - to save time when bringing other members into the development process, for creating QA, UAT, pre-prod and production environments, etc. We will now start to examine ways of doing just this. . In this section we’re going to create a Drush makefile to get the versions of Drupal core, contrib modules and themes we need to build this site as it currently is. This file will be the first file added to the  D8config profile  repo. Makefiles are not a required part of a profile, and could reside in a repo of their own. However to keep administration down to a minimum, I’ve found that this is a useful way to simplify some of the asset management for site building. . Let’s first tweak the  config.yml  in the D8config VM repo, so that we have synced folder for the profile. To do so, either: .  git checkout CG02  in the d8config-vm directory (where I’ve already made the changes for you), or; . Add the following to the  config.yml  in the  vagrant_synced_folders  section: . After doing either of the above, do a  vagrant reload  which will both create the directory on the Vagrant host, and mount it from the d8config-vm guest. . Next, let’s generate a basic makefile from the site as it now is. . This makefile is now available in the  d8config_profile  directory which is at the same level as your  d8config-vm  directory when viewing on your host machine. . Because we only have Drupal core, two contrib themes and the Devel module, it’s a very simple file and it doesn’t need any tweaking at this stage. I’ve committed it to the  D8config profile  repo and tagged it as  CG01 . . Since we’ve established that the makefile is doing very little on this site, we need to look at completing the rest of the profile which will apply the configuration changes when building the site. The  How to Write a Drupal 8 Installation Profile  is quite clear and we’ll use that page to guide us. . First, our machine name has already been chosen, as I’ve called the repo  d8config_profile . . Rather than writing the  d8config_profile.info.yml  file from scratch, let’s duplicate  standard.info.yml  from the standard profile in Drupal core, as that’s what we used to build the vanilla site to begin with. We can then modify it to reflect what we’ve done since. . The first five lines of the  d8config_profile.info.yml  need to look like this: . At the end of the file it looks like this, which shows the required core modules and adding the modules and themes we’ve downloaded: . Also, don’t forget, we uninstalled the comment module, so I’ve also removed that from the dependencies. . The profile specifies the modules to be enabled, but not how they’re to be configured. Also, what about the new content types we’ve added? And the taxonomies? With previous versions, we relied on the  features  module, and perhaps  strongarm  to manage these tasks. But now, we’re finally getting to the subject of the tutorial - Drupal 8 has a  configuration system out of the box . . This is available via the GUI, as well as Drush. Either method allows you to export and import the configuration settings for the whole of your site.  And  if you look further down the  profile how-to page , you will see that we can include configuration with installation profiles. . Let’s export our configuration using Drush. This is will be far more efficient than exporting via the GUI, which downloads a  *.tar.gz  file, which we’d need to extract a copy or move to the  config/install  directory of the profile. . While logged into the vagrant machine and inside the site’s root directory: . When I exported my configuration, there were ~215 files created. Try  ls -1 | wc -l  in the config/install directory to check for yourself. .  top  . I hope you are finding this tutorial useful - and also sensible. When I started writing this blog post, I hadn’t realised it would cover quite so much ground. The key thing I thought I would be covering was Drupal 8’s configuration management. It was something I was very excited about, and I still am. To demonstrate some of the  fun  I’ve had with it is still the central point of this blog. All of the previous steps to get to this point were fun too, don’t get me wrong. From my point of view, there were no surprises. . Configuration management, on the other hand - this is true drama. Taking an existing shared development site and recreating it locally using Drush make and a basic profile (without the included  config/install  directory) is just a trivial soap opera. If you want real fun, visit the configuration-syncing aspect, armed only with knowledge of prior versions of Drupal and don’t  RTFM . . No, really. Do it. . After doing the export of the configuration in the previous section, I finally started running into the problems that I faced during my real world project - the project mentioned at the beginning of this post. Importing the configuration repeatedly and consistently failed with quite noisy and complex stack trace errors which were difficult to make sense of. Did I mention that perhaps I should have  read the manual ? . We need to do two things to make the configuration files usable in this tutorial before committing: . The removal of those two files was found to be required thanks to reading  this  and  this . At this stage, I can confirm these were the only two files necessary for removal, and perhaps as Drupal 8’s configuration management becomes more sophisticated, this will not be necessary. The second command will recursively remove the lines with the uuid key/value pairs in all files. .  top  . We’ve done all the preparation, and now need to make some small tweaks and commit them so our colleagues can start where we’ve left off. To do so we need to: . add the profile to the makefile; . commit our changes to the d8config_profile repo; . tweak the  config.yml  file in the d8config-vm repo, to use  our  makefile and profile during provisioning. . To have the profile be installed by the makefile, add this to the bottom of  d8config.make  (in the  D8config profile ): . I’ve committed the changes to the  D8Config profile  and tagged it as  CG02 . . Then the last change to make before testing our solution is to tweak the  config.yml  in the  D8config VM  repo. Three lines need changing: . As you can see, the changes to the vagrant project are all about the profile. . With both the  D8Config VM  and the  D8Config profile  in adjacent folders, and confident that this is  all going to work , from the host do: . Once the provisioning is complete, you should be able to check that the site is functioning at  http://d8config.dev . Once there, check the presence of the custom content types, taxonomy, expected themes, placement of blocks, etc. .  top  . The steps we’ve taken in this tutorial have given us an opportunity to look at the latest version of  Drupal VM , build a  quick-and-dirty  prototype in Drupal 8 and make a profile which our colleagues can use to collaborate with us. I’ve pointed out some  gotchas  and in particular some things you will want to consider regarding exporting and importing Drupal 8 configuration settings. . There are more questions raised as well. For example, why not simply keep the  d8config.make  file in the  d8config-vm  repo? And what about the other ways people use  Drupal VM  in their workflow - for example  here  and  here ? Why not use the minimal profile when starting a protoype, and save the step of deleting content types? . Questions or comments? Please let me know. And next time we’ll just use  Docker , shall we? ", "date": "2016-10-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "What to look for in a code review\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/what-to-look-for-in-code-review/", "abstract": " In a previous article on this blog, I talked about  why code review is a good idea , and some aspects of how to conduct them. This time I want to dig deeper into the practicalities of reviewing code, and mention a few things to watch out for. .  Code review is the first line of defence against hackers and bugs . When you approve a pull request, you’re putting your name to it - taking a share of responsibility for the change. . Once bad code has got into a system, it can be difficult to remove. Trying to find problems in an existing codebase is like  looking for an unknown number of needles in a haystack , but when you’re reviewing a pull request it’s more like looking in a handful of hay. The difficult part is recognising a needle when you see one. Hopefully this article will help you with that. . Code review shouldn’t be a box-ticking exercise, but it can be helpful to have  a list of common issues  to watch out for. As well as the important question of whether the change will actually work, the main areas to consider are: . Security . Perfomance . Accessibility . Maintainability . I’ll touch on these areas in more detail - I’ll be talking about Drupal and PHP in particular, but a lot of the points I’ll make are relevant to other languages and frameworks. . I don’t claim to be an expert on security, and often count myself lucky that I work in what my colleague Andrew Harmel-Law calls  “a creative-inventive market, not a safety-critical one” . . Having said that, there are a few common things to keep an eye out for, and developers should be aware of the  OWASP top ten list of vulnerabilities . When working with Drupal, you should bear in mind the  Drupal security team’s advice for writing secure code . For me, the most important points to consider are: . In short - don’t trust user input. The big attack vectors like XSS and SQL injection are based on malicious text strings. Drupal provides several types of text filtering - the appropriate filter depends on what you’re going to do with the data, but you should always run user input through  some kind of sanitisation . . Security isn’t just about stopping bad guys getting in where they shouldn’t. Think about what kind of data you have, and what you’re doing with it. Make sure that you’re not logging people’s private data inappropriately, or passing it across network in a way you shouldn’t. Even if the site you’re working on doesn’t have anything as sensitive as  the Panama papers , you have a legal, professional, and personal responsibility to make sure that you’re handling data properly. . When we’re considering code changes, we should always think about what impact they will have on the end user, not least in terms of how quickly a site will load. As  Google recently reminded us , page load speed is vital for user engagement. Slow, bloated websites cost money, both in terms of  mobile data charges  and  lost revenue . . Most Drupal performance strategies will talk about the value of caching. The aim of the game is to reduce the amount of work that your web server does. Ideally, the web server won’t do any work for a page request from an anonymous user - the whole thing will be handled by a reverse proxy cache, such as Varnish. If the request needs to go to the web server, we want as much of the page as possible to be served from an object cache such as Redis or Memcached, to minimise the number of database queries needed to render the page. . Typically, reverse proxy servers like Varnish will not cache pages for authenticated users. If the browser has a session, the request won’t be served by Varnish, but by the web server. . Here’s an illustration of why this is so important. This graph shows the difference in response time on a load test environment following a deployment that included some code to create sessions. There were some other changes that impacted performance, but this was the big one. As you can see, overall response time increased six-fold, with the biggest increase in the time spent by the web server processing PHP (the blue sections on the graphs), mainly because a few lines of code creating sessions had slipped through the net. .   . The developers’ maxims “Don’t Repeat Yourself” and “Keep It Simple Stupid” apply to servers as well. If the server is doing work to render a page, we don’t want that work to be repeated or overly complex. . There’s no substitute for actually testing, but there are a few things that you can keep an eye out for when reviewing change. Does the change introduce any additional HTTP requests? Perhaps they could be avoided by using sprites or icon fonts. Have any images been optimised? Are you making any  repeated DOM queries ? . Even if you’re not an expert on accessibility, and don’t know  ARIA roles , you can at least bear in mind a few general pointers. When it comes to testing, there’s a good  checklist from the Accessibility Project , but here are some things I always try to think about when reviewing a pull request. . Doing proper accessibility testing is difficult, and you may not have access to assistive technology, but a good rule of thumb is that if you can navigate using only a keyboard, it will probably work for someone using  one of the myriad input devices . Testing is the only way to be certain, but here are a couple of simple things to remember when reviewing CSS changes: hover and focus should usually go together, and  you should almost never use  outline: none;  . . One piece of low-hanging fruit is to make sure that text is available to screen readers and other assistive technology. Any time I see  display: none;  in a pull request, alarm bells start ringing. It’s usually not  the right way to hide content . . Hopefully the system you’re working on will last for a long time. People will have to work on it in the future. You should try to make life easier for those people, not least because you’ll probably be one of them. . Are you writing more code than you need to? It may well be that the problem you’re looking at has already been solved, and one of the great things about open source is that you’re able to recruit  an army of developers and testers you may never meet . Is there already a module for that? . On the other hand, even if there is an existing module, it might not always make sense to use it. Perhaps the contributed module provides more flexibility than our project will ever need, at a performance cost. Maybe it gives us 90% of what we want, but would force us to do things in a certain way that would make it difficult to get the final 10%. Perhaps it isn’t in a very healthy state - if so, perhaps you could fix it up and contribute your fixes back to the community, as I did  on a recent project . . If you’re writing a custom module to solve a very specific problem, could it be made more generic and contributed to the community? A couple of examples of this from the Capgemini team are  Stomp  and  Route . . One of the jobs of the code reviewer is to help draw the appropriate line between the generic and the specific. If you’re reviewing custom code, think about whether there’s prior art. If the pull request includes community-contributed code, you should still review it. Don’t assume that it’s perfect, just because someone’s given it away for nothing. . Is your team using your chosen frameworks as they were intended? If you see someone writing a custom function to solve a problem that’s already been solved, maybe you need to share a link to the API docs for the existing solution. . If your logs are littered with notices about undefined variables or array indexes, not only are you likely to be suffering a performance hit from the logging, but it’s much harder to separate the signal from the noise when you’re trying to investigate something. . Remember that sometimes,  it’s good to be boring . As a reviewer, one of your jobs is to stop your colleagues from getting carried away with shiny new features like ES6, or CSS variables. Tools like  Can I Use  are really useful in being able to check what’s going to work in the browsers that you care about. . Sometimes, code seems wrong. As I learned from  Larry Garfield’s excellent presentation on code smells  at the first Drupalcon I went to, code smells are indications of things that might be a deeper problem. Rather than re-hash the points Larry made, I’d recommend reading  his slides , but it is worth highlighting some of the anti-patterns he discusses. . A function should have a function. Not two functions, or three. If an appropriate comment or function name includes “and”, it’s a sign you should be splitting the function up. . Another bad sign is the word “or” in the comment. Functions should always do the same thing. . Long functions are usually a sign that you might want to think about refactoring. They tend to be an indicator that the code is more complex than it needs to be. The level of  complexity can be measured , but you don’t need a tool to tell you that if a function doesn’t fit on a screen, it’ll be difficult to debug. . Even if functions are simple enough to write tests for, do they depend on a whole system? In other words, can they be genuinely unit tested? . There’s more to be said on the subject of code comments than I can go into here, but suffice to say code should have useful, meaningful comments to help future maintainers understand it. . Modules should be modular. If two parts of a system need to interact, they should have a clearly defined and documented interface. . Side effects and global variables should generally be avoided. . Is the purpose of a function or variable obvious from the name? I don’t want to  rehash old jokes , but naming things is difficult, and it is important. . Why would you comment out lines of code? If you don’t need it, delete it. The beauty of version control is that you can go back in time to see what code used to be there. As long as you write a good commit message, it’ll be easy enough to find. If you think that you might need it later, put it behind a  feature toggle  so that the functionality can be enabled without a code release. . In CSS, IDs and  !important  are the big code smells for me. They’re a bad sign that a specificity arms race has begun. Even if you aren’t going to go all the way with a system like  BEM  or  SMACSS , it’s a good idea to keep specificity as low as possible. The excellent articles on CSS specificity by  Harry Roberts  and  Chris Coyier  are good starting points for learning more. . It’s important to follow  coding standards . The point of this isn’t to get some imaginary Scout badge - code that follows standards is easier to read, which makes it easier to understand, and by extension easier to maintain. In addition, if you have your IDE set up right, it can warn you of possible problems, but  those warnings will only be manageable if you keep your code clean . . Will your changes be available in environments built by Continuous Integration? Do you need to set default values of variables which may need overriding for different environments? Just as your functions should be testable, so should your configuration changes. As far as possible, aim to make everything repeatable and automatable - if a release needs any manual changes it’s a sign that your team may need to be thinking with more of a  DevOps mindset . . With all this talk of coding style and standards, don’t get distracted by trivialities - it is worth caring about things like whitespace and variable naming, but remember that  it’s much more important to think about whether the code actually does what it is supposed to . The trouble is that our eyes tend to fixate on those sort of things, and they cause  unnecessary cognitive load . . Pre-commit hooks can help to catch coding standards violations so that reviewers don’t need to waste their time commenting on them. If you’re on a big project, it will almost certainly be worth investing some time in integrating your CI server and your code review tool, and  automating checks for issues like code style , unit tests, mess detection - in short, all the things that a computer is better at spotting than humans are. . Does the code actually solve the problem you want it to? Rather than just looking at the code, spend a couple of minutes reading the ticket that it is associated with - has the developer understood the requirements properly? Have they approached the issue appropriately? If you’re not sure about the change, check out the branch locally and test it in your development environment. . Even if there’s nothing  wrong  with the suggested change, maybe there’s a better way of doing it. The whole point of code review is to share the benefit of the team’s various experiences, get extra eyes on the problem, and hopefully make the end product better. . I hope that this has been useful for you, and if there’s anything you think I’ve missed, please let me know via the comments. ", "date": "2016-10-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Kubernetes, Ingress controllers and Traefik\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/kubernetes/kube-traefik/", "abstract": " When running your application services on top of an orchestration tool like Kubernetes or Mesos with Marathon there are some common necessities you’ll need to satisfy. Your application will usually contain two types of services, those that should be visible only from inside of the cluster, and those that you want to expose to the external world, outside your cluster and maybe to the internet (e.g frontends). . This article will focus on how to approach this on Kubernetes. . You can make use of the different service types that Kubernetes makes available for you when creating a new service in order to achieve what you want. .       ClusterIP: This is the default. Choosing this value means that you want this service to be reachable only from inside of the cluster.     . ClusterIP: This is the default. Choosing this value means that you want this service to be reachable only from inside of the cluster. .       ExternalName: It serves as a way to return an alias to an external service residing outside the cluster.     . ExternalName: It serves as a way to return an alias to an external service residing outside the cluster. .       NodePort: Expose the service on a port on each node of the cluster.     . NodePort: Expose the service on a port on each node of the cluster. .       LoadBalancer: on top of having a cluster-internal IP and exposing service on a NodePort, also ask the cloud provider for a load balancer which forwards requests to the Service exposed as a  &lt;NodeIP&gt;:NodePort  for each Node. If the cloud provider does not support the feature, the field will be ignored.     . LoadBalancer: on top of having a cluster-internal IP and exposing service on a NodePort, also ask the cloud provider for a load balancer which forwards requests to the Service exposed as a  &lt;NodeIP&gt;:NodePort  for each Node. If the cloud provider does not support the feature, the field will be ignored. . So, if your cloud does not support “loadBalancer” (e.g. you run an on-premise private cloud), and you need something more sophisticated than exposing a port on every node of the cluster, then it used to be that you’d need to build your own custom solution. Fortunately this is not true anymore. . Since  Kubernetes v1.2.0  you can use  Kubernetes ingress  which includes support for TLS and L7 http-based traffic routing. .  You can also ask  Kuwit  “How can I expose services to the external world?” whenever you need to remember this ;-)  . An Ingress is a collection of rules that allow inbound connections to reach the cluster services. It can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, offer name based virtual hosting, and other useful configuration. Users request Ingress by POSTing the Ingress resource to the API server. . In order for the Ingress resource to work, the cluster must have an Ingress controller running. The Ingress controller is responsible for fulfilling the Ingress dynamically by watching the ApiServer’s /ingresses endpoint. . This is handy! Now you could go even further, isolate at the infra level where your Ingress controller runs and think of it as an “edge router” that enforces the firewall policy for your cluster. The picture for a High Available Kubernetes Cluster would look something like this: .   . We’ll show how to use  Traefik  for this purpose. Traefik is a modern HTTP reverse proxy and load balancer made to deploy microservices with ease. It supports several backends among Mesos/Marathon and Kubernetes to manage its configuration automatically and dynamically. . We’ll deploy a Kubernetes cluster similar to the picture above and will run Traefik as  DaemonSet . . The source code is  here  .  You can configure Traefik to use automatic TLS config for your services on demand . . Our edge-router will be just another  Kubernetes node  with some restrictions. . We don’t want any other pod to be scheduled to this node so we set  --register-schedulable=false  when running the  kubelet  as well as giving it a convenient label:  --node-labels=edge-router . . Kubernetes will run DaemonSets on every node of the cluster even if they are non-schedulable. We only want this DaemonSet to run on the edge-router node so we use “nodeSelector” to match the label we previously added. . Notice that with this approach, if you want to add a new edge-router to the cluster, all you need to do is spin up a new node with that label and a new DaemonSet will be automatically scheduled to that machine. Nice! . Here is a video demo of all this in action using two different clouds (DigitalOcean and AWS), deploying two Kubernetes clusters from scratch: . Recently I’ve seen a lot of users on  Kubernetes slack  with issues communicating to the Ingress controller. This is often due to a known problem. . The Ingress controller might want to use  hostPort  to expose itself. . If you are using a  CNI   Network Plugin  for your cluster networking, hostPort is not supported yet. . You can track the current status of this problem here: .  https://github.com/kubernetes/kubernetes/issues/23920   https://github.com/kubernetes/kubernetes/issues/31307   https://github.com/containernetworking/cni/issues/46  . For now potential workarounds for this are to use “hostNetwork” or run a service using the previously mentioned “nodePort” to match your Ingress controller running as a DaemonSet. . Hopefully this post will give you some insight about how to expose services on Kubernetes and the benefits of Ingress controllers. ", "date": "2016-11-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Ansible and Weave step by step\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/cloud/ansible-weave/", "abstract": " This is a pragmatic guide to Ansible for beginners. This use case will guide you on how to set up a cross-cloud software defined network for containers using  Weave Net ,  Weave Scope  and  Docker . There is  a full gitbook  including also a theoretical introduction to the main concepts of Ansible. . This tutorial is using  ansible 2.0.2.0  . The source code is  available on GitHub  . This tutorial will assume that you have two machines running coreOS on DigitalOcean and AWS. You can create them manually or using something like  Terraform  or  docker-machine . We provide the  docker-machine-bootstrap  script so you can use it and modify it for this purpose. . We’ll use the public IPs of these machines to create the weave network. Make sure your AWS security groups configuration match the Weave requirements. For this demo I used a totally open configuration. .   . If you don’t want to create these machines you could use any machine with docker and  systemd  reachable via ssh from where you are running Ansible. . At the end we’ll deploy two containers that will communicate with each other: one in DigitalOcean and another in AWS. . As we’ll use CoreOS that is a minimal OS and do not ship with any version of Python we’ll need to install a Python interpreter inside the machines. We’ll use an Ansible module from the community for this, let’s begin… .   Source code :   git checkout step-1  . Before reinventing the wheel you should try reusing community modules.  Ansible galaxy  is a website for sharing and downloading Ansible roles and a command line tool for managing and creating roles. You can download roles from Ansible galaxy or from your specific git repository. Ansible allows you to define your dependencies with standalone roles in a yaml file. See  requirements.yml  . By default Ansible assumes it can find a  /usr/bin/python  on your remote system. The  coreos-bootstrap role  will install  pypy  for us. . Certain settings in Ansible are adjustable via a  configuration file .  Click here for a very complete template . . We’ll set here the target folder for our community roles. . In  ansible.cfg : .   . Just run  ansible-galaxy install -r requirements.yml  .   Source code :   git checkout step-2  . We’ll create an inventory so we can specify the target hosts. You can create meaninful groups for your hosts in order to decide what systems you are controlling at what times and for what purpose. . You can also specify variables for groups. We set the CoreOS specifics here. . We’ll create a playbook so we can declare our expected configuration for every host. . In this step our  playbook.yml  will only include the role downloaded previewsly on every coreos machine (just one so far). By default. . The folder tree will look like this now: .   . Run ansible: .   .   Source code :   git checkout step-3  . We add the new machine into our Inventory file: . Run: . You will see it fails for aws01 as the python interpreter is not there yet. .   . So let’s apply the playbook again. .   . Now: .   . Nice! .   Source code :   git checkout step-4  . So far we have used Ansible to set up a python interpreter for the CoreOS machines so we can run Ansible effectively as many modules rely on python. . In this Step we’ll setup a  Weave network  and  Weave Scope  between both clouds so docker containers can communicate with ease. . We add a new role dependency on the requirements. . Run: . We’ll modify the inventory to create a group of hosts that belong to the weave network. By using the  “children”  tag you can create a group of groups . We’ll override the weave role variables for satisfying our needs. Ansible allows to create variables per host, per group, or site wide variables by setting  group_vars/all  . In  group_vars/weave_server.yml  . Add te weave role into our playbook: . Run ansible again to configure weave: . You can run commands remotely from Ansible cli. Lets check that weave is up and running: .   . We should be able to access to the Scope UI on the browser now: .   . The weave role relies on Ansible templates for generating Systemd scripts: . weave.service.j2: . weave.env.j2: . Weave needs to know the ips of the different host of the network.  Ansible provide some magic variables so you can get information from the different hosts while running a playbook . . This templates are populated at runtime by using  hostvars  magic variable. .   Source code :   git checkout step-5  . In this step we’ll use the power of  tags  and conditional in order to deploy some services running on docker so we can test that they can communicate from DigitalOcean to AWS. . The playbook will look like this now: . We’ll run this on demand by using the conditional  when: deployment_enabled  and tags. . We’ll create a site wide variables file at  group_vars/all.yml  . Run only the deployment tasks by specifying the tag: .  weaveworks/gs-spring-boot-docker  is running on AWS now and  weaveworks/weave-gs-ubuntu-curl  is running on DigitalOcean. . If you check the logs for the  weaveworks/weave-gs-ubuntu-curl  container or you run  curl http://spring-hello.weave.local:8080/  inside the container you’ll see how is communicating with the  weaveworks/gs-spring-boot-docker  container that is running on AWS. . You can also check the connection on Scope. . Grouping by DNS Name: .   . Grouping by Container: .   . After following all the steps your folder tree should look something like this: .   . Hopefully, You’ll now have a better idea about the strengths of Ansible and how to make the most out of it. . Contributions are very welcome  on the github repo  . Check out  the full gitbook including a “Concepts” section . ", "date": "2016-11-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "React-ing to change\n", "author": ["Greg Wolverson"], "link": "https://capgemini.github.io/react/reacting-to-change/", "abstract": " With the recent release of the  ‘The State of JavaScript’  the clear big gainer of 2016 is ReactJS. So what’s all the fuss about? I’m going to look at the two big boys in the front end space. Angular and React! . Cast your mind back to 2010 when users started to demand interactive web applications. Back then the only real solution was jQuery, and I’d love to know how many of us have sat debugging 1000+ lines of jQuery at some point in our careers. . Cue Google announcing the release of a framework  AngularJS  to solve all our problems. Bringing  MVC  to our JavaScript. It sounded a bit mad, the uptake was slow. But wait a minute, we can write unit tests, we can modularise, and we can relate our backend practices to our front end code. . Then Node/npm came along making it easier for us to use third party libraries. So we installed some tools; bower, grunt, gulp (really just things that made npm easy at the time) and we automated everything. Our pages became a bit bloated, our code started to look messy and over time we ended in a tangled web of dependencies and our beautiful Angular code was well… no longer beautiful anymore. . In 2013 Facebook joined the JavaScript party, with their first release of  ReactJS !  sigh  What’s that, another JS framework I hear you say?  Huh  they write HTML in JavaScript? No chance is this going to go anywhere, all my best practices say this is bad. Before you click away from this post in despair, bear with me, firstly, it’s not a framework, but a library, and a comparably small one at that. With its virtual DOM, one-way data flow and the introduction of  JSX  (more on this later), you’ll soon see why it’s so popular amongst front end developers. Not to mention that ReactJS does not use the typical MVC architecture, React only provides the View layer, which actually works really well! However with the introduction of  ES6 , and fantastic tooling it seems ReactJS could well have changed the game. . It’s clear that the JS world is changing, the question is; is React taking over, or has the Angular bubble burst? Both technologies have or do provide a solid approach to front end development, we are going to take a very high level view and try to help provide some clarity on the following question  “I’m starting a new project what should I use?”  . Enough of the small talk, how do we go about comparing these two goliaths? In reality this can’t be a true comparison, as Angular is a framework and React a library; but we will be looking at some of the important aspects we look at when considering technologies for our projects: . Abstraction – Can we represent complex things in ways that are understandable? . Performance – Does it scale well, with fast response times? In my browser? In my code? . Integration – Can I use libraries/frameworks I’m already familiar with? Can I get help if I have a problem? . Simplicity - What is the learning curve? Does it follow relatable architectural principles? How quickly can a new developer become productive? . Testability - How easy is it to test? . Debugging – Problems occur, how easy is it to understand what happened? . State Management – Does it handle state correctly and efficiently? . So, the stage is set, the big guns are out and it’s time to put each of our contestants’ mettle to the test… .    Source  . Firstly, we look at how well both technologies achieve levels of abstraction. Any good technology should strive to achieve a high level of abstraction, by ensuring only the essential details of its API are exposed; it allows the developer to focus on the key concepts, whilst being able to build upon that foundation, leading to  achieving a higher level of abstraction  in their own applications. When learning or beginning to use Angular, one gripe people can have is that you need to understand the underlying model fully, and the framework as a whole, in order to build efficient apps. It can be quite easy to get into a mess of cross dependencies in your application. It can be said that  Angular suffers from poor abstraction in places , and when working with a technology which doesn’t have good abstraction levels in place, it can  hold developers back . . Angular is opinionated, but that’s  not necessarily a bad thing . In one way, it can prove more difficult to bring in external dependencies if you don’t like the way Angular does something in particular. On the other hand, sometimes having an opinionated framework helps steer you in the right direction, and for new developers especially, it can take away the difficulty of learning lots of additional libraries, and can lead to finding the best solution quickly. Furthermore, you could argue that React provides great freedom in being able to choose whichever libraries fit best with your design and application, but that can be a burden, sometimes meaning that finding the best fit solution takes longer. . As a technology, React is a  just  a library, at its core it’s made up of functional components, with clear direction on how to manage state properly. In reality there isn’t a great deal to React at its core, but what is there, is done very well. It follows well known development principles; you  D on’t  R epeat  Y ourself ( DRY ), React encourages modularisation and stateless components. Things are kept simple, components are programmed with functions and objects just like you would in OO or functional programming, which both keeps with solid language practices and lowers the learning curve. That’s not to say however, that Angular doesn’t follow the DRY principles and modularisation, but React employs simpler mechanisms to achieve them. . However, React can suffer from loss of flexibility; you can’t add attributes to HTML tags, or single tags per components. Although, React solves this through the fact that  everything is a component , and they all benefit from a simple life cycle, as well as being   first class citizens  . In turn, by React ensuring its abstraction doesn’t leak, you don’t find yourself needing to read React code under the hood to understand things, which is a huge bonus for both new and veteran developers. . Onto one of the most important aspects of any JS framework:  performance . This is where users are often won or lost. When it comes to load times,  more than 1 second from page load to being able to interact with a page is too long . . Angular uses fast looped dirty checking to monitor scope; when the scope changes, Angular re-renders the UI. As a result of this, you need to be careful of how the scope grows, and the impact this will then have on performance. There is also a recommended  limit of 2000 data-bound objects , which isn’t that difficult to hit with a large scale application, so as a developer, you have to be conscious about the way you bind data objects, and how many. Of course, one-way data binding helps avoid this issue, but as point 7 in  this article  points out, creating too many watchers is a mistake Angular developers often make. Although Angular does have in built performance optimisations, it also comes down to developer awareness to avoid some of the pitfalls of the framework. . Now, take the traditional JS application model; you need to look at what data has changed and imperatively force the DOM to change, so you can keep your view up to date. React takes a different approach. On component initialisation, the  render  method is called, generating a lightweight representation of your view to be injected into the DOM. Now each subsequent call to  render  allows React to compare what’s changed between this update and the last, making updates fast and efficient – only updating what has  actually  changed (more here:  Why React ). . As mentioned above, React will automatically decipher what’s changed and update it efficiently, each time  render  is called. React’s functional model gives us two simple, but powerful features to bolster performance; firstly, it allows us to specify if a component should be re-rendered, and secondly, when said component should be re-rendered. These two principles coupled together can lead to huge performance savings when dealing with lots of data, giving us a big tick in the  scalability  box. . Alongside its functional model, React employs a Virtual DOM; in a nutshell, it’s a virtual JavaScript implementation of the physical DOM structure. Its beauty lies in its simplicity; React captures snapshots of the DOM, when an underlying data change occurs, React re-renders the UI in a Virtual DOM, this new copy is then compared with the previous ‘snapshot’ to see what’s  actually  changed, the change is then applied to the actual DOM – almost like a patch. .    Source  . This one is tricky to do a true comparison, Angular being the opinionated framework, vs. React the small library (David vs Goliath anyone?). That aside, Angular is feature rich from the get go, and through its use of directives, it provides the likes of packaging, modules, server communication and form validation up front – what’s not to like! However, it’s not all sunshine and rainbows. Due to Angular’s opinionated nature it can prove difficult to integrate other external libraries. For example, take Bootstrap, being a well used library itself, often developers like to integrate it into their projects, to save re-inventing the wheel with regards to existing front end components. However, with  Angular this can prove challenging . On the up side, Angular provides a lot of functionality out-of-the-box, so you don’t find yourself needing too many external libraries or dependencies. The benefits of having an opinionated framework shine through when everything you need is ‘supplied in the box’. . React on the other hand naturally plays well with others. Being a small library itself, it uses Node’s module system, meaning any Node module is easily integrated, but the downside is due to React’s relatively small size, and only providing the View layer, you kind of need to integrate with a lot of other libraries to fill in the gaps – especially if you’re in need of a full MVC model. . Confucius once said,  ‘Life is really simple, but we insist on making it complicated’ . The same can be said of web development to an extent, and especially JavaScript frameworks. Learning JavaScript in 2016 can be a major headache;  How it feels to learn JavaScript in 2016  (cue the lulz). .    Source  . On a serious note, why does it have to be so challenging to both learn and implement new JS frameworks? Take Angular for instance, it’s a full blown framework, with an extensive API, leading to a steep learning curve when first getting to grips with it.  This article  nicely highlights the learning curve change between starting out with Angular and writing serious apps.  React on the other hand, is a small library, it’s JavaScript centric and is made up of components which have a  simple life cycle . . A downfall of Angular is that it tries to empower HTML, by injecting JavaScript into it. If that’s not enough, it also utilises both one and two-way binding systems, with both being different to boot! As a developer it’s important to be aware of the impacts on both performance and complexity when utilizing two-way binding, as  there can be various nuances with it . . React on the other hand keeps it simple, through its use of JSX; a pre-processor which injects XML style syntax into your JavaScript code. See the  HelloWorld  example below: . Or, with a more common ES6 approach; . As you can see from the above; React components are just code, and as components they are composed the way normal functions and objects work in the development world,  React feels like  normal  programming , not a confusing JavaScript library. React also sticks to one-way data binding, which makes life much simpler, it also ensures binding is handled in the same way in all cases, making it a lot simpler to both write and debug. . Simply put; .  To learn Angular: Learn a long list of Angular-specific syntax  .  To learn React: Learn JavaScript  . This actually flows nicely on from simplicity, the simpler the technology, surely the less of a learning curve? In a nutshell, the answer is usually yes. Looking at Angular, it’s a full blown, opinionated framework, which in turn means a lot of up front learning before you’re truly comfortable writing it. However, having said that, I’m sure people have gotten up to speed with Angular quickly, and it is slightly unfair to compare with React, as Angular is a fully fledged framework, whereas React is  just  a library. Angular comes with more baggage, more to learn, and more to get to grips with in order to become productive, whereas React benefits from just being a View UI layer, having a small library at its core and being pure JavaScript at heart. . As a result, React is very quick to get up to speed with. Yes it might feel slightly strange at first getting used to one-way data binding, or injecting HTML into JS through the use of JSX, but in reality, a web developer new to React could realistically get a basic ‘grounding’ within a week. On the other hand, React itself maybe quick to get up to speed with, so you can start writing components and building out your application fairly quickly and to a good standard, but soon enough you’ll need to start bringing in more libraries and then the learning curve increases by the difficulty of the frameworks and libraries you use. Besides their size and the ‘framework vs library’ argument, when you strip both technologies back, you get Angular’s custom syntax and forced coding style, against React’s use of JSX. . Testing of JS based framework and libraries has vastly improved over the last few years, and it’s important to choose a front end framework with testability in mind. . Angular comes with good documentation and libraries in order to test its components in isolation, which is great. With Angular being a full MVC framework, there is lots more to it than to React’s simple library, there is also more to think about when it comes to design and testing. One of the main features in Angular is dependency injection, which amongst other benefits, aims to make testing easier. Angular allows for good unit testing of business logic, which is great, however, it  encourages putting business logic into the UI , within controllers and services, which could be viewed as bad practice in relation to the   Separation of Concerns   principle.  When designing any application or API it’s important to ensure that each key functional area isn’t tightly coupled to another. . Furthermore, Angular doesn’t support the testing of browser driven events at its core, which doesn’t allow the developer to test how your HTML renders in response to interactions and events, which is a key goal of testing a front end framework. However, Angular mitigates this though the use of  Protractor , an end-to-end testing framework. The testability section in  this article  addresses some of the issues highlighted above. . On the other hand, React components contain pure functions, which in themselves are  much easier to test . Combine this with   Shallow Rendering  , and you can test components in true isolation, allowing for greater separation of concerns when unit testing. Yes, you still have to pump data in and check function hooks act as expected, but due to its simplified structure and functional component patterns, you don’t need extra scaffolding to put solid unit tests together. Another benefit is due to the strong emphasis on modularisation and small components, React naturally encourages a design which incorporates the   Single Responsibility Principle  , which also helps with testability. . On the end-to-end front, both technologies allow for good end to end testing through uses of external libraries such as WebDriver, and Protractor with Angular. React’s advantage lies in its functional component design, which leads to natural testability. It makes it a lot simpler to test and less of a headache – providing front end developers with a clean, simplistic approach to unit testing, which is an ever-growing importance as the emphasis on performant, reliable front ends increases. . We all find ourselves debugging code from time to time, but JavaScript code has always been a pain in the neck when trying to decipher what went wrong, where and why. Angular uses an event driven architecture - easy to develop, a challenge to debug! Another headache is trying to debug Angular’s directives, there’s so much wizardry going on behind the scenes, that more often than not, you find yourself debugging Angular code – abstraction anyone?! . Debugging JavaScript has normally been a challenge, but  Angular adds to this by its design . Take bindings for example; . If name is undefined, an error won’t fire and you can’t put a breakpoint in there due to the fact it’s not plain JavaScript. Furthermore, the JavaScript errors are caught up in the Angular interceptor, meaning the browser will interpret those errors as caught errors, which in turn leads to running through all the internal Angular exceptions. In addition, because the digest cycle fired the error, it could of been caused anywhere in the app – meaning it’s near impossible to track down via the debugger. . So how does React stand up to the challenge? Well due to React’s good use of abstraction, you end up debugging your own code, rather than React’s, and its Virtual DOM transformation into the physical DOM is kind of like a black box, which requires less debugging! However, there is a niggle with React; JSX transformations. When debugging React code (providing you’re using JSX, which you are, right?), because it converts JSX to plain old JavaScript, you find you’re not always debugging the code you’ve actually written, which can prove tricky. However, it’s worth noting, and a big bonus as well, is that  React fails at compile time , meaning you find errors quickly, its time-to-find vs time-to-fix is much lower as a result! .  ‘Look at the State of that!’  said many a front end developer when trawling through reams of procedural JavaScript code. However, we’re not talking about that kind of state; here we’re talking about application state, the state of what’s currently displayed and the underlying data that supports that. . Angular manages state through scope and services; what is displayed will only ever be data bound to the scope, but what is stored should be held in services. The most important part of state management is ensuring consistency! When something changes the state, can I guarantee the right things have been updated, re-rendered and restored in the View? With the introduction of Redux, the ability to  implement simplified one-way data flow  into your Angular apps has made developing predictable systems a lot easier. A problem previously with Angular apps has been the questions of ‘what’s the current state of my application’, and two-way data binding has often exacerbated the issue. With Redux, it helps you to derive the UI from the application state and also see how actions affect that state. . Looking at React on the other hand, it employs unidirectional data flow, and components re-render when state is updated. With React you also have the option to make full use of other architectural libraries such as  Flux  and  Redux , which both help wrap up state management nicely, as mentioned above with Angular. The fact that React has a lot less going on - due to only being the ‘View’ aspect of MVC, and also a small component library at heart – makes it easier to monitor and be aware of your applications state, and how updates to components and user interactions affect that state. . So, is it happily ever after for the tale of these two technologies? Both are still going strong; at my last check, Angular had 53,170 stars on Github, compared with React’s 53,395. Now you could say that with React further ahead, and with it being a much newer framework in comparison to Angular, that it’s being used more. However, although this post has been a kind of ‘face-off’, there isn’t any reason why you can’t use  React and Angular together , although I’m yet to see this in practice. . The purpose of this post was to look at the question of;  “I’m starting a new project what should I use?”  In reality, some of this comes down to personal preference, there will be those of you reading this that have been, or still are firmly on the Angular side of fence. There will also be those of you who are big React fans, either way, I hope this post has provided you with a better understanding of Angular and React from a high level, and hopefully peaked your interest enough to go and learn a new technology! . From my own perspective, I believe React has provided a refreshing take on the sometimes chaotic, ever changing JavaScript world. It’s ideal for developing performant, highly scalable, reliable front end applications. Along with its relatively short learning curve and simplistic JavaScript centric nature, whilst also encouraging and implementing functional patterns and closely encapsulated state management. . Of course with the  release of Angular 2 , it’s yet to be seen whether React will be challenged on the JavaScript front, or has the Angular bubble burst and a new JS flagship come to town. Ultimately, with JS frameworks being released at an ever increasing pace, the real test of React’s credentials will be time… ", "date": "2016-11-25T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Throwing stones at Clouds\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/cloud/throwing-stones-at-clouds/", "abstract": " Once I was a serious developer, but this year all I have been allowed to do is play with Platform as a Service (PaaS) offerings. Instead of sulking, I have embraced the concept, and I think it’s time a lot of other techies did too. . Today I have been taking a frankly delicious free lunch at the Radisson Blu in Manchester courtesy of “Mendix On Tour” – not a free lunch of course but a chance for the guys at  Mendix  to showcase some of their success stories to potential customers.  Frankly, they’re preaching to the converted – I’ve long been a Mendix fan. Any company that rewrites their entire platform in Scala to improve performance classifies as a proper tech company to me! . In very brief terms, Mendix offer a Platform as a Service to develop web front ends, business process logic plus the ability to persist to a data source. They provide a drag-and-drop development environment to allow incredibly fast development of fairly complex business applications with interfaces that will adapt to work on a variety of devices. There were numerous clients at the event, showcasing their success stories - typically a one-man development team creating a proof of concept application which was then launched by the business. . There is a cry associated with these NoCode/LowCode PaaS offerings which will send a chill to the bones of any seasoned developer. “This is so easy, even a [insert-non-technical-role-here] can do it!”  Oh help us all. All those decades and decades of refining the project deployment process, all those hours spent discovering that test-driven development really works, all those carefully crafted automated unit test suites and continuous integration builds with branching and merging and colourful alerts and dunce-hat-allocation, all those DevOps architectures with built-in monitoring points and self-documenting code and self-generating API documentation, all for nothing! Because now Lee from Marketing is going to be in charge of building the new business applications and they haven’t heard of any of these things. How long will it take them to learn? Surely at least as long as it took us computer scientists. . I exaggerate, but even at the Mendix Tour it became apparent that these issues are rearing their heads. The initial prototypes go swimmingly, but then you are left with a complex piece of business logic dangerously exposed and open to change from many fronts (i.e. any user). Mendix appeared particularly vulnerable in that the platform really expects a small development team, or even a single developer, to be working closely on the project for a short period of time. I heard many comments along the lines of, “I wish I could lock that down so people stop tampering with it”, “Nobody can touch that module, it’s mine only”, and so on. . Mendix has had version control based on  Subversion  since version 3, so all the necessary functionality is there – it seemed to me to be a question of process and an understanding of how to use versioning in a shared environment that was missing from the picture. There did seem to be an issue that versioning is only implemented within an application, so dependencies between applications could not be modelled – the kind of hurdles we crossed long ago in the Code world. Look at how far we’ve come with Java – from JARs to WARs to EARs to full-on containerisation. LowCode and NoCode products are only just starting up that slope. . To share another story –  Salesforce  offerings can often be considered low-code / no-code, even though you can Java away with Salesforce to your heart’s content. On one of my recent projects, the Salesforce team got into trouble for the very high number of defects appearing in testing that one would expect to have been picked up in Unit Testing. Turns out this was because “Lee from Marketing” was running the Salesforce team with a bunch of junior developers and he’d overlooked introducing the concept of the Unit Test to them because he’d never come across it. This is reinforcing my viewpoint that us techies need to embrace the LowCode /NoCode world before the, er, lunatics start running the asylum…. . There were a couple of consultancies at the Mendix Tour offering their services to help companies built their Mendix apps – I wondered if any of these had managed to define a process for larger-scale Mendix development, but from what I could glean in a quick conversation they mainly offered headcount with development experience rather than any further layers of process. . I’ve been looking at other PaaS offerings lately too, namely  Websphere Cast Iron , a data orchestration platform which in its “Live” form is hosted by IBM in the cloud. It sits jarringly on many an architect’s pattern to expose all your APIs externally just so that the cloud product can align them, but for the most part and if you already have cloud components, it works well. The “development” process takes a matter of minutes, and deployment is neatly wrapped in a web API. The project were so impressed with the quick turnaround of functionality we could offer that they ended up moving some functions out of their AWS development programme and into Cast Iron. Anything where a number of API calls were being made and the result sets amalgamated could be configured and deployed by our team in a matter of minutes, compared to a good couple of days for the Java equivalent running on AWS. It does seem to me that in the future, programming may well be seen as a  luxury  in many projects. It’s something we all want to do, we’d all rather build our own - even if we do base everything on existing open-source libraries and just copy and paste most of our code from Stack Exchange - but is it necessarily cheaper or better? With today’s wider range of mature PaaS options and ever-decreasing service costs, perhaps not. . Cast Iron Live has expended some effort in the area of software reuse and modular components. There is the concept of a reusable “TIP”, so if you develop a message flow (or an “orchestration”, as it’s known) that you are particularly proud of, you can package it up for reuse by your peers. Yet, Cast Iron isn’t too hot on versioning – there are two different concepts of automatic versioning within the tool and neither do quite what you need or expect. Occasionally we’d be prevented from overwriting each others’ work, occasionally we wouldn’t - we never quite figured out the logic. We struggled to manage parallel changes to our orchestrations and ended up creating our own, third method of versioning involving orchestration naming conventions and a Subversion backup, and ignoring the Cast Iron offerings altogether. . It seems to me that many Low Code and No Code offerings are reaching a level of maturity where it’s time they started looking at the Enterprise Development picture. The prototypes were a success, the new features are being used, it’s time to Robustify. But how? . It’s been interesting watching how the fact that PaaS offerings limit the available functionality affects an agile project, we’re linking to components running on  Amazon Web Services  which, being custom built on the IaaS (Infrastructure as a Service) platform, are much more flexible. When it came down to the question of securing the endpoints, our Cast Iron Live product offered very few options so we didn’t need to spend long choosing the one we would offer! In contrast, the AWS team became bamboozled with choice and because of this, overran on their deadline to secure interfaces. Hmmm. Could this be seen as a benefit of PaaS? . So, on to creating a process. With Cast Iron Live, we were able to create unit tests fairly easily because all of our orchestrations were basically proxying HTTP endpoints. We could reference a mock HTTP endpoint and use a tool like  SoapUI  or  Postman  to script the expected result for various edge case scenarios. For instance, one of our orchestrations was exposing a SOAP service functionality as RESTful JSON. For our edge cases we created a mock service that returned each of the possible SOAP faults depending on incoming query parameters, and in our Postman test “scripts” we ensured that the JSON service returned a suitably relevant HTTP response code. We could also emulate and test endpoint timeout scenarios, and mismatching Content-Type headers (for instance, somebody sends in content that they said was application/xml but actually consisted of JSON). The existence of the Cast Iron web interface allowed us to update the endpoint details programmatically so the entire unit test could be scripted. . The other handy thing about Cast Iron is the SOAP service which exposes the admin functionality. So we were able to run a kind of “test driven deployment”, whereby in a program we defined which endpoints and other configuration values we would expect to be referenced in each environment (for example, the production AWS endpoint should only be referenced from the Production Cast Iron environment, all outgoing connections should time out after 15 seconds) and then fix our deployment based on the program outcome. This was also a great opportunity to weave in writing some real code and using Java 8 streams into an otherwise boring project… . What’s the most annoying thing about the cloud? The immaturity of many products is certainly annoying. Whilst Cast Iron itself is a mature product, its Live offering certainly seems fairly new - I can’t believe there are that many people using it in anger as we didn’t get too badly shouted at by IBM when we almost brought on a Denial of Service attack by adding the decimal point in the wrong place whilst load-testing our system… and we have had numerous teething problems requiring version upgrades which, of course, we had to rely on IBM to perform. There is a feeling of helplessness attached to PaaS offerings! . Days when the network is slow are not particularly rewarding, either, especially as Cast Iron’s NetBeans-based web interface feels rather dated and prone to cache errors. . For me, it’s the vast array of restrictions that aggravates me – for instance the inability to restart anything at a process level, and the inability to see server and network log files or memory profiles, or even to set up your own test harnesses and have them access your cloud app. In the example above when we were figuring out how to unit test, we had to deploy our mock services to a cloud and make them internet-available so that Cast Iron could see them. No handy installs on localhost for us! The fact that we had to configure our orchestrations to communicate via HTTPS also extremely limited the number of testing services we could use. Here’s a summary of the pages that got us through. .  http://httpbin.org  - allows HTTPS and even has handy things like an endpoint with a configurable delay so that we could emulate timeouts, but we couldn’t connect to httpbin from Cast Iron Live via HTTPS due to some unfortunate SSL incompatibility. .  http://requestb.in/  - generating a new “bin” will provide you with a unique ID of a “bin” to send requests to. You can then view the requests and headers you have sent. .  http://www.utilities-online.info/xmltojson  - when you are converting a SOAP API into a RESTful one, this is great to help define what your JSON will look like .  http://www.freeformatter.com/xsl-transformer.html  - testing the parse of your SOAP responses .  http://www.restapitutorial.com/httpstatuscodes.html  - translate your SOAP faults to sensible HTTP codes. I love the easy layout of this site. HTTP 418 “I am a Teapot”? Who knew! .  Postman  (Google Chrome plugin) – HTTP calls with auth support. Easy to install, link it to your Google account and carry it with you everywhere, remembers what you’ve sent if you forget to save it.. .  SoapUI  from SmartBear – still my favourite SOAP message dispenser, great support for various authentication options, the free option will probably give you everything you need .  Python  – the language of choice for our team to knock up a quick script or two – simple HTTP connection interfaces, easy to set up the environment, easy to read the code, easy to set up the runtime environment on a laptop. .  Altova XMLSpy  – our team’s vote for the best XML / XSLT / XSD / WSDL editor. Pricey, but apparently worth it! . Then there’s the painful world of tickets and service agreements. For a low-level developer who is used to being able to scramble through application and server logs at will, to have to open an online ticket and wait for an SLA to kick in to find out what’s causing your issue is extremely frustrating. It also encourages a slopey-shouldered “it’s the provider’s fault” type of mentality; if you can’t see what you’ve done wrong it’s hard to take ownership for it. You can probably plot the maturity of a PaaS project by looking at the number of tickets open - at the start there will be a proliferation of user-error-type tickets, then all will go quiet, then performance testing will kick in and a small but extremely serious sub-set of tickets will appear. .   . Even the providers don’t seem to have this process right - with IBM Cast Iron, when we phoned the IBM support call centre they denied all knowledge of the product, and submitting tickets via the support portal has led to two flows of information - one logged on the ticket, and one drifting around in email chains as the reply to an email on a subject does not seem to necessarily make it back to the rest of the ticket info. . My conclusion is that PaaS offerings can’t be ignored by developers - we need to assess the total cost of ownership of PaaS apps vs build-your-own, and I think there will often be occasions where it no longer makes financial sense to build. The popularity of IaaS platforms such as AWS means that the argument around owning/managing your own hardware is no longer present in favour of the Build camp, which further evens the playing fields. . If PaaS is going to become a more common occurrence in solution architectures, we need to define a better development process. There is a powerful temptation for companies to associate technology project processes with code, rather than technical architecture - and as such, skip a lot of the important rigours when implementing a PaaS solution. Just because there’s no code doesn’t mean you can’t build a test-driven development and deployment framework, that you can’t script a lot of the required processes.  The PaaS selling points are all around speed of solution development, so vendors seldom take the time to remind you to wrap all this speed in some structure. Time to start setting up some guidelines. ", "date": "2016-07-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Unit Tests are the Best Documentation\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/unit-tests-as-documentation/", "abstract": " As a relatively new developer (I’ve only been writing code and learning for just over 3 years) this has been a bit of a revelation for me. I’m sure for most seasoned developers this is old news but it’s improved my productivity no end and it’s worth making a habit of it. . So you’ve found this new library and it looks really cool, nice syntax, good abstractions and you want to try it. But after doing their “hello world”, you want to do something more “real world” and you find you don’t know how to do what you want from the README.md. Working with Open Source projects I increasingly find myself looking at sparse documentation thinking, “I’m sure it said it could do this, but there’s not an example of it here”. So what do you do when there’s no documentation for your idea, but you know it can be done? . When in this situation, I start a process of looking through a number of places that I haven’t before. This is not just googling for the answer for hours until I find some obscure forum post of someone who’s done the same. . Some of the best documentation is on the methods you’re trying to use. Having the documentation next to the code it’s talking about  usually  means it’s up to date and relevant. However, there isn’t always extensive documentation in code so, failing that some looking through the source code is gold compared to looking at a wiki page or usage page. The source code will tell you how things are implemented. Reading method names you will be able to find out the full public API of a class whilst also understanding its internal processes. For example, recently I wanted to use  MultipartEntity  to send a multipart-form upload in Camel - an HTTP POST with a file attached. I happily added this code to IntelliJ: . However, I promptly realised this was deprecated because of IntelliJ’s strikethrough. I looked at the source of  MultipartEntity  and it told me it was deprecated in favour of MultipartEntityBuilder. So I tried to instantiate one of those: . A compile error and a check of the source for  MultipartEntityBuilder  informs me that that constructor is private, however, I can also see a static create method. So I use that and everything compiles nicely. Then I wonder if I need to add the Content-Type header and how can I get a hold of the boundary it generates randomly? Again, I go to the source of  MultipartEntityBuilder  and see that it builds the entity with a boundary and the correct Content-Type as part of the object so I can assume Camel uses that and sure enough it all works fine. . Looking at the source saved me looking for the documentation and how to use it. It also prevented me from writing unnecessary code like adding the Content-Type with my own hand-generated boundary and setting that on both the MultipartEntityBuilder and the Camel exchange. . Looking at source code is good but the real secret weapon recently has been looking at the unit tests. In my opinion, they are the best form of documentation - even better than developer written wiki pages. . Unit tests are a working example of how to use the library or framework: full,  executable  examples with all the necessary set up, tear down and dependencies specified. . Unit tests tell you how the original authors intended their code to be used. There’s usually a good way and a bad way to use library code and the people who wrote it know how to use it best. Read the unit tests and you can learn how the authors consume their own produce. . Unit tests are never out of date, they are always using the latest functionality and demonstrating all the functionality possible. On a par of unhelpfulness with no documentation is documentation that is wrong and out of date. Units tests are the one source of truth and can even be used to verify that the documentation is correct. . Lastly, if there are no unit tests for the framework you’re using then it’s a good indicator to not use it. . A good example of this is when I wanted to use the Camel Mail Component to parse incoming emails, but I had no idea how to mock an email server or how to send in-memory emails. I was quickly able to find out how the Camel authors tested their component, how they used the Java MockMail dependency and how to use the configuration correctly all just by looking at the unit tests in the camel-mail project. It saved me a huge amount of time. . It follows that our tests should be highly readable, with clear indication of the features and functionality implemented. Writing tests that are readable like documentation is a whole other subject but, for starters,  this post  talks about an excellent technique which goes a long way to producing feature-descriptive tests. . Look in the source by getting your editor to download the file . Checkout (literally, using git or other VCS) the project you’re working with . Look at the unit tests to discover all the features of the framework and how to use each and every one . This should be a normal habit of ours. Not only does it help cut the time taken to use the framework it helps us get more familiar with how others write, how others unit test and how the frameworks we use everyday are structured and created. With this depth of knowledge we are far better placed and more likely to be using the libraries correctly and efficiently just as the authors intended them to be used. ", "date": "2016-07-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Hacking on Bluemix\n", "author": ["Ant Broome"], "link": "https://capgemini.github.io/cloud/hacking-on-bluemix/", "abstract": "   . I’m currently sitting at an airport in Paris awaiting my flight back to Cardiff and thought it would be a good time to share my experience of using  IBM Bluemix . . First off some background:  Capgemini  has its own  University Campus Capgemini Les Fontaines ! It’s not mentioned enough when attracting people to our team (btw  we’re hiring! ), good food and drink - catering for all tastes. I’d come for a hackathon on DevOps and IoT with  IBM Bluemix . We assembled engineers from across the company spanning the globe, all with the intention of  winning  learning. The objective: create a mobile application using various sensor data from a bunch of Raspberry Pi’s to provide extra security for a private jet, Rihanna’s no less. We were split into teams mixing technical skillsets, nationality, age, gender, etc. to maximise the diversity and experience in each team and told we had two days, at the end of which we had to present a working solution to the representatives of a private jet manufacturer. . Onto Bluemix, it’s a vast array of services, boilerplate templates, virtual machines, databases and tools. I’ll be honest, I was a bit overwhelmed with my options initially. Personally I’ll always actively avoid proprietary software, I believe in open source and freedom from licenses. Saying that, when you get what you pay for with tools like IntelliJ or Slack,  I don’t mind. This brings me onto my first criticism.  Cloudant , an IBM noSQL database, it’s a variant of the open source  CouchDB . I’d love to know the reasoning behind not using the open source option. I spun an instance up and started feeding IoT data into it using another service  Node-RED  (I’ll get back to that). Then I had to try and get the data back out…mind blown. Gone were the SQL queries I am so used to, replaced with a JavaScript function to create a view to map-reduce the data. I just about got something working using  Stackoverflow  for CouchDB questions, but thats only fruitful when you know it’s CouchDB. Perhaps IBM could make this clearer on Bluemix e.g. powered by CouchDB. Giving kudos back to the community would be a powerful gesture and not feel like an attempt to lock you in to something vendor specific. . Criticism two: if you’re providing boilerplate code I expect high standards and good basic structure to the code. Unfortunately the ones I saw were primitive “Hello World”-type examples, which for us fell down like a house of cards as you try and bend it into some sort of order and coding standards. This is something I think should be addressed quickly and would happily contribute to. Our team started off using the mobile app builder. We toiled away for four hours, eventually abandoning it to start again writing an Android application from scratch. This is my first tip, avoid using the boilerplate for anything other than getting snippets of code or experimenting with connections to other services on Bluemix. . Criticism three: the explosion of browser tabs trying to configure anything. Every service basically has its own landing screen and then opens its interface into another tab, try using more than three services and its starts becoming irritating. Combining services together results in a series of copy/pastes of the generated credentials from one service to the next. This isn’t the case for all services, some integrate nicely but not nearly enough. . I’ve finally got to a point where I feel like I’ve got some stuff off my chest,  did I like any of it?  Yes, back to  Node-RED . It’s a browser-based flow editor using  Node.js  to wire up IoT data. The best way I can describe it is as an electronic circuit for streams of IoT data. It’s really simple, really powerful and something I’d recommend getting kids into to hack stuff with IoT (something I’ve thought more about since finding out about  devoxx4kids ). There’s a subset of the plugins available on the service but I think that’ll improve. One issue I did find with Node-RED was that occassionaly it lost my changes and the source control wasn’t integrated well, again something I think will be addressed soon. The integration with other services for this was really easy,  Twitter ,  Watson , Push notifications to name but three - and it has a host of custom functions. We used it to send push notifications when the plane detected an intruder for example and read a twitter feed using a Watson component to assess the sentiment of the tweet. The potential power of this really excites me; the ability to drag and drop to combine datasources ranging from IoT streams, social feeds, api’s, database etc. then filter for the data you need and apply cognitive behaviour from Watson opens up endless possibilities. . I also liked the DevOps pipeline service - you essentially build a delivery pipeline and an app can be built, tested and deployed even with green / blue deployments on commit. It’s a really easy way to get started and had support for  Ant ,  Maven  and  Gradle . Do I think it’ll replace  Jenkins ? No. However for small projects it’s perfect, the online code editor and git client work well and there’s even an issue tracking option. This would’ve been ideal on some of my university group assignments for example. . Getting back to the hackathon,  what have I learnt?  Training based on a challenge, throwing people together with diverse backgrounds and cultures, being well supplied in coffee and doughnuts whilst having limited sleep produces great results. Why? Because as Software Engineers we’re highly skilled, analytical, agile and have an ingrained culture of sharing. When another team found out I’d cracked querying CouchDB (I refuse to call it Cloudant) I was in a competition but still gave them ten minutes of my time. This trait could be seen across all the teams. . Something I’d love to see is an even broader mix of abilities within the teams, perhaps going so far as to include VPs onto each hackathon team. I think they’d learn a lot witnessing the DevOps process, agility and a sharing culture first hand. Who knows they could even learn some coding skills (Node-RED was very easy for newbies to pick up). .  Would I use Bluemix again?  Yes, in the right circumstance the ability to get a database, web server and deployment pipeline up and running in minutes is great. Node-RED and the IoT platform impressed me, especially given it’s a domain I’ve little experience in. I’d like to end by saying thanks to everyone who organised the hack, my fellow hackletes and the two IBM experts who had the unenviable task of assisting us. Hopefully there’ll be more events of this style. I’ve certainly learnt a lot more about  IBM Bluemix  than I would have in a standard training based event. ", "date": "2016-07-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How to write effective bug reports\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/testing/effective-bug-reports/", "abstract": " Unfortunately,  software will always have bugs , and those  defects need to be tracked . Whether you’re doing  automated or manual testing , when that testing finds a problem, you need to communicate that problem to the development team. The way to communicate it is by writing a bug report. Opening an issue, raising a ticket, whatever you want to call it - the issue tracker should be the main channel for persistent communication about any project, so it’s important that the message is clear. . Incidentally, I’ve never understood why we say “raise a ticket” - is it like raising a toast to someone? Perhaps it’s more like raising a flag. You might wonder why I’m quibbling over language here, but language matters, especially when we’re talking about bug reports. . The main output of testing is bug reports, and unfortunately far too many teams suffer from the consequences of badly written bug reports. Badly written requirements are often a problem too, but that’s another story. If bug reports are unclear, it’s easy for details to get lost along the way, or confusion to arise. Confusion takes time to clear up, and causes friction and frustration on the team. . Ideally testers and developers should be co-located, have a friendly relationship, and  see themselves as a single team who share responsibility for quality . If there’s any confusion, they can clear it up with a quick chat while looking at the issue together. However, in large companies, QA is often one of the first functions to be offshored or outsourced. Even if everyone speaks the same language, and even with recent advances in communication technologies,  face-to-face conversations are far more effective than remote communication . When you add differences in time zone, culture and language into the mix, clear and concise communication becomes even more valuable. . Incidentally, one of the lessons I’ve learned from working with offshore QA teams is the value of having at least one member of that team onshore, so that face-to-face conversations can still happen, mitigating the risk of cross-cultural miscommunication, and reducing the number of conference calls carried out in people’s second language. . In my experience, the main sources of badly written bug reports are bored, inexperienced manual testers, and clients who are in a big rush because they need to fit in UAT alongside their day job, and are annoyed because the software  should just work . In short, people who don’t appreciate the value of well-written bug reports, and don’t want to be spending time writing them anyway. . There are  lots of different issue trackers , each with their own strengths and weaknesses, but most of them share a few things in common. An issue usually contains the following components: . This should stand alone to let the reader understand and identify the issue quickly. Remember that the summary will often be viewed in a big list of other issues, so it should be meaningful and unique, to help you quickly recognize the relevant issue. . Think about the signal to noise ratio of your issue summaries. If you see a big list that’s full of similar summaries like “Cosmetic issues”, you’ll end up wasting more time looking for the right issue. . This is the main meat of the issue. It should include everything that you need to know in order to fix it, and to test that the fix is successful. For web projects, this should always include the URL where the bug happened. . Does the bug happen immediately, or does it only happen in a specific set of circumstances? . How will we know if the bug has been fixed? . What have you observed? How does it differ from the expected result? A screenshot usually helps. . Most issue trackers have at least one way of grouping issues. Invest a little time in learning how this works in your system, and then you can use it, instead of writing things like “Bug” or “Regression” in the title of a bug, or prefixing titles with words like “accessibility” or “performance”. . This reduces the amount of noise in your summaries, making them much easier to read. The other benefit of using labels is that it’s easier to query your issue list, and build specific boards or reports. . The whole point of having an issue tracker is so that you know whether things have been fixed yet. It’s really important to make sure that the status of issues reflects reality, to prevent wasted time. It’s worth investing some time in configuring your issue tracker to match your workflow. That way you can have a board   with relevant columns to give you a quick overview of how the project is doing, and where any bottlenecks are building up. Ideally you can also  integrate your issue tracker with your code review and build process to automate status updates . . Some trackers treat these as a single field, others split them up. However you record it, it’s important to remember that there’s nothing to be gained by slipping into  the political games of severity inflation . If all the issues are top priority, nothing can be prioritised, unless you go  one louder . . The aim when writing a bug report is that they will get closed quickly. As  Cem Kaner puts it , “The best tester isn’t the one who finds the most bugs or embarrasses the most programmers. The best tester is the one who gets the most bugs fixed.” Well-written bug reports are quicker for developers to fix, and quicker to re-test. Over the last few years, I’ve noticed some patterns in what makes a good bug report, and picked up some useful points from articles, particularly by  Joe Strazzere  and  Adam Zimmerman . Here are some ideas that it’s good to bear in mind when reporting defects and testing bug fixes. . Each defect should have its own issue. If you’ve observed multiple small bugs on the same page, don’t be tempted to create a single issue with a list of problems.  Similarly, don’t be tempted to let bugs evolve. If a bug report says that a button doesn’t work, and the developer makes it work but in the meantime the colour has changed, that’s a different issue. Unless your issue tracker is badly configured, it shouldn’t cost much more effort to create a new issue than it does to include both problems in the same ticket. . The more facets an issue has, the more likely it is that the developers won’t fix all of them the first time round, and you’ll end up failing the ticket again. Pretty soon, the issue history will be cluttered with comments and edits, and it’ll become hard to see what the problem actually is any more. This will slow everything down, forcing the team to spend more time looking at the issue tracker than they do looking at the product. Not only that, but issues ping-ponging between developers and testers is a sure-fire way to damage team cohesion and morale. . This isn’t the same as  assuming that they are stupid . Imagine that the person is new to the project, or the company. They don’t necessarily know what you mean when you use jargon or refer to things by names that aren’t known by the outside world. . Ideally, a ticket should stand alone, including all the information needed to reproduce the bug. For instance, rather than saying “the sign-in page”, include the URL. If there’s a complex set of configuration steps or other dependencies, link to the documentation. You do have documentation, don’t you? Incidentally, having a lot of manual steps in test cases is a warning sign that you’re probably not doing enough work on automating your deployments. . Learn about the features of your bug tracker. Being able to link tickets together is a very powerful tool. Learn the markdown format - most systems have all sorts of neat little tricks. For example, you can enormously improve readability by writing steps to reproduce the issue as an ordered list, including image thumbnails in the relevant place, or formatting error messages as quotes. . Bug reports should (almost) always have a screenshot. But they should (almost) never just be a screenshot. With just a screenshot, it isn’t necessarily obvious where the problem is. If it was that obvious, the developer probably would have spotted it before saying that the component was ready for testing. . Whatever you do, please don’t paste screenshots into a Word document before attaching it to the ticket. This just adds to the time it takes for developers and testers to figure out what the issue actually is, as they have to download and open the file before they can start reading it. If you attach files as an image (like JPEG or PNG), most issue trackers will display it as a thumbnail image. . If you’re referencing the original design, include a link to it. Ideally, your designs will have a single source of truth, so linking to the design is better than uploading another image. . Don’t make the developers play spot the difference - highlighting where the problem is can be really helpful. A lot of tools, like  JIRA Capture  and  Skitch , make it easy to annotate screenshots with circles, arrows, and text. . A ticket needs to include all the necessary information, and nothing else. Providing too much information can be almost as bad as not enough information. Writing concisely is a valuable skill, and as Raymond Carver put it, “Everything is important in a story, every word, every punctuation mark.” . Only include what’s relevant. Of course, the difficult bit is knowing what’s relevant. As with most things, the answer is “ it depends ”. . For example, if the bug appears any time you navigate to a certain page, there’s no need to include any prior steps like going to home page, navigate using the menu to page 2 then page 3. Just say “go to  http://example.com/page ”. On the other hand, if the bug only appears after a certain sequence of events, then that list of events is important. . If the issue occurs on every browser you’ve tried, don’t bother listing them all individually. . There’s no need to say please, or explain why it’s important that the defect be fixed. Don’t copy and paste whole emails, including the signature, into the issue tracker. . The issue description isn’t the place for theories about how to fix the problem - it  “should explain the problem, not your suggested solution.” . Comments are a more suitable place for notes on investigating the root cause. Some bug trackers allow for private comments, only visible to developers. There’s something to be said for not making clients read a load of technical notes, but I think I’d prefer to  encourage a culture of transparency between the team and the client . . If a defect is intermittent, try to isolate the circumstances where it can be reproduced. Change one thing at a time until you can rule things out. . Learn  how to use the browser console  so that you can include details of any JavaScript errors. Browser incognito mode can be your friend. Learn how to clear cookies and flush the browser cache. Make sure that you’ve refreshed the page if you’re re-testing something. . Before you spend time creating a ticket, check that nobody’s already done the job for you. If all new issues are funnelled through one person to be  triaged , that person will quickly be able to spot any duplicates, but ideally you’ll have access to a list of existing known defects and their status. If you’re constantly creating defects, the developers will be cursing you for the time they have to spend rejecting the ticket. . Once upon a time, I worked on a project which used an issue tracker called Rational Quality Manager. It was generally pretty terrible (or at least the version we were using was), but it did have one excellent feature - a button on each issue that allowed you to search for similar issues. Some issue trackers even automate that search on the issue creation form, presenting you with some existing issues that might match yours. . Some of my suggestions may seem pedantic, but all of them are born of the same motivation - investing a little time when creating issues, in order to save more time later on. Well-written bug reports will help to keep the team sane as things get closer to go-live, and the time savings will really add up, especially on a large project. So in the spirit of saving time, I think it’s worth summarising my points about what makes a well-written bug report: . Search before creating a ticket . One ticket per defect . Meaningful summary . Screenshot, as an image file, ideally with annotations . URL . Steps to reproduce . Expected result . Actual result . All relevant information . No irrelevant information ", "date": "2016-08-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Say Hello to KuWit\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/bots/kuwit/", "abstract": " It is not a secret that the industry is betting hard on building AI engines using the power of machine learning and big data to provide the foundation for delivering extremely personalized user experience. Platforms like  FBLearner Flow ,  Google TensorFlow , or services like  Microsoft Cortana ,  Google Machine learning  or  IBM Watson  are just a few examples. . One the many possibilities that arises is building chatbots based on  natural language processing  for changing the way humans interact with computers and reducing the friction between them when offering a service. . A great example on this line is  wit.ai . Wit is a service that helps you to build your conversational app by using Natural Language Processing (NLP) to understand and extract meaningful information out of the user input. . If you haven’t heard of it I really encourage you to give it a try. You can build a  recipe  for your bot and test it through the browser on the same site in minutes. . At Capgemini we have been experimenting with containers and building  solutions around Mesos  or  Kubernetes  for a while now. . These technologies are in a certain sense an abstraction over the underlying infrastructure. They completely change the way that humans interact with datacenters, clusters, machines and the way we deal with the orchestration of the apps. It is now more natural for us. . In accordance with this idea it would also be possible to go a bit farther and to change the way we interact with these platforms by making it easier and more natural for the human to transmit his desires. .  KuWit.io  is the AI bot who knows everything about Kubernetes. .   . It’s built on the shoulders of  wit.ai  and is becoming smarter every day by training and using Natural Language Processing. . KuWit relies on Wit for making predictions and extracting entities from natural language and then executes actions and retrieve information from external APIs in order to finally deliver a meaningful user output. . At the moment KuWit has three main functions: . Teach about Kubernetes .   . Interact with your Kubernetes cluster. .   . Follow the latest news about Kubernetes on the internet. .   . You can check it out and use it as a service on  KuWit.io  or just clone it from  https://github.com/capgemini/kuwit  and customise it and deploy it on your own platform. .       Make it smarter by training it on  wit.ai .     . Make it smarter by training it on  wit.ai . .       Add ability to communicate with slack and more third-party services.     . Add ability to communicate with slack and more third-party services. .       At the moment KuWit runs in a single Docker container. We should be able to provide a PoC with a serverless approach with something like  s3  for the static frontend +  lambda  for the backend under demand.     . At the moment KuWit runs in a single Docker container. We should be able to provide a PoC with a serverless approach with something like  s3  for the static frontend +  lambda  for the backend under demand. . If you want to know more about what we do come and see us at  Container Camp UK on September 9th  ", "date": "2016-08-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Defining a Minimum Viable Redesign\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/minimum-viable-redesign/", "abstract": " These days, it’s pretty rare that we build websites that aren’t some kind of redesign. Unless it’s a brand new company or project, the client usually has some sort of web presence already, and for one reason or another, they’ve decided to replace it with something shiny and new. . In an ideal world, the existing system has been built in a sensible way, with a sound content strategy and good  separation of concerns , so all you need to do is re-skin it. In the Drupal world, this would normally mean a new theme, or if we’re still in our dream  Zen Garden  scenario, just some new CSS. . However, the reality is usually different. In my experience, redesigns are hardly ever  just  redesigns. When a business is considering significant changes to the website like some form of re-branding or refresh, it’s also an opportunity to think about changing the content, or the information architecture, or some aspects of the website functionality. After all, if you’re spending time and money changing how your website looks, you might as well try to improve the way it works while you’re at it. . So the chances are that your redesign project will need to change more than just the theme, but if you’re unlucky, someone somewhere further back along the chain has decided that it’s ‘just a re-skinning’, and therefore it should be a trivial job, which  shouldn’t take long . In the worst case scenario,  someone has given the client the impression  that the site just needs a new coat of paint, but you’re actually inheriting some kind of nasty mess with unstable foundations that should really be fixed before you even think about changing how it looks. Incidentally, this is one reason why sales people should always consult with technical people who’ve seen under the bonnet of the system in question before agreeing prices on anything. . Even if the redesign is relatively straightforward from a technical point of view, perhaps it’s part of a wider rebranding, and there are associated campaigns whose dates are already expensively fixed, but thinking about the size of the website redesign project happened too late. . In other words, for whatever reason, it’s not unlikely that redesign projects will find themselves behind schedule, or over budget - what should you do in this situation? The received agile wisdom is that  time and resources are fixed , so you need to flex on scope. But what’s the minimum viable product for a redesign? When you’ve got an existing product, how much of it do you need to rework before you put the new design live? . This is a question that I’m currently considering from a couple of angles. In the case of  one of my personal projects , I’m upgrading an art gallery listings site from Drupal 6 to Drupal 8. The old site is the first big Drupal site I built, and is looking a little creaky in places. The design isn’t responsive, and the content editing experience leaves something to be desired. However, some of the contributed modules don’t have Drupal 8 versions yet, and I won’t have time to do the work involved to help get those modules ready, on top of the content migration, the new theme, having a full-time job and a family life, and all the rest of it. . In my day job, I’m working with a large multinational client on a set of sites where there’s no Drupal upgrade involved, but the suggested design does include some functional changes, so it isn’t just a re-theming. The difficulty here is that the client wants a broader scope of change than the timescales and budget allow. . When you’re in this situation, what can you do? As usual with  interesting questions , the answer is ‘it depends’. Processes like  impact mapping  can help you to figure out the benefits that you get from your redesign. If you’ve looked at your burndown rates, and know that you’re not going to hit the deadline, what can you drop? Is the value that you gain from your redesign worth ditching any of the features that won’t be ready? To put it another way, how many of your existing features are worth keeping? A redesign can (and should) be an opportunity for a business to look at their content strategy and consider rationalising the site. If you’ve got a section on your site that isn’t adding any value, or isn’t getting any traffic, and the development team will need to spend time making it work in the new design, perhaps that’s a candidate for the chop? . We should also consider the  Pareto principle  when we’re structuring our development work, and start by working on the things that will get us most of the way there. This fits in with an important point made by scrum, which can sometimes get forgotten about: that each sprint should deliver “a potentially shippable increment”. In this context, I would interpret this to mean that we should make sure that the site as a whole doesn’t look broken, and then we can layer on the fancy bits afterwards, similar to a progressive enhancement approach to dealing with older browsers. If you aren’t sure whether you’ll have time to get everything done, don’t spend an excessive amount of time polishing one section of the site to the detriment of basic layout and styling that will make the whole site look reasonably good. . Starting with a  style guide  can help give you a solid foundation to build upon, by enabling you to make sure that all the components on the site look presentable. You can then test those components in their real contexts. If you’ve done any kind of content audit (and somebody really should have done), you should have a good idea of the variety of pages you’ve got. At the very least, your CMS should help you to know what types of content you have, so that you can take a sample set of pages of each content type or layout type, and you’ll be able to validate that they look good enough, whatever that means in your context. . There is another option, though. You don’t have to deliver all the change at once. Can you (and should you) do a partial go-live with a redesign? Depending on how radical the redesign is, the attitudes to change and continuous delivery within your organisation and client, and the technology stack involved, it may make sense to deliver changes incrementally. In other words, put the new sections of the site live as they’re ready, and keep serving the old bits from the existing system. There may be brand consistency, user experience, and content management process reasons why you might not want to do this, but it is an option to consider, and it can work. . On one previous project, we were carrying out a simultaneous redesign and Drupal 6 to 7 upgrade, and we were able to split traffic between the old site and the new one. It made things a little bit more complicated in terms of handling user sessions, but it did give the client the freedom to decide when they thought we had enough of the new site for them to put it live. In the end, they decided that the answer was ‘almost all of it’. . So what’s the way forward? . In the case of my art gallery listings site, the redesign itself has a clear value, and with Drupal 6 being unsupported, I need to get the site onto Drupal 8 sooner rather than later. There’s definitely a point that will come fairly soon, even if I don’t get to spend as long as I’d like working on it, where the user experience will be improved by the new site, even though some of the functionality from the old site isn’t there, and isn’t likely to be ready for a while. I’m  my own client  on that project, so I’m tempted to just put the redesign live anyway. . In the case of my client, there are decisions to be made about which of the new features need to be included in the redesign. De-scoping some of the more complex changes will bring the project back into the realm of being a re-theming, the functional changes can go into subsequent releases, and hopefully we’ll hit the deadline. . A final point that I’d like to make is that we shouldn’t fall into the trap of thinking of redesigns as big-bang events that sit outside the day-to-day running of a site. Similarly, if you’re thinking about painting your house, you should also think about whether you also need to fix the roof, and when you’re going to schedule the cleaning. Once the painting is done, you’ll still be living there, and you’ll have the opportunity to do other jobs if and when you have the time, energy, and money to do so. . Along with software upgrades, redesigns should be considered as part of a business’s long-term strategy, and they should be just one part of a plan to keep making improvements through continuous delivery. ", "date": "2016-06-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "INVEST in User Stories\n", "author": ["Abhilash Nair"], "link": "https://capgemini.github.io/agile/invest-in-user-stories/", "abstract": " An INVEST-able User Story evolves through the journey of a Sprint. Let us follow this journey through the eyes of an Agile Team Member. . This post is directed at Team Members in companies planning on adopting an Agile methodology, especially for those coming from a background of a traditional Waterfall model. . After having implemented Agile in a couple of companies that adapted from a traditional Waterfall model to an Agile Scrum approach as well as initiating the methodology from scratch, I hope the following helps you to channel your time and master the art of investing in User Stories. To elucidate this further, I would like to take inspiration from a real-life scenario and track the evolution of a User Story during the journey of a Sprint and present it from the perspective of an Agile Team Member. .  The website of Business XYZ processes huge amounts of payments daily through their checkout process. The business user from the Marketing department requires an urgent “One-click Payment” feature to be implemented within the check-out process that would immensely simplify the experience of the customers to complete their order faster, thereby increasing customer delight and reducing the overall cart abandonment rate.  .  User summarises, “We need to urgently integrate a ‘One-click Payment’ facility within our Checkout process!”  . A User Story must aim to have the following facets: . User Story Template . Acceptance Criteria . Conversation . Sub-tasks . A User Story is a short and simple description of a feature (the “what”) told from the perspective of the person who desires the new capability (the “who”), usually the customer of the system (hereinafter referred to as the “customer”) [Reference:  User Stories and User Story Examples by Mike Cohn ]. . If the feature is an internal requirement, the User Story must be told from the perspective of the area of the business that is responsible for managing the efficiency and effectiveness of the customer-facing business of the system (hereinafter referred to as the “user” or “business user”). . Additionally, a User Story must clearly list the direct “benefit” or “value” that the customer or the user will enjoy (the “why”), once the User Story is successfully delivered. . Finally and most importantly, a well-written User Story must always answer the question: “Is the User Story INVEST-able?” .  I ndependent .  N egotiable .  V aluable .  E stimable .  S mall .  T estable . This INVEST mnemonic for agile software projects was created by Bill Wake in his original article,  INVEST in Good Stories, and SMART Tasks . . Based on my experience, with tools like JIRA available to keep track and record details for every User Story, the more time you spend in getting your user stories INVEST-able, the better your understanding of the requirements become and the lesser time you spend in the actual coding and testing of those stories. It avoids the team going off course and acts as a guide to help steer the team towards getting their User Story “done” or to take appropriate corrective measures early on, in the journey of the team’s Sprint. . Example template of a User Story: .  As a… &lt;”CUSTOMER” or “USER” who directly benefits from the successful delivery of this User Story&gt;  .  I want to… &lt;perform a “FEATURE” specified in the User Story&gt;  .  So that… &lt;”BENEFIT” / “VALUE” that the customer or user will enjoy on the successful delivery of this User Story&gt;  . Let us write the aforementioned user requirement, following the template of a User Story: .  As an XYZ customer ordering using the website, I want to be able to use the One-click Payment option, so that I can skip the payment pages and complete my order faster  . Is the above User Story INVEST-able? The simple answer: NO! . This User Story leaves a number of questions unanswered: .  Type of Customers:  Is this feature available to all new and existing customers? .  New Customers:  What should be the workflow for a new customer after the One-click Payment option has gone live? .  First Visit of Existing Customers without saved card details:  What should be the workflow of an existing customer who has not saved any card details and visits for the first time after the One-click Payment option has gone live? .  First Visit of Existing Customers with saved card details:  What should be the workflow of an existing customer who has already saved their card details and visits for the first time after the One-click Payment option has gone live? .  Existing Customers not opted for One-click Payment:  What should be the workflow of an existing customer who has not opted to register for the One-click Payment in their previous visit after the feature has gone live? .  Existing One-click Payment Customers:  What should be the workflow of an existing customer who has successfully registered their card for the One-click Payment in their previous visit? . Let us assume that the  Existing One-click Payment Customers  use-case received the below answers from the user: . Customers must be presented with a “Buy Now with 1-click” button on the Checkout Screen (where the Checkout Screen is any section that can act as the start of a checkout process). . Clicking on the button must skip the Payment screens and the customer must be presented with the Order Confirmation screen directly. . Based on the above answers, let us rewrite the User Story: .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the website, I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  . The above User Story is  Independent  as this feature can be developed and delivered independently  after  the  First Visit of Existing Customers  use cases have been done. . The above User Story is  Negotiable  as this feature can incorporate changes if required. . Let us look at this scenario in our case study: .  As always, the business user comes to our Product Owner (PO) once the Sprint has commenced and says, “Whoops! I just got the numbers in from my market research teams and it highlights the fact that 55% of our customers are using their mobiles and tablets (30% of those are Apple device users). Since we are already adding the feature for our web users, could we increase the scope of the delivery of this feature just a teeny-weeny bit so that this is available not only to our web but also to our Apple and Android tablet and mobile apps users as a priority? Later maybe we could make this facility available to our Windows app users since that’s not much of a priority as of right now!”  . In such a scenario, is the above User Story Negotiable? Yes! I would negotiate the User Story by breaking it down further as below: .        As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using a  DESKTOP OR LAPTOP , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster      .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using a  DESKTOP OR LAPTOP , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .        As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  APPLE TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster      .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  APPLE TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .        As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  ANDROID TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster      .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  ANDROID TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .        As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  MOBILE APP  using an  APPLE TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster      .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  MOBILE APP  using an  APPLE TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .        As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  MOBILE APP  using an  ANDROID TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster      .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  MOBILE APP  using an  ANDROID TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  . The priority of which deliverables need to be included in the current Sprint and which can be excluded can then be negotiated with the business user by the PO as a “Minimum Viable Product” (MVP). . The above User Story is obviously  Valuable  as the value / benefit has been clearly stated. . Each of the above User Story is easily  Estimable  as we have now broken it down to an independent, negotiable, valuable, small and testable MVP. . The User Story is  Small  enough to be “done” within a Sprint. . The User Story is  Testable  as it is small enough to be developed and tested within a Sprint. . I would use this section of a User Story in conjunction with the PO and a team member who has specialised in Quality Assurance (QA) and / or a team member with a QA mindset. I have seen the quality of deliverables increase immensely, when all Acceptance Criteria is written in a combination of Given-When-Then and checklist formats. . A Behaviour-Driven Development (BDD) approach follows a “Given-When-Then” format to help break the behaviour of the system down to an agreed and specific flow. According to  behaviourdriven.org , BDD relies on the use of a very specific (and small) vocabulary to minimise miscommunication and to ensure that everyone – the business, developers, testers, analysts and managers – are not only on the same page but using the same words. . Hence, using the BDD approach ensures that everyone involved with the User Story has a consistent understanding of what is expected to be delivered of the new functionality that would be built from tests that satisfy the Acceptance Criteria. . Use a Given-When-Then format for a one-to-one mapping of acceptance tests that must be met and tested against. .  Given:  An existing scenario .  When:  I perform some action .  Then:  I expect a specific result . Use a checklist format for a scenario where an acceptance test satisfies multiple Acceptance Criteria checklist items. .  After discussions with the Team Members, our PO has now negotiated with the business user that in this Sprint, the team would commit to deliver the web version of the User Story.  . So let us write Acceptance Criteria specifically for those user stories. .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using a  DESKTOP or a LAPTOP , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .   Acceptance Criteria:   .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer orders through the website and clicks on Checkout using a desktop or a laptop,  Then:  The Customer must be presented with a One-click Payment button               Checklists:  This test must pass when tested on:                      Approved versions of IE browsers             Approved versions of Chrome browsers             Approved versions of Firefox browsers             Approved versions of Safari browsers                             .  Checklists:  This test must pass when tested on:                      Approved versions of IE browsers             Approved versions of Chrome browsers             Approved versions of Firefox browsers             Approved versions of Safari browsers                   . Approved versions of IE browsers . Approved versions of Chrome browsers . Approved versions of Firefox browsers . Approved versions of Safari browsers .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer clicks on the One-click Payment button using a desktop or a laptop,  Then:  The Customer must directly be presented the Order Confirmation screen               Checklists:  This test must pass on:                      Approved versions of IE browsers             Approved versions of Chrome browsers             Approved versions of Firefox browsers             Approved versions of Safari browsers                             .  Checklists:  This test must pass on:                      Approved versions of IE browsers             Approved versions of Chrome browsers             Approved versions of Firefox browsers             Approved versions of Safari browsers                   . Approved versions of IE browsers . Approved versions of Chrome browsers . Approved versions of Firefox browsers . Approved versions of Safari browsers .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  APPLE TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .   Acceptance Criteria:   .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer orders through the website and clicks on Checkout using an Apple device,  Then:  The Customer must be presented with a One-click Payment button               Checklists:  This test must pass on:                      Approved list of Apple mobiles             Approved list of Apple tablets                             .  Checklists:  This test must pass on:                      Approved list of Apple mobiles             Approved list of Apple tablets                   . Approved list of Apple mobiles . Approved list of Apple tablets .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer clicks on the One-click Payment button using an Apple device,  Then:  The Customer must directly be presented the Order Confirmation screen               Checklists:  This test must pass on:                      Approved list of Apple mobiles             Approved list of Apple tablets                             .  Checklists:  This test must pass on:                      Approved list of Apple mobiles             Approved list of Apple tablets                   . Approved list of Apple mobiles . Approved list of Apple tablets .  As an EXISTING ONE-CLICK PAYMENT XYZ CUSTOMER ordering from the  WEB  using an  ANDROID TABLET OR MOBILE , I want to be able to CLICK ON THE ONE-CLICK PAYMENT OPTION so that I can skip the payment pages and complete my order faster  .   Acceptance Criteria:   .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer orders through the website and clicks on Checkout using an Android device,  Then:  The Customer must be presented with a One-click Payment button               Checklists:  This test must pass when tested on:                      Approved list of Android mobiles             Approved list of Android tablets                             .  Checklists:  This test must pass when tested on:                      Approved list of Android mobiles             Approved list of Android tablets                   . Approved list of Android mobiles . Approved list of Android tablets .  Given:  An EXISTING ONE-CLICK PAYMENT WEB XYZ Customer,  When:  The Customer clicks on the One-click Payment button using an Android device,  Then:  The Customer must directly be presented the Order Confirmation screen               Checklists:  This test must pass when tested on:                      Approved list of Android mobiles             Approved list of Android tablets                             .  Checklists:  This test must pass when tested on:                      Approved list of Android mobiles             Approved list of Android tablets                   . Approved list of Android mobiles . Approved list of Android tablets . This section of the User Story summarises all important and relevant email discussions, code discussions, visual representations / diagrams, screen grabs, test results, et al. that have occurred during the uncovering of the User Story and recorded either as comments and / or attachments. . I use this section to record a summary of what discussions and actions have occurred on this User Story during its journey through the Sprint. .  Continuing on with our case study, after the delivery of the User Story in the Sprint, it is time for the team’s Retrospective. Let us refer to the “Conversation” of the User Story to record the team’s Retrospective points:  .   Conversation:   . Created this User Story as a part of a change agreed by the PO with the business user after the Sprint had commenced. Team had to spend a time-boxed amount of time in re-planning and re-estimating the story. Work on this story commenced 3 days into the Sprint and delivery has been agreed to the Web platforms only. . Why invest precious time on a Conversation? . To decipher the actual “complexity” of the User Story . To empower the team or any other team working on a similar User Story to make better and more accurate estimations . To save any time that might be spent on potential rework . To act as documentation on the feature delivered . To act as a tool to take corrective measures mid-Sprint rather than render the story as not done at the end of the Sprint . One has the capability of adding sub-tasks to one’s User Story but this can easily be misused unfortunately. From my perspective, the smaller a User Story, the easier it is for the story to be INVEST-able, thereby rendering sub-tasks as unimportant in most cases! I have only used sub-tasks as a mere logical set of one-liner reminders of tasks that need to be completed for the User Story to be marked as “done”. Usually, I try and avoid sub-tasks as much as possible as it masks the complexity of the task listed and managing them within an already INVEST-able User Story ends up becoming an additional overhead. Moreover, since they are not monitored as closely as a User Story, sub-tasks tend to become complex and quite as often, root to a cascade of problems in the following sprints. I have encountered sub-tasks that should have been user stories in itself (if not “Epics”). .   Consequences:   . Noticeable long and stagnant parallel lines in the team’s Sprint Burn-up / Burn-down charts . Same user stories carried over across several Sprints . Frustration amongst the Team Members due to failure of meeting commitments Sprint after Sprint . Team losing credibility amongst stakeholders . Nervousness to commit . Unable to calculate the Velocity of the team .  Continuing on with our case study, no sub-tasks are required for this story as all potential tasks are already well-covered in the other sections of the story.  . Can you see why the below are “Horror” (user) stories? .  “Finish coding for microservice”  .  “Deploy code to QA”  .  “We need 3 additional codes to be added to the Shipping module”  .  “This is an easy JSP change - Integrate a new option in an existing dropdown”  . Successfully adopting Agile requires implementing a change in the organisation’s “culture” and this is only as successful as the overall collaborative effort of every member in the organisation investing into this change. Composing INVEST-able user stories form a strong foundation towards paving the way for a successful journey. Implementing this change and Bill Wake’s INVEST mnemonic will help remind you of the mandatory characteristics required for writing good user stories. .  Are YOUR user stories INVEST-able?  ", "date": "2016-08-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "My take on Capgemini University's recent DevOps Hackathon\n", "author": ["Gayathri Thiyagarajan"], "link": "https://capgemini.github.io/cloud/my-take-on-capgemini-university-devops-hackathon/", "abstract": " Recently, I took part in a DevOps Hackathon organised by our  Capgemini University  in Les Fontaines. It was part of the overall Talent Week program which had an underlying theme “Leading All the Way”. Twenty four Capgemini Engineers with different technology backgrounds from six countries took part. We were to come up with an innovative solution for a real-life challenge faced by one of our clients, not unlike the unfortunate event in  Nice , and implement it with a DevOps mindset using  IBM Bluemix Platform . The teams were assessed based on several criteria such as Approach, Innovation, Fitness for Purpose and Level of Engagement. . The challenge was to create a standalone, real-time, situational awareness system for a luxury corporate jet manufacturer, designed to secure the jets and their VIP passengers when they are on the ground.  The system could make use of a variety of physical and virtual information streams such as IoT sensor feeds, camera feeds and other publicly available data. The solution was then put through a real-time-simulation round which assessed the systems response to hypothetical security threats.  The overall assessment included several jury rounds at different stages of the Hackathon. Finally, each team had to pitch their solution to the stakeholders and Capgemini architects for the account and scores awarded based on the business viability of the solution. .       Platform: The solution had to be implemented on IBM Bluemix. A pre-requisite for the Hackathon was to complete online training for the platform. There were two flavours of the same thing - One from IBM and another from our internal MyLearning site. The eight hours worth of training included videos and exercises. Unfortunately, this just was not enough to give us a decent head start.  Consequently, the first day of the Hackathon was pretty much spent on playing with the platform and figuring out why our application crashed for apparently no reason. Bluemix also provides some boilerplate code, but any hope of using them to make a quick start was soon dashed.  (My colleague Ant has written about this in more depth in his post about the same hackathon -  Hacking on Bluemix .)     . Platform: The solution had to be implemented on IBM Bluemix. A pre-requisite for the Hackathon was to complete online training for the platform. There were two flavours of the same thing - One from IBM and another from our internal MyLearning site. The eight hours worth of training included videos and exercises. Unfortunately, this just was not enough to give us a decent head start.  Consequently, the first day of the Hackathon was pretty much spent on playing with the platform and figuring out why our application crashed for apparently no reason. Bluemix also provides some boilerplate code, but any hope of using them to make a quick start was soon dashed.  (My colleague Ant has written about this in more depth in his post about the same hackathon -  Hacking on Bluemix .) .       Team: We were not teamed up based on country but instead mixed up apparently randomly. (I couldn’t figure out any pattern.) At the start of the Hackathon we were all taught the different stages of forming a high-performing team based on  Bruce Tuckman’s model  - Forming, Storming, Norming, Performing. I realize now that these are so true. Especially when you have to be high-performing in a  very  short span of time. These stages have a massive impact on the outcome.     . Team: We were not teamed up based on country but instead mixed up apparently randomly. (I couldn’t figure out any pattern.) At the start of the Hackathon we were all taught the different stages of forming a high-performing team based on  Bruce Tuckman’s model  - Forming, Storming, Norming, Performing. I realize now that these are so true. Especially when you have to be high-performing in a  very  short span of time. These stages have a massive impact on the outcome. . Our solution used the Mobile App builder to develop a personalized Android app for displaying alerts and push notifications to the VIP users. . As well as the IoT sensor feeds made available, our backend consumed and analyzed a Twitter stream using  Node-RED . This is a browser based tool for quickly building applications using “nodes” through drag &amp; drop. Underneath, it uses Node.js to construct and store these flows as JSON. With Node-RED, device interfaces were quick and simple to develop. We then used the Bluemix Sentiment Analyzer to analyze the Twitter feed. I have to say, this aspect was disappointing for example, it failed to detect gunfire at an airport as a major threat. We had to add some logic on top of it to adjust the sentiment scores based on our keyword criteria.  However, this did mean that all alerts we generated were then personalized, meaning threats were detected based on a list of keywords that would be most relevant to the VIP for e.g paparazzi, military coup etc. . Our ultimate intention was to use  IBM’s Alchemy API  or Watson’s Tone Analyzer or Natural Language Analyzer to make our app more intelligent by analyzing news feeds etc. It would have been nice if we had managed to deduce a threat from the analysis output. For example, Loud noise from the sound sensor + Fire alarm + Smoke alarm = Bomb went off at the airport. . For our demo, a set of simulated security scenarios were run and our app was scored based on how many threats we effectively detected. This solution went down very well with our client for two reasons - their customers i.e. the VIPs are absolutely paranoid about security (for good reasons) and quoting the client, “Technology is more a risk than a solution for them” they said. Secondly, the jet is pretty much customized for them to be indulgent and personal, so why not a personalized app too? I know it goes against the grain to develop something that works for a single user but this solution aligns perfectly with the client’s business model and let’s face it, their customers are not your everyday Joe Bloggs! . Security is a top threat not just for this client but to almost everyone. It was amazing to develop a solution which has applications in a real-life scenario and present it to real clients based on real-time information in such a short period of time. Although my degree was on Instrumentation Engineering some ten years ago, technology has come a long way and I never imagined I would be re-visiting my old grounds this way. . Finally, I went to this Hackathon hoping someone in my team would take the leadership mantle and just tell me what to do. But I came out realizing that I am the person I was looking for and this Hackathon has helped me find skills within myself that I was not aware of before. For me this experience was more a Leadership Hackathon than a DevOps one, and a very exciting one at that. ", "date": "2016-08-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Digital Developer Meetup, Birmingham, 14th July, 2016\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/development/Digital-Developer-Meetup-Birmingham/", "abstract": " After the success of our first digital developers meetup last month, we are holding another one at the  Birmingham Custard Factory on the 14th July . . This event will give you the opportunity to find out about the innovative projects we are working on and how we are using the latest technologies to deliver them. Some of our leading developers will give a series of informal presentations covering our use of Java/JVM, Open Source, DevOps and integration technologies during the evening and there will also be the opportunity to network with them and other representatives from our team. Our recruitment team will also be on hand should you wish to find out more about the interesting technical career opportunities within Capgemini. . Beer and pizza will be provided to fuel the conversation! ", "date": "2016-07-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Controlling the state of your infrastructure\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/devops/Controlling-the-state-of-your-infrastructure/", "abstract": " It is now a necessity to be able to rapidly and easily deploy and evolve a platform where your apps will be running. In order to do this you need to manage your own compute resources in a given cloud and follow patterns using infrastructure as code. .  Terraform  exists for this purpose. It gives you a single view of your entire infrastructure by providing a common domain specific language for every cloud while using the specific APIs underneath. . Terraform keeps track of how your infrastructure evolves by using a  state  file. When working in a team environment you can make use of the  remote state  feature. This will help to avoid conflicts, delegate the outputs between teams, and create a smoother workflow. . It would look something like this: .   . However this won’t help you to safely run Terraform in parallel. Issues may occur as a result of engineers applying changes at the same time and using stale versions of the state. If this is a must for your use case  Atlas by HashiCorp  is a commercial service that allows you to run Terraform in parallel safely by handling infrastructure locking among other features. . So we had this idea of having the full workflow controlled by an on-premise solution for managing and evolving the state while safely running Terraform in parallel driven by continuous integration. The changes should be kept in a history accessible on a web UI. This would provide an immediate way to visualize and track the development of the state of any platform for a team. . The web UI is built on top of this nice  React/Redux universal example . . As Terraform is built with  Go , we were thinking on locking environments and Go has rich support for concurrency, it just seemed a natural choice for the backend. . So I started to do some research and these are some good resources that I found pretty useful, including the official docs, where you can see and run examples straight away: .  https://golang.org/pkg/sync/#example_Once  .  https://gobyexample.com/  .  https://tour.golang.org/concurrency/1  .  https://github.com/golang/go/wiki/LearnConcurrency  .  https://golang.org/doc/effective_go.html  . As my knowledge on Go is quite limited, I was looking for Go projects to help me to take off when  Otto  came to my mind. As Otto runs Terraform behind the scenes for generating infrastructure it was the boost that I needed to get started so we got cracking and we built  Terraform-Control . It may well happen to be the foundation for a usable solution in the future but has no more ambition than serving as a simple PoC at the moment. . The workflow now looks a bit like this: .   . And as we have automated the deployment of opinionated platforms on top of  Mesos  and  Kubernetes  you can see a full demo deploying and evolving the infrastructure for both clusters here: . So if you have similar necessities for using Terraform in a team environment and you like what you see, please feel free to drop a comment or  create an issue sharing your ideas . ", "date": "2016-05-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Cleaning the Camel\n", "author": ["Nick Walter"], "link": "https://capgemini.github.io/java/cleaning-the-camel/", "abstract": " You can almost smell bad code and it’s not a nice thing to behold. . It can be even worse when you have to get your hands dirty and fix the stuff. In this post I’m going to take a look at some examples of dirty coding using  Apache Camel ’s Java API. And I’ll show how I think you can improve things. I’ve chosen Camel not because I think Camel or Java are particularly bad, you can write dirty code in almost any language. The name certainly helps, not sure ‘Cleaning the Scala’ has the same ring to it, but  Apache Camel  is a good example of a fluent API and it’s a framework that I’ve been working with a lot the last couple of years at Capgemini. . They say ‘Beauty is in the eye of the beholder’ and that is no different for coding. I’ve seen so many pull request comments on my code that are merely someone’s opinion. Be it for style, formatting, the layout or the class structure. So here are my views, you definitely don’t have to agree with them (I’m hoping you do) and feel free to comment on them either way. . So fluent APIs give you the ability to construct a single line of code doing many actions. But just because you can do that, doesn’t mean you should. Take the example below of a Camel Route to complete an order: . Ok It’s an extreme example, it will compile and run but its un-readable. You can’t tell what action is being done where and by which bean. Try this: . Now we have separated the lines and correctly tabbed them and we can see the flow and which method relates to which bean. This example shows how correctly formatting routes is more than just an OCD cosmetic thing, but essential. When working with Camel or any other fluent API you need to be able to understand what the flow is, what is actually happening to the Camel Exchange as it moves through the logic. . It is a battle we all have. Do we inline that logic or extract it off to a separate variable, method or class? Take a look at the example below, here we are going to do some route switching based on a header parameter. . It’s not awful, but doesn’t read amazingly well. How about this: . So I’ve gone through 2 steps to the code to improve it. 1) Extracting the logic out into Predicate variables. and 2) Sensible naming, probably the more important point, so we can read the routing like a sentence. .  e.g. When [the] Order Type Is Basic ….  . This sort of extraction of logic from inside the route and into something named makes all the difference. Where logic is placed is sort of irrelevant as long as the name makes sense. . Choices in Camel let you do ‘if, else if, else’ logic. And as with ‘if’ statements you can nest them and spiral into a whole heap of trouble. . It’s simple to get lost with nested Choice / When / Otherwise statements in your Route, and this is only showing 2 levels. You could envisage more here, for example adding another check for a null user being returned. And if you are tabbing your lines appropriately then lines can start to go off to the right, making things even harder to read. . So what alternative pattern can we use to negate the need to nest? So you can use sub routes to move the choices out of the main flow. . So here I’ve restructured the above Route to now have 2 sub Routes, each with its own Choice / When statements.  The parent Route ‘createOrder’ now flows really nicely. You know straight away it only needs to get a users details and use that to create the new order details. And as a nice byproduct I can see some potential reuse with the sub routes. . I think Split and Aggregate are one of those powerful patterns that Apache Camel gives us. Allowing us to chew through data sets, process them in a parallel manner and merging the results back together. Using this pattern in a clean and readable manner is not easy. Here’s an example of a order reporting route: . Lets go through what it does. It consumes order files and splits by the body, so each line is processed separately. We then unmarshal into a Java object and extract the sales channel, so was it shop or online. Depending on the channel the OrderService gets some additional order line data from the DB. This is all transformed into a generic reporting format and then we aggregate depending on the sales channel, so we end up with 2 report files. . First of all what’s wrong with this? You might think it’s ok, if not a little complicated. But I think it could be improved and made more readable and maintainable. . By extracting 3 seperate routes (split, get and aggregate) from the single route it becomes a very different beast. Much easier to understand, less fragile and therefore easier to change. . As you can see I’m a big fan of splitting route logic out, either sub routes within a class or into a new route class. One of the reasons I move logic into new route classes is to benefit from having different error handling patterns. . Take this example, we are reading a message from a queue, calling an API and then writing the result to a DB. .  OrderFulfilmentRoute.class  . Lets say the ‘orderCRMService’ makes a call out via HTTP to an external API, and therefore we have the potential to have timeouts and connection issues. But the CRM system just stores the latest version of the data so we retry any failed calls, which is nice. But if the CRM element fails Camel will  restart  the route from the beginning, and we definitely don’t want to be sending multiple requests to the fulfilment service! .  OrderFulfilmentRoute.class  .  OrderPublisherRoute.class  . So moving the CRM logic into it’s own route gives us the ability to define different error handling strategies. So if the CRM element fails  only  that is retried. . I think if I had to sum up my clean Camel code message it would be to make your routes read like paragraphs in a book. With the correct separation of Route elements (tabbing) and good naming of routes and variables it will allow you to write code that is readable to another developer (and yourself) and reduce the time taken to maintain or enhance it. ", "date": "2016-06-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "We're Sponsoring ContainerSched\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/sponsoring-containersched/", "abstract": " Coinciding with our  our sponsorship of Devoxx UK  we’re also sponsoring, talking at and attending  ContainerSched 2016  at the same time! . ContainerSched UK has two days packed full of platform and container goodies, with everything from Mesos to Kubernetes, Calico to Weave and Docker to Rkt. We’re looking forward to meeting, learning and talking all things DevOps, containers and schedulers! . The Capgemini UK Platform Engineering team will be running a trade stand, manned with some of our cheeriest platform engineers. We’ll be showing off some of our latest work (we’re always keen to share what we’ve been up to), answering questions such as “what’s it like to work for the largest SI no-one has ever heard of?” but also listening to you and what you have to share with us. . In addition to the stand, our very own  Cam Parry  will be talking about “What is enough security in kubernetes and mesos”, so come and listen to what he has to say about security, authentication and authorisation in your favourite schedulers. . As a quick plug we’re also hiring. Here are just a few of the perks: . Surrounded by a growing set of talented and inspiring individuals. . The chance to work with some of the largest organisations in the world, delivering complex platforms using container based technologies. . Two weeks of training and conference attendance a year. We love sending people to SkillsMatter’s CodeNode in London. . Encouragement and support to get involved in and contribute to open-source communities and meetups in any way you can! . If you want to find out what it’s like to be an engineer @ Capgemini drop on by our booth to have a chat. ", "date": "2016-06-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Win Tickets to Devoxx UK 2016!\n", "author": ["Shana Dacres"], "link": "https://capgemini.github.io/agile/win-tickets-to-devoxx/", "abstract": " We recently announced that  we are sponsoring Devoxx UK 2016 , and so we are giving away all-inclusive 2-day passes to the conference on the 8th and 9th of June. This includes your choice of conference talks, access to all video content produced, and a chance to get insights into ground breaking new technology and tools that will inevitably make you a better developer! . Devoxx is the leading European non-profit developers conference focusing on the full spectrum of open source concerns.  Showcasing the best in technology and talent from the local development community. Its an unrivalled experience, in which developers from across the tech ecosystem come together for content, networking and inspiration. The conference provides a snapshot on future trends and technical innovation with 2000+ attendees immersed in the knowledge of over 50 tech talks and Meetups covering a series of topics including Architecture &amp; Security, Cloud, Containers &amp; Infrastructure and Mobile, IoT &amp; Embedded. . You can find out more about Devoxx and take a peek at the vast range of  content being delivered here . . Enter the contest by sending your full name to  technologyconferences.uk@capgemini.com  with the subject “Devoxx UK 2016” . Winners will be selected at random. . The entry deadline is 11am Wednesday June 1st 2016 and the winners will be announced shortly thereafter. . No purchase necessary to win. . One entry per email. . If you’ve already purchased a ticket, no worries - if you win, you can give your spare ticket to someone you really like.  . Winners will receive a tickets to attend Devoxx UK 2016 on the 8th and 9th June at the Business Design Centre in London. . We will contact the winners by email, so be sure to use your real email address! . Good luck! ", "date": "2016-05-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Kubeform: Kubernetes clusters in any cloud\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/kubeform/", "abstract": " Today we are introducing  Kubeform : A tool for provisioning production ready  Kubernetes  clusters to any cloud with security, scalability and maintainability in mind. We’ve had this project open-source for a little while but have been a tad slow to get a blog post out! . As of today, Kubeform comes with support for  AWS ,  DigitalOcean  and local clusters via  Docker Compose . We plan on adding support for  Google Compute Engine  and  Microsoft Azure  in the very near future. . Kubeform leverages  Terraform ,  Ansible  and  CoreOS  as the basic building blocks for your Kubernetes clusters. We’ve been using these technologies in combination successfully for a while and this builds on some work we’ve already done with our sister project  Apollo  around  Apache Mesos . . Our approach (although we didn’t discover the community were looking at this until recently) falls in line with some of the thinking around  a proposed v2 for kubernetes deployments . . Out of the box we configure Kubernetes in HA mode with 3 master API servers by default using Podmaster for leader election and a configurable number of worker nodes (which can be configured via a terraform variable). We also provide “edge router nodes” (again configurable) used for ingress load balancing. . The AWS setup closely follows the  CoreOS guide for Kubernetes on AWS  with all elements secured with  TLS certificates using tf_tls . . We set up the edge router nodes with  Traefik  as an ingress controller by default. We are also looking to support  Nginx  and alternative solutions like  Vamp  for richer A-B testing/canary releasing. . SkyDNS is enabled by default and the  Kubernetes Dashboard  project is turned on as well, allowing an operator to view the state of the cluster through a nice web UI. . We have additional support for  Helm  which can be enabled to provide the  Deis Workflow  by default. . Please give it a spin and let us know if you have any feedback. We have  documentation on Github  which includes getting started guides for various providers. . We plan on improving the cloud support and adding more features in the very near future. We’re looking at things such as increased integration with Deis and Helm, Authentication support via  Dex , Storage support for  Torus ,  integration with Kubernetes network policy APIs  and  multi-datacenter cluster federation through Ubernetes .  Check out the issue queue  and  roadmap  to see what’s coming and feel free to pitch in with any ideas. . For more information, and to get up and running please see  our Github repository . Feel free to get in touch or open an issue if you run into trouble. ", "date": "2016-06-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "London Calling - Devoxx UK, 2016\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/development/Devoxx-2016-report/", "abstract": "   . UPDATE (21/07/2016): the Devoxx folks published the video of the sessions, so we linked them in. . We came to  Devoxx UK 2016  because we wanted to hire people. (A LOT of people.) But we also wanted to let the community from whom we take so much that we exist, and to give back to them by participating in this grand knowledge-share which is one of the things which make being a software developer such an amazing thing. . We weren’t disappointed. .   . Eleven of us attended in all - representing the full #diversityOfExperience and #diversityOfOpinion that we have in our UK Java team.  Between us we had innumerable conversations, attended almost all of the sessions available, and handed out (and gathered in return) a LOT of schwag. Some of us caught up with old friends, and others of us made new ones.  All of us learned a great deal.  Some of the things we particularly wanted to give a shout out to are the following: .  The Silver Bullet Syndrome  – Hadi Hadidi - ( video ) .  Just Enough App Server with Wildfly Swarm  – Antonio Gonclaves - ( video ) .  DevOps is about People Too  – Daniel Bryant - ( video ) .  Modular Monoliths  – Simon Brown - ( video ) .  Java EE 8 functional opportunities  - David Blevins - ( video ) .  The JVM and Docker, a good idea?  - Christopher Batey - ( video ) . But it wasn’t just a training event - we were there to stand alongside household-name companies (OK, developer household) and let every attendee know what it’s like to work for  the largest consultancy they’d never heard of , and hopefully convince a good number of them to join us. .   . The fact that we had awesome T-shirts, a signed  #EisnerNominated   book  giveaway, and free pick-and-mix didn’t hurt either. .   .   . The T-shirts were all gone by midway through day two, the small size having run out before 10AM on day one.  But these were more than just cool freebies.  We put them together to celebrate the 200th Anniversary of  Ada Lovelace , the first computer programmer, colleague of  Charles Babbage  (and daughter of  Lord Byron ) and as such they were a living embodiment of who we in the Java team are and how we think.  It was nice to have this recognised when one of the keynote speakers ( @mnowster ) told  Sarah  as she manned the stall that we were the only people properly promoting diversity.  Since then we’ve had many requests from public figures in the world of Java development asking for one.  We’re already looking into our next print run… .   . So why do we love Devoxx UK so?  I’ve thought about this a good deal since I first attended in 2014 and now I know why. It’s because it’s hand-made; and I mean that in the best sense of the word - in the classic punk “don’t wait for someone to give you permission, just do it and see what happens” sense. Its the same way we run our team, and you could see it all over - from  the rabble-rousing anti-corporate opening keynotes  (trolling of 419’ers; being honest about failure as well as the industry’s dark underside; and our personal responsibilities as makers in this second great internet age), to the tweet-pic wall to the Quickie and Ignite sessions, to the booths staffed by real-life, hands on developers, ready to chat about your specific problem  in depth , to the University and Lab sessions, to the diversity-minded staging and planet-aware pick-your-own flyer distribution.  This year it was even explicitly echoed in the theme; Devoxx Calling indeed… . So will we be back?  I certainly hope so, because the Devoxx mindset is our team’s mindset - underneath the cold, blue, corporate exterior, we’re a growing collective of free-thinking, always-learning, diversity-seeking, sharing-loving, unlike-minded always-shipping developers, just like you.  Yes, you.  Come and  join us ! . P.S. oh, and we spoke too.  Please let us know what you think about both our sessions via twitter, the comments below, or even face-to-face next time you see us: .  Harnessing Domain Driven Design for Distributed Systems  –  Gayathri Thiuyagarajan  and  Andrew Harmel-Law  ( slides )( video ) .  NFR Testing and Tuning: A Scientific Approach  –  Kevin Rudland  and  Andrew Harmel-Law  ( slides )( video ) . P.P.S. if you missed it, or if you were there and want to remember what it was like, there is  a brilliant photoset on Flickr  you can browse. ", "date": "2016-06-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Unit Testing Timer Based Code\n", "author": ["John Nash"], "link": "https://capgemini.github.io/development/testing-timers/", "abstract": " Code that relies on timers is tricky to unit test. You have to check the functionality of your code and ensure there is the correct synchronisation between the test and the timer thread. . Unless special measures are taken, exceptions thrown from production code will be swallowed by the timer thread and when this happens the unit test fails with an ambiguous failure (usually a time-out or failed mock expectation). . You can be left in a situation where you have to choose between conservatively long time out values (that make your tests run slowly) or faster unreliable tests that have occasional failures. . Recently I’ve adopted a technique for dealing with these issues, by splitting the functional responsibilities of a class from its timing, so I can test them in isolation. . As an example, imagine you’ve got a  HouseKeeper  class that removes all the old transactions from a system. It uses a timer to trigger this behaviour every ten minutes. It might look something like this in Java 8: . How do you go about testing this? You certainly don’t want your tests to have to wait 10 minutes each time they start the  HouseKeeper , before checking if the class removed any transactions. . You could make the timer duration configurable so that the unit test can reduce the timer period to something much shorter. . Now you can write a unit test that reduces the timer period to something more manageable and waits long enough for it to trigger. . This works, but it’s fragile because its based on timing. External factors like the load on the PC will affect when exactly the timer triggers and when the test fails it will be hard to see the cause of the failure (did the code fail or did we not wait long enough). . In addition, the wait slows the test suite down. You can probably tolerate a one second pause in the tests, but as you add more tests that rely on the  HouseKeeper  those pauses will soon add up. . Instead of these slow or unreliable tests you could do something different. Let’s create a  Clock  interface that you can use to represent different types of timers. . You now update the  HouseKeeper  class to use an instance of this interface that is passed to its constructor. . The rest of the production code will supply the  HouseKeeper  class with an implementation of this interface that works just like before. . Now you can test the  HouseKeeper  class using a new implementation of  Clock  that lets you control when the time period elapses. . Here’s the overall picture. .   . The unit test now looks like this. . The  Thread.sleep  has gone so there’s no waiting around and the test is fast. More importantly, there’s no timer thread being run so if the test fails it must be because the functionality is wrong. Finally, any exceptions thrown from the  HouseKeeper  will be visible to the unit test, making debugging errors easier. . Of course, you’ll still want to exercise the timer set up code inside your integration and stress tests, but you can go forwards with confidence in the code’s core functionality. ", "date": "2016-06-17T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Transparency of Things\n", "author": ["Cam Parry"], "link": "https://capgemini.github.io/devops/transparency-of-things/", "abstract": " So this blog post will mainly cover what dashboards can bring to transparency, which is at the core of what most IT projects are about these days, unlocking some feature set in the business or exposing a data set previously hidden away. What dashboards do is bring this transparency about what your customers are doing online in real time. I will also cover the transparency in work environments and how it relates to agile. It is also worth noting that transparency is one of the “three pillars of empirical process control” mentioned by  Scrum . . The lightbulb moment for me was a couple of years ago, when I built a dashboard in a couple of weeks with  Statsd ,  Graphite  and  Grafana  for a client before go live. All I was doing was sending some metrics to graphite, like how long third party service calls were taking, how long different parts of the end user journey were taking and trying to relate that to business terms, so how much money was coming in, comparing journeys to different parts of the day and previous days. . As soon as metrics started to appear other developers and architects started asking how do we add different metrics. The power was already kicking in, people saw what was happening in the system and they wanted to know more. The client CIO even started asking questions when certain metrics went red and how do we fix that. This was another lightbulb moment in how to get funding for poorly implemented parts of a system that were affecting the bottom line. The dashboard even caught an issue with a third party system before the third party integrator noticed, because we were transparent about every part of the user journey. This was effectively doing real time data analytics before it was cool. We have now used this same pattern in some other client projects and bid work. Clients love this, as it adds so much value and custom dashboards can be adjusted or built for different business owners depending on what they care about. . This is invaluable to the customer and this same pattern can be invaluable in different aspects of a project like giving the customer access to reports on project costs, story points or issues closed in a sprint. Which brings me to the next example of transparency in the work environment. Invariably if you are doing development these days, you will be having daily standups, weekly reviews and client demos. All of these activities are strong examples of openness or transparency in the work environment that come about due to agile processes, but being open in the workplace also brings a lot better culture. You openly communicate with colleagues and find out each others strengths and weaknesses, which can lead to better development or just a better work environment. . Another prime example that is becoming prolific these days is open source, which one of the core principles is transparency. Groups of people getting together or just one person, to show others how cool something is or how something works in a given technology or hardware. Last year we open-sourced a piece of software, this was another piece of transparency in which we tried to document everything that we were trying to achieve and all our decisions made to date. We also have tried our best to make full guides to getting started in an effort to provide the full information required for collaboration, cooperation and collective decision making, which is the very definition of transparency. Here is the full blog post about  Apollo . . Another example came up recently, where the client wanted a better view of how the project was tracking, what costs they were using for their hosting, how many deployments were happening and what artifact versions were being used. We were tracking this information on different parts of their wiki and we had set up different operational dashboards in our cloud provider, but the client wanted one place they could check on frequently. I was surprised this service wasn’t offered by many cloud providers, a more holistic view of what is happening for the service. So I knocked up a dashboard in  Dashing , and plugged a few metrics in, as a starting point e.g. service availability, incidents, jira tickets open, hosting costs per environment, number of build and deployments and version deployed to each environment for each application. There will definitely be more metrics, but I figure you need a base point to start the conversation around what the client really wants us to be transparent about. . If we show we are transparent, people tend to be more open and honest and the trust relationship can really be built from a strong base. We often get greater collaboration and cooperation from ongoing transparency in projects and in our personal lives. All the examples above and patterns can be applied to our personal lives with great success. ", "date": "2016-04-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How Not To Lead A Team\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/how-not-to-lead/", "abstract": " In  my previous article about good lead developers , I said that there was another story, about some of the bad leaders I’ve worked with. Well, it’s time to tell that story. . Initially I was reluctant to focus on the negatives, but it’s just as instructive to look at what has gone wrong in the past, and hopefully we can learn from the mistakes of others. Here are a few of the leadership anti-patterns I’ve seen. . Knowledge is power, and some misguided leaders try to hold on to their power by keeping things hidden from the team or the client. Perhaps their project is behind schedule, and they try to save face by covering that fact up through dishonesty, misdirection, or being economical with the truth. This is almost never a good idea. As  Mark Twain suggested , the truth is much easier to remember, and openness is easier to manage. . I once had a very awkward weekend in the office during a particularly stressful UAT period, being asked difficult questions by a client who had been kept in the dark by the project manager about the state of the project. Because I wasn’t sure what the official position was, I had to walk a diplomatic tightrope between annoying the client and dropping my manager in it. It’s much healthier to build  a culture of transparency , and develop longer-term relationships (with both clients and colleagues) based on trust. . Sometimes information should be shared on a need-to-know basis. Don’t drag everyone and anyone into endless meetings, and remember that a stand-up should be short. Don’t waste everyone’s time by going into minute detail on every little issue. . Don’t forget  how expensive meetings are , especially for people who work on  the maker’s schedule  - do you really need to talk through that document, or can you just send it to people and ask them to read it and get back to you? Don’t underestimate the value of a proper  responsibility matrix , and always think about whether information is relevant or valuable before you clutter up someone’s inbox with it. . You don’t always need to get consensus, and  you don’t always need a meeting . For instance, why bother with a conference call to give a status update when the status should be available to everyone at any time by looking at your issue tracker? It can be really valuable to spend a bit of time setting up some dashboards and making sure that everyone knows where they are and understands them. . Sooner or later, things will go wrong. They might go wrong in a big way or a small way, but they will go wrong. When they do, it’s important  not to turn the problem into a big drama . Even if the problem is a big one, flapping around and talking about escalations is generally not the most efficient way of solving it. . The leader sets the tone, not just for the team but for the client. Panic is infectious, and can inhibit the whole team from making good decisions or performing to the best of their ability. If the leader stays calm, they can project a healthy sense of calm and defuse most situations by helping to calm other people down. . Far too many times, I’ve been part of a development team who have found themselves committed to delivering something by an unrealistic deadline over which they had no influence. All too often,  delivery dates seem to have been decided on a whim  before projects have been properly estimated, and bear little or no correlation with the project scope. At the very least, before making any promises on behalf of the team, you should check (and ideally document) your assumptions regarding feasibility, availability, and capability. . Building an effective relationship with clients isn’t just about saying yes all the time. Quite apart from the question of what the client  wants  and what the client  needs , sometimes you need to say no. It’s been said that “ design is much more about saying ‘no’ than it is about saying ‘yes’. Even when it’s painful.  Especially  when it’s painful. ” I’d say the same is true of most client relationships. Being able to say no is what protects you from scope creep and unrealistic expectations, and it can help you to  under-promise and over-deliver . The tricky part is when you need to say no to someone who’s already said yes to someone else. Especially in large organisations, there’s often a chain of stakeholders and a chain of promises made. Saying no is difficult, especially when the relationship is new or unstable - you may need to earn the respect that means people will listen when you say no, and sometimes the only way to earn that respect is to say yes and slog through the consequences. . I once worked with a manager who had a nasty habit of leaning over a developer’s shoulder and telling him which lines of code to change. Thankfully he never tried doing that with me, probably because my body language made it pretty clear that I wouldn’t tolerate backseat driving. . Designers suffer from this more than developers, and most of them have got horror stories about clients who want to  push pixels back and forth . That tends to be less of a problem for developers, probably  because of the more specialised subject matter , but it can happen. Your team members have been hired because they (hopefully) know what they’re doing - you need to trust them to get on with their jobs. . We’ve probably all had the experience of hitting send on an email, and immediately noticing something wrong with the message we’ve just sent. So many times I’ve seen someone desperate to get something off their desk, and by rushing their response they cause extra confusion, setting off a vicious circle of more emails. It can be a problem for everyone, but it’s especially harmful when it comes from the person who’s supposed to be setting the tone for the team, and even more so when you’re in the planning stages of a project. . The best way to prevent that problem is to take the time to read emails carefully and make sure you understand them before starting your reply, and to slow down and read your own message through before sending it. . Some people I’ve worked with have taken a perverse pride in the number of unread emails in their inbox. It’s not healthy, and it’s generally unnecessary. It isn’t difficult to set up rules to get rid of the automated notifications. . If you’re going to be unable to get back to people for a while, let them know. Whether that’s setting up an out-of-office reply, or just a Slack message to tell people not to expect an answer straight away, it’s helpful to set expectations. . Incidentally, it may seem obvious, but I’ve worked with a surprising number of leaders who absent themselves from their team’s preferred communication channel. If your team is using Slack, you should be using Slack. You may want to mute some of the channels so that you’re not driven mad by the noise, but you should at least be checking in from time to time to make sure you don’t miss anything important. . Especially if you’re in charge, don’t let yourself become a bottleneck for the project. . Sometimes people don’t respond because they’ve got too much on their plate. Perhaps they don’t trust their colleagues enough to do the job right, or perhaps they see themselves as some kind of hero. Whatever the reason, it’s not a good idea. They become a single point of failure, and their junior colleagues never get the opportunity to learn. . Some leaders seem to have sloping shoulders, with an almost pathological unwillingness to take responsibility for anything. To me, this doesn’t really count as leadership - sooner or later you need stand up and be counted, even if it’s just to justify your salary. . Creating a ‘them and us’ mentality is harmful. When walls are constructed between participants in a project, whether that’s between vendor and client, developers and testers, or particular individuals, something is lost. The most effective projects I’ve worked on have been the ones where everyone involved shares a common goal, and feels  invested in the project’s success . . I once worked with a project manager who had the remarkable knack of persuading the client that all the problems were the fault of another team. It made us popular with the client for a while, but it also made our colleagues unpopular with them, which made us unpopular with our colleagues. Eventually, and inevitably, it came back to bite us. . A team shouldn’t work  under  a leader, they should work  with  them. There’s probably no quicker way to make people resent you than to behave as if you consider yourself more important than them, or your time more valuable than theirs. It’s unlikely that you’ll get the best out of people if they don’t respect you, and it’s unlikely that people will respect you if they think that you don’t respect them. Every member of the team deserves to be treated with respect, no matter what their pay grade. . One key element in treating people with respect is listening to them. As has been previously noted, there’s a reason we have  two ears and one mouth . Don’t talk over people, and don’t treat meetings as your opportunity to tell the team how things are. If your team has concerns, pay attention. If they’re struggling with something, remember that  what seems trivial to you may not be trivial to others . . If there’s dirty work to be done, don’t dump it on the juniors. If the team ends up needing to work on a weekend (and  this should almost never happen ), it shouldn’t just be the developers and testers who get lumbered with it. Even if all he or she is doing is making tea and ordering pizza, the manager should be there. After all, we’re all in this together… . One project manager I worked with put great store in “setting yourself up for success”. It’s an admirable idea, but for him, in practice it meant making sure that the early sprints were full of easy targets that the team would hit without problems, with the intention of keeping the client happy. The trouble was that there were dragons lurking in the long grass of the later sprints, and we all knew it. . “Failing to plan is the same as planning to fail” may be a cliché, but there’s a lot of truth in it - without sufficient preparation, you’re doomed to be perpetually reacting to events, lurching from one crisis to the next, and always playing catch-up. . Without wanting to get into re-hashing the benefits of agile over waterfall, it seems fairly uncontroversial to say that requirements change, and when they do, the plan will need to change. If nothing can happen until you’ve amended your Gantt chart, the team is in trouble. . Writing this, I’ve noticed a couple of common threads running through a lot of these points. The first is short-sightedness. When a leader is only focussed on their immediate goals, and the problems that are officially their responsibility, it’s likely that they’ll fail to see potential problems. The second is lack of respect for colleagues and clients. So maybe this can be summed up by saying that leaders need to slow down and think a bit more about how their behaviour affects other people. ", "date": "2016-04-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Scala Appetizer for Java Devs\n", "author": ["Amir Aryanpour"], "link": "https://capgemini.github.io/scala/scala-appetizer-for-java-devs/", "abstract": " I have been a Java developer for a long time. That goes beyond the time when EJBs or even Struts were introduced. In my madness I decided to start my PhD a few years back. As part of my research I wanted to design a Domain Specific Language (DSL) for security policy languages from the ground up, and due to the limitations of Java (at that time) I was forced to look at other options.  At this time I was introduced to Scala. . My journey from Java to Scala was not only far from smooth - it was a bumpy ride almost all the time. Having said that, I have to admit I’ve enjoyed every moment of it. So I decided to share my knowledge with the hope to help those of you who would like to have a smoother journey from other universes to Scala. In order to achieve that in a series of short/medium length blog posts (as opposed to a very long one) I am going to provide you with my experiences. So here we go: . I do remember when I converted to Java. I bought a book and started to learn. Everything made sense: if, while, do, methods, objects etc. But why is this not the case for Scala or any other functional programming language? In my opinion the answer is that functional programming languages are hard(er) to grasp especially if you have been an imperative language programmer for a long time. . Let’s pick up an example here. Functional programming languages (including Scala) are less verbose compared to imperative languages. This usually is labelled as an advantage of these languages by their developers. Well, that is true, however that also implies the code will be harder to read, especially for a novice. Those impatient individuals who would love to start functional programming by looking at  code will probably struggle to learn the language in this way. . Another example can be presented as the concept of “state” in imperative programming languages.  State plays a great role in our day-to-day life (as Java developers). We create (sometimes) complicated code to deal with the state of the data and present it fresh in our code at different tiers. Now imagine a world that is stateless, I mean no state at all! Certainly it will be a simpler world but it will definitely also be hard to grasp and live in such a world whilst you code your applications the other way all your life. It can be compared to a great photographer who has been taken lots of great natural pictures and now he has been asked to shoot ONLY in Black-&amp;-White! . Irrespective of starting point, you will realise functional programming is hard(er) to grasp and harder to live in  (OK, at least to the point that you feel comfortable with it). Inevitably at some point you will ask yourself the questions “why do I have to go through this pain?” and “what is the justification beyond all these efforts?” Undoubtedly if you do not have concrete answers to these questions, most likely either you never finish the journey or you never feel and love the greatness of functional programming. In my case, I was lucky because the limitations of Java (at the time) forced me to stay on the track. Who knows what would have happened if I wanted to start my PhD course today? I reckon the result might have been different. . To summarise the section, set your mindset, set your goals, be prepared for challenges, have determination, do not give up and rest assured good days will come. . As it has been detailed in the previous section, you need to be prepared for challenges but I would say be prepared for surprises - in a good way - before that. Use these so called surprises as your encouragement to keep learning and ultimately enjoy coding Scala. I could write a book about the greatness of Scala and perhaps to motivate you to start learning this language but I probably will rewrite a Scala textbook here; so let us avoid that! I also would like to go a bit deeper than the very basics, as I am sure you already know about them. Things like forgetting about the semicolon, or that immutable objects are preferred in Scala, or that you can write the import command anywhere in your code!  So let us explore a few tricks that might motivate you to start your journey sooner rather than later. .        Values (and Methods) are Objects         Scala has the same data types as Java, but unlike Java all the values in Scala are objects that includes numerical values. That is not a huge difference compared to Java however in Scala methods are also Objects! This could be considered as a massive advantage as methods (that are objects) can be sent around.        Let us look at an example:             def abstractStringMethod(s: String, f: String =&gt; String) = if (s.length &gt; 0) f(s) else s   abstractStringMethod (s1, _.trim())   abstractStringMethod (s2, _.toLowerCase())                As it appears from the above the AbstractStringMethod accepts a method as a parameter. Nice isn’t it? Treating everything as Object (especially methods) becomes incredibly handy when you try to abstract away the complexity of some operations in your code. Scala made the abstraction much easier compared to other languages.        Having said that, it has been noted that this feature of Scala often has been misused. That could lead to  Blinkered Abstraction  where abstraction puts a blinker on developers’ thinking and they try to fit the entire world into their abstraction. This phenomenon is perfectly described by Martin Fowler in his book  Domain-Specific Languages .     .  Values (and Methods) are Objects  . Scala has the same data types as Java, but unlike Java all the values in Scala are objects that includes numerical values. That is not a huge difference compared to Java however in Scala methods are also Objects! This could be considered as a massive advantage as methods (that are objects) can be sent around. . Let us look at an example: . As it appears from the above the AbstractStringMethod accepts a method as a parameter. Nice isn’t it? Treating everything as Object (especially methods) becomes incredibly handy when you try to abstract away the complexity of some operations in your code. Scala made the abstraction much easier compared to other languages. . Having said that, it has been noted that this feature of Scala often has been misused. That could lead to  Blinkered Abstraction  where abstraction puts a blinker on developers’ thinking and they try to fit the entire world into their abstraction. This phenomenon is perfectly described by Martin Fowler in his book  Domain-Specific Languages . .        Operators are also Methods         I would also reckon this comes as a surprise to Java developers that Scala does not come with operators (that includes arithmetic operators like  + ). All these lookalike operators are treated as methods under the covers.  This also implies all these arithmetic operators can be overloaded. Methods come with no restriction/limitation in their names. The other useful feature that can be explored here is that  any  method with one parameter can use infix syntax.        So based on what we have discussed so far:             1.+(1) // is equal to 1+1                 Although all these can be described as exciting features, a question could be raised as to where these features come into play? Well, the one that I am able to point you at is designing DSLs. Assume that you have been asked to design a DSL for testing purposes so testers can code their test scripts using the DSL. Then a line of these scripts could look like this:             DirectFlights from London to Manchester at 12:30                The above human-readable sentence can be a valid Scala code as follow:             Val numberOfFlights=DirectFlights.from(London).to(Manchester).at(12:30)             .  Operators are also Methods  . I would also reckon this comes as a surprise to Java developers that Scala does not come with operators (that includes arithmetic operators like  + ). All these lookalike operators are treated as methods under the covers.  This also implies all these arithmetic operators can be overloaded. Methods come with no restriction/limitation in their names. The other useful feature that can be explored here is that  any  method with one parameter can use infix syntax. . So based on what we have discussed so far: . Although all these can be described as exciting features, a question could be raised as to where these features come into play? Well, the one that I am able to point you at is designing DSLs. Assume that you have been asked to design a DSL for testing purposes so testers can code their test scripts using the DSL. Then a line of these scripts could look like this: . The above human-readable sentence can be a valid Scala code as follow: .        Love the Case Classes         We (as Java developers) love Data Access Objecs and Data Transfer Objects. If that is the case for you most probably you will also like  case classes  that are perfect for such a purpose and many other scenarios. The definition of case classes is easy:             case class fun (Foo : String , Bar : String)  { \tdef  someMoreFn = ....    }\t                When you create a  case class  , Scala also creates a  toString  ,  hashCode ,  equals , constructor field(s)  extractor  and  copy  methods on the fly that come very handy specially when you are performing some pattern matching. You  are not even obliged to use the  new  keyword to create case classes. Also taking the fact into account that  ==  in Scala always delegate to  equals  then you are safe and sound when you use  ==  against case classes:             case class Author(lastname: String, firstname: String)    val a = Author (“Aryanpour” , “Amir”)   val a1 = Author (“Harmel-Law” , “Andrew”)   val a2 = Author (“Harmel-Law” , “Andrew”)    a==a1   // results false    a1==a2  // results true                 Nice and neat isn’t it? Be truthful, how many times you have encountered that mistake in your Java code?        Most probably you already know how powerful Scala is in pattern matching. And by pattern matching I meant defining some useful function like:             def convertToYesNo(choice: Int): String = choice match { \tcase 1 =&gt; \"yes\" \tcase 0 =&gt; \"no\" \tcase _ =&gt; \"error\"   }                The above function receives an  Int  (choice) and based on the value received, it converts the input to  Yes  or  No  (or even error if it cannot match it). Nice and useful. Now with the case classes you can do the same on an instance of class:             sealed trait MyAbstract[+T]   case class  FirstSubClass[T](value: T) extends MyAbstract[T]   case class  SecondSubClass[T](value: T) extends MyAbstract[T]    def decision[T](value: MyAbstract[T]): Unit = value match { \tcase FirstSubClass(v) =&gt;  // do some stuff \tcase SecondSubClass(v) =&gt;  // do other stuff   }                Do not be distracted by  sealed ,  trait  and all other unknown keywords. Here we have two case classes that both extend the base class (MyAbstract). The  decision  method however can perform pattern matching based on class type. Wonderful!     .  Love the Case Classes  . We (as Java developers) love Data Access Objecs and Data Transfer Objects. If that is the case for you most probably you will also like  case classes  that are perfect for such a purpose and many other scenarios. The definition of case classes is easy: . When you create a  case class  , Scala also creates a  toString  ,  hashCode ,  equals , constructor field(s)  extractor  and  copy  methods on the fly that come very handy specially when you are performing some pattern matching. You  are not even obliged to use the  new  keyword to create case classes. Also taking the fact into account that  ==  in Scala always delegate to  equals  then you are safe and sound when you use  ==  against case classes: . Nice and neat isn’t it? Be truthful, how many times you have encountered that mistake in your Java code? . Most probably you already know how powerful Scala is in pattern matching. And by pattern matching I meant defining some useful function like: . The above function receives an  Int  (choice) and based on the value received, it converts the input to  Yes  or  No  (or even error if it cannot match it). Nice and useful. Now with the case classes you can do the same on an instance of class: . Do not be distracted by  sealed ,  trait  and all other unknown keywords. Here we have two case classes that both extend the base class (MyAbstract). The  decision  method however can perform pattern matching based on class type. Wonderful! .        Collections, Collections and Collections         I would say one of the features you (as a Java developer) are going to love when you convert to Scala is the enhancement they have made on iterable objects.  The enhancement can be categorised as follows:                          They are  neat . On average each iterable object has in the region of 30 methods. These methods make the vocabulary efficient, short and precise. Forget all those endless iteration, wrapping exercises that we go through in Java. Whatever you would require is made available for you.                           They are  fast  and  responsive . The majority of methods (especially search and pattern matching ones) are optimized, hence you will see the difference when you actually use them in your application.                           They are  concise . Thus whilst they are in use they significantly reduce the number of unnecessary loops and lines of code that usually written in other languages to achieve the very same goal. You might also combine a few methods to go even further.                val (minors, adults) = people partition (_.age &lt; 18)                                  As an example, the above line of code takes a collection as input (people) and partitions them into to sub-collections (minors and adults) based on the criteria that has been defined: age &lt; 18.                They are  safe  and  universal . The same operations are provided across range of iterable objects. For example, if we consider the String object as a sequence of chars, the operations that are available on arrays also available on String object. How cool is that? In addition to that collection methods inputs and outputs are explicitly and statically type checked. This means compare to other languages majority of error that you unintentionally introduce during implementation are caught at compile-time whilst you write your code in Scala.           .  Collections, Collections and Collections  . I would say one of the features you (as a Java developer) are going to love when you convert to Scala is the enhancement they have made on iterable objects.  The enhancement can be categorised as follows: .           They are  neat . On average each iterable object has in the region of 30 methods. These methods make the vocabulary efficient, short and precise. Forget all those endless iteration, wrapping exercises that we go through in Java. Whatever you would require is made available for you.         . They are  neat . On average each iterable object has in the region of 30 methods. These methods make the vocabulary efficient, short and precise. Forget all those endless iteration, wrapping exercises that we go through in Java. Whatever you would require is made available for you. .           They are  fast  and  responsive . The majority of methods (especially search and pattern matching ones) are optimized, hence you will see the difference when you actually use them in your application.         . They are  fast  and  responsive . The majority of methods (especially search and pattern matching ones) are optimized, hence you will see the difference when you actually use them in your application. .           They are  concise . Thus whilst they are in use they significantly reduce the number of unnecessary loops and lines of code that usually written in other languages to achieve the very same goal. You might also combine a few methods to go even further.                val (minors, adults) = people partition (_.age &lt; 18)                     . They are  concise . Thus whilst they are in use they significantly reduce the number of unnecessary loops and lines of code that usually written in other languages to achieve the very same goal. You might also combine a few methods to go even further. . As an example, the above line of code takes a collection as input (people) and partitions them into to sub-collections (minors and adults) based on the criteria that has been defined: age &lt; 18. . They are  safe  and  universal . The same operations are provided across range of iterable objects. For example, if we consider the String object as a sequence of chars, the operations that are available on arrays also available on String object. How cool is that? In addition to that collection methods inputs and outputs are explicitly and statically type checked. This means compare to other languages majority of error that you unintentionally introduce during implementation are caught at compile-time whilst you write your code in Scala. . I would like to keep going but I would say this is just about enough for the first blog post. I will continue on other posts, but before I close, I’d like to highlight one more thing. . We (us developers, and not just Java ones now) love “Hello World” kind of exercises that teach us new languages, features etc. But we all know that is not enough. We need to get our hands dirty on real projects with what we have learned. We need to see how other developers approach the very same problem that we are about to solve. We need to see what can be done to make the code more reliable, enhanced and defect-less. Unfortunately none of the above can be achieved if you are not working on a real project but fortunately there are ways to address this challenge. . Contribution to an open source community can be described as a win-win situation. There are wide ranges of open source projects available that you can contribute to. Usually contribution starts with contribution to documentation, which leads to reading the code and documenting it accordingly. As your confidence and experience grows you might be able to move to bug fixing or even implementing some features for the project. That would be a great starting point for almost everyone. . The other way of course would be to join multi dimensional organisations like Capgemini. Within Capgemini we have wide range of projects that suit almost everybody’s tastes and appetites. You will certainly enjoy the diversity of projects that we have within Capgemini. If you are eager to learn more, challenge yourself, work shoulder-to-shoulder with very famous individuals who constantly shape the IT industry and more importantly being paid at the same time, why not join us and achieve all in one go.  We are hiring now.  ", "date": "2016-04-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Adventures in Blockchain\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/blockchain/adventures-in-blockchain/", "abstract": " I’ve recently been engaged with the  Capgemini Applied Innovation Exchange  (AIE) where I’ve been experimenting and working with emerging technologies in conjunction with our business and our clients. . As part of that we’ve been looking to leverage blockchain technology for a number of potential use cases. The ideas led on to developing a proof of concept application to explore blockchain around identity management and smart contracts. . If you’re thinking about dabbling with blockchain and you are unsure if you actually need the benefits of a distributed ledger or not, chances are you probably don’t need it! I found this as a simple, great starting reference to question the use case: .   . [Source:  https://twitter.com/FDorleans/status/680692738275282944/photo/1 ] . For our use case we settled on building our application around the open-source  Ethereum blockchain application platform . . Choosing a blockchain platform is no easy task; there are several out there. I won’t go into too many details or comparisons (perhaps one for a future blog post) but our main reasons behind choosing Ethereum were: . It’s open source with a strong community behind it . Good support for Smart Contracts via  Solidity  (a programming language for smart contracts) . The APIs are pretty well documented and there are a range of API  clients in different languages  . There are a bunch of tools, compilers, libraries etc. already out there to help with development . After settling on an underlying blockchain platform, now what? Where do we go from here? . Since it was a proof of concept build of a web application we thought it would make sense to use  Meteor  as the main application development framework. This gives us a full-stack JavaScript framework and out of the box support for MongoDB as well as access to a huge number of plugin modules to get going quickly with things like OAuth authentication via  https://atmospherejs.com/ . . There are also a couple of Meteor boilerplate examples for Ethereum out there that we could learn from (See  meteor-dapp-boilerplate ,  meteor-dapp-wallet , and  meteor-boilerplate . . I’ve also been using a lot of  ReactJS  lately. Not being a huge fan of Meteor’s default templating option Blaze, I decided I wanted to use React primarily for the view layer in combination with  Redux . . Again, there’s some work in the community that we could borrow for a Meteor + React boilerplate ( meteor-webpack-react ). At the time of development React support was in preview for Meteor, since the release of Meteor 1.3 it has been integrated fully, see  https://www.meteor.com/tutorials/react/creating-an-app . . The entire application used the following - . Ethereum as the underlying Blockchain platform / infrasturcture . Meteor for frontend/backend code as the main development framework . ReactJS + Redux for the view layer and actions .  React Bootstrap  for theming .  Webpack  as a build system .  Karma  +  Mocha  for testing . In addition I added the following Meteor packages (from atmosphere.js) especially for Ethereum: .  silentcicero:solc  - Contract compilation for Solidity contract files .  ethereum:web3  - Exposes the  web3.js  API to the Meteor app .  ethereum:accounts  - Synchronises the Ethereum accounts with a collection in Meteor, persisted in browser local storage . And for local Solidity contract compilation and testing we used  Truffle . . Finally, for development purposes - the application was integrated with  testRPC  to spin up a single Ethereum node (pre-populated with accounts + Ether) on our machine to connect the web application to Ethereum via the  JSON RPC APIs . . As a result we were successfully able to build an Ethereum blockchain application and learn a great deal along the way. . As is often the benefit of hindsight if I were to start another blockchain project today I’d probably approach it slightly differently. Here’s a bunch of thoughts, tips and lessons learned which hopefully might help anyone trying to get started: .       If you embark on a project, start at the smart contract level and map out your domain model, relationships, events and get your smart contract design as good as possible from the start. Make sure the contract maps properly on to the real business problem. If you have to change all your contracts down the line, its possible but you could end up have to re-architect your entire application as a result.     . If you embark on a project, start at the smart contract level and map out your domain model, relationships, events and get your smart contract design as good as possible from the start. Make sure the contract maps properly on to the real business problem. If you have to change all your contracts down the line, its possible but you could end up have to re-architect your entire application as a result. .       Be prepared for some hiccups and allow yourself time to deal with them. If you are using some of the open-source code and libraries, don’t expect them all to be bug free. Contribute back to help improve those projects if you can (e.g. Here’s a  couple  of  pull requests  I had to open along the way to fix issues I ran into).     . Be prepared for some hiccups and allow yourself time to deal with them. If you are using some of the open-source code and libraries, don’t expect them all to be bug free. Contribute back to help improve those projects if you can (e.g. Here’s a  couple  of  pull requests  I had to open along the way to fix issues I ran into). .       Be ready to update your dependencies at pace. Related to #2 - A lot of the open-source libraries are moving very fast and new features and fixes are being merged every day. If you’re running into issues it’s always worth checking if it has been fixed upstream.     . Be ready to update your dependencies at pace. Related to #2 - A lot of the open-source libraries are moving very fast and new features and fixes are being merged every day. If you’re running into issues it’s always worth checking if it has been fixed upstream. .       Do your homework on which blockchain platform you want to use. If you want a public or private blockchain, permissioned or permissionless. Consider using cloud-based solutions if that fits your needs (for example  Azure blockchain as a service , or something like  IBM Blockchain ).     . Do your homework on which blockchain platform you want to use. If you want a public or private blockchain, permissioned or permissionless. Consider using cloud-based solutions if that fits your needs (for example  Azure blockchain as a service , or something like  IBM Blockchain ). .       You’ll need to deal with eventual consistency and distributed systems problems (since transactions are not necessarily mined immediately on the blockchain). This is tricky to deal with in applications. Be prepared to make heavy use of  Promises  if you are building a JS based application. If you have a chain of transactions that depend on each other things get hard.  ether-pudding  looks like a promising solution in this space to simplify things a bit.     . You’ll need to deal with eventual consistency and distributed systems problems (since transactions are not necessarily mined immediately on the blockchain). This is tricky to deal with in applications. Be prepared to make heavy use of  Promises  if you are building a JS based application. If you have a chain of transactions that depend on each other things get hard.  ether-pudding  looks like a promising solution in this space to simplify things a bit. .       Don’t be fooled into thinking that because you are building a blockchain application you won’t need a database. You probably will need a database (we used a hybrid of storing data in the blockchain itself and in MongoDB). 90% of the time you’re probably going to need to store stuff off-chain as well.     . Don’t be fooled into thinking that because you are building a blockchain application you won’t need a database. You probably will need a database (we used a hybrid of storing data in the blockchain itself and in MongoDB). 90% of the time you’re probably going to need to store stuff off-chain as well. .       Related to #6 - Decide exactly what you are going to store on the blockchain (In Ethereum there’s no fixed block size limit, but larger payloads require more gas, and there is a  gas limit ) versus what you are going to store in your database. Map out your data model and flow accordingly.     . Related to #6 - Decide exactly what you are going to store on the blockchain (In Ethereum there’s no fixed block size limit, but larger payloads require more gas, and there is a  gas limit ) versus what you are going to store in your database. Map out your data model and flow accordingly. .       Storing documents on the blockchain (integrated with technology like  IPFS  is still incredibly young and not that easy. There are some specific cloud-based solutions out there attempting to solve a similar problem (e.g.  Factom  and  Storj ) and some promising early work being done with  Swarm  in the Ethereum community.     . Storing documents on the blockchain (integrated with technology like  IPFS  is still incredibly young and not that easy. There are some specific cloud-based solutions out there attempting to solve a similar problem (e.g.  Factom  and  Storj ) and some promising early work being done with  Swarm  in the Ethereum community. .       Having to deal with the proof-of-work model and gas limits as well as having enough Ether in accounts to send transactions can get painful at times. There’s some interesting community work being done to switch from proof-of-work to proof-of-stake ( see here for a comparison of the two ) for Ethereum, going by the name  Casper .     . Having to deal with the proof-of-work model and gas limits as well as having enough Ether in accounts to send transactions can get painful at times. There’s some interesting community work being done to switch from proof-of-work to proof-of-stake ( see here for a comparison of the two ) for Ethereum, going by the name  Casper . .       I agree with  @ChrisLundkvist  when he says:     . I agree with  @ChrisLundkvist  when he says: . Making #blockchain usable for normal people is an unsolved UX problem. End-user key management with great UX is biggest challenge in blockchain . Be prepared to deal with some difficult UX/DX around wallets and accounts and the general technology. . If I was to embark on a project like this again, I would probably start out with a framework more closely aligned to smart contract development, like  Truffle  and then build out from there. At the time of development Truffle was only at 0.3.0, and was fairly rigid in its approach to a JavaScript build pipeline. It has since hit 1.0.0 and contains  a lot more features  and flexibility for integrating different build tools. Truffle also has good integration for  ether-pudding . Combining Truffle and ether-pudding would potentially have saved me some pain I encountered trying to integrate different tools from different development houses. . Lastly, I’d just like to emphasise, if you’re planning to use distributed ledger technology, make sure you’re using it for the right reasons. Make sure the business case is sound and the technology plays to the advantage of the use case you are looking at. . I had fun (and a little pain) along the way but stay tuned for hopefully more posts and adventures in blockchain. It’s really interesting to see what the community is going to come up with next in this fast moving space. ", "date": "2016-05-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Musings on estimation\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/agile/on-estimating/", "abstract": " Thinking about writing a blog post about estimating but not sure how long it will take. . I knew when I posted that tweet that I wanted to write a blog post about estimating. After all, I’ve  written on the topic before , but wasn’t sure exactly what form the post would take or what message I really wanted to get across. What I did know was that estimation is an important theme, and that I could create some value for both myself and others by putting my thoughts on a page. . Seeking inspiration, I looked back at a sample of past blog posts I’ve written, measuring word count of posts and looking at time from first commit to when the pull request to publish was opened: . Looking at the data, it appears I can write a couple of hundred words a day. I am likely to turn out a blog post in just over a week. In other words, if you asked me to write a blog post, I could think about blog posts I’ve written before and tell you I’ll take about a week to do it. I’ll mostly meet expectations, but will disappoint you 20% of the time. . Simple, right? If you come back in a week’s time, there should be a fully formed blog post ready for your reading pleasure. . Well… not exactly. There’s some pretty big variances in the details of even that small data set. Let’s explore. . The data includes one post, written in a day, which was very short and was heavily influenced by a press release which we’d released through another channel. This meant that the amount of thought that had to go into the content was limited because someone else had already done the hard work for me. I could just tweak it a little bit for my intended audience and away we go. Excluding that post lifts the averages to over 10 days for a post, and raises the average word count to about 1200. .  This means I’m reducing my average effort by about 20% if I incorporate re-use of work that’s done before.  . Let’s look at the other end of the scale. My post on  Drupal Integration Patterns  was longest in both word count and duration to write. That’s because it was a topic I was wrestling with, and I wasn’t sure how I wanted to get the message across. In writing it, I was referring to a lot of other content and inputs. It was also (relatively) original thought. My perspective on something.  I needed to stop and consider what I really did think about the topic, which obviously adds to the effort.  It was also written at a time I had a lot of other things going on. Excluding this post drops my days per post to about 5, without substantially altering the average word count. .  This means I’m roughly 60% less efficient if I’m not focused on the task, or if I’m creating something I’ve not done before.  . Even looking at two posts that should be very similar, “ Reflections of Symfony Live London ” and “ The Lead Developer Conference ”, both about experiences at conferences, we can see quite significant differences in productivity. I’ll be honest: .  I have no idea why one took longer than the other - or why I was 460% more efficient writing one post than the other.  . Of course, the other factor in all of this is that I’m only estimating the things that I’m in control of. I can control the topic I’m writing about. I can decide when it’s done. I’m responsible for organising my time and focusing on getting it done. . What I’m not looking at is the uncontrollable factors. Our publishing process is to open a pull request on our blog Github repository. Other authors then review the posts, make suggestions, ask for clarifications, highlight grammatical errors or places the content doesn’t flow. We go through this cycle of review and amend until the post is in a publishable state, agreed by at least two of the reviewers. This keeps quality high (and shout out time - they all do an amazing job) but does mean that the review cycle can vary from a couple of hours to… longer… depending on who is around and available to provide feedback. . Then we have to wait for a publishing spot. We try not to overload our readers with too many posts in a short space of time, and will often have a small queue of posts ready to go just to give us a buffer in case we have a period of low inspiration (touch wood, not so far!). This can mean there’s a few items ahead waiting to be published. For time critical posts, we might jump the queue - if something is related to a specific event or hot topic. . Finally, there’s the act of publishing. Fortunately, we use  Jekyll  and  Github Pages , which means our publishing process is very simple. Carry out a final preview of the branch locally, merge the pull request and wait a minute or so whilst the magic happens. However, this isn’t a zero time activity, and we have a smaller number of people with permissions to publish new posts, so this is another factor that could influence how long the new blog post is ready. . So, back to the original question - did you want to know how long it would take to get my post in draft form, or how long it would take to be published? . I imagine a number of people will be wondering what I’m waffling on about. Fortunately, I do have a point. Everything we have looked at so far has direct parallels to the question we all get on a regular basis - “How long is this piece of work likely to take?”. Hopefully, we can draw some lessons from this: . In agile practices, this would be your definition of done. What exactly are you being asked to estimate, and what will it look like when it is done? The time to produce some working code in your local development environment can be vastly different to the time taken to get through code review, testing cycles, user acceptance and deployment. Make sure you know what it is you’re being asked to estimate and what the person asking is expecting. . We talked about cycles outside your control - like getting peers to look at your code, arranging users to test it. Even once these are in place, you’re losing control of the duration. At best you can time box the activity, but you can’t guarantee what is going to be found. Testing might uncover one bug or one hundred, just like I’ve had blog posts published with an “all OK” as often as I’ve written terribly and had loads of comments to address. . In reality, your best bet for predictability is  keeping items small  and automating as much as possible. Why wait for someone to point out ten spelling mistakes when you could run a spell-checker? In the same way, automate your code checks - whether through unit tests, code style checks or test environment deployments.  The smaller the item under inspection, the less likely something will crop up that will delay things , but if it does then the queue of other things can carry on past it in a steady flow. . The other factor is that developers tend to only estimate for the bit that they think of as “actual dev work” and forget about including time for meetings, documentation, testing, and all the general bits and pieces around the project. The pieces that tend to turn “it’ll take 2 hours” into a full day task, or longer. All of these are  forms of waste , all of which can extend the duration of our work, and influence the time to get to “done”. . Remember to think in terms of both effort (or time spent on the activity) and elapsed time that it’s likely to take to complete that effort. . We saw, in the blog post examples, evidence of being quicker at things where “prior art” already exists, or things we’ve done before. We also saw that doing something for the first time takes longer. Finally, we observed that  sometimes there is no explaining why some things take longer than others even when they appear similar.  We’ve all had that bug that crops up in the middle of something routine that sends us down a rabbit hole and expends valuable time. . In software engineering, we often come across all three scenarios and think we can get a feel for how long things can take. However, don’t get too comfortable with this, as focusing on the high level “it’s just another blog post” can quickly make us over confident. Focus on the details, the nuances of this particular scenario, to make sure you challenge yourself to think about how long it will take. . Finally, we see that looking back at past projects to feed your estimates can be misleading. It only takes one hugely successful project (or one truly disastrous one) to skew your numbers. The numbers alone also don’t take into account the circumstances around them. Did the project take place over a holiday season? Were the team working on multiple things at once? Was the problem domain completely new to the team? . Every blog post, like every project, every feature, every block of code is different. There are similarities, and opportunities for us to learn from what we’ve done before, but making sure we focus on what we’re doing now can we build an accurate picture of what we need to do, and create an appropriate estimate. . Of course, that leads on to the question of  whether we should even bother to estimate at all , but I’ll leave that for another time… ", "date": "2016-05-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "We're Sponsoring Devoxx\n", "author": ["Matt Davidson"], "link": "https://capgemini.github.io/agile/were-heading-to-devoxx/", "abstract": " Following on from our sponsorship of  Voxxed Days Bristol 2016  and  Women of Silicon Roundabout , we’re continuing to put our money where our mouth is and sponsoring yet another major UK tech conference. In the UK JVM team, we care about open source, so much so that we’re taking loads of our team to  Devoxx UK ; and it’s only one month away! . Devoxx UK is the London flavour of the largest Java Community conference series. It features world class speakers keynoting in such topics as cutting edge JVM development, cloud based systems, and which JavaScript framework might last until Devoxx 2017. This year Devoxx UK is on the 8th to 10th of June, hosted at the Business Design Centre in Islington, London. Over the course of the three days, attendees can choose from five parallel conference sessions, the Hackergarten, open-table Birds of a Feather sessions and rapid lunchtime talks.  Oh, and on the Friday there are in-depth Hands-On Labs. . On top of all this however, it’s perhaps the impromptu conversations which are the best part of these events.  We’re hoping to have loads of them.  The crowd at Devoxx is always incredibly diverse – diverse in background and diverse in opinion - and that is its strength.  We know that software is built by people, for people, and by widening the circle of influences and influencers, everyone benefits. . Consequently we, the Capgemini JVM team will be running a trade stand, manned with some of our cheeriest 1x developers. We’ll be showing off some of our latest work (we’re always keen to share what we’ve been up to), answering questions such as “what’s it like to work for the largest SI no-one has ever heard of?” but also listening to you and what you have to share with us. Rumour has it we’ll even be giving out free pick and mix to anyone taking the time out to come visit! (Bring your own breadcrumbs if you want to find your way back to the talks.) . In addition to the stall, a number of our team members are also taking part as speakers.  Gayathri Thiyagarajan  and  Andrew Harmel-Law  will be presenting  “Harnessing Domain Driven Design for Distributed Systems” .  Kevin Rudland  and Andrew (again) will be sharing their experiences on  “Performance Testing Distributed Systems for the Masses – A Scientific Approach” . I think I’m beginning to spot a trend… . Personally, this is my first major UK conference. I’m really looking forward to catching some of the keynotes in person, rather than on Youtube. I’m also relishing the chance to catch up with friends and colleagues. In Capgemini, we work in distributed teams (mostly) throughout the UK, I’m looking forward to putting names to slack aliases, but also meeting loads of new people throughout the three days. . Don’t forget we’re hiring (see the ad below this post). Here are just a few of the perks: . Surrounded by a growing set of talented and inspiring individuals. . Flexible working time and environment. . Two weeks of training and conference attendance a year. We love sending people to SkillsMatter’s CodeNode in London. . The best tools - we let developers choose Windows, OSX (and in the future Ubuntu – we’re working hard on it) for their corporate machine. We provide tricked out Macbook Pros to our developers if you want one.  We also provide IntelliJ IDEA licences for everyone who’s broken away from Eclipse. ", "date": "2016-05-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The ramblings of an open source newbie\n", "author": ["Gareth Sullivan"], "link": "https://capgemini.github.io/open%20source/experiences-of-open-source/", "abstract": " As with most developers, I’ve used open source software.  As with most developers I expect to find this free, well documented, easy to use software within a few clicks and I expect it to perform and to deliver the advertised functionality. . Having never contributed back to any open source projects, I was lured into helping with the newly created  spring-boot-capgemini  project that would provide some useful common spring boot starters.  With Spring (and I still don’t know how this happened) being very new to me, it seemed a useful and effective way for me to put some of the theory into practice. . I’m no writer, so some bullet points with some of my thoughts, experiences and perhaps some prompts for discussion will have to suffice. . Joining the project opened up multiple lines of communication via github issue tracking, emails, &amp;  slack . . I missed the face to face contact and the quick “can you just look at this over my shoulder, it’s bound to be something stupidly obvious”.  Slack was bamboozling me with too much detail - commits, builds and IMs notification were popping up continuously. . I suggested a few changes to minimise the noise, but this resulted in a longer than expected discussion and debate, which again I felt could have just been sorted quickly had we sat down and discussed. . I struggled when I got stuck with things - this wasn’t like my day job, I couldn’t tap up the many experts available to me.  The usual suspects (Google, StackOverflow) were useful to a point, but i did spend some time staring at slack, waiting for one of the other guys to come on line! . As previously mentioned, being new to Spring and Spring Boot I expected to struggle - but there were far more hurdles in the way than I expected. . Gradle? Where’s Maven?  Github? Where’s SVN? Jenkins….where are you? . I see this as a plus - being dragged out of your comfort zone and learning new technologies because you need to.  You have some context and specific goals, so you are more driven and focused to get past specific and relevant hurdles, rather than picking up something new, getting “Hello World!” out of the way and then wondering where to go now. . I took on the task of integrating the repo with a build system.  I realised how spoiled I was on my day to day project work, with Jenkins a click away and someone else to fix and maintain it. . Finding a solution compatible with our private repo was a challenge, but  circleci.com  was chosen,\tand again, configuring it, picking up YAML and getting it all working seamlessly, was down to me. . On the flip side, you’re not bogged down with someone else’s choices - your project can have what you like as long as you sort it out and maintain it, although I imagine on an open source project of any serious scale, democracy and/or mob rule and even a  dictatorship  decides the technology and infrastructure. . Checkstyle issues breaking the build! I wasn’t expecting that, but at its inception it was the right time to make decisions like this. . No time or cost considerations, no imminent deadlines - why not strive for the highest quality software? I have heard horror stories of simple pull requests turning into scathing attacks on people’s work, warring factions and even submitters not toeing the line being banned! . This  story , seemingly one of many examples of the Linux guru Linus Torvalds\t throwing his toys out of the pram, are testament to the “keyboard warrior” culture present in some open source environments. . Every line of submitted code being reviewed, every checkin requiring a test - this is how it should be done. Does this discourage contributors, or does it just weed out the bad/lazy ones? . “The code is the best documentation” - a quote I’ve heard several times, but in open source, surely the README.md file is the most important file in the repo? This is the project’s marketing, sales, and support, all in a file which will get rapidly scanned and form the basis of an opinion or decision - is this worth looking at let alone forking? Is it a serious contender compared to the competition? Is this project still even active and how well will it be supported if I choose to use it? . The spring-boot-capgemini repo started off strong, enthusiasm was high and activity was sustained and productive. It tailed off however, day to day account work is always higher priority, and I wonder perhaps this was inevitable? . When someone is paying for software, expecting features on a certain date, there is a drive to deliver, deadlines to hit, functionality to provide. On larger, more established open source projects I imagine this is the same, but how does a small open source project get to that point? . In conclusion, I think I have ended up with more questions than answers. . The more I write the more I realise that the spring-boot-capgemini project is too small, too early in its lifecycle on which to base anything than a first impression of open source. I don’t feel I can make any firm opinions or judgements until I have looked into and/or contributed to a number of open source products, and that goes beyond just pulling in the jar as a dependency. . That’s the great bit - the transparency, clue’s in the name, “open” source ! I can find how the libraries I use day to day are written, tested, documented, versioned, and released. I don’t need to be an active contributor, until I feel ready, and I choose what I contribute. . I’m sure in time I’ll look back on this as a bit of a naive blog entry, and hopefully come back with some answers, backed up with experience. ", "date": "2016-05-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "SVG Icon Workflow for Jekyll with Gulp\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/frontend/jekyll-svg-icons/", "abstract": " The engineering blog is a great opportunity for us to make decisions without those pesky clients getting in the way. For one thing, the audience is other developers, which means that the browser profile of our visitors is very different to our normal work with large corporations. In other words, we don’t need to support IE8. . When the blog started, we wanted to just get something up and running quickly, so we chose Jekyll, and found the  Notepad theme  which uses Foundation and Font Awesome for a nice clean blog theme. As time has gone on, we’ve made a few tweaks to the appearance and performance, and user experience enhancements, which we’re putting into  our fork of Notepad  as we get round to it. . One tweak which I’ve been meaning to investigate for a while is SVG icons, ever since I heard  Chris Coyier talking about SVG at Smashing Conference . I’ve been pleasantly surprised by how easy it was to implement within our existing Jekyll and Gulp setup. . There’s an excellent write-up of the  pros and cons of various icon systems by Damon Bauer , and there’s a good  summary of the benefits of SVG compared to icon fonts on CSS Tricks , but something that it doesn’t mention much is performance. Icon fonts can be quite inefficient, especially if you’re only using a few glyphs from the font. The Font Awesome font itself is 82kb, which may not seem too bad, but this site only uses 10 glyphs - combined into a single SVG, the file is only 4kb. . On top of that, Font Awesome needs styles. In our case, we were minifying and including the styles for Font Awesome within our main stylesheet, so it was only adding one HTTP request, but removing the styles from Font Awesome reduced the size of our stylesheet by 20kb (before gzip). . There are plenty of good tutorials out there already, not least by  Chris Coyier  and  Charles Peters . This setup is mainly lifted straight from those tutorials - the only original work here is making it work in Jekyll. . The first step was to get the images - luckily there’s a repo with  the Font Awesome glyphs as separate SVG and PNG files . Put the ones you want in a directory in your Jekyll install - I’ve created  ./src/assets/icons/  for this. Within your gulpfile, set up an  icons  task, which uses  gulp-svgmin ,  gulp-svgstore  and  gulp-cheerio . The README for gulp-svgstore is very helpful, and even proposes a PNG fallback (although we haven’t used it), as well as the main gulp task: . We’re sending the output to the  _includes  directory of our Jekyll theme, and then we include the icons inline at the top of the page - in our case, in the  header.html  include: . Then, it’s easy to use the icons inline, using the symbol IDs generated from the file names of the source SVGs. For instance, here’s the markup for our Search button: . Then you can style the icons however you see fit - in our case, it’s fairly minimal - here’s the whole of our  _icons.scss  partial: . Finally, just to keep your production site tidy, exclude your source icons from being included in the Jekyll build, by adding it to the exclude variable in your  _config.yml  - here’s our full exclude list: . The effect of this has been to reduce overall weight of our home page (running locally without gzip) from 558kb in 26 HTTP requests to 450kb in 20 HTTP requests. Given how simple this site looks, there’s definitely still room for improvement - even though we’re minifying and concatenating everything, our stylesheet weighs in at 143kb and our JavaScript is 121kb. Next on my hit-list is thinking about whether we can get rid of jQuery, and stripping out the unnecessary bits of CSS and JavaScript that are included from Foundation. ", "date": "2016-04-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Building private Ethereum networks with Docker Compose\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/blockchain/ethereum-docker-compose/", "abstract": " In  my previous article about building a blockchain application , I shared some of the tools, tips and techniques I used to create an end-to-end blockchain web application. . Let’s hone in on a specific part of that and explain in more depth how I built an underlying  Ethereum  private blockchain for testing purposes. . I mentioned that I predominantly used  testrpc  to stand up a simple, single-node Ethereum instance which had the APIs exposed and some accounts pre-filled with Ether for testing. . Testrpc is a great tool, super simple and fast to get started, which makes it ideal for development and if you are new to the tech. However there are some situations where you might want some extra flexibility or you need to test different scenarios where you require a more “production like” setup. These include - . Testing the effects of a network of nodes (e.g. a multi-node Ethereum cluster with each node in sync with other nodes via the P2P protocol) . Access to other Ethereum APIs, for example the  management API  . Access to the  Ethereum JavaScript Console  . The confidence of developing against a full  Geth node  . Easier integration with or ability to test alongside other tools or technology such as  IPFS  . If you find yourself having to set up a private Ethereum cluster (as I did) you’ll find that it’s not actually a straightforward process. . While there are  one  or  two  pretty clear tutorials out there as well as  some scripts  both for me had some problems. The scripts prefer that you are running Ubuntu, the preferred/recommended platform for Ethereum and the tutorials contain anywhere between 5-20 steps. . I wanted a simple, repeatable and cross platform way to bring up and tear down my clusters. Enter  Docker  and  Docker Compose . . Today I’m announcing our open-sourced  ethereum-docker  which contains a bunch of Dockerfiles and Docker Compose scripts for bringing up standalone full Geth Ethereum nodes or multi-node clusters along with the very nice  eth-netstats  web dashboard for viewing the status of your cluster. . Let’s take it for a spin. . Grab the repo - . To run a single node do: . If you are using Docker-machine with the “default” box you can do: . Otherwise on Docker for Mac you can just  open http://localhost:8545  this will give you a connection to the JSON RPC API. . To get attached to the geth JavaScript console on the node you can run the following - . From there you should have access to the full  JavaScript runtime environment  where you can execute commands against the node. . To perform tasks like starting and stopping mining you can run: . To bootstrap the cluster run: . This will create 3 nodes: 2 Ethereum nodes + 1 netstats node. To access netstats UI you can do: . Or on Docker for Mac: . And you should see something that looks like:   . You can scale the number of Ethereum nodes by doing: . There you go, you have a private Ethereum cluster you can scale up, down and test your applications with. . This provides basic support at the moment for Docker and Docker compose, as always there are  a bunch of open tickets  on Github for improvements. We would like to add things like support for deploying on top of  Kubernetes  and  Marathon , support for deploying a cluster with IPFS enabled and ability to interact with the cluster nodes and APIs in an easier way. . If you want to give it a try or help out  head on over to the Github repository . ", "date": "2016-05-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Docker Continuous Delivery Workflows\n", "author": ["Cam Parry"], "link": "https://capgemini.github.io/devops/docker-ci-workflows/", "abstract": " So this big new world of containers has come along, and with it we are solving a lot of problems we previously had with virtual machines, and we are developing much more elegant solutions to problems as a community. One thing that really is amazing is the Docker Continuous Delivery workflows that are being brought out by all the different SaaS continuous integration tools e.g.  TravisCI ,  CircleCI ,  Wercker  etc. . Then there is the automated build workflow offering by DockerHub. A lot of these tools are covering one major use case, that is you merge changes into a branch, usually the master branch and the CI tool will automatically build your latest or tagged Docker image and then deploy to a cloud service for you. While this is brilliant, lately I have been wanting to make the containers as slim as possible, so I don’t want to be building my containers inside the container I will be deploying, I also don’t want to have to make massive remove statements at the end of my image to remove any superfluous packages or directories that aren’t needed or that could cause unnecessary attack vectors. Even though we are removing these directories and packages, often they will still be in the image, in one of the layers making it up. I also do not want development packages in my container if I can help it. . So let’s cover the different workflows and some possible use cases. . Automated  DockerHub  Build,  Tutum  automatically picks up new image and deploys it to run functional tests. Use Case:              Small open source projects that you want to make sure are working and are as hands off as much as possible           . Small open source projects that you want to make sure are working and are as hands off as much as possible . Build in CI SaaS service, deploy to Docker private or public registry, pull down and deploy to cloud. Run functional tests. Use Case:              Small to large teams coming from virtual machine ecosystem, as it will match most closely to a CI process to what they are used to.           . Small to large teams coming from virtual machine ecosystem, as it will match most closely to a CI process to what they are used to. . Build in Hosted CI service, deploy to cloud service. Run functional tests. Use Cases:              Large team with high frequency of commits. Must pass acceptance tests. Only final artifact to be put in registry.         High frequency commits with small team, but with a lot of deploys based on features. Only signed off artifact to be put into registry.           . Large team with high frequency of commits. Must pass acceptance tests. Only final artifact to be put in registry. . High frequency commits with small team, but with a lot of deploys based on features. Only signed off artifact to be put into registry. . These are the main use cases for build and deploy but what about setting some quality gates and security.  Project Nautilus  will scan your containers for vulnerabilties, but is this too late, if its already in your registry or running on a machine somewhere or a new piece of functionality didn’t work as you abandoned all those years of testing your application for this wonderful world of containers. . So here is an opinionated workflow for docker, let’s say we are developing a NodeJS application with Angular frontend. We make a change in the application, run any linting, style and unit checks locally, as part of some githooks, then we commit to a branch. This then runs the same linting, style and unit checks just in case we missed anything locally, this shall be our first quality gate. We then want to check to see if any of the code or node modules are vulnerable, for this we can use  retire.js . We should then make sure the application tests have good code coverage, one of the better tools for this is  codeclimate . . We now want to make sure my application works end to end as well, so we run the grunt:e2e task we have setup to run my  protractor  tests against  saucelabs . If that passes, we can then build the container, but first we might want to run a  docker lint  test to make sure this won’t throw any errors, as well as a  Docker Label Validator  test to make sure that any labels we have included are valid. So now we are ready to build the container but what if we have unnecessarily added some vulnerable packages to the application, we can’t be responsible for keeping up with all the security warnings that come out these days. So we run a vulnerability scanner which checks the container against OWASP and CVE databases for any vulnerabilties, much the same as the  Project Nautilus ,  CoreOS Clair ,  TwistLock  and many other companies and open source projects are doing these days. For now we have decided to use  Lynis . This article  Docker Security Tools: Audit and Vulnerability Assessment , covers off most of the toolset out there in this space. . So we have gone through a Docker quality gate and can breathe easier now we have a greater sense of confidence our container is both tested and secure. But let’s take this a bit further because we want a seriously secure and performant container. So we run something like  docker-slim  over it, to minimise the container size, as a bonus it will also generate a apparmor and seccomp profile for your container. . Okay so now we have a slim, secure, performant container that we can now either tag and push to  DockerHub  or a private registry. Now we want to deploy this release of this new container, so now we can setup an automatic hook from tutum to deploy or get a CI SaaS tool to deploy it to a cloud of our choice via a webhook from my registry. Hooray the application is live. . Here is a diagram of the CI flow we have just undergone. .   . Now the container is running, we want to put some checks in to make sure it stays secure, so we make sure the apparmor or seccomp profile are implemented, we also want to make sure we am running something that is going to check for security updates such as the tools mentioned earlier e.g. Clair or libsecurity. . Now we want to test the performance of our container from the point of view of security and load, so we can see if this has changed over time and if we are regressing our application. For this we are going to use our  Vegeta  test harness, but something like  Bonesi  will do the job. While running load we are going to be running chaos monkey at our applications and hosts, to make sure they behave correctly and don’t do anything we are not expecting, when they move hosts or restart. We are also going to be running  Minion . Looks to be a fairly good security framework from Mozilla. Minion will aggregate all our security tools reporting, issues and scans. This will be our final security and performance quality gate. . At this point we have a pretty high degree of confidence in our new container. We then deploy to production and start on our next brilliant idea. As a side note, we need to make sure our host OS is secure as well, so using many of the  CoreOS principles of trusted computing  or running  DockerBench , over your host, you will go along way to securing your host, hardware and all the way up to your application. . By no means am I suggesting use one tool over another, or that this is the best workflow. It is more of an example of how to integrate these set of tools into your CI workflow and more of a call to arms for people to start utilise these security tools in their day to day development. . For a full working example see  our MEAN stack shopping cart demo . ", "date": "2016-01-26T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Women in Technology is an issue for everyone\n", "author": ["Shana Dacres", "Mat Perry"], "link": "https://capgemini.github.io/learning/women-in-tech-conf", "abstract": " Last week Thursday (28th of Jan) a few of our ladies and a gent had the opportunity to attend the inaugural  Women of Silicon Roundabout conference . Over 200 women and men working in technology were present at the conference and workshops, which focused on practical and technical development of women in the sector. The  agenda  included talks on closing the gender gap, how to become a software engineer, panel discussions on women in leadership and workshops on how to market yourself; to name a few. .   [Lola Balogun, Catherine Morris, Polina Korosteleva, Sophie Field, Hillary Bannister, Simina Camin, Apurva Auresh, Shana Dacres, Sandra Koree] . Mat Perry and Shana Dacres provide us with their perspective on the conference. . Despite my initial reservations about how awkward I might feel attending what I thought might be an all women event, I’m really glad I went. I listened to some really inspiring people and got a better understanding of how the technology industry needs to change for everyone’s benefit. . Here is a summary of the talks I attended: .  Charlotte Briscall  gave an insight into the things she values in her workplace that enable her to be a Mum and a Digital consultant at Sainsbury’s, also previously at Orange and EE.  The importance of early years of child development, engaging girls in IT, the factors that might be limiting this and the value of single sex schooling to support child development. .   .  Lan O’Connor  gave a view from a leadership perspective, having had a successful career both as an entrepreneur and a corporate career at Capgemini.  Leading Capgemini’s transformation to a Right-shore operating model, and now enjoying a portfolio of work with start-ups and academia alongside her work at Capgemini. . Maria Cristina Tarantino of Elsevier Publishing identified the importance of gender balance in agile teams, the need to foster a caring thoughtful culture to enable everyone to contribute and her belief in ensuring there is a fair distribution of work to build Team liquidity. .  Adriana Vasiu  described her journey from a small village in Transylvania, Romania, to London to become a Software Engineer at Sky. She identified what makes her feel valued in her role and what stereotypes and prejudices she has fought against, with humour, courage and determination. .  Dr. Sue Black  gave an inspiring account of the challenges she faced in developing her career, the campaign she led to save Bletchley Park from closure and her recognition in this year’s Queen’s honours list. She modestly claimed, “If I can do it, so can you”. .   . Michelle Anderson from Deliveroo gave an introduction into a number of different Crypto currencies, explaining the role of blockchain in securing everything from transactions using Triple Entry accounting, digital rights management, to securing networks using blockchain to prevent DDOS attacks. .  Filipa Moura  from Twitter gave a talk on how she became a Software Engineer, initially at UBS and more recently at Twitter.  She stressed the need for continuing education, the importance of feedback and distinguishing feedback from criticism. . During lunch I also had the opportunity to meet the team at Facebook involved in a program called  Internet.org  which aims to provide internet access to less developed parts of the world via a fleet of low altitude drones, controlled by satellites, which are being partly developed and tested in the UK, in Bridgewater, Somerset. . After listening to very talented and inspirational women speak on how they got into working in technology, the challenges they faced and the advice they would give other women (and a few men), I left with three key take away points: . This was the opening statements from  Rebecca Muir , head of research at ExchangeWire. Rebecca got straight to the point by outlining that there is no right way to be a woman in technology. Preconceived expectations and stereotypes of what a woman in technology should be like should be left behind. Her message was to be yourself; as this is your power.  This message was also continued by  Clare Sudbery , Software engineer at laterooms.com. She spoke about walking into a room and being yourself in her talk titled ‘Women can do that too’. ‘If they don’t like you, you don’t like them. And if they don’t want you, you don’t want them’. Clare was all about empowering women to be themselves and not trying to fit in with others expectation of what a woman should be like. She also pointed out that it’s OK to be a geek – geeks are cool! It is cool to be smart and multi-skilled. There is no shame in knowing you have exactly what you need to get ahead in your career. She urged each woman to challenge the unconscious bias, keep breaking the rules on what a woman can’t do. . Lan O’Conner, Corporate Vice President here at Capgemini, delivered one of the most captivating talks. She spoke about not letting fear, environmental factors, or even a ‘you can’t do that’ hold you back from getting to where you wan to be. In her talk, ‘Room at the top: making it or taking it’, she spoke about making or taking a place in leadership whilst providing a view into her career development from building dams to becoming a Corporate VP. She describes ‘taking it’ as saying ‘yes’ to every opportunity that arises. Were as, ‘making it’ is all about creating a place for yourself if the position or opportunity isn’t there already. Both premise is based around what you can do if you put yourself forward and fight for what you want. “I’ve always been outnumbered and had to fight for what I wanted to do” says Lan. She gave an example where she showed up at a client’s office, walked in and asked to speak to the them without setting up a meeting prior or asking anyone’s permission to do so; which enabled her to win new business. She ended with ‘if you want it to happen, go out and make it happen!’ .   . Being a woman working in or interested in technology is a great place to be – you are in demand. Or as Lan O’Conner puts it ‘You are hot! You’re in a great position, with great choices that are yours.’ We heard how females have a significantly higher chance of being being interviewed and hired than men as companies are desperate to hire women. As Charlotte Briscall, head of digital experience at Sainsbury’s, explained ‘hiring more women in senior roles makes companies better as a whole, as women bring a different prospective’. The challenge for tech companies today is how best to support and encourage women entering or already in the field into senior leadership positions. During the all women panel discussion ‘Women in technology leadership’ where we heard from a Thoughtworks Lead consultant, a Goldman Sachs VP, a findmypast.com CEO, a Barclays Director and a BSC Fellow they gave a few words of advice: . Consciously think about your transferable and technical skills – they can open doors. It is what can evaluate you quickly. . Believe in yourself – don’t stay up worrying about if they are going to find out that you don’t know. Take comfort in knowing that you can handle it. . Take a risk – What can you lose? Everyone gets something wrong, it’s just about learning from it. . Embrace change – Shake it up a bit. Change roles. If you don’t like it, get out and don’t look back. .   . Although it has been said the  gender gap won’t be closed until the year 2133 , it gives me great comfort knowing that the world is changing in relations to attitude towards gender. I look forward to seeing the changes which will be accomplished by female leaders in technology. The current achievements of inspirational role models I heard from is helping to change the status quo and empower generations of women. ", "date": "2016-02-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Seniority, strength, and serendipity - what makes a good lead developer?\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/seniority-serendipity/", "abstract": " Like many other people, I fell into a lead developer role, mainly because I found myself part of a team that had no leader. The idea of  self-organising teams  makes a lot of sense to me, but I think that people need, or at least want, leadership, and that social dynamics generally tend towards a leader emerging within most groups. Lately my Twitter feed has seemed to contain  lots of  people  talking  about  what defines  a  senior developer  and  their role , and it got me wanting to figure out my own thoughts on the subject. . As I’ve previously written,  job titles are a curious thing , but there’s something particularly odd about the word “senior”. It’s certainly true that it makes a difference to the way that people perceive you and your employability. This isn’t just true for developers - a designer colleague reported a significant increase in LinkedIn spam from recruiters immediately after including the s-word in her job title. . There are different types of leaders, and I’ve been lucky to have worked with some excellent ones in my career so far. I’ve also worked with some less inspiring leaders, but that’s another story… . Some of the lead developers I’ve worked with have been technical experts who knew their stuff inside and out. Others have been more like project managers, making sure that the team is keeping on top of the backlog. Some are like sports captains, motivating the team through exhortation or example. . Here are some of the qualities that I think make an effective lead developer. . Sometimes the team needs someone to say “yes we can do this”, to help them believe in their own abilities. Other times, they need someone to say no, to protect the team from unreasonable requests. Either way, the leader needs to have the strength of character to stand up and make a statement; to say that something will happen, and have the commitment to see it through. . Perhaps this isn’t specific to leaders, and it may be obvious, but teams work better when there’s trust and openness. . Everyone gets it wrong sometimes. How a person reacts when they realise they’ve made a mistake is an important measure of their character. Chris Hartjes  recently put it well on Twitter  when he talked about the importance of “learning to let go of the desire to always appear right”. . Something that goes along with humility is the ability to not try to be a hero. Leaders should share knowledge and help other members of the team to build their skills. Especially if the team includes more inexperienced members, it’s enormously valuable for a lead developer to be willing to spend time helping colleagues to learn. Part of that is often about giving team members enough space to do the work themselves - it’s important that leaders let their team-mates gain their own experience. Leaders shouldn’t micromanage, and they shouldn’t be backseat drivers. . It may seem facile to say it, but there is a lot of value in having been there and done that, and not being fazed by whatever might be thrown at you next. . As Rudyard Kipling famously put it, you should try to  keep your head, even if others are losing theirs . At the risk of repeating my previous point, it’s important for a leader not to panic. If a team is struggling, and the politicians are jumping up and down and talking about escalating issues, the lead developer needs to remember that it takes much longer to fix the mistakes that will be caused by rushing than it does to do slow down and do the job properly. Trouble is, sometimes it takes longer to explain that point to a project manager who’s getting grief from all sides. . All too often, developers plough their own furrow, getting on with the job without considering the tasks in the wider context of the business goals that they relate to. Too few developers value the ability (and desire) to think about the why as well as the how of the project. Similarly, it’s all too easy for a developer to start building things without adequate consideration of edge cases and exceptions - an effective lead developer will look at a project from a lot of different angles. . It’s important to know what the team is capable of, and to make sure that targets are sensible. Too many people in the tech industry talk about  impostor syndrome  and its inverse twin  Dunning-Kruger , but clichés arise because everyone notices the same truths. Having a realistic awareness of the team’s strengths and weaknesses goes a long way. . Many developers like to think of themselves as somehow above all the nonsense, that they are purely engineers who don’t need to sully themselves with the messy realities of human interaction in a corporate environment. But messages need to be told in a certain way if they’re going to be heard. Especially when the only way to get the stakeholders to pay attention is by getting them in a meeting together. Yes,  some meetings are toxic , but sometimes they are necessary, and when you’re in them, you need to behave in a way that means people will listen to you. . Again, it may seem facile, but there is something of an X factor when it comes to being able to motivate a team to get the thing done. In short, a good leader is a good leader - someone whose leadership is accepted and supported by the team, and who helps the team to succeed. . All of the attributes I’ve mentioned go to making an effective leader of development teams, but I’m sure that there are other important qualities I’ve left out, both interpersonal and technical. Please share your thoughts in the comments. ", "date": "2016-02-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Win tickets to Voxxed Days Bristol\n", "author": [], "link": "https://capgemini.github.io/learning/win-tickets-to-voxxeddays-bristol/", "abstract": " Want to win a free ticket to  Voxxed Days’  first open source conference in Bristol? . We at Capgemini Engineering are giving away a few full day passes to the  Voxxed Days Bristol  conference, February 25th 2016 (retail value £180). . As a winner you will have access to a choice of 18 talks, 3 Track Keynotes, 1 Plenary Keynote, and as many lighting talks as you can fit in on the day! You will also have the opportunity to present your very own Lightning Talk where you can share your ideas, knowledge and enthusiasm with 300 developers. On top of all this, you’ll walk away with techniques and tools that will immediately make you a better developer! . The winner will also be invited to attend  Find A Tech Job In Bristol , taking place right after, where they can scope out opportunities on offer in the local area whilst enjoying beer and pizza. . As you may have heard,  Capgemini is sponsoring Voxxed Days Bristol , an event dedicated to showcasing and highlighting technology and architecture that will be big this year. Some of the area’s most respected technology experts will share their views and strategies with you on Programming, Cloud Data and Cloud Architecture. . See a complete  list of the speakers and full programme here . . The contest goes live today (Friday 19th) and ends on Monday, February 22nd 2016 at 11 am. . Winners will receive a tickets to attend Voxxed Days Bristol February 25th 2016 at the Watershed in Bristol. . We will contact the winners by email. . You can earn extra entries to the contest by sharing the giveaway link with friends and following our social accounts:               Twitter - @CapgeminiUKppl           LinkedIn            .  Twitter - @CapgeminiUKppl  .  LinkedIn  . Enter the contest by sending your full name to  technologyconferences.uk@capgemini.com  . Winners will be selected at random. . No purchase necessary to win. . If you’ve already purchased a ticket, no worries. If you win, you can give your spare ticket to someone you really like. . The entry deadline is 11am Monday February 22nd 2016 and the winners will be announced shortly thereafter. . Good luck!   ", "date": "2016-02-19T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Voxxed Bristol Feb. 25th 2016\n", "author": [], "link": "https://capgemini.github.io/learning/voxxed-bristol-feb-25th-2016/", "abstract": " What’s the technology / architecture / way of working that will be big this year? . You’re not alone if you’re not sure. The Voxxed Days Bristol conference will highlight some of these big choices over the space of 18 talks, 3 Track Keynotes and with 300 developers in attendance from the region.  Organised by  High Tech Bristol and Bath (HBB) , to share the Devoxx philosophy and community, Bristol is the first UK regional open source conference from Voxxed Days. .   . By sponsoring Voxxed, Capgemini wants to demonstrate our commitment to our customers &amp; staff in Bristol &amp; the Southwest.  At Voxxed we will be speaking, attending the talks and hosting a trade stand; all with our local engineers from Bristol, Cardiff and Gloucester.    Please come and meet our team, we are  keen to strengthen our involvement in the Southwest open source community, share our views on technology and tell you about our regional activity. . Why are we doing this? Capgemini has an open culture which believes in a diversity of processes, people and technology.   We regularly contribute to Devoxx/Unvoxxed and other UK conferences, because we want to understand the best new technologies and meet the people developing and applying it. . Open source technology is a key part of the solutions delivered by Capgemini, we aim to use and re-use open source software when we can so that we can focus our efforts on implementing code that delivers value to our clients.  While not exclusively, our selection of technology is often from the open source community, which is characterised by: . Rapidly evolving products . Quality matching, if not higher than licensed software . Support based subscriptions models . Standardised legal contractual obligations (Eg. GPL, Apache, BSD) . Capgemini has for 49 years maintained its independence, selecting the best technology for our customers’ needs.  Under this approach we have developed many trusted relationships with leading developers and vendors in the open source community. . Capgemini engineers often prefer to use open source products because they offer: . Freedom to experiment without incurring costs . Information is widely and openly available on products . Longevity and stability (examples : Linux/Java/Apache) . Products can be branched or forked if necessary to create custom builds. . Typically an open democratic decision making process . A thriving market for third party services (support, training, consultancy) . In Capgemini we are all about empowering our people so that they can achieve their true potential. We specialise in many different technologies and are very passionate about the work we do and the businesses  we represent. We work in a variety of sectors, with the world’s largest brands and on some of the most interesting and complex projects. . We work on solutions to revolutionise the way our customers do business. From continuously deployed distributed systems for household names, to core national infrastructure, to back ends for major fashion brands. . We want energetic, bold and passionate people to join our team. Your desire to grow and succeed in a range of technologies including, but not limited to, Docker, Spring, Play, Drupal, AngularJS and using architecture and delivery  styles such as MicroServices and DevOps will be your key to success at Capgemini. But we are just scratching the surface of what is possible with us! . If you’re interested, just reach out and start the conversation. . We are sponsoring Voxxed Days Bristol on the 25th of February 2016, where you can speak to us directly and hear from our very own  Sarah Saunders . .   . You can also catch us at Devoxx UK from the 8th to 10th of June 2016. .  Twitter - @CapgeminiUKppl  .  LinkedIn  .  Meet our people  .  Capgemini UK careers site  ", "date": "2016-01-29T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "The Thing about Things\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/infrastructure/the-thing-about-things/", "abstract": " This article looks at two sides of the Internet of Things – the emerging technologies and the cultural impacts. . The other week I attended a talk by  Mat Henshall , Head of Things at Thoughtworks, San Francisco. I went along hoping to get some insight into the development of IoT interface standards, and was blown away when the contents of the talk were deep, philosophical, dire warnings of the consequences of IoT on our cultural wellbeing. . To go back to interfaces and standards, I first delved into the Internet of Things at Devoxx UK in June. The team who make the Honeycomb platform that run British Gas’s  Hive  home central heating controls gave a presentation about their product and how it used the ZigBee protocol to send messages between the thermostat and the router. My IoT knowledge was currently stalled at Bluetooth so I went home to do a little research, and was in for a few surprises. Firstly, I was surprised at how immature the concept of IoT actually is. . Earlier last year I found myself  denouncing  the development of heavyweight standards and specifications in the Java world. Here in IoT we are at the other end of the spectrum, there is no standardisation at all. . The special thing about the internet, the tower-of-Babel thing that got it to take off and the thing which raised Tim Berners-Lee to godlike status was not the cable network stretching round the world or the connected software on the computers, it was the simple and powerful internet protocol stack which allowed everybody to communicate. IoT has no such unifying concept, and it can’t just steal what the internet has laid down because the drivers are very different. . A key difference between Internet and IoT is the requirement for secure, short-range, low-power network protocols to allow small devices to communicate, both with each other and with a hub device. This hub device is then responsible for the easy bit, the translation of the device messages to the internet. . Let’s start with transport protocols. . Bluetooth was probably the first high-uptake IoT protocol. It’s not used much these days in its original form as its power consumption is just too high, and it was really designed for point-to-point, master-slave configuration rather than network communication over star or hub topologies. There is a new version, Bluetooth Low Energy, which addresses both of these weaknesses, but it may possibly have come a little too late. . As I mentioned, Hive is based on  ZigBee , which is a fairly common protocol. ZigBee made its appearance just as Bluetooth was reaching popularity, but is much more forward-thinking in its definition. For instance, ZigBee devices can be networked in a star, mesh or other generic topologies. . There are other players in other market areas. The  AllJoyn  framework is backed by LG and other hardware vendors in the Allseen Alliance, and has made good headway in that it will be deployed with all LG smart TVs from 2014 onwards. It provides an open source framework for IoT communication, abstracting the underlying protocol. Sounds powerful to me; and a great marketing strategy to get into the hardware market. . Then there is  Thread , acquired into the Google empire when they bought Nest Labs. With the power of Google behind it and many of its backers coming from traditional ZigBee backgrounds, perhaps this will be the new standard? Only time will tell. Thread one-ups ZigBee by being based on IPv6, allowing longer and therefore more IP addresses – I guess when you think about it this is quite a big deal if we intend to massively increase the number of connected devices! The question mark is whether within the IoT we really want the overhead of IP addressing, or whether we want to stick to the current hub model with a local network connecting to the internet at a single point. Seems even Google are hedging their bets, as Android has built in support for BLE… . The engineers have built up our hopes for compatibility and then let us down here. Despite Bluetooth, ZigBee, Thread and others being compliant with IEEE specification 802.15 wireless protocol, they are actually completely incompatible at network level and require a bridge to communicate. The protocol seems to have been defined a little too loosely to be of much use. . Most IoT suppliers are, understandably, not necessarily interested in interoperability between themselves and other vendors. The Apple style lock-in strategy of creating your own bespoke hardware/protocols to encourage consumers to stick with your own products has worked before and is surely tempting for IoT suppliers.  Even within the protocols, there is trouble. For instance, ZigBee applications may all be compatible at the network layer but each implements a software “profile” on top of this. The profiles are developed independently by ZigBee users and aren’t necessarily compatible. . AllJoyn claims to be OS and protocol agnostic; allowing it to float over Zigbee, and Bluetooth. Great, in one way; but in another way this allows for a whole second layer of incompatibilities! . Let us take a moment to reflect on and appreciate the wonder that is the TCP/IP stack. . As I sat down at the talk ready to ask my ZigBee questions, Mat was begging us all to think about the consequences of our actions. Hang on! What consequences? . It was a sit-up moment for the room full of developers; did we really have enough power to need to be aware of the consequences of our actions? Mat believes so. He said he really felt that IoT was reaching the maturity where it could, and would, have an equally large (or greater) impact on our society as the mechanisation of farming. He was incredibly serious about the consequences of IoT on all of our lives. I think his talk may be slightly more relevant out in San Francisco where concepts like robot chefs and connected fridges are more widely available to consumers, but I was surprised by the strength of his conviction that we are on the cusp of a new age. . He used self-driving cars as his example, comparing their possible impact on the automotive industry to the arrival of the car in 1900s USA. In 1900 there were in the region of 200,000 horses in New York City, with all the associated industries – forage provision, manure removal, farriers, harness makers, carriage maintenance. Within twenty years the entire industry had gone and the economic and social impact was enormous, as the figures suggest.  Will it really happen that way with self-driving cars, can they replace car ownership entirely? Will we see such a massive impact on the automobile industry? Mat points out that even taking Uber as the cheapest way to run a taxi firm, 75% of the cost of the ride is in the driver. He had some other great statistics – the average family car spends 90% of its time on your driveway, and 30% of time spent driving in a city is in looking for a parking space! All convincing arguments for replacing city cabs and car ownership with a fleet of self-driving cars - with the resulting massive reduction of demand on the car production industry and its dependents. . I must admit I am not completely convinced by all of Mat’s arguments. Job losses due to mechanisation have happened before and in comparable numbers – mechanisation of farming since the industrial revolution has freed people up to work in other industries (and, sadly, create new and pointless industries such as estate agency and suing each other.. amusingly even these industries are becoming automated! I have a beautiful utopian vision of the future where robots are merrily trading houses to each other for ever-increasing prices whilst the phone networks are jammed with automated call centres communicating with smart houses…) I do, however, completely agree with his overall opinion that the onset of IoT will require a cultural change for humanity. I think that Western cultures will be particularly vulnerable, because of their intrinsic belief that success and happiness are linked to hard labour. With mechanisation should have come reduced working hours – and indeed, in some Northern European countries it has done so, with the EU introduction of the 35 hour working week and then the Swedes trialling the 30 hour week last year. . Most UK employees opt out of the 35-hour week, however, and I don’t think such a thing exists in the US - we are all working as many hours as ever before. How will we cope as capitalists when there is nothing to do? I still don’t see this as the armageddon that Mat is hinting at, though. We can look to the East for a philosophy of living, we can focus on medicine and understanding disease, we can turn to art and music for accomplishment. But will we? Or will we just work longer and harder hours finding legal niches that allow us to sue the programmer of our automated car because its algorithm placed the value of the life of a dog above our neck muscle structure and swerved to avoid the dog, giving us whiplash? Come on humanity, don’t let me down! . We touched on the ethical dilemmas of the Internet of Things. No new ground, Isaac Asimov had documented the famous three rules of robotics and discussed their consequences in novels such as ‘I, Robot’ decades ago, in 2016 we can see a real need to understand the debate. Capgemini’s Rob Kerr  blogged on this recently  and it’s very relevant to IoT. There was the first instance of a  self-driving car getting a speeding ticket  in Florida recently, apparently because it failed to recognise a temporary speeding sign. . Imagine you are a programmer in charge of the algorithms controlling a self-driving car. If something falls in the road in front of you,  how do you choose the correct action to take ? If the item is a hay bale, it may be safer to just hit it. But what if the item is a child? Or what if swerving to avoid the item pushes you up onto a busy pavement? We briefly discussed the upcoming trend for programmers to take out insurance that will pay their legal bills should they end up in court over a decision algorithm they had created. . That old sci-fi idea that robots would out-compete humans and replace them was raised in the talk. No, said Mat, of course not – but we will become shepherds of robots. I love that metaphor. . I managed to get in my question about interoperability. Mat reckons we are a good five years off convergence on a standard or a single protocol. He argued that a bridge is no big deal; and that as long as you have application layer compatibility your devices can communicate. . For me, I’m not so sure that bridges are as hassle-free as Mat makes out. In current home networks, it is theoretically possible to get many of your devices to communicate – TVs, stereos, laptops and personal computers, phones… but the ones that really work are the ones based on bespoke protocols or common vendors. A pure Apple home network is a lot easier to manage than a combination of vendor technologies, and you’ll struggle to get a sound system as effective as Sonos without using their proprietary technology. Maybe he’s right though and the “home ESB” is just around the corner. . Mat covered another fascinating and super-important area in his talk - security. In IoT world, maybe ultra-compatibility is not a good thing. Some of the most amazing bugs and disasters have come from the fact that everybody is driving their Things from the same cloud. Similar, I suppose, to having just one giant internet banking system for everybody. Great when it works – owners of Tesla cars wake up to find new and exciting changes to their automobiles such as the sudden ability to self park! But. There are bound to be some security issues and leaks, and the impact of these is huge. Mat talked about how many hotels use connected devices with no security whatsoever, so if you have the right transmission device you can randomly unlock doors and change heating settings to your heart’s content. Might not seem like much of an intrusion, but how many times do you have to flick a heater on and off before it catches fire? Another example Mat mentioned was that of Nest, which is the US equivalent of British Gas Hive, rolling out an automatic update to everybodys’ heating systems which resulted in every system in Minnesota failing to work for FOUR DAYS. In the depths of winter. It’s probably simply a factor of the low uptake of Nest amongst the elderly that prevented many, many deaths from occurring. . Seems a wise choice to follow Mat’s advice when architecting an IoT solution, and to use your own secure cloud. . As I stated, I ( and others ) don’t really buy Mat’s argument that the culture changes the IoT will introduce are apocalyptic. I am just old enough to remember similar dire warnings in the early 1980s, before  the proliferation of personal debt  and the expansion of offshore teams  made us all so tied to our jobs that it sadly became cheaper to employ humans to do menial work than to develop robots to build them. Maybe this time we will finally see the proposed labour reductions and it will be no bad thing. We are all  working too hard as it is . . I did, however, take away a lot of great lessons from Mat’s talk and the big one for me was how important and underestimated the  security  of IoT systems is. Many live systems are being  exposed  as having  security holes , and the first thing on your mind when you boot up your robot chef is probably not “how secure is my data”, but it should be! So next time you fill in a hotel feedback form, be sure to query the ability of passing hackers to set off the sprinkler systems. And when you’re architecting for the Internet of Things, ensure you use a private cloud and address security FIRST. ", "date": "2016-02-25T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Understanding and Addressing our Waste\n", "author": ["Satvinder Hullait"], "link": "https://capgemini.github.io/agile/understanding-and-addressing-our-waste/", "abstract": " How often do you find yourself having to wait for an event to happen so you can progress what you are trying to do? . Waiting for your application to build? . Waiting for someone to code review so you can commit? . Waiting for a new deployment in the test environment so you can test new functionality? . Awaiting a response from a client? . People cancelling attendance to workshops so you need to reschedule? . These are all types of waste, why should we suffer from them? . What I mean when talking about waste is the time we are taken away from being fully focused on a task. When we give full attention to a task we start to get in to  the flow , being focused becomes difficult if for example you are constantly being pulled into lots of meetings or your computer keeps crashing and your waiting for it to catch up. . Now I’m not saying this is the case all of the time but normally there are areas of waste in most projects in some way, shape or form. Off the top of my head some examples of waste: . The client desktops we need to use are slow and crash frequently, inhibiting our ability to work efficiently. . Restrictions on what tools we can use to communicate with our team (this problem is more prevalent in a distributed team) . It takes 4 hours to deploy to an environment so testers can start testing newer functionality (which means either you stop all testers for 4 hours during the working day or make one wait until the next day to test), possible solutions are;              Set up a CI environment with a streamlined deployment process         Analyse why your deployment takes so long and start working on a fix for it.           . Set up a CI environment with a streamlined deployment process . Analyse why your deployment takes so long and start working on a fix for it. . Stand ups are too long or not focused enough, possible solutions are;              Have the right number of people who are relevant to the stand up         Ensure people turn up on time         Try to keep it time boxed so each speaker must be succinct         Take things offline           . Have the right number of people who are relevant to the stand up . Ensure people turn up on time . Try to keep it time boxed so each speaker must be succinct . Take things offline . These are just a few possible areas of waste on a project, each project is different and  will have its own set of challenges and external influences. However for the most part there is always something we can do to improve our projects, thus improving our working day, by reducing waste. People who are familiar with Lean may have heard of the 7 types of waste or Muda (if you haven’t heard of it before, here are a few links for reading:  the 7 types of waste ,  Muda ,  Seven Wastes Software . I’m keen not to cover it here as I believe it’s deserving of a separate post.) . Firstly before we start trying to address our waste we need to work out what exactly it is, for the most part we all love a good moan, so it’s not difficult to get the ball rolling in this area. The important part is to have a team environment where we have a willingness to change and adapt our processes (be ‘Agile’) and create a safe collaborative environment so people aren’t scared to suggest improvements to process. . Trying to work out what our waste is can be a struggle if we don’t have the right focus; luckily most projects that we work on these days tend to be some flavour of Agile. So we have the ability as a team to ask ourselves in a retrospective, what has been inhibiting our productivity or causing us to be frustrated during our work day. . Ultimately it is the project team who know what the areas of waste are; this is due to the fact the team has to face it, day in day out. We tend to know what the solution is but we are a little hazy on how we go about implementing it, hence why it’s beneficial having the time in retrospectives to discuss. . Talking about waste can kick start the thought process on how we could deal with it. Writing down our issues is good practice, as we start to get visibility of them, it’s important to make sure we commit to actually addressing them. There is no point in having a wiki page with 100 issues on and just adding to them each retrospective without tackling any of them. . A key part of looking at a solution is to ensure the action we decide to take, is achievable and measurearble. By getting feedback quickly and frequently we can make several small adjustments (but that is also dependent on what the problem is). We need to set a timescale in which we can say if our experiment has been a success, we need more time to judge or we need a different solution (I believe one 2 week iteration is an ideal timeframe). We don’t always have to fix something in one iteration either, it may take a few to mitigate an issue. That is because dealing with waste can be the team adopting a slightly different way of working and that doesn’t happen overnight. We should look to always look to break down the solution we come up with into small, achievable and measureable tasks. . Just because we deal with some of our waste it does not mean we will all start working at 100% all of the time every day, we still need that level of slack in a team where people are not being overworked. An analogy I once read was  “you wouldn’t want your CPU working at 100% so why would you want your team doing that?”  . When thinking about an area of waste we need to run it through a thought process, see below for an example workflow: . Identifying what the problem actually is before we think about how to solve it . This is probably the most difficult part of the process as a lot of the time we tend to see symptoms of a problem and not the root cause. . There are times when our areas of waste are caused by external influences. Our first port of call should be to attempt to resolve things by speaking to the right people about this issue, however communication channels aren’t always in place to allow these discussions to happen. There will also be situations in which our hands are tied, so we instead capture metrics and attempt to create a well reasoned case. . It is normally in our interest to fix these problems, but the timescale in which they are done can vary. Sometimes it could be better to leave and monitor an issue over a longer period of time, due to the fact it can expose wider issues, again this is normally caused by external influences. . If we can fix a problem and it adds value to the team we should always try to do it, provided it’s not to the detriment of a successful delivery or cost too much. . What value will we get out doing this, why do we want to do it? This is perhaps the most important question. We always strive to ‘add value’ in terms of technical stories, it’s the same for improving the way in which we work. . What is the solution to this problem?              Cost versus benefit (If cost is too high, alternate workflow?)         Have we got the people to do it?           . Cost versus benefit (If cost is too high, alternate workflow?) . Have we got the people to do it? . We need to understand the effects of what we do, if fixing one thing breaks several others is it worthwhile? For instance, if we are updating a class that effects the way a webpage works, have we got a comprehensive set of both unit and automated regression tests we can run, to ensure we have not caused any unwanted side effects? . It’s also a good idea but not always practical to run a root-cause analysis on why these problems have occurred as well. (If you are unfamiliar:  5 whys wiki ,  5 whys ) . An application takes 20 minutes to build and deploy locally and you need to do this to see any change you make (even if it’s a HTML change - bit of a dramatisation as there are normally workarounds). . The problem may feel like a number of things e.g. a redesign of our system is needed (this may be our root cause), the sympton we see is that when a developers makes a change, it takes 20 minutes to see if it renders as expected on the UI. However the real problem here is that the developer keeps getting distrupted, so they are likely to get frustrated if this happens constantly as it’s delaying them completing a task. . Yes, both within in our remit and interest to fix. . Reduce the turnaround time for developers to be able to complete the coding for a story and allow developers to get into a flow for a longer period of time, without constant breaks, whilst waiting to redeploy. By reducing this turnaround time it means our testers can get started quicker, meaning we get closer to delivering value quicker. . A solution is to upgrade a core part of the system. However the cost of doing this whilst implementing business critical stories is just not feasible, despite the time we could save. So an alternative solution is to use a tool like  JRebel  which reloads changes instantly. Whilst this doesn’t actually fix the problem with the application, it does mitigate needing to redeploy for every change. However there are some changes that will require a redeploy and rebuild but this won’t be as frequent. The cost here is a need for licenses and a slight learning curve on how to use the tool, but the benefit outweighs the cost of a license. Also the cost of a license is less than the cost of upgrading the system. We only need one person to understand the tool and then transfer knowledge to the rest of the team. . Little risk, only a bit of a learning curve and knowledge sharing is required for how to use it. . So, once we’ve worked out what our waste is and we’ve started to address it, the next step is to track our progress. It’s important we capture metrics on this. There are a few ways in which this can be done: . An opinion poll in the stand up ‘How are you feeling about issue X’ and get a rating off each person - this can be a little to repetitive though. . Capture it in the retrospective say, ‘Since our last retrospective has issue X become less of an issue?’ and dedicate sometime in the retrospective to for this issue. . Too often we get bogged down into the “what went well, what didn’t go so well, what can be done better” style of retrospective, I feel like it isn’t always the best format as  it is difficult to get the valuable feedback we need from it. Especially when we have a focus on improving our productivity. . It’s also a good idea to keep sight of the issue, even if we feel it has been mitigated, as sometimes it’s easy to fall back into old habits unless we remind ourselves why we made a change. We need to be able to define a state of done for improving our process and addressing the waste. This can be done in a similar fashion to how we create a Definition of Done for our software tasks; it just requires us to think about what our desired outcome should be when we are addressing something, by keeping our tasks small and measurable it becomes easier for us to say when we’ve achieved something. . Ultimately time is a valuable commodity, which we can’t get back, so why should we waste it? Time is money, time that we spend waiting for stuff to happen so that we can be productive is wasting the clients money and our time. This also applies to our own personal lives and processes, even stuff as simple as how we commute to work and making some small, incremental changes can add great benefit. . I believe that we have lots of examples of waste around us in our day to day lives; it’s about being able to identify them and trying to fix them in order to increase the quality of our day. Personally I feel good about a day’s work when I’ve been able to crack on and be productive, however when I’ve just been caught up and not been able to deliver value I find that I leave work feeling frustrated. . At the very least we need to be able to identify our areas of waste, as a result of doing that we can start to work on fixing them iteratively and measuring our progress as our iterations go by. We don’t always need to focus on the big issues either, sometimes it’s just as good getting several small wins. Whilst writing this blog, I was put on to the concept of  ‘marginal gains’ , which was a major factor in the success of the 2012 British Olympic Cycling Team: “The whole principle came from the idea that if you broke down everything you could think of that goes into riding a bike, and then improved it by 1%, you will get a significant increase when you put them all together” . We should be able to apply this concept to developing our software, everything from gathering requirements, writing code, system testing functionality through to our deployment process (each of these area’s comprise of several smaller things but just for a high level example). If we improved each area by just 1% we would be well on our way to getting rid of some of our waste, delivering value to our customer quicker and improving our working day. ", "date": "2016-03-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "How to do NFR Testing (Non Functional Testing)\n", "author": ["Sanjay Kumar"], "link": "https://capgemini.github.io/testing/how-to-do-nft/", "abstract": " On my current project I’m working as an Integration developer (using among other things  Camel ,  Hystrix  and  MongoDB ). .  This post is based on a series of conversations with  Andrew Harmel-Law  ( @al94781 ).  . I’ve recently been involved in Non Functional Requirement (NFR) testing of some  microservices  which were developed as part of microservice architecture for Integration layer. . This was my first experience with NFR testing, and while doing it I realized its importance and the difficulties it can bring. In this post I’ll share some of my key learnings around how to do NFR testing (providing examples where possible to back my points). . NFR testing serves to validate that your system meets all the non-functional requirements (e.g concurrent requests, transactions per second, response times, sub-service failures etc) expected of it. . Today, applications and the eco-systems in which they run have changed drastically. Older applications used to run in closed, barely-distributed, named-(pet)-system environments where everything was most probably largely within your control. Now, with the arrival of clouds and microservices the eco-system has changed drastically, and the importance of testing NFRs has risen considerably. . I think you should care because: . 1)  It’s important - actually very important  : Its importance can simply be understood from the point of view that if a system can fulfill all its functional requirements, but not fulfill is just one critical NFR (be it related to capacity, speed, security, availability, scalability etc.), then that system will soon get outdated or fail to deliver at all in today’s fast-paced digital world. . 2)  It’s free  : Its free in the sense that you don’t have to invest heavily (for tools or manpower) to do NFR testing. It’s best done by the developers themselves, with freely available open source tools. . 3)  It’s fun  : It is fun for the developers to test the system they build; to see how it performs in the actual world and fine tune, improve and re-engineer their code before it’s too late. . Now you know what NFR testing is as well as its importance and why you should care about it, let me explain how you might do it. . NFR testing should be planned from the beginning of the project as NFRs can have a big effect on the coding/architecture of your application. . Suppose you have an NFR which states that your application should handle very high traffic of say 1000 requests per second. Now, if you start NFR testing from the beginning of the project, you may come to know early in the development cycle that your application architecture can or can’t handle it. This may lead to changes in the code or design or adoption of some coding practice which allows you to achieve this. For example, you may have to use thread pools to minimize the time in creating and spawning new threads. You may also use multi casting and aggregation patterns, or even swap out one framework for another to achieve better response times. . Say another NFR states that the system should not be overloaded in case of failure in any one components of the system. Again, if you test this NFR from the beginning you can find whether your system can cope with this requirement or not. You may decide to build your application to fail fast in case of any error using the  hystrix  circuit breaker from Netflix. . The above example clearly show that if we are Agile (and early) with our NFR testing then it can assist in verifing the coding approach / architecture of our application and help us to change if required early in the development cycle; thereby saving us a lot of time and money later. . Being Agile in your NFR testing is not enough - you have to plan for it properly. Planning is important as will help to decide which NFR tests to run when you get constrained by time or resources.  Believe me - you will . When Planning your NFR testing, you should consider the following points to get the most out of your NFR testing: . To better understand this, when creating an NFR test plan make sure you map all your NFR tests with the corresponding NFRs. . Suppose you created a test in which you are going to verify that in the circumstance where one of your sub-services fails, your system fails fast without overloading. This is a really important and good test as it validates your system resilience in case of failures and also how your system resources are used in such a scenario. . But if you can map this with a corresponding NFR which states ‘Failure to connect or receive a response from the Sub-Service is handled gracefully, logged, and reported back to the consuming application within X seconds of the call’ then not only will this achieve the above said goals, it will also help you to provide your stakeholders with statistics and data for the corresponding NFR and boosting their confidence in the system being built. . Suppose you have a system which uses  MongoDB 2.6  as the database and now you’re told to upgrade to  MongoDB 3.0 . In this scenario  MongoDB 3.0  is new to your solution and may impact you in unknown ways - most probably in a positive way, but you don’t know. There will be always be concern/fear about the effect of this change. You should address this concern/fear with high priority. It is therefore better to run all NFR tests which are related to your Database as compared to other NFR test which are non related such as sub-service failure. . When you define your NFR tests, make sure you prioritize them too, as you may/will be constrained by time and manpower. . Let say you have three NFR tests: . DB failover and recovery . Handling transaction at 100tps . Graceful handling of Sub service failure. . Now, taking the above example, since your last test you have recently changed your Database from  MongoDB 2.6  to  MongoDB 3.0 . Suppose when the system was built all the NFRs were “MUST”, but in this scenario you may not have enough time to re-run them all, so you have to prioritize them. In this example, it is clear that we “MUST” run NFR 1 and 2 as they specifically depend on the Database. NFR 3 can be termed as nice-to-have, as the system has not changed this test is less likely to be affected. . NFRs can also be prioritized with respect to an environment you are running them. Suppose you have three Database related NFRs: . DB failover and recovery . DB Handling transaction at 100tps . Data center failure and recovery. . Now NFR 1 and 2 were already tested on the Dev and Test environments, but NFR 3 was not tested, as these environments don’t have the infrastructure to support it. Well, Pre-prod does have this infrastructure, so when doing NFR testing on pre-prod, you should prioritize the NFR 3 as this is the one which you have never tested before and has a high significance in production. . Setting up the proper environment for the your NFR testing is very important as it may nullify/invalidate your testing or create doubt around the test results. Consider the points below when setting up your NFR test environment. . When setting up your NFR test environment you should make sure it has the same configuration, physical architecture and monitoring as you will have in production. (Or as close to it as you can get to tell you what you want to know.) If any aspect of this (CPU, RAM, network settings / config) deviates from the production configuration, then results which you will get by running your test will be skewed in some way and be less valid when you present them to your stakeholders. . Take the example of NFR where your system is supposed to handle 100tps. Perhaps you set up your environment with 2 CPU with 4 cores. But, your production haS 4 CPUs and 8 cores. In this circumstance, any test which you will run could have two outcomes: . you will get good results (system meets or exceeds your NFR target) . you will get bad results (system fails to meet your NFR target). . In the case of a good result, you should be happy that you have written very optimized code which is working good on limited hardware. But, if get a bad result you may unnecessarily suspect that there is some issue with your application. You may waste time and manpower in optimizing code which is already optimized, but the result is skewed due to hardware limitation. . Also, if you reverse the scenario and use a more superior configuration in your test environment than the production box you might gain a false sense of security. You can easily understand the impact. . A system can operate under different conditions and loads. You  ideally  set up your test environment to simulate all possible conditions (normal load, peak load, failure scenario and n/w latency etc). Environment setup may also include creating data, tests and scripts to simulate these conditions. You should note down the run-state as part of NFR result. Run your NFR tests in all such environments to prove the predictable functioning of the system. . You should ensure that test environment setup should be easily configurable and repeatable. If your test requires some data to be preloaded into the system, then that data should be easy to re-create and should be available to anyone going to run the test. All configuration files should be stored in a central repository so that anyone running the tests can access them as and when required. . The next obvious thing after planning, prioritizing your test runs and setting up the test environment is to actually run them. Again there are several aspects to this: . When running your NFR tests make sure they reflect the real use cases as closely as possible. Suppose, your system caters for four types of request (add, update, delete and read). As we can see the nature of each of these requests are different, and can have a significant effect on overall response times. Now suppose your test is to validate the NFR that you must support 100tps, then your NFR test should not just send one type of request at 100tps, but it should send a request mix of all four request types. You should also take care of the percentage of each request type to be sent for proper request mix (e.g. 20% add, 5% update, 1% delete and 74% Read. You can get this info from your product owner, a user or a business analyst). You should also consider sending bad and invalid requests mixed into your test load to simulate the real life scenario where not everything is going according to plan elsewhere. . There may also be a test to just send successful requests, in such scenario also you should pass all the four types of request with agreed percentage for each type. . This brings up a key point. Request mix is very important, as it doesn’t just validate that your system can handle all sorts of requests, but also how your system behaves when all sorts of requests come at the same time. A lack of request mix can skew your request and give a false impression of high performance. For example, if you just send  read  requests, the response time will be quick, but if you send all  add  requests the response time will be longer than the read. And if you add in some bad requests, resultant error handling and compensatory transactions, then you get a very different picture. . So, take care of the request mix when creating test case for your NFRs. . Again the transactions per second and the number of concurrent users have a huge impact on the performance of the system, hence take care with it and make sure your run your test with the same transactions per second specified in NFRs or what you expect in Production. Suppose you are aware that your system will at max hit with 6tps in Production (tps being controlled by your load balancer), then you should run your test at 6tps only. . You will have NFRs related to system failure/recovery or slowdown/speedup. So, your tests should be run under conditions of sub-system failure and recovery, system slow down and speed up to cover these NFRs. . So you’ve run your tests and have your results - you’re done right?  Well not really. Recording your NFR test results is a must, without it you will never be able to prove your system performance and go back to it in the future when you learn something new. The following are some basic guidelines about what you should record in your NFR findings and how. . The key elements to measure in your NFR testing are most usually throughput and response time, and to view these in relation to the 95th and 99th percentiles. Percentiles are important too (especially in a microservice based system) so its important you know about how they work.  A good introduction can be found on  Wikipedia . . The best way to record test results is to use templates. Create a generic template which covers what what you want to capture and use it whenever recording the results. Templates gives many benefits, a few of them are: . create once, use many times . they help to easily compare the results . you get consistency . they are easy to understand . Recording your NFR test results is important, and it is most important to record the measured variables and environment settings used after each run (e.g throughput, response time, JVM and CPU usage, http and tomcat thread pool and response codes). If you don’t record the same set of parameters in each run it will become hard to validate the test result and see any improvement or side effect after each new release of code. . A practical example will be, for NFR test of 1000req/sec you ran a first test and capture the throughput, response time and CPU and Memory usage and got values of 70tps, 2000ms per request average, and with an average CPU usage of 30% and average Memory usage of 20%. Now to improve the throughput you decided to use more http and tomcat threads. This time you captured only the throughput and response times (100tps and 1800ms). This seems to be good and you have achieved your NFR of 100tps. But you have not captured the CPU or Memory related information. To your surprise the CPU usage was very high 90% and Memory use was almost 100%, which is not desirable state of system are normal load. Therefore, your approach to increase the thread pool was not effective as you expected.  This can trip you up later on when you scale even more. . So remember to capture the same set of data every time and have a fixed template to captured it. . It is also important to capture the NFR test results in the raw, as this is the data used to validate your findings. There is no point in just writing on a wiki page saying that you ran the test and achieved the throughput of 100tps with response time being less then 1 sec with no errors, until you can back it up with some sort of data (logs, graphs, etc.). . If you can capture logs and graphs from a test tool such as JMeter, this is strong evidence that can be very helpful in supporting your conclusions. . In the end, you should summarise the NFR test result to your TL;DR crew. If they can understand and agree with the NFR test results then most probably your stakeholders will too. . One of the most important tasks of NFR testing is not just to run the tests and capture the data, but also to capture all failures and errant behaviours of the system and attribute explanations to each of them. . Every system is supposed to throw errors, but if we don’t have an explanation for each, then the system is unreliable. So you should make sure that not only do you capture the errors, but also that you attribute them each with a proper explanation. Without a valid explanation for failures your NFR testing becomes invalid. If you don’t have any explanation for the error or errant behaviour of the system, then you’ll need to investigate. This may lead to code changes, documentation updates or simply a deeper understanding of the system. More importantly it can lead to finding bugs in system or at least tuning of the logs being captured. Again, going back to the example of the 100tps NFR. When running this test, one of your requests failed. If you don’t provide any explanation for this error, your NFR test becomes invalid. But if you check the logs and you find an error related to “tcp connection timeout”, then you can attribute that failure to this error and can proceed. . If your investigation requires, you should re-run your test to validate your findings. It is good practice to level up the logging when investigating. When it comes to configuration changes, make sure you change only one parameter at a time, to verify the effect of the changes made. To raise the capacity from an observed 75tps to 100tps, suppose you changed the default thread pool size of 10 to 20 and also modified the memory setting from 200mb to 500mb; here you have modified two things. Now, suppose after his change you were able to get 100tps. So, what will you do? will you keep 20 as default thread pool size or memory size to 500mb. You can’t decide until you know the effect of each change individually. So the best way is to make one change at a time and re-run your test to verify the effect of the change made. . NFR testing is not just to check that your application meets NFR specification but also to improve your application. NFR testing always leads to finding bottlenecks in a system and help to improve it. When improving the system we should use the helpers below. . When improving/tuning the system to meet NFRs specification, you should start with configurable settings such as thread pool size, timeouts, queries, caches etc. A code change should be amongst your last options unless you can clearly associate an issue with it. . When it come to tuning, it is always good to find the slowest bit and tune that. If you are able to fine tune this then most probably other bits too will improve. This is not a universal law - sometimes this can lead to a flood cascading on downstream elements which in turn has an even more detrimental effect on performance. The best way to find the slowest bit is to view the response time graphs for each request. Through it you can easily figure out which service is taking more time and you can start tuning it. . When running NFR tests, if you find any error either there will be a valid explanation for it or there will be some bug which is causing it. In the latter case, fix it. Similarly, if your system is slow and does not meet your NFR specification, then to look for improving the system as explained in 6.a. . The key thing to note when improving the system is, if your system is able to meet your NFRs, we should not over-optimize it. Suppose, when you were running a NFR test at 30tps you were getting Hystrix thread denied for thread group size of 10. Then, you changed it to thread group size to 15 and it was working fine. Now there is no need to over optimize and test it whether it works with thread group size of 11, 12, 13 -that is unnecessary. . If there are still some NFRs to be run, repeat the cycle. . We hope you’ve enjoyed this deep dive into NFR Testing. But there is far more. If you enjoyed this, have a look at: .  Applying the Universal Scalability Law to Organisations  - not actually about software, but a very good explanation of this key concept .  Scalability! But at what cost?  - A bit of contrarian thinking to get you thinking ", "date": "2016-03-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Techniques to improve Application Design\n", "author": ["Gayathri Thiyagarajan"], "link": "https://capgemini.github.io/architecture/application_design_made_easy/", "abstract": " Design is important because it gives an application a vision; a vision that can be shared amongst the team. . It gives the team a direction; a foundation on which to build the code. It is not a means to make the implementation easy but to make it efficient. . There are several ways to design an application. I am by no means an expert but I would like to share my experience that can make application design a little bit easier. . Start the design with something that gives the team the best possible start with coding. It is not necessary to design the entire application in one go. It’s all about  microservices  now. But before that it was (it still is) modules, services, interfaces etc. However you decide to break down the application in question, start with the smallest logical part. Start building a mental picture of how this all fits together. Address the functional complexities first, making sure not to lose track of the domain principles. Add the additional but essential features later on. For e.g. you do not want to start the design by worrying about incorporating configuration, logging, monitoring or metrics stuff into your design. . Simple designs are usually the most powerful, efficient and also the most difficult to conceive. . I once designed a screen which would allow a developer/support person to configure the UI for various modules of the application, a settings screen, if you like. I was just finding my new talent in design, so in all the excitement I started off with the vision that my configuration screen will be totally configurable - Yes, a configurable configuration screen. The intention behind this was to make it utterly flexible for the user which in my case was probably another developer or someone with an IT background. It had asynchronous front end loading and all the hot stuff (at that time). But developing that screen was a nightmare. I managed to finish it but a couple of months down the line, even I couldn’t make head or tail of the configurations. The lesson for me was to keep things simple. It doesn’t have to be the Superman of the design world - the one that Flies, Saves the World and looks nice and handsome doing it. . Delegate the complexity if it is bearable elsewhere. In my case, the user would have been perfectly capable of handling that extra bit of work. Knowing the audience really helps. . You have a reasonable design - what next? Here comes the difficult part. Implementing it. This is when the design is put to actual test. . All designs work well on paper. You have to navigate through a myriad of technological challenges before the design actually works. More often than not, you might need to rethink parts of it. Trade-offs are an unavoidable part of designing an application. Sometimes, a better user experience will call for a trade off in the design. . The important thing to keep in mind is not to forget why the decisions were made in the first place. It might have been done in a certain way to represent an important domain concept or parts of it. The challenge is to make a trade off without sacrificing on the things that really matters such as core domain principles,  performance etc. . Look for constant feedback from the implementation cycle. Keep an open mind and never hesitate to go back to the drawing board if required. Some of the best designs on paper may end up being the least efficient ones. Agile delivery methodologies and some recent architectural patterns such as microservices can help us in this regard. Find out the trip ups early on in the cycle and fix it. . Starting small, as mentioned above, really helps in this regard. I prefer doing my design on a whiteboard because it is easy to accommodate changes, instead of drawing endless number of UML diagrams using tools and redrawing them when a tiny thing changes. It also helps me be disciplined about the scale of the design. . I have found in my experience that adding a bit of flexibility in the design really helps. It could as small as having switches to turn things on and off - these are tiny little things that don’t get mentioned in the requirements but really helps when the code rolls out to production. It does need a bit of foresight, but it is important not to get carried away. . It is absolutely essential to make the application as fault tolerant as possible. Think about all possible scenarios however unlikely or brief they may be while designing the application. Now, that is not enough. Because in the real world there is always something lurking in the dark corners (a.k.a edge cases), that will break the code. The best possible thing to do is to build a cushion to protect your application when the inevitable happens. Let it break, but make sure it recovers gracefully. . To sum up, while designing an application it is good to follow three A’s: . Accept - that the design is not prefect and strive for constant improvement. It is not a one-off activity, otherwise it runs the risk of becoming stale. . Accommodate changes to the design if it means improvement elsewhere. A rigid design can easily compromise the usability of an application. . Be Agile. Start small and iterate. Be sure to incorporate the feedbacks in the subsequent iterations. ", "date": "2016-03-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Microservices Like Change\n", "author": ["Nick Walter"], "link": "https://capgemini.github.io/architecture/microservices-like-change/", "abstract": " I got kind of excited the other day, I got told the client wanted a change. I know, weird, right? To make matters worse it was just a month before the project was going live. So you would expect the team and I to be perhaps a bit upset and to say, ‘No that change would be too much’ or ‘That means changing the API’. To generally behave like my 6 year old does, when I tell him he can’t have chocolate before bed. . But in fact we were the exact opposite; we wanted the change! Why? Because our design could accommodate it. Our design was paying dividends. We could see this change being implemented with a small amount of work and minimal (or in fact no) effect to the users of our existing API. Actually we could even use this as an opportunity to produce a better and cleaner API for our customers. . In my previous post  Microservices Gotchas  I talked about how we were implementing a suite of Microservices; using Java, Apache Camel, Spring Boot, backed by MongoDB. The services were split into varying roles; Adapter services, calling dumb pipelines which called smart data services. If you’ve read or been to talks on Microservices, one of the benefits that always gets mentioned is the ability to accommodate change. Microservices are not all that easy to build from scratch; there is a perceived overhead in the splitting up of the functionality. So the benefit won’t always appear straight away. . In our design we only had to make changes to any affected services, which in our case was only one. We added a new API to an existing service which would consume an already implemented downstream service. And finally the big win was we could move this into production by releasing just one Microservice, with tools like  Apollo  the change could be made with no downtime in production. . I’m sure not all changes will be like this, but with the right upfront design of your services there is no reason why they can’t be. Using great design tools like the  Life Preserver  you can model services around components that change together, reducing the impact of those last minute change requests. . Some additional reading if you get time: .  Vivek Juneja - Microservices: Four Essential Checklists when Getting Started  .  Russ Miles - Building Adaptable Software With Microservices  .  Tony Mauro - Adopting Microservices at Netflix: Lessons for Architectural Design  ", "date": "2015-10-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Super, Smashing, Great in Barcelona\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/super-smashing-great/", "abstract": " I’ve just come back from  Smashing Conference in Barcelona  and first of all, I have to thank the organisers and speakers for a great event. . Secondly, I must apologise for the terribly predictable (for British people of a certain age) title, but as everyone with a passing knowledge of bad computer science jokes knows, naming things is difficult, and it’s always hard to resist an obvious pun. . My expectations for the conference were high.  Smashing’s event in Oxford last year  was excellent - an inspiring blend of talks, with a great mix of speakers on a range of topics across design, UX and development. Combine that with an opportunity to go back to Barcelona, and how could I resist? . Having spent a month in Barcelona 15 years ago, it was interesting to wander around and see what’s changed. I got a sense of gentrification - areas that had felt edgy then feel safe now, perhaps even a little sanitised. The same is probably true of a lot of places. It’s still a wonderful city, and it was great to be back there, even if just for a few days. . I also ate a ridiculous amount of  jamon , and have to thank  my friend Tom  for recommending tapas at  Bar Ramon . Between that, the excellent catering, and the free beer at the party, the conference didn’t do my waistline any favours. . Anyway, to the main point of going to Barcelona - the conference. . In short, I wasn’t disappointed. The speakers, the venue, and the atmosphere were all excellent. Even the  conference swag  from the sponsors was useful. . Monday was Smashing main man  Vitaly Friedman’s Responsive Design workshop . With a mountain of material to cover, at times it felt more like a long conference talk than a workshop, but there was a lot of useful stuff in there, and plenty of new additions to my reading list. . The conference itself was at the beautiful  Palau de la Musica Catalana , and there was a buzz of anticipation, helped along by  the band . I didn’t envy them - a roomful of geeks at 9am can’t be the easiest gig, but they gave it a good go, and warmed the place up for Chris Coyier. Chris hosts the  Shop Talk podcast  and runs  CSS Tricks  - both of which are great resources, so I was looking forward to his talk on SVG. It’s something I’ve always put off looking at, mainly because of the need to support IE8, but has been on my mental to-do list for a while. With some helpful examples of when SVG is appropriate,  featuring bears , he discussed a  workflow for SVG icons with fallbacks  that I definitely plan to investigate. . Jina Bolton from Salesforce gave an interesting insight into the challenges of designing and building  Living Design Systems  at enterprise scale. Her insistence on the value of a single source of truth really resonated with me.  Style guides have been a hobby horse of mine for a while , but I haven’t yet had the chance to implement them as fully as I’d like, and certainly nowhere near the kind of thing that Jina’s team are working on. . I wasn’t expecting a Flaubert reference in a tech talk, but the notion that  discipline can create freedom to be creative  was interesting. It may seem slightly like a Newspeak slogan, but the idea goes all the way back to  Aristotle . It made me think of the need for jazz musicians to learn music theory before they can be good improvisers. The other association in my mind was with the discipline of craftsmanship, whether that’s in making sushi or building software. . Another point Jina made was about valuing clarity over brevity. I know this will resonate with quite a few of my colleagues - I’m always banging on about how much I dislike acronyms. Which brings me on to HTTP2 and  Patrick Hamann’s talk on Embracing the Network . I’ve long admired the work done by his old team at The Guardian - even more so since I read about  the low weight of advertising on their site  - so I was keen to hear what he’s been working on at the FT. . This was probably the most developer-focused talk of the conference, and it gave me a  lot  to think about. A particular point of note was Patrick’s echo of Vitaly’s suggestion that the widespread adoption of HTTP2 may lead to some of our current best practices for front-end performance becoming anti-patterns. In the shorter term, I’ll be trying to implement some performance improvements on this site (and the  open source Jekyll theme we use for it ) as soon as I get a bit of time. . Just like last year, there was one talk that I suspected might be for typography geeks only, and it was from type designer and calligrapher  Seb Lester . I was pleasantly surprised. In particular, I was struck by Seb’s suggestion that  “ play working is the best investment you’ll make ” , which reinforced a lot of my thoughts around  the value of side projects . . He also brought home the difficulty of pricing projects  with a story about his logo design for a NASA mission . . One of the main themes that emerged through the conference was the tension between data and creativity, which was the core of  Joe Leech’s talk . As with his talk in Oxford last year, it was an interesting presentation, with some valuable insights from applying his background in psychology to user experience. The key takeaway for me was the importance of knowing what data is worth gathering, and focusing on a limited number of measurements - in other words, knowing “what number is most important to drive decisions”. . Self-confessed pencil geek Brendan Dawes closed the first day with tales of his  digital art projects using Paper, Plastic &amp; Pixels . As with Seb’s session, it was a talk which perhaps can’t be directly related to my day-to-day work, and perhaps my employers might wonder if sending me to talks like this is good value for money. . However, for me the point of talks like this isn’t that they provide you with practical applications that you can immediately put into practice in your day job. It’s that they can inspire you to take a more imaginative approach, to see the possibilities that might be out there, and that they can be invigorating, even if you’re working on projects that may seem less obviously creative. . It was interesting to hear him talk about “the battle to reduce the gap between the thing in your head and the thing you make”, which must be familiar to anyone who’s worked on any creative project. Another point he made was that it’s easier to talk about things that exist, rather than in the abstract. This isn’t just true for art projects - for me, it’s one of the most valuable points of an iterative, agile approach to anything. . On the Tuesday night was the party, and when the beer abruptly stopped flowing, there were some good-natured comments about people organising events in breweries, but it was probably for the best, with the amount of information to take in from the talks. . A tradition of Smashing Conference is the Mystery Speaker - last year in Oxford  Seb-Lee Delisle’s laser show  blew everyone away. Having spotted Bruce Lawson around the conference the day before, it wasn’t a big surprise to see him on stage, although I must admit that the  unicorn onesie  was unexpected.  Bruce’s talk  was a welcome insight into the relationship between browser vendors, the W3C, and web developers - with the positive message that change is achievable. . Next,  Anna Debenham  showed us that the browser and device landscape is much more diverse than most of us imagined, with  surprisingly high usage of games console browsers  - yet more devices to care about, and yet another reminder that device detection isn’t a sustainable strategy. As ever,  from so many points of view, progressive enhancement is the way forward . . Zoe Mickley Gillenwater’s talk on Flexbox at Smashing Oxford got the whole conference talking, and it was the tipping point for me (and many others) to finally start implementing flexbox as progressive enhancement. I’ve lost count of the number of times I’ve shared a link to  her article on Flexbox , so I wasn’t entirely surprised that some of the content was familiar. Even so, it was a valuable session - the main takeaway for me was being reminded of the utility of flexbox, especially in situations where the size of content is unknown (i.e. always), which is an even bigger issue on multilingual sites. . The next speaker, Espen Brunborg, gave us more inspiration, asking whether  the internet is killing creativity . Pointing out that “ the internet is a crazy scary wonderful place ”, he drew parallels with print design, pointing out that the evolution of the web industry has a long way to go, and gaving us hope that we can push things forward. He also made an important point about the place of content in the design process, and the danger of building CMS-based sites with Lorem Ipsum. I agree about  the value of using real content , but I’d question his suggestion that you need to bypass the CMS if you want to create individually designed pages - for instance in Drupal, preprocess functions or the  ThemeKey module  would let you display specific content differently to the rest of the site. . Andrew Tider and Jeff Greenspan’s work lies somewhere in the spaces between technology, guerrilla art, improvisational comedy, and political activism. In a mildly NSFW talk, they promised  “We’ll Teach You Everything We Don’t Know” , reminding us of  the value of pushing yourself beyond your comfort zone, and the exciting possibilities of starting things without knowing where you’ll end up , of “stepping into the unknown”. Again, it calls back to the value of iteration in an agile workflow, as does their reminder of the value of learning from failures: “One project’s struggle may be another project’s preparation”. . Another inspiring message that stayed with me was their  succinct echo  of the idea, so popular in open source, of  forgiveness being easier to get than permission . . Zach Leatherman provided plenty of useful insight and practical suggestions for handling web fonts, echoing Patrick’s suggestion that they can be a single point of failure, and emphasising the need to optimise performance, with the danger of the  Mitt Romney webfont problem . With everything I’ve learned this week from Zach,  Chris Coyier , and  Bruce Lawson , I’ve finally come round to the idea that SVGs are the way forward for icon systems. . Andrew Clarke rounded things off, as he did in Oxford, with more of a conceptual talk, touching on the lessons from advertising, and the difficulty of balancing creativity with processes and systems. A lot of Andrew’s ideas provided a callback to Espen’s discussion of the way that so many websites are becoming predictable. Again, a lot of food for thought. . My flight time meant that I had to miss the photowalk, but coming home with my mind (and my Evernote) full of ideas, I know that I’ll be revisiting a lot of the talks via the  collaborative notes  and the slides and videos, once they’ve been  collected on Lanyrd , and hopefully I’ll be able to go to another Smashing conference before too long. ", "date": "2015-10-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Stylish Unit Tests\n", "author": ["John Nash"], "link": "https://capgemini.github.io/development/unit-test-structure/", "abstract": " Back in March I went to  DEVWEEK 2015   and listened to a talk by  Kevlin Henney  called  Programming with GUTs . (You can watch the video behind the link. It’s a good talk.) . Since then I’ve been trying to write my unit tests in a different style. . I’m not talking about   TDD   here (although that’s good too and if you’re not doing TDD, you should try it). This is a naming and structural style that I think has brought my unit tests closer to a readable specification of my production code. . The first thing I’ve changed is what I call my test classes. . Let’s say I had a class to test called  Foo . In the past I’d create a test class called  FooTests  (or more likely I’d ask my IDE to create it for me). . I don’t think that tells the reader very much. We know the class contains tests because it’s probably in a test project or folder and, just in case there was any doubt, it will also appear in our test report or the IDE’s test report window. . Instead I now create a class called  AFoo  or, if the class was static or a Singleton I’d call it  TheFoo . That tells you something about the class under test (how many instances of it are expected to exist) and it also ties nicely into the way I’ve changed my test methods (more on that in a below). . The next thing I’ve changed is the relationship between production class methods and test methods. . In the past I’d let my IDE create a test method for each public method in my production class. That lead to a lot of repetition in my unit tests as testing one method invariably required that I call a different method to set up the test. . Imagine I’m testing a Cache class. . I could write a test for the  put  method. It would add a value and then check that the cache contains it. . That works, but now when I move on to write a test for the  contains  method I realise that I’ve already tested that it can tell me if the cache contains a value. I did that in the test I just wrote. . I could ignore this and carry on, but if I do I’ll need to write a test that puts a value into the cache so I can check that the  contains  method works. That’s an exact repeat of the earlier test. . What I need to do is test the behaviours of the class and behaviours often span multiple methods. Once I accepted this, it didn’t make sense to name my test methods after the production methods, so I started to name them after the behaviour. . Combine this with the earlier change in test class naming and you get something like this. . I think that reads pretty nicely, not just in the source code, but even better in the test report. .   . As you add unit tests you build up a specification of the Production code. . (Of course this all assumes that you aren’t using JUnit 3. In JUnit 3 you need to prefix your test method name with  test  otherwise it won’t be run.) . Sometimes, as I write my tests in this new style, I find it hard to think of the correct name for a test method. Imagine I’ve got a service that doesn’t accept bad configurations. What I want to write is something like this. . But there are lots of different ways that a configuration can be invalid. If I write a test for each configuration corruption I’d end up with something like this. . That’s going to swamp any other tests I write for the service, making it harder for the reader to see the service’s specified functionality. . I could put all those test cases inside the single  rejectsInvalidConfigurations  method, but that would create a very large, difficult to maintain method. It would also make it less obvious which test case was broken when the test fails and it would miss the opportunity to tell the read what constitutes a valid or invalid configuration. . What if I break the one to one mapping between production class and test class? Then I’m free to write tests about concepts other than the production classes. . Even though the concept of configuration validity lives in the Service class (that’s where it is checked) I can name my tests as if it lived in another non-existent entity, an  invalid configuration . . Now that I’ve specified what an invalid configuration is, I’m free to use my original test name that I wanted to use all along ( rejectsInvalidConfigurations ). . I’ve been using this test style for six months now, on both my own code and client projects and I’m pretty happy with it. I haven’t come across a scenario where this approach makes the tests less readable. . More importantly, colleagues who have reviewed my code have started to adopt this style too. I find that a reassuring endorsement. . Give it a try and see if it works for you. ", "date": "2015-10-30T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "New Year, New Career?\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/blog/capgemini-hiring-fatjil/", "abstract": " If your New Year’s resolution will be ‘find a new job’ now’s the time to start looking for that new and exciting opportunity. . On Thursday 26th November Capgemini will be hosting the  Find A Tech Job In London (FATJIL) meetup  at our Holborn office – complete with free beer and pizza! . Come along to talk with a whole host of people from the Digital Solutions Unit (DSU) - one of the largest delivery units in Capgemini UK; which works with some of the world’s largest brands, on some of the most interesting and complex projects, helping clients achieve business value though digital transformation and the application of social, mobile, analytics and cloud technologies. . There will be the opportunity to meet many of the contributors to this blog, from a variety of roles within the organisation, and find out what working at Capgemini is really like. We’ll also be sharing some insight on the innovative projects we’re working on and what our future looks like. . We are currently recruiting permanent positions for: . Software Engineers - Java/JVM, .NET, Integration, Drupal, DevOps and Mobile . CRM Architects, Functional and Technical Consultants – Salesforce and Dynamics CRM . UX Designers . Test Managers and Analysts . Project Managers and Business Analysts .  Sign up on  meetup.com , visit our  careers site  for more information  or connect with us on  Linkedin .  ", "date": "2015-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Meet our Team\n", "author": ["Levi Govaerts"], "link": "https://capgemini.github.io/blog/team-promo-movie/", "abstract": " In a  recent blog article ,  Tom spoke about the “Find a Tech Job In London” meetup that was hosted by Capgemini. If you missed the meetup or if you’re still looking for an exciting opportunity, why not consider joining Capgemini? Join our Software Engineering team and discover how fun it can be to be a part of a team that works on challenging projects. . Some of our team members (including yours truly) were asked to pick up our actor role and got in front of the camera to describe life as a software engineer at Capgemini. You will not only get informed about what we are doing on a day-to-day basis but you will also discover some very interesting facts about your (possible) future colleagues! . We are currently recruiting permanent positions for: . Software Engineers - Java/JVM, .NET, Integration, Drupal, DevOps and Mobile . CRM Architects, Functional and Technical Consultants – Salesforce and Dynamics CRM . UX Designers . Test Managers and Analysts . Project Managers and Business Analysts .  Visit our  careers site  for more information or connect with us on  Linkedin .  ", "date": "2015-12-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "DevOps Legacy\n  ", "author": ["Ant Broome"], "link": "https://capgemini.github.io/devops/devops_legacy/", "abstract": " We all know that a DevOps culture should be a core principle of any new projects, and there’s plenty of blog posts to get us started - just take a look at some of our  previous posts . But what happens when you’re running various legacy solutions created long before the concepts of DevOps were born? When we tackle legacy solutions we can apply the  cultural elements of DevOps but what about the technical ones? Much like Flynn in Tron, we strive to build a “perfect” computer system that can manage itself, but where do we start? . ###1. Metrics How long does a deployment take? What was the downtime to the users? How much time does it take to prepare a release? How many tests have we got passing? The list is endless but we can’t definitively say we’re improving the situation until we know where we’re starting from. . ###2. Developer Environments Automation starts with the development team. A scripted environment setup allowing the team to create, update and destroy their environments is a good place to begin. For an introduction take a look at  my previous post . This gives developers the tools to push the boundaries and experiment safe in the knowledge they can get back to a working environment no matter what. Using configuration management tools like  ansible ,  puppet  or  chef  we can reuse a lot of the scripts in other environments. This consistency in setup across environments allows for reusable deployment scripts. . ###3. Deployment Scripts Deployment is often the most complex part of dealing with legacy systems. Generally speaking, most legacy systems will have a web services api, command line interface or custom scripting solution for automation. The challenge is finding enough about it in the documentation. Start by creating basic build scripts that can be run from the command line to interact with these interfaces. These small individual deployment scripts are your building blocks to more complex deployment. Sometimes this will involve creating a thin client that we can run from the command line. As with the automation of environments, giving the development team early access to the scripts will give faster feedback loops to improve the deployments further. For example, in one instance we used  IBM Websphere  and to automate deployments we created a thin-client with a reusable  jython  script to deploy WAR and EAR archives. This meant that developers could deploy updated applications without going through the web UI. . ###4. Continuous Integration Now we’ve got the scripts to deploy our code, and we can run them ad-hoc we’ll want to take things further. We use CI tools to deploy artefacts to testing environments using the same scripts as we did in development and will do in production. This gives us a level of consistency and speed. Once we know the deployment is reliable and repeatable, moving the same release through all the environments isn’t the headache it once was. Continuing the Websphere example above, we installed the newly created thin-client on our jenkins infrastructure. This enabled developers to configure nightly deployments for the testing environments. This even allowed testers to deploy the latest successful builds themselves, this empowered them and free’d engineers to do other things. . ###5. Analytics Using the logs in the CI server we can go now full circle and compare how long our various deployments are taking, look for bottlenecks and try and run much of the deployments in parallel. This allows us to make changes to the script, give it to developers and repeat the process again - iterate. In our Websphere example we found a number of teams were deploying multiple applications in a single CI build, the script were changed to cater for multiple application deployments in a single request, this reduced the deployment time significantly. . Nothing! It shouldn’t make any difference how many legacy solutions are involved. Legacy systems will need a little more effort to get an automation solution in place but often once you do the benefits can be significant. I’ll leave you with one final Tron reference before you get back on your neon bike; with DevOps “the game has changed.” ", "date": "2015-11-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Is REST Best in a Microservices Architecture?\n", "author": ["Craig Williams"], "link": "https://capgemini.github.io/architecture/is-rest-best-microservices/", "abstract": " During my journey into microservices, it has become apparent that the majority of online sample/howto posts regarding implementation focus solely on REST as a means for microservices to communicate with each other.  Because of this, you could be forgiven for thinking that RESTful microservices are the de facto standard, and the approach to strive for when designing and implementing a microservices-based system.  This is not necessarily the case. . The reason why REST based microservices examples are most popular is more than likely due to their simplicity; services communicate directly and synchronously with each other over HTTP, without the need for any additional infrastructure. . As an example consider a system that notifies customers when a particular item is back in stock.  This could be implemented via RESTful microservices as so: .   . An external entity sends an inventory update request to a REST gateway address. . The gateway forwards this request to the Inventory Manager Service. . The Inventory Manager updates the inventory based on the request it receives, and subsequently sends a request to the Back in Stock Notifier. . The Back in Stock Notifier sends a request to the Subscriber Manager, requesting all users that have registered to be notified when the item is back in stock. . It then emails each in turn, by sending an email REST request to the Email Service for each user. . Each service then responds in turn, unwinding back to the gateway and subsequently to the client. . It should be noted that although communication is point-to-point, hard coding services addresses would be a very bad design choice, and goes against the very fundamentals of microservices.  Instead a Service Discovery mechanism should be used, such as  Eureka  or  Consul , where services register their API availability with a central server, and clients to request a specific API address from this server. . Diving deeper, there are a number of fundamental flaws or things to consider in this implementation: . Due to the synchronous nature of REST, the update stock operation will not return until the notification service has completed its task of notifying all relevant customers.  Imagine the effect of this if a particular item is very popular, with 1000s of customers wishing to be notified of the additional stock.  Performance could potentially be severely affected and the scalability of the system will be hindered. . The knowledge that ‘when an item is back in stock, customers should be notified’ is ingrained into the inventory manager service but I would argue that this should not be the case.  The single responsibility of this service should be to update system inventory (the inventory aggregate root) and nothing else.  In fact, it should not even need to know about the existence of the notification service at all.  The two services are tightly coupled in this model. . Services WILL fail, and a microservice based system should continue to function as well as possible during these situations.  Due to the tightly coupled system described above, there needs to be a failure strategy within the Inventory Manager (for example) to deal with the scenario where the Back in Stock Notifier is not available.  Should the inventory update fail? Should the service retry? It is also vital that the request to the Notifier fails as quickly as possible, something that the circuit breaker pattern (e.g.  Hystrix ) can help with.  Even though failure scenarios will have to be handled regardless of the communication method, bundling all this logic into the calling service will add bloat.  Coming back to the single-responsibility issue, again it’s my opinion that the Inventory Manager should not be responsible for dealing with the case where the Notifier goes dark. . One way to overcome the coupling of services and to move the responsibility of routing away from a microservice is to follow the pipeline enterprise pattern.  Our subsystem would now look like this: .   . Communication may still be REST based, but is no longer ‘point-to-point’; it is now the responsibility of the pipeline entity to orchestrate the data flows, rather than the services themselves.  Whilst this overcomes the coupling issue (and blocking with a bit more work, via asynchronous pipelines), it is considered good practice within the microservices community to strive for services that are as autonomous and coherent as possible.  With this approach, the services must rely on a third party entity (the pipeline orchestrator) in order to function as a system and are therefore not particularly self sufficient. . For example, notice that the pipeline will receive a single response from the Back in Stock Notifier (even though there are 2 subscribers), but must be configured in such a way that it can parse the response so that it can subsequently send individual “send email” requests to the Email Notifier for each subscriber.  It could be argued that the Email Sender could be modified to batch send emails to many different subscribers via a single request, but if for example, each users name must be included in the email body, then there would have to be some kind of token replace functionality.  This introduces additional behavioural coupling, where the Notifier has specific knowledge about the behaviour of the Email Sender. . In a messaging based system, both the input and output from services are defined as either commands or events.  Each service subscribes to the events that it is interested in consuming, and then receives these events reliably via a mechanism such as a messaging queue/broker, when the events are placed on the queue by other services. . Following this approach, the stock notification subsystem could now be remodelled as follows: .   . Cohesion is obtained via a shared knowledge of queue names, and a consistent and well known command/event format; an event or command fired by one service should be able to be consumed by the subscriber services.  In this architecture, a great deal of flexibility, service isolation and autonomy is achieved. . The Inventory Manager for instance, has a single responsibility, updating the inventory, and is not concerned with any other services that are triggered once it has performed its task.  Therefore, additional services can be added that consume Inventory Updated events without having to modify the Inventory Manager Service, or any pipeline orchestrator. . Also, it really doesn’t care (or have any knowledge of) if the Back in Stock Notifier has died a horrible death; the inventory has been updated so it’s a job well done, as far as the Inventory Manager is concerned.  This obliviousness of failure from the Inventory Manager service is actually a good thing; we MUST still have a strategy for dealing with the Back in Stock Notifier failure scenario, but as I have stated previously, it could be argued that this is not the responsibility of the Inventory Manager itself. . This ability to deal with change such as adding, removing or modifying services without affecting the operation or code of other services, along with gracefully handling stressors such as service failure, are two of the most important things to consider when designing a microservices based system. . Everything in the world of asynchronous messaging isn’t entirely rosy however, and there are still a few pitfalls to consider: . The programming model is generally more complex in an asynchronous system compared to a synchronous counterpart, making it more difficult to design and implement.  This is because there are a number of additional issues that may have to be overcome, such as message ordering, repeat messages and message idempotency. . Also, the configuration of the message broker will also need some thought.  For example, if there are multiple instances of the same service, should a message be delivered to both of the services or just one? There are use cases for both scenarios. . The fact that the result of an action is not returned immediately can also increase the complexity of system and user interface design and in some scenarios it does not even make logical sense for a subset of a system to function in an asynchronous manner.  Take the Back in Stock Notifier for example, and its relationship with the Subscriber Manager; it is impossible for the notifier to function without information about the subscribers that it should be notifying, and therefore a synchronous REST call makes sense in this case.  This differs from the email sending task, as there is no need for emails to be sent immediately. . Due to the dispersed and autonomous nature of messaging based microservices, it can be difficult to fully get a clear view of the flow of messages within the system.  This can make debugging more difficult, and compared to the pipeline approach, the business logic of the system is harder to manage. .  Note:  Event based messaging can expanded even further by applying event-sourcing and CQRS patterns, but this is beyond the scope of this article.  See the further reading links for more information. . So which communication approach is best when designing your microservices?  As with most things in software development (and life??), it depends on the requirements!  If a microservice has a real need to respond synchronously, or if it needs to receive a response synchronously itself, then REST may well be the approach that you would want to take.  If an enterprise requires the message flow through the system to be easily monitored and audited, or if it’s considered beneficial to be able to modify and view the flow through the system from one centralised place, then consider a pipeline.  However, the loosely coupled, highly scalable nature of asynchronous messaging based systems fits well with the overall ethos of microservices.  More often than not, despite some significant design and implementation hurdles, an event based messaging approach would be a good choice when deciding upon a default communication mechanism in a microservices based system. .  Netflix OSS  - The home of Eureka and Hystrix .  Consul  .  Antifragile Software  - By Russ Miles .  Event Sourcing  - By Martin Fowler .  Building and Deploying Microservices with Event Sourcing, CQRS and Docker  - By Chris Richardson ", "date": "2015-12-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Creating a CXF REST service with Camel Blueprint\n", "author": ["Phil Hardwick"], "link": "https://capgemini.github.io/development/creating-a-cxf-restful-service-in-camel-blueprint/", "abstract": " Before diving into the main content I’ll just give some short snippets about the technology used here. .  Camel  is a framework which implements all the (widely-used) enterprise integration patterns and allows for communication between multiple transports (JMS, HTTP and others) through routing. .  Apache CXF  implements the  JAX-RS  specification and Camel provides support for it through the  CXFRS  component. When using this combination to develop REST services there’s been a number of things I’ve wanted to do which haven’t been immediately obvious. This blog will be looking at the parts of developing and testing a REST service, and explaining some of the gotchas involved. . The second interesting aspect of this is using Blueprint as the dependency injection mechanism for deploying this OSGi ready service. Blueprint can be difficult to test but hopefully that can be made easier with the tips that follow. . My resource is a simple blog post which allows one operation on it: getting a single post by id using a path parameter. Define your resources as follows: .  The first media type defined in your produces tag will be the default.  If a client does not specify an Accept header on its HTTP request then the service will default to returning the first MediaType in the Produces tag. . I use the fact that the CXFRS component puts the method name of the resource in the operationName header to route each request via a recipient list to a direct endpoint to process it. The restEndpoint field is set by blueprint, utilising it’s DI and default property capabilities. The inline processor simply returns a Post object. For anyone not familiar with Camel, the route is by default a request-reply (or in-out in Camel’s language) route which means as long as you don’t send it somewhere else like an activemq queue then the exchange will be routed back to where it came from i.e. the CXF server, Camel takes care of all the http responding and marshalling for you which is excellent. . Finally, wire it all up in the blueprint. I have given my property placeholder element a persistent-id and an update strategy of none. Using these default properties is what makes the service testable because I can swap the service url to a  local url in my test. The cxf-server needs to know about it’s resources in the serviceBeans element and it’s providers which automatically give you marshalling to JSON or XML for free. (As long as an annotated object is in the body when it reaches the end of the Camel route). . Using camel-blueprint-test is the hardest part of this as it requires a lot of hidden dependencies. However, it is a useful test to have and whilst it is slow I would recommend having at least one test in your suite to ensure your blueprint file deploys. It’s main drawback above the flakiness is it’s speed, which is a lot slower than CamelTestSupport based tests. So use CamelTestSupport for most tests and CamelBlueprintTestSupport for a single test. . The key to being able to test it is substituting an absolute local URL in for the cxf-server’s address in the blueprint, so I have used a property to be able to switch it at test time. This can be achieved with a simple CamelTestSupport based test as well by setting the restEndpoint field on the route builder to an absolute local URL like so: cxfrs:http://localhost:10001/api/1/blog. . The persistent id returned from useOverridePropertiesWithConfigAdmin must be the same as the persistent-id attribute in the property-placeholder element in the blueprint file . If your properties being substituted in from useOverridePropertiesWithConfigAdmin aren’t being deployed when the Camel  context starts then specify update-strategy=”reload” . Make sure asm-all:4.1 is the first dependency in your pom file’s dependency list. . If you aren’t receiving a message through your route then check that the route you expect is actually being started.  Blueprint testing takes only the first Camel context it finds and loads that. To make sure it always loads your route  use the bundle filter method. The below example will load any Camel context it finds except bundles whose names start with bundle.name.unwanted and bundle.name.unwanted2: . If you are still receiving exceptions the best thing may be to update the version of your blueprint-test dependency to  the latest version to avoid exceptions such as felix complaining about a null pointer in the main thread and  “NoSuchComponentException: No component with id ‘blueprintBundle’ could be found” appearing in your logs. . It’s very important to have logging enabled to diagnose issues if your blueprint test is not starting correctly.  Issues such as blueprint giving up waiting for service is indicative of a wrong blueprint file with missing ids or   incorrectly specified beans. . This test allows you to use a real HTTP client to test that your service implementation will work when deployed. This is a much better test than overriding the endpoints with directs and simply testing the processing in the route because it gives you confidence in your specification of the CXF server. . The end result is a real CXF service being run at test time enabling you to test your full implementation before it’s even been deployed! . The full project can be found here:  https://github.com/PhilHardwick/cxf-blueprint-camel-example  .  CXFRS component  .  Marshalling in CXF  .  Blueprint Test  .  Camel Property Placeholder  ", "date": "2016-01-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Techniques on transforming your team to create a better culture\n", "author": ["Shana Dacres"], "link": "https://capgemini.github.io/development/techniques-for-a-better-culture", "abstract": " We are all individuals - our experiences and personality affect who we are and how we see the world. We all learn in different ways and grasp concepts at different rates. Some of us need to fully understand how a piece of technology works before implementing / using it, while others can just run with a basic understanding. Knowing this, why do we sometimes assume each of us work and learn in the same way as ourselves? Here are a few techniques on getting the best out of your team by understanding each of your roles, creating a safe environment, and transforming your workforce so that you can all grow and learn from each experience. . In any environment it is essential to know how you are perceived. It is also important to understand the motivations of each of your team members in a workplace. I have found, colour personality profiling, determining a person’s personality type based on the work of  Carl Jung , really useful to identify the types of people in my teams. Jung uses colour energies to identify the four basic personality types. .   . He explains that each person will exhibit tendencies of each colour to differing degrees at different points, which is why the four personality types can be split further into eight different behavioural types. What we can take away from this and apply to our teams, is knowing how to spot the dominant colour within ourselves and each other to best manage interactions and build on relationships. Colour personality profiling can help each team member to understand theirs and others role and the contribution they play in helping the team succeed. .   . In my opinion, the best way to find out someone’s colour profile is in group environment. You will be able to spot the more introverted, reserved individuals whose primary focus may be problem solving and wanting to be correct. Versus the extroverts within your team who are motivated by the company of others and can easily influence those that they interact with.  Knowing this, it is no wonder then that strongly orientated extraverts and introverts see things in different ways, which can cause conflict and misunderstanding. Two people may look at the same situation and yet see different things. One team member may be able to thrive better by bouncing ideas off the group, while the cool blues among us may do better when you give them a quiet minute to properly think through their ideas before presenting them to others. . Now, I’m not saying go ahead and learn each percentage of colours of each team member, but what I am suggesting is to try and pull out their dominant colour so that you can get a better gauge of who they are and their motivations, so that you can be adaptive in your approach when communicating with them. Colour profiling will help you work to your team strengths and motivate your team in such a way they feel more driven. . Don’t let the fear of being wrong rule the day. For new ideas to be adopted, we need to make room for experimentation and a trial and error approach. Experimenting with new ideas can lead to new ways of solving problems. Nobody wants to work in a toxic environment where they fear taking risks because of the consequences that may follow. As leaders we need to make sure that the environment which we create for our workforce is one where our team is comfortable being uncomfortable. Let them take a risk, let them change the game; there are no repercussions for getting things wrong just as long as you have learnt from it. Healthy discomfort comes from pushing the envelope on new approaches, while managing and monitoring risks and learning in the process.  . Enabling experimentation brings out creativity. Once a problem or a new idea have been encountered, give your team the confidence and incentive to solve it until problem solving just becomes another part of your agile culture. One way of finding time to experiment on new ideas and solving problems is by having your core team working towards your everyday goal, while a few others are experimenting with the new idea /problem. Not all experiments will lead to a breakthrough, and this is still a good thing as you will be able to make a valid decision based on the outcome. Successful innovation is all about making decisions based on data-driven insights gained from experimentation and trials. . When it come to the end of a project / delivery one of the things I have learned is to always have a wash-up session where you cover all the lessons learned. This is an ideal way to go over the highlights and lowlights which has occurred. It also gives you time to reflect as a team, so you can appreciate what your team do well, and what areas you may need to work on. One of the most valuable activities I have come across in a wash-up is a Failure SwapShop. This is where each team member comes to the wash-up with one failure. This could be something small as not asking for help when they needed it, to missing a deadline due to bugs in their code. The idea is you go around the room taking it in turn to explain your failure, then (but most importantly) the lesson(s) learned from it. . So something like this… .        Introduction   Hi, my name is Shana, and I have failed. Group Acknowledgement – Cheers and claps     .  Introduction   Hi, my name is Shana, and I have failed. Group Acknowledgement – Cheers and claps .        Explain the failure     I failed because I did not put enough effort into testing my bug fixes before merging it in with the master branch.     .  Explain the failure     I failed because I did not put enough effort into testing my bug fixes before merging it in with the master branch. .        Explain the lesson learned   I have learned that I should spend more time in testing my branch before requesting a merge, as it only slows down the process.  I have also learned that fixing one issue may have a knock on effect on to the rest of the application, so don’t just test the fix, try to test everything associated with it. Group Acknowledgement – Cheers and claps     .  Explain the lesson learned   I have learned that I should spend more time in testing my branch before requesting a merge, as it only slows down the process.  I have also learned that fixing one issue may have a knock on effect on to the rest of the application, so don’t just test the fix, try to test everything associated with it. Group Acknowledgement – Cheers and claps . The main idea here is that everyone shares and reflects on their failure in a safe way and each lesson learnt is celebrated with a round of applause – because we are all learning from our mistakes. I find this to be an effective technique to acknowledge and grow from failures as a team. We can learn from others experiences. As they say; a smart person learns from their mistakes and a wise person learns from the mistakes of others. . At the end of the day, true failure is when you have failed and don’t learn your lesson. So create a culture where sharing information, even about negative things, is desirable. Where failing will have a positive effect on the growth and culture of your team. ", "date": "2016-01-15T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Specialism Constrains Throughput\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/agile/specialism-constrains-throughput/", "abstract": " Tim, a good developer, has been working on a story. “Fred, I need the build configuration changed to pick up the new component I’ve added.” . Fred responds, “okay, I can fit that in next week.” . “Oh, it’s the demo on Tuesday and we need to deploy to test it before then.” . “Well I’m busy with other tasks at the moment, perhaps you should have planned ahead.” . – . This sounds reasonable on behalf of Fred, the Build Engineer. He has limited availability and needs his work planned out. . But that is  exactly  the sort of behaviour agile practices are trying to move away from. We want to move towards shorter cycles and faster feedback. We want to be able to do things now. This is the kind of problem the DevOps mentality is  supposed  to resolve. . I had the privilege to hear  Chris Matts speak at Agile Cambridge  a couple of years ago. He was talking about options and availability of people. How do we make people available? Get them to do  less . This is counter-intuitive - how do we get more done by doing less? Surely we want to give our best people (our specialists) the  most  work? . It turns out high utilisation doesn’t necessarily lead to high productivity. Plans are fragile as estimates are a guess; plans are fragile as unexpected work arises; plans are fragile as people aren’t always available. Delays lead to blocked work. . Chris’s suggestion was to resolve this by  not  assigning work to the specialist. Better to get them to mentor and guide others to raise their skill level and to leave the specialist free to tackle emergency issues when they arise. Over time this resolves the dependency on key individuals and, overall, makes them available for high value work. .  The Dreyfus Model of Skill Acquisition  describes a few levels of skill, with “expert” being the top level. . It is these experts that are in short-supply. It is these experts we want to work on the critical aspects of our work. But it is also these experts that we build dependencies on. . A better model is to raise the level of the team, over time, and only consult “the expert” when needed. Of course this doesn’t mean the expert cannot help out day-to-day, they can even take on some important but lower-priority work. They can also add a lot of value by reviewing work output of others and helping improve quality and technique. . Generally speaking, I’ve found that experts “go their own way” anyway. It’s very difficult to tie-down an expert to a rigid plan. They are best off deciding for themselves where best they can add value. . There may of course be a few areas of specialist knowledge. Chris’s suggestion was to have a skills matrix and try to bring everyone up to a certain level. This won’t necessarily be even across the team as people have different interests and skills, but the idea is for everyone to have a broad level of competency and one or two stronger areas. . Let’s say each skill for each person is ranked 1 (novice) to 5 (expert). We probably want to bring everyone up to level 2 for all skills and have a few team members at level 4 for all skills. I haven’t heard the term “generalising specialist” used for a while now (some people thought it an oxymoron!), but I like the term and I think this is what it meant. Not quite “Jack of all trades, master of none”, more good at everything, but great at some. . But note I didn’t suggest anyone at level 5, expert. I think such people are rarely good across the board (how would they have time?) and are in so much demand that it’s difficult to tie them into a team. Better to treat them as an advisor or a consultant. . I do know one person who is actually a specialist-generalist. By this I mean he specialises in being able to pick things up quickly, being flexible, being adaptable, being approachable and most of all a great problem solver. It is this broad range of skills mixed with great soft-skills (which are underrated in his case, even by himself) which mean he is always in demand. . Part of the problem described in my fictional (!) conversation above is a lack of trust between the build engineer and the developer. . Of course there needs to be a certain level of control, but better to trust people to do the right thing and address the occasional problem rather than never do the task and never learn. . So how might that conversation go? Perhaps something like: . Tim, a good developer, has picked up a story. “Fred, I’ve changed the build configuration to pick up the new component I’ve added. It’s built and deployed to the test environment so it looks okay.” . Fred responds, “ah, I had noticed the build was taking a little longer than usual, I’ll see if I can do something to speed it up.” . “It’s the demo on Tuesday so we need some stability around then.” . “I’m busy with other tasks at the moment, the build should run okay for now but if there’s a problem give me a shout.” ", "date": "2016-01-22T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Writing custom fields in Drupal 8\n", "author": ["Deji Akala"], "link": "https://capgemini.github.io/drupal/writing-custom-fields-in-drupal-8/", "abstract": "  Based on my presentations at DrupalCamp London, on Saturday 28th February 2015 and DrupalCamp Bristol, July 4th 2015  . Fields are the data entry points to a web application. Usually, they provide HTML elements and may be responsible for any manipulation of data before it goes into and comes out of the application. The data captured from a single field can be simple or complex. . Assuming we want to create a field for country, all we need is a single HTML element - textfield or select options. An address field, on the other hand, is a collection of discrete data which may include standalone simple fields including a textfield representing postcode (provided by a core module) and a country field (maybe from a custom or contributed module). .   . In the Drupal world, when considering solutions the first phrase you may hear is, “there’s a module for that!”. However, for the task at hand, “you need a module for that!”. We are now going to write a module which provides this custom country field.. . To create a new field in Drupal 7, a number of hooks need to be implemented in a custom module and these include the following: . hook_field_info() - the field type definition as well as its settings. . hook_field_schema() - the database schema for the field structure. . hook_field_widget_info() - the widget types to use for the field type. . hook_field_formatter_info() - the display of field values. . The task in Drupal 8 is founded on the same principles although the implementation differs. The first thing to remember here is, “there is a class for that!”. A lot of the hard work has been done in Drupal core and all we need to do is extend some classes and override default methods to suit our implementation. .   . All contrib and custom modules should be placed inside the “modules” folder in your site root. Core-related code is now in “core”. However, it’s also best practice to have “contrib” and “custom” sub-folder in “modules” for clear separation of these types of modules. So we’ll create our “country” folder under modules\\custom. What used to go inside *.info file is now in country.yml, so we create that file too and add the following: . Inside your module directory, you need a “src” subdirectory to keep all your class files. To organise things further, you need a “Plugin” sub-folder in “src”. There are different types of plugins e.g. fields, actions, blocks and menus. So you need to create another folder called “Field” inside Plugin and you’ll end up with a directory structure like src\\Plugin\\Field .   . Next, we need to define our data type, widget and formatter. These will be in classes with each occupying its own folder again. Let’s take them one by one. . The folder is called FieldType, so create it - src\\Plugin\\Field\\FieldType. Create a class, which we shall call “CountryItem”. The file is called CountryItem.php and in it we should have: . How do we define our field type? With the new plugin system, this requires an annotation  1   - something like a special comment block to allow core classes know about our field type. Your code should now look like this: . The information provided in our annotation is quite similar to that provided when implementing hook_field_info() in Drupal 7. Next, at the top of our file, we add a few things like namespaces and import required core classes. . Then we make our class inherit from core FieldItem class by extending it. . There are two functions we must implement in our class -  schema() and propertyDefinitions(). They’re as follows: . Here we define the schema for this field. The column is to be called “value”, and will hold a 2-character string, representing the ISO-2 name of countries.  Oh, don’t forget to add the constant for the length in your class: . However, we need to add two methods to make our life easier. Firstly, we want to know when our field is considered empty which is what hook_field_is_empty() does in Drupal 7. Then, we want to add some validation so that the country code we want to store doesn’t exceed the maximum length we have defined for our schema. When we are through, our class should look like this: . Now we have defined our data type and we want to store the ISO-2 country code. How do we want users to input data? We have two options - select dropdown options and an autocomplete textfield. The select options can be the default widget. . We start by creating a class called CountryDefaultWidget in src\\Plugin\\Field\\FieldWidget\\CountryDefaultWidget.php with the following code: . There’s still an important thing missing from our widget class - annotation of the class as provider of a FieldWidget. Add this just above the class statement; . This is similar to the old array keys for the old hook_widget_info() in Drupal 7. Additional annotation keys may be defined by a hook_field_widget_info_alter() function. . Our CountryDefaultWidget class isn’t complete yet. Widgets handle how fields are displayed in edit forms. The missing method we need to implement will do this for us. Add this formElement() method: . Other modules may alter the form element provided by this function using hook_field_widget_form_alter() or hook_field_widget_WIDGET_TYPE_form_alter(). . This default country widget is a simple widget of select options. Drupal core provides a list of all countries in the world as an array of country names keyed by ISO-2 country codes. This is made available as a service for use anywhere in a Drupal project. . Let’s start off with a complete implementation for this. Create src\\Plugin\\Field\\FieldWidget\\CountryAutocompleteWidget.php with this code: . There’s nothing unusual here at all. We need to implement same defaultSettings() and formElement() methods as for the default widget. Add this to the class: . We want a textfield that’s wide enough (‘size’ =&gt; 60). For the autocomplete_route_name key we provide the name of the route from which our autocomplete functionality will return matching values. We’ll be implementing that shortly. We don’t want anything as the placeholder. . Finally, let’s add our formElement() method: . This is a standard autocomplete widget. Looking at the FAPI array keys, some are very familiar. #autocomplete_route_name matches what we entered in our defaultSettings() a while ago. The value is retrieved from there with $this-&gt;getSetting(‘autocomplete_route_name’). The same goes for #size and #placeholder. Our #autocomplete_route_parameters has no default value. In order to ensure that the final value to be submitted doesn’t include unwanted values, we add #element_validate and enter the name of our callback function. We will also implement this shortly. . Create a YAML configuration file called country.routing.yml in your main module directory with the following: . The key or name of the route is country.autocomplete which is how this route will be referred to anywhere in the application. At least a route should define 3 things: path, code to execute and who can access the path. . path: This is the URL for our AJAX autocomplete calls. . _controller: This is the code we want to execute when we visit the defined path. It’s written as CLASS::FUNCTION. If our function requires any parameters, this is where they will be specified. We will be creating our controller shortly. . _permission: The string representation of the permission for access control . Now we move on to the creation of our controller class. Create a folder called Controller under src. Then add CountryAutocompleteController.php inside it. Add this code: . Whatever we type in our autocomplete widget will get passed to our autocomplete method. Then, we simply search for it in the array of country names which we pull from the country_manager service we have come across before. Finally, we return any matches or an empty array in a JSON response. . That looks more like it now and we’re nearly there. If you look back at our formElement() method in CountryAutocompleteWidget.php we specified a validation callback. We are going to do that now in our country.module file. Add this code: . We get our array of countries, and compare the value we want to send to the database with possible values.  Now that we’re here, let’s just implement hook_help to give some information about our module. Just add this  below the last use statement: . We have now finished our autocomplete widget and learned something about routing. Not bad at all! . We have everything ready for creating our field and allowing users to input data. Everything should work. There’s little work left before we can display the output. That’s where the need for a field formatter comes in. Add a new folder: src\\Plugin\\Field\\FieldFormatter and inside it create CountryDefaultFormatter.php. Then add this code. . If we don’t do anything now, everything will work except where we expect to see a country name, we will see the ISO-2 which was saved as the value of the field. To display our country name, we need to override the viewElements() method. So let’s do it: . Once again, we get our array of countries, find the country name for our ISO-2 value and return it as markup. Job done. The module we have created is now functional and can now be installed. . Let’s take a look at what fields UI offer us with our Country field. You may use an existing content type or start off with a new one. From the list of content types (admin/structure/types) clicking on the Edit operation will take you to the edit form with three tabs at the top: Manage fields, Manage form display, and Manage display which give us access to our field type, field widget and field formatter, respectively. . Field type (Manage fields): . Click “Add field”. Select “Country” from the “Add a new field” select dropdown. That dropdown is populated by all @FieldType plugins from all enabled modules. They are grouped according to the “category” key of field plugins. This is optional, and if it isn’t defined for a field, it goes under “General”. If you inspect it, you’ll see that the name is “country_default” which matches the “id” key of our plugin and the “Country” we see there is the “label”. . Enter “Location” as the field name in the “Label” textfield and save it. . If you want to see how the field type settings are reflected in the field UI, go back to the annotation in CountryItem.php. Edit your plugin label, change it to something like “Country - edited” and save it. Then go back to the fields UI page and you’ll see not only the new value in the select dropdown when creating a new field but also under the “Field Type” column for the new Location field. You can revert the change now. . Field widget (Manage form display): . On this screen you can see what was defined in the FieldWidget plugin. In the URL you can see the plugin “id” and the “label” is the description. . Field formatter (Manage display): . There isn’t much choice here so we just leave the default formatter as is. . You may now take the completed field module for a spin. Finish by adding it to a content type, create a node and view the output. That’s all. We’ve come to the end of looking at how to create custom field modules and in the process learned a few things about Drupal 8. Go make yourself a coffee. You deserve it! .  1 : See  https://www.drupal.org/node/1882526  for an excellent introduction to annotation-based plugins. ", "date": "2015-08-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "BDD Test Execution Throughput\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/development/bdd-test-execution-throughput/", "abstract": " This post is based on the experience of one project in particular. It is the implementation of a large enterprise solution, primarily to speed up the administration  of a contract management process. . The user-based part of the solution is based on a CRM package, but this post is not based on UI-style testing. The bulk of the solution is based on  ESB  technology - which is the focus of this post. . The project had the aim to set up a full continuous delivery pipeline from the outset. As such it was necessary to choose technologies up-front, with the mindset that they can always be changed later if required. . BDD -  Behaviour Driven Development  - is a technique for establishing business scenario test specifications in advance of development work. Adding these to the continuous delivery pipeline is ideal to gain fast feedback - especially for guarding against regression. . A “GWT” (Given-When-Then) style syntax for the test specifications was preferred.  SpecFlow  and  NUnit  were chosen. This choice was taken quickly rather than thoroughly investigated - mostly based on the experience of a couple of members of the team. NUnit was chosen for the underlying test framework as it appeared to be the most mature of the frameworks SpecFlow can integrate with. I also got the impression most of the team didn’t really care about tests at the time, there were far more “sexier” technology choices to be made! . Test specifications were written. It took a while to get these added to the build process. It took even longer to provide an environment in which to run the tests as part of the build process. Eventually this was all fully integrated into the continuous delivery pipeline. This looked something like: .   . I say “eventually”, in reality it was less than three months from the beginning of development. In the grand scheme of things this was actually pretty good. There were other aspects of the project that required focus and so the continuous delivery pipeline didn’t get the attention it otherwise would have. . The pipeline was set up, the BDD tests were proving continuous feedback on the state of the product. Right? . Reality wasn’t quite so. . Many tests were failing. Many tests were unreliable. . Essentially the tests had been written and tested on developer’s own machines. The tests were running in a different environment, configuration settings needed to be addressed. . The developers of course reacted immediately and fixed all the tests. (Sarcasm. Ed.) . In reality the developers were ignoring the problem. There were far more interesting things to do and the pressure was on from the Product Owner to deliver. The Delivery Manager (lead Agile Coach) insisted no stories would be accepted at the next Sprint Review until these tests were fixed! This bold move ensured the continuous delivery pipeline finally had the attention it needed. . To be fair to the developers, the message from the Delivery Manager was also to the Product Owners to verify the tests. Essentially more and more work was being expected of the developers, but at the expense of quality. Not a sustainable situation. . So the tests were fixed, but that is still not the end of the story. The tests were taking a long time to run, sometimes as long as eight hours. For an overnight run, this isn’t too much of a problem, but it certainly was not in keeping with our vision of “rapid feedback”. . One of the problems with adding BDD tests to the continuous delivery pipeline is that the story with which they are associated may not be complete yet. Across a large team, this could amount to a good few tests, perhaps a dozen or more. So how should the pipeline discriminate between tests that won’t pass (story not yet implemented), tests that should pass (story implementation complete awaiting test verification) and tests that must pass (to detect regression caused by other development work)? . This situation was addressed by re-categorising the tests and putting in place a simple promotion mechanism: . @AcceptanceTest - tests marked as ready to be added to the continuous delivery pipeline . @StoryComplete - tests for stories that are deemed “done” . @Rapid - tests that execute quickly . With the addition of these categories we could improve the feedback given by the builds in the continuous delivery pipeline. .   . The third step in the pipeline is the “continuous delivery to acceptance test” build. A bit of a mouthful and the naming is historic/convention rather than meaningful: essentially it’s the main build deployed to the Acceptance Testing environment. . This build is scheduled overnight and it doesn’t really matter how long it takes. The purpose of this build is to provide comprehensive coverage, at the expense of frequency of feedback. . The tests are two tests runs: those tests that might pass and tests that must pass. If any of the tests in the latter run fail, the build is marked as failed. The use of the @StoryComplete tag is used to select the “must pass” tests. . The test run usually took between four and eight hours, depending on the number of failures. Since failures were usually due to time-outs in the asynchronous solution, this would result in a much longer execution time. . The second step in the pipeline is the “continuous delivery to continuous integration” build. The main build deployed to the continuous integration environment. Again, not a particularly meaningful name. . This build is a defined as a rolling build. (This a  Microsoft Team Foundation Server  build server build type definition. Many change-sets are rolled-up together into a build. The next build can be configured to start immediately after the previous has finished or at a specified interval after the previous finished, again with all the change-sets during the interval rolled-up.) . Speed is clearly more important here than for the overnight build. The build was configured to select tests marked with the @Rapid category. . The tests themselves were marked manually with the @Rapid category. Some filtering criteria was used to do this - essentially tests that were passing reliably and executed in under 10 seconds. . This resulted in a build that executed in around thirty minutes (which included the build, deployment, integration tests and acceptance tests). The rolling build was set to sixty minutes so that the build agent wasn’t in continuous use. . For the project to receive quicker feedback, the incentive was to improve tests. This way they could be promoted from the daily feedback loop to the hourly feedback loop, far more desirable in seeking “done” against a story. . The pipeline now looks something like: .   . (Note this is not the  full  pipeline, there are further steps to deploy to Model Office, Pre-production and Production environments.) . The situation was better. A framework had been put in place to get faster feedback. But it’s still not quite the original “vision”. . So how can the speed of the tests be improved? . This has proven to be challenging. The framework executes each test sequentially, one by one. Failures caused the biggest delay and so these needed addressing first. One of the developers started experimenting with a “circuit breaker”, essentially detecting a broken state that would cause multiple tests to fail and so flagging those failures quickly. . But the very nature of the solution means that delays are expected. During the execution of any one test there can be many delays whilst execution happens in the background - essentially dead time as far as the build server is concerned. . But from the point of view of the solution there is little or no benefit to improving the response time of these actions. Scalable architectures are all about throughput, not response time. . (I have a previous post on the topic of  Little’s Law .) . Little’s Law is a mathematical theory of probability in the context of queueing theory: . L = λW . L = number in the system, λ = throughput, W = response time . So since we cannot control the response time, to increase the throughput we have to increase the number in the system. Currently this is fixed to one since the tests run sequentially. . I thought of two approaches to this problem. . The obvious solution is to run multiple tests in parallel. After investigation though, it appears it is not possible to do this with the current tooling we have. There are a few limitations, but the biggest is NUnit itself - the test runner. . SpecFlow has a tool called SpecFlow+ Runner. This is a paid-for addition. We’re not against purchasing extra software, but for this aspect of the project we’re trying to avoid tie-in to a particular tool. From the documentation it was also unclear if this tool would definitely help us. . A tool called PNunit came up. Unfortunately that tool doesn’t really run tests in parallel. What it does is split tests into sets to distribute across many build agents - not really what we are looking for. . It did occur to me that a better use of our message-driven solution would be to set all the tests running in some asynchronous fashion and then collect all the results later. . Potentially, I thought this to be a very fresh and exciting approach to the problem! It would certainly be very fitting for the asynchronous nature of our solution. Perhaps it would work something like: execute all the “Given”s first, then the “When”s, and finally the “Then”s. . The drawback is it would require some pretty major changes to our tests. How would the context be held between execution of the different steps in the test specification? How would the steps be executed asynchronously? It would also require some changes to the underlying test framework. . Pretty exciting stuff, but too much to take on now. Definitely something I will revisit at a later time though. . During my investigation I kept getting drawn back to NUnit as it does have some asynchronous/parallel features. It turns out these are aimed solely at testing Silverlight applications and are not transferable to other technologies. A Dead-end, or so I thought. . The next major version of NUnit ( NUnit 3.0 ) contains some better features for parallel test execution. At the time of writing it’s still in beta, but it doesn’t look too far off. . I decided to give it a try. I set up some dummy tests against a dummy asynchronous solution using the SpecFlow and NUnit versions we currently use. I then upgraded to NUnit 3.0 and upgraded the VisualStudio test adapter, also NUnit 3.0. This all worked fine, the beta version looks pretty stable and usable. . So now to running the tests in parallel. Currently, the new NUnit framework only allows parallel execution at the “test fixture” level (essentially a test class). Test fixtures are marked with the addition of the following attribute: . Which is simple enough. Except that SpecFlow  generates  the NUnit test code - so where to put the attribute? . I created a simple SpecFlow plug-in to add the attribute. The creation of the plug-in and the syntax of adding the attribute is a bit fiddly, but it worked! I was then able to run tests in parallel! . I haven’t actually tried this out on the project code-base yet, but I hope to do so in the coming weeks. The plug-in requires a bit more refinement, but the principle is there. I also have a slight reservation in that on the build server it turns out the tests aren’t actually executed by NUnit - apparently VSTest intercepts and runs them, though I don’t see this as an insurmountable problem. . To gain rapid feedback from BDD tests against an asynchronous solution requires extra thought. The weakness appears to be the test runners, they are not geared towards asynchronous processing. . In the first instance you may well need to make a compromise on speed and coverage. Tests that are fairly fast can be executed often, slower tests can be run overnight. . Running tests in parallel may bring about an improvement, but ultimately test runner architecture must evolve if they are to be suited to a asynchronous solutions. The idea of triggering the tests then collecting the results later is potentially quite powerful for this. . I’ll follow up with another post when I do try it out. . I would be very interested to hear any war stories, insights or similar problems you may have encountered, please submit a comment to start the conversation. . I’ve uploaded the source code, the proof-of-concept  BDD-Parallel  and the SpecFlow plugin  ParallelAttributePlugin . ", "date": "2015-09-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The Lead Developer conference\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/learning/lead-developer/", "abstract": " On Friday, I attended  The Lead Developer  conference, the first outing of a new conference for technical leads. I first heard about the conference back in May, and the tag line piqued my interest immediately: . “When you’re busy leading a team, coding and maintaining standards, how do you find the time to stay ahead of new technologies and develop yourself as a technical lead? The Lead Developer is a new conference featuring practical advice from experts on leading and motivating your team and high-level sessions on new and disruptive technologies.” . It was like the organisers were reading my mind! I’ve often struggled with getting the balance right between the technical and non-technical aspects of my role, and often felt “the fear” of losing touch with my technical skills as I become more involved with leadership responsibilities. . It’s always a good sign when you know the venue for a new conference is a good one, and The Lead Developer was held at the QEII Centre in Westminster, a venue I knew from  Symfony Live London . Once again it was an excellent space, with good options for food, drink and healthy snacks. As for the not-so-healthy - cream tea for the mid-afternoon break was an inspired choice! . The conference kicked off with a keynote from  Camille Fournier , CTO of Rent the Runway, telling us all that as cloning ourselves isn’t an option we need to demonstrate “laziness, impatience and hubris” to become productivity multipliers. This was a really excellent talk and got the conference off to a great start, immediately tuning into the questions I’ve been asking myself. The advice on how to measure your success through evaluating your impact and influence was well delivered and valuable. . Rather appropriately, the conference continually sought a balance between technical and non-technical talks - one of the paradoxes (Staying Technical vs Enabling People) discussed in Thoughtworks’  @patkua ’s talk on  Riding the Paradox as a Lead Developer . The six paradoxes… . Staying technical vs Enabling People . Going faster vs Learning . Delivering vs Innovating . Consistency vs Improvement . Being transparent vs Protecting the Team . Business needs vs Technical needs . …all struck a chord, following on nicely from the first talk and teeing up the rest of the day. . The late morning sessions consisted of a whistle-stop tour of AngularJS and Go in two lightning talks, going  “Beyond Developer”  from Dan North, and 12 tips for better interviewing (tl;dr; - understand your biases and be empathetic) from  Cate Huston . . The morning then concluded with  Russ Miles  using an  electric guitar, AC/DC  and a  marriage proposal  to keep us on the right direction on the Highway to (Microservices) Hell. Russ gave a thoroughly entertaining talk, leading the audience through his views on achieving success with a microservices architecture (and some hints on avoiding failure) by focusing not on what you have built, but what you can do with it. As always, the war stories and warnings were as valuable as the tips for success. . The after lunch slot, normally a bit of a graveyard as everyone digests, was filled by  Sam Barnes  presenting one of the highlights of the day in his talk  “People are Weird, I’m Weird” . Highlighting that every person we meet is different, and that we only consider weird from our own perspectives, Sam used a very personal story combined with some highly amusing animated gifs to dive into the heart of the issue and remind us how close team leadership and psychology are: . [Psychology/Team management] is a profession where it’s important to study people; how they think, how they act, react and interact. It’s concerned with all aspects of behaviour and the thoughts, feelings and motivation underlying them. . Next, Becky Stafford and Preet Sandhu talked about  Adaptive Testing  and their journey at Just Eat. As someone who definitely recognised the “Software Testing Ice-Cream Cone”, and the Legendary Automation Team, it was good to hear their journey to Acceptance. . Either side of that cream tea I mentioned were lightning talks on  NativeScript  (looks very interesting),  Rust  (great to hear about, and definitely something I want to try sometime), Practical Web Security (always useful to be reminded) and  Docker  (regular readers will know we love Docker here!). . As the end of the day approached, it was clear that the day would be ending with a bang rather than a whimper. .  Una Kravets  gave a  Primer on Performance , giving a really good walkthrough on front-end performance considerations and techniques, along with good ammunition for anyone who needs to build business cases of why we should care about performance  ($40m a year for 0.4 seconds anyone?) . CMAT (Concatenate, Minify, Async, Test) was a really good mantra for people to follow. Although a lot of the material in this talk was familiar to me, the delivery was excellent, and the slides stand on their own as a reference so I’d recommend checking them out. . The talk that chimed most with me from the day was “Staying Ahead of the Curve” by  Trisha Gee , developer advocate at JetBrains. This talk focused on to (safely) keep up with the latest technology for both your team and yourself, and approaches to combining this with your daily work without blowing up production. Carving out time for this kind of thing has always been something I’ve wrestled with unsuccessfully, and it was reassuring to hear that others find the same problem, and some ideas to get around it. Looking forward to the slides and video on this one to rewatch and digest further as my note taking was horrendous! . Finally, the day came to a close with  Oren Ellenbogen  talking about Building Happier Engineering Teams. I’ve been subscribed to Oren’s weekly newsletter -  Software Lead Weekly  - for a while now, so it was exciting to finally see on stage someone whose name pops up in my inbox once a week. Oren talked about how to grow teams whilst keeping everyone happy, and gave some insightful techniques for both keeping and attracting the right people. I had to smile at one of these, one of the reasons we started this blog in the first place: .   . Overall, the inaugural Lead Developer conference was a pleasing combination of technical and people talks, a balance struck perfectly in my view. It is rare that the first run of a new conference goes completely smoothly, but I can’t find fault in anything - for which the organisers must take a lot of credit. I’m delighted to hear it will be returning next year, and it will certainly be going on the list of conferences I look to attend, and recommend others do too. Thank you to the organisers, speakers and those I talked to at the event. ", "date": "2015-09-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Delivering at Devoxx\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/learning/delivering-at-devoxx/", "abstract": " There comes a point in a developer’s life where one develops a solid confidence in one’s opinions and actions. For me, this aligned with parenthood; the horrifying responsibility of being in charge of a little one’s life makes choices such as “which java based presentation technology shall I use?” a decision that can be approached with a lot more perspective! And this year I took another leap towards the “Definition of Me” when I agreed to submit a conference paper. My new-found confidence allowed me to submit a punchy title without fear of being unable to live up to it, and when my concept was accepted I began a fascinating journey through the history of my entire career and emerged with a comprehensive set of opinions and ideas that I didn’t even know were there. A highly recommended journey. . My initial concept came from a small set of technical problems that had been annoying me at the time. I’d been learning Scala and decided to try and use it in anger in a web environment, so created a RESTful API and stuck an AngularJS front end on to see what it looked like. I was hosting the two separately – AngularJS in its seeded http-server container, and Scala through a template Akka/Spray app in Typesafe Activator. Bang – of course, I hit the Same Origin Policy which wouldn’t let my AngularJS app talk to my Scala app when called from a browser. Thinking about why this policy was introduced and how it had been implemented got me musing on why it couldn’t be re-done in a better way now that our browsers and use of HTTP security features are much better. It was a policy implemented in haste to prevent hacker damage, which we can now all regret at our leisure. . Then fast-forward to my next project, we were building an ASP.NET MVC project which needed to communicate with a SOAP service – easy, right? Just create a SOAP client in ASP.NET? Wrong. The SOAP service was written in an old version of Apache Axis, and some of the types represented in the WSDL represented Java classes without definition and assumed a Java client! Yuck. . My final irritation was yet another one of those bugs where somebody has unwittingly loaded an entire database into memory through a misplaced Hibernate call and incorrectly-defined loading policies. For the X hundredth time I wondered whether we were really gaining anything from abstracting the power of the database and insulating developers from the complexity of mapping state to in-memory objects. A good long winge with a DBA friend of mine suggested that I was not alone in this muse! And a brief google search for “ORM anti pattern” confirmed it. . It was about this point that Java team lead  Andrew Harmel-Law  suggested that we submit some papers for the upcoming Devoxx conference. I had my recent bug-bears in mind and wondered about creating a paper called “Modern Antipatterns”, until I googled it and discovered it was the title of a hilarious poor-working-practices speech from a previous Devoxx. So I had to rethink. What was it about the two problems I’d hit? What did they have in common? Well, they were both things that made sense at the time, but were for whatever reason outdated in today’s architectures. This concept of patterns in time led me to my title: “Technical Archaeology – unearthing the dinosaur antipatterns in modern distributed architecture stacks”. Bingo! So, I submitted the title with a list of three “antipatterns” (Singleton, Same Origin Policy and Object Relational Modelling) and a very brief explanation, and forgot about the whole thing. . A few weeks later I received an email from  Daniel Bryant , one of the Devoxx conference organisers. He wanted me to elaborate on my paper. Would I be able to talk about additional patterns in the presentation – maybe something slightly more complex for their more senior and advance-skilled audience? More complex how, I queried? Perhaps my use of the term “patterns” had been misconstrued.  I didn’t mean GoF exactly - the “this-is-dated” angles on most GoF patterns are rather language-dependent - for example the visitor pattern is not relevant for functional languages, and javascript has the prototype pattern built in to the language. I was thinking of patterns at a more enterprise level and architectural level rather than diving into the complexities of a language.  I offered a couple more pattern examples – the horror that was EJB 2.0 and my SOAP experience, and that clinched it for Daniel as he’d been bitten by the same axis bug. I was in! . The process of actually writing the conference speech was the real revelation for me. I had started with a title, and what ended up as five examples of “antipatterns”, and when I presented this to a demo audience it was clear that I needed more cohesion between my five examples. What was the link between them? What point could I make about the whole thing? I had to go back and drill down into my concept once again. I emerged with a few opinions that I realised I’d been formulating my whole career. . It’s interesting to watch the evolution of software engineering because it’s such a new discipline. It’s only had time to go through a couple of oscillations through trends. It seems to me that there has been a general trend towards “do everything you can”, the apex of which I would say was EJB 2.0 with a specification so heavy it could not be lifted, let alone read and digested. This can be seen in many areas of software architecture – look at SAP, for example, and the merge of the concepts of a portal website, content management system, document management system that we see in Sharepoint and Vignette amongst others. We are well on the way back down that particular trend now, with “do-one-thing-and-do-it-well” microservice architectures becoming more and more popular. I can see that there are drivers behind these trends – sometimes social, sometimes driven by a particularly powerful company or a particularly well-defined global specification but often driven by the underlying hardware available for processing and networking, and the algorithms that support them. What would the point be in a microservice if your network latency is huge? But now we have optic fibre cables and broadband and near-global 4G wifi, we can rely on messages getting from A to B in a reasonable timeframe. Similarly, why have a wireless camera take a 20 megapixel photo if it doesn’t have the algorithm to compress that picture to be sent? . My five patterns all fit well into this angle of software evolution driven by hardware over time. The singleton pattern was important and useful back in the day when you had a single CPU and your entire program ran on one machine. It doesn’t fit so well in a message-based architecture… The Same Origin Policy was implemented in the brave new world of internet commerce, when thick client browser architectures were immature and there were so many open doors for hackers they were spoilt for choice and a quick implementation was more important than a long-lived design. SOAP was a good advancement on CORBA, and at the time was a real step forwards in cross platform architectures, but because the specification became too broad in the “do everything you can” trend there became too many points where underlying implementations could differ. And at the time that the SOAP spec was written, there weren’t really any mobile platforms so the relative size of an XML message didn’t constrain the hardware at either end of the transaction. It reminded me of the human appendix, and goosebumps – things that had a purpose at the time and kind of hung around because there wasn’t a strong enough drive to remove them from the gene pool. . I then realised that if I was going to talk about evolution, I’d better explain a bit about what it meant to me. It’s a word which is often used outside of its dictionary definition. My favourite explanation is the one picked up in a  speech given by Douglas Adams . “That which survives, survives”. Simple as that. Things (be they technical architectures or biological mutations) exist because they aren’t killed off by their environment – whether their environment is the African plains or the open source software playing field or whatever.  This didn’t quite cover all the parallels I saw with the evolution of technical architecture and biological evolution, so I browsed through some books and papers, and found the two definitions I was looking for.  Divergence , and  Vestigial Functionality . All five of my antipatterns could be classified as dinosaurs for one of two reasons – either architecture had diverged away from those patterns, as in the example of EJBs and ORM whereby we are moving towards a simpler do-one-thing-well architecture and away from complexity and abstraction. Or, the pattern is what I’d consider vestigial functionality – as in the case of the singleton, which is a pattern based on single-CPU object oriented programming in a world where this is becoming less common. . I could feel this all coming together. I added in some demos of my Scala example and my SOAP disaster to keep people awake, tried to add some humour by asking my friends for some, and now all I needed was a conclusion. . I wondered if I could extrapolate out some prediction of the future from my musings on the evolution of software being driven by hardware. What was coming up in the hardware world? Bio-computers? I couldn’t see that necessarily changing software. The end of batteries due to lithium supply constraints? Possibly, however the more I thought about it the more I realised that I wouldn’t be able to make a useful prediction. There are so many variables, who knows which ones will be the “fittest”? I decided that a better conclusion would be to advise people NOT to try and predict the future, but just to be aware that things change. We often get in the habit of using something because we used it before. If you were to ask a technical architect to design a stack to host a website, they’d probably stick Hibernate in between the data source and the business logic layer out of habit. It’s this kind of behaviour that we need to be aware of – sometimes it might be valid, but sometimes things will have changed and the architecture of yesterday not quite fit today. I finished with the line, “Design for Disposability, not for the Future”. Done. I went off on holiday for a week to recover. . I returned to an email asking me to join the panel for the Devoxx closing speech. Why? Why me? None of these people had ever met me, I’d never presented at a conference before and I’m not exactly famous in the IT community. Apparently I offered a different perspective to the other speakers and they wanted the variety. I decided to go along with it so I agreed to come along to the opening dinner and meet some of the other presenters. I’m glad I did – it felt like my five minutes of fame! We were ushered through a restaurant past the mere public and into a huge back room where we were plied with champagne and attention and generally made to feel incredibly important. I spoke to some very nice people, not having a clue who any of them were, and having to manage awkward moments like being told, “You do realise that was THE  Simon Brown ?” Oh dear. None the wiser. I can’t have disgraced myself too badly though as I even got an offer to present at an American conference next year. I can see how this strange detached world of conference presenting can appeal, although the jet lag must be something else.  Josh Long  had hit 75,000 air miles in three months. .   . My presentation the next day went as well as I could have hoped. The room was fairly full – I hadn’t had that nightmare scenario of talking at the same time as someone really famous (or one of  the robots ) and although I think there was a slight tremor of nervousness in my voice I hope nobody noticed. The demos went smoothly, I did talk rather too fast and finish in 45 minutes but I don’t think anybody minded. I think the humour went okay – at least, I managed to get a groan and a head held in hands from the front row. The feedback afterwards was that my delivery was so deadpan that it took a while for people to realise that I was being funny! Might need to practice that one… I also got a lot of agreement, people pleased that I had “killed off SOAP” and sharing my opinions of EJBs, and generally positive comments and tweets throughout. . By the time the closing keynote came around I was much more relaxed and the panel-style staged interview questions flew by. I had to give a brief overview of my speech, and add in some agile client anecdotes whilst trying not to be star struck by the rest of the panel. Before I knew it, the beers were open and the credits were rolling and my five minutes of fame were up. .   . I went home that evening on a high, promising myself that I’d be back. Unfortunately however, this speech is the culmination of fifteen years of musings, I can’t just come up with something like that every day! Devoxx 2030 it is, then, I guess… ", "date": "2015-09-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Agile Cambridge 2015\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/agile/agile-cambridge-2015/", "abstract": " I’ve been lucky enough to be selected to speak at  Agile Cambridge 2015 . My talk is titled “Failing Fast - An Autopsy of a Failed Release”, please  read more about this talk on the conference site . . I attended the same conference in 2013. I very much enjoyed the conference and I was inspired enough to submit a proposal the following year. Fortunately this was accepted and I enjoyed the pleasure of speaking about DevOps in the Fellows Dining Room at Churchill College, Cambridge. (A dining room? It’s an impressive room with no doubt an invaluable collection of artwork on the walls, including some very eye-catching Andy Warhol pieces on the walls!) . Speaking about DevOps proved to be a little controversial, but it did prompt some good discussion - kind of my aim for the talk. Essentially I was saying DevOps is a way of thinking, not a separate team. I wanted to define the title “DevOps Engineer”, something I found very difficult to do because of the wide skill-set and level of experience required. . This year I decided to be a little “safer” in my proposal so I decided a case study was the way to go. But then I used the word “Failed” in my title and I seemed to stir-up a few emotions! I can understand that “failure” has negative connotations, but surely  not  admitting failure early is even worse! “Failing fast” is seen as a benefit of agile practices and adopting such practices is supposed to make catastrophic project failure a thing of the past. . I’ve written a few blog posts now, loosely around agile practices (of course!). Please see my section on the  Capgemini Engineering blog  to find out more, or indeed to find other posts from my fellow bloggers. ", "date": "2015-09-25T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Tooling as a service via executable images\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/devops/tooling-as-a-service/", "abstract": " This is a recap about how we use the isolation of  containers  and the power, portability and simplicity of  Docker  for providing tooling as a service via  executable images  using  DCOS-CLI  as an example. True story! . We want to install  Mesos Frameworks . . We want to deploy apps with  Marathon . . We want to automate the installation of Mesos Frameworks. . We want to automate the deployment of apps. .  DCOS-CLI  from  Mesosphere  is the best solution for the first two problems but we need to follow all the steps to  install it . . We need to make sure our environment is suitable to install and run it so it has all the required dependencies. . We need to make sure every environment where we want to run it is suitable to install and run it so every environment has all the required dependencies. . We need to ensure all the dependencies and any executables comply with security requirements, as the attack vector can be quite extensive. . To offer DCOS-CLI as a service that can be executed with zero dependencies with the specific environment. . To consume this service from a given automation tool like  Ansible . . We use Docker for running an  ephemeral container with dcos-cli contained . This means Docker is your only dependency no matter what you want to run. . The container  creates a temporary config  file from your environment variables,  executes  your command  and dies.  Check out the  source code here . . Now it’s up to you how to automate it. We use ansible and the  Frameworks role  for  Apollo  which looks something like this: . Reduced impacting surface. . Only one dependency with the run time container, i.e Docker. . Crazily easy to integrate with other automation tools! . Reusable. . Build once, Run everywhere! . At the time of writing this post,  Mesosphere , the creators of DCOS-CLI among such other amazing tools around Mesos, are working on their  own image for DCOS-CLI  ", "date": "2015-09-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Developer Automation\n", "author": ["Ant Broome"], "link": "https://capgemini.github.io/devops/developer-automation/", "abstract": " As software engineers we’ve all experienced the steps below when joining an established project: . Walk in first day, excited at the prospect we can change the world! . Get told task one is to read the documentation :/ followed by task two get a development environment up and running ( sigh as reality hits, you won’t be changing the world today ). . Spend many hours searching for documentation on file shares, wikis and random machines sitting under desks. You now piece together all this information and let battle commence to get things up and running. . You’re three days in, have a shaky local development environment that’s just about hanging together, there were a few scripts you found online and some dependencies you’ve copied over from someone else’s machine that got you over some hurdles and you tell yourself you’ll update the documentation for the next new joiner. . Some weeks later a new joiner starts, they’re attempting to setup a developer environment, you smile, wish them luck and the process begins again (including your additional notes in yet another place). . This of course is an exaggeration, however some elements of the story can still be found in many projects! There is also the problem of implementing changes to existing developer environments, often an error prone scary process. What we need is a way of source controlling this process ….. . Automation is the solution and in particular an awesome tool called Vagrant. Vagrant is a wrapper around virtualisation tools allowing you to script your environment setup. This information is held in a ‘vagrantfile’ so we can version control this along with our source code and update it as changes occur. This then solves the two problems mentioned above, new joiners need only be pointed to the source code and run a single command and existing developers feel safe to ‘destroy’ and recreate their environment as changes are made. The scripts we use to ‘provision’ our vagrant virtual machines can even be re used further down the delivery pipeline. . Below I outline the steps taken to automate a basic environment, I’ve used VirtualBox and simple bash scripts to provision a virtual machine. However vagrant offers many alternative providers (what it hosts the virtual machine on) and provisioners (how we set-up the virtual machine once its running). . The flow below is aimed at getting a basic environment set-up, more complex scenarios re-use much of the example below. . Before we can begin we need to install  VirtualBox  and  Vagrant . .  In the root of your project run the following commands:  . The first line will download a base image, in this case a 64-bit ubuntu image. The second creates a default vagrantfile. .  Open the vagrantfile and replace the contents with the following:  . The file outlines the virtual machine requirements and the scripts to run for set-up. It’ll create a 64-bit Ubuntu virtual machine with 1024mb of memory running on the IP with port forwarding. This is all very familiar if you’ve worked with a virtual machines before or read our  previous blog post on VirtualBox . . The provisioning steps are the automation of what we as developers would normally do manually. In the example I’m copying a jar file into the machine and then running some bash scripts to install Java and MySQL followed by some additional specific bootstrapping. There’s nothing stopping you provisioning in a single script but personally I like to keep things re-usable. For more advanced provisioning you could use puppet or chef. .  Create a provisioning file for MySQL:  . p.s. remember to run “mysqladmin -u root password myPrivatePassword” later on to secure the database. This is a non interactive install script. .  Create a provisioning file for Java:  .  Create a provisioning file for bootstrapping:  . This may take a while first time, it’s setting up the virtual machine and applying the provisioning scripts. After that you’ll have a build fully configured. You’ll be able to ssh into this machine and manually modify it if you wish. . Running vagrant destroy will delete the virtual machine. When you need the environment again re run vagrant up. . Hopefully this post will inspire you to give automation a go and get started changing the world a lot sooner! ", "date": "2015-10-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Introducing mesos-ui: An alternative frontend for Apache Mesos\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/mesos-ui/", "abstract": " Today we are open-sourcing  mesos-ui  - A realtime, responsive dashboard for Apache Mesos, built with  React ,  D3 ,  Nodejs ,  Socket.io  and  Google Material UI for React . . As part of our work on  Apollo  we got thinking that we could potentially improve on the UI offered by Mesos out of the box and started putting a few ideas together as a proof of concept. . We are releasing that proof of concept today in the hope that others in the community might either find it useful, help us test it and report bugs or potentially get involved in taking the development of it forward. . ## Taking it for a tour . The easiest way to run the UI is to run it as a Docker container. You can do this (in development mode) by executing the following - . This should start up the UI on port 5000 as well as a backend stub (JSON) server on port 8000. The stub json-server is used so you do not have to have a running Mesos cluster up and running to experiment with the UI. Of course you won’t be able to experience any of the realtime updates, because the stub server is essentially static. . If you are already running Mesos and Marathon, the easiest way to deploy the UI is through Marathon itself. We’ve included a  marathon.json file  in the repository to make this a bit easier for you. . To run the UI in marathon simply edit  marathon.json  and replace  MESOS_ENDPOINT  with the URL of your Mesos master leader node and then execute - . replacing  MARATHON_ENDPOINT  with your live Marathon endpoint URL. . For more instructions about getting up and running see  our readme . . If you notice any problems with our instructions please  report an issue over on Github  . Here’s a short video of the UI in action which shows the realtime updates occurring as tasks and applications are deployed and scaled in the cluster. . We’d love to hear from you and welcome any feedback, issues and contributions to help us improve the web interface. If you’re looking to get involved here’s some specific areas we’d welcome contributions on: . Building out new dashboard widgets . Adding custom themes . Helping us shape the roadmap and future features of the dashboard . For more information, please see our Github repository -  https://github.com/Capgemini/mesos-ui  or dive straight into the  issue queue  to get involved. . If you’re planning on attending  Mesoscon Europe  and want to hack on mesos-ui or just want to chat in general - be sure to find our Engineer  Cam Parry  who will be attending all week and at the hackathon on Friday. ", "date": "2015-10-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Better Learning Through Code Reviews\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/learning/better-learning-code-reviews/", "abstract": " One of the main reasons I wanted to join a big company was the opportunity to learn. I wanted the chance to work on bigger projects with colleagues who’ve been there and done that, and to benefit from their experience. . In the 4 years I’ve been at Capgemini, I’ve been lucky enough to get those opportunities, and for me, the thing that has been the most helpful in terms of my own learning has probably been code reviews - both other people reviewing my code, and me reviewing other people’s code. . Some people say  code reviews are a waste of time , and if they’re talking about what code reviews used to be, I’d be inclined to agree. The idea of scheduling  a set of formal meetings  where a bunch of developers get together and go through a file line by line seems ludicrous nowadays. . Thankfully, it doesn’t have to be like that any more. With modern tools, code reviews are far more efficient. They’re an integral part of our workflow at Capgemini, and are  vital in improving the quality of our code . . Velocity is a term that gets used a lot, and it’s generally agreed that high velocity is a good thing. But sometimes, when a team of developers gets into their stride, they can resemble a team of galloping horses. Too much change in a short space of time can be overwhelming, and it can sometimes be good to create a bottleneck to slow down the pace of change and make sure that the change is appropriate. Besides, code reviews and the improvement in quality they bring about can actually  increase productivity . . Once bad code has entered the code base, it can be very difficult to get rid of it. You may think that you can take care of coding standards and unit tests once you’ve got a few more high-priority tickets out of the way, and got your project manager off your back, but the truth is that  you probably won’t get round to tidying up later . . Code reviews are the ideal place to ask questions like: . Does this code actually solve the right problem? . Does it solve the problem in an appropriate way? . Is there unit-test coverage? . Is the code secure? . Is it likely to introduce any bugs? . Does it comply with relevant standards? . Are there any relevant accessibility considerations? . Don’t imagine that you can rely on the QA team to catch any bugs that might be lurking, or that the developers are too busy for this sort of thing. The test team have got enough to do already, without devs deploying shoddy code onto the test environment. . Code reviews are an ideal place to encourage a mindset of  caring about quality . They’re an opportunity to reinforce positive values within the development team, and get everyone thinking about how the code base can be improved. . The code review can also be a springboard for conversation, and an opportunity to explain the thinking behind a code change (although beware of the temptation to  bikeshed ). . For these reasons, the reviewers shouldn’t just be the senior devs - quality is everybody’s responsibility. Similarly, the senior members of the team shouldn’t imagine that  their code doesn’t need to be reviewed . Reviewers don’t need to be the subject matter experts for that feature. Even if the review comments are “I don’t understand this code” - that’s a sign that the code may need more comments. In fact, code reviews are a great way to ease people into learning about new areas of your codebase. . Everyone makes mistakes, and if you don’t realise that it’s a mistake, you’ll never be able to learn. For junior developers, or even those with more experience, it’s easy to get into bad habits, especially if you’re used to working on your own. Pull requests can be a fantastic learning resource - a chance to see the workings of your colleagues’ minds, to consider alternative approaches to solving problems, and to ask why a particular approach has been chosen. . Hopefully you see the value of doing code reviews, and either they’re part of your workflow already, or you’re planning to start doing them. So I’d like to share a few ideas about improving the quality of the code reviews, and hopefully that will help to improve the quality of the code itself. . When we moved from SVN to Git a couple of years ago, we had the opportunity to put a system in place. By using a feature branch workflow, pull requests become part of the process, and in theory no piece of code should go live unless it’s been reviewed. So it’s important that your tools support you. . Github pull requests are good as far as they go, but code review tools like  Stash  provide more advanced features, which we’ve found to be really useful. For example: . Reviewing isn’t just a case of looking at code, but of signing off that you’re happy for it to go live. Stash includes the notion of approvals, and allows you to require a certain number of approvals before the pull request can be merged. One important thing to check is whether the repo is set up so that approvals are cancelled when more changes are pushed to the branch - annoyingly, this doesn’t seem to be the default. . Per-branch permissions can be set up so that only certain members of the team can merge to the main branches - this helps to keep team leads aware of what changes are going to go live. . Another nice feature of Stash is tasks - you can add a checkbox to a comment, and set your repo up so that pull requests can’t be merged while they have tasks outstanding. For instance, you can approve the pull request with the caveat that something needs to be fixed before it gets merged. . One of the drawbacks of Github pull requests is that when someone pushes changes, the diff view will no longer show comments from previous versions, whereas Stash will still show comments on lines that haven’t changed since the comment was made. The other nice thing is that you can easily see which changes you’ve already viewed - especially useful in pull requests that go through a lot of iterations. A useful feature that was in Crucible (Atlassian’s SVN code review tool) but not in Stash, is an indicator of what percentage of the review each reviewer has viewed, so you could get an idea of whether you need to chase up the reviewers. Nice, but not enough to make me want to go back to SVN. . The other good thing about Stash is that it plays nicely with our issue tracker. We have quite a granular workflow set up in Jira, and a lot of the transitions are automated. For example, when a pull request is created, the relevant ticket goes into “Code Review” status, and when the pull request is merged or declined, the status is updated again. We also have integration between Jenkins and Jira so that when the test environments are built, it updates the status of tickets pending build, and the QA team can see which tickets are ready for testing. . This is great, because it means no more nagging people to update the status of their tickets - it’s easy to see the current state of play for any of our projects, and to trust that the status reflects the truth. .  Computers are better at boring jobs than humans . Boring jobs like checking that code comments are correctly punctuated and indented. I’ve written before about  the importance of coding standards , and while it may come across as pedantic, when code is written according to standards, it’s much easier to debug. . Tools like  overcommit  make it simple to integrate code linters into your workflow, and in our  standard Vagrant box , there are pre-commit hooks in place to prevent commits which violate Drupal coding standards. So hopefully this saves reviewers (and the author of the code) from the dispiriting experience of dozens of comments about indentation - much better to get the machine to do the nagging for you. . More importantly, you can set up a continuous integration server to make sure that the change isn’t breaking any tests. For example, whenever a developer pushes to our repos, it triggers Jenkins to run unit test builds on that branch. If your pull request has failed builds, it can’t be merged. . When (or even before) you create a pull request, look at the diff in the code review tool. Before your colleagues look at the change, you should be your own first reviewer. The context switch from editing to reviewing can help you to see the code you’ve written in a different light, and this can sometimes highlight errors or omissions. . Ideally, pull requests are small -  attention fatigue  means it’s almost impossible to thoroughly review large changes. When the diff view is a sea of red and green, it’s really hard to be sure that you’ve got your head around everything. In extreme cases in Stash, you’ll see “This pull request is too large to render. Showing the first 1000 files.” Seeing that message gives me the fear. . When a pull request is bigger than a couple of files, it can be a sign that you might not be splitting your tasks into small enough units. Sometimes it can help if developers create a feature branch, and branch off that for smaller tasks, so that when you come to do the big merge into the main branch, the code has already been reviewed in small chunks. . If in doubt, check out the branch and test it yourself. It’s a really good habit to get into, especially for larger changes that may need a bit more consideration. It may seem time-consuming, but it’s much quicker to find problems  before  they get into a code branch that will be going live. . Sometimes online comments aren’t the best way to communicate. Especially when changes are complex, or if a pull request needs a lot of work, talk them through together. Nobody wants to be on the receiving end of a big pile of negative comments. If it seems like someone needs a lot of guidance, maybe  pair programming  would be a better approach. . When you’re making a comment, don’t just tell your colleague that their code is bad. It’s really helpful to give examples of how it could be improved, or link to useful articles that will help people understand why you’re making that comment. For instance, when I’m reviewing CSS by junior developers, I very often find myself linking to the  Drupal coding standards , or an explanation of  why hover styles should be accompanied by focus styles . . Some developers dread code reviews. I think we’re lucky in that the atmosphere within our team is very supportive, but I can imagine in some organisations a code review could feel like a trial by fire. It should go without saying, but code review comments should never get personal. The aim is to improve the quality of the team’s code, not prove that you’re cleverer than your colleagues. . The other thing to remember is that criticism isn’t just about pointing out things that are wrong. It can have a really beneficial effect on morale to make positive comments about something someone has done well. . Over the last few years, code reviews have really helped our team to improve and learn a lot, and we’ve come a long way towards a culture of quality, but there’s always room for improvement. There are more quality checks that we can automate, but top of my wish list would be to  spin up a test environment for each pull request . I’d also like to get more front-end quality checks automated, perhaps including performance metrics using a tool like  devperf , or visual regression testing using  Wraith  or  Backstop . . But even if you don’t have robots doing all that clever stuff, if code review is part of your process, you’re going in the right direction. And besides, no matter how much you automate, you’ll still need someone to use their judgement. ", "date": "2015-10-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "When It's Clever to Admit That You're Not Feeling Clever\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/development/its-sometimes-clever-to-admit/", "abstract": " On the first day of my first professional job, an experienced colleague gave me a piece of advice which has served me well for many years: . Simplistically, this works because it prevents you from getting in too deep when things get tough.  But it’s not only that - there’s more here than meets the eye. Below are three of the hidden benefits that careful application of this adage can deliver. . Some folks hate posting on Stack Overflow. I love it.  But a good half of my posts never make it to the submit button. . Why? Because frequently, the very  act  of crafting a coherent, to the point, detailed-enough question makes you re-visit your progress to-date, to the extent that the “obvious” solution which you knew was staring you in the face all along finally reveals itself.  It’s because of this that I’ll ask a question on StackOverflow if I’ve been stuck on something for more than half day. . Why Stack Overflow specifically? Because it’s built up a culture where badly asked questions are at best edited by others into what they should have been, and at worst publicly humiliated, all of which is attached to your public profile.  It’s the very threat of this that makes me do a good job.  It’ll make you do a good job too. . And aside from the benefit of solving your own problem, it means you know the questions you do post are genuinely hard ones - hard for you anyway. . TDD is primarily a tool for combating fear.  Don’t believe me? Read Kent Beck’s foreword to his Ur-text Test Driven Development By Example.  And where does the fear come from?  From trying to put too much in your head at once, and then to keep it there while you design, implement and refactor. . And Kent’s right.  Admit it.  When you’re coding, how often do you get the “brain full” feeling because you have to keep so much up there in   L1 cache  ?  In TDD, the tests are there so you can offload loads of that into the code, freeing you to do the creative analysis and thinking, to find the best solution. . But to admit it you need to face the fact that perhaps you’re not as clever - not as good a developer as you think you are (Luckily for me I had a head start in this area; I never thought I was a good developer).  Don’t believe me?  (Re-)read Kent’s book and realise how much he talks about his limitations and shortcomings.  But he’s a great developer right? He’s written all these books and worked on all these systems right? Think about it… . Finally, lets take this specific lesson one step further.  How often have you picked some task up - a bug, a feature, any kind of task in fact, when you’ve had to admit to yourself that “the code / design / other input I’m looking at is just too complicated. I just can’t grok it  at all .”  If we’re being honest, and unless we’ve been very lucky I’m guessing this happens every now and then, or perhaps more frequently. . My experience is that we as developers use this emotional feedback simply to beat ourselves up.  I did, for years and years.  However, as I’ve developed, I’ve realised it’s one of the most useful pieces of feedback you can gift yourself. . Why? Because we’re always told that simplicity is the Holy Grail of development; to be aimed at and prized above all else. Furthermore we’re told that to get to this Celestial City we have to typically work through the Slough of Unnecessary Complexity.  And yet we reward complex solutions - we fete those who create them, and throw laurels at them when they mount a Herculean effort to fix a bug in the face of which many lesser mortals had failed. . How do we combat this? We combat it by trusting our inner voice - the feeling that “this is too complex” and “I’m having trouble holding all this in my head”.  Perhaps if we owned up to these feelings, and even better vocalised when we found something “too complicated”, we’d start off something from which not just ourselves, but others would benefit.  I mean, who amongst us doesn’t want to have been responsible for a “simple” solution? . All these sound like brilliant ideas.  Lets go! But something will stand in your way - your Ego, and it’s associated fear of being knocked.  Get over it!  These are only the beginnings of the benefits that you can realise if you give yourself the gift of admitting when you’re not feeling too bright.  Others riches will soon be yours too - colleagues will want to work with you more, you will learn from them and they will learn from you, and freed from the tyranny of self-doubt, you’ll be plugged into the mainline of simple elegant design and be reaping the benefits.  Let go! Kill your ego! . Like this post? Think you’re the kind of person who’d thrive in the environment where you’re encouraged to challenge the status quo. Know you’d love to collaborate with others, create simple and elegant solutions to customer’s complex problems?  Drop us a line ‘cos we’re hiring. ", "date": "2015-03-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Microservices Gotchas\n", "author": ["Nick Walter"], "link": "https://capgemini.github.io/architecture/microservices-gotchas/", "abstract": " Don’t get me wrong, I’m not anti Microservice, far from it. . Come and ask me and I’ll give you a lot of reasons why they can benefit any development project. And there are plenty of posts on the web and on this blog that talk about the very good points of implementing a system using Microservices. . However after getting one implementation into production, and now being the majority of the way through a second, I have some observations from the other side of the coin that you may not have thought about. . Yep, there it is everybody, my implementation of your requirements … it’s all there, go ahead take a look. Once upon a time you would implement the “system” and to everyone it would be that box on the diagram that did stuff. Something went in, and things happened and something popped out the other end. That was the “monolith” and in the new world you have multiple services running that make up that “system” with lines of communication draw between them. And that’s great. It means you have a flexible architecture, you can make changes to your services and not affect the whole system. . But it does have a side effect, now everyone sees your implementation and it can turn into something like a mass code review with people you wouldn’t expect pointing things out. You may have noticed I didn’t say “dirty” laundry in the title, as sharing the actual implementation isn’t a bad thing. But managing the message is something you should be ready for. I don’t think I was ready to talk about such low level implementations of our code to everyone. . Think about how you talk and document your system, pitch it at the right level for the audience. Drawing lots of boxes with a spaghetti of overlapping lines can scare the hell out of some people. . Microservices have to talk to each other; That is pretty obvious right? Or is it? When as a developer you’re implementing a single requirement it may not be. Take a step back and look at the bigger picture, and it can become very obvious that lots of small conversations are starting to give you a major headache. . In our implementation we’re communicating with REST over HTTP, which is pretty light-weight, but given high volumes it did cause us some issues. We had to re-think one of our design decisions to resolve it. Luckily we found this out before it went into production and fixed it with very little fuss. How? Performance Testing that’s how. . I can’t stress how important Performance Testing your Microservices is. If you can get an understanding of your volumes and expected performance early on, then you can get on testing it as soon possible. Other “gotchas” to bare in mind: . Make the tests realistic. Is hitting every API at the maximum TPS at the same time the right thing? . Use as real a dataset as possible. . With real volumes. . Push it through as close to a realistic production setup as possible. . This will flush out something that you may not have expected. Usually a piece of hardware or a restriction that’s outside of your domain. Doing Performance Testing early on gives you the time to make the necessary changes in time. And given you’re using Microservices the change is going to be nice and contained, right? Watch this space for a future blog on Performance Testing Microservices. . You’re writing good code, and you’re writing unit tests. You feel confident. But you still need external testing, the question I got asked a lot by our testing team was how do I test this story, where do I do that? And you can understand why. . We had built a suite of integration Microservices, that you could put into 2 groups; “smart services”, which exposed APIs that do specific jobs and “pipeline services”, which glued together calls to the different smart services (usually in a very specific order) in order to a complete a business task. Despite this, we still found at certain points we got swamped by the amount of test effort required to test every single Microservice. It created a lot of noise within the team. . What helped us was to identify the right Microservices that could be externally tested. Helping to focus the teams effort and reducing the noise of defects that may never occur. . By their very nature Microservices are a distributed architecture. This brings its challenges when debugging issues; Where do you look? You get told one of your APIs is throwing an error. With the Java “monolith” you had a had a stacktrace that you could work your way down and find the line of code that caused the error. But with Microservices it could be anywhere, especially when your service calls multiple down-stream services, and in some cases in parallel. . Something simple that developers can overlook is logging and logging the right details, you need to be able to trace a message from it’s entry point to everywhere it gets routed, so having some key data that makes a message unique and “searchable” is a major plus point. Additionally don’t just log on errors, you want to know to where it went and how it got to be there. Monitoring is also a big winner for us; knowing if services are running and performing within norms helps identify issues. We’re using  Codahale metrics  outputting to  Graphite  and  Graphana , but to get a runtime view we’re moving towards implementing  Netflix’s Hystrix  Dashboard with  Turbine  in order to see how all our Microservices and their dependencies are behaving. . Some additional reading if you get time: .  Martin Fowler - You Must Be This Tall To Use Microservices  .  Russ Miles - Building Adaptable Software With Microservices  .  Tony Mauro - Adopting Microservices at Netflix: Lessons for Architectural Design  ", "date": "2015-04-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Code Beauty\n", "author": ["John Nash"], "link": "https://capgemini.github.io/development/code-beauty/", "abstract": " When was the last time you or a colleague looked at some code and made a comment about its attractiveness? You may have said that some code was “ugly” or, if you were more fortunate, you might have called it “neat” or “elegant”. What is it about code that makes us say and think these things and why should we care, after all our compilers, interpreters and computers generally don’t care? . This is a topic that Adam Tornhill covered, amongst other things, in his DevWeek 2015 talk “Code that fits your brain”. It’s an interesting lecture and Adam is an entertaining speaker so I recommend it to you (you can find the  DevWeek slides here  and  an earlier presentation of the talk here ). . When all other factors are equal, humans prefer attractiveness or beauty and we instil subjects that we find attractive with additional positive qualities. For example, we perceive people who are more attractive to be more intelligent and competent. It seems that this bias is something that is hard-wired into us. Even new born babies, who haven’t had time to develop any other preconceptions, exhibit this behaviour, as shown by their tendency to prefer to stare at more attractive faces (Slater, et al - Newborn Infants Prefer Attractive Faces). It’s clear that there is some deep seated psychology at work here. . The theory is that we use beauty subconsciously as an approximation for good health. From a survival and evolutionary point of view this makes a lot of sense. In the past you couldn’t medically tell the health of a new potential ally or mate, but if they looked attractive there was a good chance that they weren’t suffering from any detrimental illnesses. That was probably a good enough measure to use when deciding if you wanted to hunt sabre tooth tigers together. . So if people see your code as attractive, they should be more inclined to trust it and want to work with it. Code beauty is a proxy for code quality that we can immediately and intuitively make use of, without any deep analysis work or tools. That’s not to say those tools and techniques aren’t useful, just that people will form an opinion of a code base without them. . How do we go about making our code attractive? What does attractiveness even mean when we talk about code? .   . Back in 1995, psychologists working on a study made a discovery. They found that the majority of people rated the face on the top left as the most attractive. That’s interesting because the face doesn’t belong to a real person. It is a composite that is made up of the average of other faces. . It turns out that the more faces you take an average from, the more attractive the face becomes and this suggests that beauty is in fact a negative concept. It is the absence of imperfections, that got averaged out, that make the face more attractive. We can use this idea to help us decide what beautiful code is. . Code is beautiful when it has all the imperfections removed. For code an imperfection is unnecessary distraction and noise that prevents us from seeing and reasoning about the problem domain and its solution. Anything that puts an additional mental load on the reader can be considered to fall into this category. For example: . Inconsistencies that draw attention where it isn’t needed, . Large code blocks that don’t fit easily into our brain’s working memory, . Special cases (such as error handling), . Code that is written in language that is at a lower level than the problem domain. . In fact the effects of these sorts of distractions and noise can mount up over time to such an extent that a code base can become unreadable and impossible to maintain. You only have to read the opening chapter of Robert Martin’s book Clean Code (a book that I highly recommend if you haven’t already read it) to get a sense of how big an issue this can become and how it can kill projects. . I could stop here, but there’s one last gotcha regarding code beauty that I think I should flag. We all prefer (and find attractive) familiar things. This is called the  Mere-exposure effect  and it can make you select styles or techniques purely because you’ve used them before and this in turn makes you reluctant to adopt new styles or techniques. . As an example, you might prefer this code, written in C# in an imperative style. . Or you might prefer this code, written in Clojure in a functional style. . They both do the same thing and your preference probably depends on what sort of code style and language you usually use. . So, next time you’re writing some new code, or updating some existing code; don’t stop once you’ve got it working. Spend a little bit of extra time asking yourself: . “Could I make this code a bit more beautiful?” . “Could it read a little more clearly?” . “Could it be a little less cluttered?” . If the answer is yes, make that change. Making your code a little bit more attractive reflects well on you and the extra time will pay your team back tenfold by reducing further development and maintenance costs. ", "date": "2015-04-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Apollo, an open-source platform for running your apps\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/apollo/", "abstract": " Today we are releasing  Apollo  - A platform for running your next generation web services and applications. . This is a project that we have been experimenting with internally to power microservices and big data platforms for our clients. . Apollo is built on top of a number of open source components and has bootstrap scripts for  Vagrant ,  AWS  and  Digitalocean . We are aiming to expand cloud support and features even further in the coming months. If you are interested in getting involved, or just want to see where we are going you can  check out our current roadmap . . Lets have a look at some of the features / components of Apollo. . The following open source components are baked into Apollo core: .  Apache Mesos : The backbone of Apollo. Mesos handles cluster management and orchestration. Highly scalable and fault tolerant out of the box Mesos allows you to simplify the datacenter treating your cluster like one big machine. Mesos handles resource availability (CPU/memory/storage) and is able to distribute arbritrary tasks across your datacenter, making it ideal for big data workloads.  http://mesos.apache.org      .  Docker : For packaging and running your applications. We believe Docker will be a big player in the future of building and running applications in the enterprise. In combination with Mesos frameworks like  Marathon  and  Aurora  you can build your own platform as a service leveraging Linux containers.  https://www.docker.com/      .  Consul : For service discovery and DNS. Never hard-code an application endpoint or service URL in your configuration files again. We use Consul to automatically register service addresses, ports and arbritary information about Docker containers that start in the datacenter. It also comes packed with its own built-in DNS server so you can query services easily.  https://www.consul.io/      .  Weave : Weave simplifies the networking for Docker containers. We use Weave to create a separate overlay network which all containers start up in. This removes some of the headaches around Docker networking - no need to carefully co-ordinate which ports are mapped from host &gt; container anymore. All containers inside the Weave network can communicate with each other directly using their Consul DNS and port meaning the end result looks a lot like standard networking (in the bare-metal / VM world).  http://weave.works/      .  Terraform : Terraform allows you to rapidly and consistently spin up resources in the cloud. Think Amazon Cloud Formation / OpenStack Heat but cloud agnostic. We’ve already built terraform plans that allow you to spin up Amazon VPC and Digitalocean public cloud. We’re looking to expand that with support for Rackspace(Openstack) and Microsoft Azure.  https://www.terraform.io/      . For further documentation on Apollo components see the docs on Github  https://github.com/Capgemini/Apollo/tree/master/docs . . ## Use Cases . We think Apollo fits nicely into the following use cases - . Build your own internal Plaform as a service (PAAS) leveraging Docker container runtimes . Large scale Continuous Integration using the Jenkins Mesos framework. The folks at  Ebay have already done this in production  and we’re looking to provide the Jenkins framework as a Docker container plugin to Apollo soon. . Manage and distribute your big data workloads. There are a number of big data processing frameworks built on Mesos, such as  Spark  and  Storm . Again we’re looking to provide these pluggable to Apollo in the future. . If you manage to get any of the above or other Mesos frameworks working on Apollo, please think about contributing that back via a  pull request  . ## Give it a spin . Dont take my word for it, give it a spin. We have some more  documentation on our github  and some  getting started guides  for various cloud providers. . ## Credits . Thank you to following peeps for providing some amazing open source tools and software that we’ve built upon: . The guys at  Docker, Inc ,  Hashicorp ,  Weaveworks , All the contributors to  Mesos , and anyone who has ever provided a patch into all of the open source good stuff we are using here. . For more information, please see our Github repository -  https://github.com/Capgemini/Apollo . ", "date": "2015-05-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Drupal integration patterns\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/drupal/drupal-integration-patterns/", "abstract": " As Drupal has evolved, it has become more than  just a CMS . It is now a fully fledged Web Development Platform, enabling not just sophisticated content management and digital marketing capabilities but also any number of use cases involving data modelling and integration with an endless variety of applications and services. In fact, if you need to build something which responds to an HTTP request, then you can pretty much find a way to do it in Drupal. . “Just because you can, doesn’t mean you should.” . However, the old adage is true. Just because you can use use a sledgehammer to crack a nut, that doesn’t mean you’re going to get the optimal nut-consumption-experience at the end of it. . Drupal’s flexibility can lead to a number of different integration approaches, all of which will “work”, but some will give better experiences than others. . On the well trodden development path of Drupal 8, giant steps have been taken in making the best of what is outside of the Drupal community and “getting off the island”, and exciting things are happening in making Drupal less of a sledgehammer, and more of a finely tuned nutcracker capable of cracking a variety of different nuts with ease. . In this post, I want to explore ways in which Drupal can create complex systems, and some general patterns for doing so. You’ll see a general progression in line with that of the Drupal community in general. We’ll go from doing everything in Drupal, to making the most of external services. No option is more “right” than others, but considering all the options can help make sure you pick the approach that is right for you and your use case. . One option, and probably the first that occurs to many developers, is to implement business logic, data structures and administration of a new applications or services using Drupal and its APIs. After all,  Entity API  and the schema system give us the ability to model custom objects and store them in the Drupal database;  Views  gives us the means to retrieve that data and display it in a myriad of ways. Modules like  Rules ;  Features  and  CTools  provide extensive options for implementing specific business rules to model your domain specific data and application needs. . This is all well and good, and uses the strengths of Drupal core and the wide range of community  contributed modules  to enable the construction of complex sites with limited amounts of coding required, and little need to look outside Drupal. The downside can come when you need to scale the solution. Depending on how the functionality has been implemented you could run into performance problems caused by large numbers of modules, sub-optimal queries, or simply the amount of traffic heading to your database - which despite caching strategies, tuning and clustering is always likely to end up being the performance bottleneck of your Drupal site. . It also means your implementation is tightly coupled to Drupal - and worse, most probably the specific version of Drupal you’ve implemented. With Drupal 8 imminent this means you’re most likely increasing the amount of re-work required when you come to upgrade or migrate between versions. . Drupal sites can benefit hugely from being part of the larger PHP ecosystem. With  Drush make , the  Libraries API ,  Composer Manager , and others providing the means of pulling external, non-Drupal PHP libraries into a Drupal site, there are huge opportunities for building complexity in your Drupal solution without tying yourself to specific Drupal versions, or even to Drupal at all. This could become particularly valuable as we enter the transition period between Drupal 7 and 8. . In this scenario, custom business logic can be provided in a framework agnostic PHP library and a  Naked Module  approach can be used to provide the glue between that library and Drupal - utilising  Composer  to download and install dependencies. . This approach is becoming more and more widespread in the Drupal community with Commerce Guys (among others) taking a  libraries first approach  to many components of Commerce 2.x which will have generic application outside of Drupal Commerce. . The major advantage of building framework agnostic libraries is that if you ever come to re-implement something in another framework, or a new version of Drupal, the effort of migrating should be much lower. . Building on the previous two patterns, one of Drupal’s great strengths is how easy it is to integrate with other platforms and technologies. This gives us great opportunity to implement functionality in the most appropriate technology and then simply connect to it via web services or other means. . This can be particularly useful when integrating with “internal” services - services that you don’t intend to expose to the general public (but may still be external in   the sense of being SaaS platforms or other partners in a multi-supplier   ecosystem). It is also a useful way to start using Drupal as a new part of your   ecosystem, consuming existing services and presenting them through   Drupal to minimise the amount of architectural change taking place at one time. . Building a solution in this componentised and integrated manner gives several advantages: . Separation of concerns - the development, deployment and management of the service can be run by a completely separate team working in a different  bounded context . It also ensures logic is nicely encapsulated and can be changed without requiring multiple front-end changes. . Horizontal scalability - implementing services in alternate technologies lets us pick the most appropriate for scalability and resilience. . Reduce complex computation taking place in the web tier and let Drupal focus on delivering top quality web experience to users. For example, rather than having Drupal publish and transform data to an external platform, push the raw data into a queue which can be consumed by “non-Drupal” processes to do the transform and send. . Enable re-use of business logic outside of the web tier, on other platforms or with alternative front ends. .  Headless Drupal  is a phrase that has gained a lot of momentum in the Drupal community - the basic concept being that Drupal purely responds with RESTful endpoints, and completely independant front-end code using frameworks such as Angular.js is used to render the data and completely separate content from presentation. . Personally, I prefer to think of a “nearly headless” approach - where Drupal is still responsible for the initial instantiation of the page, and a framework like Angular is used to control the dynamic portion of the page. This lets Drupal manage the things it’s good at, like menus, page layout and content management, whilst the “app” part is dropped into the page as another re-usable component and only takes over a part of the page. . For an example use case, you may have business requirements to provide data from a service which is also provided as an API for consumption by external parties or mobile apps. Rather than building this service in Drupal, which while possible may not provide optimal performance and management opportunities, this could be implemented as a standalone service which is called by Drupal as just another consumer of the API. . From an Angular.js (or  insert frontend framework of choice ) app, you would then talk directly to the API, rendering the responses dynamically on the front end, but still use Drupal to build everything and render the remaining elements of the page. . As we’ve seen, Drupal is an incredibly powerful solution, providing the capability for highly-consolidated architectures encapsulated in a single tool, a perfect enabler for projects with low resources and rapid development timescales. It’s also able to take its place as a mature part of an enterprise architecture, with integration capabilities and rich programming APIs able to make it the hub of a Microservices or Service Oriented Architecture. . Each pattern has pros and cons, and what is “right” will vary from project to project. What is certain though, is that Drupal’s true strength is in its ability to play well with others and do so to deliver first class digital experiences. . New features in Drupal 8 will only continue to make this the case, with  more tools in core  to provide the ability to build rich applications,  RESTful APIs for entities out of the box  allowing consumption of that data on other platforms (or in a headless front-end), improved HTTP request handling with  Guzzle  improving options for consuming services outside of Drupal, and much more. ", "date": "2015-05-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Last year an unconference changed my life\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/learning/last-year-an-unconf/", "abstract": " NOTE: This post first appeared on  Voxxed.com . .  Come to  Unvoxxed , the newest London Unconference brought to you by The Java Posse Roundup and  Devoxx UK , 15-16 June, £125.00 - £115 if you book before the end of 21st May and quote the code “UNV1015”  . In 2005 I went to my first JavaOne conference expecting something amazing to happen; something transformative even. Please don’t get me wrong, it was great. But by the end I was tired and overwhelmed. Some sessions had been amazing and inspiring, some hadn’t.  Little did I know, a decade later, I’d be co-organising my own event:  UnVoxxed  - £125.00 for Early Bird, 15-16 June, London. But for the 2005-me, that was all a long way off. . At the good sessions, I’d desperately wanted to speak to the speakers afterwards – but these were inevitably the super-well-subscribed ones and the lines were long, and I’d not really had any idea what I wanted to say besides “thanks”; I bottled it every time, justifying this to myself by concluding this line was too long if I wasn’t to miss the next possibly great session. . So I just left everyone else to it, and went and sat in the next darkened room, looking at slides, and trying to take notes. .   .  image credit: Wikipedia  . Back home, when I heard folks talk about conferences or read about their experiences, they always seemed to have had a different, more transformative experience to mine – it was all about the “hallway conversations” many of them said or wrote. . What conversations? I’d had none. I’d been too shy – too shy to even chat to the guy in front of me in the queue for free coffee. Perhaps if I had persisted and gone up to the front things might have been different. Was I consigned to just being a spectator? Was this all conferences had to offer me? . I refused to believe it. I was lucky enough to attend a few more JavaOne conferences, each time determined to break out of my shell a little more. But while I became more confident, the ideal circumstances for that golden interaction never presented themselves. I got a lot out of attending, but I was still a spectator, never a participant. . Then I went to the third  Java Posse Roundup . . Rather than being hosted in Moscone, downtown San Francisco and attended by many thousands of attendees, the Roundup was in a church hall, in Crested Butte Colorado. On my first year, I think there were about 50 of us in attendance. More significantly, there were no pre-planned sessions, and no speakers. There were only attendees. Not only that, there wasn’t much in the way of organisation. That’s because it was self-organising. We were the organisers. .   .  image credit: Wikipedia  . And it was  phenomenal . . I initially still had my “fear” (Bruce Eckel reminded me later that I was incredibly quiet on the first few days) and whenever I opened my mouth to contribute my heart raced. But slowly, the fear drained away, to be replaced by excitement. If I recall correctly, by the third day I’d proposed my first session, but I’d also contributed – in small ways to begin with – to many more. . What had happened? The conference had made itself. There were folks who’d been there before, and folks who were new. The first day had been kicked off by Bruce with a quick intro to the “rules” and mechanics, and then we were off. Sessions were put on post-its and stuck on a pre-prepared grid cross-referencing venues and time (the venues were different rooms in the Church hall as well as the coffee shop across the street). . Just before the allotted session time, folks would head off to where the session they were interested in / had proposed was allocated. We’d then sit in a circle / semi circle / concentric circles (depending on the size of the group) and wait, perhaps introducing ourselves or chatting a little to pass the time. With all the usual paraphernalia of conferences removed, it was just a self-selected group of people with a shared purpose. .   .  image credit: Handerson Gomes  . When a session began, the convener would introduce themselves, and then the topic, and perhaps talk for a few minutes, ad-libbing, to get things going. They’d then relinquish “control” and the discussion would start. There were no more “rules” aside from the “Law of Two Feet” – if you felt the session wasn’t for you, you were obliged to leave to avoid bringing down the energy – and “No Lecturing”. I never had to leave a single session in the entire week – things were too engaging.  I never saw anyone lecture either – what would you gain when the contributions of the group were potentially so much richer? . How the conversations proceeded depended significantly on the folks present. Some had a interrogative style, others liked to offer experiences and anecdotes, others seemed to take on a role of natural facilitator, bringing things back to topic or re-energising things when they slacked off.  Sometimes we would uncover a topic far bigger than had been anticipated by the proposer. When this happened we’d create a spin-off session and put it up on the board as another post-it.  As the week progressed it became clear we’d achieved our goal – looking back at the board it was clear we’d built our own conference, and wow, what a conference we’d built. .   .  image credit: Handerson Gomes  . OK, so all of this is great, but it’s mostly just mechanics. Why did it change my life? . Well, firstly, it changed my life because a conversation is far more stimulating and engaging than a monologue. Everything else is stripped away – you must engage with the topic because there is nothing else. . Not only that, but you must engage with the conference because you are part of it. By engaging, I’d been forced to overcome my fear – it had happened because it just looked so damn exciting to contribute, and it was.  I’d ended up interacting directly with some of the “famous” attendees, but I’d also had amazing conversations with folks just like me. . And in having these conversations, I’d realised that perhaps there was less separating the “famous” folks and the likes of the rest of us. These folks weren’t up on a stage any more. They were simply sitting on another church-hall chair, just like me, and we all treated each other as peers and co-contributors. . And that’s when an epiphany struck me – and this was the thing that really changed my life. Seeing the strengths and weaknesses of everyone present, and realising the myriad roles that can be played in this insane circus called Software Development – seeing that in some small areas there are valid and valuable things that I might be able to contribute – this changed my life forever. .   . I realised that aspect of the unconference I enjoyed the most was facilitating the conversations that we were having. And it’s because of this that last year I worked with Dick Wall, originator of the Java Posse Roundup to bring things to the UK, and it’s why we’re doing it again, under a different title, but with exactly the same ethos. .  This year, on the 15th – 16th of June, Dick and I are putting on  Unvoxxed , in association with Devoxx. The theme is “Programming at any Scale” and it’s in the Crypt on the Green, Clerkenwell, you can partake for the tiny sum of £125.00 for two days. I urge you to come. Trust me, its the best thing you’ll ever do.  Oh, and if you quote the discount code “UNV1015” you’ll get a further £10 off the already  ludicrously cheap  price.  .  If you have any questions, please add a comment on this post, or ping me on twitter at @al94781 or @unvoxxed. Hopefully, we’ll see you there.  ", "date": "2015-05-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Making ethical development decisions\n", "author": ["Rob Kerr"], "link": "https://capgemini.github.io/development/making-ethical-development-choices/", "abstract": " We are moral agents.  Each of us makes countless choices throughout the course of a day, and many of these choices have ethical parameters and ramifications.  We seldom, if ever, stop to think about those choices and their impact on ourselves and on others.  It’s my aim in this blog post to redress that lack, and hopefully to help us think more clearly about the moral choices that face us as developers. . In order to illustrate my points, first let me introduce you to Getchure Widgets Co.  Getchure Widgets have a business model selling widgets to unsuspecting customers, and they have hired our team at CleanTeam to create their website.  We here at CleanTeam have many projects on the go, and the Getchure Widgets site is only one of our current projects — but an important one, a flagship project.  So the relationship is important not just reputationally, but monetarily. . Our team for the Getchure Widgets project is a few developers (of which you, dear reader, are one, as am I); a designer; a project manager; and our usual DevOps colleagues.  We meet with representatives from Getchure Widgets every week, and have a good relationship.  We are just about to start a series of sprints that will result in the release of some important new functionality for the Getchure Widgets site, featuring a brand new line of widgets, and we’re all excited about the work. . Balancing decision-making is a juggling act. Often it’s the people who shout the loudest that get decisions to go their way, and of course, we must be aware of the  hippo in the room . Unfortunately, this often means that users, whose voices are not loud, and whom we must seek out in order to hear their voices, can become the smallest part of the equation, when ideally (to some) they would be the largest. . We are moral creatures, and we have moral instincts. But in order to gain access to those instincts (having had them atrophy due to disuse) we need to trigger them by asking the right questions. Start asking yourself moral questions: the answers might lead you in surprising directions. . In our last meeting with Getchure Widgets, they mentioned a feature they’ve asked for which they’re quite keen to get live as early as possible.  They want to encourage customers to sign up for updates and news items related to widgets they have purchased in the past.  They’ve seen a pattern in use on the web which they think will lead to more people signing up, and they’d like us to implement that pattern.  We have some concerns…. . Every decision that we make will affect someone, either directly or indirectly.  Therefore, when we make decisions that have moral parameters, it behooves us to trace the chain of impact as far as we need to, so that we can know on which side of the fence the angels would fall.  A useful start to this process is asking who will be affected by your decision. . Clearly, any change to functionality on the site will affect users of the site.  However, the choices we make have much wider effects than the merely obvious.  The first person to be affected by any choice is the person making the choice.   Reinforcement Theory  suggests that once we make a choice, of any sort, we are subsequently far more likely to make the same choice in the future in similar situations.  We should therefore make certain that we are making the best choice we can; and further, we should ensure that we have the best information to inform us about our choice.  If we make choices based on information, then new information will permit us to make different choices, where we might otherwise have been swayed by our beliefs, attitudes, or prejudices. . Decisions we make also affect our team.  If we implement the pattern Getchure Widgets have asked without asking any questions, that sets a precedent for our client interactions that the rest of the team can see and use as a model.  Is that the kind of model we want to be putting in front of them? . Our team also have their own moral intuitions.  We need, when making choices that affect the product (particularly in an Agile environment), to be certain that the team has visibility of all changes, and the opportunity to argue against them. . Clearly, decisions that we make will also affect our client: both directly and indirectly, propagated to them via the direct effects we have on the piece of work we are undertaking on their behalf.  As ethical developers, therefore, we should strive to make sure that the effects of our decisions that pertain to the immediate task do not propagate adversely to the client.  All of us have likely been involved in discussions about proposed changes where we argued that the net effect of the change would be damaging from a reputation point of view.  In these cases, we are clearly arguing from an ethical position.  However, the decisions that we make on a day-to-day basis can also have this kind of damaging effect on reputation.  When we kludge a fix for a bug and push it live; when we fail to complete our target test coverage; when we choose the easy option over the more correct but also more time-consuming option; in these situations our choices adversely affect the reputation of the owner of our site, because while the end user may not know it,  their experience could have been better . . Decisions that we make as developers clearly redound to the benefit of some parties, as otherwise, no decision would be necessary.  Often, though, we fail to look beyond the immediate beneficiaries, and thus our weighing of the pros and cons of any given decision may be biased.  We might choose a path based on the immediate beneficiaries without considering another option whose immediate beneficiaries were not as compelling, but whose more remote beneficiaries could have been a much stronger selling point, did we only take them into account.  How can we fix this?  Here are some points to consider. .  Proximate benefits versus ultimate benefits   In the case of Getchure Widgets’s new requirements, clearly the immediate benefit of acceding to their demands is a satisfied client, the ability to start work straight away, and a sense of progress.  However, if we look beyond the short term, we might see that those benefits are illusory, and the long-term benefits of arguing the right thing with Getchure Widgets could be a better product, more trust between Getchure Widgets and its customers, and more trust between Getchure Widgets and us as developers.  Sometimes we can be our clients’ best advocates in the marketplace, and save them from themselves. .  Short-term benefits versus long-term benefits   If we focus on the short-term when making decisions, we store up problems for our future.  This can most clearly be seen when making architecture decisions, or when deciding what technology to use to solve a problem.  It is also seen in questions around the benefit of refactoring, and it is here that we as developers can really sell refactoring to clients.  Building trust with a client entails transparency about decisions, and  sometimes creating technical debt is unavoidable   We know that in the short-term, clients prefer new functionality to invisible improvements in old code.  But as developers, we should be able to sell refactoring as a  reduction in time to market  for future features, building on the trust we establish by looking beyond proximate benefits.  For every development decision we make, then, we should be asking ourselves, are we creating technical debt?  Is it necessary?  Are we kidding ourselves, and actually creating  technical waste ?  These questions may not be answerable by one person, and some developers will draw the line in different places.  Peer review, but more usefully, pre-implementation discussion, can be valuable in making these decisions. .  Effect size   Fairly simple, this one: it is merely to point out that close inspection of any decision can uncover repercussions that are well beyond what might have been thought on a cursory inspection.  It therefore behooves us as conscientious developers to think clearly and thoroughly about the effect that changes introduced by our decisions might have.  Which leads us to… .  Ripple effect   The effect of our decisions ripples beyond our immediate circle of influence.  Being aware of this ripple effect can help us to make better decisions that are more engaged with the entire effect size of our decision. . While modern commerical transactions (in the main) are mandated on the idea that commerce is not a  zero-sum game , for any decision that we make there is an opportunity cost to us the decision maker, and potentially to some other party or parties.  In our Getchure Widgets question, deciding to value the long-term benefits over the short-term gains might mean that the person in favour of the short-term decision at Getchure Widgets loses face.  Our question then becomes, can we eliminate this loss?  If we can’t, can we minimise it?  Can we spread the risk of loss for any decision so that its impact doesn’t fall solely on one person (or organisation)?  Can we apportion potential loss to those best equipped to deal with it?  And if we can, is it a moral choice to do so? . When we make a decision to remove or change functionality, or reduce scope, recognise that we are adversely affecting some hypothetical person’s experience of our product.  Conversely, we may be enhancing someone else’s experience.  Is the balance of harms equally weighted?  Questions of accessibility are clear examples here, where the harm done to a user by removing accessible features clearly outweighs the harm to a user of ignoring or disabling accessible options they do not need.  If the loss of functionality, and therefore the harm, is temporary, then we can say that the loss is mitigated, and thereby potentially justifiable.  But we need to ensure that we are not facilely attributing mitigations to actual features lost (and harms done thereby).  We must be sure that any mitigation of losses is real and not imagined before we make a decision to proceed. . When we make decisions that negatively impact people, we should clearly try to minimise those losses.  But then our question becomes, what does that mitigation cost us?  There is a trade-off in resource usage between mitigation of loss and development of new features, between time used and cognitive load of switching between different requirements, and between all of the above and their perceived benefit.  In general, it is better to focus on maximising gains rather than reducing losses, as users (and clients) value gains more than reductions in negative impacts.  But this will vary from user to user, and our point above about long term benefits of refactoring will also play a part in deciding when to draw a line under minimising losses created by the decisions that we make. . This may be the crucial question, certainly for new developers.  The answer lies in empathy and understanding, in experience of the effects of previous decisions, and in the nature of being human.  To be human is to be able to imagine what the consequences of a decision will be without having to actually go through with the decision (and thereby find out exactly what the effects might be through hard-won experience).  In philosophy, this is known as  higher-order thinking  where we think about what we might think should a certain state of affairs come to pass.  We do this naturally in social situations.  This quote clearly demonstrates that it is natural for us to do this kind of thinking: . I know you think you understand what you thought I said, but I’m not sure you realize that what you heard is not what I meant. .   Alan Greenspan  . But aside from this natural anticipatory sense that we all share to one degree or another, there are specific things we can do to aid our native senses. .  Put yourself in the user’s place   Be empathic.  Understand how actual users use your product, and how your decisions might affect their experience.  Talk to actual users, watch them use the product and gain a real-life understanding of how your decisions might change their experience.  Do this again and again.  Over time, build up a picture of how users interact with your decisions, and how they react to changes.  Gain experience of how users behave in general.   BUT  remember to check your assumptions! .  Prototype and test   One of the best ways to anticipate changes and their effect on users (and others) is to trial them quickly, and gauge reaction, and then to feed that back into the decision-making process.  It is essential when doing this to be timely, and to make efforts to avoid sources of bias.  There is a great deal of research on avoiding bias in testing with users (and in making decisions) out there, and I urge you to seek it out.  http://alistapart.com/article/collaborative-user-testing-less-bias-better-research is a good starting point.  Understanding your own biases can help you to avoid them, though this is always difficult, and you will generally be better off relying on others to point out your own biases while you point out theirs.  This can be painful, but also a source of fun in office-based teams ;) .  Beta feedback   If it is possible for your product, releasing beta versions and getting feedback from early adopters and other users can be invaluable,  so long as the feedback is genuine, the user sample is random (well, as random as it can practically be), and the feedback reaches the decision makers unfiltered . .  Follow the chain   Don’t be satisfied with uncovering the immediate impacts of decisions that you make.  Follow the logical consequences as these propagate outwards.  The ripple effect of our decisions can be anticipated, but only with a cost in time and structured thinking. . As I said at the outset, we are moral agents, and we make moral decisions all the time in our lives.  However, it is much rarer that we formally decide on a moral pathway having considered fully the alternatives.  Where can we see examples of this process, and where can we get advice on how to process moral quandaries? . Our peers, colleagues, and life in general are rich sources of moral choices that need to be made.  If you have children, you will have made moral decisions regarding your care of them, and these decisions will often have been carefully considered.  Each time you are faced with a choice in your life outside work, you can choose to inspect the moral ramifications of that choice at your leisure.  This can then become a framework for how you approach the same kinds of choices in your working life. . You could choose to adopt a specific ethical framework or guiding principle.  Philosophy has been concerned with ethical choices from the time of Aristotle and before, and a great deal of careful thought has passed under the bridge, the conclusions of which are available for all to consume.  Thought experiments like the  trolley problem  can help to shed light on what your ethical intuitions are, and understanding these will help you to align the choices you make with your own moral instincts. . I’d like to close by giving an example of the foregoing, using the  Kantian ethical principle  of universalising choices.  This principle requires that, when a moral choice is extended to all people, a contradiction does not occur.  I like to think of this as asking, for any choice I make, whether I would want to live and work in a world where everyone made the same choice I make.  For Getchure Widgets, if everyone simply acceded to the demands of the client without any argument, that would quickly contribute to a working environment where developers were not empowered to argue decisions they did not agree with, and eventually to developers becoming simply machines for executing the commands of management.  That would not be a world I would wish to live in, and so the moral choice from a Kantian point of view would be  not  to accede to those demands without arguing for a better way. . Finally, you need to inspect your choices.  If you make every choice as though it were independent of the others, you cannot gain any understanding of yourself as an agent of change.  Decisions we make as developers contribute to an environment where moral considerations are at the forefront of people’s thinking.  If we ignore our contributions to that enviroment, we can fail to adjust our understanding sufficiently to incorporate new influences and new agents also making choices in the same space.  As moral agents making choices, it behooves us  if we are making good moral choices  to enable ourselves to continue to influence the moral atmosphere in which choices are made.  Sometimes that will mean that the very best choices are not available to us.  (The kinds of choices where, in protest at a decision whose morality one doesn’t agree with, one resigns.) The difficulty of making ethical choices lies not in the easy decisions, but in the hard ones.  Ultimately, making the most moral choice can mean making the hard choice, because it is the  most right  of the choices available to us. ", "date": "2015-06-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Demo: Launching an Apollo cluster on AWS\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/apollo-launch-aws/", "abstract": " This post aims to show how you can get up and running with an  Apollo  cluster on AWS inside 5 minutes. . For more background information on Apollo see  our Github repo  or the  original post  which gives an insight into the various components. . In short, Apollo is a fully open-source PAAS built on primarily on Apache Mesos and Docker as well as  some other components . . For the purposes of the demo we will spin up a 5 node cluster on AWS public cloud in the eu-west-1 region. . The final deployment architecture will be - . 3 Mesos master nodes running  Mesos  (in master mode),  Marathon ,  Consul  (in server mode),  Docker ,  Weave  and  Weave Scope . . 2 Slave nodes running Mesos (in slave mode), Consul (in agent mode), Docker, Weave, Weave Scope and HAProxy. . If you want to short-cut the text and  skip straight to the video  feel free. . ## Prerequisites . It’s worth reading through our  getting started guide for AWS public cloud , but essentially, you will need - . An active account setup on  Amazon AWS  . An account setup on  Hashicorp Atlas  . An SSH key created, ready for pushing to AWS: .  Terraform  installed and in your $PATH .  Python  &gt;= 2.7.5 installed along with  pip . . To start, lets clone the repo and install some dependencies - . Next, we need to set some environment variables in our shell. Usually I like to keep a little script that i can  source  to bring those into my current shell environment, but its up to you, you can simply  export  the variables on the command-line if you wish. . To create the file containing the environment variables  vi aws-public.env  and the contents of this file should look similar to this - . Lets just step through and explain what each of these variables do - .  APOLLO_PROVIDER  - This notifies the bootstrap scripts that we want to deploy on to AWS public cloud. Valid providers are ‘vagrant’, ‘digitalocean’, ‘gce’, ‘aws’ (private VPC). .  TF_VAR_access_key  - This is the Access Key ID for AWS .  TF_VAR_secret_key  - This is the AWS secret key .  TF_VAR_slave_size  - (Optional) This is the size of image we want to use. I’m using c3.large here because I want a bit more grunt in my slave nodes. The default setting is m1.medium (if you leave this omitted) .  TF_VAR_key_file  - This is the AWS key we generate (part of prerequisites) that we want to upload to AWS .  TF_VAR_key_name  - This is the name we want to give that key. This must not clash with any keys you already have in your AWS account. .  TF_VAR_slaves  - (Optional) This is the number of slaves we want to provision. The default is 1. Turn this up depending on how large you want your cluster to be. .  ATLAS_INFRASTRUCTURE  - This is the name of the space in  Atlas  you want to use to monitor your infrastructure. This will give you a dashboard in Atlas allowing you to view the state of your infrastructure and services directly from Atlas. The dashboard will be available in Atlas at (for example) https://atlas.hashicorp.com/capgemini/environments/apollo-aws-public . To launch the cluster execute - . This will bring the environment variables into your shell and kick off the launch process which should start bootstrapping the cluster on AWS. . The bootstrapping process does the following - . Runs  Terraform  to provision the instances, security groups, SSH keys, etc… inside AWS cloud . After that, runs  Ansible  on those provisioned instances to bring up the cluster configuration . Finally, once Ansible has completed it should open the web interfaces in the browser to the following -              Mesos master UI         Marathon UI         Consul UI         Weave scope UI           . Mesos master UI . Marathon UI . Consul UI . Weave scope UI . You should now have a fully working  Apollo  cluster ready for you to start deploying your applications to. . In a more real world scenario you might want to look at using  the VPC set up we have  to provide a bit more security around the cluster. We are aiming to add more features around security and cloud provisioning/providers to the platform soon, so stay tuned to  our Github repo  for more info. . If you want to try this out for yourself, head on over to  https://github.com/Capgemini/Apollo  and get cloning! . Look out for more posts coming soon on how to deploy applications to the cluster and further insight into how we use tools such as  Weave  to improve the developer experience around deploying and managing Docker containers. ", "date": "2015-06-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How Apollo uses Weave and Weave Scope\n", "author": ["Graham Taylor"], "link": "https://capgemini.github.io/devops/how-apollo-uses-weave/", "abstract": " In my previous post we  launched an Apollo cluster on AWS in under 5 minutes . . Some of the magic around how we enable an easier developer experience around deploying and managing the communication between Docker containers is hidden in the provisioning of the cluster, so let’s take a deeper look at how that works. . To enable networking for Docker containers we use  Weave . The Weave guys have built a fantastic product that allows developers to hook into it with ease and simplicity. This really is the killer feature for me, it is just so easy to get up and running with Weave. . By default, Docker uses host-private networking. It creates a virtual bridge, called docker0 by default, and allocates a subnet from one of the private address blocks defined in RFC1918 for that bridge. For each container that Docker creates, it allocates a virtual ethernet device (called veth) which is attached to the bridge. The veth is mapped to appear as eth0 in the container, using Linux namespaces. The in-container eth0 interface is given an IP address from the bridge’s address range. . The result is that Docker containers can talk to other containers only if they are on the same machine (and thus the same virtual bridge). Containers on different machines can not reach each other - in fact they may end up with the exact same network ranges and IP addresses. . In order for Docker containers to communicate across nodes, they must be allocated ports on the machine’s own IP address, which are then forwarded or proxied to the containers. This obviously means that containers must either coordinate which ports they use very carefully or else be allocated ports dynamically. . We use Weave to get around these problems. This means - . All containers can communicate with other containers via the container IP address and port inside the Weave network . All nodes can communicate with containers via the container IP address and port inside the Weave network . Containers do not have to be statically linked to each other to be able to communicate . Forwarding / co-ordinating ports from the host machine is not required . We use  Ansible  to provision the configuration on to the  Apollo  cluster for Weave. Our  Ansible role for Weave is here . . We configure  /etc/network/interfaces.d/weave.cfg  with the following - .  WEAVE_BRIDGE  is a dynamic Ansible variable that changes for each host in the Weave network. For example on HOST1 the WEAVE_BRIDGE would be  10.2.0.1/16  but on HOST2 it would be  10.2.0.2/16. , and so on. . ### Step 2 - bring up the Weave interface . Next, we bring up the Weave network by executing . This gives us a network bridge on the host for the Weave network. . The next step is to upload an upstart service file to  /etc/init/weave.conf . This allows us to enable Weave as a service. . Inside that file we have - . This ensures that when the Docker service is started, Weave is also started, and similar behaviour occurs on stop. . When the Weave service starts it will launch Weave with the following command - . Where  ${WEAVE}  is the path to the Weave binary and  ${PEERS}  are the other peer nodes in the cluster to join on launch. .  ${PEERS} ` is a dynamic variable in Ansible which changes based on the size of the cluster. This is to accommodate potential differing sizes of clusters (e.g. if you spin up a cluster with 10 slaves instead of 1). . Finally we set the  DOCKER_OPTS  in  /etc/default/docker  to run with   --bridge=weave --fixed-cidr=weave_docker_subnet  . This means that any containers being started up via Docker will be started on the Weave bridge. This enables Docker to allocate an IP address for that container inside the Weave network. .  weave_docker_subnet  will be different for each host in the Weave network. For example on HOST1 the weave docker subnet would be  10.2.1.0/24  but on HOST2 it would be  10.2.2.0/24 , and so on. This ensures that containers started on HOST1 do not clash IP addresses with containers started on HOST2. This gives the ability for up to 256 containers per host. . As an example, if a container was started on HOST1 its IP address might be 10.2.1.1. If a container was started on HOST2 its IP address might be 10.2.2.1. . Both containers would be able to have services running on the same port (e.g. 80) without any IP address/port clashes because they are using a different network subnet. . The only thing left to do is to start up the Docker service on the host (which will in turn start the Weave service, bringing up the Weave containers). . As part of  Apollo , we’ve also bundled in  Weave Scope . This provides a nice visual interface showing the current hosts, containers and applications in the cluster. We think it could provide a useful tool for developers trying to debug issues with containers in the cluster, as well as providing a helpful bridge between Operations and Engineering teams. .   . ## Let’s take a look at it all in action . Let’s launch a simple Docker container with a NodeJS REST API. You can fire up this example yourself if you want - its in our repository  https://github.com/Capgemini/Apollo/tree/master/examples/nodejs-rest-api . . To launch it we are going to use  marathonctl  a CLI for Marathon. . We do that by executing - . This calls the Marathon REST API to initiate the deployment. When its up you should see something like this in the Marathon web interface, showing that we have 2 instances of our service container running - .   . If we look in the Consul web interface, we can see that we have 2 service containers registered in Consul, and that Mesos has distributed the containers across 2 nodes. .   . If we drill into the node view of Consul, we can see that one of the service containers has been given an IP address of  10.2.5.2:8080 . This is inside the Weave network. .   . Let’s go back to Marathon and scale the number of instances to 4 - .   . In Consul we can now see that we have 4 services, with 2 each across 2 nodes - .   . If we drill into one of the nodes here we see the magic of  Weave  - .   . We now have 2 nodejs-rest-api service containers both running on the node  54.171.137.140  both exposed on port 8080 with different IP addresses. . If we jump on to that machine, we can see that we are able to resolve the address of the containers via  Consul DNS  .   . This is pretty neat as it means we can just use DNS to resolve the address of the containers internally, so we don’t need any IP addresses. So if we called  nodejs-rest-api.service.consul:8080  We would get a response. By default this would Round-Robin DNS between the actual containers. This is pretty handy if you’re doing service discovery, because it means you can rely on SERVICE_NAME.service.consul being available to you (if you want to do inter-container communication). . Finally if we look at Weave Scope we can see that the container has appeared and you can inspect it through the web interface - .   . This only starts to scratch the surface of the things we can do with  Apollo . There are much more advanced deployment techniques you can take advantage of with service discovery, DNS and the Weave networking just working out of the box. . Check out the video below to see all of the above in action: .  NOTE : we have manually added an entry to our hosts file for  nodejs-rest-api.example.com . This is for the purposes of the demo because we have not set up proper DNS. In a production setup you would have DNS routing for your applications.  nodejs-rest-api.example.com  points to an HAproxy instance on the Mesos Slaves. The HAProxy config is built completely dynamically based on  Consul template . This is also a core part of  Apollo  . Although we’ve chosen to use Consul for DNS here, Weave can actually do DNS as well, through Weave DNS. See  http://blog.weave.works/2014/11/04/have-you-met-weavedns/  for more information on that. . With the  announcement of the Weave fast datapath  just in the last week, we want to look into providing that as part of Apollo to enable faster underlying networking for containers. We’re  tracking this issue  as part of the progress on that one. . We’re really excited to see how  Weave Scope  evolves as well, as we think it provides valuable insight for engineers regarding the current state of containers in the cluster. . Additionally with the  release of Docker 1.7 and the announcement of a plugin system , which gives initial support for Network and Volume plugins it will be interesting to see what unfolds in the community over the next few months around this ecosystem. This should allow much richer (and nicer) integration with Docker itself for things like Weave for which work is already well underway.  Check out the Weave blog  to find out more information on how they are progressing with the Weave network plugin for Docker. For more information on experimental networking and services in docker  see this page on the Docker Github repo . . If you want to give any of this a try or help us improve the project,  head on over to our Github repo  to get started! ", "date": "2015-06-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Continuously deploying Apollo with Wercker\n", "author": ["Alberto Garcia Lamela", "Cam Parry"], "link": "https://capgemini.github.io/open%20source/continuously-deploying-apollo/", "abstract": " When deploying  Apollo  it will create the cluster infrastructure in the cloud using an image built by  Atlas  and  Packer.  Additionally, it will provision and configure the new machines providing a full PAAS ready for deploying containers across the datacenter. . When dealing with several layers of abstraction and using many different technologies (Bash, Packer,  Terraform ,  Ansible , etc.) it is fairly easy to break things during development. . We need to make sure we are not committing syntax errors, that every commit is deployable and that the behaviour of the platform with all the bits communicating to each other works as expected. . Changes need to be continuously integrated and driven by tests. For this purpose we chose  Wercker  out of a list of available CI as a service tools like  Travis ,  circleci ,  Drone  and  Shippable  . “Wercker is an open automation platform for developing, building and delivering your applications” from the simplicity of a YAML file and it is highly customizable via  custom steps  . For achieving our continuous deployment approach we’ll use two different pipelines:  build  and  deploy  . “Builds are the result of a run-through of the steps in the wercker pipeline. If all steps in the pipeline are successful, a build has passed”. . Every time a Pull Request is created the build steps below will run: . Syntax checks against all our json packer templates using  packer validate . . Linting checks against the ansible code using our custom  step-ansible-lint  that relies on  https://github.com/willthames/ansible-lint . .  Very simplistic Bash unit testing  for ensuring our bash functions do what they are meant to do. . In the future we might be adding a new step here for “packer push” the images into  Atlas  when merging into master branch. . You can see the feedback for every build in wercker: .   . Wercker allows you to deploy your application into supported platforms like Heroku or Openshift or you can create you own Deployment Target. . Every successful build from any specific commit can be deployed either automatically or manually. .   . At the moment we are continuously deploying Apollo into Amazon and Digitalocean using a  step-apollo-deploy . . Environment variables can be set for every target you create in wercker: .   . We have created  step-apollo-deploy  step for: . Installing a given version of Terraform. . Deploying Apollo in parallel into multiple clouds. . Running different types of testing inside the vms. At the moment  Serverspecs  and  docker bench security.  . Destroying the whole platform. . Every change in step-apollo-deploy is also continuously integrated by wercker itself using  step-validate-wercker-step : . So in essence with both “build” and “deploy” in place we have the ability to automatically triggering a fully tested ephemeral Apollo deployment for any specific commit. . We use ansible roles for installing and running  dockerbench  and  serverspecs  covering components of Apollo: . Mesos . Marathon . Docker . Weave . Consul . Registrator . Dnsmasq . HAProxy . Zookeeper . Testing during a deploy can be optionally enabled using a toggle environment variable. . We have recently decided to try and integrate  dockerbench  into our test suite. . The Docker Bench for Security is a script that checks for all the automatable tests included in the  CIS Docker 1.6 Benchmark  . It gives INFO, PASS and WARNING messages for each of the tests that run. We run tests at deploy time, to check we haven’t slipped any unwanted security risks into our build. Currently we only test if WARNING’s are above a certain configurable threshold. . The dockerbench project is still fairly young, and is still to settle on a standard way all tests can be run across any environment, whilst having configurable rules. We think this is a great project, as it allows us to bring security into our Continuous Integration cycle, which can decrease attack vectors and catch risks earlier in development cycles. . Apollo default deployment provides 3 machines playing mesos master role and running zookeeper and marathon. As some configuration like the ips can be assigned on deployment time we need to populate some of the serverspecs dynamically by using an ansible  j2 template . . Below are some examples with more of the serverspecs we run for marathon: . Finally we get the feeback for the deployment in every cloud in our Slack channel using  wantedly/pretty-slack-notify : .   . You can check out the  full wercker.yml file here  and below is a video showing the whole cycle: . If for your use case you need to keep track of the state of Terraform you might find this  Atlas and Github integration  useful. . Check out our upcoming posts if you want to hear more about CI, CD and our Continuous Delivery approach for services running inside docker containers deployed by Apollo, running Serverspecs into ephemeral containers via “docker exec” across multiple environments. ", "date": "2015-07-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Agile Analogies for Software Development\n", "author": ["Sarah Saunders"], "link": "https://capgemini.github.io/agile/agile-analogies/", "abstract": " I’ve recently been working in a training role, with people who have never been exposed to the software engineering lifecycle before. I was drawn to considering analogies to try and explain to them what we were trying to do. It occurred to me that a bad analogy can be as destructive as a good analogy can be helpful, and with software engineering a lot of the analogies that immediately spring to mind are not necessarily conducive to understanding how a software development project should work. . Other engineering disciplines (mechanical engineering, building construction) often require a waterfall construction method - it’s rare to be able to deliver subcomponents before the whole build is complete. You can’t address the riskiest area of a tower block construction project first if the riskiest area happens to be the roof or the windows. Change is harder and more costly - if you want to move a door, or (heaven forbid) the whole building, you’re looking at huge deconstruction/replan/reconstruction costs. The design documents are not fluid. When would you ever start construction without a detailed, low level plan covering the position of every door and window? It’s just not done. . So, if not the other engineering disciplines, where do we look for a suitable analogy? How about the world of television and film production? Even the end product itself fits better to this analogy. Yes you have a finished product, but it’s ethereal. It can be expanded upon, there can be another “series”, it can be cloned or reworked for different scenarios. Let’s consider the methodology now. . How would the analogy of creating a TV series fit with an agile way of working? Well, you can focus on risk first. Get those scenes with the old bloke / pregnant woman under wraps! You can build a disposable prototype and trial it on a user group; then you can adapt your requirements based on their feedback. You can work iteratively and deliver production-ready output in each iteration; you can go live with a subset of the total (think, for example, of a pilot episode from a series which airs before the entire series has been made). As for documentation, a TV script is a fluid document. We don’t  write  that we are  writing  the script, we just  write  it. You can think of code, or the Software Architecture document, in this way. I think that the analogy of the product owner as the director can also assist in explaining to a non-techy client what will be expected of them in this role. It’s not project management, it’s not technical details (the director doesn’t need to know about lighting rigs). . In my opinion, this explanation would put naive or new-to-software clients in just the right frame of mind to begin an agile project. Do we have a timeline? Well, yes, we know when we’re wanting to go live but we don’t yet know for certain how many “scenes” (think how many user stories) we are going to be able to build by then. Do we have a requirements document? We have our overall outline; we will drill into the detail of each one during its iteration. That way, we can easily incorporate a little change as we understand our product better. Let’s go! . There are certainly a few places where the analogy breaks down. A television programme is seldom interactive or reactive, yet a computer system will be. And a computer system doesn’t often have the requirement for historical consistency - if you kill off a character in a TV series it’s tough to bring them back in later, whereas with programs the whole thing is disposable and can be replaced with version 2.0. But let’s not get carried away with the details, that’s not the point of this exercise. . In summary, if you have a new client who is struggling with the concepts of agile software development, give this analogy a spin and see if it helps with clarity and understanding. ", "date": "2014-11-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Pair Programming and Budō\n", "author": ["Rob Kerr"], "link": "https://capgemini.github.io/development/pair-programming-budo/", "abstract": " Confession time: I find pair programming hugely useful, productive, informative - and draining.  Like many developers of my acquaintance, I’m an introvert; which doesn’t mean I don’t like people, just that I am less than enthused about spending hours at a time with them. So pair programming could have been designed as a particularly insidious torture. Like a cliché fantasy maze, it hides its treasures while forcing you to engage with a situation you’d really prefer to avoid. . However, I’m not just a developer.  [I’m a real boy! – Ed.]  I’m also a practitioner of the traditional Japanese martial art called  jujitsu .  (For quibbles about the spelling of jujitsu I refer you to the response given in the case Arkell vs. Pressdram.)  And in the course of my 15 years of training and teaching jujitsu, I’ve learned a number of attitudes to self-development and the acquisition and improvement of skill that I’ve recently realised could be usefully applied to making pair programming less of an ordeal.  I hope you find them useful. . Lesson the first: everyone has something to teach . Lesson the second: If you compete instead of collaborating, the end result can suffer . Lesson the third: dwelling on mistakes is counter-productive . Lesson the fourth: repetition is the only path to instinctive ability . Lesson the fifth: if you can’t explain something in a few different ways, you don’t really understand it . Lesson the sixth: talking is no substitute for doing . Lesson the seventh: calm is not the same as slow . Let’s look at each of those in turn. Some are related to each other, sometimes in subtle ways. .   © xckd.com . A teacher is one who makes himself progressively unnecessary.   Thomas Carruthers  . This is more subtle a point than it might appear at first.  There is the obvious meaning, that there are tips and tricks about programming; about particular coding habits; and about the peculiarities of specific IDEs that I can learn from anyone, however experienced or not they might happen to be.  It is also clear, however, that everyone has an individual worldview that they bring to bear on problems they need to solve, whether that’s implementing an efficient caching strategy for a distributed db query or executing a solid flying armbar that doesn’t snap a collarbone on landing. People pay attention in different ways, focus on different parts of problems, address issues in different orders, and in general are different in countless ways. . Pairing with different people, therefore, can help (and has helped me) to think about problems from angles different from how I might usually approach them.  I can then add that perspective to my toolbag of development tricks to be used at any time.  But I would never have learned to do that without being exposed to how it worked for someone else.  I might read about the same technique in a blog, but trying it without seeing that it works for some people reduces the possibility that I will persevere with it if it doesn’t show immediate benefits for me. .   © xckd.com . Every man lives by exchanging.   Adam Smith  . This is a difference in mentality when training versus the mentality required for performance in competition.  In competition, all your focus, all your effort, is designed to create openings, exploit weaknesses, and weaken the position of your opponent, and leave them open to attack. . Crucially, however, that mentality is left behind in the ring/cage/octagon.  In training, which is where I would draw the parallel with pairing, you can have the same attitude with a different focus.  You may still be identifying holes in your training partner’s game, openings you might choose to exploit or not, and taking advantages of weaknesses, but you do so in order to help them improve and shore up those weaknesses, limit or eliminate those openings, and become a better fighter.  And they are doing the same to you. . If you are both pushing each other, the end result is that both of you become a better fighter, and your training bouts (which we can think of as the product in analogy to the product of pairing) will be of a higher quality, drawing more attention.  If, in training, your focus is solely on winning and not on self-improvement, you will quickly plateau.  In order to improve, you have to learn to subordinate your ego to the benefit of the larger goal, improving as a martial artist. . Accepting criticism, accepting loss, is part of that, and that’s the lesson I’ve found most applicable to pairing.  Joining any kind of collective is for many good reasons abhorrent to humans – that’s why the Borg are probably the most popular adversaries on Star Trek.  Individuality is prized, collectivism and subordinating one’s self to a higher cause is feared (or admired when done, because it is so rare).  Learn this skill, and you will be that rarest of developers: one that people want to pair with. .   © Bill Watterson . Have no fear of perfection - you’ll never reach it.   Salvador Dalí  . Now here I think I need to clarify what I mean.  The difference between dwelling on mistakes and focusing on improvement through removing them can be summed up as the difference between two attitudes.  One says, “Next time, I will….”  The other says, “If only I had….” . We learn when looking at accounts of samurai training, and from  the Budō manual the Hagakure , that the samurai mentality that was encouraged was one of equanimitable acceptance of death.  Mistakes in training, which in a real encounter would have led directly to death, thus become learning experiences because they cannot be anything else.  “You are dead.  You cannot change what you did.”  Mistakes thus become irrelevant to one’s training except to the extent that they can be avoided due to prior experience. . This is something that carries over not only to pair programming, but also to one’s own individual coding – but it is in pairing where its real benefit is experienced, because it is in pairing that the temptations to focus on one’s mistakes; to make excuses for ones past code; to reinterpret one’s thinking processes through rose-tinted glasses; and generally to present oneself in a more flattering light, are magnified.  But if we remember that mistakes cannot be changed, they can only be learned from, we can let go of the need to do any of those things, accept that they are in the past, and move on with what we can accomplish today. .   © Bill Watterson . We are what we repeatedly do. Excellence, then, is not an act, but a habit.   Aristotle  . We’ve all heard the maxim, “Practice makes perfect,” and some of us may even have read Malcolm Gladwell’s “Outliers”, in which the now-famous claim of 10,000 hours of practice before attaining world-class performance is made.  Gladwell has since retreated from that claim, and there are many modifications to the maxim in existence.  Two of them have featured centrally in my martial arts training, and they are not only applicable to martial arts training and pair programming, but to any kind of endeavour where repetition builds skill and judgement. . First: “ Practice makes permanent .”  The meaning is hopefully clear, and is elucidated by the second variation: “Perfect practice makes perfect.”  It should be obvious that practise of the wrong things, without correction (either self-correction or criticism and adaptation) will not lead to perfection.  We need feedback in order to adapt our practice, and the best feedback is that which is immediately connected to the activity in question.  Pairing is thus the best way for a developer to incorporate improvements to their own practice of development. . One method for doing this that I have found useful is the “Three I’s” method: Introduction; Isolation; Integration.  First the new concept or method is introduced; then it is practised in isolation, and increasingly complex examples are used; then finally it is integrated into the pre-existing set of techniques available.  This model of skill acquisition can be seen to follow the  4-stage model of competence . .   © Bill Watterson . The art of teaching is the art of assisting discovery.   Mark Van Doren  . If you read the competence article linked above, you’ll note that at the end it talks about a potential 5th stage of competence.  Now, there are a few competing possibilities for the 5th stage, and they fall broadly into two categories: either an increase, or a decrease in competence following the achievement of the 4th stage. . As I’m an optimistic kinda guy  [This from the guy who insists the glass is twice as large as it needs to be? – Ed.] , I prefer to focus on the positive 5th stage, which is where competence has reached a level where the skill can be taught, and the stages of acquisition shortened for new learners due to the degree of understanding possessed by the teacher. . It is clear from many disciplines that one way to gain increased understanding is to attempt to articulate one’s understanding in response to prompts, questions, and situations where one is collaborating with a junior (or even an equal, or a senior) colleague.  [Ref: point 1].  For many, it is this very articulation, the requirement to put one’s understanding into words so that they can be comprehended by others, that deepens one’s own understanding. . The more one tries to do this, the better one gets at it [ref: repetition].  And the more different people one teaches, the more different ways of learning and modes of skill acquisition one encounters.  One gains thereby a facility for understanding the mental models underlying the direction a colleague’s questions might be coming from, and can address oneself to the mental model in addition to answering the question.  I have seen this bear astonishingly rapid fruit when an experienced instructor can correct someone’s mental model of a technique with merely a single word or phrase, entirely transforming how the recipient thinks about what they are trying to accomplish. . If our aim as pair programmers is to improve both how we code and how we pair effectively, I would argue that we should be looking to effect the same kind of change. . Recognising that different people learn in different ways will become instinctive with repetition and exposure to that truth, and we can tailor how we engage with people in order to pair with them most effectively.  The more you pair, and the more different people with whom you pair, the more broad your toolkit for effective pairing will become as you explore different options for engaging with the problem space. .   © xckd.com . Think like a man of action, act like a man of thought.   Henri Bergson  . Or woman, of course.  But the substance of the quote is fundamental to the way in which effective developers approach problem solving, in my experience. Think, and talk through approaches, always with a view to how those thoughts will ultimately be implemented.  And when it comes to action, the carrying out of our chosen course needs to refer back to what we agreed.  Should a new approach become apparent, we do not plunge forward, but consider the implications of progressing down the newly revealed path.  This consideration may only take a few minutes, but it needs to happen. . Following from the point about repetition, above, if all we do is talk, all we’ll get good at is talking.  We need to do in order to get better at doing. And thus, in order to get better at pairing, we need to pair.  We need to pair effectively, and inspect and adapt how we pair so that next time it’s an even better experience. .   © Tatsuya Ishida . The cyclone derives its powers from a calm center. So does a person.   Norman Vincent Peale  . You may have heard of the state known as “flow” or being “in the zone” [http://en.wikipedia.org/wiki/Flow_(psychology)] as being a desirable one to achieve, but which can be easily broken by distractions in one’s immediate environment: interruptions from colleagues; loud music or tannoy announcements; alerts popping up on your screen; family calling at inopportune moments; lunchtimes; and many others.  Techniques for not breaking the flow state once it has been achieved are many, but most of them require the elimination of potential distractions.  I’d like to introduce a few concepts from Zen Buddhism, also widely known by practitioners of Japanese martial arts, which may help. .   Mushin no shin   (無心の心) translated usually as “no mind” or “the mind without mind” is a state of relaxed awareness and openness.  One anticipates nothing, is preoccupied by no thought or emotion, so one is ready to respond to anything. .   Zanshin   (残心) translated as “remaining mind” is a state of complete awareness without focus on any specific part of that awareness.  One is alert and ready to respond to any change in the environment. .   Fudōshin   (不動心) translated as “immovable mind” or “unmoving mind/heart” is a state of imperturbability, in which the advanced practitioner remains while responding to change in the environment, and which persists afterwards. . Taken together, these states can help one not to break flow even if it is interrupted.  Advanced practitioners of the Japanese martial arts try to carry the awareness states into their normal lives outside the dojo, and it is entirely possible to allow oneself to slip into and out of the state of mind that is most useful for achieving one’s goal without being mindful of the goal itself.  The immovable mind permits retention of focus on a task while responding to a stimulus outside the task.  Shifting focus thus is not necessarily a detriment, so long as the appropriate mindful state can be maintained. . All of the above states are useful for sole development activity, but there is a fourth state which is particularly useful for pairing.  It is   shoshin   (初心), or “beginner’s mind,” and refers to an attitude of eager openness to new experiences and a lack of preconceptions, even when working at an advanced level.  In my opinion, there are very few developers who are not eager to learn new things, new ways of working, new technologies: but there are few indeed who come to new concepts as a beginner would, emptying their mind of what has gone before and soaking up concepts and information as a beginner.  Only afterwards do they process and filter the information and relate them to their previous framework of knowledge. . It’s an unfortunate truth that many developers separate the act of development from how they live the rest of their lives.  We generally know when we are most effective, but often ignore other aspects of what makes any particular dev session productive or otherwise.  Plus,  confirmation bias  and  attribution error  can quickly lead us to adopting strategies for productivity which are actually harmful.  Once in a while, it can be useful to step back and look at our process, inspect it and see if there are any improvements that we can find to make.  And looking beyond the circle of our colleagues and peers for inputs into that can help enormously.  I hope the above has given some food for thought. . …and if it brings more eager students to the martial arts, even just one curious quick-brained dev, I’ll consider that a success. ", "date": "2014-11-21T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Solving problems being technology agnostic\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/architecture/solving-problems/", "abstract": " We are used to building systems and solving problems under high quality requirements as developers. . When you are creating a whole system with its own behaviour and idiosyncrasy your solution is probably going to have some specific characteristics due to your requirements, your particular priorities and the context that you build it to. . There are many factors that matter when creating new architecture. Maybe the scalability of your system is a key factor for you. Maybe the requirements of your system can change frequently, so flexibility would be crucial. Maybe you have legacy code dependencies. Maybe you need to give priority to readability over performance. Depending on the capacity that your system needs to support, you could make different decisions. Valid approaches for monolith systems could not apply for decentralised or distributed systems. . Talking now about a lower level of abstraction, you will try to make the most of the language, technology stack and programming paradigm (imperative, functional, OOP, etc) that you choose or that you have to use. So this will yield some peculiarities to your creation. . All this being said I find that we can identify some principles and techniques applicable for shaping almost any solution so sometimes we should think in a more “technology agnostic” way to make the right moves. Our system could be a “Drupal module”, an “Automated Deployment Pipeline” or an “Enterprise Web Application”. I think there are principles generic enough for applying to such different systems and that can help us to ensure quality and good results regardless of the nature of the problem when building a solution: .       Choose the most appropriate form of separation for your problem and divide the system into reusable pieces in order to reduce the coupling between parts with different purposes. A piece is something that is able to interact with other components using a given protocol or “idiom” but that can work by itself (e.g. you will usually want to decouple your business logic from the framework, the back-end from the front-end, the database technology from the application, you could want to decouple the physical deployment in the form of a n-tier architecture).  SoC.      . Choose the most appropriate form of separation for your problem and divide the system into reusable pieces in order to reduce the coupling between parts with different purposes. A piece is something that is able to interact with other components using a given protocol or “idiom” but that can work by itself (e.g. you will usually want to decouple your business logic from the framework, the back-end from the front-end, the database technology from the application, you could want to decouple the physical deployment in the form of a n-tier architecture).  SoC.  .       Fewer components are better. Humans are not good at understanding complexity.     . Fewer components are better. Humans are not good at understanding complexity. .       Create abstractions of complexity.  Abstraction Principle.      . Create abstractions of complexity.  Abstraction Principle.  .       Keep it simple.  KISS.      . Keep it simple.  KISS.  .       Isolate details.     . Isolate details. .       Define a clear boundary for responsibilities. One thing for doing just one thing.  Single responsibility principle.      . Define a clear boundary for responsibilities. One thing for doing just one thing.  Single responsibility principle.  .       More frequent small changes are better than big occasional changes.     . More frequent small changes are better than big occasional changes. .       Don´t duplicate things. Reuse everything that can be reused.  DRY.      . Don´t duplicate things. Reuse everything that can be reused.  DRY.  .       Use meaningful names but without involved and unnecessary commitments.     . Use meaningful names but without involved and unnecessary commitments. .       Choose conventions. Be consistent. Use the same approach for solving the same small problem several times.     . Choose conventions. Be consistent. Use the same approach for solving the same small problem several times. .       Don´t procrastinate. Don´t increase debt. Do it now.     . Don´t procrastinate. Don´t increase debt. Do it now. .       “Yes” is forever. If you’re not sure about something new, say no. You can change your mind later, otherwise you will probably be technically committed forever.     . “Yes” is forever. If you’re not sure about something new, say no. You can change your mind later, otherwise you will probably be technically committed forever. .        Automate everything that can be automated.      .  Automate everything that can be automated.  . So having these principles in mind can be a helpful reference for driving your decisions when solving problems being technology agnostic. Here is a list with some resources that drove me to embrace these principles: .  Codemanship  .  Libcontainer principles  .  GoF  .  Martin Fowler blog  .  Code complete  .  Continuous Delivery  .  Patterns of Enterprise Application Architecture  .  The Clean Coder  .  Clean Code  ", "date": "2014-12-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Little's Law and KanBan\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/agile/Littles-Law-and-KanBan/", "abstract": " This post delves into Little’s Law and how it relates to KanBan for software development. Understanding these concepts and their relationship will help agile practitioners improve the delivery pipeline, in particular by knowing what to measure and inspect. (Software engineers will also spot the parallels with scaling a solution: capacity, response latency and throughput.) . Little’s Law is often quoted in the context of KanBan.  Little’s Law  is a mathematical theory of probability in the context of queueing theory: . L = λW. . L = number in the system  λ = throughput  W = response time . In practical terms this means: . response time can be calculated from number in the system and throughput (W = L/λ) . number in the system can be calculated from throughput and response time (L = λW) . throughput can be calculated from number in the system and response time (λ = L/W) . The relationships described by Little’s Law only work in a steady state. Steady state is achieved when the arrival rate is the same as the departure rate on average over the long-term. . This means that throughput, arrival rate and departure rate  are the same metric . . Guides to KanBan discussing Little’s Law will introduce the concept of WIP (work in process or work in progress), or more to the point,  WIP limits . In order to maintain steady state, a WIP limit is defined before the bottleneck. This ensures the “system” (a particular step in the process) is protected from overload by limiting the arrival rate. . If we take a commonly-used example - a supermarket store - the throughput is the same as number of customers entering the store and is the same as the number of customers leaving the store. Minor disruptions to this flow are okay as Little’s Law holds over the long-term. . But what happens if too many customers enter the store? First, the number of customers in the store will increase. This will lead to a greater demand on the checkouts. More checkouts may need to be opened. Once the capacity of the checkouts is reached, the queues at the checkout will get longer. Eventually these queues will get long enough to fill the capacity of the store and block customers trying to enter the store. This may even block the exit to the store and so customers can no longer leave the store: chaos. Log jam. A broken system no longer in steady-state. . In other words, the arrival rate exceeded the departure rate and the throughput couldn’t be maintained: Little’s Law no longer held. . A WIP limit would have prevented the problem. A example WIP limit would be periodically closing the doors to restrict the number of customers entering the store. .   .   . Of course closing the doors only prevents the problem of blocking the entrance. Ultimately this would have led to another problem: queues outside the store or, worse still, turning customers away. . So what else could have been done? In the example the service rate was increased - more checkouts were opened - in an attempt to match the increased arrival rate. To enhance this option further even more checkouts could be installed. Perhaps the capacity of the store could be increased: building work to increase the size of the shop floor. . Queues are a form of waste within Lean. This is because queues are considered a form of inventory - potential value sitting on the shelf. In the example, customers waiting at the checkout are finished shopping (the trolley contains the potential value), the value will only be realised when they have paid for the goods. And of course, nobody likes queueing! . So a better approach might be to improve the efficiency of the checkouts - better training of staff or more efficient checkout systems. . The mere presence of the WIP limit tells us there is a weakness in the system. Regularly hitting the WIP limit (or even breaching it) should tell us we have a problem that needs attention. Lean (or rather the  Toyota Production System ) might refer to this as an “Andon” - a warning lantern of some sort to indicate there is a problem. . Conversely, never getting anywhere near the WIP limit might indicate our system is under-utilised, essentially another form of waste. To much resource has been allocated to the system to match the demand. Perhaps improvement work could be done to release this spare capacity and allocate it elsewhere. .   . These constrained systems are often referred to as bottlenecks. However there is actually only  one  bottleneck step in a steady process. The bottleneck step is the step that determines the overall throughput, that is the throughput of the overall process  is equal to  the throughput of the bottleneck step - the step with the lowest throughput. This is because the upstream steps are feeding the bottleneck at a rate equal to or higher than it can handle; downstream steps are being fed at the rate of the bottleneck as so cannot achieve a higher throughput. . Addressing any other step in the overall process will have  absolutely no effect whatsoever  on the overall throughput. There is  no point  increasing the efficiency of any step other than the bottleneck step in trying to increase throughput. .   . One of the challenges software development faces is that the process is rarely steady state: it is notoriously difficult to estimate accurately and it’s unpredictable when problems will occur. The arrival rate into the process is often very variable - feast or famine. Add to that pressures to deliver rapidly and competing priorities it’s easy to see why software delivery is an unpredictable art rather than a nice predictable science. . So what can KanBan and Little’s Law offer to software development? . Firstly, we need to know what our process actually  is . How often does the delivery pipeline get mapped out end to end? This is worth doing, even the steps outside our influence. Even if all this achieves is to visualise the process, this is a valuable in itself. . Then we can start to analyse the steps. Collecting metrics will help us do this. Referring to Little’s Law, for each step we need to know any two of: . L = number in the system . λ = throughput . W = response time . …then the third can be calculated  assuming a steady state . (On that basis it may be worth tracking overall throughput to check how steady the overall process is.) . High numbers in any of these for a particular step (compared to other steps in the process) will indicate a problem / potential bottleneck. . Assigning WIP limits to steps will also tell us how often desirable throughput rate is breached. If a low-WIP is set then this can also tell us if the step is under-utilised. . For over-utilised steps either make the step more efficient, assign more resource or increase the storage capacity  within  the step. . Since L is proportional to W, increasing the number in the system is a trade-off that will increase the response time. i.e. increase lead-time. . Given that the demand is fluctuating, estimates are inaccurate at best and problems can occur any time, it is difficult to maintain a steady state. There are two ways to achieve this: . Reduce the demand to a throughput level that will never breach  any  step in the process. . Increase the throughput level of  every  step in the the system to match the maximum demand. . The first option gives the ultimate control. The process is streamlined, there is little waste in the system. The cost is that throughput is decreased. . The second option gives the best performance. Any level of demand can be met. A good deal of redundancy has been introduced to the system to cope with a sharp increase in demand. The cost is that it may require a great deal of investment to achieve such a system and it may take a significant period of time to get there. . Of course reality is somewhere between the two. Rarely can we afford ultimate performance, but rarely can we constrain the system to a low-level of serviceable demand. The economic version of this is turning customers away - rarely good business sense. . A blockage of some sort is usually a pinch point in software development (e.g. a defect or a failed build). Ideally people will “swarm” to the problem - but this depends on availability of people to be able to swarm. If the system is overloaded people are not available - thus it’s better to build in some spare capacity to free people up should such a situation arise. . Out of software development, motorways are an example. When the system is running at full capacity, it only takes a minor disruption in flow to create a traffic jam. The incident may have been as simple as a driver changing lanes causing the car behind to apply the brakes. This causes other drivers to brake causing a disruption - chaos ensues. (See also  traffic compression wave .) . Traffic management has improved with the relatively recent use of variable speed limits. This works in two ways: a slower speed limit is introduced (effectively a WIP limit) but also the hard shoulder is opened up as another traffic lane (capacity is increased). I’m not sure how this is achieved technically - I presume there’s some traffic control centre monitoring traffic flow and throughput - but it will be down to some measure of this metric, plus number in the system and response time. Perhaps if the number of cars passing junction 4 is greater than the number of cars passing junction 5 this indicates a problem between junction 4 and 5? Response: activate variable speed limit traffic control. . Closer to software development, elastic computing works in a similar way. Demand increases, more CPU is added. Demand decreases, CPU is taken away. . Could this be achievable in software development? Detect (or predict) an increase in demand, so increase the available resource? Maybe, it will depend on the lead-time to react. If by resources we mean people then it often takes a while to get more people in, familiarise them and re-establish the team (forming, storming, norming, performing; see also  Brooke’s Law ). If by resources we mean computing power (e.g. for an automated pipeline) then elastic computing is desirable - so long as it has been designed for. . Of course we cannot make any informed decisions if we don’t have the information. To get the information we need to measure things. . Recommendation is to: start at a high level and map the process end to end; measure enough to find out where the hot-spots are, then delve deeper into those steps. . Automated measurements are better than manual (rapid, repeatable, reliable). Automation is essential if used as an indication for immediate attention. . A high-threshold mark will tell us when the demand on the system is too high. A low-threshold mark will tell us when the demand on the system is too low. Plotting these measures on a time-line would be very useful for analysis - perhaps as input to a retrospective. . Simply providing the metrics may well be enough to grab attention. . My motivation for creating this post was not to explain Little’s Law, rather to show how it relates to KanBan. I had struggled for some time relating the concepts as Little’s Law is all about steady state and software development is very rarely steady state. I had understood this related to WIP and WIP limits, but had not appreciated how. . In this post I believe I’ve also introduced a new concept, that of the low-utilisation threshold. Tracking such indicators over time could be very useful for elimination of waste or re-deployment of resources. ", "date": "2014-12-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal 8 in 2 steps\n", "author": ["Gabriele Manna"], "link": "https://capgemini.github.io/drupal/drupal-8-in-2-steps/", "abstract": " Drupal 8 is the latest version of Drupal, a modern, PHP 5.4-boasting, REST-capable, object-oriented powerhouse. The concepts are still the same as the previous versions but the approach is now different. Drupal 8 comes with a modern Object Oriented Programming (OOP) approach to most parts of the system thanks to the use of the  Symfony2  framework. . I took part in the Drupalcon in Amsterdam and I enjoyed a number of really interesting talks about Drupal 8, among those ‘Drupal 8: The Crash Course’ realized and presented by  Larry Garfield . In this post the idea is to recap few key points of his talk as I think they are important to fully understand the basics of this new Drupal version. In case you are interested you can also  watch the full talk . . In Drupal 8 to define a module we need only a YAML (.info.yml) file: .  /modules/d8_example_module/d8_example_module.info.yml  . In Drupal 8 the .module file is not required anymore, so with only the .info.yml file the module is ready to be enabled. . Start creating a controller extending the ControllerBase class and return the output of the page: .  /modules/d8_example_module/src/Controller/D8ExampleModuleController.php  . Once this is done, within the .routing.yml file we can define the path, the controller, the title and the permissions: .  /modules/d8_example_module/d8_example_module.routing.yml  . We still have the hook_theme() function to define our theme: .  /modules/d8_example_module/d8_example_module.module  . For the template page Drupal 8 uses  Twig , a  third-party template language  used by many PHP projects. For more info about Twig have a look at  Twig in Drupal 8 . One of the cool parts of Twig is that we can do string translation directly in the template file: .  /modules/d8_example_module/template/d8-theme-page.html.twig  . And then we assign the theme to the page: .  /modules/d8_example_module/src/Controller/D8ExampleModuleController.php  . Drupal 8 has a whole new configuration system that uses human-readable  YAML  (.yml) text files to store configuration items. For more info have a look at  Managing configuration in Drupal 8 . . We define variables in config/install/*.settings.yml: .  /modules/d8_example_module/config/install/d8_example_module.settings.yml  . The variables will be stored in the database during the installation of the module. We define the schema for the variables in config/schema/*.settings.yml: .  /modules/d8_example_module/config/schema/d8_example_module.settings.yml  . To create a form we extend a ConfigFormBase class: .  /modules/d8_example_module/src/Form/TestForm.php  . Then within the .routing.yml file we can define the path, the form, the title and the permissions: .  /modules/d8_example_module/d8_example_module.routing.yml  . We use another YAML file (.permissions.yml) to define permissions: .  /modules/d8_example_module/d8_example_module.permissions.yml  . We also use another YAML file (.links.menu.yml) to define menu links: .  /modules/d8_example_module/d8_example_module.links.menu.yml  . To create a block we extend a ConfigFormBase class: .  /modules/d8_example_module/src/Plugin/Block/TestBlock.php  . In this way the block is ready to be configured in the CMS (/admin/structure/block). Here is an example of a more complex block: . The structure of a module should look like the example module  d8_example_module : .  Drupal 8 in 2 steps: Extend a base Class or implement an Interface and tell Drupal about it.  .  Download the example module  . Updates: .       19th January 2015               The code in this post has been updated to reflect changes in Drupal 8 Beta 4. Thanks to  Geoff Lawrence  for the updates to the example repo.           . The code in this post has been updated to reflect changes in Drupal 8 Beta 4. Thanks to  Geoff Lawrence  for the updates to the example repo. ", "date": "2015-01-07T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal 8 PSR-4 Form compatibility in Drupal 7\n", "author": ["Oliver Polden"], "link": "https://capgemini.github.io/drupal/drupal-7-psr-forms/", "abstract": " Up until Drupal 8 there has been little to encourage well organised code. It now has  PSR-4  autoloading so your classes are automatically included. Even though Drupal 8 is just round the corner, a lot of us will still be using Drupal 7 for quite a while, however that doesn’t mean we can’t benefit from this structure in Drupal 7. . This post covers two parts: . Autoloading class files. . Avoiding extra plumbing to hook into your class methods. . You’re probably familiar with  drupal_get_form(‘my_example_form’)  which then looks for a function  my_example_form() . The issue is that your form definition will no longer be in such a function but within a method in a class. To cover both these parts we will be using two modules: .  XAutoLoad  - Which will autoload our class. .  Cool  - Which allows us to abstract the usual functions into class methods. . Drupal 8 was originally using  PSR-0  which has been deprecated in favour of  PSR-4 . As a consequence the  Cool  module uses  PSR-0  in its examples although it does support  PSR-4 . We will create an example module called  psr4_form . . The information on  autoloading and folder structure for PSR-4 in Drupal 8  states that we should place our form class in  psr4_form/src/Form/FormExample.php  however the cool module instead loads from a  FormControllers  folder:  psr4_form/src/FormControllers/FormExample.php . . We can get round this by providing our own  hook_forms()  as laid out in the  Cool  module: . If you are ok placing your class in the  FormControllers  folder then you can omit the above function to keep your  .module  file simple or you could put the hook in another module. Potentially the  Cool  module could be updated to reflect this. . This class requires a namespace of the form  Drupal\\&lt;module_name&gt;\\Form . It also extends the BaseForm class provided by the  Cool  module so we don’t need to explicitly create our form functions: . Within our FormExample class we need a method  getId()  to expose the  form_id  to Drupal: . And of course we need the form builder: . All that is left is to define your validate and submit methods following  the Drupal 8 form API . . At the time of writing, the  Cool  module isn’t up to date with  Drupal 8 Form API conventions . I started this blog post with the intention of a direct copy and paste of the src folder. Unfortunately the methods don’t quite follow the exact same conventions and they also need to be static: . This example module can be found at  https://github.com/oliverpolden/psr4_form . . Drupal 8 is just round the corner but a lot of us will still be using Drupal 7 for the foreseeable future. Taking this approach allows us to learn and make use of Drupal 8 conventions as well as making it easier to migrate from Drupal 7. It would be nice to see the  Cool  module be brought up to date with the current API, perhaps something I will be helping with in the not so distant future. .  XAutoLoad  .  Cool  .  PSR-4 namespaces and autoloading in Drupal 8  .  Drupal 8 Form API conventions  ", "date": "2015-01-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "An Introduction to VirtualBox\n", "author": ["Geoff Lawrence"], "link": "https://capgemini.github.io/infrastructure/an-introduction-to-virtualbox/", "abstract": " What is my aim with this post? It is to give you a good introduction to  Oracle’s VirtualBox  so you know all about it and what it can do. This is not a tutorial on how to install it and do things with it, more of an introductory overview with enough detail to whet your appetite. . So, what is VirtualBox? Well, it is an open source virtualization solution suitable for most laptop and desktop computers. Some people may ask what virtualization is. It is the ability to run one or most virtual computers on your physical computer. They are “virtual” because they do not exist physically but rather software is used to emulate the hardware. So let’s say you turn your laptop on and it boots into Windows 8.1, which is fairly normal. If you now download an ISO image (the file equivalent of a CD/DVD) of Linux and start VirtualBox you can create a new Virtual Machine, or VM and start it up, booting off the ISO. This VM can then launch off the ISO and install Linux, however the screen is just a window or application running on Windows 8.1 and the hard drive of the VM is just a single but big file on your laptop. Now you have Windows 8.1 running, as normal which is known as your host and it is running a guest, Linux. It’s quite simple on the surface, but brilliantly powerful and it works and works well. . I guess the next question is why? Why would you use VirtualBox? That will get clearer after we have covered the “When” but for now, let’s consider Why VirtualBox. There are several virtualisation products available depending on what operating system you use. If you have a Mac, you might prefer  Parallels , or if you need lots of VMs in a production environment with good management you might choose  VMWare , or you might be a Microsoft user and prefer Hyper-V (found in Windows Server 2008 onwards or Windows 8 - but not all editions). There are several options in the Linux world and that is all before you touch on cloud based solutions or container solutions like  Docker . . Personally I have a couple of reasons for using VirtualBox. Your personal preferences might be different and that’s fine. I like the fact that VirtualBox works well, is robust, fast,  open source  and cross platform. Performance is the reason I first switched to VirtualBox and since then it has been my solution of choice, doing everything I need it to, including running preview versions of Windows 8 and Windows 10. The other reason is that it is open source. Does this mean that I downloaded the source, customised it and compiled it? No. However it is great to know I can. I also like the fact it is free, which is very nice, so, thank you Oracle for keeping it this way. . So, when might you use VirtualBox? I believe there are several good use cases which fall into a couple of groups. . Firstly you might choose to run a number of VMs instead of physical machines, this might be for some kind of virtual desktop solution or a virtual server solution, either way, you are running a small farm of virtual machines. Large organisations often choose VMWare for this but VirtualBox is very capable and excellent value for money. . Secondly you might run VirtualBox on your laptop or desktop, and in this scenario it is most likely for reasons of isolation. For example you might be doing network related things like installing Active Directory Domain Services, DHCP servers and so forth, you really don’t want this on your main network! Another example is keeping your main machine clean and doing software installations in a VM. Sometimes this is useful for developing in a different operating system or just trying something out. . A virtual machine is a great place to test software, whether that is trying out different install options to see what they do or just running software in a VM to keep your main machine clean, either way it is a good choice. You can do this assessment without interference from the software on your main machine. You can also see how software behaves if you reduce the amount of physical memory or slow its processor speed. . Sometimes you just have to test a website with an old browser. Thankfully this is becoming less of an issue than it used to be but even so it is still handy to try your site from Linux as well as Windows for example. . This is where you want to deploy your software to a consistent, known platform and a VM is ideal for this as you can either start from a clean OS, a known appliance or a good snapshot. VirtualBox supports  Python  and has a comprehensive command line toolset, so you can automate/script some tasks. Alternatively you can use something like  Vagrant  to help automate the process and get a consistent platform to deploy your solution to. So whether this is part of your nightly build/test cycle or just regular development lifecycle, VirtualBox will support the way you work. . Once you have your DevOps automated deployments running then automated testing is the next logical step and is arguably part of DevOps. . Originally VirtualBox was developed by a company called  innotek  and they produced a no cost edition and then made it  open source . They were bought by  Sun Microsystems  in 2008, who in turn were bought by  Oracle  in 2010. So today it is officially known as  Oracle VM VirtualBox . Whilst Oracle have continued with the original  www.virtualbox.org  website and kept it mostly open source, they have created a number of pre-built VMs that you can download and they are configured with Oracle technology, very handy. . So, where can we install VirtualBox? Well, as long as you have an Intel  x86, Intel64 or AMD64 based processor and you are running Windows, Linux or Mac OS - or even  Solaris  - then you can install VirtualBox and host some virtual machines. Clearly your computer will need enough performance to do this but most modern computers have enough processor power to do this, it is more often a case of having enough physical memory. . These are a nice easy way to get files in and out of your VM and they can be automatically mounted. They are though I little trickier with Linux compared to Windows. . This is a standard virtual machine feature, where you take a snapshot of a VM state, you can then make some changes, as many as you like over several days and then revert back to the snapshot and everything you did since taking the snapshot is undone. It is more accurate to say that the VM state is reverted and it is as if you have never changed it, very handy. . Sometimes you need to take a working VM from one host machine to another. The recommended way to do this is to create an appliance, copy the generated file to the new machine and import it as an appliance. It copies all the VM stuff into a single, portable file. Just copying the virtual disk file misses the virtual machine configuration. . This could be a whole article in itself! However I would recommend configuring your VM with two network adapters, the first using NAT for access to the outside world and the second as Host-Only so you can access its services from your host. . If you clone a VM you can have a second copy of the same guest VM running, which is an easy and quick way to build a cluster. However care does have to be taken about the stage of the cluster build process that you do this! . Sadly VirtualBox does not yet support USB 3, however any USB 2 device can be used within the VM. Note this is one area where you might need to license VirtualBox but read https://www.virtualbox.org/wiki/Licensing_FAQ and you will see exactly when. In summary there are very few situations where a license is needed. . This is brilliant if you only have 1 screen but need to test a multi-monitor system. Yes, you do run out of screen space but it does make software testing easy. If you have two physical screens you can get VirtualBox to display 3 or more virtual monitors for testing. . Being an open source solution often means people starting building on top of things like VirtualBox and this indeed has happened. . Well, this is really a whole other blog post but if you need rapid VM creation, management and deployment for development work or DevOps style work then Vagrant can save a lot of time. The nice thing is you don’t have to stop using VirtualBox, you can just add Vagrant based VMs into your mix. . I have become a fan of VirtualBox since first installing it and have had very few problems with it. Some features take a bit more learning about than others but fundamentally it does the job and works very well for me. If you want more posts on VirtualBox then please add a comment and if I get enough interest maybe I will write another post….. . Official Web Site -  https://www.virtualbox.org/  . Oracle Product Page -  http://www.oracle.com/us/technologies/virtualization/virtualbox/overview/index.html  . Oracle Pre-Built VMs -  http://www.oracle.com/technetwork/community/developer-vm/index.html  . Handy Blog -  https://blogs.oracle.com/fatbloke/  ", "date": "2015-02-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal, Symfony and friends\n", "author": ["Alex Moreno Lopez"], "link": "https://capgemini.github.io/drupal/drupal-composer-symfony/", "abstract": " There are thousands of situations in which you do not want to  reinvent the wheel . It is a well known principle in  Software Engineering , but not always well applied/known into the Drupal world. . Let’s say for example, that you have a url that you want to convert from relative to absolute. It is a typical scenario when you are working with Web (but not just Web) crawlers. Well, you could start building your own library to achieve the functionality you are looking for, packaging all in a  Drupal module format . It is an interesting challenge indeed but, unless for training or learning purposes, why wasting your time when someone else has already done it instead of just focussing on the real problem? Especially if your main app purpose is not that secondary problem (the url converter). . What’s more, if you  reuse libraries and open source  code, you’ll probably find yourself in the situation in which you could need an small improvement in that nice library you are using.  Contributing your changes  back  you are closing the circle of the  open source , the reason why the open source is here to stay and conquer the world (diabolical laugh here). . That’s another one of the main reasons why lot’s of projects are moving to the Composer/Symfony binomium, stop working as isolated projects and start working as global projects that can share code and knowledge between many other projects. It’s a pattern followed by Drupal, to name but one, and also by projects like like  phpBB ,  ezPublish ,  Laravel ,  Magento , Piwik , … . Coming back to our crawler and the de-relativizer library that we are going to need, at this point we get to know  Composer . Composer is a great tool for using third party libraries and, of course, for contributing back those of your own. In our web crawler example,  net_url2  does a the job just beautifully. . Nice, but at this point you must be wondering… What does this have to do with  Drupal , if any at all? Well, in fact, as everyone knows,  Drupal 8  is being (re)built following this same principle ( DRY or don’t repeat yourself ) with an strong presence of the great  Symfony 2 components  in the core. Advantages? Lots of them, as we were pointing out, but that’s the purpose of another discussion . The point here is that you don’t need to wait for  Drupal 8 , and what’s more, you can start applying some of this principles in your  Drupal 7 libraries , making your future transition to Drupal 8 even easier. . So, using a  php library or a Symfony component in Drupal 7  is quite simple. Just: . Install   composer manager   . Create a composer.json file in your custom module folder . Place the content (which by the way, you’ll find quite familiar if you’ve already worked with Symfony / composer yaml’s):         \"require\": {   \"pear/net_url2\": \"2.0.x-dev\"  }             . enable the custom module . And that’s it basically. At this point we simply need to tell drupal to generate the main composer.json. That’s basically a composer file generated from the composer.json found in each one of the modules that include a composer themselves. . Lets generate that file: . At this point we have the main composer file, normally in a vendor folder  (if will depend on the composer manager settings). . Now, let’s make some composer magic : . At this point, inside the vendors folder we should now have a classmap, containing amongst others our newly included library. . Hopefully all has gone well, and just like magic, the class net_url2 is there to be used in our modules. Something like : . Just remember to add the library to your class. Something like: . In the next post we’ll be doing some more exciting stuff. We will create some code that will live in a php library, completely decoupled but at the same time  fully integrated with Drupal . All using  Composer magic  to allow the integration. . Why? Again, many reasons like: . Being ready for  Drupal 8  (just lift libraries from D7 or D6 to D8), . Decoupling things so we code things that are ready to use not just in Drupal, and .  Opening the door to other worlds to colaborate with our Drupal world , … . Why not use  Dependency Injection  in Drupal ( as it already happens in D8 )? What about using the  Symfony Service container ? Or something more light like  Pimple ? . Choose between many other reasons… . See you in my next article about  Drupal, Composer and friends , on the meantime, be good :-). . Updated: Clarified that we are talking about PHP Libraries and / or Symfony components instead of bundles. Thanks to @drrotmos and @Ross for your comments. ", "date": "2015-02-27T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Why Microservices Are Right For Us\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/architecture/why-microservices-are-right-for-us-pt1/", "abstract": " I  posted previously  about the fact we’re “doing” Microservices.  At that time I was hedging pretty extensively (as various commenters pointed out). This was predominantly because we we’re pathfinding (or at least broadening the paths for the rest of us that had already been exposed and marked by others just ahead of us). . Well, time has moved on; and while we’re still not fully paid-up cult members, unwilling to drop our skeptical, one-step-at-a-time, always-check-this-is-right-for-us-in-our-specific-situation approach, we are increasingly happy with the path we’ve taken.  Why?  Let me lay it out for you. . First up, it’s been overwhelmingly noticeable that by adopting Microservices, we’ve allowed ourselves to work on things that are the right size for a single developer to grapple with.  By this I mean you can work on them quickly and in isolation, and you can consume them easily too. . What’s more, you can reason about their internals really simply; their cognitive load feels “just right”.  Their granularity feels good too - you know when they “do one thing, and do it well” because it’s patently obvious; or more accurately, you know when they’re not, and when its time to split them down. ( Others have already written  far more eloquently about this than I ever could, but I’d like to add ourselves as a data point which bears this out.) . What is more, not only are they human-sized for us as developers, they are also human-sized for that  “auld enemy”  of ours - testers.  By this I mean that they (Microservices, not testers) can initially be tested quickly, and in isolation - the test work gets a lot closer to our unit tests and, because we’re all REST-based in our APIs, these tests are nice and simple to automate with a myriad of free and well-known and well-CI-CD-chain-integrated tools. . But perhaps even better than both these is the fact that Microservices are also consumer-sized.  We write all ours so that with only a Java and Maven dependency installed you can check them out and have them running locally with two commands: . We set up our code so that, when the Microservice starts up, it’s automatically in stub-mode, by which I mean it’s downstream Microservice and other dependencies are stubbed out. The range of requests you can send in, and the various valid and failure states you’ll mimic as a result are specified in standard README.md files at the top of each Microservice git repo. . This, plus the bundled and served  Swagger  docs,  Hystrix Dashboard  and  Spring Boot  /  Codahale Metrics  monitoring JSON feeds, means everyone is a lot more self-supporting and doesn’t need to depend upon shared server environments with the associated time-slicing and wait for the next release. . Picking up right where our last point left off is our next benefit; adopting Microservices has allowed us to throw out a lot of the heavyweight-and-rarely-used shared development infrastructure, allowing us to get a lot more as individual developers out of the grunt of our development and test machines.  Running  Tomcat  (we’re not on  Jetty  yet, but we will be eventually - its inevitable) and possibly an in-memory-db (or even a local  Redis  or  MongoDB ) is a LOT lighter on resources and quicker to turnaround after a change than a traditional Java EE app server.  Despite this, we’ve lost none of the monitoring or configurability we’ve come to expect from the more “full-featured” EE cousins; none that we’re missing anyway (see above).  And if we do need to run up more than one Microservice locally? Nae bother. . And before we move on, it’s worth us pointing out that this is all without the much-discussed  Docker  currently being in the mix. (We’re looking at moving to using this in Dev but its not imminent, though our minimal platform-dependencies should make it very simple once we find the time to do it properly.) . Still on the sizing, here’s our penultimate benefit of this post: Teams of 3-5 folks typically work on a Microservice together.  This is a good fit for us - we typically  Pair Program , or review all of each others code, which keeps it  clean , focussed and legible.  We’ve also found that a right-sized Microservice is a great size for a productive team-discussion, with collectively-owned outcomes. When there is disagreement - and there is occasionally - then quick spikes of a few days max produce great results. . One thing we’ve not tried, but which appeals to us, would be to take this approach one step further and let developers outside of the service-team make changes to a Microservice’s code via an OSS-like pull request model.  Of course this is predicated on each service having its own git repository - but we have that - and a PR / code review model approach to working - but we have that too. . Last benefit time. For this one, let’s push beyond the “size” theme. I know I’m walking into a minefield here (hence the inverted commas) but I mean Microservices are “agile” in the true sense in that they: . keep the humans at the centre (see everything in this post up to now) . help keep the focus on running, tested (TDD’d?) code . absorb change well (multiple parallel versions and refactoring!) . The first bullet we’ve already done to death. The second we’ve mentioned and I won’t go into any more.  The third however, bears a little exposition. . From the start we knew that we’d be in the situation at some point where we’d have to run multiple versions of a Microservice.  Fine, we versioned the URLs (major number only if you’re wondering).  We soon afterwards extended this so that multiple instances of a given service (even the same version) could be run alongside each other.  To do this we had to ensure they didn’t use each others port(s), config, output directories, and we had to add an instanceId to all the metrics we shipped else they looked like one big instance in things like  Graphite  /  Grafana .  With this in place, and a  backwards-compatibility policy  set out we were good to grow.  But that’s growth in the large, external to each Microservice. What about change internally?  It turns out, that’s pretty great too. . Because they are by their very nature self contained and “simple”, and exist behind cleanly defined interfaces we’ve found our Microservices a  lot easier  to refactor.  Changes are smaller as the codebase is smaller.  The changes are easier to grok for a similar reason.  And because of this, most forms of debt accrete a lot more slowly.  Beyond this, because you’re in a completely separate codebase, looked after by a smaller team, experiments are easier to undertake, and are hidden from everyone else.  That helps free folks from the tyranny of “that’s not how we do it”, and lets them make decisions based on their requirements, and their requirements alone.  Now, in my opinion, that’s agile. . That’s it for this post - we have other potential benefits that we’re tracking as I write, and rest-assured, I’ll post them as and when we’re sure they really are “beneficial”. . In the meantime, if you have any comments / questions, please share them in the comments section below, or tweet we on  @al94781 .  I’d love to hear from you. . Fancy working as part of the team I’m talking about, or on any other number of exciting Digital projects?  Let us know because,  we’re  always  hiring . ", "date": "2015-03-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Risk Burndown\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/agile/risk-burndown/", "abstract": " Few Agile approaches actively manage risk. There are some tools to help with risk, such as sprints, Definition of Done, etc. but there are few tools that are  explicit  about risk. . Risk burndown is an approach I have used to visualise risk. . A key part of Rational Unified Process (and other similar unified processes) is the concept of risk-value lifecycle. It’s not a tool as such, more a thought process alluding to how project engagements should discover and analyse risk. . In RUP, this is an overlay to the phases, especially Inception and Elaboration. . Inception deals with discovery of requirements, or exploring the problem space. The end of the Inception phase is when it is agreed that risks have been discovered and evaluated. . Elaboration deals with the mitigation of risks, or exploring the solution space. The end of the Elaboration phase is when the risks have been mitigated. This is usually when a candidate architecture has been agreed and proven through working software. . To relate this to the risk-value lifecycle, the end of Inception is when all the risks have been discovered and the end of Elaboration is when all risks have been mitigated. . (Of course this is an  ideal  view. Further risks may be identified after the end of Inception and risks may not be completely mitigated at the end of Elaboration. These are guidelines and are not to be taken too rigidly - it just has to be a consensus view that the project can proceed  knowing  the level of risk.) . The phase after Elaboration is Construction. By actively discovering risks and mitigating them the project team have built a good foundation for the remainder of the project. This phase deals with scaling and the team can now concentrate much more on delivering value. . Donald Rumsfeld gave a famous  known knowns speech  a few years ago. This was largely derided as incomprehensible rhetoric, but he actually made a good point. There are risks out there that we know about, but there are risks out there that we do not know about. Projects must take deliberate action to discover these risks. This action must not be based solely on discussion, it must be based on trying out aspects of the solution. . That said, how does one know the problem space has been explored? Unfortunately I do not have a definitive answer to this, but some pointers are: . Define the boundary of scope. This will determine the space to be explored. (Anything outside the boundary is waste!) . Define the entire space within the boundary. This is likely to be in high level stories (or themes, epics) on the backlog. . Be precise about the boundary of each of these stories to prevent items slipping between the gaps. . Estimate the stories. This is likely to be imprecise for high-level stories, but should drive out some risks. E.g. t-shirt sizing may be good enough. . Assign a risk to all stories. This should be a number, very much in the way stories points are used. The higher the number, the higher the risk. It might be enough to use the estimate itself and a flag to indicate the story carries risk. A risk-matrix approach of balanced severity/likelihood could be used too. . Until this space is explored thoroughly, the project might be carrying a large  unidentified  risk. . Of course that only identifies risk. The next step is to mitigate risk. In traditional project management this means assigning some sort of action to reduce the impact or a pointer to some other action that will lower that impact. In the Product Backlog risk is lowered by delivering that story, or more likely, extracting the risky parts of the high level story into a smaller story to be delivered high up in the backlog. In some cases this may be a spike or an experiment of some sort. (RUP aims to mitigate features that are identified as  architecturally significant  - again by taking an action to deliver part or all of that feature.) . Applying the method described above would result in a backlog with stories broken down with risky elements extracted into smaller stories. .  Warning: Product Owners like to order the backlog by business priority.  . Fair enough, let’s deliver the most important features first. Of course what I’m driving at is that business priority isn’t the only important factor, especially early on in the development. Early on we need to perform experiments to prove the architecture. Equally we may explore user experience and other important aspects - but these are usually more obvious than the abstract concepts of architecture and non-functional requirements. Product Owners need assistance here; the Architect is an obvious source of help but a visualisation may help too. . The Risk Burndown is simply a plot of the sum of the risk in the Product Backlog. It is analogous to the Product Burndown, a plot of the sum of story points remaining in the backlog (which  might  equate to value???). . This gives the Product Owner a visualisation of the risk, an important tool for discussion with the Architect and engineers. Of course the Product Owner may have good reason to prioritise on factors other than technical risk - but at least they are armed to make an informed decision. .   . The chart above is an example I used on a previous engagement. The vertical scale was number of stories (but could easily have been story points). Risk was marked as count of stories carrying significant risk (in this case marked as “architecturally significant” in the backlog) multiplied by 10 (weighted so the plot line would show up as significant compared to the other lines). (Applying the 80:20 rule, a factor of 5 would be expected: as a rule of thumb about 20% of the backlog carries risk. In our case a factor of 10 seemed to work.) . The risk-value crossover was somewhere around the 10th April, though don’t read too much into this. It’s not a milestone in itself (especially as an arbitrary weighting of 10 was applied to risk in this case!) it just indicates a good time to hold conversations around mitigation of risk vs speed of delivering value. . Lightweight frameworks (e.g. Scrum) are not explicit about managing risk. Backlogs and project management techniques may not cover the detail that the delivery teams themselves require. . The Risk Burndown approach described in this article is one such technique, allowing the current risk level to be visualised. This allows the right questions to be asked during the lifecycle of the project. Very early in the project the trend would be for risk to increase as the risks are uncovered. As the development starts to gain pace the risks should plateau and start to drop. . As always, transparency allows us to inspect. Inspection allows up to adapt. ", "date": "2015-03-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Reflections on Symfony Live London 2014\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/open%20source/symfony-live/", "abstract": " At the end of September, I went to my first “non-Drupal” PHP event,  Symfony Live London 2014 . With Symfony components becoming a large part of Drupal 8 it was an excellent opportunity to learn a bit about what it all means, and meet the Symfony community. I’ve dabbled with Drupal 8, and we use some Symfony components in our existing Drupal 6 and 7 projects, so I wasn’t coming in completely cold. . Throughout the conference, I was struck by how much focus there was on re-use, decoupling and framework interoperability. It was the most common thread running through both days - from a workshop diving  into the depths of the HTTPKernel , talks on avoiding  The Dependency Trap   and  building composable HTTP middlewares using Stack  -  as well as one I couldn’t make it to, which covered  Decoupling with design patterns  -  all driving home a message of using frameworks to give you power, but to ensure  your code has minimal dependencies and maximum opportunity for re-use. . “Decoupled code is easier to maintain, easier to re-use, easier to read” .   Jakub Zalas   . Top among these presentations for me though, was  The Naked Bundle  presented by  Matthias Noback . Matthias focused on how the bundle (Symfony’s equivalent of a module) could be stripped back to the minimum Symfony specific code, and merely act as a bridge between the framework and your business logic (which, he asserted,   has no place belonging in a framework specific implementation, and should be in   its own standalone, interoperable library). . “The framework is for you, but your code doesn’t need it” .   Matthias Noback   . The short summary of The Naked Bundle was the premise that it should be easy to take your business logic and expose it to a Symfony bundle, a Laravel package, a Drupal module - it’s all PHP after all. Rather than repeat the talk here, I’ll direct you to the  excellent slides  describing a number of approaches to meet this goal. . The concept of a Naked Bundle has some direct relevancy for the Drupal world. We’re in the middle of the Drupal 7 lifecycle, many of us still supporting or evolving Drupal 6 sites, and most of us looking forward to Drupal 8 and the changes that it will bring to the Drupal ecosystem. It’s therefore not unreasonable that a large number of Drupal developers are building functionality which may be in use on three versions of Drupal, or maybe more, not to mention the liklihood of Drupal developers looking at other solutions for some sites, whether it be Symfony, Laravel or something different. . So then, is the Naked Module a concept worth considering? Why not move as much business logic as possible into interoperable PHP libraries which get pulled into Drupal via a bridge module? There’s obviously a fine line to tread in ensuring you don’t throw out all the benefits of Drupal’s core and contrib functionality, but if building something specific to your domain then there’s a lot to be said for this approach: . Portability - PHP libraries are more easily moved between Drupal versions, or to other frameworks altogether. Migrating functionality from one version of Drupal to another therefore becomes much less of an issue, as your custom code is now less coupled to specific versions. . Testability - a standalone PHP library is more easily unit testable, regardless of whether it’s going to be used in Drupal or not. . In projects at Capgemini, we’re already using several approaches which lets us get some way towards this concept: . Writing domain specific logic in standalone libraries, for example, code to handle  creation, validation and manipulation of business objects implemented in PHP classes,  and called out to from Drupal hooks . Integrating with internal or external remote services via a service wrapper which can again be called from within the Drupal module . Using the  Composer Manager  to include these PHP libraries in our Drupal installations so that they can be managed separately to site or module repositories . Using a shared ORM and DBAL such as Doctrine to access custom database objects consistently regardless of Drupal version or framework, particularly when they don’t need to be Entities . Most, if not all, of these approaches are already in use in the community, for example  Commerce Guys recently released a generic PHP addressing library  to handle creating, manipulating and formatting postal addresses for shipping or billing across different countries. Providing this as a generic library rather than hiding it in a Drupal module means the support for this logic can benefit the entire PHP community. . As Matthias stated in his Naked Bundle talk, it’s only sensible to seek  practical reusability . In the case of Symfony bundles an example of an allowed dependency would be the HTTPFoundation classes and trusting the HTTPKernel. . In Drupal, we’d want to take advantage of existing hooks, and well defined, well tested APIs  rather than re-inventing things. However, with a libraries first approach, modules can be made smaller, slimmer, avoid framework specific conventions and dependencies, and simply expose resources to add rich domain specific functionality for easier porting to other versions or frameworks. ", "date": "2014-10-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Automated testing for POODLE\n", "author": ["Mike Wallis"], "link": "https://capgemini.github.io/open%20source/testing-for-poodle/", "abstract": " Why should systems and infrastructure not be treated in the same way as other software components, especially when it comes to implementing security concerns. With today’s POODLE  announcement  of another SSL vulnerability it makes sense to add infrastructure tests to your regression tests .  ServerSpec  is a solid way of testing for this, and can be done as follows: . Use of the  &lt; /dev/null  is to force the openssl client to terminate instead of waiting for input from the shell as we are only interested in the key exchange. ", "date": "2014-10-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Trade-Offs\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/architecture/trade-offs/", "abstract": " waiting . blocked . taking the long way round . forgetting what you knew . Engineering around why stuff goes slow ", "date": "2014-10-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Microservices - A Reality Check(point)\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/architecture/microservices-reality-check/", "abstract": " It’s reached the point where it’s even a cliche to state “there’s a lot written about Microservices these days.” But despite this, here’s another post on the topic. Why does the internet need another? Please bear with me… . We’re doing Microservices. We’re doing it based on a mash-up of some “Netflix Cloud” (as it seems to becoming known - we just call it “ Archaius  /  Hystrix ”), a gloop of  Codahale Metrics , a splash of  Spring Boot , and a lot of  Camel , gluing everything together.  We’ve even found time to make a bit of open source ourselves -  archaius-spring-adapter  - and also  contribute some stuff back . . Lets be clear; when I say we’re “doing Microservices”, I mean we’ve got some running; today; under load; in our Production environment. And they’re running nicely. We’ve also got a lot more coming down the dev-pipe. . All the time we’ve been crafting these we’ve been doing our homework. We’ve followed the  great debate , some contributions of which came  from within Capgemini itself , and other less-high-profile contributions from  our very own manager . It’s been clear for a while that, while there is a lot of heat and light generated in this debate, there is also a  lot   of   valid   inputs  that we should be bearing in mind. . Despite this, the Microservices architectural style is still definitely in the honeymoon period, which translates personally into the following: whenever I see a new post on the topic from a Developer I respect my heart sinks a little as I open it and read… Have they discovered the fatal flaw in all of this that everyone else has so far missed?  Have they put their finger on the unique aspect that mean 99% of us will never realise the benefits of this new approach and that we’re all off on a wild goose chase? Have they proven that  Netflix really are unicorns  and that the rest of us are just dreaming? . Despite all this we’re persisting. Despite always questioning every decision we make in this area far more than we normally would, Microservices still feel right to us for a whole host of reasons.   In the rest of this post I hope I’ll be able to point out some of the subtleties which might have eluded you as you’ve researched and fiddled, and also, I’ve aimed to highlight some of the old “givens” which might not be “givens” any more. . They are the right size. And by “right” I mean “right” for a developer, for source-control, for CI, for documentation, for release/upgrade, for scaling, for resilience, for APIs/consumption/composition into things-larger, and finally for replacement. For all these things they’re all good . They get things out of the way. With Microservices we’re coding in Java again, well, in Camel-Java-DSL, and this lets us think like software engineers, rather than JEE architects or Spring-Bean-experts. It means we can  TDD , and  TDD like we meant it , and that means we can refactor, and keep our code and designs looking like we care about them.  (Better still we haven’t had any weird classpath error issues to debug from our JEE server as we don’t have them. And when we’ve had problems on the wire, because we’re using  HTTPClient  direct, we can get into it and find out whats going on far more quickly) . They’re more predictable. Perhaps the biggest gain is because they’re “micro” they’re easy to comprehend.  Now I’m not forgetting the dangers of combinatorial complexity (we’ll get to that later) but because we’re working with small, well-tested cohesive components here, and stateless, idempotent, circuit-broken ones at that, things are a lot more likely to do what we think they will do.  The spare-cognitive-load gains from that as someone ultimately responsible for all this is immense . They’ve made us question how we do things.  Accepted wisdom isn’t accepted any more .  The change in approach has made us question far more than we would on a “regular” project. Because this fundamental part of our job has changed, what else might have changed too?  The resulting flowering of creativity in the team has been exceptional, and its been exciting to see it unfold.  What’s more, I’ve seen properly reusable code coming out of the teams for the first time in my career. It’s almost as if all this component-thinking at the microservice level is infecting everything else. #winning . It’s fun, it’s exciting, and actively doing things that are new (and this does feel new) keeps you on your toes far more than a “standard” (read: JavaEE) approach would. That’s a good thing. A great thing . And yet its not all #winning.  Perhaps it’s the lack of balanced opinion in the general chatter that makes me feel the fear I mentioned earlier. Most posts I read are just so sychophantic on the topic, and the world  really  doesn’t need another one of them. So, deep breaths, lets dig into that a bit more and present some of the reasons Microservices might not be for you. . First up, some honesty.  We’re finding that despite all the noise in this space very few folks out there are actually  doing  this, and even fewer doing it in a public manner.  We however are.  Finding that very few others are with you can be a little scary at times.  It means you actually have to  do  research  and  make your own decisions  which for most of us in the safe world of Jave Enterprise Development is a new experience.  Consequently, we’ve staked our professional reputations on this, and we’ve got a lot less to hide behind - we’re on the front line of all this challenging of accepted wisdom . Things won’t “just work” when you glue them together; and things which do work might not have the greatest documentation in the world.  Many many decks on  slideshare  refer to all the bits you’ll need to get up and running in seconds - but there is a impedance mismatch between many of them, and as an early adopter  you  need to fix that.  Having said this, the other thing all these projects have in common is a vibrant community. In most things in this area the code is under active development, and so is the documentation. If you want to get involved and help, folks are very pleased to have you along for the ride.  That’s great, but it does slow things down a bit.  It also means you might be  ahead of the curve  on the major libraries you rely heavily on - for example we plugged Hystrix into Camel for our own Circuit-Breaking purposes before they added a CB of their own in the  latest (2.14.0) version .  We also chose  CXF  for our REST APIs (when the rest of the world was going  Jersey ) because it was a first-class citizen in Camel, only to find the exposure of Camel Routes as REST wasn’t at that point incredibly mature.  ( Note: It now is , it’s even got  Swagger support ) . Related to the point above, you need to remember you might make wrong bets. I already mentioned that we went CXF when the rest of the world seemed to be going Jersey. To be honest that’s not hurt us too much - we have what we need. We’ve also been right in adopting Hystrix, Archaius, and looking at  Eureka ,  Ribbon  and  Zuul  as they have only just been announced as being supported by the new  “Spring Cloud” .  It might however have been a mistake to go for  Spring Boot  -  Fabric8  is getting more and more mature by the day, and solves a lot of problems we might face at some point in the future, or have had to code around ourselves (i.e. by building our continuous deployment pipeline with  Jenkins ,  Puppet  and  Capistrano ) . You end up with a  lot  of moving parts. You’re confronting the  Fallacies of Distributed Computing  head on, and tackling them each in turn, in everything you build.  You end up leaning more heavily on tools like  Maven  (in our case) or  Gradle  (we’re evaluating) , and thinking about versioning, and running concurrent versions from the beginning is key.  You also end up needing to be able to “boot-up” a component with a load of stubs for all their dependencies so that you can run individual bits on their own . This is a new way of thinking.  This is INTEGRATION at EVERY LEVEL.  In the JavaEE world we never thought of threads because we weren’t allowed to. We’ve found that models like  Scala’s Akka  are a great mental tool for thinking about these problems, even if we’re not using the frameworks, but we had to get there the hard way . Following on from the two points above you end up having to make some concessions in order to be able to cope with all this - the biggest one is that you need to embrace immutability (in code and deployables, and environments) and  discard state .  This makes a lot of things easier, and, if you invest the time in proper Continuous Deployment and  elastic scalability enablement  . You end up a lot closer to the metal, and the network. We’re working with  HTTPClient  directly, and are looking at  Protobuf  /  Thrift  /  Avro  for inter-machine comms (Camel lets us do inter-process, intra-machine comms quite nicely).  Latency also hits you front and centre. Again, having to deal with this head on is no bad thing, but it’s not the usual state of being for a traditional Java developer. . Inter-team comms - because you easily gain the benefits of small teams (2-3 dev) working on individual “services” hidden behind a clean API you end up hitting the many patterns finely articulated by Eric Evans in Chapter 14 and beyond of his  Domain Driven Design: Tackling the Complexity in the Heart of Software  (perhaps the best part of that fine volume).  If you know to expect them then this isn’t too bad, but it will happen. This has meant that we’ve been looking at  Swagger  as a nice way of documenting all our APIs, internal and external, to reduce the interruptions when team A needs to consume component from team B . You end up with variation. Yes, we’ve componentized, have created some common components, and had various pull-reqest submissions back to the community accepted, but you still end up with various ways of doing the same thing.  Now this isn’t always a bad thing, but it means that you need to rely much more on keeping a  clean code(base) .  We review every pull request, with approval being required from &gt;= 2 developers. Typically we aim for these approvers to be from outside the team.  We’ve also instituted an internal RFC mechanism (stolen from  Carl Quinn  who I believe has used it at both Netflix and Riot Games to manage changes required by the dev teams he leads) . You end up flooded by data.  Everyone says that you need a lot of monitoring. They’re right. Its easy to add too. So easy in fact that we ended up submiting a  pull request  to Hystrix which allows you to filter what is produced because we were flooding the UDP port of our monitoring servers.  We’re lucky in that we have a great DevOps team who put all the supporting infrastructure in place for us to take advantage of all this too.  But we’ve needed to become expert in setting up just-the-right-amount Graphana dashboards.  Its another skill to learn. . Dev becomes support and Support becomes dev. this one relates back to Number 5. Folks who are in Support because they want to support things that look like other things will get a shock.  We’ve been lucky - our support guys have been very keen to learn something new. We’re getting them to work on new features with us as a means of teaching them how things work.  Additionally, because things are so new, we have to get a lot more involved in support. So much so that we’re trying to get a big TV for our dev area to put our  Hystrix  /  Graphana  Dashboards on permanently so we know how things are looking . And that’s it.  I’d like to point out again that so far, we’ve been  very pleased  with our decision to adopt this architectural approach. But we’re still keeping our eyes open. Remember, there’s  No Silver Bullet . . Before we close, here’s a reading list of the things we’ve found most useful in our journey to here. Please add a comment if you have any other suggestions of items to add: .  Idempotency is not a Medical Condition - Pat Helland  .  Martin Fowler - You Must Be This Tall To Use Microservices  .  Adrian Cockroft - Migrating to Microservices  .  Michael Nygaard - Stability Patterns, and Ant-Patterns…  .  Eric Evans - Domain Driven Design: Tackling Complexity in the Heart of Software  .  Uncle Bob - Microservices and Jars  .  Steve Jones - Microservices - Money for old rope or re-badging SOA for the cool kids  ", "date": "2014-10-17T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Component Based Development for the Enterprise\n", "author": ["Malcolm Young"], "link": "https://capgemini.github.io/drupal/component-based-design/", "abstract": " Recently 10 members of the Drupal development team at Capgemini went to  Drupalcon Amsterdam . Having been to two Drupalcons before, I more or less knew what to expect, but something I hadn’t previously given much thought to was how big an event it is.  Compared to most other web conferences , it’s a beast. For me personally, I wonder if it’s getting too big and too impersonal, and I think that I’ll be more interested in going to smaller events. . Some of the more interesting sessions for me were the BoFs - in particular a discussion of  open source training material and apprenticeships  provided a lot of food for thought, and hopefully we can get involved at some level. Capgemini already does a lot of work  getting graduates and apprentices into our engineering practice , and with such a big Drupal team, I hope we can benefit from and contribute to the Open Drupal initiative in 2015. . Whenever I go to an event, I come back with a to-do list, and this was no exception. I’ll definitely be digging further into CasperJS following Chris Ruppel’s session on  Automated Frontend Testing . I was also very interested to hear about the way that Lullabot spin up  test environments for pull requests  - it will be good to investigate the feasibility of incorporating this into our workflow. . The other talk that stood out for me was John Albin Wilkins on  Styleguide-Driven Development . For a long time, I’ve had a bee in my bonnet about the value of component libraries over Photoshop comps, and it was good to be reminded that I’m not alone. In an interesting session, John outlined an approach to integrating component-based design and automated style guides to agile development projects. . It’s been said many times before, but it’s worth remembering that all too often, people are still  thinking in terms of pages, rather than systems . . In the context of the work that we do, this is even more important. We’re a large development team, building CMS-driven sites for large corporate clients, where the design is done by a team working for another company. We’ve made some inroads into building a more collaborative process, but it’s still too easy to end up with the designers throwing things over the wall to the developers. Very often the designer isn’t closely involved during the build phase, and design tweaks are agreed between the client and the developer without the opportunity to go back to get the designer’s opinion. . This is the whole point of living style guides - component libraries that stay in sync with the code as it evolves. As  Shay Howe has discussed , component libraries help everyone on the project. . Designers are reminded of the visual language of the project, and it’s easier for them to see when they’re about to reinvent the wheel. . Style guides help developers by defining and documenting standards, and make it easier to dig in and find the way you solved some problem six months ago. . The projects we work on are large and complex, with a long lifecycle, and as developers we need to value maintainability of the front end code. Part of John’s approach to this was his class-naming convention. Having seen Jonathan Snook present on  SMACSS  I’d thought it was interesting, but to a certain extent it felt like a fancy name for something that was basically common sense. John’s presentation brought the concept to life well, and persuaded me that there’s more to it than that, with an impressive display of  flower power . . The other interesting aspect was splitting up SASS files into components, and using  KSS  to create the style guide - this is something I definitely intend to do on my next project. . Modularity makes sense - it’s how the back-end is going, it’s how Javascript is going, so why shouldn’t design and CSS go the same way? .  UPDATED 3rd December 2014: Unfortunately we got Chris Ruppel’s name wrong in the original version of this post, calling him “Dave Rupl”. Sorry Chris.  ", "date": "2014-10-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Agile and Lean\n", "author": ["John Shaw"], "link": "https://capgemini.github.io/agile/agile-v-lean/", "abstract": "  Or should that be Scrum and Kanban?  . I was in a short conversation a few days ago around whether we, in the business of software development, want to raise the profile of Lean. Perhaps even create some formal training materials. . The message I was getting was that Agile doesn’t need Lean. This is because Agile practices are built on Lean - especially Scrum! . This is true. Scrum especially is not really a software engineering practice at all. It’s an approach to organising people to deliver something that initially is not very well defined. . I think the fear is that Lean is a process improvement technique for processes that are already reasonably steady state. Software development is NOT steady state. It is unstable all through the lifecycle, especially during the beginning when there are many risks, possibly many unmitigated risks. Lean doesn’t really address risks. Lean is focussed on improvement of an established process. . Having reflected on the conversation, this view misses the point somewhat. Often the barriers we face are not in the software development portion of the delivery: rather gathering timely requirements, planning &amp; estimation, testing/QA and cut-over present the challenges. Often these are not addressed as they’re not in our immediate field of view, snowball and present even bigger challenges. . Lean can help here. Lean can map this “process” end to end. It can then help establish metrics to demonstrate where the problems lie. . Lean can also help by bringing a meeting structure to all levels and promote the idea of  servant leadership . . A conflict, or perhaps even a paradox, is that the customer is often the supplier too! Maybe we need to redefine this: the customer must set clear parameters for performance (the goal), then the project need to convert these into requirements themselves by consulting the stakeholders on the changes that will bring about positive impacts. . For example, an analysis I performed a while ago on a series of project releases is shown in the the charts in the image. .   . The top image is a  Cumulative Flow Diagram  (CFD) for a number of releases into production of a product. Each band shows the user stories that were queued or in in-progress at that time. The large steps are where the process was outside of the project team’s direct control. The thinner red stripe was where the team were working in an incremental way to develop and test the software within sprints. The green band after the red stripe shows the software “sitting on the shelf” for the next phase. . Restrospectives would centre on the “red stripe”. Blockers, knowledge transfer, skills, planning, estimates, etc, etc. . Perhaps we should have been looking at the larger steps. . At the time there was some thinking around improving the process. The driver was the desire for automated testing, but I had a look a little wider. I speculated what the same delivery might have looked like by addressing the larger steps, resulting in the second chart shown. . There is a clear difference. Immediately there is a noticeable change in the amount of “green” on the chart. This is because the software is not sat on the shelf for so long: it is “released” quicker! . Because the releases are more frequent, less goes into them. This reduces the demand in requirements coming through. Instead of having to define a good proportion of the stories for a large release up front, the number of stories needed is much lower and therefore with a lower lead-time. This is reflected in the thinning of the first dark blue band. . The example demonstrates the potential for using Lean tools, KanBan and CFD, to inspect and analyse the pipeline. Embracing Lean further may well reveal further opportunities for improvement. ", "date": "2014-10-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Reflections on Drupalcon Amsterdam\n", "author": ["Chris How Kin Sang"], "link": "https://capgemini.github.io/drupal/drupalcon-amsterdam/", "abstract": " Along with  Malcolm and other colleagues from the Capgemini Drupal team , I attended the recent  Drupalcon in Amsterdam . And as well as admiring the Dutch attitude to cycling and its integration in the  city (btw London, blue paint on the road != a cycle superhighway), we also caught up on the state of Drupal and its future. So here a few reflections from Drupalcon Amsterdam. . I’ve been to a few Drupalcons now, and compared to previous years, use of Drupal in the enterprise (or more generally at scale) seems much more commonplace. Dries Buytaert’s (Drupal founder)  keynotes  have made reference to Drupal’s ability to integrate with other  systems as a key strength, and in these types of projects, Drupal is not used as the all-in-one solution that maybe was more commonplace a few years ago. . Partly this is also due to the way the web has moved far  beyond the idea of ‘a thing you use on your desktop computer’, and Drupal has  shown itself to be adaptable to this. For example, the idea of  Headless Drupal  was a well covered topic this year. Of course, previous ‘cons  have had talks on uses of Drupal with other technologies ( e.g. node.js talk from London 2011 ) but whereas  it seemed more an interesting edge case then, now there are many successful  real-world projects adopting these ideas. . Based on my not-entirely-comprehensive  memory of the subset of sessions I attended from past Drupalcons, this year there seemed to be many more talks which could have easily been at a frontend or PHP specific conference.  Drupal 8’s  use of  Symfony 2  components and shift to making use of components Proudly Found Elsewhere is part of this. . A few talks that those of us who attended would recommend (not an exhaustive list). I won’t go into too much detail (that’s all in the slides and the video) but these are worth checking out. . The types of frontend testing which can be automated, covering performance ( Phantomas ), CSS ( Wraith ) and end-to-end ( CasperJS ) and integrating this into your build workflow.  Slides   Video  . I think the PHP track is a welcome addition to Drupalcon. When developing custom functionality on projects here at Capgemini, we often write the business models and logic as separate classes to Drupal which are then ‘glued’ via hooks which implement those classes. That kind of separation has advantages with portability, testability and some amount of simplification in that Drupal isn’t a dependency.  Video  . Very interesting talk on how open-source is (in some ways) critical to our individual freedom in the modern world. In an age where “a modern house is a computer that you co-inhabit”, if a system went down - or arguably worse, were controlled by overzealous authorities - it can become uninhabitable. What do we do in this case? Is the Apple iTunes/U2 debacle merely the thin end of the wedge? Interesting viewing for anyone who contributes or uses open source.  Video  . As Drupal 8 entered beta during the conference, it was an opportunity to check out the changes. The plugin system for extending functionality looks interesting. In Drupal projects at Capgemini we have adopted approaches such as abstracting  business logic and objects into standalone libraries and classes, called from  hooks and callbacks where we need integration with Drupal. This approach allows us easier unit-testing and portability of classes. D8’s plugin system looks like a good way of achieving those advantages while implementing a Drupal API. . Having spent a lot of time on projects wrestling with the various methods of deploying and updating configuration,  the CMI (Configuration Management Initiative) , which imports and exports YAML files to update and dump site configuration is a very welcome addition. . In the frontend, I’m looking forward to using the Twig templating. The idea of having cleaner PHP-free templates yet still with the flexibility to have filters and basic logic is going to help improve separation between the theme and module layer. It’ll be new to me ( as will other things ) but as with other components, they have been successfully used in other PHP projects so there is documentation and examples already out there. There are some smaller changes too - removing drupal.js’s dependency on jQuery (thereby gently encouraging use of native JS), updating the versions of included libraries (and committing to keeping them up-to-date during D8’s lifetime) and including no JavaScript by default are good steps to optimising the frontend. . Where things may be more challenging is the APIs which have both new object-oriented components and retain the hook and callback system in some combination (for example,  declaring widgets via hook_element_info). To take an example from a core module, the file module’s managed_file widget functionality is spread across a number of  callbacks as well as its own FileWidget class. It’s not the most straightforward development flow to follow. Where this has some advantages is that existing modules will not need a complete OO rewrite just to be compatible with D8 - a module author could do a simple port at first before rewriting to take advantage of the new APIs. But some care is need to ensure that the advantages of encapsulation, increased unit-testability and extendability that the OO patterns introduce are not compromised by dependencies on a particular hook or callback. . Finally, as Drupal 8 progressed from alpha to beta during Amsterdam Drupalcon,  it does seem now that it can be realistically considered for projects coming up  on the horizon. Obviously there will a lot more work going into the project to  fix bugs and improve performance and so forth. But now the major API decisions and  changes have been made. But with this iteration of Drupal incorporating many more  features from the contrib world (Views, WYSIWYG, etc) and PHP (Symfony2 components), it looks to be a healthy position for use when that 8.0.0 finally lands. ", "date": "2014-10-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Looking Forward to the Spring eXchange 2014 with Capgemini Software Engineering\n", "author": ["Scott Davies"], "link": "https://capgemini.github.io/development/spring-exchange-2014/", "abstract": "     . It’s the  2014 Spring eXchange  at  skillsmatter  Thursday, 6th - Friday, 7th November, and Capgemini are incredibly excited to be sponsoring the event! . The Spring eXchange is the best place to learn about the latest developments in the Spring ecosystem, meet real practitioners in a really interactive setting, and hear from some great speakers. . There are three tracks running over the two days; covering Enterprise Java technologies, Big Data and Spring XD, Spring in Production, Spring IO, Spring Data, REST and Microservices - a whole range of Enterprise Java topics. There is a beginners track as well, for those just getting started. . Whether it’s: . Building “Bootiful” Applications with Spring Boot with  Josh Long  . Looking at AngularJS and Spring with  Thorsten Späth  . Designing Spring Boot based REST-ful Micro-services with  Ben Hale  . Investigating Microservices with  Russ Miles  . Spring Data REST with  Oliver Gierke  . and many, many more… . There’s something for everyone. It’s going to be a great couple of days. Come and meet us there! . Thursday, 6th - Friday, 7th November at  Business Design Centre,  London.  Get tickets now at skillsmatter  . Tweet this or click the twitter link below: . Looking forward to the @skillsmatter Spring eXchange 2014 with Capgemini Software Engineering @CapgeminiUK #springX . We’re looking forward to seeing many of you there. Come and meet us there! ", "date": "2014-10-28T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Keeping Drupal Secure\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/drupal/securing-drupal/", "abstract": " The recent announcement of Drupal  SA-CORE-2014-005  caused a global scramble amongst Drupal site owners, developers and hosting providers to patch their sites and protect themselves from the vulnerability. Unfortunate site owners who weren’t quick enough were left with a site audit to ensure all traces of any successful attacks were removed from their environments. . Firstly, hats off to the Drupal Security Team, Core release co-ordinators and the Drupal community in general for a rapid, considered and clear response to the issue. The early warning that a security release was coming was well stated enough to make it clear to many that it would be a major issue. The normal release time was brought forward to a more suitable timeslot to ensure the maximum audience saw it early, and the post-release guidance,  FAQs  and blog posts covered all aspects from how to patch,  what to do if your site had been hacked , and what to look out for to indicate if you had been affected. A follow up public service announcement  PSA-2014-003  was published drawing further attention to the risks to unpatched sites and offering further guidance. . Secondly, excellent work from  Sektion Eins , the penetration testing experts who detected the vulnerability. This was a defect that has existed for many years, and many organisations will have run penetration tests that haven’t previously detected it. Having been creative enough to find the vulnerability, it was then reported and managed responsibly, which should be praised. . Now, I don’t want to focus on SA-CORE-2014-005 any further, but instead I want to talk about some ways we can protect our Drupal sites and minimise our exposure to vulnerabilities. Whilst bugs will always happen (whether security or functional), there are steps we can all take to mitigate our exposure and respond quickly to incidents. . The best cure is prevention, and the next best is to catch it early. There are lots of ways to keep up to date with what is going on in Drupal security: .  The Drupal Security pages  contain the most recent core, contrib and public service announcements for Drupal and include RSS feeds . Sign up for the Security Announcements mailing list (login to Drupal.org, go to your user profile page and subscribe to the “Security announcements” on the Edit » My newsletters tab) . Follow  @drupalsecurity  on Twitter . All three of these routes will ensure you’re on the quickest possible route to hearing about new security releases. It then becomes quick and easy to review the latest threats, decide if you’re vulnerable (by reviewing the mitigations - a lot   of vulnerabilities depend on elevated roles or other mitigations, and these   will be clearly described on each Security Advisory), and how quickly   you ought to upgrade your core installation or other modules. . It’s also worth having the Update module enabled in your dev environment to phone home and check if there are updates for any of your modules available - however if like me you only login to some sites occasionally then you’ll want a backup strategy for keeping you informed. . It should then become almost ritual that for each alert you: . Determine if your core version is affected . Determine if you use the contributed module reported as affected . Reviewing any mitigations (vulnerability requiring elevated permissions etc) that may keep you safe from the vulnerability and reduce the urgency of patching/upgrading . For any urgent updates, apply as soon as practically possible . For any non-urgent updates, apply to normal dev/test/release cycle and deploy in your next release slot . The great thing about Drupal is the wide range of contributed modules that you can use to kickstart your development and re-use functionality that other people have already built for you. However, this comes with its own risks - as you’re effectively installing code that you’ve not written and so have no guarantees of how secure it may be. . With this in mind, it’s essential to review the modules you download, rather than just installing them right-away. Just as we check if modules contain any potential performance gotchas, we also look to see if there are any obvious security flaws - use of $_GET or $_POST variables in templates, data directly concatenated into SQL queries, no roles or permissions defined, all gives clues, and we can use  Drupal secure coding guidelines , and awareness of the  OWASP Top 10 , to inform our reviews. . To some degree, these checks can be automated by running things like  Coder , which includes some basic security checks and has some good proposals to add more  security sniffs . Modules like  Security Review  can also check for some easy-to-make mistakes which may render your site insecure - and are invaluable, particularly if you’re new to site security. However, automated checks are only as good as the information they’re fed with, so there is no substitute for getting in and reading the code - you were going to look to see how it worked anyway, right? .  Remember - if you do spot any security flaws in Drupal Core or Contrib, don’t raise issues in the public issue queues, make sure you follow the process to  report a security issue  to the security team, so that it can be fixed in private and released before the vulnerability is made public.  . It’s not just Drupal that can be exposed to vulnerabilities such as these,  PHP itself has had its share over the years , and realistically no large body of code is going to go its entire life without some vulnerabilities creeping in. There are also many more people trying to find vulnerabilities to take advantage of than there are people trying to fix them. . This means keeping your entire platform secure becomes a constant activity, so a reliable and repeatable test and release cycle becomes essential - if pulling down the latest updates and deploying them takes a significant amount of time, the odds are that you’ll get lazy and skip an udpate or two. This soon becomes the security guys nightmare as sites, services and platforms become more out of date and more vulnerable to attack. . All the attention paid to Core and Contrib security updates is very valuable, but it becomes worthless if you don’t give the same care to the code you and your team write in custom modules. An awareness of how to follow Drupal best practices to write secure code is an essential part of the training and education of any good Drupal developer. Fortunately Drupal provides some excellent abstraction layers to remove some of the thinking on these topics, so make sure you  use the database layer correctly ,  handle text securely , and  create forms in a safe way , as well as all the other  best practice . . It’s also well worth commissioning penetration tests from external specialists to test your implementation and validate that it is secure. We use the expertise of these companies regularly, and rotate through a list of preferred companies to ensure that we’re constantly getting fresh eyes on the solutions, and varied approaches to testing, so as to maximise the chances of finding issues. The tests often prove their worth, and whilst every developer hates having defects raised, we’d all much rather the security issue be in a test environment than in production! . To finish, here are some excellent resources to keep up to date with best practice approaches to keeping your Drupal installation secure: .  Securing your Site  - Drupal.org Handbook entries providing security configuration advice .  Writing Secure Code  - Drupal.org Hanbook entries providing guidance on keeping your custom code secure .  Cracking Drupal: A Drop in the Bucket  - written by Greg Knaddison ( greggles ), first published in 2009 but still very relevant .  OWASP Top 10  - A list of the 10 Most Critical Web Application Security Risks ", "date": "2014-10-31T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Automation as a way of thinking... and docker\n", "author": ["Alberto Garcia Lamela"], "link": "https://capgemini.github.io/open%20source/automation/", "abstract": " Automation is a key and essential fact when solving problems and assembling pieces together. By automating you only need to do the same thing once, reducing the possibility of human errors so it helps to increase quality, efficiency and productivity with less effort. . It doesn’t matter what programming language, technologies or tools you use. It doesn’t matter what sector of Software Engineering you are focused on and it doesn’t really matter which discipline you work on. I would say automation can be more of a way of thinking that could even be applied to almost everything in your everyday life. . When you automate a process you are creating and increasing productivity whether the process is “a virtual machine provision”, “the steps you follow since you wake up until you get into the office” or “flirting with somebody”. You are giving an automatic solution to a problem by creating a system with the ability for saving, reading and reproducing a series of steps for achieving an objective that is going to save you time in the future. . When you face and solve the same or similar problems again and again you soon realise patterns and known techniques (e.g:  DRY ,  Information hiding ,  Open/closed principle , etc.) that will help you create a better design. Once you identify a regular pattern for solving a problem, you can then reuse this pattern every time you are building an automatic solution with common characteristics. But these patterns are not magic, they are always dependent to the context so you will need to suit them for fitting in your context. (E.g: There is no universal way to “break the ice with” a person as the reaction can be different depending on who you are approaching…) . Going back to the IT world, we are always trying to find, create and contribute to the best tools for automating our processes and ensuring high quality, reliable and scalable solution delivery to any users and clients we work with. In this  repo you can see the technical details  of one solution that fits together a traditional Virtual Machine,  puppet ,  docker ,  fig  and a behaviour-driven development framework ( E.g. Behat ) for automating the creation and execution of an  ephemeral User Acceptance Testing environment.  . We use the power of the  dockerfiles  for reusing the “uat_environment” puppet module (a wrapper over  selenium module  and its dependencies) and create a docker image with selenium running. . A “behat service” will be created to check if the selenium container is ready using the script inspired by  docker-wait  so it will run behat, which code is passed as a volume to the container. . We use   fig  for running the  containers  and managing our basic UAT environment. You can add as many selenium services as you need to your fig.yml and run different behaviour-tests projects simultaneously because as we are  linking containers  we don’t have to worry about port collision troubles. . Run “fig up” or any other fig command for automating and orchestrating your services. . As we only need to run processes in isolation here docker containers will be fast and less consuming than traditional Virtual Machines. . By using “fig” you can easily scale your UAT environment. . As we are passing the BDD Framework as a volume, the environment is totally decoupled from the code. . The environment can be recreated and destroyed as many times as you need without being consuming resources unnecessarily in the meantime. . Everything is under version control system and is reusable. . You can now run “fig” from Jenkins or any other Continuous Integration tool. ", "date": "2014-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Welcome to Capgemini Engineering\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/blog/first-post/", "abstract": " At Capgemini, we’re passionate about Engineering and the many facets that make up successful engineering teams. . We like to experiment with new technologies, establishing patterns that make our lives easier and more efficient. . We are active in the various open source communities that our technology choices expose us to. . Above all, we are driven to provide innovative and leading edge solutions for our clients, who often surprise us with new and interesting technical challenges and business issues. . This blog aims to capture all of this thinking, and share it with the wider software engineering community. We’ll intentionally be talking across different technologies, reflecting the diverse range of approaches taken with Capgemini. . We will strive for this to be a useful blog, and hope people will learn some things  from what we share; we know we’ll learn from the discussions that we hope these posts will trigger, so do please join in the conversation. ", "date": "2014-09-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Effective BDD\n", "author": ["Andrew Larcombe"], "link": "https://capgemini.github.io/bdd/effective-bdd/", "abstract": " It’s always important to remember that whilst Behat can be just used as a scripting language, in order to get the many benefits associated with BDD then you should always view your scenarios as ‘Executable Specifications’ for features that deliver business value and that are written in the domain language of the business. Your scenarios can then be tested using Behat by building small re-useable components, based on the good principles of  Encapsulation,   Separation of Concerns,  and  Don’t Repeat Yourself (DRY) . This post then is about the beginning the journey from Behat Novice to Behat Pro by understanding how to use the Page Object Pattern to build that encapsulated, reusable, code that marks out the expert from the novice. The complete code for this can be found at  https://github.com/andrewlcg/behat_tutorial  and although we’re using Behat here the same principles apply equally to any BDD framework. . Our scenarios should provide the complete documented specification for the software being delivered, and can be executed to provide validation of the system in a timely manner. To illustrate how to create effective Behat features, we’ll use the following example requirement written as a typical user story. . NB two things here . This is a requirement for the ‘Corporate Lawyer’. It’s too easy to write ‘As a User’ for each scenario, but this requirement is definitely not one being driven by the needs of the user. . There is no reference to any specific fields, only that the user has agreed to the Terms of Service. The mechanism for agreeing is an implementation detail, but isn’t part of the requirement. . Someone new to Behat will typically write the specification like this: . This specification has a number of inherent problems though: . It’s brittle - If any of the fieldnames, or their arrangement (eg we have a single ‘name’ field) changes, then we need to rewrite the specification. e.g. if we added or removed a field we’d need to modify the feature file even though the specification hasn’t changed. . It’s not reusable - if we wanted to reproduce this functionality as part of another page we’d have to copy-and-paste the steps. It’s not DRY! . It’s not even really a specification. The completion of those specific fields is not mentioned in the requirement, neither is the exact wording specified. . Let’s start by rewriting the specification so that it fits the requirement: . This specification is much better. It describes in plain language, with the minimum of jargon, what needs to be implemented to meet the requirement. It’s not brittle - it doesn’t rely on any specific implementation, so that if field names change then the specification, quite rightly, isn’t affected. . But, where do we implement these new step definitions such as “And they complete the signup form but do not agree to the terms of service”? The answer is that we encapsulate the steps by implementing the  Page Object pattern . This pattern enables you to interact with the functionality provided on a page, but hides the implementation behind an API (in reality, just a set of methods). The specification, encoded in a feature file, calls steps that are implemented in a PageObjectContext which in turn uses Page and Element objects to manipulate the page’s DOM. A very high level view of this can be seen below. (Click to see larger version) .   . You can read more about how Behat implements the Page Object pattern at  http://extensions.behat.org/page-object . As far as our example goes let’s take a look at the first couple of steps. We have three distinct components - a user, a page and a form within that page. Let’s look at the first step in the specification: . This step is implemented in the SignupPageContext as follows: . User::load simply returns a User object which contains information about a user, such as their name, email address etc - this information is stored in a YAML file so it’s not tied to any implementation and can be re-used across the project. This gives us a ‘user’ with attributes that we can use across all of our specifications. . $this-&gt;getPage(“Signup Page”)-&gt;open() is defined within Behat’s PageObjectContext class and auto loads a class we’ve defined called SignupPage and it simply opens the url stored in that object’s $path property. . The next step “they complete the signup form but do not agree to the terms of service” is again implemented within the SignupPageContext and, after checking that we have a valid user, does the following three things: . These should seem quite explanatory in their intent. Let’s examine what the first one does: . This is implemented within the SignupPage class as follows: . this auto loads an object of class SignupFormElement and calls its method enterUserDetails. This encapsulation enables us to re-use the functionality of the signup form on other pages, and prevents pollution of methods between classes. . The enterUserDetails method contains a simple implementation doing exactly what is required to enter the user details in the signup form. Again, this should be self explanatory: . Finally by taking a look at the layout of the files, we can see that the functionality is clearly separated and encapsulated - each class in the hierarchy is responsible for interacting with a well-defined part of the signup process. . You can read more about features as executable specifications in the excellent  “Specification by Example” . More information about Page Objects from Selenium’s perspective can be found at  http://code.google.com/p/selenium/wiki/PageObjects . ", "date": "2014-09-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Some Details on the Subtleties of Scala and the Uniform Access Principle\n", "author": ["Andrew Harmel-Law"], "link": "https://capgemini.github.io/scala/scala-uniform-access-principle/", "abstract": " Lets start off by taking a really simple function,  f , which simply makes and returns a 5-value tuple which can be captured in a  val  or  var : .  def f = (1, 3.14, “Mouse”, false, “Altitude”)  . The first element of this tuple is an  Int , the second a  Double , the third and fifth are  Strings  and the fourth is a  Boolean . In short,  f  is a very simple tuple-builder. That means we can take it and allocate the resulting tuple to a  val : .  val (n, d, a, b, h) = f  . Brilliant. Here, in microcosm, we have with an example of the  uniform access principle  in action. That is to say, as functions are first-class, I should be able to do with a function what I would do with any other object, and it should be transparent. It was this transparency that pulled me up short. . Now this is a great kick-off, but it doesn’t tell the whole story. Luckily the  Atomic Scala  Atom: “Uniform Access and Setters” is there to bring us fully up to speed. . Once you’ve seen it, it makes perfect sense, but when I came upon this chapter-ette, I’d been under the misapprehension that you could swap  def s for  val s or  var s as you pleased. However, it soon became clear that there was more to think about, and this thinking comes down to contracts; contracts that the language makes with us, and which it (and we) can’t break. . For this discussion, the relevant Scala contracts are: .  val s are immutable .  var s aren’t . functions ( def ) can return different values and so Scala can’t guarantee the result will always be the same . This means that you can’t just implement fields and methods from an abstract base type in a subtype using any old variable or function. You must make sure you don’t break the contract that was made already in the parent. Explicitly: . an abstract  def  can be implemented as a  val  or a  var  . an abstract  var  can be implemented as a  def  as long as a you also provide a setter as well as a getter . You’ll note that abstract  val s can’t be implemented with  def s. That makes sense if we think about it. A  def  could return various things – Scala can’t guarantee it’ll always be the same (especially if you consider overriding), whereas  val s are immutable. Broken contract? Denied. . But wait a minute, we missed a fourth contract. That second bullet mentioned setters. The contracts in play here are actually four-fold: .  val s are immutable .  var s aren’t . functions ( def ) can return different values and so Scala can’t guarantee the result will always be the same .  vars  require getters and setters` . But we can still roll with that if we add another little piece of Scala sugar, we can supply a setter method in the subtype:  def d3 = n def d3_=(newVal:Int) = (n = newVal)  . Here, the “ def d3_=... ” line adds the setter Scala needs to fulfill the contracts and we’re back in action. . One final thing to consider is how uniform really is the Scala implementation of the principle? Pretty well universal as far as I can see, because going beyond the scope of the Atom, what happens when the superclass and it’s methods and fields aren’t  abstract ? It turns out it’s exactly the same as above, as long as you remember your  override  keywords. Predictable. Nice. . This post originally appeared on the  “Scala Eye for the Java Guy” blog  as  two   separate  posts. ", "date": "2014-10-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Answering: How long will it take?\n", "author": ["Tom Phethean"], "link": "https://capgemini.github.io/agile/estimation/", "abstract": " “How long will it take to build my [thing]?” . A seemingly simple question that can strike fear into the heart of many developers, estimation is both essential and thankless at the same time. As a developer, you might feel you can never win - always asked to do things quicker - whilst a project manager usually wonders why developers estimates are inaccurate. In this post I hope to share some techniques that have helped me along the way, and might help reduce the pressure  on justifying your estimates! . The first thing to remember, is that an estimate is: . “an approximate calculation or judgement of the value, number, quantity, or extent of something… Synonyms: approximation, evaluation,  guess ”  The Oxford Dictionary  . So, estimate equals guess. Done properly, it should be at lease be an educated guess but your estimate is always going to be flawed in some way, something will change, be discovered, or interfere and mean you’re not going to be 100% correct. In fact, we can go further and say: you’ll almost always be wrong. . Getting it wrong doesn’t matter, it’s part of the learning process and feeds back into making future estimates better. . Simply take note of why your estimate was wrong, and make sure that the next time you are coming up with an estimate you consider whether the factors that de-railed your previous ones will apply in your new scenario. The aim should to be “less wrong” next time. . How many times have you been asked how long something would take, maybe in your daily stand-up, and quickly responded “by the end of the day” or “end of the week”, without really thinking the estimate through? Sometimes you might get lucky and things pan out ok, but many times these instinctive estimates don’t think through the problem properly and often take longer than first given. . Now that’s fine, it was only an estimate after all, but you’ve set an expectation and in all likelihood you’re going to end up working extra hours to make it happen to avoid disappointing someone. . The best tip I ever read, came from reading  The Pragmatic Programmer , and went something like: . “You almost always get better results if you slow the process down… Estimates given at the coffee machine will come back to haunt you.” . Step away from the discussion, ask for time to analyse the problem, even if its half an hour to organise your thoughts, before giving an estimate and ensuring it’s an answer you believe in. . For small tasks, arriving at an estimate can be pretty straightforward, but usually we’re not being asked for small task estimates. People want to know how long adding a new feature, or building a new website, will take. The trouble with this is that seemingly simple high level tasks can hide a multitude of traps that could inflate (or deflate) your estimate. . When faced with problems like this, the only way we can process enough information to give anything like an accurate estimate is by breaking the task down into as many constituent sub-tasks as necessary until each sub-task can easily be envisaged and estimated. . Break down the feature into components you’re comfortable estimating. As a hint, if the estimate for any one sub-task is longer than 3 or 4 days, you probably need to break it down further. But having said that, don’t make any task smaller than half a day in your estimates - even that trivial one line code change will need regression testing, unit tests tweaking, functional tests updating, and code review performing, so it’s never “just a 10 minute change”. . One word of warning though - you’re not trying to do a detailed design here, just trying to get tasks or stories at a level granular enough for you to know roughly what you’re going to need to do. . Every estimate is based on some assumptions you’ve made, so make sure you share these when presenting your estimate back to whoever asked for it. If you think there are no assumptions, then have a look through the list below and see if any would apply: . My estimate is accurate as long as … . …I get uninterrupted time to focus on it . …the environments are configured correctly . …I can re-use component X that we built last time, without significant refactoring . …the interface specification is accurate . Also, make sure you define “done”. Will done mean that you’ve committed the code and run some tests locally, or that you’ll have followed the change through various environments and its ready to be pushed into the next Production release? Regardless of how you define “done”, this is one of your key assumptions, so make sure you and and the person you’re giving the estimate to have the same definition of done and therefore share the assumptions. . We’ve covered this a little bit when saying not to estimate any sub-task at less than half a day, in order to factor in all the things we forget about when estimating. However, even when you add up all your sub-tasks (or put them into groups for adding up) we should pad our larger estimates to reflect the inherant uncertainty in the longer term estimate. . For example, if your estimate is around 3-4 days, call it a week. If it’s 3 weeks, then its a month, and so on. Using larger values of time helps set expectations with your stakeholders and also lets you have contingency for the inevitable unforseen delays that will arise. The longer the road, the more likely you’ll find a pot-hole. . The other way to approach uncertainty in your estimates is to provide a timescale range rather than a fixed number, with the size of the range depending on your confidence. For example, if you’re very confident you could answer “6-8 days” but if you’re not certain of your estimate you could say “20-30 days”. . Often the estimates we give, even when following the guidance above, will be padded out to reflect our uncertainty and unknown complexities. There might be new technologies or approaches, uncertainty over the level work that will be required  to meet a performance target, or anything else that you have little experience in on which to base your estimate. . In these cases, and to reflect the uncertainty, you could offer to re-estimate your tasks once you’ve completed a portion of the work. This could be at the end of a first iteration, or initial exploratory phase, during which you’re likely (again, if following good agile practices) to have started to tackle the things which had the highest risk, uncertainty or potential complexity. . Taking this approach allows you to give an estimate, and then use new found experience to re-baseline after a short period in order to adjust your estimates (up or down!) based on what you’ve learnt. It also demonstrates to a project manager that you are mindful of trying to reign in estimates to as much accuracy as possible, but also gives an opportunity to flag areas which turn out more complex that first thought. . Even better would be regular backlog grooming of as much of the backlog as is practical, re-visting estimates and updating them as you progress through your project and factor in lessons learnt as you go along. . The number one criticisms of developer estimation is that we never learn from our bad estimates, and so never improve our accuracy. In many ways, this is a false accusation because as we’ve seen no estimate is created equal and the things that trip us up this time might not arise in future tasks, however there is definite value in looking back and comparing your estimate with your actual, making note of the reasons you either missed or achieved your estimate. . If nothing else, this kind of retrospective helps you identify some more assumptions that you made without realising, and will give you a useful tool for future tasks. . Estimates are unfortunately a necessary part of being a developer, and being able to give confident, accurate estimates breeds confidence from the rest of your project team. . If at all possible, never estimate alone - make it part of a team activity. If you’re following good agile practices then you should be doing team planning at the start of each iteration. This can be a great opportuntity to try out the principles in this post, and learn from the things other people in your team consider for their estimates. . I hope these suggestions are useful, if you’ve got more tips to share please post in the comments. .  The Pragmatic Programmer  by Andrew Hunt and David Thomas - particularly Chapter 13: Estimating. ", "date": "2014-10-06T00:00:00+01:00"}
]