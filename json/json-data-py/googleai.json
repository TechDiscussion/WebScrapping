{
  "Google_AI": [
    {
      "title": "Presenting the iGibson Challenge on Interactive and Social Navigation",
      "date": "Wednesday, April 14, 2021",
      "abstract": "Presenting the iGibson Challenge on Interactive and Social Navigation\nComputer vision has significantly advanced over the past decade thanks to large-scale benchmarks, such as ImageNet for image classification or COCO for object detection, which provide vast datasets and criteria for evaluating models. However, these traditional benchmarks evaluate passive tasks in which the emphasis is on perception alone, whereas more recent computer vision research has tackled active tasks, which require both perception and action (often called \u201cembodied AI\u201d).ImageNetimage classificationCOCOobject detectionembodied AI\nThe First Embodied AI Workshop, co-organized by Google at CVPR 2020, hosted several benchmark challenges for active tasks, including the Stanford and Google organized Sim2Real Challenge with iGibson, which provided a real-world setup to test navigation policies trained in photo-realistic simulation environments. An open-source setup in the challenge enabled the community to train policies in simulation, which could then be run in repeatable real world navigation experiments, enabling the evaluation of the \u201csim-to-real gap\u201d \u2014 the difference between simulation and the real world. Many research teams submitted solutions during the pandemic, which were run safely by challenge organizers on real robots, with winners presenting their results virtually at the workshop.First Embodied AI WorkshopCVPR 2020Sim2Real Challenge with iGibsonwinners presenting their results\nThis year, Stanford and Google are proud to announce a new version of the iGibson Challenge on Interactive and Social Navigation, one of the 10 active visual challenges affiliated with the  Second Embodied AI Workshop at CVPR 2021. This year\u2019s Embodied AI Workshop is co-organized by Google and nine other research organizations, and explores issues such as simulation, sim-to-real transfer, visual navigation, semantic mapping and change detection, object rearrangement and restoration, auditory navigation, and following instructions for navigation and interaction tasks. In addition, this year\u2019s interactive and social iGibson challenge explores interactive navigation and social navigation \u2014 how robots can learn to interact with people and objects in their environments \u2014 by combining  the iGibson simulator, the Google Scanned Objects Dataset, and simulated pedestrians within realistic human environments.iGibson Challenge on Interactive and Social NavigationSecond Embodied AI WorkshopCVPR 2021visual navigationsemantic mapping and change detectionobject rearrangementrestorationauditory navigationnavigationinteractioniGibson simulatorGoogle Scanned Objects Datasetsimulated pedestrians within realistic human environments\nNew Challenges in Navigation\nActive perception tasks are challenging, as they require both perception and actions in response. For example, point navigation involves navigating through mapped space, such as driving robots over kilometers in human-friendly buildings, while recognizing and avoiding obstacles. Similarly object navigation involves looking for objects in buildings, requiring domain invariant representations and object search behaviors. Additionally, visual language instruction navigation involves navigating through buildings based on visual images and commands in natural language. These problems become even harder in a real-world environment, where robots must be able to handle a variety of physical and social interactions that are much more dynamic and challenging to solve. In this year\u2019s iGibson Challenge, we focus on two of those settings:point navigationrobots over kilometers in human-friendly buildingsobject navigationdomain invariant representationsobject search behaviorsvisual language instruction navigationnavigating through buildings based on visual images and commands in natural language\nNew Features of the iGibson 2021 Dataset\nTo facilitate research into techniques that address these problems, the iGibson Challenge 2021 dataset provides simulated interactive scenes for training. The dataset includes eight fully interactive scenes derived from real-world apartments, and another seven scenes held back for testing and evaluation.iGibson\nTo enable interactive navigation, these scenes are populated with small objects drawn from the Google Scanned Objects Dataset, a dataset of common household objects scanned in 3D for use in robot simulation and computer vision research, licensed under a Creative Commons license to give researchers the freedom to use them in their research.Google Scanned Objects DatasetGoogle Scanned Objects Dataset\nThe challenge is implemented in Stanford\u2019s open-source iGibson simulation platform, a fast, interactive, photorealistic robotic simulator with physics based on Bullet. For this year\u2019s challenge, iGibson has been expanded with fully interactive environments and pedestrian behaviors based on the ORCA crowd simulation algorithm.iGibson simulation platformBulletfully interactive environmentspedestrian behaviorsORCA crowd simulation algorithmORCA crowd simulations\nParticipating in the Challenge\nThe iGibson Challenge has launched and its leaderboard is open in the Dev phase, in which participants are encouraged to submit robotic control to the development leaderboard, where they will be tested on the Interactive and Social Navigation challenges on our holdout dataset. The Test phase opens for teams to submit final solutions on May 16th and closes on May 31st, with the winner demo scheduled for June 20th, 2021. For more details on participating, please check out the iGibson Challenge Page.leaderboardiGibson Challenge Page\nAcknowledgements\nWe\u2019d like to thank our colleagues at at the Stanford Vision and Learning Lab (SVL) for working with us to advance the state of interactive and social robot navigation, including Chengshu Li, Claudia P\u00e9rez D'Arpino, Fei Xia, Jaewoo Jang, Roberto Martin-Martin and Silvio Savarese. At Google, we would like to thank Aleksandra Faust, Anelia Angelova, Carolina Parada, Edward Lee, Jie Tan, Krista Reyman and the rest of our collaborators on mobile robotics. We would also like to thank our co-organizers on the Embodied AI Workshop, including AI2, Facebook, Georgia Tech, Intel, MIT, SFU, Stanford, UC Berkeley, and University of Washington.",
      "link": "http://ai.googleblog.com/2021/04/presenting-igibson-challenge-on.html",
      "author": "Posted by Anthony Francis, Software Engineer and Alexander Toshev, Staff Research Scientist, Google Research"
    },
    {
      "title": "Monster Mash: A Sketch-Based Tool for Casual 3D Modeling and Animation",
      "date": "Friday, April 9, 2021",
      "abstract": "Monster Mash: A Sketch-Based Tool for Casual 3D Modeling and Animation\n3D computer animation is a time-consuming and highly technical medium \u2014 to complete even a single animated scene requires numerous steps, like modeling, rigging and animating, each of which is itself a sub-discipline that can take years to master. Because of its complexity, 3D animation is generally practiced by teams of skilled specialists and is inaccessible to almost everyone else, despite decades of advances in technology and tools. With the recent development of tools that facilitate game character creation and game balance, a natural question arises: is it possible to democratize the 3D animation process so it\u2019s accessible to everyone?rigginggame character creationgame balance\nTo explore this concept, we start with the observation that most forms of artistic expression have a casual mode: a classical guitarist might jam without any written music, a trained actor could ad-lib a line or two while rehearsing, and an oil painter can jot down a quick gesture drawing. What these casual modes have in common is that they allow an artist to express a complete thought quickly and intuitively without fear of making a mistake. This turns out to be essential to the creative process \u2014 when each sketch is nearly effortless, it is possible to iteratively explore the space of possibilities far more effectively.creative process\nIn this post, we describe Monster Mash, an open source tool presented at SIGGRAPH Asia 2020 that allows experts and amateurs alike to create rich, expressive, deformable 3D models from scratch \u2014 and to animate them \u2014 all in a casual mode, without ever having to leave the 2D plane. With Monster Mash, the user sketches out a character, and the software automatically converts it to a soft, deformable 3D model that the user can immediately animate by grabbing parts of it and moving them around in real time. There is also an online demo, where you can try it out for yourself.Monster MashpresentedSIGGRAPH Asia 2020online demo\nCreating a 2D Sketch\nThe insight that makes this casual sketching approach possible is that many 3D models, particularly those of organic forms, can be described by an ordered set of overlapping 2D regions. This abstraction makes the complex task of 3D modeling much easier: the user creates 2D regions by drawing their outlines, then the algorithm creates a 3D model by stitching the regions together and inflating them. The result is a simple and intuitive user interface for sketching 3D figures.3D models\nFor example, suppose the user wants to create a 3D model of an elephant. The first step is to draw the body as a closed stroke (a). Then the user adds strokes to depict other body parts such as legs (b). Drawing those additional strokes as open curves provides a hint to the system that they are meant to be smoothly connected with the regions they overlap. The user can also specify that some new parts should go behind the existing ones by drawing them with the right mouse button (c), and mark other parts as symmetrical by double-clicking on them (d). The result is an ordered list of 2D regions.\nStitching and Inflation\nTo understand how a 3D model is created from these 2D regions, let\u2019s look more closely at one part of the elephant. First, the system identifies where the leg must be connected to the body (a) by finding the segment (red) that completes the open curve. The system cuts the body\u2019s front surface along that segment, and then stitches the front of the leg together with the body (b). It then inflates the model into 3D by solving a modified form of Poisson\u2019s equation to produce a surface with a rounded cross-section (c). The resulting model (d) is smooth and well-shaped, but because all of the 3D parts are rooted in the drawing plane, they may intersect each other, resulting in a somewhat odd-looking \u201celephant\u201d. These intersections will be resolved by the deformation system.Poisson\u2019s equation\nLayered Deformation\nAt this point we just have a static model \u2014 we need to give the user an easy way to pose the model, and also separate the intersecting parts somehow. Monster Mash\u2019s layered deformation system, based on the well-known smooth deformation method as-rigid-as-possible (ARAP), solves both of these problems at once. What\u2019s novel about our layered \u201cARAP-L\u201d approach is that it combines deformation and other constraints into a single optimization framework, allowing these processes to run in parallel at interactive speed, so that the user can manipulate the model in real time.as-rigid-as-possible\nThe framework incorporates a set of layering and equality constraints, which move body parts along the z axis to prevent them from visibly intersecting each other. These constraints are applied only at the silhouettes of overlapping parts, and are dynamically updated each frame.\nMeanwhile, in a separate thread of the framework, we satisfy point constraints to make the model follow user-defined control points (described in the section below) in the xy-plane. This ARAP-L method allows us to combine modeling, rigging, deformation, and animation all into a single process that is much more approachable to the non-specialist user.\nAnimation\nTo pose the model, the user can create control points anywhere on the model\u2019s surface and move them. The deformation system converges over multiple frames, which gives the model\u2019s movement a soft and floppy quality, allowing the user to intuitively grasp its dynamic properties \u2014 an essential prerequisite for kinesthetic learning.kinesthetic learning\nTo create animation, the system records the user\u2019s movements in real time. The user can animate one control point, then play back that movement while recording additional control points. In this way, the user can build up a complex action like a walk by layering animation, one body part at a time. At every stage of the animation process, the only task required of the user is to move points around in 2D, a low-risk workflow meant to encourage experimentation and play.\nConclusion\nWe believe this new way of creating animation is intuitive and can thus help democratize the field of computer animation, encouraging novices who would normally be unable to try it on their own as well as experts who often require fast iteration under tight deadlines. Here you can see a few of the animated characters that have been created using Monster Mash. Most of these were created in a matter of minutes.often require fast iteration under tight deadlines\nAll of the code for Monster Mash is available as open source, and you can watch our presentation and read our paper from SIGGRAPH Asia 2020 to learn more. We hope this software will make creating 3D animations more broadly accessible. Try out the online demo and see for yourself!open sourcepresentationour paperonline demo\nAcknowledgements\nMonster Mash is the result of a collaboration between Google Research, Czech Technical University in Prague, ETH Z\u00fcrich, and the University of Washington. Key contributors include Marek Dvoro\u017e\u0148\u00e1k, Daniel S\u00fdkora, Cassidy Curtis, Brian Curless, Olga Sorkine-Hornung, and David Salesin. We are also grateful to H\u00e9l\u00e8ne Leroux, Neth Nom, David Murphy, Samuel Leather, Pavla S\u00fdkorov\u00e1, and Jakub Javora for participating in the early interactive sessions.",
      "link": "http://ai.googleblog.com/2021/04/monster-mash-sketch-based-tool-for.html",
      "author": "Posted by Cassidy Curtis, Visual Designer and David Salesin, Principal Scientist, Google Research"
    },
    {
      "title": "Announcing the 2021 Research Scholar Program Recipients",
      "date": "Wednesday, April 7, 2021",
      "abstract": "Announcing the 2021 Research Scholar Program Recipients\nIn March 2020 we introduced the Research Scholar Program, an effort focused on developing collaborations with new professors and encouraging the formation of long-term relationships with the academic community. In November we opened the inaugural call for proposals for this program, which was received with enthusiastic interest from faculty who are working on cutting edge research across many research areas in computer science, including machine learning, human-computer interaction, health research, systems and more.we introducedResearch Scholar Program\nToday we are pleased to announce that in this first year of the program we have granted 77 awards, which included 86 principal investigators representing 15+ countries and over 50 universities. Of the 86 award recipients, 43% identify as an historically marginalized group within technology. Please  see the full list of 2021 recipients on our web page, as well as in the list below.historically marginalized group within technologyweb page\nWe offer our congratulations to this year\u2019s recipients, and look forward to seeing what they achieve!",
      "link": "http://ai.googleblog.com/2021/04/announcing-2021-research-scholar.html",
      "author": "Posted by Negar Saei, Program Manager, University Relations"
    },
    {
      "title": "Constructing Transformers For Longer Sequences with Sparse Attention Methods",
      "date": "Thursday, March 25, 2021",
      "abstract": "Constructing Transformers For Longer Sequences with Sparse Attention Methods\nNatural language processing (NLP) models based on Transformers, such as BERT, RoBERTa, T5, or GPT3, are successful for a wide variety of tasks and a mainstay of modern NLP research. The versatility and robustness of Transformers are the primary drivers behind their wide-scale adoption, leading them to be easily adapted for a diverse range of sequence-based tasks \u2014 as a seq2seq model for translation, summarization, generation, and others, or as a standalone encoder for sentiment analysis, POS tagging, machine reading comprehension, etc.  The key innovation in Transformers is the introduction of a self-attention mechanism, which computes similarity scores for all pairs of positions in an input sequence, and can be evaluated in parallel for each token of the input sequence, avoiding the sequential dependency of recurrent neural networks, and enabling Transformers to vastly outperform previous sequence models like LSTM.Natural language processingTransformersBERTRoBERTaT5GPT3seq2seqtranslationsummarizationgenerationsentiment analysisPOS taggingmachine reading comprehensionLSTM\nA limitation of existing Transformer models and their derivatives, however, is that the full self-attention mechanism has computational and memory requirements that are quadratic with the input sequence length. With commonly available current hardware and model sizes, this typically limits the input sequence to roughly 512 tokens, and prevents Transformers from being directly applicable to tasks that require larger context, like question answering, document summarization or genome fragment classification. Two natural questions arise: 1) Can we achieve the empirical benefits of quadratic full Transformers using sparse models with computational and memory requirements that scale linearly with the input sequence length? 2) Is it possible to show theoretically that these linear Transformers preserve the expressivity and flexibility of the quadratic full Transformers?self-attention mechanismquestion answeringdocument summarizationgenome fragment classification\nWe address both of these questions in a recent pair of papers. In \u201cETC: Encoding Long and Structured Inputs in Transformers\u201d, presented at EMNLP 2020, we present the Extended Transformer Construction (ETC), which is a novel method for sparse attention, in which one uses structural information to limit the number of computed pairs of similarity scores. This reduces the quadratic dependency on input length to linear and yields strong empirical results in the NLP domain. Then, in \u201cBig Bird: Transformers for Longer Sequences\u201d, presented at NeurIPS 2020, we introduce another sparse attention method, called  BigBird that extends ETC to more generic scenarios where prerequisite domain knowledge about structure present in the source data may be unavailable. Moreover, we also show that theoretically our proposed sparse attention mechanism preserves the expressivity and flexibility of the quadratic full Transformers. Our proposed methods achieve a new state of the art on challenging long-sequence tasks, including question answering, document summarization and genome fragment classification.ETC: Encoding Long and Structured Inputs in TransformersEMNLP 2020Big Bird: Transformers for Longer SequencesNeurIPS 2020question answeringdocument summarizationgenome fragment classification\nAttention as a Graph\nThe attention module used in Transformer models computes similarity scores for all pairs of positions in an input sequence. It is useful to think of the attention mechanism as a directed graph, with tokens represented by nodes and the similarity score computed between a pair of tokens represented by an edge. In this view, the full attention model is a complete graph. The core idea behind our approach is to carefully design sparse graphs, such that one only computes a linear number of similarity scores.graphcomplete graph\nExtended Transformer Construction (ETC)\nOn NLP tasks that require long and structured inputs, we propose a structured sparse attention mechanism, which we call Extended Transformer Construction (ETC). To achieve structured sparsification of self attention, we developed the global-local attention mechanism. Here the input to the Transformer is split into two parts: a global input where tokens have unrestricted attention, and a long input where tokens can only attend to either the global input or to a local neighborhood. This achieves linear scaling of attention, which allows ETC to significantly scale input length.Extended Transformer Construction\nIn order to further exploit the structure of long documents, ETC combines additional ideas: representing the positional information of the tokens in a relative way, rather than using their absolute position in the sequence; using an additional training objective beyond the usual masked language model (MLM) used in models like BERT; and flexible masking of tokens to control which tokens can attend to which other tokens. For example, given a long selection of text, a global token is applied to each sentence, which connects to all tokens within the sentence, and a global token is also applied to each paragraph, which connects to all tokens within the same paragraph.relative waymasked language model (MLM) used in models like BERT\nWith this approach, we report state-of-the-art results in five challenging NLP datasets requiring long or structured inputs: TriviaQA, Natural Questions (NQ), HotpotQA, WikiHop, and OpenKP.TriviaQANatural Questions (NQ)HotpotQAWikiHopOpenKPresult\nBigBird\nExtending the work of ETC,  we propose BigBird \u2014 a sparse attention mechanism that is also linear in the number of tokens and is a generic replacement for the attention mechanism used in Transformers. In contrast to ETC, BigBird doesn\u2019t require any prerequisite knowledge about structure present in the source data. Sparse attention in the BigBird model consists of three main parts:BigBirdWatts-Strogatz graph\nIn the BigBird paper, we explain why sparse attention is sufficient to approximate quadratic attention, partially explaining why ETC was successful. A crucial observation is that there is an inherent tension between how few similarity scores one computes and the flow of information between different nodes (i.e., the ability of one token to influence each other). Global tokens serve as a conduit for information flow and we prove that sparse attention mechanisms with global tokens can be as powerful as the full attention model. In particular, we show that BigBird is as expressive as the original Transformer, is computationally universal (following the work of Yun et al. and Perez et al.), and is a universal approximator of continuous functions. Furthermore, our proof suggests that the use of random graphs can further help ease the flow of information \u2014 motivating the use of the random attention component.BigBird papercomputationally universalYun et al.Perez et al.universal approximator\nThis design scales to much longer sequence lengths for both structured and unstructured tasks. Further scaling can be achieved by using gradient checkpointing by trading off training time for sequence length. This lets us extend our efficient sparse transformers to include generative tasks that require an encoder and a decoder, such as long document summarization, on which we achieve a new state of the art.gradient checkpointingROUGEBigPatentArXivMoreover, the fact that BigBird is a generic replacement also allows it to be extended to new domains without pre-existing domain knowledge. In particular, we introduce a novel application of Transformer-based models where long contexts are beneficial \u2014 extracting contextual representations of genomic sequences (DNA). With longer masked language model pre-training, BigBird achieves state-of-the-art performance on downstream tasks, such as promoter-region prediction and chromatin profile prediction.promoter-region predictionchromatin profile predictionchromatin-profile prediction including transcription factors (TF), histone-mark (HM) and  DNase I hypersensitive (DHS)\nMain Implementation Idea\nOne of the main impediments to the large scale adoption of sparse attention is the fact that sparse operations are quite inefficient in modern hardware. Behind both ETC and BigBird, one of our key innovations is to make an efficient implementation of the sparse attention mechanism. As modern hardware accelerators like GPUs and TPUs excel using coalesced memory operations, which load blocks of contiguous bytes at once, it is not efficient to have small sporadic look-ups caused by a sliding window (for local attention) or random element queries (random attention). Instead we transform the sparse local and random attention into dense tensor operations to take full advantage of modern single instruction, multiple data (SIMD) hardware.single instruction, multiple data\nTo do this, we first \u201cblockify\u201d the attention mechanism to better leverage GPUs/TPUs, which are designed to operate on blocks. Then we convert the sparse attention mechanism computation into a dense tensor product through a series of simple matrix operations such as reshape, roll, and gather, as illustrated in the animation below.\nRecently, \u201cLong Range Arena: A Benchmark for Efficient Transformers\u201c provided a benchmark of six tasks that require longer context, and performed experiments to benchmark all existing long range transformers. The results show that the BigBird model, unlike its counterparts, clearly reduces memory consumption without sacrificing performance.Long Range Arena: A Benchmark for Efficient TransformersThe results\nConclusion\nWe show that carefully designed sparse attention can be as expressive and flexible as the original full attention model. Along with theoretical guarantees, we provide a very efficient implementation which allows us to scale to much longer inputs. As a consequence, we achieve state-of-the-art results for question answering, document summarization and genome fragment classification. Given the generic nature of our sparse attention, the approach should be applicable to many other tasks like program synthesis and long form open domain question answering. We have open sourced the code for both ETC (github) and BigBird (github), both of which run efficiently for long sequences on both GPUs and TPUs.program synthesislong form open domain question answeringETC (github)BigBird (github)\nAcknowledgements\nThis research resulted as a collaboration with Amr Ahmed, Joshua Ainslie, Chris Alberti, Vaclav Cvicek, Avinava Dubey, Zachary Fisher, Guru Guruganesh, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang, Manzil Zaheer, who co-authored EMNLP and NeurIPS papers.",
      "link": "http://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html",
      "author": "Posted by Avinava Dubey, Research Scientist, Google Research"
    },
    {
      "title": "Recursive Classification: Replacing Rewards with Examples in RL",
      "date": "Wednesday, March 24, 2021",
      "abstract": "Recursive Classification: Replacing Rewards with Examples in RL\nA general goal of robotics research is to design systems that can assist in a variety of tasks that can potentially improve daily life. Most reinforcement learning algorithms for teaching agents to perform new tasks require a reward function, which provides positive feedback to the agent for taking actions that lead to good outcomes. However, actually specifying these reward functions can be quite tedious and can be very difficult to define for situations without a clear objective, such as whether a room is clean or if a door is sufficiently shut. Even for tasks that are easy to describe, actually measuring whether the task has been solved can be difficult and may require adding many sensors to a robot's environment.reinforcement learningadding many sensors\nAlternatively, training a model using examples, called example-based control, has the potential to overcome the limitations of approaches that rely on traditional reward functions. This new problem statement is most similar to prior methods based on \"success detectors\", and efficient algorithms for example-based control could enable non-expert users to teach robots to perform new tasks, without the need for coding expertise, knowledge of reward function design, or the installation of environmental sensors.priormethods\nIn \"Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classification,\" we propose a machine learning algorithm for teaching agents how to solve new tasks by providing examples of success (e.g., if \u201csuccess\u201d examples show a nail embedded into a wall, the agent  will learn to pick up a hammer and knock nails into the wall). This algorithm, recursive classification of examples (RCE), does not rely on hand-crafted reward functions, distance functions, or features, but rather learns to solve tasks directly from data, requiring the agent to learn how to solve the entire task by itself, without requiring examples of any intermediate states. Using a version of temporal difference learning \u2014 similar to Q-learning, but replacing the typical reward function term using only examples of success \u2014 RCE outperforms prior approaches based on imitation learning on simulated robotics tasks.  Coupled with theoretical guarantees similar to those for reward-based learning, the proposed method offers a user-friendly alternative for teaching robots new tasks.Replacing Rewards with Examples: Example-Based Policy Search via Recursive Classificationtemporal difference learningQ-learning\nExample-Based Control vs Imitation Learning\nWhile the example-based control method is similar to imitation learning, there is an important distinction \u2014 it does not require expert demonstrations. In fact, the user can actually be quite bad at performing the task themselves, as long as they can look back and pick out the small fraction of states where they did happen to solve the task.imitation learningexpertdemonstrations\nAdditionally, whereas previous research used a stage-wise approach in which the model first uses success examples to learn a reward function and then applies that reward function with an off-the-shelf reinforcement learning algorithm, RCE learns directly from the examples and skips the intermediate step of defining the reward function. Doing so avoids potential bugs and bypasses the process of defining the hyperparameters associated with learning a reward function (such as how often to update the reward function or how to regularize it) and, when debugging, removes the need to examine code related to learning the reward function.previousresearchhyperparameters associated with learning a reward functionhow often to update the reward function or how to regularize it\nRecursive Classification of Examples\nThe intuition behind the RCE approach is simple: the model should predict whether the agent will solve the task in the future, given the current state of the world and the action that the agent is taking. If there were data that specified which state-action pairs lead to future success and which state-action pairs lead to future failure, then one could solve this problem using standard supervised learning. However, when the only data available consists of success examples, the system doesn\u2019t know which states and actions led to success, and while the system also has experience interacting with the environment, this experience isn't labeled as leading to success or not.supervised learningsupervised learning\nNonetheless, one can piece together what these data would look like, if it were available. First, by definition, a successful example must be one that solves the given task. Second, even though it is unknown whether an arbitrary state-action pair will lead to success in solving a task, it is possible to estimate how likely it is that the task will be solved if the agent started at the next state. If the next state is likely to lead to future success, it can be assumed that the current state is also likely to lead to future success. In effect, this is recursive classification, where the labels are inferred based on predictions at the next time step.\nThe underlying algorithmic idea of using a model's predictions at a future time step as a label for the current time step closely resembles existing temporal-difference methods, such as Q-learning and successor features. The key difference is that the approach described here does not require a reward function. Nonetheless, we show that this method inherits many of the same theoretical convergence guarantees as temporal difference methods. In practice, implementing RCE requires changing only a few lines of code in an existing Q-learning implementation.Q-learningsuccessor featuresconvergenceguarantees\nEvaluation\nWe evaluated the RCE method on a range of challenging robotic manipulation tasks. For example, in one task we required a robotic hand to pick up a hammer and hit a nail into a board. Previous research into this task [1, 2] have used a complex reward function (with terms corresponding to the distance between the hand and the hammer, the distance between the hammer and the nail, and whether the nail has been knocked into the board). In contrast, the RCE method requires only a few observations of what the world would look like if the nail were hammered into the board.12\nWe compared the performance of RCE to a number of prior methods, including those that learn an explicit reward function and those based on imitation learning , all of which struggle to solve this task. This experiment highlights how example-based control makes it easy for users to specify even complex tasks, and demonstrates that recursive classification can successfully solve these sorts of tasks.SQILDACVICEORILPURL\nConclusion\nWe have presented a method to teach autonomous agents to perform tasks by providing them with examples of success, rather than meticulously designing reward functions or collecting first-person demonstrations. An important aspect of example-based control, which we discuss in the paper, is what assumptions the system makes about the capabilities of different users. Designing variants of RCE that are robust to differences in users' capabilities may be important for applications in real-world robotics. The code is available, and the project website contains additional videos of the learned behaviors.designing reward functionsfirst-person demonstrationscode is availableproject website\nAcknowledgements\nWe thank our co-authors, Ruslan Salakhutdinov and Sergey Levine. We also thank Surya Bhupatiraju, Kamyar Ghasemipour, Max Igl, and Harini Kannan for feedback on this post, and Tom Small for helping to design figures for this post.",
      "link": "http://ai.googleblog.com/2021/03/recursive-classification-replacing.html",
      "author": "Posted by Benjamin Eysenbach, Student Researcher, Google Research"
    },
    {
      "title": "Progress and Challenges in Long-Form Open-Domain Question Answering",
      "date": "Tuesday, March 23, 2021",
      "abstract": "Progress and Challenges in Long-Form Open-Domain Question Answering\nOpen-domain long-form question answering (LFQA) is a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given question and using them to generate an elaborate paragraph-length answer. While there has been remarkable recent progress in factoid open-domain question answering (QA), where a short phrase or entity is enough to answer a question, much less work has been done in the area of long-form question answering. LFQA is nevertheless an important task, especially because it provides a testbed to measure the factuality of generative text models. But, are current benchmarks and evaluation metrics really suitable for making progress on LFQA?Open-domain long-form question answering\nIn \u201cHurdles to Progress in Long-form Question Answering\u201d (to appear at NAACL 2021), we present a new system for open-domain long-form question answering that leverages two recent advances in NLP: 1) state-of-the-art sparse attention models, such as Routing Transformer (RT), which allow attention-based models to scale to long sequences, and 2) retrieval-based models, such as REALM, which facilitate retrievals of Wikipedia articles related to a given query. To encourage more factual grounding, our system combines information from several retrieved Wikipedia articles related to the given question before generating an answer. It achieves a new state of the art on ELI5, the only large-scale publicly available dataset for long-form question answering.Hurdles to Progress in Long-form Question AnsweringNAACL 2021Routing TransformerREALMWikipediaELI5\nHowever, while our system tops the public leaderboard, we discover several troubling trends with the ELI5 dataset and its associated evaluation metrics. In particular, we find 1) little evidence that models actually use the retrievals on which they condition; 2) that trivial baselines (e.g., input copying) beat modern systems, like RAG / BART+DPR; and 3) that there is a significant train/validation overlap in the dataset. Our paper suggests mitigation strategies for each of these issues.input copyingRAGBARTDPR\nText Generation\nThe main workhorse of NLP models is the Transformer architecture, in which each token in a sequence attends to every other token in a sequence, resulting in a model that scales quadratically with sequence length. The RT model introduces a dynamic, content-based sparse attention mechanism that reduces the complexity of attention in the Transformer model from n2 to n1.5, where n is the sequence length, which enables it to scale to long sequences. This allows each word to attend to other relevant words anywhere in the entire piece of text, unlike methods such as Transformer-XL where a word can only attend to words in its immediate vicinity.TransformerattendsTransformer-XL\nThe key insight of the RT work is that each token attending to every other token is often redundant, and may be approximated by a combination of local and global attention. Local attention allows each token to build up a local representation over several layers of the model, where each token attends to a local neighborhood, facilitating local consistency and fluency. Complementing local attention, the RT model also uses mini-batch k-means clustering to enable each token to attend only to a set of most relevant tokens.k-means clustering\nWe pre-train an RT model on the Project Gutenberg (PG-19) data-set with a language modeling objective, i.e, the model learns to predict the next word given all the previous words, so as to be able to generate fluent paragraph long text.Project Gutenberg (PG-19)\nInformation Retrieval\nTo demonstrate the effectiveness of the RT model on the task of LFQA, we combine it with retrievals from REALM. The REALM model (Guu et al. 2020) is a retrieval-based model that uses the maximum inner product search to retrieve Wikipedia articles relevant to a particular query or question. The model was fine-tuned for factoid-based question answering on the Natural Questions dataset. REALM utilizes the BERT model to learn good representations for a question and uses SCANN to retrieve Wikipedia articles that have a high topical similarity with the question representation. This is then trained end-to-end to maximize the log-likelihood on the QA task.REALMGuu et al. 2020maximum inner product searchNatural QuestionsBERT modelSCANN\nWe further improve the quality of REALM retrievals by using a contrastive loss. The idea behind this is to encourage the representation of a question to get close to its ground truth answer and diverge from the other answers in its mini-batch. This ensures that when the system retrieves relevant items using this question representation, it returns articles that are \"similar\" to ground truth answers. We call this retriever contrastive-REALM or c-REALM.contrastive loss\nEvaluation\nWe test the model on long-form question answering using the ELI5 dataset, which is a part of the KILT benchmark, and is the only publicly available large-scale LFQA dataset. The KILT benchmark measures text retrievals using Precision (R-Prec) and text generation using ROUGE-L. The two scores are combined to give a KILT R-L score, which determines a model\u2019s ranking on the leaderboard. We fine-tune the pre-trained RT model together with retrievals from c-REALM on the ELI5 dataset from KILT.ELI5 datasetKILT benchmarkPrecisionROUGE-L\nOur submission tops the KILT leaderboard for long-form question answering on ELI5 with a combined KILT R-L score of 2.36. It improves on the previous leaderboard entry of BART + DPR (KILT R-L score of 1.9), while having a similar number of parameters as the other models on the leaderboard. In terms of text generation quality, we see an improvement of +4.11, +5.78 and +9.14 Rouge-L over T5, BART + DPR and RAG, respectively.leaderboardT5\nExample Generations from the RT Model\n.fill{color:#4184F3;font-style:italic}.monospace{font-family:'Roboto Mono',monospace}.tabs{width:100%!important;margin:0!important;padding:0!important;list-style:none;position:relative;height:540px!important;margin-bottom:24px!important}.tabs li{display:inline-block;margin:0!important;padding:0!important;line-height:1!important}.tabs input[type=\"radio\"]{position:absolute;top:0;left:0;opacity:0}.tabs label{display:block;padding:8px;width:50px;cursor:pointer;border-bottom:2px solid white;text-align:center;transition:.2s all ease-in-out}.tabs [id^=\"tab\"]:checked+label{border-bottom:2px solid #4184F3;background:#e3edfd;color:#4184F3;font-weight:bold}.tabs label:hover{color:#4184F3;border-bottom:2px solid #4184F3;xbackground:#e6e9ff}.tabs .tab-content{z-index:2;display:none;overflow:auto;width:100%;height:500px;margin-top:0;padding:8px;position:absolute;left:0;line-height:1.5;border:1px solid #4184F3;border-top-color:#e6e9ff;border-left-color:#e6e9ff}.tabs .tab-content>p{margin-top:0}.tabs [id^=\"tab\"]:checked ~ .tab-content{display:block;box-sizing:border-box;}@media only screen and (max-width:535px){.tabs .tab-content{margin-top:40px!important}.tabs{height:570px!important}}@media only screen and (max-width:570px){#content7,#content6{margin-top:0!important}}@media only screen and (max-width:460px){#content5{margin-top:0!important}}@media only screen and (max-width:400px){#content7,#content6,#content5{margin-top:0!important}}@media only screen and (max-width:360px){#content4{margin-top:0!important}}\nHurdles Towards Progress in LFQA\nHowever, while the RT system described here tops the public leaderboard, a detailed analysis of the model and the ELI5 dataset reveal some concerning trends.\n.tabs2{width:100%!important;margin:0!important;padding:0!important;list-style:none;position:relative;height:590px!important;margin-bottom:24px!important}.tabs2 li{display:inline-block;margin:0!important;padding:0!important;line-height:1!important}.tabs2 input[type=\"radio\"]{position:absolute;top:0;left:0;opacity:0}.tabs2 label{display:block;padding:8px;width:190px;cursor:pointer;border-bottom:2px solid white;text-align:center;transition:.2s all ease-in-out}.tabs2 [id^=\"tab\"]:checked+label{border-bottom:2px solid #4184F3;background:#e3edfd;color:#4184F3;font-weight:bold}.tabs2 label:hover{color:#4184F3;border-bottom:2px solid #4184F3;xbackground:#e6e9ff}.tabs2 .tab-content2{z-index:2;display:none;overflow:auto;width:100%;height:560px;margin-top:0;padding:8px;position:absolute;left:0;line-height:1.5;border:1px solid #4184F3;border-top-color:#e6e9ff;border-left-color:#e6e9ff}.tabs2 .tab-content2>p{margin-top:0}.tabs2 [id^=\"tab0\"]:checked ~ .tab-content2{display:block;box-sizing:border-box;}@media only screen and (max-width:653px){.tabs2 .tab-content2{margin-top:40px!important}.tabs2{height:630px!important}}@media only screen and (max-width:653px){#content02{margin-top:0!important}}@media only screen and (max-width:460px){#content5{margin-top:0!important}}@media only screen and (max-width:400px){#content7,#content6,#content5{margin-top:0!important}}@media only screen and (max-width:360px){#content4{margin-top:0!important}}\nWe find little to no evidence that the model is actually grounding its text generation in the retrieved documents \u2014 fine-tuning an RT model with random retrievals from Wikipedia (i.e., random retrieval + RT) performs nearly as well as the c-REALM + RT model (24.2 vs 24.4 ROUGE-L). We also find significant overlap in the training, validation and test sets of ELI5 (with several questions being paraphrases of each other), which may eliminate the need for retrievals. The KILT benchmark measures the quality of retrievals and generations separately, without making sure that the text generation actually use the retrievals.\nMoreover, we find issues with the Rouge-L metric used to evaluate the quality of text generation, with trivial nonsensical baselines, such as a Random Training Set answer and Input Copying, achieving relatively high Rouge-L scores (even beating BART + DPR and RAG).Random Training Set answerInput Copying\nConclusion\nWe proposed a system for long form-question answering based on Routing Transformers and REALM, which tops the KILT leaderboard on ELI5. However, a detailed analysis reveals several issues with the benchmark that preclude using it to inform meaningful modelling advances. We hope that the community works together to solve these issues so that researchers can climb the right hills and make meaningful progress in this challenging but important task.\nAcknowledgments\nThe Routing Transformer work has been a team effort involving Aurko Roy, Mohammad Saffar, Ashish Vaswani and David Grangier. The follow-up work on open-domain long-form question answering has been a collaboration involving Kalpesh Krishna, Aurko Roy and Mohit Iyyer. We wish to thank Vidhisha Balachandran, Niki Parmar and Ashish Vaswani for several helpful discussions, and the REALM team (Kenton Lee, Kelvin Guu, Ming-Wei Chang and Zora Tung) for help with their codebase and several useful discussions, which helped us improve our experiments. We are grateful to Tu Vu for help with the QQP classifier used to detect paraphrases in ELI5 train and test sets. We thank Jules Gagnon-Marchand and Sewon Min for suggesting useful experiments on checking ROUGE-L bounds. Finally we thank Shufan Wang, Andrew Drozdov, Nader Akoury and the rest of the UMass NLP group for helpful discussions and suggestions at various stages in the project.QQPUMass NLP group",
      "link": "http://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html",
      "author": "Posted by Aurko Roy, Research Scientist, Google Research"
    },
    {
      "title": "Leveraging Machine Learning for Game Development",
      "date": "Friday, March 19, 2021",
      "abstract": "Leveraging Machine Learning for Game Development\nOver the years, online multiplayer games have exploded in popularity, captivating millions of players across the world. This popularity has also exponentially increased demands on game designers, as players expect games to be well-crafted and balanced \u2014 after all, it's no fun to play a game where a single strategy beats all the rest.\nIn order to create a positive gameplay experience, game designers typically tune the balance of a game iteratively:\nThis process is not only time-consuming but also imperfect \u2014 the more complex the game, the easier it is for subtle flaws to slip through the cracks. When games often have many different roles that can be played, with dozens of interconnecting skills, it makes it all the more difficult to hit the right balance.\nToday, we present an approach that leverages machine learning (ML) to adjust game balance by training models to serve as play-testers, and demonstrate this approach on the digital card game prototype Chimera, which we\u2019ve previously shown as a testbed for ML-generated art. By running millions of simulations using trained agents to collect data, this ML-based game testing approach enables game designers to more efficiently make a game more fun, balanced, and aligned with their original vision.ChimeraML-generated art\nChimera\nWe developed Chimera as a game prototype that would heavily lean on machine learning during its development process. For the game itself, we purposefully designed the rules to expand the possibility space, making it difficult to build a traditional hand-crafted AI to play the game.\nThe gameplay of Chimera revolves around the titular chimeras, creature mash-ups that players aim to strengthen and evolve. The objective of the game is to defeat the opponent's chimera. These are the key points in the game design:chimeras\nLearning to Play Chimera\nAs an imperfect information card game with a large state space, we expected Chimera to be a difficult game for an ML model to learn, especially as we were aiming for a relatively simple model. We used an approach inspired by those used by earlier game-playing agents like AlphaGo, in which a convolutional neural network (CNN) is trained to predict the probability of a win when given an arbitrary game state. After training an initial model on games where random moves were chosen, we set the agent to play against itself, iteratively collecting game data, that was then used to train a new agent. With each iteration, the quality of the training data improved, as did the agent\u2019s ability to play the game.imperfect informationAlphaGoconvolutional neural network\nFor the actual game state representation that the model would receive as input, we found that passing an \"image\" encoding to the CNN resulted in the best performance, beating all benchmark procedural agents and other types of networks (e.g. fully connected). The chosen model architecture is small enough to run on a CPU in reasonable time, which allowed us to download the model weights and run the agent live in a Chimera game client using Unity Barracuda.Unity Barracuda\nBalancing Chimera \nThis approach enabled us to simulate millions more games than real players would be capable of playing in the same time span. After collecting data from the games played by the best-performing agents, we analyzed the results to find imbalances between the two of the player decks we had designed.\nFirst, the Evasion Link Gen deck was composed of spells and creatures with abilities that generated extra link energy used to evolve a player\u2019s chimera. It also contained spells that enabled creatures to evade attacks. In contrast, the Damage-Heal deck contained creatures of variable strength with spells that focused on healing and inflicting minor damage. Although we had designed these decks to be of equal strength, the Evasion Link Gen deck was winning 60% of the time when played against the Damage-Heal deck.\nWhen we collected various stats related to biomes, creatures, spells, and chimera evolutions, two things immediately jumped out at us:\nFrom these insights, we made some adjustments to the game. To emphasize chimera evolution as a core mechanism in the game, we decreased the amount of link energy required to evolve a chimera from 3 to 1. We also added a \u201ccool-off\u201d period to the T-Rex creature, doubling the time it took to recover from any of its actions.\nRepeating our \u2018self-play\u2019 training procedure with the updated rules, we observed that these changes pushed the game in the desired direction \u2014 the average number of evolves per game increased, and the T-Rex's dominance faded.\nBy weakening the T-Rex, we successfully reduced the Evasion Link Gen deck's reliance on an overpowered creature. Even so, the win ratio between the decks remained at 60/40 rather than 50/50. A closer look at the individual game logs revealed that the gameplay was often less strategic than we would have liked. Searching through our gathered data again, we found several more areas to introduce changes in.\nTo start, we increased the starting health of both players as well as the amount of health that healing spells could replenish. This was to encourage longer games that would allow a more diverse set of strategies to flourish. In particular, this enabled the Damage-Heal deck to survive long enough to take advantage of its healing strategy. To encourage proper summoning and strategic biome placement, we increased the existing penalties on playing creatures into incorrect or overcrowded biomes. And finally, we decreased the gap between the strongest and weakest creatures through minor attribute adjustments.\nNew adjustments in place, we arrived at the final game balance stats for these two decks:\nConclusion\nNormally, identifying imbalances in a newly prototyped game can take months of playtesting. With this approach, we were able to not only discover potential imbalances but also introduce tweaks to mitigate them in a span of days. We found that a relatively simple neural network was sufficient to reach high level performance against humans and traditional game AI. These agents could be leveraged in further ways, such as for coaching new players or discovering unexpected strategies. We hope this work will inspire more exploration in the possibilities of machine learning for game development.\nAcknowledgements\nThis project was conducted in collaboration with many people. Thanks to Ryan Poplin, Maxwell Hannaman, Taylor Steil, Adam Prins, Michal Todorovic, Xuefan Zhou, Aaron Cammarata, Andeep Toor, Trung Le, Erin Hoffman-John, and Colin Boswell. Thanks to everyone who contributed through playtesting, advising on game design, and giving valuable feedback.",
      "link": "http://ai.googleblog.com/2021/03/leveraging-machine-learning-for-game.html",
      "author": "Posted by Ji Hun Kim and Richard Wu, Software Engineers, Stadia"
    },
    {
      "title": "Massively Parallel Graph Computation: From Theory to Practice",
      "date": "Thursday, March 18, 2021",
      "abstract": "Massively Parallel Graph Computation: From Theory to Practice\nGraphs are useful theoretical representations of the connections between groups of entities, and have been used for a variety of purposes in data science, from ranking web pages by popularity and mapping out social networks, to assisting with navigation. In many cases, such applications require the processing of graphs containing hundreds of billions of edges, which is far too large to be processed on a single consumer-grade machine. A typical approach to scaling graph algorithms is to run in a distributed setting, i.e., to partition the data (and the algorithm) among multiple computers to perform the computation in parallel. While this approach allows one to process graphs with trillions of edges, it also introduces new challenges. Namely, because each computer only sees a small piece of the input graph at a time, one needs to handle inter-machine communication and design algorithms that can be split across multiple computers.Graphsranking web pagesmapping out social networksassisting with navigationhundreds of billions of edgesdistributed\nA framework for implementing distributed algorithms, MapReduce, was introduced in 2008. It transparently handled communication between machines while offering good fault-tolerance capabilities and inspired the development of a number of distributed computation frameworks, including Pregel, Apache Hadoop, and many others. Still, the challenge of developing algorithms for distributed computation on very large graphs remained, and designing efficient algorithms in this context even for basic problems, such as connected components, maximum matching or shortest paths, has been an active area of research. While recent work has demonstrated new algorithms for many problems, including our algorithms for connected components (both in theory and practice) and hierarchical clustering, there was still a need for methods that could solve a range of problems more quickly.MapReducePregelApache Hadoopconnected componentsmaximum matchingshortest pathstheorypracticehierarchical clustering\nToday we present a pair of recent papers that address this problem by first constructing a theoretical model for distributed graph algorithms and then demonstrating how the model can be applied. The proposed model, Adaptive Massively Parallel Computation (AMPC), augments the theoretical capabilities of MapReduce, providing a pathway to solve many graph problems in fewer computation rounds. We also show how the AMPC model can be effectively implemented in practice. The suite of algorithms we describe, which includes algorithms for maximal independent set, maximum matching, connected components and minimum spanning tree, work up to 7x faster than current state-of-the-art approaches.Adaptive Massively Parallel Computationeffectively implemented in practicemaximal independent setmaximum matchingconnected componentsminimum spanning tree,\nLimitations of MapReduce\nIn order to understand the limitations of MapReduce for developing graph algorithms, consider a simplified variant of the connected components problem. The input is a collection of rooted trees, and the goal is to compute, for each node, the root of its tree. Even this seemingly simple problem is not easy to solve in MapReduce. In fact, in the Massively Parallel Computation (MPC) model \u2014 the theoretical model behind MapReduce, Pregel, Apache Giraph and many other distributed computation frameworks \u2014 this problem is widely believed to require at least a number of rounds of computation proportional to log n, where n is the total number of nodes in the graph. While log n may not seem to be a large number, algorithms processing trillion-edge graphs often write hundreds of terabytes of data to disk in each round, and thus even a small reduction in the number of rounds may bring significant resource savings.connected components problemMassively Parallel ComputationApache Giraphwidely believed\nA similar subproblem showed up in our algorithms for finding connected components and computing a hierarchical clustering. We observed that one can bypass the limitations of MapReduce by implementing these algorithms through the use of a distributed hash table (DHT), a service that is initialized with a collection of key-value pairs and then returns a value associated with a provided key in real-time. In our implementation, for each node, the DHT stores its parent node. Then, a machine that processes a graph node can use the DHT and \u201cwalk up\u201d the tree until it reaches the root. While the use of a DHT worked well for this particular problem (although it relied on the input trees being not too deep), it was unclear if the idea could be applied more broadly.connected componentshierarchical clusteringdistributed hash table\nThe Adaptive Massively Parallel Computation Model\nTo extend this approach to other problems, we started by developing a model to theoretically analyze algorithms that utilize a DHT. The resulting AMPC model builds upon the well-established MPC model and formally describes the capabilities brought by the use of a distributed hash table.\nIn the MPC model there is a collection of machines, which communicate via message passing in synchronous rounds. Messages sent in one round are delivered in the beginning of the following round and constitute that round\u2019s entire input (i.e., the machines do not retain information from one round to the next). In the first round, one can assume that the input is randomly distributed across the machines. The goal is to minimize the number of computation rounds, while assuring load-balancing between machines in each round.\nWe then formalized the AMPC model by introducing a new approach, in which machines write to a write-only distributed hash table each round, instead of communicating via messages. Once a new round starts, the hash table from the previous round becomes read-only and a new write-only output hash table becomes available. What is important is that only the method of communication changes \u2014 the amount of communication and available space per machine is constrained exactly in the same way as in the MPC model. Hence, at a high level the added capability of the AMPC model is that each machine can choose what data to read, instead of being provided a piece of data.\nAlgorithms and Empirical Evaluation\nThis seemingly small difference in the way machines communicate allowed us to design much faster algorithms to a number of basic graph problems. In particular, we show that it is possible to find connected components, minimum spanning tree, maximal matching and maximal independent set in a constant number of rounds, regardless of the size of the graph.connected components, minimum spanning tree, maximal matchingmaximal independent set\nTo investigate the practical applicability of the AMPC algorithms, we have instantiated the model by combining Flume C++ (a C++ counterpart of FlumeJava) with a DHT communication layer. We have evaluated our AMPC algorithms for minimum spanning tree, maximal independent set and maximum matching and observed that we can achieve up to 7x speedups over implementations that did not use a DHT. At the same time, the AMPC implementations used 10x fewer rounds on average to complete, and also wrote less data to disk.FlumeJavaevaluated our AMPC algorithms\nOur implementation of the AMPC model took advantage of hardware-accelerated remote direct memory access (RDMA), a technology that allows reading from the memory of a remote machine with a latency of a few microseconds, which is just an order of magnitude slower than reading from local memory. While some of the AMPC algorithms communicated more data than their MPC counterparts, they were overall faster, as they performed mostly fast reads using RDMA, instead of costly writes to disk.remote direct memory access\nConclusion\nWith the AMPC model, we built a theoretical framework inspired by practically efficient implementations, and then developed new theoretical algorithms that delivered good empirical performance and maintained good fault-tolerance properties. We've been happy to see that the AMPC model has already been the subject of further study and are excited to learn what other problems can be solved more efficiently using the AMPC model or its practical implementations.the subject of further study\nAcknowledgements\nCo-authors on the two papers covered in this blog post include Soheil Behnezhad, Laxman Dhulipala, Hossein Esfandiari, and Warren Schudy. We also thank members of the Graph Mining team for their collaborations, and especially Mohammad Hossein Bateni for his input on this post. To learn more about our recent work on scalable graph algorithms, see videos from our recent Graph Mining and Learning workshop.Graph Mining teamGraph Mining and Learning workshop",
      "link": "http://ai.googleblog.com/2021/03/massively-parallel-graph-computation.html",
      "author": "Posted by Jakub \u0141\u0105cki and Vahab Mirrokni, Research Scientists, Google Research"
    },
    {
      "title": "Contactless Sleep Sensing in Nest Hub",
      "date": "Tuesday, March 16, 2021",
      "abstract": "Contactless Sleep Sensing in Nest Hub\nPeople often turn to technology to manage their health and wellbeing, whether it is to record their daily exercise, measure their heart rate, or increasingly, to understand their sleep patterns. Sleep is foundational to a person\u2019s everyday wellbeing and can be impacted by (and in turn, have an impact on) other aspects of one\u2019s life \u2014 mood, energy, diet, productivity, and more.measure their heart rate,\nAs part of our ongoing efforts to support people\u2019s health and happiness, today we announced Sleep Sensing in the new Nest Hub, which uses radar-based sleep tracking in addition to an algorithm for cough and snore detection. While not intended for medical purposes1, Sleep Sensing is an opt-in feature that can help users better understand their nighttime wellness using a contactless bedside setup. Here we describe the technologies behind Sleep Sensing and discuss how we leverage on-device signal processing to enable sleep monitoring (comparable to other clinical- and consumer-grade devices) in a way that protects user privacy.Sleep Sensing1\nSoli for Sleep Tracking\nSleep Sensing in Nest Hub demonstrates the first wellness application of Soli, a miniature radar sensor that can be used for gesture sensing at various scales, from a finger tap to movements of a person\u2019s body. In Pixel 4, Soli powers Motion Sense, enabling touchless interactions with the phone to skip songs, snooze alarms, and silence phone calls. We extended this technology and developed an embedded Soli-based algorithm that could be implemented in Nest Hub for sleep tracking.SoliMotion Senseenabling touchless interactions\nSoli consists of a millimeter-wave frequency-modulated continuous wave (FMCW) radar transceiver that emits an ultra-low power radio wave and measures the reflected signal from the scene of interest. The frequency spectrum of the reflected signal contains an aggregate representation of the distance and velocity of objects within the scene. This signal can be processed to isolate a specified range of interest, such as a user\u2019s sleeping area, and to detect and characterize a wide range of motions within this region, ranging from large body movements to sub-centimeter respiration.Solifrequency-modulated continuous waveradar\nIn order to make use of this signal for Sleep Sensing, it was necessary to design an algorithm that could determine whether a person is present in the specified sleeping area and, if so, whether the person is asleep or awake. We designed a custom machine-learning (ML) model to efficiently process a continuous stream of 3D radar tensors (summarizing activity over a range of distances, frequencies, and time) and automatically classify each feature into one of three possible states: absent, awake, and asleep.\nTo train and evaluate the model, we recorded more than a million hours of radar data from thousands of individuals, along with thousands of sleep diaries, reference sensor recordings, and external annotations. We then leveraged the TensorFlow Extended framework to construct a training pipeline to process this data and produce an efficient TensorFlow Lite embedded model. In addition, we created an automatic calibration algorithm that runs during setup to configure the part of the scene on which the classifier will focus. This ensures that the algorithm ignores motion from a person on the other side of the bed or from other areas of the room, such as ceiling fans and swaying curtains.TensorFlow ExtendedTensorFlow Lite\n  To validate the accuracy of the algorithm, we compared it to the gold-standard of sleep-wake determination, the polysomnogram sleep study, in a cohort of 33 \u201chealthy sleepers\u201d (those without significant sleep issues, like sleep apnea or insomnia) across a broad age range (19-78 years of age). Sleep studies are typically conducted in clinical and research laboratories in order to collect various body signals (brain waves, muscle activity, respiratory and heart rate measurements, body movement and position, and snoring), which can then be interpreted by trained sleep experts to determine stages of sleep and identify relevant events. To account for variability in how different scorers apply the American Academy of Sleep Medicine\u2019s staging and scoring rules, our study used two board-certified sleep technologists to independently annotate each night of sleep and establish a definitive groundtruth.polysomnogram sleep studysleep apneainsomniaAmerican Academy of Sleep Medicine\u2019s\nWe compared our Sleep Sensing algorithm\u2019s outputs to the corresponding groundtruth sleep and wake labels for every 30-second epoch of time to compute standard performance metrics (e.g., sensitivity and specificity). While not a true head-to-head comparison, this study\u2019s results can be compared against previously published studies in similar cohorts with comparable methodologies in order to get a rough estimate of performance. In \u201cSleep-wake detection with a contactless, bedside radar sleep sensing system\u201d, we share the full details of these validation results, demonstrating sleep-wake estimation equivalent to or, in some cases, better than current clinical and consumer sleep tracking devices.sensitivity and specificitySleep-wake detection with a contactless, bedside radar sleep sensing system\nUnderstanding Sleep Quality with Audio Sensing\nThe Soli-based sleep tracking algorithm described above gives users a convenient and reliable way to see how much sleep they are getting and when sleep disruptions occur. However, to understand and improve their sleep, users also need to understand why their sleep is disrupted. To assist with this, Nest Hub uses its array of sensors to track common sleep disturbances, such as light level changes or uncomfortable room temperature. In addition to these, respiratory events like coughing and snoring are also frequent sources of disturbance, but people are often unaware of these events.\nAs with other audio-processing applications like speech or music recognition, coughing and snoring exhibit distinctive temporal patterns in the audio frequency spectrum, and with sufficient data an ML model can be trained to reliably recognize these patterns while simultaneously ignoring a wide variety of background noises, from a humming fan to passing cars. The model uses entirely on-device audio processing with privacy-preserving analysis, with no raw audio data sent to Google\u2019s servers. A user can then opt to save the outputs of the processing (sound occurrences, such as the number of coughs and snore minutes) in Google Fit, in order to view personal insights and summaries of their night time wellness over time.\n  To train the model, we assembled a large, hand-labeled dataset, drawing examples from the publicly available AudioSet research dataset as well as hundreds of thousands of additional real-world audio clips contributed by thousands of individuals.AudioSetMelspectrogram\n  When a user opts in to cough and snore tracking on their bedside Nest Hub, the device first uses its Soli-based sleep algorithms to detect when a user goes to bed. Once it detects that a user has fallen asleep, it then activates its on-device sound sensing model and begins processing audio. The model works by continuously extracting spectrogram-like features from the audio input and feeding them through a convolutional neural network classifier in order to estimate the probability that coughing or snoring is happening at a given instant in time. These estimates are analyzed over the course of the night to produce a report of the overall cough count and snoring duration and highlight exactly when these events occurred.convolutional neural network\nConclusion\nThe new Nest Hub, with its underlying Sleep Sensing features, is a first step in empowering users to understand their nighttime wellness using privacy-preserving radar and audio signals. We continue to research additional ways that ambient sensing and the predictive ability of consumer devices could help people better understand their daily health and wellness in a privacy-preserving way.\nAcknowledgements\nThis work involved collaborative efforts from a multidisciplinary team of software engineers, researchers, clinicians, and cross-functional contributors. Special thanks to D. Shin for his significant contributions to this technology and blogpost, and Dr. Logan Schneider, visiting sleep neurologist affiliated with the Stanford/VA Alzheimer\u2019s Center and Stanford Sleep Center, whose clinical expertise and contributions were invaluable to continuously guide this research. In addition to the authors, key contributors to this research from Google Health include Jeffrey Yu, Allen Jiang, Arno Charton, Jake Garrison, Navreet Gill, Sinan Hersek, Yijie Hong, Jonathan Hsu, Andi Janti, Ajay Kannan, Mukil Kesavan, Linda Lei, Kunal Okhandiar\u200e, Xiaojun Ping, Jo Schaeffer, Neil Smith, Siddhant Swaroop, Bhavana Koka, Anupam Pathak, Dr. Jim Taylor, and the extended team. Another special thanks to Ken Mixter for his support and contributions to the development and integration of this technology into Nest Hub. Thanks to Mark Malhotra and Shwetak Patel for their ongoing leadership, as well as the Nest, Fit, Soli, and Assistant teams we collaborated with to build and validate Sleep Sensing on Nest Hub.\n1 Not intended to diagnose, cure, mitigate, prevent or treat any disease or condition.\u00a0\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2021/03/contactless-sleep-sensing-in-nest-hub.html",
      "author": "Posted by Michael Dixon, Software Engineer and Reena Singhal Lee, Product Manager, Google Health"
    },
    {
      "title": "LEAF: A Learnable Frontend for Audio Classification",
      "date": "Friday, March 12, 2021",
      "abstract": "LEAF: A Learnable Frontend for Audio Classification\nDeveloping machine learning (ML) models for audio understanding has seen tremendous progress over the past several years. Leveraging the ability to learn parameters from data, the field has progressively shifted from composite, handcrafted systems to today\u2019s deep neural classifiers that are used to recognize speech, understand music, or classify animal vocalizations such as bird calls. However, unlike computer vision models, which can learn from raw pixels, deep neural networks for audio classification are rarely trained from raw audio waveforms. Instead, they rely on pre-processed data in the form of mel filterbanks \u2014 handcrafted mel-scaled spectrograms that have been designed to replicate some aspects of the human auditory response.recognize speechunderstand musicclassify animal vocalizations such as bird callscomputer visionmel-scaledspectrograms\nAlthough modeling mel filterbanks for ML tasks has been historically successful, it is limited by the inherent biases of fixed features: even though using a fixed mel-scale and a logarithmic compression works well in general, we have no guarantee that they provide the best representations for the task at hand. In particular, even though matching human perception provides good inductive biases for some application domains, e.g., speech recognition or music understanding, these biases may be detrimental to domains for which imitating the human ear is not important, such as recognizing whale calls. So, in order to achieve optimal performance, the mel filterbanks should be tailored to the task of interest, a tedious process that requires an iterative effort informed by expert domain knowledge. As a consequence, standard mel filterbanks are used for most audio classification tasks in practice, even though they are suboptimal. In addition, while researchers have proposed ML systems to address these problems, such as Time-Domain Filterbanks, SincNet and Wavegram, they have yet to match the performance of traditional mel filterbanks.fixed featuresinductive biasesrecognizing whale callsTime-Domain FilterbanksSincNetWavegram\nIn \u201cLEAF, A Fully Learnable Frontend for Audio Classification\u201d, accepted at ICLR 2021, we present an alternative method for crafting learnable spectrograms for audio understanding tasks. LEarnable Audio Frontend (LEAF) is a neural network that can be initialized to approximate mel filterbanks, and then be trained jointly with any audio classifier to adapt to the task at hand, while only adding a handful of parameters to the full model. We show that over a wide range of audio signals and classification tasks, including speech, music and bird songs, LEAF spectrograms improve classification performance over fixed mel filterbanks and over previously proposed learnable systems. We have implemented the code in TensorFlow 2 and released it to the community through our GitHub repository.LEAF, A Fully Learnable Frontend for Audio ClassificationICLR 2021TensorFlow 2our GitHub repository\nMel Filterbanks: Mimicking Human Perception of Sound\nThe first step in the traditional approach to creating a mel filterbank is to capture the sound\u2019s time-variability by windowing, i.e., cutting the signal into short segments with fixed duration. Then, one performs filtering, by passing the windowed segments through a bank of fixed frequency filters, that replicate the human logarithmic sensitivity to pitch. Because we are more sensitive to variations in low frequencies than high frequencies, mel filterbanks give more importance to the low-frequency range of sounds. Finally, the audio signal is compressed to mimic the ear\u2019s logarithmic sensitivity to loudness \u2014 a sound needs to double its power for a person to perceive an increase of 3 decibels.decibels\nLEAF loosely follows this traditional approach to mel filterbank generation, but replaces each of the fixed operations (i.e., the filtering layer, windowing layer, and  compression function) by a learned counterpart. The output of LEAF is a time-frequency representation (a spectrogram) similar to mel filterbanks, but fully learnable. So, for example, while a mel filterbank uses a fixed scale for pitch, LEAF learns the scale that is best suited to the task of interest. Any model that can be trained using mel filterbanks as input features, can also be trained on LEAF spectrograms.\nWhile LEAF can be initialized randomly, it can also be initialized in a way that approximates mel filterbanks, which have been shown to be a better starting point. Then, LEAF can be trained with any classifier to adapt to the task of interest.\nA Parameter-Efficient Alternative to Fixed Features\nA potential downside of replacing fixed features that involve no learnable parameter with a trainable system is that it can significantly increase the number of parameters to optimize. To avoid this issue, LEAF uses Gabor convolution layers that have only two parameters per filter, instead of the ~400 parameters typical of a standard convolution layer. This way, even when paired with a small classifier, such as EfficientNetB0, the LEAF model only accounts for 0.01% of the total parameters.GaborEfficientNetB0\nPerformance\nWe apply LEAF to diverse audio classification tasks, including recognizing speech commands, speaker identification, acoustic scene recognition, identifying musical instruments, and finding birdsongs. On average, LEAF outperforms both mel filterbanks and previous learnable frontends, such as Time-Domain Filterbanks, SincNet and Wavegram. In particular, LEAF achieves a 76.9% average accuracy across the different tasks, compared to 73.9% for mel filterbanks. Moreover we show that LEAF can be trained in a multi-task setting, such that a single LEAF parametrization can work well across all these tasks. Finally, when combined with a large audio classifier, LEAF reaches state-of-the-art performance on the challenging AudioSet benchmark, with a 2.74 d-prime score.speech commandsspeaker identificationacoustic scene recognitionidentifying musical instrumentsfinding birdsongsTime-Domain FilterbanksSincNetWavegramAudioSet benchmarkd-primeD-primeAudioSet\nConclusion\nThe scope of audio understanding tasks keeps growing, from diagnosing dementia from speech to detecting humpback whale calls from underwater microphones. Adapting mel filterbanks to every new task can require a significant amount of hand-tuning and experimentation. In this context, LEAF provides a drop-in replacement for these fixed features, that can be trained to adapt to the task of interest, with minimal task-specific adjustments. Thus, we believe that LEAF can accelerate development of models for new audio understanding tasks.diagnosing dementia from speechdetecting humpback whale calls from underwater microphones\nAcknowledgements\nWe thank our co-authors, Olivier Teboul, F\u00e9lix de Chaumont-Quitry and Marco Tagliasacchi. We also thank Dick Lyon, Vincent Lostanlen, Matt Harvey, and Alex Park for helpful discussions, and Julie Thomas for helping to design figures for this post.",
      "link": "http://ai.googleblog.com/2021/03/leaf-learnable-frontend-for-audio.html",
      "author": "Posted by Neil Zeghidour, Research Scientist, Google Research"
    },
    {
      "title": "A New Lens on Understanding Generalization in Deep Learning",
      "date": "Wednesday, March 10, 2021",
      "abstract": "A New Lens on Understanding Generalization in Deep Learning\nUnderstanding generalization is one of the fundamental unsolved problems in deep learning. Why does optimizing a model on a finite set of training data lead to good performance on a held-out test set? This problem has been studied extensively in machine learning, with a rich history going back more than 50 years. There are now many mathematical tools that help researchers understand generalization in certain models. Unfortunately, most of these existing theories fail when applied to modern deep networks \u2014 they are both vacuous and non-predictive in realistic settings.  This gap between theory and practice is largest for overparameterized models, which in theory have the capacity to overfit their train sets, but often do not in practice.studied extensivelyrich historymathematicaltoolsvacuousnon-predictiveoverparameterized\nIn \u201cThe Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers\u201d, accepted at ICLR 2021, we present a new framework for approaching this problem by connecting generalization to the field of online optimization. In a typical setting, a model trains on a finite set of samples, which are reused for multiple epochs. But in online optimization, the model has access to an infinite stream of samples, and can be iteratively updated while processing this stream. In this work, we find that models that train quickly on infinite data are the same models that generalize well if they are instead trained on finite data. This connection brings new perspectives on design choices in practice, and lays a roadmap for understanding generalization from a theoretical perspective.The Deep Bootstrap Framework: Good Online Learners are Good Offline GeneralizersICLR 2021online optimization\nThe Deep Bootstrap Framework\nThe main idea of the Deep Bootstrap framework is to compare the real world, where there is finite training data, to an \"ideal world\", where there is infinite data. We define these as:supervised learningTest soft-error\nA priori, one might expect the real and ideal worlds may have nothing to do with each other, since in the real world the model sees a finite number of examples from the distribution while in the ideal world the model sees the whole distribution. But in practice, we found that the real and ideal models actually have similar test error.\nIn order to quantify this observation, we simulated an ideal world setting by creating a new dataset, which we call CIFAR-5m. We trained a generative model on CIFAR-10, which we then used to generate ~6 million images. The scale of the dataset was chosen to ensure that it is \u201cvirtually infinite\u201d from the model\u2019s perspective, so that the model never resamples the same data. That is, in the ideal world, the model sees an entirely fresh set of samples.CIFAR-5mgenerative modelCIFAR-10\nThe figure below presents the test error of several models, comparing their performance when trained on CIFAR-5m data in the real world setting (i.e., re-used data) and the ideal world (\u201cfresh\u201d data). The solid blue line shows a ResNet model in the real world, trained on 50K samples for 100 epochs with standard CIFAR-10 hyperparameters. The dashed blue line shows the corresponding model in the ideal world, trained on 5 million samples in a single pass. Surprisingly, these worlds have very similar test error \u2014 the model in some sense \"doesn't care\" whether it sees re-used samples or fresh ones.ResNet\nThis also holds for other architectures, e.g., a Multi-Layer-Perceptron (red), a Vision Transformer (green), and across many other settings of architecture, optimizer, data distribution, and sample size. These experiments suggest a new perspective on generalization: models that optimize quickly (on infinite data), generalize well (on finite data). For example, the ResNet model generalizes better than the MLP model on finite data, but this is \"because\" it optimizes faster even on infinite data.Multi-Layer-PerceptronVision Transformeracross many other settings\nUnderstanding Generalization from Optimization Behavior\nThe key observation is that real world and ideal world models remain close, in test error, for all timesteps, until the real world converges (< 1% train error). Thus, one can study models in the real world by studying their corresponding behavior in the ideal world.\nThis means that the generalization of the model can be understood in terms of its optimization performance under two frameworks:\nThus, to study generalization, we can equivalently study the two terms above, which can be conceptually simpler, since they only involve optimization concerns. Based on this observation, good models and training procedures are those that (1) optimize quickly in the ideal world and (2) do not optimize too quickly in the real world.\nAll design choices in deep learning can be viewed through their effect on these two terms. For example, some advances like convolutions, skip-connections, and pre-training help primarily by accelerating ideal world optimization, while other advances like regularization and data-augmentation help primarily by decelerating real world optimization.convolutionsskip-connectionspretrainingregularizationdata-augmentation\nApplying the Deep Bootstrap Framework\nResearchers can use the Deep Bootstrap framework to study and guide design choices in deep learning. The principle is: whenever one makes a change that affects generalization in the real world (the architecture, learning-rate, etc.), one should consider its effect on (1) the ideal world optimization of test error (faster is better) and (2) the real world optimization of train error (slower is better).\nFor example, pre-training is often used in practice to help generalization of models in small-data regimes. However, the reason that pre-training helps remains poorly understood. One can study this using the Deep Bootstrap framework by looking at the effect of pre-training on terms (1) and (2) above. We find that the primary effect of pre-training is to improve the ideal world optimization (1) \u2014 pre-training turns the network into a \"fast learner\" for online optimization. The improved generalization of pretrained models is thus almost exactly captured by their improved optimization in the ideal world. The figure below shows this for Vision-Transformers (ViT) trained on CIFAR-10, comparing training from scratch vs. pre-training on ImageNet.Vision-TransformersCIFAR-10ImageNetViTs\nOne can also study data-augmentation using this framework. Data-augmentation in the ideal world corresponds to augmenting each fresh sample once, as opposed to augmenting the same sample multiple times. This framework implies that good data-augmentations are those that (1) do not significantly harm ideal world optimization (i.e., augmented samples don't look too \"out of distribution\") or (2) inhibit real world optimization speed (so the real world takes longer to fit its train set).\nThe main benefit of data-augmentation is through the second term, prolonging the real world optimization time. As for the first term, some aggressive data augmentations (mixup/cutout) can actually harm the ideal world, but this effect is dwarfed by the second term.mixupcutout\nConcluding Thoughts\nThe Deep Bootstrap framework provides a new lens on generalization and empirical phenomena in deep learning. We are excited to see it applied to understand other aspects of deep learning in the future. It is especially interesting that generalization can be characterized via purely optimization considerations, which is in contrast to many prevailing approaches in theory. Crucially, we consider both online and offline optimization, which are individually insufficient, but that together determine generalization.prevailing approaches\nThe Deep Bootstrap framework can also shed light on why deep learning is fairly robust to many design choices: many kinds of architectures, loss functions, optimizers, normalizations, and activation functions can generalize well. This framework suggests a unifying principle: that essentially any choice that works well in the online optimization setting will also generalize well in the offline setting.architecturesloss functionsoptimizersnormalizationsactivationfunctions\nFinally, modern neural networks can be either overparameterized (e.g., large networks trained on small data tasks) or underparmeterized (e.g., OpenAI's GPT-3, Google's T5, or Facebook's ResNeXt WSL). The Deep Bootstrap framework implies that online optimization is a crucial factor to success in both regimes.smalldatatasksOpenAI's GPT-3Google's T5Facebook's ResNeXt WSL\nAcknowledgements\nWe are thankful to our co-author, Behnam Neyshabur, for his great contributions to the paper and valuable feedback on the blog. We thank Boaz Barak, Chenyang Yuan, and Chiyuan Zhang for helpful comments on the blog and paper.",
      "link": "http://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html",
      "author": "Hanie Sedghi, Google Research and Preetum Nakkiran, Harvard University"
    },
    {
      "title": "Accelerating Neural Networks on Mobile and Web with Sparse Inference",
      "date": "Tuesday, March 9, 2021",
      "abstract": "Accelerating Neural Networks on Mobile and Web with Sparse Inference\nOn-device inference of neural networks enables a variety of real-time applications, like pose estimation and background blur, in a low-latency and privacy-conscious way. Using ML inference frameworks like TensorFlow Lite with XNNPACK ML acceleration library, engineers optimize their models to run on a variety of devices by finding a sweet spot between model size, inference speed and the quality of the predictions.pose estimationbackground blurTensorFlow LiteXNNPACK\nOne way to optimize a model is through use of sparse neural networks [1, 2, 3], which have a significant fraction of their weights set to zero. In general, this is a desirable quality as it not only reduces the model size via compression, but also makes it possible to skip a significant fraction of multiply-add operations, thereby speeding up inference. Further, it is possible to increase the number of parameters in a model and then sparsify it to match the quality of the original model, while still benefiting from the accelerated inference. However, the use of this technique remains limited in production largely due to a lack of tools to sparsify popular convolutional architectures as well as insufficient support for running these operations on-device.123\nToday we announce the release of a set of new features for the XNNPACK acceleration library and TensorFlow Lite that enable efficient inference of sparse networks, along with guidelines on how to sparsify neural networks, with the goal of helping researchers develop their own sparse on-device models. Developed in collaboration with DeepMind, these tools power a new generation of live perception experiences, including hand tracking in MediaPipe and background features in Google Meet, accelerating inference speed from 1.2 to 2.4 times, while reducing the model size by half. In this post, we provide a technical overview of sparse neural networks \u2014 from inducing sparsity during training to on-device deployment \u2014 and offer some ideas on how researchers might create their own sparse models.XNNPACK acceleration libraryDeepMindhand trackingMediaPipebackground featuresbackground features\nSparsifying a Neural Network\nMany modern deep learning architectures, like MobileNet and EfficientNetLite, are primarily composed of depthwise convolutions with a small spatial kernel and 1x1 convolutions that linearly combine features from the input image. While such architectures have a number of potential targets for sparsification, including the full 2D convolutions that frequently occur at the beginning of many networks or the depthwise convolutions, it is the 1x1 convolutions that are the most expensive operators as measured by inference time. Because they account for over 65% of the total compute, they are an optimal target for sparsification.MobileNetEfficientNetLitedepthwise convolutions1x1 convolutions2D convolutionsdepthwise convolutionsMobileNetMobileNetV2MobileNetV3EfficientNet-Lite\nIn modern on-device inference engines, like XNNPACK, the implementation of 1x1 convolutions as well as other operations in the deep learning models rely on the HWC tensor layout, in which the tensor dimensions correspond to the height, width, and channel (e.g., red, green or blue) of the input image. This tensor configuration allows the inference engine to process the channels corresponding to each spatial location (i.e., each pixel of an image) in parallel. However, this ordering of the tensor is not a good fit for sparse inference because it sets the channel as the innermost dimension of the tensor and makes it more computationally expensive to access.rely on the HWC tensor layout\nOur updates to XNNPACK enable it to detect if a model is sparse. If so, it switches from its standard dense inference mode to sparse inference mode, in which it employs a CHW (channel, height, width) tensor layout. This reordering of the tensor allows for an accelerated implementation of the sparse 1x1 convolution kernel for two reasons: 1) entire spatial slices of the tensor can be skipped when the corresponding channel weight is zero following a single condition check, instead of a per-pixel test; and 2) when the channel weight is non-zero, the computation can be made more efficient by loading neighbouring pixels into the same memory unit. This enables us to process multiple pixels simultaneously, while also performing each operation in parallel across several threads. Together these changes result in a speed-up of 1.8x to 2.3x when at least 80% of the weights are zero.sparse inference\nIn order to avoid converting back and forth between the CHW tensor layout that is optimal for sparse inference and the standard HWC tensor layout after each operation, XNNPACK provides efficient implementations of several CNN operators in CHW layout.several CNN operators\nGuidelines for Training Sparse Neural Networks\nTo create a sparse neural network, the guidelines included in this release suggest one start with a dense version and then gradually set a fraction of its weights to zero during training. This process is called pruning. Of the many available techniques for pruning, we recommend using magnitude pruning (available in the TF Model Optimization Toolkit) or the recently introduced RigL method. With a modest increase in training time, both of these can successfully sparsify deep learning models without degrading their quality. The resulting sparse models can be stored efficiently in a compressed format that reduces the size by a factor of two compared to their dense equivalent.magnitude pruningTF Model Optimization ToolkitRigLstored efficiently in a compressed format\nThe quality of sparse networks is influenced by several hyperparameters, including training time, learning rate and schedules for pruning. The TF Pruning API provides an excellent example of how to select these, as well as some tips for training such models. We recommend running hyperparameter searches to find the sweet spot for your application.TF Pruning API\nApplications\nWe demonstrate that it is possible to sparsify classification tasks,  dense segmentation (e.g., Meet background blur) and regression problems (MediaPipe Hands), which provides tangible benefits to users. For example, in the case of Google Meet, sparsification lowered the inference time of the model by 30%, which provided access to higher quality models for more users.Meet background blurMediaPipe Hands\nThe approach to sparsity described here works best with architectures based on inverted residual blocks, such as MobileNetV2, MobileNetV3 and EfficientNetLite. The degree of sparsity in a network influences both inference speed and quality. Starting from a dense network of a fixed capacity, we found modest performance gains even at 30% sparsity. With increased sparsity, the quality of the model remains relatively close to the dense baseline until reaching 70% sparsity, beyond which there is a more pronounced drop in accuracy. However, one can compensate for the reduced accuracy at 70% sparsity by increasing the size of the base network by 20%, which results in faster inference times without degrading the quality of the model. No further changes are required to run the sparsified models, because XNNPACK can recognize and automatically enable sparse inference.inverted residual blocksMobileNetV2MobileNetV3EfficientNetLiteIntersection over Union\nSparsity as Automatic Alternative to Distillation\nBackground blur in Google Meet uses a segmentation model based on a modified MobileNetV3 backbone with attention blocks. We were able to speed up the model by 30% by applying a 70% sparsification, while preserving the quality of the foreground mask.  We examined the predictions of the sparse and dense models on images from 17 geographic subregions, finding no significant difference, and released the details in the associated model card.attention blocksmodel card\nSimilarly, MediaPipe Hands predicts hand landmarks in real-time on mobile and the web using a model based on the EfficientNetLite backbone. This backbone model was manually distilled from the large dense model, which is a computationally expensive, iterative process. Using the sparse version of the dense model instead of distilled one, we were able to maintain the same inference speed but without the labor intensive process of distilling from a dense model. Compared with the dense model the sparse model improved the inference by a factor of two, achieving the identical landmark quality as the distilled model. In a sense, sparsification can be thought of as an automatic approach to unstructured model distillation, which can improve model performance without extensive manual effort. We evaluated the sparse model on the geodiverse dataset and made the model card publicly available.MediaPipe Handsmanually distilledavailableMediPipe solutiondensesparse\nFuture work\nWe find sparsification to be a simple yet powerful technique for improving CPU inference of neural networks. Sparse inference allows engineers to run larger models without incurring a significant performance or size overhead and offers a promising new direction for research. We are continuing to extend XNNPACK with wider support for operations in CHW layout and are exploring how it might be combined with other optimization techniques like quantization. We are excited to see what you might build with this technology!\nAcknowledgments\nSpecial thanks to all who worked on this project: Karthik Raveendran, Erich Elsen, Tingbo Hou\u200e, Trevor Gale, Siargey Pisarchyk, Yury Kartynnik, Yunlu Li, Utku Evci, Matsvei Zhdanovich, Sebastian Jansson, St\u00e9phane Hulaud, Michael Hays, Juhyun Lee, Fan Zhang, Chuo-Ling Chang, Gregory Karpiak, Tyler Mullen, Jiuqiang Tang, Ming Guang Yong, Igor Kibalchich, and Matthias Grundmann.",
      "link": "http://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html",
      "author": "Posted by Artsiom Ablavatski and Marat Dukhan, Software Engineers, Google Research"
    },
    {
      "title": "PAIRED: A New Multi-agent Approach for Adversarial Environment Generation",
      "date": "Friday, March 5, 2021",
      "abstract": "PAIRED: A New Multi-agent Approach for Adversarial Environment Generation\nThe effectiveness of any machine learning method is critically dependent on its training data. In the case of reinforcement learning (RL), one can rely either on limited data collected by an agent interacting with the real world, or a simulated training environment that can be used to collect as much data as needed. This latter method of training in simulation is increasingly popular, but it has a problem \u2014 the RL agent can learn what is built into the simulator, but tends to be bad at generalizing to tasks that are even slightly different than the ones simulated. And obviously building a simulator that covers all the complexity of the real-world is extremely challenging.reinforcement learninginteracting with the real worldbadatgeneralizing\nAn approach to address this is to automatically create more diverse training environments by randomizing all the parameters of the simulator, a process called domain randomization (DR). However, DR can fail even in very simple environments. For example, in the animation below, the blue agent is trying to navigate to the green goal. The left panel shows an environment created with DR where the positions of the obstacles and goal have been randomized. Many of these DR environments were used to train the agent, which was then transferred to the simple Four Rooms environment in the middle panel. Notice that the agent can\u2019t find the goal. This is because it has not learned to walk around walls. Even though the wall configuration from the Four Rooms example could have been generated randomly in the DR training phase, it\u2019s unlikely. As a result, the agent has not spent enough time training on walls similar to the Four Rooms structure, and is unable to reach the goal.randomizingdomain randomization\nInstead of just randomizing the environment parameters, one could train a second RL agent to learn how to set the environment parameters. This minimax adversary can be trained to minimize the performance of the first RL agent by finding and exploiting weaknesses in its policy, e.g. building wall configurations it has not encountered before. But again there is a problem. The right panel shows an environment built by a minimax adversary in which it is actually impossible for the agent to reach the goal. While the minimax adversary has succeeded in its task \u2014 it has minimized the performance of the original agent \u2014 it provides no opportunity for the agent to learn. Using a purely adversarial objective is not well suited to generating training environments, either.minimaxadversary\nIn collaboration with UC Berkeley, we propose a new multi-agent approach for training the adversary in  \u201cEmergent Complexity and Zero-shot Transfer via Unsupervised Environment Design\u201d, a publication recently presented at NeurIPS 2020. In this work we present an algorithm, Protagonist Antagonist Induced Regret Environment Design (PAIRED), that is based on minimax regret and prevents the adversary from creating impossible environments, while still enabling it to correct weaknesses in the agent\u2019s policy. PAIRED incentivizes the adversary to tune the difficulty of the generated environments to be just outside the agent\u2019s current abilities, leading to an automatic curriculum of increasingly challenging training tasks. We show that agents trained with PAIRED learn more complex behavior and generalize better to unknown test tasks. We have released open-source code for PAIRED on our GitHub repo.UC BerkeleyEmergent Complexity and Zero-shot Transfer via Unsupervised Environment DesignNeurIPS 2020minimax regretGitHub repo\nPAIRED\nTo flexibly constrain the adversary, PAIRED introduces a third RL agent, which we call the antagonist agent, because it is allied with the adversarial agent, i.e., the one designing the environment. We rename our initial agent, the one navigating the environment, the protagonist. Once the adversary generates an environment, both the protagonist and antagonist play through that environment.\n  The adversary\u2019s job is to maximize the antagonist\u2019s reward while minimizing the protagonist's reward. This means it must create environments that are feasible (because the antagonist can solve them and get a high score), but challenging to the protagonist (exploit weaknesses in its current policy). The gap between the two rewards is the regret\u00a0\u2014 the adversary tries to maximize the regret, while the protagonist competes to minimize it.The methods discussed above (domain randomization, minimax regret and PAIRED) can be analyzed using the same theoretical framework, unsupervised environment design (UED), which we describe in detail in the paper. UED draws a connection between environment design and decision theory, enabling us to show that domain randomization is equivalent to the Principle of Insufficient Reason, the minimax adversary follows the Maximin Principle, and PAIRED is optimizing minimax regret. This formalism enables us to use tools from decision theory to understand the benefits and drawbacks of each method. Below, we show how each of these ideas works for environment design:decision theoryPrinciple of Insufficient ReasonMaximin Principleminimax regret\nCurriculum Generation\nWhat\u2019s interesting about minimax regret is that it incentivizes the adversary to generate a curriculum of initially easy, then increasingly challenging environments. In most RL environments, the reward function will give a higher score for completing the task more efficiently, or in fewer timesteps. When this is true, we can show that regret incentivizes the adversary to create the easiest possible environment the protagonist can\u2019t solve yet. To see this, let\u2019s assume the antagonist is perfect, and always gets the highest score that it possibly can. Meanwhile, the protagonist is terrible, and gets a score of zero on everything. In that case, the regret just depends on the difficulty of the environment. Since easier environments can be completed in fewer timesteps, they allow the antagonist to get a higher score. Therefore, the regret of failing at an easy environment is greater than the regret of failing on a hard environment:\n  So, by maximizing regret the adversary is searching for easy environments that the protagonist fails to do. Once the protagonist learns to solve each environment, the adversary must move on to finding a slightly harder environment that the protagonist can\u2019t solve. Thus, the adversary generates a curriculum of increasingly difficult tasks.\nResults\nWe can see the curriculum emerging in the learning curves below, which plot the shortest path length of a maze the agents have successfully solved. Unlike minimax or domain randomization, the PAIRED adversary creates a curriculum of increasingly longer, but possible, mazes, enabling PAIRED agents to learn more complex behavior.\n  But can these different training schemes help an agent generalize better to unknown test tasks? Below, we see the zero-shot transfer performance of each algorithm on a series of challenging test tasks. As the complexity of the transfer environment increases, the performance gap between PAIRED and the baselines widens. For extremely difficult tasks like Labyrinth and Maze, PAIRED is the only method that can occasionally solve the task. These results provide promising evidence that PAIRED can be used to improve generalization for deep RL.\n  Admittedly, these simple gridworlds do not reflect the complexities of the real world tasks that many RL methods are attempting to solve. We address this in \u201cAdversarial Environment Generation for Learning to Navigate the Web\u201d, which examines the performance of PAIRED when applied to more complex problems, such as teaching RL agents to navigate web pages. We propose an improved version of PAIRED, and show how it can be used to train an adversary to generate a curriculum of increasingly challenging websites:Adversarial Environment Generation for Learning to Navigate the Web\n  Above, you can see websites built by the adversary in the early, middle, and late training stages, which progress from using very few elements per page to many simultaneous elements, making the tasks progressively harder. We test whether agents trained on this curriculum can generalize to standardized web navigation tasks, and achieve a 75% success rate, with a 4x improvement over the strongest curriculum learning baseline:\nConclusions\nDeep RL is very good at fitting a simulated training environment, but how can we build simulations that cover the complexity of the real world? One solution is to automate this process. We propose Unsupervised Environment Design (UED) as a framework that describes different methods for automatically creating a distribution of training environments, and show that UED subsumes prior work like domain randomization and minimax adversarial training. We think PAIRED is a good approach for UED, because regret maximization leads to a curriculum of increasingly challenging tasks, and prepares agents to transfer successfully to unknown test tasks.\nAcknowledgements\nWe would like to recognize the co-authors of \u201cEmergent Complexity and Zero-shot Transfer via Unsupervised Environment Design\u201d: Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine, as well as the co-authors of \u201cAdversarial Environment Generation for Learning to Navigate the Web\u201d: Izzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Kevin Malta, Manoj Tiwari, Honglak Lee, Aleksandra Faust. In addition, we thank Michael Chang, Marvin Zhang, Dale Schuurmans, Aleksandra Faust, Chase Kew, Jie Tan, Dennis Lee, Kelvin Xu, Abhishek Gupta, Adam Gleave, Rohin Shah, Daniel Filan, Lawrence Chan, Sam Toyer, Tyler Westenbroek, Igor Mordatch, Shane Gu, DJ Strouse, and Max Kleiman-Weiner for discussions that contributed to this work.Emergent Complexity and Zero-shot Transfer via Unsupervised Environment DesignAdversarial Environment Generation for Learning to Navigate the Web",
      "link": "http://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html",
      "author": "Posted by Natasha Jaques, Google Research and Michael Dennis, UC Berkeley"
    },
    {
      "title": "Lyra: A New Very Low-Bitrate Codec for Speech Compression",
      "date": "Thursday, February 25, 2021",
      "abstract": "Lyra: A New Very Low-Bitrate Codec for Speech Compression\nConnecting to others online via voice and video calls is something that is increasingly a part of everyday life. The real-time communication frameworks, like WebRTC,  that make this possible depend on efficient compression techniques, codecs, to encode (or decode) signals for transmission or storage. A vital part of media applications for decades, codecs allow bandwidth-hungry applications  to efficiently transmit data, and have led to an expectation of high-quality communication anywhere at any time.WebRTCcodecs\nAs such, a continuing challenge in developing codecs, both for video and audio, is to provide increasing quality, using less data, and to minimize latency for real-time communication. Even though video might seem much more bandwidth hungry than audio, modern video codecs can reach lower bitrates than some high-quality speech codecs used today. Combining low-bitrate video and speech codecs can deliver a high-quality video call experience even in low-bandwidth networks. Yet historically, the lower the bitrate for an audio codec, the less intelligible and more robotic the voice signal becomes. Furthermore, while some people have access to a consistent  high-quality, high-speed network, this  level of  connectivity isn\u2019t universal, and even those in well connected areas at times experience poor quality, low bandwidth, and congested network connections.\nTo solve this problem, we have created Lyra, a high-quality, very low-bitrate speech codec that makes voice communication available even on the slowest networks.  To do this, we\u2019ve applied traditional codec techniques while leveraging advances in machine learning (ML) with models trained on thousands of hours of data to create a novel method for compressing and transmitting voice signals.Lyra\nLyra Overview\nThe basic architecture of the Lyra codec is quite simple. Features, or distinctive speech attributes, are extracted from speech every 40ms and are then compressed for transmission. The features themselves are log mel spectrograms, a list of numbers representing the speech energy in different frequency bands, which have traditionally been used for their perceptual relevance because they are modeled after human auditory response. On the other end, a generative model uses those features to recreate the speech signal. In this sense, Lyra is very similar to other traditional parametric codecs, such as MELP.log mel spectrogramsgenerative modelMELP\n  However traditional parametric codecs, which simply extract from speech critical parameters that can then be used to recreate the signal at the receiving end, achieve low bitrates, but often sound robotic and unnatural. These shortcomings have led to the development of a new generation of high-quality audio generative models that have revolutionized the field by being able to not only differentiate between signals, but also generate completely new ones. DeepMind\u2019s WaveNet was the first of these generative models that paved the way for many to come.  Additionally, WaveNetEQ, the generative model-based packet-loss-concealment system currently used in Duo, has demonstrated how this technology can be used in real-world scenarios.parametricgenerative modelsWaveNetWaveNetEQ\nA New Approach to Compression with Lyra\nUsing these models as a baseline, we\u2019ve developed a new model capable of reconstructing speech using minimal amounts of data.  Lyra harnesses the power of these new natural-sounding generative models to maintain the low bitrate of parametric codecs while achieving high quality, on par with state-of-the-art waveform codecs used in most streaming and communication platforms today. The drawback of waveform codecs is that they achieve this high quality by compressing and sending over the signal sample-by-sample, which requires a higher bitrate and, in most cases, isn\u2019t necessary to achieve natural sounding speech.\nOne concern with generative models is their computational complexity. Lyra avoids this issue by using a cheaper recurrent generative model, a WaveRNN variation, that works at a lower rate, but generates in parallel multiple signals in different frequency ranges that it later combines into a single output signal at the desired sample rate. This trick enables Lyra to not only run on cloud servers, but also on-device on mid-range phones in real time (with a processing latency of 90ms, which is in line with other traditional speech codecs).\u00a0This generative model is then trained on thousands of hours of speech data and optimized, similarly to WaveNet, to accurately recreate the input audio.WaveRNN\nComparison with Existing Codecs\nSince the inception of Lyra, our mission has been to provide the best quality audio using a fraction of the bitrate data of alternatives.  Currently, the royalty-free open-source codec Opus, is the most widely used codec for WebRTC-based VOIP applications and, with audio at 32kbps, typically obtains transparent speech quality, i.e., indistinguishable from the original.   However, while Opus can be used in more bandwidth constrained environments down to 6kbps, it starts to demonstrate degraded audio quality.  Other codecs are capable of operating at comparable bitrates to Lyra (Speex, MELP, AMR), but each suffer from increased artifacts and result in a robotic sounding voice.OpusVOIPSpeexAMR\nLyra is currently designed to operate at 3kbps and listening tests show that Lyra outperforms any other codec at that bitrate and  is compared favorably to Opus at 8kbps, thus achieving more than a 60% reduction in bandwidth.  Lyra can be used wherever the bandwidth conditions are insufficient for higher-bitrates and existing low-bitrate codecs do not provide adequate quality.\nEnsuring Fairness\nAs with any ML based system, the model must be trained to make sure that it works for everyone.  We\u2019ve trained Lyra with thousands of hours of audio with speakers in over 70 languages using open-source audio libraries and then verifying the audio quality with expert and crowdsourced listeners.  One of the design goals of Lyra is to ensure universally accessible high-quality audio experiences.  Lyra trains on a wide dataset, including speakers in a myriad of languages, to make sure the codec is robust to any situation it might encounter.\nSocietal Impact and Where We Go From Here\nThe implications of technologies like Lyra are far reaching, both in the short and long term.  With Lyra, billions of users in emerging markets can have access to an efficient low-bitrate codec that allows them to have higher quality audio than ever before. Additionally, Lyra can be used in cloud environments enabling users with various network and device capabilities to chat seamlessly with each other.  Pairing Lyra with new video compression technologies, like AV1, will allow video chats to take place, even for users connecting to the internet via a 56kbps dial-in modem.AV1\nDuo already uses ML to reduce audio interruptions, and is currently rolling out Lyra to improve audio call quality and reliability on very low bandwidth connections. We will continue to optimize Lyra\u2019s performance and quality to ensure maximum availability of the technology, with investigations into acceleration via GPUs and TPUs. We are also beginning to research how these technologies can lead to a low-bitrate general-purpose audio codec (i.e., music and other non-speech use cases).Duoaudio interruptions\nAcknowledgements\nThanks to everyone who made Lyra possible including Jan Skoglund, Felicia Lim, Michael Chinen, Bastiaan Kleijn, Tom Denton, Andrew Storus, Yero Yeh (Chrome Media), Henrik Lundin, Niklas Blum, Karl Wiberg (Google Duo), Chenjie Gu, Zach Gleicher, Norman Casagrande, Erich Elsen (DeepMind).",
      "link": "http://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html",
      "author": "Posted by Alejandro Luebs, Software Engineer and Jamieson Brettle, Product Manager, Chrome"
    },
    {
      "title": "The Technology Behind Cinematic Photos",
      "date": "Tuesday, February 23, 2021",
      "abstract": "The Technology Behind Cinematic Photos\nLooking at photos from the past can help people relive some of their most treasured moments. Last December we launched Cinematic photos, a new feature in Google Photos that aims to recapture the sense of immersion felt the moment a photo was taken, simulating camera motion and parallax by inferring 3D representations in an image. In this post, we take a look at the technology behind this process, and demonstrate how Cinematic photos can turn a single 2D photo from the past into a more immersive 3D animation.we launched Cinematic photosparallaxRick ReitanoPortrait ModeAugmented Realitydepth mapmulti-view stereotwo camerasdual-pixel sensors\nTo enable Cinematic photos on existing pictures that were not taken in multi-view stereo, we trained a convolutional neural network with encoder-decoder architecture to predict a depth map from just a single RGB image. Using only one view, the model learned to estimate depth using monocular cues, such as the relative sizes of objects, linear perspective, defocus blur, etc.convolutional neural networkmonocular\nBecause monocular depth estimation datasets are typically designed for domains such as AR, robotics, and self-driving, they tend to emphasize street scenes or indoor room scenes instead of features more common in casual photography, like people, pets, and objects, which have different composition and framing. So, we created our own dataset for training the monocular depth model using photos captured on a custom 5-camera rig as well as another dataset of Portrait photos captured on Pixel 4. Both datasets included ground-truth depth from multi-view stereo that is critical for training a model.\nMixing several datasets in this way exposes the model to a larger variety of scenes and camera hardware, improving its predictions on photos in the wild. However, it also introduces new challenges, because the ground-truth depth from different datasets may differ from each other by an unknown scaling factor and shift. Fortunately, the Cinematic photo effect only needs the relative depths of objects in the scene, not the absolute depths. Thus we can combine datasets by using a scale-and-shift-invariant loss during training and then normalize the output of the model at inference.\nThe Cinematic photo effect is particularly sensitive to the depth map\u2019s accuracy at person boundaries. An error in the depth map can result in jarring artifacts in the final rendered effect. To mitigate this, we apply median filtering to improve the edges, and also infer segmentation masks of any people in the photo using a DeepLab segmentation model trained on the Open Images dataset. The masks are used to pull forward pixels of the depth map that were incorrectly predicted to be in the background.DeepLabOpen Images dataset\nCamera Trajectory\nThere can be many degrees of freedom when animating a camera in a 3D scene, and our virtual camera setup is inspired by professional video camera rigs to create cinematic motion. Part of this is identifying the optimal pivot point for the virtual camera\u2019s rotation in order to yield the best results by drawing one\u2019s eye to the subject.\nThe first step in 3D scene reconstruction is to create a mesh by extruding the RGB image onto the depth map. By doing so, neighboring points in the mesh can have large depth differences. While this is not noticeable from the \u201cface-on\u201d view, the more the virtual camera is moved, the more likely it is to see polygons spanning large changes in depth. In the rendered output video, this will look like the input texture is stretched. The biggest challenge when animating the virtual camera is to find a trajectory that introduces parallax while minimizing these \u201cstretchy\u201d artifacts.\nBecause of the wide spectrum in user photos and their corresponding 3D reconstructions, it is not possible to share one trajectory across all animations. Instead, we define a loss function that captures how much of the stretchiness can be seen in the final animation, which allows us to optimize the camera parameters for each unique photo. Rather than counting the total number of pixels identified as artifacts, the loss function triggers more heavily in areas with a greater number of connected artifact pixels, which reflects a viewer\u2019s tendency to more easily notice artifacts in these connected areas.\nWe utilize padded segmentation masks from a human pose network to divide the image into three different regions: head, body and background. The loss function is normalized inside each region before computing the final loss as a weighted sum of the normalized losses. Ideally the generated output video is free from artifacts but in practice, this is rare. Weighting the regions differently biases the optimization process to pick trajectories that prefer artifacts in the background regions, rather than those artifacts near the image subject.\nFraming the Scene\nGenerally, the reprojected 3D scene does not neatly fit into a rectangle with portrait orientation, so it was also necessary to frame the output with the correct right aspect ratio while still retaining the key parts of the input image. To accomplish this, we use a deep neural network that predicts per-pixel saliency of the full image. When framing the virtual camera in 3D, the model identifies and captures as many salient regions as possible while ensuring that the rendered mesh fully occupies every output video frame. This sometimes requires the model to shrink the camera's field of view.\nConclusion\nThrough Cinematic photos, we implemented a system of algorithms \u2013 with each ML model evaluated for fairness \u2013 that work together to allow users to relive their memories in a new way, and we are excited about future research and feature improvements. Now that you know how they are created, keep an eye open for automatically created Cinematic photos that may appear in your recent memories within the Google Photos app!\nAcknowledgments\nCinematic Photos is the result of a collaboration between Google Research and Google Photos teams. Key contributors also include: Andre Le, Brian Curless, Cassidy Curtis, Ce Liu\u200e, Chun-po Wang, Daniel Jenstad, David Salesin, Dominik Kaeser, Gina Reynolds, Hao Xu, Hayato Ikoma, Huiwen Chang, Huizhong Chen\u200e, Jamie Aspinall, Janne Kontkanen, Matthew DuVall, Michael Kucera, Michael Milne, Mike Krainin, Mike Liu, Navin Sarma, Orly Liba, Peter Hedman, Rocky Cai\u200e, Ruirui Jiang\u200e, Steven Hickson, Tracy Gu, Tyler Zhu, Varun Jampani, Yuan Hao, Zhongli Ding.",
      "link": "http://ai.googleblog.com/2021/02/the-technology-behind-cinematic-photos.html",
      "author": "Posted by Per Karlsson and Lucy Yu, Software Engineers, Google Research"
    },
    {
      "title": "Introducing Model Search: An Open Source Platform for Finding Optimal ML Models",
      "date": "Friday, February 19, 2021",
      "abstract": "Introducing Model Search: An Open Source Platform for Finding Optimal ML Models\nThe success of a neural network (NN) often depends on how well it can generalize to various tasks. However, designing NNs that can generalize well is challenging because the research community's understanding of how a neural network generalizes is currently somewhat limited: What does the appropriate neural network look like for a given problem? How deep should it be? Which types of layers should be used? Would LSTMs be enough or would Transformer layers be better? Or maybe a combination of the two? Would ensembling or distillation boost performance? These tricky questions are made even more challenging when considering machine learning (ML) domains where there may exist better intuition and deeper understanding than others.neural networkLSTMsTransformerensemblingdistillation\nIn recent years, AutoML algorithms have emerged [e.g., 1, 2, 3] to help researchers find the right neural network automatically without the need for manual experimentation. Techniques like neural architecture search (NAS), use algorithms, like reinforcement learning (RL), evolutionary algorithms, and combinatorial search, to build a neural network out of a given search space. With the proper setup, these techniques have demonstrated they are capable of delivering results that are better than the manually designed counterparts. But more often than not, these algorithms are compute heavy, and need thousands of models to train before converging. Moreover, they explore search spaces that are domain specific and incorporate substantial prior human knowledge that does not transfer well across domains. As an example, in image classification, the traditional NAS searches for two good building blocks (convolutional and downsampling blocks), that it arranges following traditional conventions to create the full network.AutoML123neural architecture searchreinforcement learningevolutionary algorithmscombinatorial searchconvolutional and downsampling blocks\nTo overcome these shortcomings and to extend access to AutoML solutions to the broader research community, we are excited to announce the open source release of Model Search, a platform that helps researchers develop the best ML models, efficiently and automatically. Instead of focusing on a specific domain, Model Search is domain agnostic, flexible and is capable of finding the appropriate architecture that best fits a given dataset and problem, while minimizing coding time, effort and compute resources. It is built on Tensorflow, and can run either on a single machine or in a distributed setting.Model Search\nOverview\nThe Model Search system consists of multiple trainers, a search algorithm, a transfer learning algorithm and a database to store the various evaluated models. The system runs both training and evaluation experiments for various ML models (different architectures and training techniques) in an adaptive, yet asynchronous, fashion. While each trainer conducts experiments independently, all trainers share the knowledge gained from their experiments. At the beginning of every cycle, the search algorithm looks up all the completed trials and uses beam search to decide what to try next. It then invokes mutation over one of the best architectures found thus far and assigns the resulting model back to a trainer.beam search\n  The system builds a neural network model from a set of predefined blocks, each of which represents a known micro-architecture, like LSTM, ResNet or Transformer layers. By using blocks of pre-existing architectural components, Model Search is able to leverage existing best knowledge from NAS research across domains. This approach is also more efficient, because it explores structures, not their more fundamental and detailed components, therefore reducing the scale of the search space.ResNet\n  Because the Model Search framework is built on Tensorflow, blocks can implement any function  that takes a tensor as an input. For example, imagine that one wants to introduce a new search space built with a selection of micro architectures. The framework will take the newly defined blocks and incorporate them into the search process so that algorithms can build the best possible neural network from the components provided. The blocks provided can even be fully defined neural networks that are already known to work for the problem of interest. In that case, Model Search can be configured to simply act as a powerful ensembling machine.Tensorflow\nThe search algorithms implemented in Model Search are adaptive, greedy and incremental, which makes them converge faster than RL algorithms. They do however imitate the \u201cexplore & exploit\u201d nature of RL algorithms by separating the search for a good candidate (explore step), and boosting accuracy by ensembling good candidates that were discovered (exploit step). The main search algorithm adaptively modifies one of the top k performing experiments (where k can be specified by the user) after applying random changes to the architecture or the training technique (e.g., making the architecture deeper).greedyexplore & exploit\n  To further improve efficiency and accuracy, transfer learning is enabled between various internal experiments. Model Search does this in two ways \u2014 via knowledge distillation or weight sharing. Knowledge distillation allows one to improve candidates' accuracies by adding a loss term that matches the high performing models\u2019 predictions in addition to the ground truth. Weight sharing, on the other hand, bootstraps some of the parameters (after applying mutation) in the network from previously trained candidates by copying suitable weights from previously trained models and randomly initializing the remaining ones. This enables faster training, which allows opportunities to discover more (and better) architectures.knowledge distillation\nExperimental Results\nModel Search improves upon production models with minimal iterations. In a recent paper, we demonstrated the capabilities of Model Search in the speech domain by discovering a model for keyword spotting and language identification. Over fewer than 200 iterations, the resulting model slightly improved upon internal state-of-the-art production models designed by experts in accuracy using ~130K fewer trainable parameters (184K compared to 315K parameters).recent paperkeyword spottinglanguage identification\n  We also applied Model Search to find an architecture suitable for image classification on the heavily explored CIFAR-10 imaging dataset. Using a set known convolution blocks, including convolutions, resnet blocks (i.e., two convolutions and a skip connection), NAS-A cells, fully connected layers, etc., we observed that we were able to quickly reach a benchmark accuracy of 91.83 in 209 trials (i.e., exploring only 209 models). In comparison, previous top performers reached the same threshold accuracy in 5807 trials for the NasNet algorithm (RL), and 1160 for PNAS (RL + Progressive).CIFAR-10NasNet algorithm (RL)PNAS (RL + Progressive)\nConclusion\nWe hope the Model Search code will provide researchers with a flexible, domain-agnostic framework for ML model discovery. By building upon previous knowledge for a given domain, we believe that this framework is powerful enough to build models with the state-of-the-art performance on well studied problems when provided with a search space composed of standard building blocks.Model Search code\nAcknowledgements\nSpecial thanks to all code contributors to the open sourcing and the paper: Eugen Ehotaj, Scotty Yak, Malaika Handa, James Preiss, Pai Zhu, Aleks Kracun, Prashant Sridhar, Niranjan Subrahmanya, Ignacio Lopez Moreno, Hyun Jin Park, and Patrick Violette.",
      "link": "http://ai.googleblog.com/2021/02/introducing-model-search-open-source.html",
      "author": "Posted by Hanna Mazzawi, Research Engineer and Xavi Gonzalvo, Research Scientist, Google Research"
    },
    {
      "title": "Mastering Atari with Discrete World Models",
      "date": "Thursday, February 18, 2021",
      "abstract": "Mastering Atari with Discrete World Models\nDeep reinforcement learning (RL) enables artificial agents to improve their decisions over time. Traditional model-free approaches learn which of the actions are successful in different situations by interacting with the environment through a large amount of trial and error. In contrast, recent advances in deep RL have enabled model-based approaches to learn accurate world models from image inputs and use them for planning. World models can learn from fewer interactions, facilitate generalization from offline data, enable forward-looking exploration, and allow reusing knowledge across multiple tasks.artificial agentsworld modelslearn from fewer interactionsgeneralization from offline dataforward-looking explorationacross multiple tasks\nDespite their intriguing benefits, existing world models (such as SimPLe) have not been accurate enough to compete with the top model-free approaches on the most competitive reinforcement learning benchmarks \u2014  to date, the well-established Atari benchmark requires model-free algorithms, such as DQN, IQN, and Rainbow, to reach human-level performance. As a result, many researchers have focused instead on developing task-specific planning methods, such as VPN and MuZero, which learn by predicting sums of expected task rewards. However, these methods are specific to individual tasks and it is unclear how well they would generalize to new tasks or learn from unsupervised datasets. Similar to the recent breakthrough of unsupervised representation learning in computer vision [1, 2], world models aim to learn patterns in the environment that are more general than any particular task to later solve tasks more efficiently.SimPLeAtari benchmarkDQNIQNRainbowVPNMuZero12\nToday, in collaboration with DeepMind\u00a0and the University of Toronto, we introduce DreamerV2, the first RL agent based on a world model to achieve human-level performance on the Atari benchmark. It constitutes the second generation of the Dreamer agent that learns behaviors purely within the latent space of a world model trained from pixels. DreamerV2 relies exclusively on general information from the images and accurately predicts future task rewards even when its representations were not influenced by those rewards. Using a single GPU, DreamerV2 outperforms top model-free algorithms with the same compute and sample budget.DeepMindUniversity of TorontoDreamerV2Dreamer agent\nAn Abstract Model of the World\nJust like its predecessor, DreamerV2 learns a world model and uses it to train actor-critic behaviors purely from predicted trajectories. The world model automatically learns to compute compact representations of its images that discover useful concepts, such as object positions, and learns how these concepts change in response to different actions. This lets the agent generate abstractions of its images that ignore irrelevant details and enables massively parallel predictions on a single GPU. During 200 million environment steps, DreamerV2 predicts 468 billion compact states for learning its behavior.train actor-critic behaviors\nDreamerV2 builds upon the Recurrent State-Space Model (RSSM) that we introduced for PlaNet and was also used for DreamerV1. During training, an encoder turns each image into a stochastic representation that is incorporated into the recurrent state of the world model. Because the representations are stochastic, they do not have access to perfect information about the images and instead extract only what is necessary to make predictions, making the agent robust to unseen images. From each state, a decoder reconstructs the corresponding image to learn general representations. Moreover, a small reward network is trained to rank outcomes during planning. To enable planning without generating images, a predictor learns to guess the stochastic representations without access to the images from which they were computed.PlaNetDreamerV1general representations\nImportantly, DreamerV2 introduces two new techniques to RSSM that lead to a substantially more accurate world model for learning successful policies. The first technique is to represent each image with multiple categorical variables instead of the Gaussian variables used by PlaNet, DreamerV1, and many more world models in the literature [1, 2, 3, 4, 5]. This leads the world model to reason about the world in terms of discrete concepts and enables more accurate predictions of future representations.categorical variables12345\nThe encoder turns each image into 32 distributions over 32 classes each, the meanings of which are determined automatically as the world model learns. The one-hot vectors sampled from these distributions are concatenated to a sparse representation that is passed on to the recurrent state. To backpropagate through the samples, we use straight-through gradients that are easy to implement using automatic differentiation. Representing images with categorical variables allows the predictor to accurately learn the distribution over the one-hot vectors of the possible next images. In contrast, earlier world models that use Gaussian predictors cannot accurately match the distribution over multiple Gaussian representations for the possible next images.one-hot vectorsbackpropagatestraight-through gradients\nThe second new technique of DreamerV2 is KL balancing. Many previous world models use the ELBO objective that encourages accurate reconstructions while keeping the stochastic representations (posteriors) close to their predictions (priors) to regularize the amount of information extracted from each image and facilitate generalization. Because the objective is optimized end-to-end, the stochastic representations and their predictions can be made more similar by bringing either of the two towards the other. However, bringing the representations towards their predictions can be problematic when the predictor is not yet accurate. KL balancing lets the predictions move faster toward the representations than vice versa. This results in more accurate predictions, a key to successful planning.ELBO objective\nMeasuring Atari Performance \nDreamerV2 is the first world model that enables learning successful behaviors with human-level performance on the well-established and competitive Atari benchmark. We select the 55 games that many previous studies have in common and recommend this set of games for future work. Following the standard evaluation protocol, the agents are allowed 200M environment interactions using an action repeat of 4 and sticky actions (25% chance that an action is ignored and the previous action is repeated instead). We compare to the top model-free agents IQN and Rainbow, as well as to the well-known C51 and DQN agents implemented in the Dopamine framework.standard evaluation protocolC51DQNDopamine framework\nDifferent standards exist for aggregating the scores across the 55 games. Ideally, a new algorithm would perform better under all conditions. For all four aggregation methods, DreamerV2 indeed outperforms all compared model-free algorithms while using the same computational budget.\nThe first three aggregation methods were previously proposed in the literature. We identify important drawbacks in each and recommend a new aggregation method, the clipped record mean to overcome their drawbacks.DQN papernormalization based on the human world record\nWhile many current algorithms exceed the human gamer baseline, they are still quite far behind the human world record. As shown in the right-most plot above, DreamerV2 leads by achieving 25% of the human record on average across games. Clipping the scores at the record line lets us focus our efforts on developing methods that come closer to the human world record on all of the games rather than exceeding it on just a few games.\nWhat matters and what doesn't\nTo gain insights into the important components of DreamerV2, we conduct an extensive ablation study. Importantly, we find that categorical representations offer a clear advantage over Gaussian representations despite the fact that Gaussians have been used extensively in prior works. KL balancing provides an even more substantial advantage over the KL regularizer used by most generative models.\nBy preventing the image reconstruction or reward prediction gradients from shaping the model states, we study their importance for learning successful representations. We find that DreamerV2 relies completely on universal information from the high-dimensional input images and its representations enable accurate reward predictions even when they were not trained using information about the reward. This mirrors the success of unsupervised representation learning in the computer vision community.\nConclusion\nWe show how to learn a powerful world model to achieve human-level performance on the competitive Atari benchmark and outperform the top model-free agents. This result demonstrates that world models are a powerful approach for achieving high performance on reinforcement learning problems and are ready to use for practitioners and researchers. We see this as an indication that the success of unsupervised representation learning in computer vision\u00a0[1, 2] is now starting to be realized in reinforcement learning in the form of world models. An unofficial implementation of DreamerV2 is available on Github and provides a productive starting point for future research projects. We see world models that leverage large offline datasets, long-term memory, hierarchical planning, and directed exploration as exciting avenues for future research.12available on Github\nAcknowledgements\nThis project is a collaboration with Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. We further thank everybody on the Brain Team and beyond who commented on our paper draft and provided feedback at any point throughout the project.",
      "link": "http://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html",
      "author": "Posted by Danijar Hafner, Student Researcher, Google Research"
    },
    {
      "title": "Rearranging the Visual World",
      "date": "Tuesday, February 16, 2021",
      "abstract": "Rearranging the Visual World\nRearranging objects (such as organizing books on a bookshelf, moving utensils on a dinner table, or pushing piles of coffee beans) is a fundamental skill that can enable robots to physically interact with our diverse and unstructured world. While easy for people, accomplishing such tasks remains an open research challenge for embodied machine learning (ML) systems, as it requires both high-level and low-level perceptual reasoning. For example, when stacking a pile of books, one might consider where the books should be stacked, and in which order, while ensuring that the edges of the books align with each other to form a neat pile.research challenge\nAcross many application areas in ML, simple differences in model architecture can exhibit vastly different generalization properties. Therefore, one might ask whether there are certain deep network architectures that favor simple underlying elements of the rearrangement problem. Convolutional architectures, for example, are common in computer vision as they encode translational invariance, yielding the same response even if an image is shifted, while Transformer architectures are common in language processing because they exploit self-attention to capture long-range contextual dependencies. In robotics applications, one common architectural element is to use object-centric representations such as poses, keypoints, or object descriptors inside learned models, but these representations require additional training data (often manually annotated) and struggle to describe difficult scenarios such as deformables (e.g., playdough), fluids (honey), or piles of stuff (chopped onions).Convolutional architecturesTransformer architecturesself-attentionposeskeypointsobject descriptors\nToday, we present the Transporter Network, a simple model architecture for learning vision-based rearrangement tasks, which appeared as a publication and plenary talk during CoRL 2020. Transporter Nets use a novel approach to 3D spatial understanding that avoids reliance on object-centric representations, making them general for vision-based manipulation but far more sample efficient than benchmarked end-to-end alternatives. As a consequence, they are fast and practical to train on real robots. We are also releasing an accompanying open-source implementation of Transporter Nets together with Ravens, our new simulated benchmark suite of ten vision-based manipulation tasks.Transporter NetworkCoRL 2020Ravens\nTransporter Networks: Rearranging the Visual World for Robotic Manipulation\nThe key idea behind the Transporter Network architecture is that one can formulate the rearrangement problem as learning how to move a chunk of 3D space. Rather than relying on an explicit definition of objects (which is bound to struggle at capturing all edge cases), 3D space is a much broader definition for what could serve as the atomic units being rearranged, and can broadly encompass an object, part of an object, or multiple objects, etc. Transporter Nets leverage this structure by capturing a deep representation of the 3D visual world, then overlaying parts of it on itself to imagine various possible rearrangements of 3D space. It then chooses the rearrangements that best match those it has seen during training (e.g., from expert demonstrations), and uses them to parameterize robot actions. This formulation allows Transporter Nets to generalize to unseen objects and enables them to better exploit geometric symmetries in the data, so that they can extrapolate to new scene configurations. Transporter Nets are applicable to a wide variety of rearrangement tasks for robotic manipulation, expanding beyond our earlier models, such as affordance-based manipulation and TossingBot, that focus only on grasping and tossing.definition of objectsaffordance-based manipulationTossingBot\nRavens Benchmark\nTo evaluate the performance of Transporter Nets in a consistent environment for fair comparisons to baselines and ablations, we developed Ravens, a benchmark suite of ten simulated vision-based rearrangement tasks. Ravens features a Gym API with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods. Ravens avoids assumptions that cannot transfer to a real setup: observation data contains only RGB-D images and camera parameters; actions are end effector poses (transposed into joint positions with inverse kinematics).RavensGymstochastic oracle\nExperiments on these ten tasks show that Transporter Nets are orders of magnitude more sample efficient than other end-to-end methods, and are capable of achieving over 90% success on many tasks with just 100 demonstrations, while the baselines struggle to generalize with the same amount of data. In practice, this makes collecting enough demonstrations a more viable option for training these models on real robots (which we show examples of below).RavensGym\n  Our new Ravens benchmark includes ten simulated vision-based manipulation tasks, including pushing and pick-and-place, for which experiments show that Transporter Nets are orders of magnitude more sample efficient than other end-to-end methods. Ravens features a Gym API with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods.RavensGym\nHighlights\nGiven 10 example demonstrations, Transporter Nets can learn pick and place tasks such as stacking plates (surprisingly easy to misplace!), multimodal tasks like aligning any corner of a box to a marker on the tabletop, or building a pyramid of blocks.\n  By leveraging closed-loop visual feedback, Transporter Nets have the capacity to learn various multi-step sequential tasks with a modest number of demonstrations: such as moving disks for Tower of Hanoi, palletizing boxes, or assembling kits of new objects not seen during training. These tasks have considerably \u201clong horizons\u201d, meaning that to solve the task the model must correctly sequence many individual choices. Policies also tend to learn emergent recovery behaviors.Tower of Hanoi\n  One surprising thing about these results was that beyond just perception, the models were starting to learn behaviors that resemble high-level planning. For example, to solve Towers of Hanoi, the models have to pick which disk to move next, which requires recognizing the state of the board based on the current visible disks and their positions. With a box-palletizing task, the models must locate the empty spaces of the pallet, and identify how new boxes can fit into those voids. Such behaviors are exciting because they suggest that with all the baked-in invariances, the model can focus its capacity on learning the more high-level patterns in manipulation.\nTransporter Nets can also learn tasks that use any motion primitive defined by two end effector poses, such as pushing piles of small objects into a target set, or reconfiguring a deformable rope to connect the two end-points of a 3-sided square. This suggests that rigid spatial displacements can serve as useful priors for nonrigid ones.\nConclusion\nTransporter Nets bring a promising approach to learning vision-based manipulation, but are not without limitations. For example, they can be susceptible to noisy 3D data, we have only demonstrated them for sparse waypoint-based control with motion primitives, and it remains unclear how to extend them beyond spatial action spaces to force or torque-based actions. But overall, we are excited about this direction of work, and we hope that it provides inspiration for extensions beyond the applications we\u2019ve discussed. For more details, please check out our paper.paperAcknowledgements\nThis research was done by Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee, with special thanks to Ken Goldberg, Razvan Surdulescu, Daniel Seita, Ayzaan Wahid, Vincent Vanhoucke, Anelia Angelova, Kendra Byrne, for helpful feedback on writing; Sean Snyder, Jonathan Vela, Larry Bisares, Michael Villanueva, Brandon Hurd for operations and hardware support; Robert Baruch for software infrastructure, Jared Braun for UI contributions; Erwin Coumans for PyBullet advice; Laura Graesser for video narration.",
      "link": "http://ai.googleblog.com/2021/02/rearranging-visual-world.html",
      "author": "Posted by Andy Zeng and Pete Florence, Research Scientists, Robotics at Google"
    },
    {
      "title": "3D Scene Understanding with TensorFlow 3D",
      "date": "Thursday, February 11, 2021",
      "abstract": "3D Scene Understanding with TensorFlow 3D\nThe growing ubiquity of 3D sensors (e.g., Lidar, depth sensing cameras and radar) over the last few years has created a need for scene understanding technology that can process the data these devices capture. Such technology can enable machine learning (ML) systems that use these sensors, like autonomous cars and robots, to navigate and operate in the real world, and can create an improved augmented reality experience on mobile devices. The field of computer vision has recently begun making good progress in 3D scene understanding, including models for mobile 3D object detection, transparent object detection, and more, but entry to the field can be challenging due to the limited availability tools and resources that can be applied to 3D data.Lidardepth sensing camerasradarmobile 3D object detectiontransparent object detection\nIn order to further improve 3D scene understanding and reduce barriers to entry for interested researchers, we are releasing TensorFlow 3D (TF 3D), a highly modular and efficient library that is designed to bring 3D deep learning capabilities into TensorFlow. TF 3D provides a set of popular operations, loss functions, data processing tools, models and metrics that enables the broader research community to develop, train and deploy state-of-the-art 3D scene understanding models.TensorFlow 3D\nTF 3D contains training and evaluation pipelines for state-of-the-art 3D semantic segmentation, 3D object detection and 3D instance segmentation, with support for distributed training. It also enables other potential applications like 3D object shape prediction, point cloud registration and point cloud densification. In addition, it offers a unified dataset specification and configuration for training and evaluation of the standard 3D scene understanding datasets. It currently supports the Waymo Open, ScanNet, and Rio datasets. However, users can freely convert other popular datasets, such as NuScenes and Kitti, into a similar format and use them in the pre-existing or custom created pipelines, and can leverage TF 3D for a wide variety of 3D deep learning research and applications, from quickly prototyping and trying new ideas to deploying a real-time inference system.semantic segmentationobject detectioninstance segmentationdistributed trainingpoint cloud registrationpoint cloud densificationWaymo OpenScanNetRioNuScenesKittiWaymo Open DatasetScanNet dataset\n  Here, we will present the efficient and configurable sparse convolutional backbone that is provided in TF 3D, which is the key to achieving state-of-the-art results on various 3D scene understanding tasks. Furthermore, we will go over each of the three pipelines that TF 3D currently supports: 3D semantic segmentation, 3D object detection and 3D instance segmentation.\n3D Sparse Convolutional Network\nThe 3D data captured by sensors often consists of a scene that contains a set of objects of interest (e.g. cars, pedestrians, etc.) surrounded mostly by open space, which is of limited (or no) interest. As such, 3D data is inherently sparse. In such an environment, standard implementation of convolutions would be computationally intensive and consume a large amount of memory. So, in TF 3D we use submanifold sparse convolution and pooling operations, which are designed to process 3D sparse data more efficiently. Sparse convolutional models are core to the state-of-the-art methods applied in most outdoor self-driving (e.g. Waymo, NuScenes) and indoor benchmarks (e.g. ScanNet).submanifold sparse convolutionpooling\nWe also use various CUDA techniques to speed up the computation (e.g., hashing, partitioning / caching the filter in shared memory, and using bit operations). Experiments on the Waymo Open dataset shows that this implementation is around 20x faster than a well-designed implementation with pre-existing TensorFlow operations.CUDAhashing\nTF 3D then uses the 3D submanifold sparse U-Net architecture to extract a feature for each voxel. The U-Net architecture has proven to be effective by letting the network extract both coarse and fine features and combining them to make the predictions. The U-Net network consists of three modules, an encoder, a bottleneck, and a decoder, each of which consists of a number of sparse convolution blocks with possible pooling or un-pooling operations.U-Net architecture\n  The sparse convolutional network described above is the backbone for the 3D scene understanding pipelines that are offered in TF 3D. Each of the models described below uses this backbone network to extract features for the sparse voxels, and then adds one or multiple additional prediction heads to infer the task of interest. The user can configure the U-Net network by changing the number of encoder / decoder layers and the number of convolutions in each layer, and by modifying the convolution filter sizes, which enables a wide range of speed / accuracy tradeoffs to be explored through the different backbone configurationsencoder / decoder\n3D Semantic Segmentation\nThe 3D semantic segmentation model has only one output head for predicting the per-voxel semantic scores, which are mapped back to points to predict a semantic label per point.3D semantic segmentation modelScanNet dataset\n3D Instance Segmentation\nIn 3D instance segmentation, in addition to predicting semantics, the goal is to group the voxels that belong to the same object together. The 3D instance segmentation algorithm used in TF 3D is based on our previous work on 2D image segmentation using deep metric learning. The model predicts a per-voxel instance embedding vector as well as a semantic score for each voxel. The instance embedding vectors map the voxels to an embedding space where voxels that correspond to the same object instance are close together, while those that correspond to different objects are far apart. In this case, the input is a point cloud instead of an image, and it uses a 3D sparse network instead of a 2D image network. At inference time, a greedy algorithm picks one instance seed at a time, and uses the distance between the voxel embeddings to group them into segments.2D image segmentation using deep metric learninggreedy algorithm\n3D Object Detection\nThe 3D object detection model predicts per-voxel size, center, and rotation matrices and the object semantic scores. At inference time, a box proposal mechanism is used to reduce the hundreds of thousands of per-voxel box predictions into a few accurate box proposals, and then at training time, box prediction and classification losses are applied to per-voxel predictions. We apply a Huber loss on the distance between predicted and the ground-truth box corners. Since the function that estimates the box corners from its size, center and rotation matrix is differentiable, the loss will automatically propagate back to those predicted object properties.  We use a dynamic box classification loss that classifies a box that strongly overlaps with the ground-truth as positive and classifies the non-overlapping boxes as negative.Huber loss\n  In our recent paper, \u201cDOPS: Learning to Detect 3D Objects and Predict their 3D Shapes\u201d, we describe in detail the single-stage weakly supervised learning algorithm used for object detection in TF 3D. In addition, in a follow up work, we extended the 3D object detection model to leverage temporal information by proposing a sparse LSTM-based multi-frame model. We go on to show that this temporal model outperforms the frame-by-frame approach by 7.5% in the Waymo Open dataset.DOPS: Learning to Detect 3D Objects and Predict their 3D Shapessparse LSTM-based multi-frame modelWaymo Open datasetDOPS paper\nReady to Get Started?\nWe\u2019ve certainly found this codebase to be useful for our 3D computer vision projects, and we hope that you will as well. Contributions to the codebase are welcome and please stay tuned for our own further updates to the framework. To get started please visit our github repository.our github repository\nAcknowledgements\nThe release of the TensorFlow 3D codebase and model has been the result of widespread collaboration among Google researchers with feedback and testing from product groups. In particular we want to highlight the core contributions by Alireza Fathi and Rui Huang (work performed while at Google), with special additional thanks to Guangda Lai, Abhijit Kundu, Pei Sun, Thomas Funkhouser, David Ross, Caroline Pantofaru, Johanna Wald, Angela Dai and Matthias Niessner.",
      "link": "http://ai.googleblog.com/2021/02/3d-scene-understanding-with-tensorflow.html",
      "author": "Posted by Alireza Fathi, Research Scientist and Rui Huang, AI Resident, Google Research"
    },
    {
      "title": "Uncovering Unknown Unknowns in Machine Learning",
      "date": "Thursday, February 11, 2021",
      "abstract": "Uncovering Unknown Unknowns in Machine Learning\nThe performance of machine learning (ML) models depends both on the learning algorithms, as well as the data used for training and evaluation. The role of the algorithms is well studied and the focus of a multitude of challenges, such as SQuAD, GLUE, ImageNet, and many others. In addition, there have been efforts to also improve the data, including a series of workshops addressing issues for ML evaluation.  In contrast, research and challenges that focus on the data used for evaluation of ML models are not commonplace. Furthermore, many evaluation datasets contain items that are easy to evaluate, e.g., photos with a subject that is easy to identify, and thus they miss the natural ambiguity of real world context. The absence of ambiguous real-world examples in evaluation undermines the ability to reliably test machine learning performance, which makes ML models prone to develop \u201cweak spots\u201d, i.e., classes of examples that are difficult or impossible for a model to accurately evaluate, because that class of examples is missing from the evaluation set.SQuADGLUEImageNetseries of workshopsundermines the ability to reliably test machine learning performance\nTo address the problem of identifying these weaknesses in ML models, we recently launched the Crowdsourcing Adverse Test Sets for Machine Learning (CATS4ML) Data Challenge at HCOMP 2020 (open until 30 April, 2021 to researchers and developers worldwide). The goal of the challenge is to raise the bar in ML evaluation sets and to find as many examples as possible that are confusing or otherwise problematic for algorithms to process. CATS4ML relies on people\u2019s abilities and intuition to spot new data examples about which machine learning is confident, but actually misclassifies.Crowdsourcing Adverse Test Sets for Machine LearningHCOMP 2020\nWhat are ML \u201cWeak Spots\u201d?\nThere are two categories of weak spots: known unknowns and unknown unknowns. Known unknowns are examples for which a model is unsure about the correct classification. The research community continues to study this in a field known as active learning, and has found the solution to be, in very general terms, to interactively solicit new labels from people on uncertain examples.  For example, if a model is not certain whether or not the subject of a photo is a cat, a person is asked to verify; but if the system is certain, a person is not asked. While there is room for improvement in this area, what is comforting is that the confidence of the model is correlated with its performance, i.e., one can see what the model doesn\u2019t know.active learning\nUnknown unknowns, on the other hand, are examples where a model is confident about its answer, but is actually wrong. Efforts to proactively discover unknown unknowns (e.g., Attenberg 2015 and Crawford 2019) have helped uncover a multitude of unintended machine behaviours. In contrast to such approaches for the discovery of unknown unknowns, generative adversarial networks (GANs) generate unknown unknowns for image recognition models in the form of optical illusions for computers that cause deep learning models to make mistakes beyond human perception. While GANs uncover model exploits in the event of an intentional manipulation, real-world examples can better highlight a model\u2019s failures in its day-to-day performance. These real-world examples are the unknown unknowns of interest to CATS4ML \u2014 the challenge aims to gather unmanipulated examples that humans can reliably interpret but on which many ML models would confidently disagree.Attenberg 2015Crawford 2019generative adversarial networksgenerate unknown unknownsBrown 2018\nFirst Edition of CATS4ML Data Challenge: Open Images Dataset\nThe CATS4ML Data Challenge focuses on visual recognition, using images and labels from the Open Images Dataset. The target images for the challenge are selected from the Open Images Dataset along with a set of 24 target labels from the same dataset. The challenge participants are invited to invent new and creative ways to explore this existing publicly available dataset and, focussed on a list of pre-selected target labels, discover examples of unknown unknowns for ML models.CATS4ML Data ChallengeOpen Images DatasetOpen Images DatasetCATS4ML is a complementary effort to FAIR\u2019s recently introduced DynaBench research platform for dynamic data collection. Where DynaBench tackles issues with static benchmarks using ML models with humans in the loop, CATS4ML focuses on improving evaluation datasets for ML by encouraging the exploration of existing ML benchmarks for adverse examples that can be unknown unknowns. The results will help detect and avoid future errors, and also will give insights to model explainability.CATS4MLFAIRDynaBench\nIn this way, CATS4ML aims to raise greater awareness of the problem by providing dataset resources that developers can use to uncover the weak spots of their algorithms. This will also inform researchers on how to create benchmark datasets for machine learning that are more balanced, diverse and socially aware.\nGet Involved\nWe invite the global community of ML researchers and practitioners to join us in the effort of discovering interesting, difficult examples from the Open Images Dataset. Register on the challenge website, download the target images and labeled data, contribute the images you discover and join the competition for the winning participant!challenge website\nTo score points in this competition, participants should submit a set of image-label pairs that will be confirmed by human-in-the-loop raters, whose votes should be in disagreement with the average machine score for the label over a number of machine learning models.\nThe challenge is open until 30 April, 2021 to researchers and developers worldwide. To learn more about CATS4ML and how to join, please\u00a0visit the challenge website.challenge website\nAcknowledgements\nThe release of the CATS4ML Data Challenge has been possible thanks to the hard work of a lot of people including, but not limited to, the following (in alphabetical order of last name): Osman Aka, Ken Burke, Tulsee Doshi, Mig Gerard, Victor Gomes, Shahab Kamali, Igor Karpov, Devi Krishna, Daphne Luong, Carey Radebaugh, Jamie Taylor, Nithum Thain, Kenny Wibowo, Ka Wong, and Tong Zhou.",
      "link": "http://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html",
      "author": "Posted by Lora Aroyo and Praveen Paritosh, Research Scientists, Google Research"
    },
    {
      "title": "TracIn \u2014 A Simple Method to Estimate Training Data Influence",
      "date": "Friday, February 5, 2021",
      "abstract": "TracIn \u2014 A Simple Method to Estimate Training Data Influence\nThe quality of a machine learning (ML) model\u2019s training data can have a significant impact on its performance. One measure of data quality is the notion of influence, i.e., the degree to which a given training example affects the model and its predictive performance. And while influence is a well-known concept to ML researchers, the complexity behind deep learning models, coupled with their growing size, features and datasets, have made the quantification of influence difficult.\nA few methods have been proposed recently to quantify influence. Some rely on changes in accuracy when retraining with one or several data points dropped, and some use established statistical methods, e.g., influence functions that estimate the impact of perturbing input points or representer methods that decompose a prediction into an importance weighted combination of training examples. Still other approaches require use of additional estimators, such as data valuation using reinforcement learning. Though these approaches are theoretically sound, their use in products has been limited by the resources needed to run them at scale or the additional burdens they place on training.influence functionsrepresenter methodsdata valuation using reinforcement learning\nIn \u201cEstimating Training Data Influence by Tracing Gradient Descent\u201d, published as a spotlight paper at NeurIPS 2020, we proposed TracIn, a simple scalable approach to tackle this challenge. The idea behind TracIn is straightforward \u2014 trace the training process to capture changes in prediction as individual training examples are visited. TracIn is effective in finding mislabeled examples and outliers from a variety of datasets, and is useful in explaining predictions in terms of training examples (as opposed to features) by assigning an influence score to each training example.Estimating Training Data Influence by Tracing Gradient DescentNeurIPS 2020\nThe Ideas Underlying TracIn\nDeep learning algorithms are typically trained using an algorithm called stochastic gradient descent (SGD), or a variant of it. SGD operates by making multiple passes over the data and making modifications to the model parameters that locally reduce the loss (i.e., the model\u2019s objective) with each pass. An example of this is demonstrated for an image classification task in the figure below, where the model\u2019s task is to predict the subject of the test image on the left (\u201czucchini\u201d). As the model progresses through training, it is exposed to various training examples that affect the loss on the test image, where the loss is a function both of the prediction score and the actual label \u2014 the higher the prediction score for zucchini, the lower the loss.stochastic gradient descent\nSuppose that the test example is known at training time and that the training process visited each training example one at a time. During the training, visiting a specific training example would change the model\u2019s parameters, and that change would then modify the prediction/loss on the test example. If one could trace the training example through the process, then the change in loss or prediction on the test example could be attributed to the training example in question, where the influence of a training example would be the cumulative attribution across visits to the training example.\nThere are two types of relevant training examples. Those that reduce loss, like the images of zucchinis above, are called proponents, while those that increase loss, like the images of seatbelts, are called opponents. In the example above, the image labeled \u201csunglasses\u201d is also a proponent, because it has a seatbelt in the image, but is labeled as \u201csunglasses,\u201d driving the model to better distinguish between zucchini and seatbelts.\nIn practice, the test example is unknown at training time, a limitation that can be overcome by using the checkpoints output by the learning algorithm as a sketch of the training process. Another challenge is that the learning algorithm typically visits several points at once, not individually, which requires a method to disentangle the relative contributions of each training example. This can be done by applying pointwise loss gradients. Together, these two strategies capture the TracIn method, which can be reduced to the simple form of the dot product of loss gradients of the test and training examples, weighted by the learning rate, and summed across checkpoints.\nAlternatively, one could instead examine the influence on the prediction score, which would be useful if the test example has no label. This form simply requires the substitution of the loss gradient at the test example with the prediction gradient.\nComputing Top Influence Examples \nWe illustrate the utility of TracIn by first calculating the loss gradient vector for some training data and a test example for a specific classification \u2014 an image of a chameleon \u2014 and then leveraging a standard k-nearest neighbors library to retrieve the top proponents and opponents. The top opponents indicate the chameleon\u2019s ability to blend in! For comparison, we also show the k nearest neighbors with embeddings from the penultimate layer. Proponents are images that are not only similar, but also belong to the same class, and opponents are similar images but in a different class. Note that there isn\u2019t an explicit enforcement on whether proponents or opponents belong to the same class.k-nearest neighbors\nClustering \nThe simplistic breakdown of the loss of the test example into training example influences given by TracIn also suggests that the loss (or prediction) from any gradient descent based neural model can be expressed as a sum of similarities in the space of gradients. Recent work has demonstrated that this functional form is similar to that of a kernel, implying that this gradient similarity described here can be applied to other similarity tasks, like clustering.Recent workkernel\nIn this case, TracIn can be used as a similarity function within a clustering algorithm. To bound the similarity metric so that it can be converted to a distance measure (1 - similarity), we normalize the gradient vectors to have unit norm. Below, we apply TracIn clustering on images of zucchini to obtain finer clusters.similarity functionclustering algorithm\nIdentifying Outliers with Self-Influence\nFinally, we can also use TracIn to identify outliers that exhibit a high self-influence, i.e., the influence of a training point on its own prediction. This happens either when the example is mislabeled or rare, both of which make it difficult for the model to generalize over the example. Below are some examples with high self-influence.\nApplications\nHaving no requirement other than being trained using SGD (or related variants), TracIn is task-independent and applicable to a variety of models. For example, we have used TracIn to study training data for a deep learning model used to parse queries to the Google Assistant, queries of the kind \u201cset my alarm for 7AM\u201d. We were intrigued to see that the top opponent for the query \u201cdisable my alarm\u201d with an alarm active on the device, was \u201cdisable my timer\u201d, also with an alarm active on the device. This suggests that Assistant users often interchange the words \u201ctimer\u201d and \u201calarm\u201d. TracIn helped us interpret the Assistant data.\nMore examples can be found in the paper, including a regression task on structured data and a number of text classification tasks.paper\nConclusion\nTracIn is a simple, easy-to-implement, scalable way to compute the influence of training data examples on individual predictions or to find rare and mislabeled training examples. For implementation references of the method, you can find a link to code examples for images from the github linked in the paper.paper\nAcknowledgements\nThe NeurIPS paper was jointly co-authored\u00a0with Satyen Kale and Mukund Sundararajan (corresponding author). A special thanks to Binbin Xiong for providing various conceptual and implementation insights. We also thank Qiqi Yan and\u00a0Salem Haykal for numerous discussions. Images throughout this post sourced from Getty Images.Getty Images",
      "link": "http://ai.googleblog.com/2021/02/tracin-simple-method-to-estimate.html",
      "author": "Posted by Frederick Liu and Garima Pruthi, Software Engineers, Google Research"
    },
    {
      "title": "Machine Learning for Computer Architecture",
      "date": "Thursday, February 4, 2021",
      "abstract": "Machine Learning for Computer Architecture\nOne of the key contributors to recent machine learning (ML) advancements is the development of custom accelerators, such as Google TPUs and Edge TPUs, which significantly increase available compute power unlocking various capabilities such as AlphaGo, RankBrain, WaveNets, and Conversational Agents. This increase can lead to improved performance in neural network training and inference, enabling new possibilities in a broad range of applications, such as vision, language, understanding, and self-driving cars.Google TPUsEdge TPUsAlphaGoRankBrainWaveNetsConversational Agentsvisionlanguageunderstandingself-driving cars\nTo sustain these advances, the hardware accelerator ecosystem must continue to innovate in architecture design and acclimate to rapidly evolving ML models and applications. This requires the evaluation of many different accelerator design points, each of which may not only improve the compute power, but also unravel a new capability. These design points are generally parameterized by a variety of hardware and software factors (e.g., memory capacity, number of compute units at different levels, parallelism, interconnection networks, pipelining, software mapping, etc.). This is a daunting optimization task, due to the fact that the search space is exponentially large1 while the objective function (e.g., lower latency and/or higher energy efficiency) is computationally expensive to evaluate through simulations or synthesis, making identification of feasible accelerator configurations challenging .1\nIn \u201cApollo: Transferable Architecture Exploration\u201d, we present the progress of our research on ML-driven design of custom accelerators. While recent work has demonstrated promising results in leveraging ML to improve the low-level floorplanning process (in which the hardware components are spatially laid out and connected in silicon), in this work we focus on blending ML into the high-level system specification and architectural design stage, a pivotal contributing factor to the overall performance of the chip in which the design elements that control the high-level functionality are established. Our research shows how ML algorithms can facilitate architecture exploration and suggest high-performing architectures across a range of deep neural networks, with domains spanning image classification, object detection, OCR and semantic segmentation.Apollo: Transferable Architecture Explorationrecentworkimage classificationobject detectionOCRsemantic segmentation\nArchitecture Search Space and Workloads\nThe objective in architecture exploration is to discover a set of feasible accelerator parameters for a set of workloads, such that a desired objective function (e.g., the weighted average of runtime) is minimized under an optional set of user-defined constraints. However, the manifold of architecture search generally contains many points for which there is no feasible mapping from software to hardware. Some of these design points are known a priori and can be bypassed by formulating them as optimization constraints by the user (e.g., in the case of an area budget2 constraint, the total memory size must not pass over a predefined limit). However, due to the interplay of the architecture and compiler and the complexity of the search space, some of the constraints may not be properly formulated into the optimization, and so the compiler may not find a feasible software mapping for the target hardware. These infeasible points are not easy to formulate in the optimization problem, and are generally unknown until the whole compiler pass is performed. As such, one of main challenges for architecture exploration is to effectively sidestep the infeasible points for efficient exploration of the search space with a minimum number of cycle-accurate architecture simulations.2\nThe following figure shows the overall architecture search space of a target ML accelerator. The accelerator contains a 2D array of processing elements (PE), each of which performs a set of arithmetic computations in a single instruction multiple data (SIMD) manner. The main architectural components of each PE are processing cores that include multiple compute lanes for SIMD operations. Each PE has shared memory (PE Memory) across all their compute cores, which is mainly used to store model activations, partial results, and outputs, while individual cores feature memory that is mainly used for storing model parameters. Each core has multiple compute lanes with multi-way multiply-accumulate (MAC) units. The results of model computations at each cycle are either stored back in the PE memory for further computation or are offloaded back into the DRAM.single instruction multiple datamultiply-accumulate\nOptimization Strategies\nIn this study, we explored four optimization strategies in the context of architecture exploration:VizierBayesian optimizationGaussian processexpected improvementEvolutionaryevolutionary searchtournament selectingPopulation-based black-box optimization\nAccelerator Search Space Embeddings\nTo better visualize the effectiveness of each optimization strategy in navigating the accelerator search space, we use t-distributed stochastic neighbor embedding\u00a0(t-SNE) to map the explored configurations into a two-dimensional space across the optimization horizon. The objective (reward) for all the experiments is defined as the throughput (inference/second) per accelerator area. In the figures below, the x and y axes indicate the t-SNE components (embedding 1 and embedding 2) of the embedding space. The star and circular markers show the infeasible (zero reward) and feasible design points, respectively, with the size of the feasible points corresponding to their reward.t-distributed stochastic neighbor embedding\nAs expected, the random strategy searches the space in a uniformly distributed way and eventually finds very few feasible points in the design space.\nCompared to the random sampling approach, the Vizier default optimization strategy strikes a good balance between exploring the search space and finding the design points with higher rewards (1.14 vs. 0.96). However, this approach tends to get stuck in infeasible regions and, while it does find a few points with the maximum reward (indicated by the red cross markers), it finds few feasible points during the last iterations of exploration.VizierVizier\nThe evolutionary optimization strategy, on the other hand, finds feasible solutions very early in the optimization and assemble clusters of feasible points around them. As such, this approach mostly navigates the feasible regions (the green circles) and efficiently sidesteps the infeasible points. In addition, the evolutionary search is able to find more design options with maximum reward (the red crosses). This diversity in the solutions with high reward provides flexibility to the designer in exploring various architectures with different design trade-offs.evolutionary\nFinally, the population-based optimization method (P3BO) explores the design space in a more targeted way (regions with high reward points) in order to find optimal solutions. The P3BO strategy finds design points with the highest reward in search spaces with tighter constraints (e.g., a larger number of infeasible points), showing its effectiveness in navigating search spaces with large numbers of infeasible points.P3BO\nArchitecture Exploration under Different Design Constraints\nWe also studied the benefits of each optimization strategy under different area budget constraints, 6.8 mm2, 5.8 mm2 and 4.8 mm2. The following violin plots  show the full distribution of the maximum achievable reward at the end of optimization (after ten runs each with 4K trials) across the studied optimization strategies. The wider sections represent a higher probability of observing feasible architecture configurations at a particular given reward. This implies that we favor the optimization algorithm that yields increased width at the points with higher reward (higher performance).\nThe two top-performing optimization strategies for architecture exploration are evolutionary and P3BO, both in terms of delivering solutions with high reward and robustness across multiple runs. Looking into different design constraints, we observe that as one tightens the area budget constraint, the P3BO optimization strategy yields more high performing solutions. For example, when the area budget constraint is set to 5.8 mm2, P3BO finds design points with a reward (throughput / accelerator area)  of 1.25 outperforming all the other optimization strategies. The same trend is observed when the area budget constraint is set to 4.8 mm2, a slightly better reward is found with more robustness (less variability) across multiple runs.Violin plot\nConclusion\nWhile Apollo presents the first step towards better understanding of accelerator design space and building more efficient hardware, inventing accelerators with new capabilities is still an uncharted territory and a new frontier. We believe that this research is an exciting path forward to further explore ML-driven techniques for architecture design  and co-optimization (e.g., compiler, mapping, and scheduling) across the computing stack to  invent  efficient accelerators with new capabilities for the next generation of applications.Apollo\nAcknowledgments\nThis work was performed by Amir Yazdanbakhsh, Christof Angermueller, and Berkin Akin . We would like to also thank Milad Hashemi, Kevin Swersky, James Laudon, Herman Schmit, Cliff Young, Yanqi Zhou, Albin Jones, Satrajit Chatterjee, Ravi Narayanaswami, Ray (I-Jui) Sung, Suyog Gupta, Kiran Seshadri, Suvinay Subramanian, Matthew Denton, and the Vizier team for their help and support.Vizier team\n1 In our target accelerator, the total number of design points is around 5 x 108.\u00a0\u21a91\u21a9\n2 The chip area is approximately the sum of total hardware components on the chip, including on-chip storage, processing engines, controllers, I/O pins, and etc.  \u00a0\u21a92\u21a9",
      "link": "http://ai.googleblog.com/2021/02/machine-learning-for-computer.html",
      "author": "Posted by Amir Yazdanbakhsh, Research Scientist, Google Research"
    },
    {
      "title": "Evaluating Design Trade-offs in Visual Model-Based Reinforcement Learning",
      "date": "Wednesday, February 3, 2021",
      "abstract": "Evaluating Design Trade-offs in Visual Model-Based Reinforcement Learning\nModel-free reinforcement learning has been successfully demonstrated across a range of domains, including robotics, control, playing games and autonomous vehicles. These systems learn by simple trial and error and thus require a vast number of attempts at a given task before solving it. In contrast, model-based reinforcement learning (MBRL) learns a model of the environment (often referred to as a world model or a dynamics model) that enables the agent to predict the outcomes of potential actions, which reduces the amount of environment interaction needed to solve a task.Model-free reinforcement learningroboticscontrolplaying gamesautonomous vehiclesmodel-based reinforcement learningworld model\nIn principle, all that is strictly necessary for planning is to predict future rewards, which could then be used to select near-optimal future actions. Nevertheless, many recent methods, such as Dreamer, PlaNet, and SimPLe, additionally leverage the training signal of predicting future images. But is predicting future images actually necessary, or helpful? What benefit do visual MBRL algorithms actually derive from also predicting future images? The computational and representational cost of predicting entire images is considerable, so understanding whether this is actually useful is of profound importance for MBRL research.DreamerPlaNetSimPLe\nIn \u201cModels, Pixels, and Rewards: Evaluating Design Trade-offs in Visual Model-Based Reinforcement Learning\u201d, we demonstrate that predicting future images provides a substantial benefit, and is in fact a key ingredient in training successful visual MBRL agents. We developed a new open-source library, called the World Models Library, which enabled us to rigorously evaluate various world model designs to determine the relative impact of image prediction on returned rewards for each.Models, Pixels, and Rewards: Evaluating Design Trade-offs in Visual Model-Based Reinforcement LearningWorld Models Library\nWorld Models Library\nThe World Models Library, designed specifically for visual MBRL training and evaluation, enables the empirical study of the effects of each design decision on the final performance of an agent across multiple tasks on a large scale. The library introduces a platform-agnostic visual MBRL simulation loop and the APIs to seamlessly define new world-models, planners and tasks or to pick and choose from the existing catalog, which includes agents (e.g., PlaNet), video models (e.g., SV2P), and a variety of DeepMind Control tasks and planners, such as CEM and MPPI.PlaNetSV2PDeepMind ControlCEMMPPI\nUsing the library,  developers can study the effect of a varying factor in MBRL, such as the model design or representation space, on the performance of the agent on a suite of tasks. The library supports the training of the agents from scratch, or on a pre-collected set of trajectories, as well as evaluation of a pre-trained agent on a given task. The models, planning algorithms and the tasks can be easily mixed and matched to any desired combination.\nTo provide the greatest flexibility for users, the library is built using the NumPy interface, which enables different components to be implemented in either TensorFlow, Pytorch or JAX. Please look at this colab for a quick introduction.NumPyTensorFlowPytorchJAXthis colab\nImpact of Image Prediction\nUsing the World Models Library, we trained multiple world models with different levels of image prediction. All of these models use the same input (previously observed images) to predict an image and a reward, but they differ on what percentage of the image they predict. As the number of image pixels predicted by the agent increases, the agent performance as measured by the true reward generally improves.\nInterestingly, the correlation between reward prediction accuracy and agent performance is not as strong, and in some cases a more accurate reward prediction can even result in lower agent performance. At the same time, there is a strong correlation between image reconstruction error and the performance of the agent.\nThis phenomenon is directly related to exploration, i.e., when the agent attempts more risky and potentially less rewarding actions in order to collect more information about the unknown options in the environment. This can be shown by testing and comparing models in an offline setup (i.e., learning policies from pre-collected datasets, as opposed to online RL, which learns policies by interacting with an environment). An offline setup ensures that there is no exploration and all of the models are trained on the same data. We observed that models that fit the data better usually perform better in the offline setup, and surprisingly, these may not be the same models that perform the best when learning and exploring from scratch.explorationoffline setup\nConclusion\nWe have empirically demonstrated that predicting images can substantially improve task performance over models that only predict the expected reward. We have also shown that the accuracy of image prediction strongly correlates with the final task performance of these models. These findings can be used for better model design and can be particularly useful for any future setting where the input space is high-dimensional and collecting data is expensive.\nIf you'd like to develop your own models and experiments, head to our repository and colab where you'll find instructions on how to reproduce this work and use or extend the World Models Library.repositorycolab\nAcknowledgement:\nWe would like to give special recognition to multiple researchers in the Google Brain team and co-authors of the paper: Mohammad Taghi Saffar, Danijar Hafner, Harini Kannan, Chelsea Finn and Sergey Levine.",
      "link": "http://ai.googleblog.com/2021/02/evaluating-design-trade-offs-in-visual.html",
      "author": "Posted by Mohammad Babaeizadeh, Research Engineer and Dumitru Erhan, Research Scientist, Google Research"
    },
    {
      "title": "Learning to Reason Over Tables from Less Data",
      "date": "Friday, January 29, 2021",
      "abstract": "Learning to Reason Over Tables from Less Data\nThe task of recognizing textual entailment, also known as natural language inference, consists of determining whether a piece of text (a premise), can be implied or contradicted (or neither) by another piece of text (the hypothesis). While this problem is often considered an important test for the reasoning skills of machine learning (ML) systems and has been studied in depth for plain text inputs, much less effort has been put into applying such models to structured data, such as websites, tables, databases, etc. Yet, recognizing textual entailment is especially relevant whenever the contents of a table need to be accurately summarized and presented to a user, and is essential for high fidelity question answering systems and virtual assistants.textual entailmentquestion answeringvirtual assistants\nIn \"Understanding tables with intermediate pre-training\", published in Findings of EMNLP 2020, we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data. We build upon our earlier TAPAS model, which was an extension of the BERT bi-directional Transformer model with special embeddings to find answers in tables. Applying our new pre-training objectives to TAPAS yields a new state of the art on multiple datasets involving tables. On TabFact, for example, it reduces the gap between model and human performance by ~50%. We also systematically benchmark methods of selecting relevant input for higher efficiency, achieving 4x gains in speed and memory, while retaining 92% of the results. All the models for different tasks and sizes are released on GitHub repo, where you can try them out yourself in a colab Notebook.Understanding tables with intermediate pre-trainingFindings of EMNLP 2020TAPASBERTTransformerTabFactGitHub repocolab\nTextual Entailment\nThe task of textual entailment is more challenging when applied to tabular data than plain text. Consider, for example, a table from Wikipedia with some sentences derived from its associated table content. Assessing if the content of the table entails or contradicts the sentence may require looking over multiple columns and rows, and possibly performing simple numeric computations, like averaging, summing, differencing, etc.TabFact\nFollowing the methods used by TAPAS, we encode the content of a statement and a table together, pass them through a Transformer model, and obtain a single number with the probability that the statement is entailed or refuted by the table.TAPAS model\nBecause the only information in the training examples is a binary value (i.e., \"correct\" or \"incorrect\"), training a model to understand whether a statement is entailed or not is challenging and highlights the difficulty in achieving generalization in deep learning, especially when the provided training signal is scarce. Seeing isolated entailed or refuted examples, a model can easily pick-up on spurious patterns in the data to make a prediction, for example the presence of the word \"tie\" in \"Greg Norman and Billy Mayfair tie in rank\", instead of truly comparing their ranks, which is what is needed to successfully apply the model beyond the original training data.\nPre-training Tasks\nPre-training tasks can be used to \u201cwarm-up\u201d models by providing them with large amounts of readily available unlabeled data. However, pre-training typically includes primarily plain text and not tabular data. In fact, TAPAS was originally pre-trained using a simple masked language modelling objective that was not designed for tabular data applications. In order to improve the model performance on tabular data, we introduce two novel pretraining binary-classification tasks called counterfactual and synthetic, which can be applied as a second stage of pre-training (often called intermediate pre-training).\nIn the counterfactual task, we source sentences from Wikipedia that mention an entity (person, place or thing) that also appears in a given table. Then, 50% of the time, we modify the statement by swapping the entity for another alternative. To make sure the statement is realistic, we choose a replacement among the entities in the same column in the table. The model is trained to recognize whether the statement was modified or not. This pre-training task includes millions of such examples, and although the reasoning about them is not complex, they typically will still sound natural.\nFor the synthetic task, we follow a method similar to semantic parsing in which we generate statements using a simple set of grammar rules that require the model to understand basic mathematical operations, such as sums and averages (e.g., \"the sum of earnings\"), or to understand how to filter the elements in the table using some condition (e.g.,\"the country is Australia\"). Although these statements are artificial, they help improve the numerical and logical reasoning skills of the model.semantic parsing\nResults\nWe evaluate the success of the counterfactual and synthetic pre-training objectives on the TabFact dataset by comparing to the baseline TAPAS model and to  two prior models that have exhibited success in the textual entailment domain, LogicalFactChecker (LFC) and Structure Aware Transformer (SAT). The baseline TAPAS model exhibits improved performance relative to LFC and SAT, but the pre-trained model (TAPAS+CS) performs significantly better, achieving a new state of the art.LogicalFactCheckerStructure Aware Transformer\nWe also apply TAPAS+CS to question answering tasks on the SQA dataset, which requires that the model find answers from the content of tables in a dialog setting. The inclusion of CS objectives improves the previous best performance by more than 4 points, demonstrating that this approach also generalizes performance beyond just textual entailment.SQA dataset\nData and Compute Efficiency\nAnother aspect of the counterfactual and synthetic pre-training tasks is that since the models are already tuned for binary classification, they can be applied without any fine-tuning to TabFact. We explore what happens to each of the models when trained only on a subset (or even none) of the data. Without looking at a single example, the TAPAS+CS model is competitive with a strong baseline Table-Bert, and when only 10% of the data are included, the results are comparable to the previous state-of-the-art.\nA general concern when trying to use large models such as this to operate on tables, is that  their high computational requirements makes it difficult for them to parse very large tables. To address this, we investigate whether one can heuristically select subsets of the input to pass through the model in order to optimize its computational efficiency.\nWe conducted a systematic study of different approaches to filter the input and discovered that simple methods that select for word overlap between a full column and the subject statement give the best results. By dynamically selecting which tokens of the input to include, we can use fewer resources or work on larger inputs at the same cost. The challenge is doing so without losing important information and hurting accuracy.For instance, the models discussed above all use sequences of 512 tokens, which is around the normal limit for a transformer model (although recent efficiency methods like the Reformer or Performer are proving effective in scaling the input size). The column selection methods we propose here can allow for faster training while still achieving high accuracy on TabFact. For 256 input tokens we get a very small drop in accuracy, but the model can now be pre-trained, fine-tuned and make predictions up to two times faster. With 128 tokens the model still outperforms the previous state-of-the-art model, with an even more significant speed-up \u2014 4x faster across the board.ReformerPerformer\nUsing both the column selection method we proposed and the novel pre-training tasks, we can create table parsing models that need fewer data and less compute power to obtain better results.\nWe have made available the new models and pre-training techniques at our GitHub repo, where you can try it out yourself in colab. In order to make this approach more accessible, we also shared models of varying sizes all the way down to \u201ctiny\u201d. It is our hope that these results will help spur development of table reasoning among the broader research community.our GitHub repocolabtiny\nAcknowledgements\nThis work was carried out by Julian Martin Eisenschlos, Syrine Krichene and Thomas M\u00fcller from our Language Team in Z\u00fcrich. We would like to thank Jordan Boyd-Graber, Yasemin Altun, Emily Pitler, Benjamin Boerschinger, Srini Narayanan, Slav Petrov, William Cohen and Jonathan Herzig for their useful comments and suggestions.Language Team",
      "link": "http://ai.googleblog.com/2021/01/learning-to-reason-over-tables-from.html",
      "author": "Posted by Julian Eisenschlos, AI Resident, Google Research, Z\u00fcrich"
    },
    {
      "title": "Improving Mobile App Accessibility with Icon Detection",
      "date": "Thursday, January 28, 2021",
      "abstract": "Improving Mobile App Accessibility with Icon Detection\nVoice Access enables users to control their Android device hands free, using only verbal commands. In order to function properly, it needs on-screen user interface (UI) elements to have reliable accessibility labels, which are provided to the operating system\u2019s accessibility services via the accessibility tree. Unfortunately, in many apps, adequate labels aren\u2019t always available for UI elements, e.g. images and icons, reducing the usability of Voice Access.Voice Accessaccessibility labelsaccessibility tree\nAddressing this challenge requires a system that can automatically detect icons using only the pixel values displayed on the screen, regardless of whether icons have been given suitable accessibility labels. What little research exists on this topic typically uses classifiers, sometimes combined with language models to infer classes and attributes from UI elements. However, these classifiers still rely on the accessibility tree to obtain bounding boxes for UI elements, and fail when appropriate labels do not exist.classifierslanguage models\nHere, we describe IconNet, a vision-based object detection model that can automatically detect icons on the screen in a manner that is agnostic to the underlying structure of the app being used, launched as part of the latest version of Voice Access. IconNet can detect 31 different icon types (to be extended to more than 70 types soon) based on UI screenshots alone. IconNet is optimized to run on-device for mobile environments, with a compact size and fast inference time to enable a seamless user experience. The current IconNet model achieves a mean average precision (mAP) of 94.2% running at 9 FPS on a Pixel 3A.mean average precision\nDetecting Icons in Screenshots\nFrom a technical perspective, the problem of detecting icons on app screens is similar to classical object detection, in that individual elements are labelled by the model with their locations and sizes. But, in other ways, it\u2019s quite different. Icons are typically small objects, with relatively basic geometric shapes and a limited range of colors, and app screens widely differ from natural images in that they are more structured and geometrical.\nA significant challenge in the development of an on-device UI element detector for Voice Access is that it must be able to run on a wide variety of phones with a range of performance performance capabilities, while preserving the user\u2019s privacy. For a fast user experience, a lightweight model with low inference latency is needed. Because Voice Access needs to use the labels in response to an utterance from a user (e.g., \u201ctap camera\u201d, or \u201cshow labels\u201d) inference time needs to be short (<150 ms on a Pixel 3A) with a model size less than 10 MB.\nIconNet\nIconNet is based on the novel CenterNet architecture, which extracts features from input images and then predicts appropriate bounding box centers and sizes (in the form of heatmaps). CenterNet is particularly suited here because UI elements consist of simple, symmetric geometric shapes, making it easier to identify their centers than for natural images. The total loss used is a combination of a standard L1 loss for the icon sizes and a modified CornerNet Focal loss for the center predictions, the latter of which addresses icon class imbalances between commonly occurring icons (e.g., arrow backward, menu, more, and star) and underrepresented icons (end call, delete, launch apps, etc.)..CenterNetL1 lossCornerNet Focal loss\nAfter experimenting with several backbones (MobileNet, ResNet, UNet, etc), we selected the most promising server-side architecture \u2014 Hourglass \u2014 as a starting point for designing a backbone tailored for icon and UI element detection. While this architecture is perfectly suitable for server side models, vanilla Hourglass backbones are not an option for a model that will run on a mobile device, due to their large size and slow inference time. We restricted our on-device network design to a single stack, and drastically reduced the width of the backbone. Furthermore, as the detection of icons relies on more local features (compared to real objects), we could further reduce the depth of the backbone without adversely affecting the performance. Ablation studies convinced us of the importance of skip connections and high resolution features. For example, trimming skip connections in the final layer reduced the mAP by 1.5%, and removing such connections from both the final and penultimate layers resulted in a  decline of 3.5% mAP.Hourglass\nModel Improvements\nOnce the backbone architecture was selected, we used neural architecture search (NAS) to explore variations on the network architecture and uncover an optimal set of training and model parameters that would balance model performance (mAP) with latency (FLOPs).  Additionally, we used Fine-Grained Stochastic Architecture Search (FiGS) to further refine the backbone design. FiGS is a differentiable architecture search technique that uncovers sparse structures by pruning a candidate architecture and discarding unnecessary connections. This technique allowed us to reduce the model size by 20% without any loss in performance, and by 50% with only a minor drop of 0.3% in mAP.neural architecture searchFine-Grained Stochastic Architecture Search\nImproving the quality of the training dataset also played an important role in boosting the model performance. We collected and labeled more than 700K screenshots, and in the process, we streamlined data collection by using heuristics and auxiliary models to identify rarer icons. We also took advantage of data augmentation techniques by enriching existing screenshots with infrequent icons.\nTo improve the inference time, we modified our model to run using Neural Networks API (NNAPI) on a variety of Qualcomm DSPs available on many mobile phones. For this we converted the model to use 8-bit integer quantization which gives the additional benefit of model size reduction. After some experimentation, we used quantization aware training to quantize the model, while matching the performance of a server-side floating point model. The quantized model results in a 6x speed-up (700ms vs 110ms) and 50% size reduction while losing only ~0.5% mAP compared to the unquantized model.Neural Networks APIQualcomm DSPsquantization aware training\nResults\nWe use traditional object detection metrics (e.g., mAP) to measure model performance. In addition, to better capture the use case of voice controlled user actions, we define a modified version of a false positive (FP) detection, where we penalize more incorrect detections for icon classes that are present on the screen. For comparing detections with ground truth, we use the center in region of interest (CIROI), another metric we developed for this work, which returns in a positive match when the center of the detected bounding box lies inside the ground truth bounding box. This better captures the Voice Access mode of operation, where actions are performed by tapping anywhere in the region of the UI element of interest.\nWe compared the IconNet model with various other mobile compatible object detectors, including MobileNetEdgeTPU and SSD MobileNet v2. Experiments showed that for a fixed latency, IconNet outperformed the other models in terms of mAP@CIROI on our internal evaluation set.MobileNetEdgeTPUSSD MobileNet v2\nThe performance advantage of IconNet persists when considering quantized models and models for a fixed latency budget.\nConclusion and Future Work\nWe are constantly working on improving IconNet. Among other things, we are interested in increasing the range of elements supported by IconNet to include any generic UI element, such as images, text, or buttons. We also plan to extend IconNet to differentiate between similar looking icons by identifying their functionality. On the application side, we are hoping to increase the number of apps with valid content descriptions by augmenting developer tools to suggest content descriptions for different UI elements when building applications.\nAcknowledgements\nThis project is the result of joint work with Maria Wang, Tautvydas Misi\u016bnas, Lijuan Liu, Ying Xu, Nevan Wichers, Xiaoxue Zang, Gabriel Schubiner, Abhinav Rastogi, Jindong (JD) Chen, Abhanshu Sharma, Pranav Khaitan, Matt Sharifi and Blaise Aguera y Arcas. We sincerely thank our collaborators Robert Berry, Folawiyo Campbell, Shraman Ray Chaudhuri, Nghi Doan, Elad Eban, Marybeth Fair, Alec Go, Sahil Goel, Tom Hume, Cassandra Luongo, Yair Movshovitz-Attias, James Stout, Gabriel Taubman and Anton Vayvod. We are very grateful to Tom Small for assisting us in preparing the post.",
      "link": "http://ai.googleblog.com/2021/01/improving-mobile-app-accessibility-with.html",
      "author": "Posted by Gilles Baechler and Srinivas Sunkara, Software Engineers, Google Research"
    },
    {
      "title": "Addressing Range Anxiety with Smart Electric Vehicle Routing",
      "date": "Wednesday, January 27, 2021",
      "abstract": "Addressing Range Anxiety with Smart Electric Vehicle Routing\nMapping algorithms used for navigation often rely on Dijkstra\u2019s algorithm, a fundamental textbook solution for finding shortest paths in graphs. Dijkstra\u2019s algorithm is simple and elegant -- rather than considering all possible routes (an exponential number) it iteratively improves an initial solution, and works in polynomial time. The original algorithm and practical extensions of it (such as the A* algorithm) are used millions of times per day for routing vehicles on the global road network. However, due to the fact that most vehicles are gas-powered, these algorithms ignore refueling considerations because a) gas stations are usually available everywhere at the cost of a small detour, and b) the time needed to refuel is typically only a few minutes and is negligible compared to the total  travel time.Dijkstra\u2019s algorithmpolynomial timeA* algorithm\nThis situation is different for electric vehicles (EVs). First, EV charging stations are not as commonly available as gas stations, which can cause range anxiety, the fear that the car will run out of power before reaching a charging station. This concern is common enough that it is considered one of the barriers to the widespread adoption of EVs. Second, charging an EV\u2019s battery is a more decision-demanding task, because the charging time can be a significant fraction of the total travel time and can vary widely by station, vehicle model, and battery level. In addition, the charging time is non-linear \u2014 e.g., it takes longer to charge a battery from 90% to 100% than from 20% to 30%.range anxiety\nToday, we present a new approach for routing of EVs integrated into the latest release of Google Maps built into your car for participating EVs that reduces range anxiety by integrating recharging stations into the navigational route. Based on the battery level and the destination, Maps will recommend the charging stops and the corresponding charging levels that will minimize the total duration of the trip. To accomplish this we engineered a highly scalable solution for recommending efficient routes through charging stations, which optimizes the sum of the driving time and the charging time together.we present a new approach for routing of EVsGoogle Maps built into your car\nRouting Through Charging Stations\nA fundamental constraint on route selection is that the distance between recharging stops cannot be higher than what the vehicle can reach on a full charge.  Consequently, the route selection model emphasizes the graph of charging stations, as opposed to the graph of road segments of the road network, where each charging station is a node and each trip between charging stations is an edge. Taking into consideration the various characteristics of each EV (such as the weight, maximum battery level, plug type, etc.) the algorithm identifies which of the edges are feasible for the EV under consideration and which are not. Once the routing request comes in, Maps EV routing augments the feasible graph with two new nodes, the origin and the destination, and with multiple new (feasible) edges that outline the potential trips from the origin to its nearby charging stations and to the destination from each of its nearby charging stations.graph\nRouting using Dijkstra\u2019s algorithm or A* on this graph is sufficient to give a feasible solution that optimizes for the travel time for drivers that do not care at all about the charging time, (i.e., drivers who always fully charge their batteries at each charging station). However, such algorithms are not sufficient to account for charging times. In this case, the algorithm constructs a new graph by replicating each charging station node multiple times. Half of the copies correspond to entering the station with a partially charged battery, with a charge, x, ranging from 0%-100%. The other half correspond to exiting the station with a fractional charge, y (again from 0%-100%). We add an edge from the entry node at the charge x to the exit node at charge y (constrained by y > x), with a corresponding charging time to get from x to y. When the trip from Station A to Station B spends some fraction (z) of the battery charge, we introduce an edge between every exit node of Station A to the corresponding entry node of Station B (at charge x-z). After performing this transformation, using Dijkstra or A* recovers the solution.\nGraph Sparsification\nTo perform the above operations while addressing range anxiety with confidence, the algorithm must compute the battery consumption of each trip between stations with good precision. For this reason, Maps maintains detailed information about the road characteristics along the trip between any two stations (e.g., the length, elevation, and slope, for each segment of the trip), taking into consideration the properties of each type of EV.\nDue to the volume of information required for each segment, maintaining a large number of edges can become a memory intensive task. While this is not a problem for areas where EV charging stations are sparse, there exist locations in the world (such as Northern Europe) where the density of stations is very high. In such locations, adding an edge for every pair of stations between which an EV can travel quickly grows to billions of possible edges.\nHowever, this high density implies that a trip between two stations that are relatively far apart will undoubtedly pass through multiple other stations. In this case, maintaining information about the long edge is redundant, making it possible to simply add the smaller edges (spanners) in the graph, resulting in sparser, more computationally feasible, graphs.spanners\nThe spanner construction algorithm is a direct generalization of the greedy geometric spanner. The trips between charging stations are sorted from fastest to slowest and are processed in that order. For each trip between points a and b, the algorithm examines whether smaller subtrips already included in the spanner subsume the direct trip. To do so it compares the trip time and battery consumption that can be achieved using subtrips already in the spanner, against the same quantities for the direct a-b route. If they are found to be within a tiny error threshold, the direct trip from a to b is not added to the spanner, otherwise it is. Applying this sparsification algorithm has a notable impact and allows the graph to be served efficiently in responding to users\u2019 routing requests.greedy geometric spanner\nSummary\nIn this work we engineer a scalable solution for routing EVs on long trips to include access to charging stations through the use of graph sparsification and novel framing of standard routing algorithms. We are excited to put algorithmic ideas and techniques in the hands of Maps users and look forward to serving stress-free routes for EV drivers across the globe!\nAcknowledgements\nWe thank our collaborators Dixie Wang, Xin Wei Chow, Navin Gunatillaka, Stephen Broadfoot, Alex Donaldson, and Ivan Kuznetsov.",
      "link": "http://ai.googleblog.com/2021/01/addressing-range-anxiety-with-smart.html",
      "author": "Posted by Kostas Kollias and Sreenivas Gollapudi, Research Scientists, Google Research"
    },
    {
      "title": "Stabilizing Live Speech Translation in Google Translate",
      "date": "Tuesday, January 26, 2021",
      "abstract": "Stabilizing Live Speech Translation in Google Translate\nThe transcription feature in the Google Translate app may be used to create a live, translated transcription for events like meetings and speeches, or simply for a story at the dinner table in a language you don\u2019t understand. In such settings, it is useful for the translated text to be displayed promptly to help keep the reader engaged and in the moment. transcription feature in the Google Translate app\nHowever, with early versions of this feature the translated text suffered from multiple real-time revisions, which can be distracting. This was because of the non-monotonic relationship between the source and the translated text, in which words at the end of the source sentence can influence words at the beginning of the translation.\nToday, we are excited to describe some of the technology behind a recently released update to the transcribe feature in the Google Translate app that significantly reduces translation revisions and improves the user experience. The research enabling this is presented in two papers. The first formulates an evaluation framework tailored to live translation and develops methods to reduce instability.  The second demonstrates that these methods do very well compared to alternatives, while still retaining the simplicity of the original approach. The resulting model is much more stable and provides a noticeably improved reading experience within Google Translate.firstsecond\nEvaluating Live Translation\nBefore attempting to make any improvements, it was important to first understand and quantifiably measure the different aspects of the user experience, with the goal of maximizing quality while minimizing latency and instability. In \u201cRe-translation Strategies For Long Form, Simultaneous, Spoken Language Translation\u201d, we developed an evaluation framework for live-translation that has since guided our research and engineering efforts. This work presents a performance measure using the following metrics:Re-translation Strategies For Long Form, Simultaneous, Spoken Language TranslationBLEU score\nIt is important to recognize the inherent trade-offs between these different aspects of quality. Transcribe enables live-translation by stacking machine translation on top of real-time automatic speech recognition. For each update to the recognized transcript, a fresh translation is generated in real time; several updates can occur each second. This approach placed Transcribe at one extreme of the 3 dimensional quality framework: it exhibited minimal lag and the best quality, but also had high erasure. Understanding this allowed us to work towards finding a better balance.\nStabilizing Re-translation\nOne straightforward solution to reduce erasure is to decrease the frequency with which translations are updated. Along this line, \u201cstreaming translation\u201d models (for example, STACL and MILk) intelligently learn to recognize when sufficient source information has been received to extend the translation safely, so the translation never needs to be changed. In doing so, streaming translation models are able to achieve zero erasure.STACLMILk\nThe downside with such streaming translation models is that they once again take an extreme position: zero erasure necessitates sacrificing BLEU and lag. Rather than eliminating erasure altogether, a small budget for occasional instability may allow better BLEU and lag. More importantly, streaming translation would require retraining and maintenance of specialized models specifically for live-translation. This precludes the use of streaming translation in some cases, because keeping a lean pipeline is an important consideration for a product like Google Translate that supports 100+ languages.\nIn our second paper, \u201cRe-translation versus Streaming for Simultaneous Translation\u201d, we show that our original \u201cre-translation\u201d approach to live-translation can be fine-tuned to reduce erasure and achieve a more favorable erasure/lag/BLEU trade-off. Without training any specialized models, we applied a pair of inference-time heuristics to the original machine translation models \u2014 masking and biasing.Re-translation versus Streaming for Simultaneous Translation\nThe end of an on-going translation tends to flicker because it is more likely to have dependencies on source words that have yet to arrive. We reduce this by truncating some number of words from the translation until the end of the source sentence has been observed. This masking process thus trades latency for stability, without affecting quality. This is very similar to delay-based strategies used in streaming methods such as Wait-k, but applied only during inference and not during training.Wait-k\nNeural machine translation often \u201csee-saws\u201d between equally good translations, causing unnecessary erasure. We improve stability by biasing the output towards what we have already shown the user. On top of reducing erasure, biasing also tends to reduce lag by stabilizing translations earlier. Biasing interacts nicely with masking, as masking words that are likely to be unstable also prevents the model from biasing toward them. However, this process does need to be tuned carefully, as a high bias, along with insufficient masking, may have a negative impact on quality.\nThe combination of masking and biasing, produces a re-translation system with high quality and low latency, while virtually eliminating erasure. The table below shows how the metrics react to the heuristics we introduced and how they compare to the other systems discussed above. The graph demonstrates that even with a very small erasure budget, re-translation surpasses zero-flicker streaming translation systems (MILk and Wait-k) trained specifically for live-translation.IWSLT test 2018 Engish-German (TED talks)WMT 14 English-German\nConclusion\nThe solution outlined above returns a decent translation very quickly, while allowing it to be revised as more of the source sentence is spoken. The simple structure of re-translation enables the application of our best speech and translation models with minimal effort. However, reducing erasure is just one part of the story \u2014 we are also looking forward to improving the overall speech translation experience through new technology that can reduce lag when the translation is spoken, or that can enable better transcriptions when multiple people are speaking.\nAcknowledgements\nThanks to Te I, Dirk Padfield, George Foster, Wolfgang Macherey, Pallavi Baljekar, Sami Iqram, John Richardson, Kuang-Che Lee, Bryan Lin, Mengmeng Niu, Nathan Bain, Lindsey Boran, Shilip Vaishnav, Kannu Mehta, Chris Kau, Tom Small, Jeff Pitman and Macduff Hughes.",
      "link": "http://ai.googleblog.com/2021/01/stabilizing-live-speech-translation-in.html",
      "author": "Posted by Naveen Arivazhagan, Senior Software Engineer and Colin Cherry, Staff Research Scientist, Google Research"
    },
    {
      "title": "Improving Indian Language Transliterations in Google Maps",
      "date": "Friday, January 22, 2021",
      "abstract": "Improving Indian Language Transliterations in Google Maps\nNearly 75% of India\u2019s population \u2014 which possesses the second highest number of internet users in the world \u2014 interacts with the web primarily using Indian languages, rather than English. Over the next five years, that number is expected to rise to 90%. In order to make Google Maps as accessible as possible to the next billion users, it must allow people to use it in their preferred language, enabling them to explore anywhere in the world.interacts with the webexpected to rise to 90%\nHowever, the names of most Indian places of interest (POIs) in Google Maps are not generally available in the native scripts of the languages of India. These names are often in English and may be combined with acronyms based on the Latin script, as well as Indian language words and names. Addressing such mixed-language representations requires a transliteration system that maps characters from one script to another, based on the source and target languages, while accounting for the phonetic properties of the words as well.transliteration\nFor example, consider a user in Ahmedabad, Gujarat, who is looking for a nearby hospital, KD Hospital. They issue the search query, \u0a95\u0ac7\u0aa1\u0ac0 \u0ab9\u0acb\u0ab8\u0acd\u0aaa\u0abf\u0a9f\u0ab2, in the native script of Gujarati, the 6th most widely spoken language in India. Here, \u0a95\u0ac7\u0aa1\u0ac0 (\u201ckay-dee\u201d) is the sounding out of the acronym KD, and \u0ab9\u0acb\u0ab8\u0acd\u0aaa\u0abf\u0a9f\u0ab2 is \u201chospital\u201d. In this search, Google Maps knows to look for hospitals, but it doesn't understand that \u0a95\u0ac7\u0aa1\u0ac0 is KD, hence it finds another hospital, CIMS. As a consequence of the relative sparsity of names available in the Gujarati script for places of interest (POIs) in India, instead of their desired result, the user is shown a result  that is further away.\nTransliteration vs. Transcription vs. Translation\nOur goal was to design a system that will transliterate from a reference Latin script name into the scripts and orthographies native to the above-mentioned languages. For example, the Devanagari script is the native script for both Hindi and Marathi (the language native to Nagpur, Maharashtra). Transliterating the Latin script names for NIT Garden and Chandramani Garden, both POIs in Nagpur, results in \u090f\u0928\u0906\u0908\u091f\u0940 \u0917\u093e\u0930\u094d\u0921\u0928 and \u091a\u0902\u0926\u094d\u0930\u092e\u0923\u0940 \u0917\u093e\u0930\u094d\u0921\u0928, respectively, depending on the specific language\u2019s orthography in that script.\nIt is important to note that the transliterated POI names are not translations. Transliteration is only concerned with writing the same words in a different script, much like an English language newspaper might choose to write the name \u0413\u043e\u0440\u0431\u0430\u0447\u0451\u0432 from the Cyrillic script as \u201cGorbachev\u201d\u00a0for their readers who do not read the Cyrillic script. For example, the second word in both of the transliterated POI names above is still pronounced \u201cgarden\u201d, and the second word of the Gujarati example earlier is still \u201chospital\u201d \u2014 they remain the English words \u201cgarden\u201d\u00a0and \u201chospital\u201d, just written in the other script. Indeed, common English words are frequently used in POI names in India, even when written in the native script. How the name is written in these scripts is largely driven by its pronunciation; so \u090f\u0928\u0906\u0908\u091f\u0940 from the acronym NIT is pronounced \u201cen-aye-tee\u201d, not as the English word \u201cnit\u201d. Knowing that NIT is a common acronym from the region is one piece of evidence that can be used when deriving the correct transliteration.\nNote also that, while we use the term transliteration, following convention in the NLP community for mapping directly between writing systems, romanization in South Asian languages regardless of the script is generally pronunciation-driven, and hence one could call these methods transcription rather than transliteration.  The task remains, however, mapping between scripts, since pronunciation is only relatively coarsely captured in the Latin script for these languages, and there remain many script-specific correspondences that must be accounted for.  This, coupled with the lack of standard spelling in the Latin script and the resulting variability, is what makes the task challenging.\nTransliteration Ensemble\nWe use an ensemble of models to automatically transliterate from the reference Latin script name (such as NIT Garden or Chandramani Garden) into the scripts and orthographies native to the above-mentioned languages. Candidate transliterations are derived from a pair of sequence-to-sequence (seq2seq) models. One is a finite-state model for general text transliteration, trained in a manner similar to models used by Gboard on-device for transliteration keyboards. The other is a neural  long short-term memory (LSTM) model trained, in part, on the publicly released Dakshina dataset. This dataset contains Latin and native script data drawn from Wikipedia in 12 South Asian languages, including all but one of the languages mentioned above, and permits training and evaluation of various transliteration methods. Because the two models have such different characteristics, together they produce a greater variety of transliteration candidates.sequence-to-sequencefinite-stateused by Gboard on-devicetransliteration keyboardslong short-term memoryDakshina dataset.\nTo deal with the tricky phenomena of acronyms (such as the \u201cNIT\u201d\u00a0and \u201cKD\u201d\u00a0examples above), we developed a specialized transliteration module that generates additional candidate transliterations for these cases.\nFor each native language script, the ensemble makes use of specialized romanization dictionaries of varying provenance that are tailored for place names, proper names, or common words.  Examples of such romanization dictionaries are found in the Dakshina dataset.\nScoring in the Ensemble\nThe ensemble combines scores for the possible transliterations in a weighted mixture, the parameters of which are tuned specifically for POI name accuracy using small targeted development sets for such names.\nFor each native script token in candidate transliterations, the ensemble also weights the result according to its frequency in a very large sample of on-line text. Additional candidate scoring is based on a deterministic romanization approach derived from the ISO 15919 romanization standard, which maps each native script token to a unique Latin script string. This string allows the ensemble to track certain key correspondences when compared to the original Latin script token being transliterated, even though the ISO-derived mapping itself does not always perfectly correspond to how the given native script word is typically written in the Latin script.ISO 15919\n  In aggregate, these many moving parts provide substantially higher quality transliterations than possible for any of the individual methods alone.\nCoverage\nThe following table provides the per-language quality and coverage improvements due to the ensemble over existing automatic transliterations of POI names. The coverage improvement measures the increase in items for which an automatic transliteration has been made available.  Quality improvement measures the ratio of updated transliterations that were judged to be improvements versus those that were judged to be inferior to existing automatic transliterations.\nConclusion\nAs with any machine learned system, the resulting automatic transliterations may contain a few errors or infelicities, but the large increase in coverage in these widely spoken languages marks a substantial expansion of the accessibility of information within Google Maps in India.  Future work will include using the ensemble for transliteration of other classes of entities within Maps and its extension to other languages and scripts, including Perso-Arabic scripts, which are also commonly used in the region.\nAcknowledgments\nThis work was a collaboration between the authors and Jacob Farner, Jonathan Herbert, Anna Katanova, Andre Lebedev, Chris Miles, Brian Roark, Anurag Sharma, Kevin Wang, Andy Wildenberg, and many others.",
      "link": "http://ai.googleblog.com/2021/01/improving-indian-language.html",
      "author": "Posted by Cibu Johny, Software Engineer, Google Research and Saumya Dalal, Product Manager, Google Geo"
    },
    {
      "title": "RxR: A Multilingual Benchmark for Navigation Instruction Following",
      "date": "Thursday, January 21, 2021",
      "abstract": "RxR: A Multilingual Benchmark for Navigation Instruction Following\nA core challenge in machine learning (ML) is to build agents that can navigate complex human environments in response to spoken or written commands. While today\u2019s agents, including robots, can often navigate complicated environments, they cannot yet understand navigation goals expressed in natural language, such as, \u201cGo past the brown double doors that are closed to your right and stand behind the chair at the head of the table.\u201d\nThis challenge, referred to as vision-and-language navigation (VLN), demands a sophisticated understanding of spatial language. For example, the ability to identify the position \u201cbehind the chair at the head of the table\u201d requires finding the table, identifying which part of the table is considered to be the \u201chead\u201d, finding the chair closest to the head, identifying the area behind this chair and so on. While people can follow these instructions easily, these challenges cannot be easily solved with current ML-based methods, requiring systems that can better connect language to the physical world it describes.vision-and-language navigation\nTo help spur progress in this area, we are excited to introduce Room-Across-Room (RxR), a new dataset for VLN. Described in \u201cRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding\u201d, RxR is the first multilingual dataset for VLN, containing 126,069 human-annotated navigation instructions in three typologically diverse languages \u2014 English, Hindi and Telugu. Each instruction describes a path through a photorealistic simulator populated with indoor environments from the Matterport3D dataset, which includes 3D captures of homes, offices and public buildings. To track progress on VLN, we are also announcing the RxR Challenge, a competition that encourages the machine learning community to train and evaluate their own instruction following agents on RxR instructions.Room-Across-RoomRoom-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal GroundingMatterport3D datasetRxR Challenge\nExamples of English, Hindi and Telugu navigation instructions from the RxR dataset. Each navigation instruction describes the same path.\nPose Traces\nIn addition to navigation instructions and paths, RxR also includes a new, more detailed multimodal annotation called a pose trace. Inspired by the mouse traces captured in the Localized Narratives dataset, pose traces provide dense groundings between language, vision and movement in a rich 3D setting. To generate navigation instructions, we ask guide annotators to move along a path in the simulator while narrating the path based on the surroundings. The pose trace is a record of everything the guide sees along the path, time-aligned with the words in the navigation instructions. These traces are then paired with pose traces from follower annotators, who are tasked with following the intended path by listening to the guide\u2019s audio, thereby validating the quality of the navigation instructions. Pose traces implicitly capture notions of landmark selection and visual saliency, and represent a play-by-play account of how to solve the navigation instruction generation task (for guides) and the navigation instruction following task (for followers).Localized Narrativesvisual saliency\nScale\nIn total, RxR contains almost 10 million words, making it around 10 times larger than existing datasets, such as R2R and Touchdown/Retouchdown. This is important because, in comparison to tasks based on static image and text data, language tasks that require learning through movement or interaction with an environment typically suffer from a lack of large-scale training data. RxR also addresses known biases in the construction of the paths that have arisen in other datasets, such as R2R in which all paths have similar lengths and take the shortest route to the goal. In contrast, the paths in RxR are on average longer and less predictable, making them more challenging to follow and encouraging models trained on the dataset to place greater emphasis on the role of language in the task. The size, scope and detail of RxR will expand the frontier for research on grounded language learning while reducing the dominance of high resource languages such as English.R2RTouchdownRetouchdown\nBaselines\nTo better characterize and understand the RxR dataset, we trained a variety of agents on RxR using our open source framework VALAN, and language representations from the multilingual BERT model. We found that results were improved by including follower annotations as well as guide annotations during training, and that independently trained monolingual agents outperformed a single multilingual agent.VALANmultilingual BERT\nConceptually, evaluation of these agents is straightforward \u2014 did the agent follow the intended path? Empirically, we measure the similarity between the path taken by the VLN agent and the reference path using NDTW, a normalized measure of path fidelity that ranges between 100 (perfect correspondence) and 0 (completely wrong). The average score for the follower annotators across all three languages is 79.5, due to natural variation between similar paths. In contrast, the best model (a composite of three independently trained monolingual agents, one for each language) achieved an NDTW score on the RxR test set of 41.5. While this is much better than random (15.4), it remains far below human performance. Although advances in language modeling continue to rapidly erode the headroom for improvement in text-only language understanding benchmarks such as GLUE and SuperGLUE, benchmarks like RxR that connect language to the physical world offer substantial room for improvement.NDTWGLUESuperGLUE\nCompetition\nTo encourage further research in this area, we are launching the RxR Challenge, an ongoing competition for the machine learning community to develop computational agents that can follow natural language navigation instructions. To take part, participants upload the navigation paths taken by their agent in response to the provided RxR test instructions. In the most difficult setting (reported here and in the paper), all the test environments are previously unseen. However, we also allow for settings in which the agent is either trained in or explores the test environments in advance. For more details and the latest results please visit the challenge website.RxR Challengepaperchallenge website\nPanGEA\nWe are also releasing the custom web-based annotation tool that we developed to collect the RxR dataset. The Panoramic Graph Environment Annotation toolkit (PanGEA), is a lightweight and customizable codebase for collecting speech and text annotations in panoramic graph environments, such as Matterport3D and StreetLearn. It includes speech recording and virtual pose tracking, as well as tooling to align the resulting pose trace with a manual transcript. For more details please visit the PanGEA github page.Matterport3DStreetLearnPanGEA github page\nAcknowledgements\nThe authors would like to thank Roma Patel, Eugene Ie and Jason Baldridge for their contributions to this research. We would also like to thank all the annotators, Sneha Kudugunta for analyzing the Telugu annotations, and Igor Karpov, Ashwin Kakarla and Christina Liu for their tooling and annotation support for this project, Austin Waters and Su Wang for help with image features, and Daphne Luong for executive support for the data collection.",
      "link": "http://ai.googleblog.com/2021/01/rxr-multilingual-benchmark-for.html",
      "author": "Posted by Alexander Ku, Software Engineer and Peter Anderson, Research Scientist, Google Research"
    },
    {
      "title": "ToTTo: A Controlled Table-to-Text Generation Dataset",
      "date": "Friday, January 15, 2021",
      "abstract": "ToTTo: A Controlled Table-to-Text Generation Dataset\nIn the last few years, research in natural language generation, used for tasks like text summarization, has made tremendous progress. Yet, despite achieving high levels of fluency, neural systems can still be prone to hallucination (i.e.generating text that is understandable, but not faithful to the source), which can prohibit these systems from being used in many applications that require high degrees of accuracy. Consider an example from the Wikibio dataset, where the neural baseline model tasked with summarizing a Wikipedia infobox entry for Belgian football player Constant Vanden Stock summarizes incorrectly that he is an American figure skater.natural language generationhallucinationWikibio datasetbaseline modelConstant Vanden Stock\nWhile the process of assessing the faithfulness of generated text to the source content can be challenging, it is often easier when the source content is structured (e.g., in tabular format). Moreover, structured data can also test a model\u2019s ability for reasoning and numerical inference. However, existing large scale structured datasets are often noisy (i.e., the reference sentence cannot be fully inferred from the tabular data), making them unreliable for the measurement of hallucination in model development.\nIn \u201cToTTo: A Controlled Table-To-Text Generation Dataset\u201d,  we present an open domain table-to-text generation dataset created using a novel annotation process (via sentence revision) along with a controlled text generation task that can be used to assess model hallucination. ToTTo (shorthand for \u201cTable-To-Text\u201d) consists of 121,000 training examples, along with 7,500 examples each for development and test. Due to the accuracy of annotations, this dataset is suitable as a challenging benchmark for research in high precision text generation. The dataset and code are open-sourced on our GitHub repo.ToTTo: A Controlled Table-To-Text Generation Datasetour GitHub repo\nTable-to-Text Generation\nToTTo introduces a controlled generation task in which a given Wikipedia table with a set of selected cells is used as the source material for the task of producing a single sentence description that summarizes the cell contents in the context of the table. The example below demonstrates some of the many challenges posed by the task, such as numerical reasoning, a large open-domain vocabulary, and varied table structure.\nAnnotation Process\nDesigning an annotation process to obtain natural but also clean target sentences from tabular data  is a significant challenge. Many datasets like Wikibio and RotoWire pair naturally occurring text heuristically with tables, a noisy process that makes it difficult to disentangle whether hallucination is primarily caused by data noise or model shortcomings. On the other hand, one can elicit annotators to write sentence targets from scratch, which are faithful to the table, but the resulting targets often lack variety in terms of structure and style.WikibioRotoWirefrom scratchlack variety\nIn contrast, ToTTo is constructed using a novel data annotation strategy in which annotators revise existing Wikipedia sentences in stages. This results in target sentences that are clean, as well as natural, containing interesting and varied linguistic properties. The data collection and annotation process begins by collecting tables from Wikipedia, where a given table is paired with a summary sentence collected from the supporting page context according to heuristics, such as word overlap between the page text and the table and hyperlinks referencing tabular data. This summary sentence may contain information not supported by the table and may contain pronouns with antecedents found in the table only, not the sentence itself.\nThe annotator then highlights the cells in the table that support the sentence and deletes phrases in the sentence that are not supported by the table. They also decontextualize the sentence so that it is standalone (e.g., with correct pronoun resolution) and correct grammar, where necessary.\nWe show that annotators obtain high agreement on the above task: 0.856 Fleiss Kappa for cell highlighting, and 67.0 BLEU for the final target sentence.Fleiss KappaBLEU\nDataset Analysis\nWe conducted a topic analysis on the ToTTo dataset over 44 categories and found that the Sports and Countries topics, each of which consists of a range of fine-grained topics, e.g., football/olympics for sports and population/buildings for countries, together comprise 56.4% of the dataset. The other 44% is composed of a much more broad set of topics, including Performing Arts, Transportation, and Entertainment.\nFurthermore, we conducted a manual analysis of the different types of linguistic phenomena in the dataset over 100 randomly chosen examples. The table below summarizes the fraction of examples that require reference to the page and section titles, as well as some of the linguistic phenomena in the dataset that potentially pose new challenges to current systems.\nBaseline Results\nWe present some baseline results of three state-of-the-art models from the literature (BERT-to-BERT, Pointer Generator, and the Puduppully 2019 model) on two evaluation metrics, BLEU and PARENT. In addition to reporting the score on the overall test set, we also evaluate each model on a more challenging subset consisting of out-of-domain examples. As the table below shows, the BERT-to-BERT model performs best in terms of both BLEU and PARENT. Moreover, all models achieve considerably lower performance on the challenge set indicating the challenge of out-of-domain generalization.BERT-to-BERTPointer GeneratorPuduppully 2019 modelBLEUPARENT\nWhile automatic metrics can give some indication of performance, they are not currently sufficient for evaluating hallucination in text generation systems. To better understand hallucination, we manually evaluate the top performing baseline, to determine how faithful it is to the content in the source table, under the assumption that discrepancies indicate hallucination. To compute the \u201cExpert\u201d performance, for each example in our multi-reference test set, we held out one reference and asked annotators to compare it with the other references for faithfulness. As the results show, the top performing baseline appears to hallucinate information ~20% of the time.\nModel Errors and Challenges\nIn the table below, we present a selection of the observed model errors to highlight some of the more challenging aspects of the ToTTo dataset. We find that state-of-the-art models struggle with hallucination, numerical reasoning, and rare topics, even when using cleaned references (errors in red). The last example shows that even when the model output is correct it is sometimes not as informative as the original reference which contains more reasoning about the table (shown in blue).\nConclusion\nIn this work, we presented ToTTo, a large, English table-to-text dataset that presents both a controlled generation task and a data annotation process based on iterative sentence revision. We also provided several state-of-the-art baselines, and demonstrated ToTTo could be a useful dataset for modeling research as well as for developing evaluation metrics that can better detect model improvements.\nIn addition to the proposed task, we hope our dataset can also be helpful for other tasks such as table understanding and sentence revision. ToTTo is available at our GitHub repo.table understandingGitHub repo\nAcknowledgements\nThe authors wish to thank Ming-Wei Chang, Jonathan H. Clark, Kenton Lee, and Jennimaria Palomaki for their insightful discussions and support. Many thanks also to Ashwin Kakarla and his team for help with the annotations.",
      "link": "http://ai.googleblog.com/2021/01/totto-controlled-table-to-text.html",
      "author": "Posted by Ankur Parikh and Xuezhi Wang, Research Scientists, Google Research"
    },
    {
      "title": "Recognizing Pose Similarity in Images and Videos",
      "date": "Thursday, January 14, 2021",
      "abstract": "Recognizing Pose Similarity in Images and Videos\nEveryday actions, such as jogging, reading a book, pouring water, or playing sports, can be viewed as a sequence of poses, consisting of the position and orientation of a person\u2019s body. An understanding of poses from images and videos is a crucial step for enabling a range of applications, including augmented reality display, full-body gesture control, and physical exercise quantification. However, a 3-dimensional pose captured in two dimensions in images and videos appears different depending on  the viewpoint of the camera. The ability to recognize similarity in 3D pose using only 2D information will help vision systems better understand the world.posesaugmented realityfull-body gesture controlphysical exercise quantification\nIn \u201cView-Invariant Probabilistic Embedding for Human Pose\u201d (Pr-VIPE), a spotlight paper at ECCV 2020, we present a new algorithm for human pose perception that recognizes similarity in human body poses across different camera views by mapping 2D body pose keypoints to a view-invariant embedding space. This ability enables tasks, such as pose retrieval, action recognition, action video synchronization, and more. Compared to existing models that directly map 2D pose keypoints to 3D pose keypoints, the Pr-VIPE embedding space is (1) view-invariant, (2) probabilistic in order to capture 2D input ambiguity, and (3) does not require camera parameters during training or inference. Trained with in-lab setting data, the model works on in-the-wild images out of the box, given a reasonably good 2D pose estimator (e.g., PersonLab, BlazePose, among others). The model is simple, results in compact embeddings, and can be trained (in ~1 day) using 15 CPUs. We have released the code on our GitHub repo.View-Invariant Probabilistic Embedding for Human PoseECCV 20202D body pose keypointsexisting modelsprobabilisticcamera parametersPersonLabBlazePoseour GitHub repo\nPr-VIPE \nThe input to Pr-VIPE is a set of 2D keypoints, from any 2D pose estimator that produces a minimum of 13 body keypoints, and the output is the mean and variance of the pose embedding. The distances between embeddings of 2D poses correlate to their similarities in absolute 3D pose space. Our approach is based on two observations:13 body keypointsmean and varianceprojected\nThe first observation motivates the need for view-invariance. To accomplish this, we define the matching probability, i.e., the likelihood that different 2D poses were projected from the same, or similar 3D poses. The matching probability predicted by Pr-VIPE for matching pose pairs should be higher than for non-matching pairs.\nTo address the second observation, Pr-VIPE utilizes a probabilistic embedding formulation. Because many 3D poses can project to the same or similar 2D poses, the model input exhibits an inherent ambiguity that is difficult to capture through deterministic mapping point-to-point in embedding space. Therefore, we map a 2D pose through a probabilistic mapping to an embedding distribution, of which we use the variance to represent the uncertainty of the input 2D pose. As an example, in the figure below the third 2D view of the 3D pose on the left is similar to the first 2D view of a different 3D pose on the right, so we map them into a similar location in the embedding space with large variances.deterministicmulti-view imagesprojectionsTripletsmultivariate Gaussian distribution usingsampling-based approach\nEvaluation\nWe propose a new cross-view pose retrieval benchmark to evaluate the view-invariance property of the embedding. Given a monocular pose image, cross-view retrieval aims to retrieve the same pose from different views without using camera parameters. The results demonstrate that Pr-VIPE retrieves poses more accurately across views compared to baseline methods in both evaluated datasets (Human3.6M, MPI-INF-3DHP).Human3.6MMPI-INF-3DHP3D pose estimation\nCommon 3D pose estimation methods (such as the simple baseline used for comparison above, SemGCN, and EpipolarPose, amongst many others), predict 3D poses in camera coordinates, which are not directly view-invariant. Thus, rigid alignment between every query-index pair is required for retrieval using estimated 3D poses, which is computationally expensive due to the need for singular value decomposition (SVD). In contrast, Pr-VIPE embeddings can be directly used for distance computation in Euclidean space, without any post-processing.simple baselineSemGCNEpipolarPoserigid alignmentsingular value decomposition\nApplications\nView-invariant pose embedding can be applied to many image and video related tasks. Below, we show Pr-VIPE applied to cross-view retrieval on in-the-wild images without using camera parameters.\nThe same Pr-VIPE model can also be used for video alignment. To do so, we stack Pr-VIPE embeddings within a small time window, and use the dynamic time warping (DTW) algorithm to align video pairs.dynamic time warping\nThe video alignment distance calculated via DTW can then be used for action recognition by classifying videos using nearest neighbor search. We evaluate the Pr-VIPE embedding using the Penn Action dataset and demonstrate that using the Pr-VIPE embedding without fine-tuning on the target dataset, yields highly competitive recognition accuracy. In addition, we show that Pr-VIPE even achieves relatively accurate results using only videos from a single view in the index set.nearest neighbor searchPenn ActionIqbal et al.Liu and YuanLuvizon et al.Du et al.\nConclusion\nWe introduce the Pr-VIPE model for mapping 2D human poses to a view-invariant probabilistic embedding space, and show that the learned  embeddings can be directly used for pose retrieval, action recognition, and video alignment. Our cross-view retrieval benchmark can be used to test the view-invariant property of other embeddings. We look forward to hearing about what you can do with pose embeddings!\nAcknowledgments\nSpecial thanks to Jiaping Zhao, Liang-Chieh Chen, Long Zhao (Rutgers University), Liangzhe Yuan, Yuxiao Wang, Florian Schroff, Hartwig Adam, and the Mobile Vision team for the wonderful collaboration and support.",
      "link": "http://ai.googleblog.com/2021/01/recognizing-pose-similarity-in-images.html",
      "author": "Posted by Jennifer J. Sun, Student Researcher and Ting Liu, Senior Software Engineer, Google Research"
    },
    {
      "title": "Google Research: Looking Back at 2020, and Forward to 2021",
      "date": "Tuesday, January 12, 2021",
      "abstract": "Google Research: Looking Back at 2020, and Forward to 2021\nWhen I joined Google over 20 years ago, we were just figuring out how to really start on the journey of making a high quality and comprehensive search service for information on the web, using lots of curiously wired computers. Fast forward to today, and while we\u2019re taking on a much broader array of technical challenges, it\u2019s still with the same overarching goal of organizing the world's information and making it universally accessible and useful. In 2020, as the world has been reshaped by COVID-19, we saw the ways research-developed technologies could help billions of people better communicate, understand the world, and get things done. I\u2019m proud of what we\u2019ve accomplished, and excited about new possibilities on the horizon.lots of curiously wired computers\nThe goal of Google Research is to work on long-term, ambitious problems across a wide range of important topics \u2014  from predicting the spread of COVID-19, to designing algorithms, to learning to translate more and more languages automatically, to mitigating bias in ML models. In the spirit of our annual reviews for 2019, 2018, and more narrowly focused reviews of some work in 2017 and 2016, this post covers key Google Research highlights from this unusual year. For a more comprehensive look, please see our >800 research publications in 2020. This is a long post, but is grouped into many different sections, which you can jump to directly using the table below. Hopefully, there\u2019s something interesting in here for everyone!Google Research2019201820172016>800 research publications in 2020\u00b7 COVID-19 and Health\u00b7 AutoML\u00b7 ML for Medical Diagnostics\u00b7 Understanding ML Algorithms and Models\u00b7 Weather, Environment and Climate Change\u00b7 Algorithmic Foundations and Theory\u00b7 Accessibility\u00b7 Machine Perception\u00b7 Applications of ML to Other Fields\u00b7 Robotics\u00b7 Responsible AI\u00b7 Quantum Computing\u00b7 Natural Language Understanding\u00b7 Supporting Developers and Researchers\u00b7 Language Translation\u00b7 Open Datasets and Dataset Search\u00b7 Machine Learning Algorithms\u00b7 Research Community Interaction\u00b7 Reinforcement Learning\u00b7 Looking Forward to 2021 and Beyond \nCOVID-19 and Health\nAs the impact of COVID-19 took a tremendous toll on people\u2019s lives, researchers and developers around the world rallied together to develop tools and technologies to help public health officials and policymakers understand and respond to the pandemic. Apple and Google partnered in 2020 to develop the Exposure Notifications System (ENS), a Bluetooth-enabled privacy-preserving technology that allows people to be notified if they have been exposed to others who have tested positive for COVID-19. ENS supplements traditional contact tracing efforts and has been deployed by public health authorities in more than 50 countries, states and regions to help curb the spread of infection.Apple and Google partnered in 2020\nIn the early days of the pandemic, public health officials signalled their need for more comprehensive data to combat the virus\u2019 rapid spread. Our Community Mobility Reports, which provide anonymized insights into movement trends, are helping researchers not only understand the impact of policies like stay-at-home directives and social distancing, and also conduct economic forecasting.Community Mobility Reportsimpact of policieseconomic forecasting\nOur own researchers have also explored using this anonymized data to forecast COVID-19 spread using graph neural networks instead of traditional time series-based models. forecast COVID-19 spread\nAlthough the research community knew little about this disease and secondary effects initially, we\u2019re learning more every day. Our COVID-19 Search Trends symptoms allows researchers to explore temporal or symptomatic associations, such as anosmia \u2014 the loss of smell that is sometimes a symptom of the virus. To further support the broader research community, we launched Google Health Studies app to provide the public ways to participate in research studies.COVID-19 Search Trends symptomsGoogle Health Studies app\nTeams across Google are contributing tools and resources to the broader scientific community, which is working to address the health and economic impacts of the virus.broader scientific communitymodelling COVID-19 Spread\nAccurate information is critical in dealing with public health threats. We collaborated with many product teams at Google in order to improve information quality about COVID-19 in Google News and Search through supporting fact checking efforts, as well as similar efforts in YouTube.fact checking effortsYouTube\nWe helped multilingual communities get equal access to critical COVID-19 information by sponsoring localization of Nextstrain.org\u2019s weekly Situation Reports and developing a COVID-19 open source parallel dataset in collaboration with Translators Without Borders.sponsoring localization of Nextstrain.org\u2019s weekly Situation Reportsdeveloping a COVID-19 open source parallel dataset in collaboration with Translators Without Borders\nModelling a complex global event is particularly challenging and requires more comprehensive epidemiological datasets, the development of novel interpretable models and agent-based simulators to inform the public health response. Machine learning techniques have also helped in other ways from deploying natural language understanding to helping researchers quickly navigate the mountains of COVID-19 scientific literature, applying anonymization technology to protect privacy while making useful datasets available, and exploring whether public health can conduct faster screening with fewer tests via Bayesian group testing.epidemiological datasetsnovel interpretable modelsagent-based simulatorsnatural language understandinganonymization technologyBayesian group testing\nThese are only a sample of the many pieces of work that happened across Google to help users and public health authorities respond to COVID-19. For more, see using technology to help take on COVID-19.using technology to help take on COVID-19TopTop \nResearch in Machine Learning for Medical Diagnostics\nWe continue to make headway helping clinicians harness the power of ML to deliver better care for more patients. This year we have described notable advances in applying computer vision to aid doctors in the diagnosis and management of cancer, including helping to make sure that doctors don\u2019t miss potentially cancerous polyps during colonoscopies, and showing that an ML system can achieve substantially higher accuracy than pathologists in Gleason grading of prostate tissue, enabling radiologists to achieve significant reductions in both false negative and false positive results when examining X-rays for signs of breast cancer.don\u2019t miss potentially cancerous polyps during colonoscopiescan achieve substantially higher accuracy than pathologists in Gleason grading of prostateto achieve significant reductions in both false negative and false positive results when examining X-rays for signs of breast cancerGleason grade\nWe\u2019ve also been working on systems to help identify skin disease, help detect age-related macular degeneration (the leading cause of blindness in the U.S. and U.K., and the third-largest cause of blindness worldwide), and on potential novel non-invasive diagnostics (e.g., being able to detect signs of anemia from retinal images).skin diseaseage-related macular degenerationanemia from retinal images\nThis year has also brought exciting demonstrations of how these same technologies can peer into the human genome. Google\u2019s open-source tool, DeepVariant, identifies genomic variants in sequencing data using a convolutional neural network, and this year won the FDA Challenge for best accuracy in 3 out of 4 categories. Using this same tool, a study led by the Dana-Farber Cancer Institute improved diagnostic yield by 14% for genetic variants that lead to prostate cancer and melanoma in a cohort of 2,367 cancer patients.FDA Challenge for best accuracy in 3 out of 4 categoriesstudy led by the Dana-Farber Cancer Institute\nResearch doesn\u2019t end at measurement of experimental accuracy. Ultimately, truly helping patients receive better care requires understanding how ML tools will affect people in the real world. This year we began work with Mayo Clinic to develop a machine learning system to assist in radiotherapy planning and to better understand how this technology could be deployed into clinical practice. With our partners in Thailand, we\u2019ve used diabetic eye disease screening as a test case in how we can build systems with people at the center, and recognize the fundamental role of diversity, equity, and inclusion in building tools for a healthier world.radiotherapy planningpeople at the center the fundamental role of diversity, equity, and inclusionTopTop \nWeather, Environment and Climate Change\nMachine learning can help us better understand the environment and make useful predictions to help people in both their everyday life as well as in disaster situations. For weather and precipitation forecasting, computationally intensive physics-based models like NOAA\u2019s HRRR have long reigned supreme. We have been able to show, though, that ML-based forecasting systems can predict current precipitation with much better spatial resolution (\u201cIs it raining in my local park in Seattle?\u201d and not just \u201cIs it raining in Seattle?\u201d) and can produce short-term forecasts of up to eight hours that are considerably more accurate than HRRR, and can compute the forecast more quickly, yet with higher temporal and spatial resolution.HRRRcan predict current precipitation with much better spatial resolutioncan produce short-term forecasts of up to eight hours\nWe\u2019ve also developed an improved technique called HydroNets, which uses a network of neural networks to model the actual river systems in the world to more accurately understand the interactions of upstream water levels to downstream inundation, resulting in more accurate water-level predictions and flood forecasting. Using these techniques, we've expanded our coverage of flood alerts by 20x in India and Bangladesh, helping to better protect more than 200 million people in 250,000 square kilometers.developed an improved technique called HydroNetsour coverage of flood alerts by 20x in India and Bangladesh\nBetter analysis of satellite imagery data can also give Google users a better understanding of the impact and extent of wildfires (which caused devastating effects in California and Australia this year). We showed that automated analysis of satellite imagery can help with rapid assessment of damage after natural disasters even with limited prior satellite imagery. It can also aid urban tree-planting efforts by helping cities assess their current tree canopy coverage and where they should focus on planting new trees. We\u2019ve also shown how machine learning techniques that leverage temporal context can help improve ecological and wildlife monitoring.better understanding of the impact and extent of wildfiresrapid assessment of damage after natural disasterslimited prior satellite imageryurban tree-planting effortsmachine learning techniques that leverage temporal context\nBased on this work, we\u2019re excited to partner with NOAA on using AI and ML to amplify NOAA\u2019s environmental monitoring, weather forecasting and climate research using Google Cloud\u2019s infrastructure.partner with NOAATopTop \nAccessibility\nMachine learning continues to provide amazing opportunities for improving accessibility, because it can learn to transfer one kind of sensory input into others. As one example, we released Lookout, an Android application that can help visually impaired users by identifying packaged foods, both in a grocery store and also in their kitchen cupboard at home. The machine learning system behind Lookout demonstrates that a powerful-but-compact machine learning model can accomplish this in real-time on a phone for nearly 2 million products.Lookoutmachine learning system behind Lookout\nSimilarly, people who communicate with sign language find it difficult to use video conferencing systems because even if they are signing, they are not detected as actively speaking by audio-based speaker detection systems. Developing Real-Time, Automatic Sign Language Detection for Video Conferencing presents a real-time sign language detection model and demonstrates how it can be used to provide video conferencing systems with a mechanism to identify the person signing as the active speaker.Developing Real-Time, Automatic Sign Language Detection for Video Conferencing\nWe also enabled useful Android accessibility capabilities such as Voice Access and Sound Notifications for important household sounds.Voice AccessSound Notifications\nLive Caption was expanded to support calls on the Pixel phone with the ability to caption phone calls and video calls. This came out of the Live Relay research project, which enables deaf and hard of hearing people to make calls without assistance.Live CaptionLive Relaydeaf and hard of hearing people to make calls without assistanceTopTop \nApplications of ML to Other Fields\nMachine learning continues to prove vital in helping us make progress across many fields of science. In 2020, in collaboration with the FlyEM team at HHMI Janelia Research Campus, we released the drosophila hemibrain connectome, the large synapse-resolution map of brain connectivity, reconstructed using large-scale machine learning models applied to high-resolution electron microscope imaging of brain tissue. This connectome information will aid neuroscientists in a wide variety of inquiries, helping us all better understand how brains function. Be sure to check out the very fly interactive 3-D UI!FlyEMJanelia Research Campusreleased the drosophila hemibrain connectomevery fly interactive 3-D UI\nThe application of ML to problems in systems biology is also on the rise. Our Google Accelerated Science team, in collaboration with our colleagues at Calico, have been applying machine learning to yeast, to get a better understanding of how genes work together as a whole system. We\u2019ve also been exploring how to use model-based reinforcement learning in order to design biological sequences like DNA or proteins that have desirable properties for medical or industrial uses. Model-based RL is used to improve sample efficiency. At each round of experimentation the policy is trained offline using a simulator fit on functional measurements from prior rounds. On various tasks like designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of Ising models based on protein structures, we find that model-based RL is an attractive alternative to existing methods.Google Accelerated Scienceapplying machine learning to yeastmodel-based reinforcement learning in order to design biological sequencesIsing models\nIn partnership with X-Chem Pharmaceuticals and ZebiAI, we have also been developing ML techniques to do \u201cvirtual screening\u201d of promising molecular compounds computationally. Previous work in this area has tended to focus on relatively small sets of related compounds, but in this work, we are trying to use DNA-encoded small molecule libraries in order to be able to generalize to find \u201chits\u201d across a wide swath of chemical space, reducing the need for slower, physical-based lab work in order to progress from idea to working pharmaceutical.X-Chem PharmaceuticalsZebiAIdeveloping ML techniques to do \u201cvirtual screening\u201d of promising molecular compounds computationally\nWe\u2019ve also seen success applying machine learning to core computer science and computer systems problems, a growing trend that is spawning entire new conferences like MLSys. In Learning-based Memory Allocation for C++ Server Workloads, a neural network-based language model predicts context-sensitive per-allocation site object lifetime information, and then uses this to organize the heap so as to reduce fragmentation. It is able to reduce fragmentation by up to 78% while only using huge pages (which are better for TLB behavior). End-to-End, Transferable Deep RL for Graph Optimization described an end-to-end transferable deep reinforcement learning method for computational graph optimization that shows 33%-60% speedup on three graph optimization tasks compared to TensorFlow default optimization, with 15x faster convergence over prior computation graph optimization methods.MLSysLearning-based Memory Allocation for C++ Server WorkloadsTLBEnd-to-End, Transferable Deep RL for Graph OptimizationTensorFlow\nAs described in Chip Design with Deep Reinforcement Learning, we have also been applying reinforcement learning to the problem of place-and-route in computer chip design. This is normally a very time-consuming, labor-intensive process, and is a major reason that going from an idea for a chip to actually having a fully designed and fabricated chip takes so long. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. The system is able to generate placements that usually outperform those of human chip design experts, and we have been using this system (running on TPUs) to do placement and layout for major portions of future generations of TPUs. Menger is a recent infrastructure we\u2019ve built for large-scale distributed reinforcement learning that is yielding promising performance for difficult RL tasks such as chip design.Chip Design with Deep Reinforcement LearningMengeropen-source RISC-VTopTop \nResponsible AI \nThe Google AI Principles guide our development of advanced technologies. We continue to invest in responsible AI research and tools, update our recommended technical practices in this area, and share regular updates \u2014 including a 2020 blog post and report \u2014 on our progress in implementation.Google AI Principlesresponsible AI researchtoolsrecommended technical practicesshare regular updatesblog postreport\nTo help better understand the behavior of language models, we developed the Language Interpretability Tool\u00a0(LIT), a toolkit for better interpretability of language models, enabling interactive exploration and analysis of their decisions. We developed techniques for measuring gendered correlations in pre-trained language models and scalable techniques for reducing gender bias in Google Translate. We used the kernel trick to propose a simple method to estimate the influence of a training data example on an individual prediction. To help non-specialists interpret machine learning results, we extended the TCAV technique introduced in 2019 to now provide a complete and sufficient set of concepts. With the original TCAV work, we were able to say that \u2018fur\u2019 and \u2018long ears\u2019 are important concepts for \u2018rabbit\u2019 prediction. With this work, we can also say that these two concepts are enough to fully explain the prediction; you don\u2019t need any other concepts. Concept bottleneck models are a technique to make models more interpretable by training them so that one of the layers is aligned with pre-defined expert concepts (e.g., \u201cbone spurs present\u201d, or \u201cwing color\u201d, as shown below) before making a final prediction for a task, so that we can not only interpret but also turn on/off these concepts on the fly.Language Interpretability Toolmeasuring gendered correlations in pre-trained language modelsscalable techniques for reducing gender bias in Google Translate.to propose a simple method to estimate the influence of a training data example on an individual predictionTCAV techniquecomplete and sufficient set of conceptsConcept bottleneck modelsConcept Bottleneck Models.\nIn collaboration with many other institutions, we also looked into memorization effects of language models, showing that training data extraction attacks are realistic threats on state-of-the-art large language models. This finding along with a result that embedding models can leak information can have significant privacy implications (especially for models trained on private data). In Thieves of Sesame Street: Model Extraction on BERT-based APIs, we demonstrated that attackers with only API access to a language model could create models whose outputs had very high correlation with the original model, even with relatively few API queries to the original model. Subsequent work demonstrated that attackers can extract smaller models with arbitrary accuracy. On the AI Principle of safety we demonstrated that thirteen published defenses to adversarial examples can be circumvented despite attempting to perform evaluations using adaptive attacks. Our work focuses on laying out the methodology and the approach necessary to perform an adaptive attack, and thus will allow the community to make further progress in building more robust models.memorization effects of language modelsembedding models can leak informationThieves of Sesame Street: Model Extraction on BERT-based APIsextract smaller models with arbitrary accuracydefenses to adversarial examples can be circumvented\nExamining the way in which machine learning systems themselves are examined is also an important area of exploration. In collaboration with the Partnership on AI, we defined a framework for how to audit the use of machine learning in software product settings, drawing on lessons from the aerospace, medical devices, and finance industries and their best practices. In joint work with University of Toronto and MIT, we identified several ethical concerns that can arise when auditing the performance of facial recognition systems. In joint work with the University of Washington, we identified some important considerations related to diversity and inclusion when choosing subsets for evaluating algorithmic fairness. As an initial step in making responsible AI work for the next billion users and to help understand if notions of fairness were consistent in different parts of the world, we analyzed and created a framework for algorithmic fairness in India, accounting for datasets, fairness optimizations, infrastructures, and ecosystemsPartnership on AIframework for how to audit the use of machine learning in software product settingsseveral ethical concerns that can arise when auditing the performance of facial recognition systemsimportant considerations related to diversity and inclusion when choosing subsetsframework for algorithmic fairness in India\nThe Model Cards work that was introduced in collaboration with the University of Toronto in 2019 has been growing in influence.  Indeed, many well-known models like OpenAI\u2019s GPT-2 and GPT-3, many of Google\u2019s MediaPipe models and various Google Cloud APIs have all adopted Model Cards as a way of giving users of a machine learning model more information about the model\u2019s development and the observed behavior of the model under different conditions. To make this easier for others to adopt for their own machine learning models, we also introduced the Model Card Toolkit for easier model transparency reporting. In order to increase transparency in ML development practices, we demonstrate the applicability of a range of best practices throughout the dataset development lifecycle, including data requirements specification and data acceptance testing.Model CardsGPT-2GPT-3MediaPipe modelsGoogle Cloud APIsModel Card Toolkitbest practices throughout the dataset development lifecycle\nIn collaboration with the U.S. National Science Foundation (NSF), we announced and helped to fund a National AI Research Institute for Human-AI Interaction and Collaboration. We also released the MinDiff framework, a new regularization technique available in the TF Model Remediation library for effectively and efficiently mitigating unfair biases when training ML models, along with ML-fairness gym for building simple simulations that explore potential long-run impacts of deploying machine learning-based decision systems in social environments.announced and helped to fund a National AI Research Institute for Human-AI Interaction and CollaborationMinDiff frameworkTF Model Remediation libraryML-fairness gym\nIn addition to developing frameworks for fairness, we developed approaches for identifying and improving the health and quality of experiences with Recommender Systems, including using reinforcement learning to introduce safer trajectories. We also continue to work on improving the reliability of our machine learning systems, where we\u2019ve seen that approaches such as generating adversarial examples can improve robustness and that robustness approaches can improve fairness.reinforcement learning to introduce safer trajectoriesgenerating adversarial examplesrobustness approaches can improve fairness\nDifferential privacy is a way to formally quantify privacy protections and requires a rethinking of the most basic algorithms to operate in a way that they do not leak information about any particular individual. In particular, differential privacy can help in addressing memorization effects and information leakage of the kinds mentioned above. In 2020 there were a number of exciting developments, from more efficient ways of computing private empirical risk minimizers to private clustering methods with tight approximation guarantees and private sketching algorithms. We also open sourced the differential privacy libraries that lie at the core of our internal tools, taking extra care to protect against leakage caused by the floating point representation of real numbers. These are the exact same tools that we use to produce differentially private COVID-19 mobility reports that have been a valuable source of anonymous data for researchers and policymakers.Differential privacyempirical risk minimizersprivate clustering methodsprivate sketching algorithmsopen sourced the differential privacy librariesextra careCOVID-19 mobility reports\nTo help developers assess the privacy properties of their classification models we released an ML privacy testing library in Tensorflow. We hope this library will be the starting point of a robust privacy testing suite that can be used by any machine learning developer around the world.ML privacy testing library in TensorflowCIFAR10\nIn addition to pushing the state of the art in developing private algorithms, I am excited about the advances we made in weaving privacy into the fabric of our products. One of the best examples is Chrome\u2019s Privacy Sandbox, which changes the underpinnings of the advertising ecosystem and helps systematically protect individuals\u2019 privacy. As part of the project, we proposed and evaluated a number of different APIs, including federated learning of cohorts (FLoC) for interest based targeting, and aggregate APIs for differentially private measurement.Privacy SandboxevaluatedFLoCprivate measurement\nLaunched in 2017, federated learning is now a complete research field unto itself, with over 3000 publications on federated learning appearing in 2020 alone. Our cross-institutional Advances and Open Problems in Federated Learning survey paper published in 2019 has been cited 367 times in the past year, and an updated version will soon be published in the Foundations & Trends in Machine Learning series. In July, we hosted a Workshop on Federated Learning and Analytics, and made all research talks and a TensorFlow Federated tutorial publicly available.federated learning3000 publicationsAdvances and Open Problems in Federated Learning367 timesbe publishedall research talksTensorFlow Federated tutorial\nWe continue to push the state of the art in federated learning, including the development of new federated optimization algorithms including adaptive learning algorithms, posterior averaging algorithms, and techniques for mimicking centralized algorithms in federated settings, substantial improvements in complimentary cryptographic protocols, and more. We announced and deployed federated analytics, enabling data science over raw data that is stored locally on users\u2019 devices. New uses of federated learning in Google products include contextual emoji suggestions in Gboard, and pioneering privacy-preserving medical research with Google Health Studies. Furthermore, in Privacy Amplification via Random Check-Ins we presented the first privacy accounting mechanism for Federated Learning.adaptive learning algorithmsposterior averaging algorithmstechniques for mimicking centralized algorithms in federated settingscryptographic protocolsfederated analyticscontextual emoji suggestionsGoogle Health StudiesPrivacy Amplification via Random Check-Ins\nSecurity for our users is also an area of considerable interest for us. In 2020, we continued to improve protections for Gmail users, by deploying a new ML-based document scanner that provides protection against malicious documents, which increased malicious office document detection by 10% on a daily basis. Thanks to its ability to generalize, this tool has been very effective at blocking some adversarial malware campaigns that elude other detection mechanisms and increased our detection rate by 150% in some cases.new ML-based document scanner\nOn the account protection side, we released a fully open-source security key firmware to help advance state of art in the two factor authentication space, staying focused on security keys as the best way to protect accounts against phishing.a fully open-source security key firmwareTopTop \nNatural Language Understanding\nBetter understanding of language is an area where we saw considerable progress this year. Much of the work in this space from Google and elsewhere now relies on Transformers, a particular style of neural network model originally developed for language problems (but with a growing body of evidence that they are also useful for images, videos, speech, protein folding, and a wide variety of other domains).Transformersoriginally developed for language problemsimagesvideosspeechprotein folding\nOne area of excitement is in dialog systems that can chat with a user about something of interest, often encompassing multiple turns of interaction. While successful work in this area to date has involved creating systems that are specialized around particular topics (e.g., Duplex) these systems cannot carry on general conversations. In pursuit of the general research goal of creating systems capable of much more open-ended dialog, in 2020 we described Meena, a learned conversational agent that aspirationally can chat about anything. Meena achieves high scores on a dialog system metric called SSA, which measures both sensibility and specificity of responses. We\u2019ve seen that as we scale up the model size of Meena, it is able to achieve lower perplexity and, as shown in the paper, lower perplexity correlates extremely closely with improved SSA.DuplexMeenapaper\nOne well-known issue with generative language models and dialog systems is that when discussing factual data, the model\u2019s capacity may not be large enough to remember every specific detail about a topic, so they generate language that is plausible but incorrect. (This is not unique to machines \u2014 people can commit these errors too.) To address this in dialog systems, we are exploring ways to augment a conversational agent by giving it access to external information sources (e.g., a large corpus of documents or a search engine API), and developing learning techniques to use this as an additional resource in order to generate language that is consistent with the retrieved text. Work in this area includes integrating retrieval into language representation models (and a key underlying technology for this to work well is something like ScaNN, an efficient vector similarity search, to efficiently match the desired information to information in the corpus of text). Once appropriate content is found, it can be better understood with approaches like using neural networks to find answers in tables\u00a0and extracting structured data from templatic documents. Our work on PEGASUS, a state-of-the-art model for abstractive text summarization can also help to create automatic summaries from any piece of text, a general technique useful in conversations, retrieval systems, and many other places.integrating retrieval into language representation modelsScaNN, an efficient vector similarity searchusing neural networks to find answers in tablesextracting structured data from templatic documentsPEGASUS, a state-of-the-art model for abstractive text summarization\nEfficiency of NLP models has also been a significant focus for our work in 2020. Techniques like transfer learning and multi-task learning can dramatically help with making general NLP models usable for new tasks with modest amounts of computation. Work in this vein includes transfer learning explorations in T5, sparse activation of models (as in our GShard work mentioned below), and more efficient model pre-training with ELECTRA. Several threads of work also look to improve on the basic Transformer architecture, including Reformer, which uses locality-sensitive hashing and reversible computation to more efficiently support much larger attention windows, Performers, which use an approach for attention that scales linearly rather than quadratically (and discusses its use in the context of protein modeling), and ETC and BigBird, which utilize global and sparse random connections, to enable linear scaling for larger and structured sequences. We also explored techniques for creating very lightweight NLP models that are 100x smaller than a larger BERT model, but perform nearly as well for some tasks, making them very suitable for on-device NLP. In Encode, Tag and Realize, we also explored new approaches for generative text models that use edit operations rather than fully general text generation, which can have advantages in computation requirements for generation, more control over the generated text, and require less training data.transfer learning explorations in T5GShard workmore efficient model pre-training with ELECTRAReformerPerformersETCBigBirdcreating very lightweight NLP modelsEncode, Tag and RealizeTopTop \nLanguage Translation\nEffective language translation helps bring the world closer together by enabling us to all communicate, despite speaking different languages. To date, over a billion people around the world use Google Translate, and last year we added support for five new languages (Kinyarwanda, Odia, Tatar, Turkmen and Uyghur, collectively spoken by 75 million people). Translation quality continues to improve, showing an average +5 BLEU point gain across more than 100 languages from May 2019 to May 2020, through a wide variety of techniques like improved model architectures and training, better handling of noise in datasets, multilingual transfer and multi-task learning, and better use of monolingual data to improve low-resource languages (those without much written public content on the web), directly in line with our goals of improving ML fairness of machine learning systems to provide benefits to the greatest number of people possible.five new languagesTranslation quality continues to improveBLEU\nWe strongly believe that continued scaling of multilingual translation models will bring further quality improvements, especially to the billions of speakers of low-resource languages around the world. In GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, Google researchers showed that training sparsely-activated multilingual translation models of up to 600 billion parameters leads to major improvements in translation quality for 100 languages as measured by BLEU score improvement over a baseline of a separate 400M parameter monolingual baseline model for each language. Three trends stood out in this work, illustrated by Figure 6 in the paper, reproduced below (see the paper for complete discussion):GShard: Scaling Giant Models with Conditional Computation and Automatic ShardingGShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nWe\u2019re actively working on bringing the benefits demonstrated in this GShard research work to Google Translate, as well as training single models that cover 1000 languages, including languages like Dhivehi and Sudanese Arabic (while sharing some challenges that needed solving along the way).some challenges that needed solving\nWe also developed techniques to create language-agnostic representations of sentences for BERT models, which can help with developing better translation models. To more effectively evaluate translation quality, we introduced BLEURT, a new metric for evaluating language generation for tasks like translation that considers the semantics of the generated text, rather than just the amount of word overlap with ground-truth data, illustrated in the table below.language-agnostic representations of sentences for BERT modelsBLEURT, a new metric for evaluating language generationTopTop \nMachine Learning Algorithms\nWe continue to develop new machine learning algorithms and approaches for training that enable systems to learn more quickly and from less supervised data. By replaying intermediate results during training of neural networks, we find that we can fill idle time on ML accelerators and therefore can train neural networks faster. By changing the connectivity of neurons dynamically during training, we can find better solutions compared with statically-connected neural networks. We also developed SimCLR, a new self-supervised and semi-supervised learning technique that simultaneously maximizes agreement between differently transformed views of the same image and minimizes agreement between transformed views of different images. This approach significantly improves on the best self-supervised learning techniques.replaying intermediate results during training of neural networkschanging the connectivity of neurons dynamically during trainingSimCLRImageNetResNet-50\nWe also extended the idea of contrastive learning to the supervised regime, resulting in a loss function that significantly improves over cross-entropy for supervised classification problems.contrastive learning to the supervised regimeTopTop \nReinforcement Learning\nReinforcement learning (RL), which learns to make good long-term decisions from limited experience, has been an important focus area for us. An important challenge in RL is to learn to make decisions from few data points, and we\u2019ve improved RL algorithm efficiency through learning from fixed datasets, learning from other agents, and improving exploration.\nA major focus area this year has been around offline RL, which relies solely on fixed, previously collected datasets (for example, from previous experiments or human demonstrations), extending RL to the applications that can\u2019t collect training data on-the-fly. We\u2019ve introduced a duality approach to RL, developed improved algorithms for off-policy evaluation, estimating confidence intervals, and offline policy optimization. In addition, we\u2019re collaborating with the broader community to tackle these problems by releasing open-source benchmark datasets, and DQN dataset for Atari.introduced a duality approach to RLoff-policy evaluationconfidenceintervalsoffline policy optimizationopen-source benchmark datasetsDQN dataset for Atari\nAnother line of research improved sample efficiency by learning from other agents through apprenticeship learning. We developed methods to learn from informed agents, matching other agent\u2019s distribution, or learning from adversarial examples. To improve the exploration in RL, we explored bonus-based exploration methods including imitation techniques able to mimic structured exploration arising in agents having prior knowledge about their environment.informed agentsmatching other agent\u2019s distributionadversarial examplesbonus-based explorationmimic structured exploration\nWe\u2019ve also made significant advances in the mathematical theory of reinforcement learning. One of our main areas of research was studying reinforcement learning as an optimization process. We found connections to the Frank-Wolfe algorithm, momentum methods, KL divergence regularization, operator theory, and convergence analysis; some of these insights led to an algorithm that achieves state-of-the-art performance in challenging RL benchmarks and discovery that polynomial transfer functions avoid convergence problems associated with softmax, both in RL and supervised learning. We\u2019ve made some exciting progress on the topic of safe reinforcement learning, where one seeks to discover optimal control rules while respecting important experimental constraints. This includes a framework for safe policy optimization. We studied efficient RL-based algorithms for solving a class of problems known as mean field games, which model systems with a large number of decision-makers, from mobile networks to electric grids.Frank-Wolfe algorithmmomentumKL divergence regularizationoperator theoryconvergence analysisalgorithm that achieves state-of-the-art performancediscovery that polynomial transfer functionsframework for safe policy optimizationefficient RL-based algorithms\nWe\u2019ve made breakthroughs toward generalization to new tasks and environments, an important challenge for scaling up RL to complex real-world problems. A 2020 focus area was population-based learning-to-learn methods, where another RL or evolutionary agent trained a population of RL agents to create a curriculum of emergent complexity, and discover new state-of-the-art RL algorithms. Learning to estimate the importance of data points in the training set and parts of visual input with selective attention resulted in significantly more skillful RL agents.emergent complexitynew state-of-the-art RL algorithmsestimate the importance of data pointsparts of visual input\nFurther, we made progress in model-based RL by showing that learning predictive behavior models accelerates RL learning, and enables decentralized cooperative multi-agent tasks in diverse teams, and learning long-term behavior models. Observing that skills bring predictable changes in the environment, we discover skills without supervision. Better representations stabilize RL learning, while hierarchical latent spaces and value-improvement paths yield better performance.accelerates RL learningdecentralized cooperativelong-term behavior models skills without supervisionstabilize RL learninghierarchical latent spacesvalue-improvement paths\nWe shared open source tools for scaling up and productionizing RL. To expand the scope and problems tackled by users, we\u2019ve introduced SEED, a massively parallel RL agent, released a library for measuring the RL algorithm reliability, and a new version of TF-Agents that includes distributed RL, TPU support, and a full set of bandit algorithms. In addition, we performed a large empirical study of RL algorithms to improve hyperparameter selection and algorithm design.SEEDmeasuring the RL algorithm reliabilityTF-Agentsbanditlarge empirical study\nFinally, in collaboration with Loon, we trained and deployed RL to more efficiently control stratospheric balloons, improving both power usage and their ability to navigate.trained and deployed RL to more efficiently control stratospheric balloonsTopTop \nAutoML\nUsing learning algorithms to develop new machine learning techniques and solutions, or meta-learning, is a very active and exciting area of research. In much of our previous work in this area, we\u2019ve created search spaces that look at how to find ways to combine sophisticated hand-designed components together in interesting ways. In AutoML-Zero: Evolving Code that Learns, we took a different approach, by giving an evolutionary algorithm a search space consisting of very primitive operations (like addition, subtraction, variable assignment, and matrix multiplication) in order to see if it was possible to evolve modern ML algorithms from scratch. The presence of useful learning algorithms in this space is incredibly sparse, so it is remarkable that the system was able to progressively evolve more and more sophisticated ML algorithms. As shown in the figure below, the system reinvents many of the most important ML discoveries over the past 30 years, such as linear models, gradient descent, rectified linear units, effective learning rate settings and weight initializations, and gradient normalization.AutoML-Zero: Evolving Code that Learns\nWe also used meta-learning to discover a variety of new efficient architectures for object detection in both still images and videos.  Last year\u2019s work on EfficientNet for efficient image classification architectures showed significant accuracy improvements and computational cost reductions for image classification. In follow-on work this year, EfficientDet: Towards Scalable and Efficient Object Detection builds on top of the EfficientNet work to derive new efficient architectures for object detection and localization, showing remarkable improvements in both highest absolute accuracy, as well as computational cost reductions of 13-42x over previous approaches to achieve a given level of accuracy.EfficientNetEfficientDet: Towards Scalable and Efficient Object Detection\nOur work on SpineNet describes a meta-learned architecture that can retain spatial information more effectively, allowing detection to be done at finer resolution. We also focused on learning effective architectures for a variety of video classification problems. AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures, AssembleNet++: Assembling Modality Representations via Attention Connections, and AttentionNAS: Spatiotemporal Attention Cell Search for Video Classification demonstrate how to use evolutionary algorithms to create novel state-of-the-art video processing machine learning architectures.SpineNetAssembleNet: Searching for Multi-Stream Neural Connectivity in Video ArchitecturesAssembleNet++: Assembling Modality Representations via Attention ConnectionsAttentionNAS: Spatiotemporal Attention Cell Search for Video Classification\nThis approach can also be used to develop effective model architectures for time series forecasting. Using AutoML for Time Series Forecasting describes the system that discovers new forecasting models via an automated search over a search space involving many interesting kinds of low-level building blocks, and its effectiveness was demonstrated in the Kaggle M5 Forecasting Competition, by generating an algorithm and system that placed 138th out of 5558 participants (top 2.5%). While many of the competitive forecasting models required months of manual effort to create, our AutoML solution found the model in a short time with only a moderate compute cost (500 CPUs for 2 hours) and no human intervention.Using AutoML for Time Series ForecastingM5 Forecasting CompetitionTopTop \nBetter Understanding of ML Algorithms and Models\nDeeper understanding of machine learning algorithms and models is crucial for designing and training more effective models, as well as understanding when models may fail. Last year, we focused on fundamental questions around representation power, optimization, model generalization, and label noise, among others. As mentioned earlier in this post, Transformer networks have had a huge impact on modeling language, speech and vision problems, but what is the class of functions represented by these models? Recently we showed that transformers are universal approximators for sequence-to-sequence functions. Furthermore, sparse transformers also remain universal approximators even when they use just a linear number of interactions among the tokens. We have been developing new optimization techniques based on layerwise adaptive learning rates to improve the convergence speed of transformers, e.g., Large batch optimization for deep learning (LAMB): Training BERT in 76 minutes.Transformer networkstransformers are universal approximators for sequence-to-sequence functionssparse transformers also remain universal approximatorsLarge batch optimization for deep learning (LAMB): Training BERT in 76 minutes\nAs neural networks are made wider and deeper, they often train faster and generalize better. This is a core mystery in deep learning since classical learning theory suggests that large networks should overfit more. We are working to understand neural networks in this overparameterized regime. In the limit of infinite width, neural networks take on a surprisingly simple form, and are described by a Neural Network Gaussian Process (NNGP) or Neural Tangent Kernel (NTK). We studied this phenomenon theoretically and experimentally, and released Neural Tangents, an open-source software library written in JAX that allows researchers to build and train infinite-width neural networks.theoreticallyexperimentallyNeural Tangentsopen-source software libraryJAX\nAs finite width networks are made larger, they also demonstrate peculiar double descent phenomena \u2014 where they generalize better, then worse, then better again with increasing width. We have shown that this phenomenon can be explained by a novel bias-variance decomposition, and further that it can sometimes manifest as triple descent.bias-variance decomposition, and further that it can sometimes manifest as triple descent\nLastly, in real-world problems, one often needs to deal with significant label noise. For instance, in large scale learning scenarios, weakly labeled data is available in abundance with large label noise. We have developed new techniques for distilling effective supervision from severe label noise leading to state-of-the-art results. We have further analyzed the effects of training neural networks with random labels, and shown that it leads to alignment between network parameters and input data, enabling faster downstream training than initializing from scratch. We have also explored questions such as whether label smoothing or gradient clipping can mitigate label noise, leading to new insights for developing robust training techniques with noisy labels.distilling effective supervision from severe label noisetraining neural networks with random labelslabel smoothinggradient clippingTopTop \nAlgorithmic Foundations and Theory\n2020 was a productive year for our work in algorithmic foundations and theory, with several impactful research publications and notable results. On the optimization front, our paper on edge-weighted online bipartite matching develops a new technique for online competitive algorithms and solves a thirty-year old open problem for the edge-weighted variant with applications in efficient online ad allocation. Along with this work in online allocation, we developed dual mirror descent techniques that generalize to a variety of models with additional diversity and fairness constraints, and published a sequence of papers on the topic of online optimization with ML advice in online scheduling, online learning and online linear optimization. Another research result gave the first improvement in 50 years on the classic bipartite matching in dense graphs. Finally, another paper solves a long-standing open problem about chasing convex bodies online \u2014 using an algorithm from The Book, no less.edge-weighted online bipartite matchingcompetitive algorithms thirty-year old open problemdual mirror descent techniquesfairness online schedulingonline learningonline linear optimizationAnother research result paperan algorithm from The Book\nWe also continued our work in scalable graph mining and graph-based learning and hosted the Graph Mining & Learning at Scale Workshop at NeurIPS\u201920, which covered work on scalable graph algorithms including graph clustering, graph embedding, causal inference, and graph neural networks. As part of the workshop, we showed how to solve several fundamental graph problems faster, both in theory and practice, by augmenting standard synchronous computation frameworks like MapReduce with a distributed hash-table similar to a BigTable. Our extensive empirical study validates the practical relevance of the AMPC model inspired by our use of distributed hash tables in massive parallel algorithms for hierarchical clustering and connected components, and our theoretical results show how to solve many of these problems in constant distributed rounds, greatly improving upon our previous results. We also achieved exponential speedup for computing PageRank and random walks. On the graph-based learning side, we presented Grale, our framework for designing graphs for use in machine learning. Furthermore, we presented our work on more scalable graph neural network models, where we show that PageRank can be used to greatly accelerate inference in GNNs.Graph Mining & Learning at Scale Workshopsolve several fundamental graph problems faster, both in theory and practiceMapReduceBigTableAMPC modelprevious resultscomputing PageRank and random walksGralescalable graph neural network modelsPageRank\nIn market algorithms, an area at the intersection of computer science and economics, we continued our research in designing improved online marketplaces, such as measuring incentive properties of ad auctions,  two-sided markets, and optimizing order statistics in ad selection. In the area of repeated auctions, we developed frameworks to make dynamic mechanisms robust against lack of forecasting or estimation errors of the current market and/or the future market, leading to provably tight low-regret dynamic mechanisms. Later, we characterized when it is possible to achieve the asymptotically optimal objective through geometry-based criteria. We also compared the equilibrium outcome of a range of budget management strategies used in practice, showed their impact on the tradeoff between revenue and buyers' utility and shed light on their incentive properties. Additionally, we continued our research in learning optimal auction parameters, and settled the complexity of batch-learning with revenue loss. We designed the optimal regret and studied combinatorial optimization for contextual auction pricing, and developed a new active learning framework for auctions and improved the approximation for posted-price auctions. Finally, motivated by the importance of incentives in ad auctions, and in the hope to help advertisers study the impact of incentives in auctions, we introduce a data-driven metric to quantify how much a mechanism deviates from incentive compatibility.measuring incentive properties of ad auctions two-sided marketsoptimizing order statistics in ad selectionlack of forecastingthe current marketthe future marketachieve the asymptotically optimal objectivecompared the equilibrium outcome of a range of budget management strategiesshed light on their incentive propertiescomplexity of batch-learning with revenue lossoptimal regretstudied combinatorial optimizationnew active learning framework for auctionsimproved the approximation for posted-price auctionsdata-driven metric to quantify how much a mechanism deviates from incentive compatibilityTopTop \nMachine Perception\nPerceiving the world around us \u2014 understanding, modeling and acting on visual, auditory and multimodal input \u2014 continues to be a research area with tremendous potential to be beneficial in our everyday lives.\nIn 2020, deep learning powered new approaches that bring 3D computer vision and computer graphics closer together. CvxNet, deep implicit functions for 3D shapes, neural voxel rendering and CoReNet are a few examples of this direction. Furthermore, our research on representing scenes as neural radiance fields (aka NeRF, see also this blog post) is a good example of how Google Research's academic collaborations stimulate rapid progress in the area of neural volume rendering.CvxNetdeep implicit functions for 3D shapesneural voxel renderingCoReNetNeRFblog post\nIn Learning to Factorize and Relight a City, a collaboration with UC Berkeley, we proposed a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. This gives the ability to change lighting effects and scene geometry for any Street View panorama, or even turn it into a full-day timelapse video.Learning to Factorize and Relight a City\nOur work on generative human shape and articulated pose models introduces a statistical, articulated 3D human shape modeling pipeline, within a fully trainable, modular, deep learning framework. Such models enable 3D human pose and shape reconstruction of people from a single photo to better understand the scene.generative human shape and articulated pose modelsGHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models\nThe growing area of media compression using neural networks continued to make strong progress in 2020, not only on learned image compression, but also in deep approaches to video compression, volume compression and nice results in deep distortion-agnostic image watermarking.learned image compressionvideo compressionvolume compressiondistortion-agnostic image watermarkingDistortion Agnostic Deep Watermarking\nAdditional important themes in perceptual research included:self-training with noisy studentslearning from simulated datalearning with noisy labelscontrastive learningexploiting cross-modal supervisionaudiovisual speech enhancementlanguage groundingOpen Images (V6) update featuring localized narrativesfast sparse convnetsstructured multi-hashing for model compressiondetecting 3D objects and predicting 3D shapes3D scene reconstruction from a single RGB imageleveraging temporal context for object detectionlearning to see transparent objectsestimating their pose from stereoautomatic creation of videos from web pagesintelligent video reframingusing GANs to create fantastical creaturesrelighting portraits\nEngaging with the broader research community through open sourcing of solutions and datasets is another important aspect of furthering perceptual research. In 2020, we open sourced multiple new perceptual inference capabilities and solutions in MediaPipe, such as on-device face, hand and pose prediction, real-time body pose tracking, real-time iris tracking and depth estimation, and real-time 3D object detection.MediaPipeon-device face, hand and pose predictionreal-time body pose trackingreal-time iris tracking and depth estimationreal-time 3D object detection\nWe continued to make strides to improve experiences and promote helpfulness on mobile devices through ML-based solutions. Our ability to run sophisticated natural language processing on-device, enabling more natural conversational features, continues to improve. In 2020, we expanded Call Screen and launched Hold for Me to allow users to save time when performing mundane tasks, and we also launched language-based actions and language navigability of our Recorder app to aid productivity.Call ScreenHold for Melanguage-based actionslanguage navigability of our Recorder app\nWe have used Google's Duplex technology to make calls to businesses and confirm things like temporary closures. This has enabled us to make 3 million updates to business information globally, that have been seen over 20 billion times on Maps and Search. We also used text to speech technology for easier access to web pages, by enabling Google Assistant to read it aloud, supporting 42 languages.Duplexenabled read it aloud\nWe also continued to make meaningful improvements to imaging applications. We made it easier to capture precious moments on Pixel with innovative controls and new ways to relight, edit, enhance and relive them again in Google Photos. For the Pixel camera, beginning with Pixel 4 and 4a, we added Live HDR+, which uses machine learning to approximate the vibrance and balanced exposure and appearance of HDR+ burst photography in real time in the viewfinder. We also created dual exposure controls, which allow the brightness of shadows and highlights in a scene to be adjusted independently \u2014 live in the viewfinder.captureinnovative controlsrelighteditenhancereliveLive HDR+HDR+ burst photography\nMore recently, we introduced Portrait Light, a new post-capture feature for the Pixel Camera and Google Photos apps that adds a simulated directional light source to portraits.  This feature is again one that is powered by machine learning, having been trained on 70 different people, photographed one light at a time, in our pretty cool 331-LED Light Stage computational illumination system.Portrait LightLight Stage\nIn the past year, Google researchers were excited to contribute to many new (and timely) ways of using Google products. Here are a few examplesget help with homework or explore concepts in 3Dbackground blur & replace in Google Meettry products onkey moments in videohummingidentify potentially harmful content for human reviewenhancing their voices and reducing background noiseTopTop \nRobotics\nIn the area of robotics research, we\u2019ve made tremendous progress in our ability to learn more and more complex, safe and robust robot behaviors with less and less data, using many of the RL techniques described earlier in the post.robotics research\nTransporter Networks are a novel approach to learning how to represent robotic tasks as spatial displacements. Representing relations between objects and the robot end-effectors, as opposed to absolute positions in the environment, makes learning robust transformations of the workspace very efficient.Transporter Networks\nIn Grounding Language in Play, we demonstrated how a robot can be taught to follow natural language instructions (in many languages!). This required a scalable approach to collecting paired data of natural language instructions and robot behaviors. One key insight is that this can be accomplished by asking robot operators to simply play with the robot, and label after-the-fact what instructions would have led to the robot accomplishing the same task.Grounding Language in Playplay with the robot\nWe also explored doing away with robots altogether (by having humans use a camera-equipped grasping stick) for even more scalable data collection, and how to efficiently transfer visual representations across robotic tasks.doing away with robots altogethertransfer visual representations\nWe investigated how to learn very agile strategies for robot locomotion, by taking inspiration from nature, using evolutionary meta-learning strategies, human demonstrations, and various approaches to training data-efficient controllers using deep reinforcement learning.taking inspiration from natureevolutionary meta-learninghuman demonstrationsdata-efficient controllers\nOne increased emphasis this year has been on safety: how do we deploy safe delivery drones in the real world? How do we explore the world in a way that always allows the robot to recover from its mistakes? How do we certify the stability of learned behaviors? This is a critical area of research on which we expect to see increased focus in the future.safe delivery dronesexplore the worldcertify the stabilityTopTop \nQuantum Computing\nOur Quantum AI team continued its work to establish practical uses of quantum computing. We ran experimental algorithms on our Sycamore processors to simulate systems relevant to chemistry and physics. These simulations are approaching a scale at which they can not be performed on classical computers anymore, making good on Feynman\u2019s original idea of using quantum computers as an efficient means to simulate systems in which quantum effects are important. We published new quantum algorithms, for instance to perform precise processor calibration, to show an advantage for quantum machine learning or to test quantum enhanced optimization. We also worked on programming models to make it easier to express quantum algorithms. We released qsim, an efficient simulation tool to develop and test quantum algorithms with up to 40 qubits on Google Cloud.Quantum AISycamore processorschemistryphysicsquantum machine learningquantum enhanced optimizationmake it easier to express quantum algorithmsqsim\nWe continued to follow our roadmap towards building a universal error-corrected quantum computer. Our next milestone is the demonstration that quantum error correction can work in practice. To achieve this, we will show that a larger grid of qubits can hold logical information exponentially longer than a smaller grid, even though individual components such as qubits, couplers or I/O devices have imperfections. We are also particularly excited that we now have our own cleanroom which should significantly increase the speed and quality of our processor fabrication.roadmap towards building a universal error-corrected quantum computerTopTop \nSupporting the Broader Developer and Researcher Community\nThis year marked TensorFlow\u2019s 5th birthday, passing 160M downloads. The TensorFlow community continued its impressive growth with new special interest groups, TensorFlow User Groups, TensorFlow Certificates, AI Service partners, and inspiring demos #TFCommunitySpotlight. We significantly improved TF 2.x with seamless TPU support, out of the box performance (and best-in-class performance on MLPerf 0.7), data preprocessing, distribution strategy\u00a0and a new NumPy API.5th birthdaynew special interest groupsTensorFlow User GroupsTensorFlow CertificatesAI Service partners#TFCommunitySpotlightand best-in-class performance on MLPerf 0.7data preprocessingdistribution strategy\u00a0and a new NumPy API\nWe also added many more capabilities to the TensorFlow Ecosystem to help developers and researchers in their workflows: Sounds of India demonstrated going from research to production in under 90 days, using TFX for training and TF.js for deployment in the browser. With Mesh TensorFlow, we pushed the boundaries of model parallelism to provide ultra-high image resolution image analysis. We open-sourced the new TF runtime, TF Profiler for model performance debugging, and tools for Responsible AI, such as the Model Card Toolkit for model transparency and a privacy testing library. With TensorBoard.dev we made it possible to easily host, track, and share your ML experiments for free.Sounds of IndiaTFXTF.jsMesh TensorFlow ultra-high image resolution image analysisTF runtimeTF Profilertools for Responsible AI Model Card Toolkitprivacy testing libraryTensorBoard.dev\nIn addition, we redoubled our investment in JAX, an open-source, research-focused ML system that has been actively developed over the past two years. Researchers at Google and beyond are now using JAX in a wide range of fields, including differential privacy, neural rendering, physics-informed networks, fast attention, molecular dynamics, tensor networks, neural tangent kernels, and neural ODEs. JAX accelerates research at DeepMind, powering a growing ecosystem of libraries and work on GANs, meta-gradients, reinforcement learning, and more. We also used JAX and the Flax neural network library to build record-setting MLPerf benchmark submissions, which we demonstrated live at NeurIPS on a large TPU Pod slice with a next-generation Cloud TPU user experience (slides, video, sign-up form). Finally, we\u2019re ensuring that JAX works seamlessly with TF ecosystem tooling, from TF.data for data preprocessing and TensorBoard for experiment visualization to the TF Profiler for performance debugging, with more to come in 2021.JAXdifferential privacyneural renderingphysics-informed networksfast attentionmolecular dynamicstensor networksneural tangent kernelsneural ODEsaccelerates research at DeepMindecosystem of libraries and work on GANs, meta-gradients, reinforcement learning, and moreFlax neural network libraryrecord-setting MLPerf benchmark submissionsslidesvideosign-up formTF.dataTensorBoardTF Profiler\nMany recent research breakthroughs have been enabled by increased computing power, and we make more than 500 petaflops of Cloud TPU computing power available for free to researchers around the world via the TFRC program to help broaden access to the machine learning research frontier. More than 120 TFRC-supported papers have been published to date, many of which would not have been possible without the computing resources that the program provides. For example, TFRC researchers have recently developed simulations of wildfire spread, helped analyze COVID-19 content and vaccine sentiment changes on social media networks, and advanced our collective understanding of the lottery ticket hypothesis and neural network pruning. Members of the TFRC community have also published experiments with Persian poetry, won a Kaggle contest on fine-grained fashion image segmentation, and shared tutorials and open-source tools as starting points for others. In 2021, we will change the name of the TFRC program to the TPU Research Cloud program to be more inclusive now that Cloud TPUs support JAX and PyTorch in addition to TensorFlow.TFRC programsimulations of wildfire spreadhelped analyze COVID-19 contentvaccine sentiment changesthe lottery ticket hypothesisneural network pruningexperiments with Persian poetrywon a Kaggle contest on fine-grained fashion image segmentationtutorialsopen-source toolsJAXPyTorch\nFinally, this was a huge year for Colab. Usage doubled, and we launched productivity features to help people do their work more efficiently, including improved Drive integration and access to the Colab VM via the terminal. And we launched Colab Pro to enable users to access faster GPUs, longer runtimes and more memory.Colabimproved Drive integrationvia the terminalColab ProTopTop \nOpen Datasets and Dataset Search\nOpen datasets with clear and measurable goals are often very helpful in driving forward the field of machine learning. To help the research community find interesting datasets, we continue to index a wide variety of open datasets sourced from many different organizations with Google Dataset Search. We also think it's important to create new datasets for the community to explore and to develop new techniques, while ensuring that we share open data responsibly. This year, in addition to open datasets to help address the COVID crisis, we released a number of open datasets across many different areas:Google Dataset Searchshare open data responsiblyopen datasets to help address the COVID crisisAn Analysis of Online Datasets Using Dataset Search (Published, in Part, as a Dataset)Google Compute Cluster Trace DataAnnouncing the Objectron DatasetOpen Images V6 \u2014 Now Featuring Localized NarrativesA Challenge and Workshop in Efficient Open-Domain Question Answeringtechnical reportTyDi QA: A Multilingual Question Answering BenchmarkWiki-40B: Multilingual Language Model DatasetXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual GeneralizationHow to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed QuestionsOpen-Sourcing Big Transfer (BiT): Exploring Large-Scale Pre-training for Computer VisionThe 2020 Image Matching Benchmark and ChallengeUniversity of VictoriaCzech Technical UniversityEPFLMeta-Dataset: A Dataset of Datasets for Few-Shot LearningGoogle Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and RetrievalEnhancing the Research Community\u2019s Access to Street View Panoramas for Language Grounding TasksTopTop \nResearch Community Interaction\nWe are proud to enthusiastically support and participate in the broader research community. In 2020, Google researchers presented over 500 papers at leading research conferences, additionally serving on program committees, organizing workshops, tutorials and numerous other activities aimed at collectively progressing the state of the art in the field. To learn more about our contributions to some of the larger research conferences this year, please see our blog posts for ICLR 2020, CVPR 2020, ACL 2020, ICML 2020, ECCV 2020 and NeurIPS 2020.ICLR 2020CVPR 2020ACL 2020ICML 2020ECCV 2020NeurIPS 2020\nIn 2020 we supported external research with $37M in funding, including $8.5M in COVID research, $8M in research inclusion and equity, and $2M in responsible AI research. In February, we announced the 2019 Google Faculty Research Award Recipients, funding research proposals from 150 faculty members throughout the world. Among this group, 27% self-identified as members of historically underrepresented groups within technology. We also announced a new Research Scholar Program to support early-career professors who are pursuing research in fields relevant to Google via unrestricted gifts. As we have for more than a decade, we selected a group of incredibly talented PhD student researchers to receive Google PhD Fellowships, which provides funding for graduate studies, as well as mentorship as they pursue their research, and opportunities to interact with other Google PhD Fellows.2019 Google Faculty Research Award RecipientsResearch Scholar ProgramGoogle PhD Fellowships\nWe are also expanding the ways that we support inclusion and bring new voices into the field of computer science. In 2020, we created a new Award for Inclusion Research program that supports academic research in computing and technology addressing the needs of underrepresented populations. In the inaugural set of awards, we selected 16 proposals for funding with 25 principal investigators, focused on topics around diversity and inclusion, algorithmic bias, education innovation, health tools, accessibility, gender bias, AI for social good, security, and social justice. We additionally partnered with the Computing Alliance of Hispanic-Serving Institutions (CAHSI) and the CMD-IT Diversifying Future Leadership in the Professoriate Alliance (FLIP) to create an award program for doctoral students from traditionally underrepresented backgrounds to support the last year of the completion of the dissertation requirements.Award for Inclusion ResearchComputing Alliance of Hispanic-Serving InstitutionsCMD-IT Diversifying Future Leadership in the Professoriate Allianceaward program for doctoral students from traditionally underrepresented backgrounds\nIn 2019, Google\u2019s CS Research Mentorship Program (CSRMP) helped provide mentoring to 37 undergraduate students to introduce them to conducting computer science research. Based on the success of the program in 2019/2020, we\u2019re excited to greatly expand this program in 2020/2021 and will have hundreds of Google researchers mentoring hundreds of undergraduate students in order to encourage more people from underrepresented backgrounds to pursue computer science research careers. Finally, in October we provided exploreCSR awards to 50 institutions around the world for the 2020 academic year. These awards fund faculty to host workshops for undergraduates from underrepresented groups in order to encourage them to pursue CS research.Google\u2019s CS Research Mentorship ProgramprogramexploreCSR awardsTopTop \nLooking Forward to 2021 and Beyond\nI\u2019m excited about what\u2019s to come, from our technical work on next-generation AI models, to the very human work of growing our community of researchers.\nWe\u2019ll keep ensuring our research is done responsibly and has a positive impact, using our AI Principles as a guiding framework and applying particular scrutiny to topics that can have broad societal impact. This post covers just a few of the many papers on responsible AI that Google published in the past year. While pursuing our research, we\u2019ll focus on:AI Principlesthe many papers on responsible AIBerkeleyCMUCornellGeorgia TechHowardUW\nFinally, looking ahead to the year, I\u2019m particularly enthusiastic about the possibilities of building more general-purpose machine learning models that can handle a variety of modalities and that can automatically learn to accomplish new tasks with very few training examples. Advances in this area will empower people with dramatically more capable products, bringing better translation, speech recognition, language understanding and creative tools to billions of people all around the world. This kind of exploration and impact is what keeps us excited about our work!TopTop \nAcknowledgements\nThanks to Martin Abadi, Marc Bellemare, Elie Bursztein, Zhifeng Chen, Ed Chi, Charina Chou, Katherine Chou, Eli Collins, Greg Corrado, Corinna Cortes, Tiffany Deng, Tulsee Doshi, Robin Dua, Kemal El Moujahid, Aleksandra Faust, Orhan Firat, Jen Gennai, Till Hennig, Ben Hutchinson, Alex Ingerman, Tom\u00e1\u0161 I\u017eo, Matthew Johnson, Been Kim, Sanjiv Kumar, Yul Kwon, Steve Langdon, James Laudon, Quoc Le, Yossi Matias, Brendan McMahan, Aranyak Mehta, Vahab Mirrokni, Meg Mitchell, Hartmut Neven, Mohammad Norouzi, Timothy Novikoff, Michael Piatek, Florence Poirel, David Salesin, Nithya Sambasivan, Navin Sarma, Tom Small, Jascha Sohl-Dickstein, Zak Stone, Rahul Sukthankar, Mukund Sundararajan, Andreas Terzis, Sergei Vassilvitskii, Vincent Vanhoucke, and Leslie Yeh and others for helpful feedback and for drafting portions of this post, and to the entire Research and Health communities at Google for everyone\u2019s contributions towards this work.",
      "link": "http://ai.googleblog.com/2021/01/google-research-looking-back-at-2020.html",
      "author": "Posted by Jeff Dean, Senior Fellow and SVP of Google Research and Health, on behalf of the entire Google Research community"
    },
    {
      "title": "End-to-End, Transferable Deep RL for Graph Optimization",
      "date": "Thursday, December 17, 2020",
      "abstract": "End-to-End, Transferable Deep RL for Graph Optimization\nAn increasing number of applications are driven by large and complex neural networks trained on diverse sets of accelerators. This process is facilitated by ML compilers that map high-level computational graphs to low-level, device-specific executables. In doing so, ML compilers need to solve many optimization problems, including graph rewriting, assignment of operations on devices, operation fusion, layout and tiling of tensors, and scheduling. For example, in a device placement problem, the compiler needs to determine the mapping between operations in the computational graph to the target physical devices so that an objective function, such as training step time, can be minimized. The placement performance is determined by a mixture of intricate factors, including inter-device network bandwidth, peak device memory, co-location constraints, etc., making it challenging for heuristics or search-based algorithms, which typically settle for fast, but sub-optimal, solutions. Furthermore, heuristics are hard to develop and maintain, especially as newer model architectures emerge.graphsdevice placement problem\nRecent attempts at using learning-based approaches have demonstrated promising results, but they have a number of limitations that make them infeasible to be deployed in practice. Firstly, these approaches do not easily generalize to unseen graphs, especially those arising from newer model architectures, and second, they have poor sample efficiency, leading to high resource consumption during training. Finally, they are only able to solve a single optimization task, and consequently, do not capture the dependencies across the tightly coupled optimization problems in the compilation stack.\nIn \u201cTransferable Graph Optimizers for ML Compilers\u201d, recently published as an oral paper at NeurIPS 2020, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO) that overcomes all of the above limitations. We demonstrate 33%-60% speedup on three graph optimization tasks compared to TensorFlow default optimization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet, GO achieves an average 21% improvement over expert optimization and an 18% improvement over the prior state of the art with 15x faster convergence.\u201cTransferable Graph Optimizers for ML CompilersNeurIPS 2020TensorFlowInception-v3Transformer-XLWaveNet\nGraph Optimization Problems in ML Compilers\nThere are three coupled optimization tasks that frequently arise in ML compilers, which we formulate as decision problems that can be solved using a learned policy. The decision problems for each of the tasks can be reframed as making a decision for each node in the computational graph.\nThe first optimization task is device placement, where the goal is to determine how best to assign the nodes of the graph to the physical devices on which it runs such that the end-to-end run time is minimized.\nThe second optimization task is operation scheduling. An operation in a computational graph is ready to run when its incoming tensors are present in the device memory.  A frequently used scheduling strategy is to maintain a ready queue of operations for each device and schedule operations in first-in-first-out order. However, this scheduling strategy does not take into account the downstream operations placed on other devices that might be blocked by an operation, and often leads to schedules with underutilized devices. To find schedules that can keep track of such cross-device dependencies, our approach uses a priority-based scheduling algorithm that schedules operations in the ready queue based on the priority of each. Similar to device placement, operation scheduling can then be formulated as the problem of learning a policy that assigns a priority for each node in the graph to maximize a reward based on run time.\nThe third optimization task is operation fusion. For brevity we omit a detailed discussion of this problem here, and instead just note that similar to priority-based scheduling, operation fusion can also use a priority-based algorithm to decide which nodes to fuse. The goal of the policy network in this case is again to assign a priority for each node in the graph.\nFinally, it is important to recognize that the decisions taken in each of the three optimization problems can affect the optimal decision for the other problems. For example, placing two nodes on two different devices effectively disables fusion and introduces a communication delay that can influence scheduling.\nRL Policy Network Architecture\nOur research presents GO, a deep RL framework that can be adapted to solve each of the aforementioned optimization problems \u2014 both individually as well as jointly. There are three key aspects of the proposed architecture:\nFirst, we use graph neural networks (specifically GraphSAGE) to capture the topological information encoded in the computational graph. The inductive network of GraphSAGE leverages node attribute information to generalize to previously unseen graphs, which enables decision making for unseen data without incurring significant cost on training.GraphSAGE\nSecond, computational graphs for many models often contain more than 10k nodes. Solving the optimization problems effectively over such large scales requires that the network is able to capture long-range dependencies between nodes. GO\u2019s architecture includes a scalable attention network that uses segment-level recurrence to capture such long-range node dependencies.\nThird, ML compilers need to solve optimization problems over a wide variety of graphs from different application domains. A naive strategy of training a shared policy network with heterogeneous graphs is unlikely to capture the idiosyncrasies of a particular class of graphs.  To overcome this, GO uses a feature modulation mechanism that allows the network to specialize for specific graph types without increasing the number of parameters.\nTo jointly solve multiple dependent optimization tasks, GO has the ability to add additional recurrent attention layers for each task with parameters shared across different tasks. The recurrent attention layers with residual connections of actions enables tracking inter-task dependencies.\nResults\nNext, we present evaluation results on a single-task speedup on a device placement task based on real-hardware measurements, generalization to unseen graphs with different GO variants, and multi-task performance jointly optimizing operations fusion, device placement, and scheduling.\nSpeedup:\nTo evaluate the performance of this architecture, we apply GO to a device placement problem based on real-hardware evaluation, where we first train the model separately on each of our workloads. This approach, called GO-one, consistently outperforms expert manual placement (HP), TensorFlow METIS placement, and Hierarchical Device Placement (HDP) \u2014 the current state-of-the-art reinforcement learning-based device placement. Importantly, with the efficient end-to-end single-shot placement, GO-one has a 15x speedup in convergence time of the placement network over HDP.workloadsMETISHierarchical Device Placement\nOur empirical results show that GO-one consistently outperforms expert placement, TensorFlow METIS placement, and hierarchical device placement (HDP). Because GO is designed in a way to scale up to extremely large graphs consisting of over 80,000 nodes like an 8-layer Google Neural Machine Translation (GNMT) model, it outperforms previous approaches, including HDP, REGAL, and Placeto. GO achieves optimized graph runtimes for large graphs like GNMT that are 21.7% and 36.5% faster than HP and HDP, respectively. Overall, GO-one achieves on average 20.5% and 18.2% run time reduction across a diverse set of 14 graphs, compared to HP and HDP respectively. Importantly, with the efficient end-to-end single-shot placement, GO-one has a 15x speedup in convergence time of the placement network over HDP.hierarchical device placementGoogle Neural Machine TranslationREGALPlaceto\nGeneralization: \nGO generalizes to unseen graphs using offline pre-training followed by fine-tuning on the unseen graphs. During pre-training, we train GO on heterogeneous subsets of graphs from the training set. We train GO for 1000 steps on each such batch of graphs before switching to the next. This pretrained model is then fine-tuned (GO-generalization+finetune) on hold-out graphs for fewer than 50 steps, which typically takes less than one minute. GO-generalization+finetune for hold-out graphs outperforms both expert placement and HDP consistently on all datasets, and on average matches GO-one.\nWe also run inference directly on just the pre-trained model without any fine-tuning for the target hold-out graphs, and name this GO-generalization-zeroshot. The performance of this untuned model is only marginally worse than GO-generalization+finetune, while being slightly better than expert placement and HDP. This indicates that both graph embedding and the learned policies transfer efficiently, allowing the model to generalize to the unseen data.Inception-v3AmoebaNetrecurrent neural network language modelGoogle Neural Machine TranslationTransformer-XLWaveNet\nCo-optimizing placement, scheduling, and fusion (pl+sch+fu):\nOptimizing simultaneously for placement, scheduling and fusion provides 30%-73% speedup compared to the single-gpu unoptimized case and 33%-60% speedup compared to TensorFlow default placement, scheduling, and fusion. Comparing to optimizing each tasks individually, multi-task GO (pl+sch+fu) outperforms single-task GO (p | sch | fu) \u2014 optimizing all tasks, one at a time \u2014 by an average of 7.8%. Furthermore, for all workloads, co-optimizing all three tasks offers faster run time than optimizing any two of them and using the default policy for the third.\nConclusion\nThe increasing complexity and diversity of hardware accelerators has made the development of robust and adaptable ML frameworks onerous and time-consuming, often requiring multiple years of effort from hundreds of engineers. In this article, we demonstrated that many of the optimization problems in such frameworks can be solved efficiently and optimally using a carefully designed learned approach.\nAcknowledgements\nThis is joint work with Daniel Wong, Amirali Abdolrashidi, Peter Ma, Qiumin Xu, Hanxiao Liu, Mangpo Phitchaya Phothilimthana, Shen Wang, Anna Goldie, Azalia Mirhoseini, and James Laudon.",
      "link": "http://ai.googleblog.com/2020/12/end-to-end-transferable-deep-rl-for.html",
      "author": "Posted by Yanqi Zhou and Sudip Roy, Research Scientists,  Google Research"
    },
    {
      "title": "Privacy Considerations in Large Language Models",
      "date": "Tuesday, December 15, 2020",
      "abstract": "Privacy Considerations in Large Language Models\nMachine learning-based language models trained to predict the next word in a sentence have become increasingly capable, common, and useful, leading to groundbreaking improvements in applications like question-answering, translation, and more. But as language models continue to advance, new and unexpected risks can be exposed, requiring the research community to proactively work to develop new ways to mitigate potential problems.question-answeringtranslation\nOne such risk is the potential for models to leak details from the data on which they\u2019re trained. While this may be a concern for all large language models, additional issues may arise if a model trained on private data were to be made publicly available. Because these datasets can be large (hundreds of gigabytes) and pull from a range of sources, they can sometimes contain sensitive data, including personally identifiable information (PII) \u2014 names, phone numbers, addresses, etc., even if trained on public data. This raises the possibility that a model trained using such data could reflect some of these private details in its output. It is therefore important to identify and minimize the risks of such leaks, and to develop strategies to address the issue for future models.GPT-2\nIn \u201cExtracting Training Data from Large Language Models\u201d, a collaboration with OpenAI, Apple, Stanford, Berkeley, and Northeastern University, we demonstrate that, given only the ability to query a pre-trained language model, it is possible to extract specific pieces of training data that the model has memorized. As such, training data extraction attacks are realistic threats on state-of-the-art large language models. This research represents an early, critical step intended to inform researchers about this class of vulnerabilities, so that they may take steps to mitigate these weaknesses.Extracting Training Data from Large Language ModelsOpenAIAppleStanfordBerkeleyNortheastern University\nEthics of Language Model Attacks\nA training data extraction attack has the greatest potential for harm when applied to a model that is available to the public, but for which the dataset used in training is not. However, since conducting this research on such a dataset could have harmful consequences, we instead mount a proof of concept training data extraction attack on GPT-2, a large, publicly available language model developed by OpenAI, that was trained using only public data. While this work focuses on GPT-2 specifically, the results apply to understanding what privacy threats are possible on large language models generally.GPT-2\nAs with other privacy- and security-related research, it is important to consider the ethics of such attacks before actually performing them. To minimize the potential risk of this work, the training data extraction attack in this work was developed using publicly available data. Furthermore, the GPT-2 model itself was made public by OpenAI in 2019, and the training data used to train GPT-2 was collected from the public internet, and is available for download by anyone who follows the data collection process documented in the GPT-2 paper.GPT-2 paper\nAdditionally, in accordance with responsible computer security disclosure norms, we followed up with individuals whose PII was extracted, and secured their permission before including references to this data in publication. Further, in all publications of this work, we have redacted any personally identifying information that may identify individuals. We have also worked closely with OpenAI in the analysis of GPT-2.\nThe Training Data Extraction Attack\nBy design, language models make it very easy to generate a large amount of output data. By seeding the model with random short phrases, the model can generate millions of continuations, i.e., probable phrases that complete the sentence. Most of the time, these continuations will be benign strings of sensible text. For example, when asked to predict the continuation of the string \u201cMary had a little\u2026\u201d, a language model will have high confidence that the next token is the word \u201clamb\u201d. However, if one particular training document happened to repeat the string \u201cMary had a little wombat\u201d many times, the model might predict that phrase instead.\nThe goal of a training data extraction attack is then to sift through the millions of output sequences from the language model and predict which text is memorized. To accomplish this, our approach leverages the fact that models tend to be more confident on results captured directly from their training data. These membership inference attacks enable us to predict if a result was used in the training data by checking the confidence of the model on a particular sequence.membership inference attacks\nThe main technical contribution of this work is the development of a method for inferring membership with high accuracy along with techniques for sampling from models in a way that encourages the output of memorized content. We tested a number of different sampling strategies, the most successful of which generates text conditioned on a wide variety of input phrases. We then compare the output of two different language models. When one model has high confidence in a sequence, but the other (equally accurate) model has low confidence in a sequence, it's likely that the first model has memorized the data.\nResults\nOut of 1800 candidate sequences from the GPT-2 language model, we extracted over 600 that were memorized from the public training data, with the total number limited by the need for manual verification. The memorized examples cover a wide range of content, including news headlines, log messages, JavaScript code, PII, and more.  Many of these examples are memorized even though they appear infrequently in the training dataset.  For example, for many samples of PII we extract are found in only a single document in the dataset. However, in most of these cases, the originating document contains multiple instances of the PII, and as a result, the model still learns it as high likelihood text.\nFinally, we also find that the larger the language model, the more easily it memorizes training data. For example, in one experiment we find that the 1.5 billion parameter GPT-2 XL model memorizes 10 times more information than the 124 million parameter GPT-2 Small model. Given that the research community has already trained models 10 to 100 times larger, this means that as time goes by, more work will be required to monitor and mitigate this problem in increasingly large language models.\nLessons\nWhile we demonstrate these attacks on GPT-2 specifically, they show potential flaws in all large generative language models. The fact that these attacks are possible has important consequences for the future of machine learning research using these types of models.\nFortunately, there are several ways to mitigate this issue. The most straightforward solution is to ensure that models do not train on any potentially problematic data. But this can be difficult to do in practice.\nThe use of differential privacy, which allows training on a dataset without revealing any details of individual training examples, is one of the most principled techniques to train machine learning models with privacy. In TensorFlow, this can be achieved with the use of the tensorflow/privacy module (or similar for PyTorch or JAX) that is a drop-in replacement for existing optimizers. Even this can have limitations and won\u2019t prevent memorization of content that is repeated often enough. If this is not possible, we recommend at least measuring how much memorization occurs so appropriate action can be taken.differential privacytensorflow/privacy modulemeasuring\nLanguage models continue to demonstrate great utility and flexibility\u2014yet, like all innovations, they can also pose risks. Developing them responsibly means proactively identifying those risks and developing ways to mitigate them. We hope that this effort to highlight current weaknesses in large language modeling will raise awareness of this challenge in the broader machine learning community and motivate researchers to continue to develop effective techniques to train models with reduced memorization.\nAcknowledgements\nThis work was performed jointly with Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.",
      "link": "http://ai.googleblog.com/2020/12/privacy-considerations-in-large.html",
      "author": "Posted by Nicholas Carlini, Research Scientist, Google Research"
    },
    {
      "title": "Portrait Light: Enhancing Portrait Lighting with Machine Learning",
      "date": "Friday, December 11, 2020",
      "abstract": "Portrait Light: Enhancing Portrait Lighting with Machine Learning1\nProfessional portrait photographers are able to create compelling photographs by using specialized equipment, such as off-camera flashes and reflectors, and expert knowledge to capture just the right illumination of their subjects. In order to allow users to better emulate professional-looking portraits, we recently released Portrait Light, a new post-capture feature for the Pixel Camera and Google Photos apps that adds a simulated directional light source to portraits, with the directionality and intensity set to complement the lighting from the original photograph.Portrait Light\nIn the Pixel Camera on Pixel 4, Pixel 4a, Pixel 4a (5G), and Pixel 5, Portrait Light is automatically applied post-capture to images in the default mode and to Night Sight photos that include people \u2014 just one person or even a small group. In Portrait Mode photographs, Portrait Light provides more dramatic lighting to accompany the shallow depth-of-field effect already applied, resulting in a studio-quality look. But because lighting can be a personal choice, Pixel users who shoot in Portrait Mode can manually re-position and adjust the brightness of the applied lighting within Google Photos to match their preference. For those running Google Photos on Pixel, this relighting capability is also available for many pre-existing portrait photographs.Night SightPortrait Mode\nToday we present the technology behind Portrait Light. Inspired by the off-camera lights used by portrait photographers, Portrait Light models a repositionable light source that can be added into the scene, with the initial lighting direction and intensity automatically selected to complement the existing lighting in the photo. We accomplish this by leveraging novel machine learning models, each trained using a diverse dataset of photographs captured in the Light Stage computational illumination system. These models enabled two new algorithmic capabilities:Light Stage\nThese innovations enable Portrait Light to help create attractive lighting at any moment for every portrait \u2014 all on your mobile device.\nAutomatic Light Placement \nPhotographers usually rely on perceptual cues when deciding how to augment environmental illumination with off-camera light sources. They assess the intensity and directionality of the light falling on the face, and also adjust their subject\u2019s head pose to complement it. To inform Portrait Light\u2019s automatic light placement, we developed computational equivalents to these two perceptual signals.\nFirst, we trained a novel machine learning model to estimate a high dynamic range, omnidirectional illumination profile for a scene based on an input portrait. This new lighting estimation model infers the direction, relative intensity, and color of all light sources in the scene coming from all directions, considering the face as a light probe. We also estimate the head pose of the portrait\u2019s subject using MediaPipe Face Mesh.high dynamic rangelighting estimation modellight probeMediaPipe Face Mesh\nUsing these clues, we determine the direction from which the synthetic lighting should originate. In studio portrait photography, the main off-camera light source, or key light, is placed about 30\u00b0 above the eyeline and between 30\u00b0 and 60\u00b0 off the camera axis, when looking overhead at the scene. We follow this guideline for a classic portrait look, enhancing any pre-existing lighting directionality in the scene while targeting a balanced, subtle key-to-fill lighting ratio of about 2:1.key lightkey-to-fill\nData-Driven Portrait Relighting \nGiven a desired lighting direction and portrait, we next trained a new machine learning model to add the illumination from a directional light source to the original photograph. Training the model required millions of pairs of portraits both with and without extra light. Photographing such a dataset in normal settings would have been impossible because it requires near-perfect registration of portraits captured across different lighting conditions.\nInstead, we generated training data by photographing seventy different people using the Light Stage computational illumination system. This spherical lighting rig includes 64 cameras with different viewpoints and 331 individually-programmable LED light sources. We photographed each individual illuminated one-light-at-a-time (OLAT) by each light, which generates their reflectance field \u2014 or their appearance as illuminated by the discrete sections of the spherical environment. The reflectance field encodes the unique color and light-reflecting properties of the subject\u2019s skin, hair, and clothing \u2014 how shiny or dull each material appears. Due to the superposition principle for light, these OLAT images can then be linearly added together to render realistic images of the subject as they would appear in any image-based lighting environment, with complex light transport phenomena like subsurface scattering correctly represented.Light Stagereflectance fieldsuperposition principleimage-based lightingsubsurface scattering\nUsing the Light Stage, we photographed many individuals with different face shapes, genders, skin tones, hairstyles, and clothing/accessories. For each person, we generated synthetic portraits in many different lighting environments, both with and without the added directional light, rendering millions of pairs of images. This dataset encouraged model performance across diverse lighting environments and individuals.\nLearning Detail-Preserving Relighting Using the Quotient Image\nRather than trying to directly predict the output relit image, we trained the relighting model to output a low-resolution quotient image, i.e., a per-pixel multiplier that when upsampled can be applied to the original input image to produce the desired output image with the contribution of the extra light source added. This technique is computationally efficient and encourages only low-frequency lighting changes, without impacting high-frequency image details, which are directly transferred from the input to maintain image quality.quotient image\nSupervising Relighting with Geometry Estimation\nWhen photographers add an extra light source into a scene, its orientation relative to the subject\u2019s facial geometry determines how much brighter each part of the face appears. To model the optical behavior of light sources reflecting off relatively matte surfaces, we first trained a machine learning model to estimate surface normals given the input photograph, and then applied Lambert\u2019s law to compute a \u201clight visibility map\u201d for the desired lighting direction. We provided this light visibility map as input to the quotient image predictor, ensuring that the model is trained using physics-based insights.Lambert\u2019s law\nWe optimized the full pipeline to run at interactive frame-rates on mobile devices, with total model size under 10 MB. Here are a few examples of Portrait Light in action.\nGetting the Most Out of Portrait Light\nYou can try Portrait Light in the Pixel Camera and change the light position and brightness to your liking in Google Photos. For those who use Dual Exposure Controls, Portrait Light can be applied post-capture for additional creative flexibility to find just the right balance between light and shadow.\u00a0On existing images from your Google Photos library, try it where faces are slightly underexposed, where Portrait Light can illuminate and highlight your subject. It will especially benefit images with a single individual posed directly at the camera.Dual Exposure Controls\nWe see Portrait Light as the first step on the journey towards creative post-capture lighting controls for mobile cameras, powered by machine learning.\nAcknowledgements\nPortrait Light is the result of a collaboration between Google Research, Google Daydream, Pixel, and Google Photos teams. Key contributors include: Yun-Ta Tsai, Rohit Pandey, Sean Fanello, Chloe LeGendre, Michael Milne, Ryan Geiss, Sam Hasinoff, Dillon Sharlet, Christoph Rhemann, Peter Denny, Kaiwen Guo, Philip Davidson, Jonathan Taylor, Mingsong Dou, Pavel Pidlypenskyi, Peter Lincoln, Jay Busch, Matt Whalen, Jason Dourgarian, Geoff Harvey, Cynthia Herrera, Sergio Orts Escolano, Paul Debevec, Jonathan Barron, Sofien Bouaziz, Clement Ng, Rachit Gupta, Jesse Evans, Ryan Campbell, Sonya Mollinger, Emily To, Yichang Shih, Jana Ehmann, Wan-Chun Alex Ma, Christina Tong, Tim Smith, Tim Ruddick, Bill Strathearn, Jose Lima, Chia-Kai Liang, David Salesin, Shahram Izadi, Navin Sarma, Nisha Masharani, Zachary Senzer.\n\n1\u00a0 Work conducted while at Google.\u00a0\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html",
      "author": "Posted by Yun-Ta Tsai1 and Rohit Pandey, Software Engineers, Google Research"
    },
    {
      "title": "MediaPipe Holistic \u2014 Simultaneous Face, Hand and Pose Prediction, on Device",
      "date": "Thursday, December 10, 2020",
      "abstract": "MediaPipe Holistic \u2014 Simultaneous Face, Hand and Pose Prediction, on Device\nReal-time, simultaneous  perception of human pose, face landmarks and hand tracking on mobile devices can enable a variety of impactful applications, such as fitness and sport analysis, gesture control and sign language recognition, augmented reality effects and more. MediaPipe, an open-source framework designed specifically for complex perception pipelines leveraging accelerated inference (e.g., GPU or CPU), already offers fast and accurate, yet separate, solutions for these tasks. Combining them all in real-time into a semantically consistent end-to-end solution is a uniquely difficult problem requiring simultaneous inference of multiple, dependent neural networks.MediaPipe\nToday, we are excited to announce MediaPipe Holistic, a solution to this challenge that provides a novel state-of-the-art human pose topology that unlocks novel use cases. MediaPipe Holistic consists of a new pipeline with optimized pose, face and hand components that each run in real-time, with minimum memory transfer between their inference backends, and added support for interchangeability of the three components, depending on the quality/speed tradeoffs. When including all three components, MediaPipe Holistic provides a unified topology for a groundbreaking 540+ keypoints (33 pose, 21 per-hand and 468 facial landmarks) and achieves near real-time performance on mobile devices. MediaPipe Holistic is being released as part of MediaPipe and is available on-device for mobile (Android, iOS) and desktop. We are also introducing MediaPipe\u2019s new ready-to-use APIs for research (Python) and web (JavaScript) to ease access to the technology.MediaPipe HolisticposefacehandMediaPipe HolisticMediaPipePythonJavaScript\nPipeline and Quality\nThe MediaPipe Holistic pipeline integrates separate models for pose, face and hand components, each of which are optimized for their particular domain. However, because of their different specializations, the input to one component is not well-suited for the others. The pose estimation model, for example, takes a lower, fixed resolution video frame (256x256) as input. But if one were to crop the hand and face regions from that image to pass to their respective models, the image resolution would be too low for accurate articulation. Therefore, we designed MediaPipe Holistic as a multi-stage pipeline, which treats the different regions using a region appropriate image resolution.\nFirst, MediaPipe Holistic estimates the human pose with BlazePose\u2019s pose detector and subsequent keypoint model. Then, using the inferred pose key points, it derives three regions of interest (ROI) crops for each hand (2x) and the face, and employs a re-crop model to improve the ROI (details below). The pipeline then crops the full-resolution input frame to these ROIs and applies task-specific face and hand models to estimate their corresponding keypoints. Finally, all key points are merged with those of the pose model to yield the full 540+ keypoints.BlazePose\u2019sfacehand\nTo streamline the identification of ROIs, a tracking approach similar to the one used for the standalone face and hand pipelines is utilized. This approach assumes that the object doesn't move significantly between frames, using an estimation from the previous frame as a guide to the object region in the current one. However, during fast movements, the tracker can lose the target, which requires the detector to re-localize it in the image. MediaPipe Holistic uses pose prediction (on every frame) as an additional ROI prior to reduce the response time of the pipeline when reacting to fast movements. This also enables the model to retain semantic consistency across the body and its parts by preventing a mixup between left and right hands or body parts of one person in the frame with another.facehand\nIn addition, the resolution of the input frame to the pose model is low enough that the resulting ROIs for face and hands are still too inaccurate to guide the re-cropping of those regions, which require a precise input crop to remain lightweight. To close this accuracy gap we use lightweight face and hand re-crop models that play the role of spatial transformers and cost only ~10% of the corresponding model's inference time.spatial transformersinter-pupillary\nPerformance\nMediaPipe Holistic requires coordination between up to 8 models per frame \u2014 1 pose detector, 1 pose landmark model, 3 re-crop models and 3 keypoint models for hands and face. While building this solution, we optimized not only machine learning models, but also pre- and post-processing algorithms (e.g., affine transformations), which take significant time on most devices due to pipeline complexity. In this case, moving all the pre-processing computations to GPU resulted in ~1.5 times overall pipeline speedup depending on the device. As a result, MediaPipe Holistic runs in near real-time performance even on mid-tier devices and in the browser.affine transformationsTFLite GPU\nThe multi-stage nature of the pipeline provides two more performance benefits. As models are mostly independent, they can be replaced with lighter or heavier versions (or turned off completely) depending on the performance and accuracy requirements. Also, once pose is inferred, one knows precisely whether hands and face are within the frame bounds, allowing the pipeline to skip inference on those body parts.\nApplications\nMediaPipe Holistic, with its 540+ key points, aims to enable a holistic, simultaneous perception of body language, gesture and facial expressions. Its blended approach enables remote gesture interfaces, as well as full-body AR, sports analytics, and sign language recognition. To demonstrate the quality and performance of the MediaPipe Holistic, we built a simple remote control interface that runs locally in the browser and enables a compelling user interaction, no mouse or keyboard required. The user can manipulate objects on the screen, type on a virtual keyboard while sitting on the sofa, and point to or touch specific face regions (e.g., mute or turn off the camera). Underneath it relies on accurate hand detection with subsequent gesture recognition mapped to a \"trackpad\" space anchored to the user\u2019s shoulder, enabling remote control from up to 4 meters.remote control interface\nThis technique for gesture control can unlock various novel use-cases when other human-computer interaction modalities are not convenient. Try it out in our web demo and prototype your own ideas with it.Try it out in our web demoTry it out!\nMediaPipe for Research and Web\nTo accelerate ML research as well as its adoption in the web developer community, MediaPipe now offers ready-to-use, yet customizable ML solutions in Python and in JavaScript. We are starting with those in our previous publications: Face Mesh, Hands and Pose, including MediaPipe Holistic, with many more to come. Try them directly in the web browser: for Python using the notebooks in MediaPipe on Google Colab, and for JavaScript with your own webcam input in MediaPipe on CodePen!PythonJavaScriptFace MeshHandsPoseMediaPipe HolisticMediaPipe on Google ColabMediaPipe on CodePen\nConclusion\nWe hope the release of MediaPipe Holistic will inspire the research and development community members to build new unique applications. We anticipate that these pipelines will open up avenues for future research into challenging domains, such as sign-language recognition, touchless control interfaces, or other complex use cases. We are looking forward to seeing what you can build with it!Dr. Bill Vicars\nAcknowledgments\nSpecial thanks to all our team members who worked on the tech with us: Fan Zhang, Gregory Karpiak, Kanstantsin Sokal, Juhyun Lee, Hadon Nash, Chuo-Ling Chang, Jiuqiang Tang, Nikolay Chirkov, Camillo Lugaresi, George Sung, Michael Hays, Tyler Mullen, Chris McClanahan, Ekaterina Ignasheva, Marat Dukhan, Artsiom Ablavatski, Yury Kartynnik, Karthik Raveendran, Andrei Vakunov, Andrei Tkachenka, Suril Shah, Buck Bourdon, Ming Guang Yong, Esha Uboweja, Siarhei Kazakou, Andrei Kulik, Matsvei Zhdanovich, and Matthias Grundmann.",
      "link": "http://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html",
      "author": "Posted by Ivan Grishchenko and Valentin Bazarevsky, Research Engineers, Google Research"
    },
    {
      "title": "Google at NeurIPS 2020",
      "date": "Monday, December 7, 2020",
      "abstract": "Google at NeurIPS 2020\nThis week marks the beginning of the 34th annual Conference on Neural Information Processing Systems (NeurIPS 2020), the biggest machine learning conference of the year. Held virtually for the first time, this conference includes invited talks, demonstrations and presentations of some of the latest in machine learning research. \nAs a Platinum Sponsor of NeurIPS 2020, Google will have a strong presence with more than 180 accepted papers, additionally contributing to and learning from the broader academic research community via talks, posters, workshops and tutorials.Conference on Neural Information Processing SystemsPlatinum Sponsor\nIf you are registered for NeurIPS 2020, we hope you\u2019ll visit our virtual booth and chat with our researchers about the projects and opportunities at Google that go into solving the world's most challenging research problems, and to see demonstrations of some of the exciting research we pursue, such as Transformers for image recognition, Tone Transfer, large-scale distributed RL, recreating historical streetscapes and much more. You can also learn more about our work being presented in the list below (Google affiliations highlighted in blue).virtual boothTransformers for image recognitionTone Transferlarge-scale distributed RLrecreating historical streetscapesRankmax: An Adaptive Projection Alternative to the Softmax FunctionUnsupervised Sound Separation Using Mixture Invariant TrainingLearning to Select Best Forecast Tasks for Clinical Outcome PredictionInterpretable Sequence Learning for Covid-19 ForecastingTowards Learning Convolutions from ScratchEmergent Complexity and Zero-shot Transfer via Unsupervised Environment DesignInverse Rational Control with Partially Observable Continuous Nonlinear DynamicsOff-Policy Evaluation via the Regularized LagrangianCoinDICE: Off-Policy Confidence Interval EstimationUnsupervised Data Augmentation for Consistency TrainingVIME: Extending the Success of Self- and Semi-supervised Learning to Tabular DomainFunnel-Transformer: Filtering out Sequential Redundancy for Efficient Language ProcessingBig Bird: Transformers for Longer SequencesProvably Efficient Neural Estimation of Structural Equation Models: An Adversarial ApproachConservative Q-Learning for Offline Reinforcement LearningMOReL: Model-Based Offline Reinforcement LearningMaximum-Entropy Adversarial Data Augmentation for Improved Generalization and RobustnessGenerative View Synthesis: From Single-view Semantics to Novel-view ImagesPIE-NET: Parametric Inference of Point Cloud EdgesEnabling Certification of Verification-Agnostic Networks via Memory-Efficient Semidefinite ProgrammingAn Analysis of SVD for Deep Rotation EstimationDirect Policy Gradients: Direct Optimization of Policies in Discrete Action SpacesFaster Differentially Private Samplers via R\u00e9nyi Divergence Analysis of Discretized Langevin MCMCDISK: Learning Local Features with Policy GradientRobust Large-margin Learning in Hyperbolic SpaceGamma-Models: Generative Temporal Difference Learning for Infinite-Horizon PredictionAdversarially Robust Streaming Algorithms via Differential PrivacyFaster DBSCAN via Subsampled Similarity QueriesExact Recovery of Mangled Clusters with Same-Cluster QueriesA Maximum-Entropy Approach to Off-Policy Evaluation in Average-Reward MDPsFairness in Streaming Submodular Maximization: Algorithms and HardnessEfficient Active Learning of Sparse Halfspaces with Arbitrary Bounded NoisePrivate Learning of Halfspaces: Simplifying the Construction and Reducing the Sample ComplexitySynthetic Data Generators -- Sequential and PrivateLearning Discrete Distributions: User vs Item-level PrivacyLearning Differential Equations that are Easy to SolveAn Optimal Elimination Algorithm for Learning a Best ArmThe Convex Relaxation Barrier, Revisited: Tightened Single-Neuron Relaxations for Neural Network VerificationEscaping the Gravitational Pull of SoftmaxThe Complexity of Adversarially Robust Proper Learning of Halfspaces with Agnostic NoisePAC-Bayes Learning Bounds for Sample-Dependent PriorsFictitious Play for Mean Field Games: Continuous Time Analysis and ApplicationsWhat Do Neural Networks Learn When Trained With Random Labels?Online Planning with Lookahead PoliciesSmoothly Bounding User Contributions in Differential PrivacyDifferentially Private Clustering: Tight Approximation RatiosHitting the High Notes: Subset Selection for Maximizing Expected Order StatisticsMyersonian RegressionAssisted Learning: A Framework for Multi-Organization LearningAdversarial Robustness via Robust Low Rank RepresentationsMulti-Plane Program Induction with 3D Box PriorsPrivacy Amplification via Random Check-InsRethinking Pre-training and Self-trainingReinforcement Learning with Combinatorial Actions: An Application to Vehicle RoutingOnline Agnostic Boosting via Regret MinimizationFrom Trees to Continuous Embeddings and Back: Hyperbolic Hierarchical ClusteringFaithful Embeddings for Knowledge Base QueriesContextual Reserve Price Optimization in Auctions via Mixed Integer ProgrammingAn Operator View of Policy Gradient MethodsReinforcement Learning with Feedback GraphsOn Completeness-aware Concept-Based Explanations in Deep Neural NetworksRewriting History with Inverse RL: Hindsight Inference for Policy ImprovementThe Flajolet-Martin Sketch Itself Preserves Differential Privacy: Private Counting with Minimal SpaceWhat is Being Transferred in Transfer Learning?Latent Bandits RevisitedMetaSDF: Meta-Learning Signed Distance FunctionsMeasuring Robustness to Natural Distribution Shifts in Image ClassificationRobust Optimization for Fairness with Noisy Protected GroupsLearning Discrete Energy-based Models via Auxiliary-variable Local ExplorationBreaking the Communication-Privacy-Accuracy TrilemmaDifferentiable Meta-Learning of Bandit PoliciesMulti-Stage Influence FunctionCompositional Visual Generation with Energy Based ModelsO(n) Connections are Expressive Enough: Universal Approximability of Sparse TransformersCurriculum By SmoothingOnline Linear Optimization with Many HintsPrediction with Corrupted Expert AdviceAgnostic Learning with Multiple ObjectivesCoSE: Compositional Stroke EmbeddingsReparameterizing Mirror Descent as Gradient DescentUnderstanding Double Descent Requires A Fine-Grained Bias-Variance DecompositionDisARM: An Antithetic Gradient Estimator for Binary Latent VariablesBig Self-Supervised Models are Strong Semi-Supervised LearnersJAX MD: A Framework for Differentiable PhysicsGradient Surgery for Multi-Task LearningLoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh RegistrationICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICADemystifying Orthogonal Monte Carlo and BeyondFixMatch: Simplifying Semi-Supervised Learning with Consistency and ConfidenceCompositional Generalization via Neural-Symbolic Stack MachinesUniversally Quantized Neural CompressionSelf-Distillation Amplifies Regularization in Hilbert SpaceShapeFlow: Learnable Deformation Flows Among 3D ShapesEntropic Optimal Transport between Unbalanced Gaussian Measures has a Closed FormHigh-Fidelity Generative Image CompressionCOT-GAN: Generating Sequential Data via Causal Optimal TransportWhen Do Neural Networks Outperform Kernel Methods?Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved ConfoundingExemplar VAE: Linking Generative Models, Nearest Neighbor Retrieval, and Data AugmentationMitigating Forgetting in Online Continual Learning via Instance-Aware ParameterizationConsistent Plug-in Classifiers for Complex Objectives and ConstraintsOnline MAP Inference of Determinantal Point ProcessesOrganizing Recurrent Network Dynamics by Task-computation to Enable Continual LearningRL Unplugged: A Collection of Benchmarks for Offline Reinforcement LearningNeural Execution Engines: Learning to Execute SubroutinesSpin-Weighted Spherical CNNsAn Efficient Nonconvex Reformulation of Stagewise Convex Optimization ProblemsStochastic Optimization with Laggard Data PipelinesRegularizing Towards Permutation Invariance In Recurrent ModelsFast and Accurate kk-means++ via Rejection SamplingFairness Without Demographics Through Adversarially Reweighted LearningGradient Estimation with Stochastic Softmax TricksJust Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign DropoutA Spectral Energy Distance for Parallel Speech SynthesisOde to an ODERandAugment: Practical Automated Data Augmentation with a Reduced Search SpaceOn Adaptive Attacks to Adversarial Example DefensesFair Performance Metric ElicitationRobust Pre-Training by Adversarial Contrastive LearningWhy are Adaptive Methods Good for Attention Models?PyGlove: Symbolic Programming for Automated Machine LearningFair Hierarchical ClusteringFairness with Overlapping Groups; a Probabilistic PerspectiveDifferentiable Top-k with Optimal TransportThe Origins and Prevalence of Texture Bias in Convolutional Neural NetworksApproximate Heavily-Constrained Learning with Lagrange Multiplier ModelsEvaluating Attribution for Graph Neural NetworksSliding Window Algorithms for k-Clustering ProblemsMeta-Learning Requires Meta-AugmentationWhat Makes for Good Views for Contrastive Learning?Supervised Contrastive LearningCritic Regularized RegressionOff-Policy Imitation Learning from ObservationsEffective Diversity in Population Based Reinforcement LearningMemory Based Trajectory-conditioned Policies for Learning from Sparse RewardsObject-Centric Learning with Slot AttentionOn the Power of Louvain in the Stochastic Block ModelLearning to Execute Programs with Instruction Pointer Attention Graph Neural NetworksSMYRF - Efficient Attention using Asymmetric ClusteringGraph Contrastive Learning with AugmentationsWOR and p's: Sketches for \u2113p-Sampling Without ReplacementFourier Features Let Networks Learn High Frequency Functions in Low Dimensional DomainsModel Selection in Contextual Stochastic Bandit ProblemsAdapting to Misspecification in Contextual BanditsLeverage the Average: an Analysis of KL Regularization in Reinforcement LearningLearning with Differentiable Pertubed OptimizersMunchausen Reinforcement LearningLog-Likelihood Ratio Minimizing Flows: Towards Robust and Quantifiable Neural Distribution AlignmentYour GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent SamplingSample Complexity of Uniform Convergence for MulticalibrationImplicit Regularization and Convergence for Weight NormalizationMost ReLU Networks Suffer from \u2113\u00b2 Adversarial PerturbationsGeometric Exploration for Online ControlPLLay: Efficient Topological Layer Based on Persistent LandscapesSimple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance AwarenessBayesian Deep Ensembles via the Neural Tangent KernelHyperparameter Ensembles for Robustness and Uncertainty QuantificationConic Descent and its Application to Memory-efficient Optimization Over Positive Semidefinite MatricesOn the Training Dynamics of Deep Networks with L\u2082 RegularizationThe Surprising Simplicity of the Early-Time Learning Dynamics of Neural NetworksAdaptive Probing Policies for Shortest Path RoutingOptimal Approximation \u2014 Smoothness Tradeoffs for Soft-Max FunctionsAn Unsupervised Information-Theoretic Perceptual Quality MetricLearning Graph Structure With A Finite-State Automaton LayerEstimating Training Data Influence by Tracing Gradient Descent\nTutorials\n\nDesigning Learning Dynamics\nOrganizers: Marta Garnelo, David Balduzzi, Wojciech Czarnecki\n\nWhere Neuroscience meets AI (And What\u2019s in Store for the Future)\nOrganizers: Jane Wang, Kevin Miller, Adam Marblestone\n\nOffline Reinforcement Learning: From Algorithm Design to Practical Applications\nOrganizers: Sergey Levine, Aviral Kumar\n\nPractical Uncertainty Estimation and Out-of-Distribution Robustness in Deep Learning\nOrganizers: Dustin Tran, Balaji Lakshminarayanan, Jasper Snoek\n\nAbstraction & Reasoning in AI systems: Modern Perspectives\nOrganizers: Francois Chollet, Melanie Mitchell, Christian Szegedy\n\nPolicy Optimization in Reinforcement Learning\nOrganizers: Sham M Kakade, Martha White, Nicolas Le Roux\n\nFederated Learning and Analytics: Industry Meets Academia\nOrganizers: Brendan McMahan, Virginia Smith, Peter Kairouz\n\nDeep Implicit Layers: Neural ODEs, Equilibrium Models, and Differentiable Optimization\nOrganizers: David Duvenaud, J. Zico Kolter, Matthew Johnson\n\nBeyond Accuracy: Grounding Evaluation Metrics for Human-Machine Learning Systems\nOrganizers: Praveen Chandar, Fernando Diaz, Brian St. ThomasWorkshops\nBlack in AI Workshop @ NeurIPS 2020 (Diamond Sponsor)Mentorship Roundtables: Natasha JacquesDesigning Learning DynamicsWhere Neuroscience meets AI (And What\u2019s in Store for the Future)Offline Reinforcement Learning: From Algorithm Design to Practical ApplicationsPractical Uncertainty Estimation and Out-of-Distribution Robustness in Deep LearningAbstraction & Reasoning in AI systems: Modern PerspectivesPolicy Optimization in Reinforcement LearningFederated Learning and Analytics: Industry Meets AcademiaDeep Implicit Layers: Neural ODEs, Equilibrium Models, and Differentiable OptimizationBeyond Accuracy: Grounding Evaluation Metrics for Human-Machine Learning SystemsBlack in AI Workshop @ NeurIPS 2020LatinX in AI Workshop @ NeurIPS 2020 (Platinum Sponsor)Organizers include: Pablo Samuel CastroInvited Speaker: Fernanda Vi\u00e9gasMentorship Roundtables: Tomas IzoLatinX in AI Workshop @ NeurIPS 2020\nQueer in AI Workshop @ NeurIPS 2020\u00a0(Platinum Sponsor)\nOrganizers include: Raphael Gontijo LopesWomen in Machine Learning (Platinum Sponsor)Organizers include: Xinyi Chen, Jessica SchrouffInvited Speaker: Fernanda Vi\u00e9gasSponsor Talk: Jessica SchrouffMentorship Roundtables: Hanie Sedghi, Marc Bellemare, Katherine Heller, Rianne van den Berg, Natalie Schluter, Colin Raffel, Azalia Mirhoseini, Emily Denton, Jesse Engel, Anusha Ramesh, Matt Johnson, Jeff Dean, Laurent Dinh, Samy Bengio, Yasaman Bahri, Corinna Cortes, Nicolas le Roux, Hugo Larochelle, Sergio Guadarrama, Natasha Jaques, Pablo Samuel Castro, Elaine Le, Cory SilvearQueer in AI Workshop @ NeurIPS 2020Women in Machine Learning\nMuslims in ML\nOrganizers include: Mohammad Norouzi\n\nResistance AI Workshop\nOrganizers include: Elliot Creager, Raphael Gontijo Lopes\n\nPrivacy Preserving Machine Learning \u2014 PriML and PPML Joint Edition\nOrganizers include:\u00a0Adria Gascon, Mariana Raykova\n\nOPT2020: Optimization for Machine Learning\nOrganizers include: Courtney Paquette\n\nMachine Learning for Health (ML4H): Advancing Healthcare for All\nOrganizers include: Subhrajit Roy\n\nHuman in the Loop Dialogue SystemsOrganizers include: Rahul GoelInvited Speaker: Ankur ParikhMuslims in MLResistance AI WorkshopPrivacy Preserving Machine Learning \u2014 PriML and PPML Joint EditionOPT2020: Optimization for Machine LearningMachine Learning for Health (ML4H): Advancing Healthcare for AllHuman in the Loop Dialogue Systems\nSelf-Supervised Learning for Speech and Audio Processing\nOrganizers include:\u00a0Tara SainathInvited Speaker: Bhuvana RamabhadranSelf-Supervised Learning for Speech and Audio Processing3rd Robot Learning Workshop\nOrganizers include: Alex Bewley, Vincent VanhouckeInvited Speaker: Pete Florence\n\nWorkshop on Deep Learning and Inverse Problems\nInvited Speaker: Peyman Milanfar\n\nCrowd Science Workshop: Remoteness, Fairness, and Mechanisms as Challenges of Data Supply by Humans for Automation\nInvited Speakers: Lora Aroyo, Praveen Paritosh\n\nWorkshop on Fair AI in Finance\nInvited Speakers: Berk Ustun, Madeleine Clare Elish\n\nObject Representations for Learning and Reasoning\nPanel Moderator: Klaus Greff\nDeep Reinforcement Learning\nOrganizers include: Chelsea FinnInvited Speaker: Marc Bellemare\n\nAlgorithmic Fairness Through the Lens of Causality and Interpretability\nOrganizers include: Awa Dieng, Jessica Schrouff, Fernando Diaz\n\nMachine Learning for the Developing World (ML4D)\nSteering Committee Member: Ernest Mwebaze\n\nMachine Learning for Engineering Modeling, Simulation and Design\nOrganizers include:\u00a0Stephan Hoyer\n\nMachine Learning for Creativity and Design\nOrganizers include: Adam Roberts, Daphne Ippolito\nInvited Speaker: Jesse Engel\n\nCooperative AI\nInvited Speaker: Natasha Jaques\n\nInternational Workshop on Scalability, Privacy, and Security in Federated Learning (SpicyFL 2020)\nInvited Speaker: Brendan McMahan\n\nMachine Learning for Molecules\nOrganizers include:\u00a0Jennifer WeiInvited Speaker: Benjamin Sanchez-Lengeling\n\nNavigating the Broader Impacts of AI Research\nPanelists include: Nyalleng Moorosi, Colin Raffel, Natalie Schluter, Ben Zevenbergen\n\nBeyond BackPropagation: Novel Ideas for Training Neural Architectures\nOrganizers include: Yanping Huang\n\nDifferentiable Computer Vision, Graphics, and Physics in Machine Learning\nInvited Speaker: Andrea Tagliasacchi\n\nAI for Earth Sciences\nInvited Speaker: Milind Tambe\n\nMachine Learning for Mobile Health\nOrganizers include: Katherine Heller, Marianne Njifon\n\nShared Visual Representations in Human and Machine Intelligence (SVRHM)\nInvited Speaker: Gamaleldin Elsayed\n\nThe Challenges of Real World Reinforcement Learning\nOrganizers include:\u00a0Gabriel Dulac-ArnoldInvited Speaker: Chelsea Finn\n\nWorkshop on Computer Assisted Programming (CAP)\nOrganizers include: Charles Sutton, Augustus Odena\n\nSelf-Supervised Learning \u2014 Theory and Practice\nOrganizers include: Barret ZophInvited Speaker: Quoc V. Le\n\nOffline Reinforcement Learning\nOrganizers include: Rishabh Agarwal, George Tucker\n\nMachine Learning for Systems\nOrganizers include: Anna Goldie, Azalia Mirhoseini, Martin Maas\nInvited Speaker: Ed Chi\n\nDeep Learning Through Information Geometry\nOrganizers\u00a0include:\u00a0Alexander Alemi3rd Robot Learning WorkshopWorkshop on Deep Learning and Inverse ProblemsCrowd Science Workshop: Remoteness, Fairness, and Mechanisms as Challenges of Data Supply by Humans for AutomationWorkshop on Fair AI in FinanceObject Representations for Learning and ReasoningDeep Reinforcement LearningAlgorithmic Fairness Through the Lens of Causality and InterpretabilityMachine Learning for the Developing World (ML4D)Machine Learning for Engineering Modeling, Simulation and DesignMachine Learning for Creativity and DesignCooperative AIInternational Workshop on Scalability, Privacy, and Security in Federated Learning (SpicyFL 2020)Machine Learning for MoleculesNavigating the Broader Impacts of AI ResearchBeyond BackPropagation: Novel Ideas for Training Neural ArchitecturesDifferentiable Computer Vision, Graphics, and Physics in Machine LearningAI for Earth SciencesMachine Learning for Mobile HealthShared Visual Representations in Human and Machine Intelligence (SVRHM)The Challenges of Real World Reinforcement LearningWorkshop on Computer Assisted Programming (CAP)Self-Supervised Learning \u2014 Theory and PracticeOffline Reinforcement LearningMachine Learning for SystemsDeep Learning Through Information Geometry\nExpo\n\nDrifting Efficiently Through the Stratosphere Using Deep Reinforcement Learning\nOrganizers include: Sal Candido\n\nAccelerating Eye Movement Research via Smartphone Gaze\nOrganizers include: Vidhya Navalpakkam\n\nMining and Learning with Graphs at Scale\nOrganizers include: Bryan Perozzi, Vahab Mirrokni, Jonathan Halcrow, Jakub Lacki\n\n\n*Work performed while at GoogleDrifting Efficiently Through the Stratosphere Using Deep Reinforcement LearningAccelerating Eye Movement Research via Smartphone GazeMining and Learning with Graphs at Scale",
      "link": "http://ai.googleblog.com/2020/12/google-at-neurips-2020.html",
      "author": "Posted by Jaqui Herman and Cat Armato, Program Managers"
    },
    {
      "title": "Using AutoML for Time Series Forecasting",
      "date": "Friday, December 4, 2020",
      "abstract": "Using AutoML for Time Series Forecasting\nTime series forecasting is an important research area for machine learning (ML), particularly where accurate forecasting is critical, including several industries such as retail, supply chain, energy, finance, etc. For example, in the consumer goods domain, improving the accuracy of demand forecasting by 10-20% can reduce inventory by 5% and increase revenue by 2-3%. Current ML-based forecasting solutions are usually built by experts and require significant manual effort, including model construction, feature engineering and hyper-parameter tuning. However, such expertise may not be broadly available, which can limit the benefits of applying ML towards time series forecasting challenges.Time series forecastingimproving the accuracy of demand forecasting by 10-20% can reduce inventory by 5% and increase revenue by 2-3%feature engineeringhyper-parameter tuning\nTo address this, automated machine learning (AutoML) is an approach that makes ML more widely accessible by automating the process of creating ML models, and has recently accelerated both ML research and the application of ML to real-world problems. For example, the initial work on neural architecture search enabled breakthroughs in computer vision, such as NasNet, AmoebaNet, and EfficientNet, and in natural language processing, such as Evolved Transformer. More recently, AutoML has also been applied to tabular data.automated machine learningneural architecture searchNasNetAmoebaNetEfficientNetEvolved Transformertabular data\nToday we introduce a scalable end-to-end AutoML solution for time series forecasting, which meets three key criteria:TensorFlow\nWe demonstrate the success of this approach through participation in the M5 forecasting competition, where this AutoML solution achieved competitive performance against hand-crafted models with moderate compute cost.M5 forecasting competition\nChallenges in Time Series Forecasting\nTime series forecasting presents several challenges to machine learning models. First, the uncertainty is often high since the goal is to predict the future based on historical data. Unlike other machine learning problems, the test set, for example, future product sales, might have a different distribution from the training and validation set, which are extracted from the historical data. Second, the time series data from the real world often suffers from missing data and high intermittency (i.e., when a high fraction of the time series has the value of zero). Some time series tasks may not have historical data available and suffer from the cold start problem, for example, when predicting the sales of a new product. Third, since we aim to build a fully automated generic solution, the same solution needs to apply to a variety of datasets, which can vary significantly in the domain (product sales, web traffic, etc), the granularity (daily, hourly, etc), the history length, the types of features (categorical, numerical, date time, etc), and so on.\nAn AutoML Solution\nTo tackle these challenges, we designed an end-to-end TensorFlow pipeline with a specialized search space for time series forecasting. It is based on an encoder-decoder architecture, in which an encoder transforms the historical information in a time series into a set of vectors, and a decoder generates the future predictions based on these vectors. Inspired by the state-of-the-art sequence models, such as Transformer and WaveNet, and best practices in time series forecasting, our search space included components such as attention, dilated convolution, gating, skip connections, and different feature transformations. The resulting AutoML solution searches for the best combination of these components as well as core hyperparameters.TransformerWaveNetattentiondilated convolutiongatingskip connections\nTo combat the uncertainty in predicting the future of a time series, an ensemble of the top models discovered in the search is used to make final predictions. The diversity in the top models made the predictions more robust to uncertainty and less prone to overfitting the historical data. To handle time series with missing data, we fill in the gaps with a trainable vector and let the model learn to adapt to the missing time steps. To address intermittency, we predict, for each future time step, not only the value, but also the probability that the value at this time step is non-zero, and combine the two predictions. Finally, we found that the automated search is able to adjust the architecture and hyperparameter choices for different datasets, which makes the AutoML solution generic and automates the modeling efforts.\nBenchmarking in Forecasting Competitions \nTo benchmark our AutoML solution, we participated in the M5 forecasting competition, the latest in the M-competition series, which is one of the most important competitions in the forecasting community, with a long history spanning nearly 40 years. This most recent competition was hosted on Kaggle and used a dataset from Walmart product sales, the real-world nature of which makes the problem quite challenging.M5 forecasting competitionM-competition serieson Kaggle\nWe participated in the competition with our fully automated solution and achieved a rank of 138 out of 5558 participants (top 2.5%) on the final leaderboard, which is in the silver medal zone. Participants in the competition had almost four months to produce their models. While many of the competitive forecasting models required months of manual effort to create,  our AutoML solution found the model in a short time with only a moderate compute cost (500 CPUs for 2 hours) and no human intervention.final leaderboard\nWe also benchmarked our AutoML forecasting solution on several other Kaggle datasets and found that on average it outperforms 92% of hand-crafted models, despite its limited resource use.Rossman Store SalesWeb TrafficFavorita Grocery Sales\nThis work demonstrates the strength of an end-to-end AutoML solution for time series forecasting, and we are excited about its potential impact on real-world applications.\nAcknowledgements\nThis project was a joint effort of Google Brain team members Chen Liang, Da Huang, Yifeng Lu and Quoc V. Le. We also thank Junwei Yuan, Xingwei Yang, Dawei Jia, Chenyu Zhao, Tin-yun Ho, Meng Wang, Yaguang Li, Nicolas Loeff, Manish Kurse, Kyle Anderson and Nishant Patil for their collaboration.",
      "link": "http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html",
      "author": "Posted by Chen Liang and Yifeng Lu, Software Engineers, Google Research, Brain Team"
    },
    {
      "title": "Transformers for Image Recognition at Scale",
      "date": "Thursday, December 3, 2020",
      "abstract": "Transformers for Image Recognition at Scale\nWhile convolutional neural networks (CNNs) have been used in computer vision since the 1980s, they were not at the forefront until 2012 when AlexNet surpassed the performance of contemporary state-of-the-art image recognition methods by a large margin. Two factors helped enable this breakthrough: (i) the availability of training sets like ImageNet, and (ii) the use of commoditized GPU hardware, which provided significantly more compute for training. As such, since 2012, CNNs have become the go-to model for vision tasks.convolutional neural networkssince the 1980sAlexNetImageNet\nThe benefit of using CNNs was that they avoided the need for hand-designed visual features, instead learning to perform tasks directly from data \u201cend to end\u201d. However, while CNNs avoid hand-crafted feature-extraction, the architecture itself is designed specifically for images and can be computationally demanding. Looking forward to the next generation of scalable vision models, one might ask whether this domain-specific design is necessary, or if one could successfully leverage more domain agnostic and computationally efficient architectures to achieve state-of-the-art results.\nAs a first step in this direction, we present the Vision Transformer (ViT), a vision model based as closely as possible on the Transformer architecture originally designed for text-based tasks. ViT represents an input image as a sequence of image patches, similar to the sequence of word embeddings used when applying Transformers to text, and directly predicts class labels for the image. ViT demonstrates excellent performance when trained on sufficient data, outperforming a comparable state-of-the-art CNN with four times fewer computational resources. To foster additional research in this area, we have open-sourced both the code and models.Vision TransformerTransformercode and modelsnatural language processing\nThe Vision Transformer\nThe original text Transformer takes as input a sequence of words, which it then uses for classification, translation, or other NLP tasks. For ViT, we make the fewest possible modifications to the Transformer design to make it operate directly on images instead of words, and observe how much about image structure the model can learn on its own.classificationtranslation\nViT divides an image into a grid of square patches. Each patch is flattened into a single vector by concatenating the channels of all pixels in a patch and then linearly projecting it to the desired input dimension. Because Transformers are agnostic to the structure of the input elements we add learnable position embeddings to each patch, which allow the model to learn about the structure of the images. A priori, ViT does not know about the relative location of patches in the image, or even that the image has a 2D structure \u2014 it must learn such relevant information from the training data and encode structural information in the position embeddings.\nScaling Up\nWe first train ViT on ImageNet, where it achieves a best score of 77.9% top-1 accuracy. While this is decent for a first attempt, it falls far short of the state of the art \u2014 the current best CNN trained on ImageNet with no extra data reaches 85.8%. Despite mitigation strategies (e.g., regularization), ViT overfits the ImageNet task due to its lack of inbuilt knowledge about images.current bestregularization\nTo investigate the impact of dataset size on model performance, we train ViT on ImageNet-21k (14M images, 21k classes) and JFT (300M images, 18k classes), and compare the results to a state-of-the-art CNN, Big Transfer (BiT), trained on the same datasets. As previously observed, ViT performs significantly worse than the CNN equivalent (BiT) when trained on ImageNet (1M images). However, on ImageNet-21k (14M images) performance is comparable, and on JFT (300M images), ViT now outperforms BiT.ImageNet-21kJFTBig Transfer\nFinally, we investigate the impact of the amount of computation involved in training the models. For this, we train several different ViT models and CNNs on JFT. These models span a range of model sizes and training durations. As a result, they require varying amounts of compute for training. We observe that, for a given amount of compute, ViT yields better performance than the equivalent CNNs.\nHigh-Performing Large-Scale Image Recognition\nOur data suggest that (1) with sufficient training ViT can perform very well, and (2) ViT yields an excellent performance/compute trade-off at both smaller and larger compute scales. Therefore, to see if performance improvements carried over to even larger scales, we trained a 600M-parameter ViT model.\nThis large ViT model attains state-of-the-art  performance on multiple popular benchmarks, including 88.55% top-1 accuracy on ImageNet and 99.50% on CIFAR-10.  ViT also performs well on the cleaned-up version of the ImageNet evaluations set \u201cImageNet-Real\u201d, attaining 90.72% top-1 accuracy. Finally, ViT works well on diverse tasks, even with few training data points. For example, on the VTAB-1k suite (19 tasks with 1,000 data points each), ViT attains 77.63%, significantly ahead of the single-model state of the art (SOTA) (76.3%), and even matching SOTA attained by an ensemble of multiple models (77.6%). Most importantly, these results are obtained using fewer compute resources compared to previous SOTA CNNs, e.g., 4x fewer than the pre-trained BiT models.cleaned-up versionVTAB-1k suiteensemble of multiple modelsReaLCIFARPetsFlowers\nVisualizations\nTo gain some intuition into what the model learns, we visualize some of its internal workings. First, we look at the position embeddings \u2014 parameters that the model learns to encode the relative location of patches \u2014 and find that ViT is able to reproduce an intuitive image structure. Each position embedding is most similar to others in the same row and column, indicating that the model has recovered the grid structure of the original images. Second, we examine the average spatial distance between one element attending to another for each transformer block. At higher layers (depths of 10-20) only global features are used (i.e., large attention distances), but the lower layers (depths 0-5) capture both global and local features, as indicated by a large range in the mean attention distance. By contrast, only local features are present in the lower layers of a CNN. These experiments indicate that ViT can learn features hard-coded into CNNs (such as awareness of grid structure), but is also free to learn more generic patterns, such as a mix of local and global features at lower layers, that can aid generalization.\nSummary\nWhile CNNs have revolutionized computer vision, our results indicate that models tailor-made for imaging tasks may be unnecessary, or even sub-optimal. With ever-increasing dataset sizes, and the continued development of unsupervised and semi-supervised methods, the development of new vision architectures that train more efficiently on these datasets becomes increasingly important. We believe ViT is a preliminary step towards generic, scalable architectures that can solve many vision tasks, or even tasks from many domains, and are excited for future developments.\nA preprint of our work as well as code and models are publically available.preprintcode and models\nAcknowledgements\nWe would like to thank our co-authors in Berlin, Z\u00fcrich, and Amsterdam: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and Jakob Uszkoreit. We would like to  thank Andreas Steiner for crucial help with infrastructure and open-sourcing, Joan Puigcerver and Maxim Neumann for work on large-scale training infrastructure, and Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu\u010di\u0107, Noam Shazeer, and Colin Raffel for useful discussions. Finally, we thank Tom Small for creating the Visual Transformer animation in this post.",
      "link": "http://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html",
      "author": "Posted by Neil Houlsby and Dirk Weissenborn, Research Scientists, Google Research"
    },
    {
      "title": "Navigating Recorder Transcripts Easily, with Smart Scrolling",
      "date": "Tuesday, November 24, 2020",
      "abstract": "Navigating Recorder Transcripts Easily, with Smart Scrolling\nLast year we launched Recorder, a new kind of recording app that made audio recording smarter and more useful by leveraging on-device machine learning (ML) to transcribe the recording, highlight audio events, and suggest appropriate tags for titles. Recorder makes editing, sharing and searching through transcripts easier. Yet because Recorder can transcribe very long recordings (up to 18 hours!), it can still be difficult for users to find specific sections, necessitating a new solution to quickly navigate such long transcripts.Recorder\nTo increase the navigability of content, we introduce Smart Scrolling, a new ML-based feature in Recorder that automatically marks important sections in the transcript, chooses the most representative keywords from each section, and then surfaces those keywords on the vertical scrollbar, like chapter headings. The user can then scroll through the keywords or tap on them to quickly navigate to the sections of interest. The models used are lightweight enough to be executed on-device without the need to upload the transcript, thus preserving user privacy.\nUnder the Hood\nThe Smart Scrolling feature is composed of two distinct tasks. The first extracts representative keywords from each section and the second picks which sections in the text are the most informative and unique.\nFor each task, we utilize two different natural language processing (NLP) approaches: a distilled bidirectional transformer (BERT) model pre-trained on data sourced from a Wikipedia dataset, alongside a modified extractive term frequency\u2013inverse document frequency (TF-IDF) model. By using the bidirectional transformer and the TF-IDF-based models in parallel for both the keyword extraction and important section identification tasks, alongside aggregation heuristics, we were able to harness the advantages of each approach and mitigate their respective drawbacks (more on this in the next section).bidirectional transformerWikipedia datasetterm frequency\u2013inverse document frequency\nThe bidirectional transformer is a neural network architecture that employs a self-attention mechanism to achieve context-aware processing of the input text in a non-sequential fashion. This enables parallel processing of the input text to identify contextual clues both before and after a given position in the transcript.self-attention\nThe extractive TF-IDF approach rates terms based on their frequency in the text compared to their inverse frequency in the trained dataset, and enables the finding of unique representative terms in the text.TF-IDF\nBoth models were trained on publicly available conversational datasets that were labeled and evaluated by independent raters. The conversational datasets were from the same domains as the expected product use cases, focusing on meetings, lectures, and interviews, thus ensuring the same word frequency distribution (Zipf\u2019s law).Zipf\u2019s law\nExtracting Representative Keywords\nThe TF-IDF-based model detects informative keywords by giving each word a score, which corresponds to how representative this keyword is within the text. The model does so, much like a standard TF-IDF model, by utilizing the ratio of the number of occurrences of a given word in the text compared to the whole of the conversational data set, but it also takes into account the specificity of the term, i.e., how broad or specific it is. Furthermore, the model then aggregates these features into a score using a pre-trained function curve. In parallel, the bidirectional transformer model, which was fine tuned on the task of extracting keywords, provides a deep semantic understanding of the text, enabling it to extract precise context-aware keywords.\nThe TF-IDF approach is conservative in the sense that it is prone to finding uncommon keywords in the text (high bias), while the drawback for the bidirectional transformer model is the high variance of the possible keywords that can be extracted. But when used together, these two models complement each other, forming a balanced bias-variance tradeoff.\nOnce the keyword scores are retrieved from both models, we normalize and combine them by utilizing NLP heuristics (e.g., the weighted average), removing duplicates across sections, and eliminating stop words and verbs. The output of this process is an ordered list of suggested keywords for each of the sections.\nRating a Section\u2019s Importance\nThe next task is to determine which sections should be highlighted as informative and unique. To solve this task, we again combine the two models mentioned above, which yield two distinct importance scores for each of the sections. We compute the first score by taking the TF-IDF scores of all the keywords in the section and weighting them by their respective number of appearances in the section, followed by a summation of these individual keyword scores. We compute the second score by running the section text through the bidirectional transformer model, which was also trained on the sections rating task. The scores from both models are normalized and then combined to yield the section score.\nSome Challenges\nA significant challenge in the development of Smart Scrolling was how to identify whether a section or keyword is important - what is of great importance to one person can be of less importance to another. The key was to highlight sections only when it is possible to extract helpful keywords from them.\nTo do this, we configured the solution to select the top scored sections that also have highly rated keywords, with the number of sections highlighted proportional to the length of the recording.  In the context of the Smart Scrolling features, a keyword was more highly rated if it better represented the unique information of the section.\nTo train the model to understand this criteria, we needed to prepare a labeled training dataset tailored to this task. In collaboration with a team of skilled raters, we applied this labeling objective to a small batch of examples to establish an initial dataset in order to evaluate the quality of the labels and instruct the raters in cases where there were deviations from what was intended. Once the labeling process was complete we reviewed the labeled data manually and made corrections to the labels as necessary to align them with our definition of importance.\nUsing this limited labeled dataset, we ran automated model evaluations to establish initial metrics on model quality, which were used as a less-accurate proxy to the model quality, enabling us to quickly assess the model performance and apply changes in the architecture and heuristics. Once the solution metrics were satisfactory, we utilized a more accurate manual evaluation process over a closed set of carefully chosen examples that represented expected Recorder use cases. Using these examples, we tweaked the model heuristics parameters to reach the desired level of performance using a reliable model quality evaluation.\nRuntime Improvements\nAfter the initial release of Recorder, we conducted a series of user studies to learn how to improve the usability and performance of the Smart Scrolling feature. We found that many users expect the navigational keywords and highlighted sections to be available as soon as the recording is finished. Because the computation pipeline described above can take a considerable amount of time to compute on long recordings, we devised a partial processing solution that amortizes this computation over the whole duration of the recording. During recording, each section is processed as soon as it is captured, and then the intermediate results are stored in memory. When the recording is done, Recorder aggregates the intermediate results.\nWhen running on a Pixel 5, this approach reduced the average processing time of an hour long recording (~9K words) from 1 minute 40 seconds to only 9 seconds, while outputting the same results.\nSummary\nThe goal of Recorder is to improve users\u2019 ability to access their recorded content and navigate it with ease. We have already made substantial progress in this direction with the existing ML features that automatically suggest title words for recordings and enable users to search recordings for sounds and text.  Smart Scrolling provides additional text navigation abilities that will further improve the utility of Recorder, enabling users to rapidly surface sections of interest, even for long recordings.\nAcknowledgments\nBin Zhang, Sherry Lin, Isaac Blankensmith, Henry Liu\u200e, Vincent Peng\u200e, Guilherme Santos\u200e, Tiago Camolesi, Yitong Lin, James Lemieux, Thomas Hall\u200e, Kelly Tsai\u200e, Benny Schlesinger, Dror Ayalon, Amit Pitaru, Kelsie Van Deman, Console Chen, Allen Su, Cecile Basnage, Chorong Johnston\u200e, Shenaz Zack, Mike Tsao, Brian Chen, Abhinav Rastogi, Tracy Wu, Yvonne Yang\u200e.",
      "link": "http://ai.googleblog.com/2020/11/navigating-recorder-transcripts-easily.html",
      "author": "Posted by Itay Inbar, Senior Software Engineer, Google Research"
    },
    {
      "title": "The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models",
      "date": "Friday, November 20, 2020",
      "abstract": "The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models\nAs natural language processing (NLP) models become more powerful and are deployed in more real-world contexts, understanding their behavior is becoming increasingly critical. While advances in modeling have brought unprecedented performance on many NLP tasks, many research questions remain about not only the behavior of these models under domain shift and adversarial settings, but also their tendencies to behave according to social biases or shallow heuristics.\nFor any new model, one might want to know in which cases a model performs poorly, why a model makes a particular prediction, or whether a model will behave consistently under varying inputs, such as changes to textual style or pronoun gender. But, despite the recent explosion of work on model understanding and evaluation, there is no \u201csilver bullet\u201d for analysis. Practitioners must often experiment with many techniques, looking at local explanations, aggregate metrics, and counterfactual variations of the input to build a better understanding of model behavior, with each of these techniques often requiring its own software package or bespoke tool. Our previously released What-If Tool was built to address this challenge by enabling black-box probing of classification and regression models, thus enabling researchers to more easily debug performance and analyze the fairness of machine learning models through interaction and visualization. But there was still a need for a toolkit that would address challenges specific to NLP models.pronoun genderWhat-If Tool\nWith these challenges in mind, we built and open-sourced the Language Interpretability Tool (LIT), an interactive platform for NLP model understanding. LIT builds upon the lessons learned from the What-If Tool with greatly expanded capabilities, which cover a wide range of NLP tasks including sequence generation, span labeling, classification and regression, along with customizable and extensible visualizations and model analysis.open-sourcedLanguage Interpretability ToolWhat-If Tool\nLIT supports local explanations, including salience maps, attention, and rich visualizations of model predictions, as well as aggregate analysis including metrics, embedding spaces, and flexible slicing. It allows users to easily hop between visualizations to test local hypotheses and validate them over a dataset. LIT provides support for counterfactual generation, in which new data points can be added on the fly, and their effect on the model visualized immediately. Side-by-side comparison allows for two models, or two individual data points, to be visualized simultaneously. More details about LIT can be found in our system demonstration paper, which was presented at EMNLP 2020.salience mapsattentionpaperEMNLP 2020\nCustomizability\nIn order to better address the broad range of users with different interests and priorities that we hope will use LIT, we\u2019ve built the tool to be easily customizable and extensible from the start. Using LIT on a particular NLP model and dataset only requires writing a small bit of Python code. Custom components, such as task-specific metrics calculations or counterfactual generators, can be written in Python and added to a LIT instance through our provided APIs. Also, the front end itself can be customized, with new modules that integrate directly into the UI.\u00a0For more on extending the tool, check out our documentation on GitHub.GitHub\nDemos\nTo illustrate some of the capabilities of LIT, we have created a few demos using pre-trained models. The full list is available on the LIT website, and we describe two of them here:websiteSentiment analysisBERTStanford Sentiment TreebankLIMEintegrated gradientsback-translationMasked word prediction\nLIT in Practice and Future WorkAlthough LIT is a new tool, we have already seen the value that it can provide for model understanding. Its visualizations can be used to find patterns in model behavior, such as outlying clusters in embedding space, or words with outsized importance to the predictions. Exploration in LIT can test for potential biases in models, as demonstrated in our case study of LIT exploring gender bias in a coreference model. This type of analysis can inform next steps in improving model performance, such as applying MinDiff to mitigate systemic bias. It can also be used as an easy and fast way to create an interactive demo for any NLP model.case study of LIT exploring gender bias in a coreference modelapplying MinDiff to mitigate systemic bias\nCheck out the tool either through our provided demos, or by bringing up a LIT server for your own models and datasets. The work on LIT has just started, and there are a number of new capabilities and refinements planned, including the addition of new interpretability techniques from cutting edge ML and NLP research. If there are other techniques that you\u2019d like to see added to the tool, please let us know! Join our mailing list to stay up to date as LIT evolves. And as the code is open-source, we welcome feedback on and contributions to the tool.mailing listfeedback\nAcknowledgments\nLIT is a collaborative effort between the Google Research PAIR and Language teams. This post represents the work of the many contributors across Google, including Andy Coenen, Ann Yuan, Carey Radebaugh, Ellen Jiang, Emily Reif, Jasmijn Bastings, Kristen Olson, Leslie Lai, Mahima Pushkarna, Sebastian Gehrmann, and Tolga Bolukbasi. We would like to thank all those who contributed to the project, both inside and outside Google, and the teams that have piloted its use and provided valuable feedback.PAIRLanguage",
      "link": "http://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html",
      "author": "Posted by James Wexler, Software Developer and Ian Tenney, Software Engineer, Google Research"
    },
    {
      "title": "Haptics with Input: Using Linear Resonant Actuators for Sensing",
      "date": "Wednesday, November 18, 2020",
      "abstract": "Haptics with Input: Using Linear Resonant Actuators for Sensing\nAs wearables and handheld devices decrease in size, haptics become an increasingly vital channel for feedback, be it through silent alerts or a subtle \"click\" sensation when pressing buttons on a touch screen. Haptic feedback, ubiquitous in nearly all wearable devices and mobile phones, is commonly enabled by a linear resonant actuator (LRA), a small linear motor that leverages resonance to provide a strong haptic signal in a small package. However, the touch and pressure sensing needed to activate the haptic feedback tend to depend on additional, separate hardware which increases the price, size and complexity of the system.hapticslinear resonant actuator\nIn \u201cHaptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental Awareness\u201d, presented at ACM UIST 2020, we demonstrate that widely available LRAs can sense a wide range of external information, such as touch, tap and pressure, in addition to being able to relay information about contact with the skin, objects and surfaces. We achieve this with off-the-shelf LRAs by multiplexing the actuation with short pulses of custom waveforms that are designed to enable sensing using the back-EMF voltage.  We demonstrate the potential of this approach to enable expressive discrete buttons and vibrotactile interfaces and show how the approach could bring rich sensing opportunities to integrated haptics modules in mobile devices, increasing sensing capabilities with fewer components. Our technique is potentially compatible with many existing LRA drivers, as they already employ back-EMF sensing for autotuning of the vibration frequency.Haptics with Input: Back-EMF in Linear Resonant Actuators to Enable Touch, Pressure and Environmental AwarenessACM UIST 2020back-EMF voltage\nBack-EMF Principle in an LRA\nInside the LRA enclosure is a magnet attached to a small mass, both moving freely on a spring. The magnet moves in response to the excitation voltage introduced by the voice coil. The motion of the oscillating mass produces a counter-electromotive force, or back-EMF, which is a voltage proportional to the rate of change of magnetic flux. A greater oscillation speed creates a larger back-EMF voltage, while a stationary mass generates zero back-EMF voltage.back-EMF\nActive Back-EMF for Sensing\nTouching or making contact with the LRA during vibration changes the velocity of the interior mass, as energy is dissipated into the contact object. This works well with soft materials that deform under pressure, such as the human body. A finger, for example, absorbs different amounts of energy depending on the contact force as it flattens against the LRA. By driving the LRA with small amounts of energy, we can measure this phenomenon using the back-EMF voltage. Because leveraging the back-EMF behavior for sensing is an active process, the key insight that enabled this work was the design of a custom, off-resonance driver waveform that allows continuous sensing while minimizing vibrations, sound and power consumption.\nWe measure back-EMF from the floating voltage between the two LRA leads, which requires disconnecting the motor driver briefly to avoid disturbances. While the driver is disconnected, the mass is still oscillating inside the LRA, producing an oscillating back-EMF voltage. Because commercial back-EMF sensing LRA drivers do not provide the raw data, we designed a custom circuit that is able to pick up and amplify small back-EMF voltage. We also generated custom drive pulses that minimize vibrations and energy consumption.\nApplications\nThe behavior of the LRAs used in mobile phones is the same, whether they are on a table, on a soft surface, or hand held. This may cause problems, as a vibrating phone could slide off a glass table or emit loud and unnecessary vibrating sounds. Ideally, the LRA on a phone would automatically adjust based on its environment. We demonstrate our approach for sensing using the LRA back-EMF technique by wiring directly to a Pixel 4\u2019s LRA, and then classifying whether the phone is held in hand, placed on a soft surface (foam), or placed on a table.\nWe also present a prototype that demonstrates how LRAs could be used as combined input/output devices in portable electronics. We attached two LRAs, one on the left and one on the right side of a phone. The buttons provide tap, touch, and pressure sensing. They are also programmed to provide haptic feedback, once the touch is detected.\nThere are a number of wearable tactile aid devices, such as sleeves, vests, and bracelets. To transmit tactile feedback to the skin with consistent force, the tactor has to apply the right pressure; it can not be too loose or too tight. Currently, the typical way to do so is through manual adjustment, which can be inconsistent and lacks measurable feedback. We show how the LRA back-EMF technique can be used to continuously monitor the fit bracelet device and prompt the user if it's too tight, too loose, or just right.tactor\nEvaluating an LRA as a Sensor\nThe LRA works well as a pressure sensor, because it has a quadratic response to the force magnitude during touch. Our method works for all five off-the-shelf LRA types that we evaluated. Because the typical power consumption is only 4.27 mA, all-day sensing would only reduce the battery life of a Pixel 4 phone from 25 to 24 hours. The power consumption can be greatly reduced by using low-power amplifiers and employing active sensing only when needed, such as when the phone is active and interacting with the user.\nThe challenge with active sensing is to minimize vibrations, so they are not perceptible when touching and do not produce audible sound. We optimize the active sensing to produce only 2 dB of sound and 0.45 m/s2 peak-to-peak acceleration, which is just barely perceptible by finger and is quiet, in contrast to the regular 8.49 m/s2 .\nFuture Work and Conclusion\nTo see the work presented here in action, please see the video below.In the future, we plan to explore other sensing techniques, perhaps measuring the current could be an alternative approach. Also, using machine learning could potentially improve the sensing and provide more accurate classification of the complex back-EMF patterns. Our method could be developed further to enable closed-loop feedback with the actuator and sensor, which would allow the actuator to provide the same force, regardless of external conditions.\nWe believe that this work opens up new opportunities for leveraging existing ubiquitous hardware to provide rich interactions and closed-loop feedback haptic actuators.\nAcknowledgments\nThis work was done by Artem Dementyev, Alex Olwal, and Richard Lyon. Thanks to Mathieu Le Goc and Thad Starner for feedback on the paper.",
      "link": "http://ai.googleblog.com/2020/11/haptics-with-input-using-linear.html",
      "author": "Posted by Artem Dementyev, Hardware Engineer, Google Research"
    },
    {
      "title": "Using GANs to Create Fantastical Creatures",
      "date": "Tuesday, November 17, 2020",
      "abstract": "Using GANs to Create Fantastical Creatures\nCreating art for digital video games takes a high degree of artistic creativity and technical knowledge, while also requiring game artists to quickly iterate on ideas and produce a high volume of assets, often in the face of tight deadlines. What if artists had a paintbrush that acted less like a tool and more like an assistant? A machine learning model acting as such a paintbrush could reduce the amount of time necessary to create high-quality art without sacrificing artistic choices, perhaps even enhancing creativity.\nToday, we present Chimera Painter, a trained machine learning (ML) model that automatically creates a fully fleshed out rendering from a user-supplied creature outline. Employed as a demo application, Chimera Painter adds features and textures to a creature outline segmented with body part labels, such as \u201cwings\u201d or \u201cclaws\u201d, when the user clicks the \u201ctransform\u201d button. Below is an example using the demo with one of the preset creature outlines.Chimera Painter\nIn this post, we describe some of the challenges in creating the ML model behind Chimera Painter and demonstrate how one might use the tool for the creation of video game-ready assets.\nPrototyping for a New Type of Model\nIn developing an ML model to produce video-game ready creature images, we created a digital card game prototype around the concept of combining creatures into new hybrids that can then battle each other. In this game, a player would begin with cards of real-world animals (e.g., an axolotl or a whale) and could make them more powerful by combining them (making the dreaded Axolotl-Whale chimera). This provided a creative environment for demonstrating an image-generating model, as the number of possible chimeras necessitated a method for quickly designing large volumes of artistic assets that could be combined naturally, while still retaining identifiable visual characteristics of the original creatures.axolotlchimera\nSince our goal was to create high-quality creature card images guided by artist input, we experimented with generative adversarial networks (GANs), informed by artist feedback, to create creature images that would be appropriate for our fantasy card game prototype. GANs pair two convolutional neural networks against each other: a generator network to create new images and a discriminator network to determine if these images are samples from the training dataset (in this case, artist-created images) or not. We used a variant called a conditional GAN, where the generator takes a separate input to guide the image generation process. Interestingly, our approach was a strict departure from other GAN efforts, which typically focus on photorealism.generative adversarial networksphotorealismTo train the GANs, we created a dataset of full color images with single-species creature outlines adapted from 3D creature models. The creature outlines characterized the shape and size of each creature, and provided a segmentation map that identified individual body parts. After model training, the model was tasked with generating multi-species chimeras, based on outlines provided by artists. The best performing model was then incorporated into Chimera Painter. Below we show some sample assets generated using the model, including single-species creatures, as well as the more complex multi-species chimeras.Stadia Research presentation\nLearning to Generate Creatures with Structure\nAn issue with using GANs for generating creatures was the potential for loss of anatomical and spatial coherence when rendering subtle or low-contrast parts of images, despite these being of high perceptual importance to humans. Examples of this can include eyes, fingers, or even distinguishing between overlapping body parts with similar textures (see the affectionately named BoggleDog below).high perceptual importance\nGenerating chimeras required a new non-photographic fantasy-styled dataset with unique characteristics, such as dramatic perspective, composition, and lighting. Existing repositories of illustrations were not appropriate to use as datasets for training an ML model, because they may be subject to licensing restrictions, have conflicting styles, or simply lack the variety needed for this task.\nTo solve this, we developed a new artist-led, semi-automated approach for creating an ML training dataset from 3D creature models, which allowed us to work at scale and rapidly iterate as needed. In this process, artists would create or obtain a set of 3D creature models, one for each creature type needed (such as hyenas or lions). Artists then produced two sets of textures that were overlaid on the 3D model using the Unreal Engine \u2014 one with the full color texture (left image, below) and the other with flat colors for each body part (e.g., head, ears, neck, etc), called a \u201csegmentation map\u201d (right image, below). This second set of body part segments was given to the model at training to ensure that the GAN learned about body part-specific structure, shapes, textures, and proportions for a variety of creatures.Unreal Engine\nThe 3D creature models were all placed in a simple 3D scene, again using the Unreal Engine. A set of automated scripts would then take this 3D scene and interpolate between different poses, viewpoints, and zoom levels for each of the 3D creature models, creating the full color images and segmentation maps that formed the training dataset for the GAN. Using this approach, we generated 10,000+ image + segmentation map pairs per 3D creature model, saving the artists millions of hours of time compared to creating such data manually (at approximately 20 minutes per image).\nFine Tuning \nThe GAN had many different hyper-parameters that could be adjusted, leading to different qualities in the output images. In order to better understand which versions of the model were better than others, artists were provided samples for different creature types generated by these models and asked to cull them down to a few best examples. We gathered feedback about desired characteristics present in these examples, such as a feeling of depth, style with regard to creature textures, and realism of faces and eyes. This information was used both to train new versions of the model and, after the model had generated hundreds of thousands of creature images, to select the best image from each creature category (e.g., gazelle, lynx, gorilla, etc).\nWe tuned the GAN for this task by focusing on the perceptual loss. This loss function component (also used in Stadia\u2019s Style Transfer ML) computes a difference between two images using extracted features from a separate convolutional neural network (CNN) that was previously trained on millions of photographs from the ImageNet dataset. The features are extracted from different layers of the CNN and a weight is applied to each, which affects their contribution to the final loss value. We discovered that these weights were critically important in determining what a final generated image would look like. Below are some examples from the GAN trained with different perceptual loss weights.Stadia\u2019s Style Transfer MLImageNet\nSome of the variation in the images above is due to the fact that the dataset includes multiple textures for each creature (for example, a reddish or grayish version of the bat). However, ignoring the coloration, many differences are directly tied to changes in perceptual loss values. In particular, we found that certain values brought out sharper facial features (e.g., bottom right vs. top right) or \u201csmooth\u201d versus \u201cpatterned\u201d (top right vs. bottom left) that made generated creatures feel more real.\nHere are some creatures generated from the GAN trained with different perceptual loss weights, showing off a small sample of the outputs and poses that the model can handle.\nChimera Painter\nThe trained GAN is now available in the Chimera Painter demo, allowing artists to work iteratively with the model, rather than drawing dozens of similar creatures from scratch. An artist can select a starting point and then adjust the shape, type, or placement of creature parts, enabling rapid exploration and for the creation of a large volume of images. The demo also allows for uploading a creature outline created in an external program, like Photoshop. Simply download one of the preset creature outlines to get the colors needed for each creature part and use this as a template for drawing one outside of Chimera Painter, and then use the \u201cLoad\u2019 button on the demo to use this outline to flesh out your creation.Chimera Painter demo\nIt is our hope that these GAN models and the Chimera Painter demonstration tool might inspire others to think differently about their art pipeline. What can one create when using machine learning as a paintbrush?\nAcknowledgments\nThis project is conducted in collaboration with many people. Thanks to Ryan Poplin, Lee Dotson, Trung Le, Monica Dinculescu, Marc Destefano, Aaron Cammarata, Maggie Oh, Richard Wu, Ji Hun Kim, Erin Hoffman-John, and Colin Boswell. Thanks to everyone who pitched in to give hours of art direction, technical feedback, and drawings of fantastic creatures.",
      "link": "http://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html",
      "author": "Posted by Andeep Singh Toor, Stadia Software Engineer and Fred Bertsch, Software Engineer, Google Research, Brain Team"
    },
    {
      "title": "Mitigating Unfair Bias in ML Models with the MinDiff Framework",
      "date": "Monday, November 16, 2020",
      "abstract": "Mitigating Unfair Bias in ML Models with the MinDiff Framework\nThe responsible research and development of machine learning (ML) can play a pivotal role in helping to solve a wide variety of societal challenges. At Google, our research reflects our AI Principles, from helping to protect patients from medication errors and improving flood forecasting models, to presenting methods that tackle unfair bias in products, such as Google Translate, and providing resources for other researchers to do the same.AI Principleshelping to protect patients from medication errorsflood forecasting modelsGoogle Translateproviding resources\nOne broad category for applying ML  responsibly is the task of classification \u2014 systems that sort data into labeled categories.  At Google, such models are used throughout our products to enforce policies, ranging from the detection of hate speech to age-appropriate content filtering.  While these classifiers serve vital functions, it is also essential that they are built in ways that minimize unfair biases for users.classification\nToday, we are announcing the release of MinDiff, a new regularization technique available in the TF Model Remediation library for effectively and efficiently mitigating unfair biases when training ML models.  In this post, we discuss the research behind this technique and explain how it addresses the practical constraints and requirements we\u2019ve observed when incorporating it in Google\u2019s products.MinDiffTF Model Remediation library\nUnfair Biases in Classifiers\nTo illustrate how MinDiff can be used, consider an example of a product policy classifier that is tasked with identifying and removing text comments that could be considered toxic. One challenge is to make sure that the classifier is not unfairly biased against submissions from a particular group of users, which could result in incorrect removal of content from these groups.examplethe classifier is not unfairly\nThe academic community has laid a solid theoretical foundation for ML fairness, offering a breadth of perspectives on what unfair bias means and on the tensions between different frameworks for evaluating fairness. One of the most common metrics is equality of opportunity, which, in our example, means measuring and seeking to minimize the difference in false positive rate (FPR) across groups. In the example above, this means that the classifier should not be more likely to incorrectly remove safe comments from one group than another. Similarly, the classifier\u2019s false negative rate should be equal between groups. That is, the classifier should not miss toxic comments against one group more than it does for another.unfair bias meansevaluating fairnessequality of opportunityfalse positive rate\nWhen the end goal is to improve products, it\u2019s important to be able to scale unfair bias mitigation to many models. However, this poses a number of challenges:equality of opportunityadversarial learning\nMinDiff Framework\nWe iteratively developed the MinDiff framework over the previous few years to meet these design requirements. Because demographic information is so rarely known, we utilize in-process approaches in which the model\u2019s training objective is augmented with an objective specifically focused on removing biases. This new objective is then optimized over the small sample of data with known demographic information. To improve ease of use, we switched from adversarial training to a regularization framework, which penalizes statistical dependency between its predictions and demographic information for non-harmful examples. This encourages the model to equalize error rates across groups, e.g., classifying non-harmful examples as toxic.regularization framework\nThere are several ways to encode this dependency between predictions and demographic information. Our initial MinDiff implementation minimized the correlation between the predictions and the demographic group, which essentially optimized for the average and variance of predictions to be equal across groups, even if the distributions still differ afterward.  We have since improved MinDiff further by considering the maximum mean discrepancy (MMD) loss, which is closer to optimizing for the distribution of predictions to be independent of demographics. We have found that this approach is better able to both remove biases and maintain model accuracy.improvedmaximum mean discrepancy\nTo date we have launched modeling improvements across several classifiers at Google that moderate content quality.  We went through multiple iterations to develop a robust, responsible, and scalable approach, solving research challenges and enabling broad adoption.\nGaps in error rates of classifiers is an important set of unfair biases to address, but not the only one that arises in ML applications.  For ML researchers and practitioners, we hope this work can further advance research toward addressing even broader classes of unfair biases and the development of approaches that can be used in practical applications. In addition, we hope that the release of the MinDiff library and the associated demos and documentation, along with the tools and experience shared here, can help practitioners improve their models and products.demosdocumentation\nAcknowledgements\nThis research effort on ML Fairness in classification was jointly led with Jilin Chen, Shuo Chen, Ed H. Chi, Tulsee Doshi, and Hai Qian.  Further, this work was pursued in collaboration with Jonathan Bischof, Qiuwen Chen,\u00a0Cristos Goodrow,\u00a0Pierre Kreitmann, and Christine Luu.  The MinDiff infrastructure was also developed in collaboration with Nick Blumm, James Chen, Thomas Greenspan, Christina Greer, Lichan Hong, Manasi Joshi, Maciej Kula, Summer Misherghi, Dan Nanas, Sean O'Keefe, Mahesh Sathiamoorthy, Catherina Xu, and Zhe Zhao.\u00a0Further, this post was written with significant feedback and guidance from Reena Jana.\u00a0(All names are listed in alphabetical order of last names.)",
      "link": "http://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html",
      "author": "Posted by Flavien Prost, Senior Software Engineer and Alex Beutel, Staff Research Scientist, Google Research"
    },
    {
      "title": "The Machine Learning Behind Hum to Search",
      "date": "Thursday, November 12, 2020",
      "abstract": "The Machine Learning Behind Hum to Search\nMelodies stuck in your head, often referred to as \u201cearworms,\u201d are a well-known and sometimes irritating phenomenon \u2014 once that earworm is there, it can be tough to get rid of it. Research has found that engaging with the original song, whether that\u2019s listening to or singing it, will drive the earworm away. But what if you can\u2019t quite recall the name of the song, and can only hum the melody?earwormshas foundengaging with the original song\nExisting methods to match a hummed melody to its original polyphonic studio recording face several challenges. With lyrics, background vocals and instruments, the audio of a musical or studio recording can be quite different from a hummed tune. By mistake or design, when someone hums their interpretation of a song, often the pitch, key, tempo or rhythm may vary slightly or even significantly.  That\u2019s why so many existing approaches to query by humming match the hummed tune against a database of pre-existing melody-only or hummed versions of a song, instead of identifying the song directly. However, this type of approach often relies on a limited database that requires manual updates.existing approachesquery by humming\nLaunched in October, Hum to Search is a new fully machine-learned system within Google Search that allows a person to find a song using only a hummed rendition of it. In contrast to existing methods, this approach produces an embedding of a melody from a spectrogram of a song without generating an intermediate representation. This enables the model to match a hummed melody directly to the original (polyphonic) recordings without the need for a hummed or MIDI version of each track or for other complex hand-engineered logic to extract the melody. This approach greatly simplifies the database for Hum to Search, allowing it to constantly be refreshed with embeddings of original recordings from across the world \u2014 even the latest releases.Hum to Search\nBackground\nMany existing music recognition systems convert an audio sample into a spectrogram before processing it, in order to find a good match. However, one challenge in recognizing a hummed melody is that a hummed tune often contains relatively little information, as illustrated by this hummed example of Bella Ciao. The difference between the hummed version and the same segment from the corresponding studio recording can be visualized using spectrograms, seen below:this hummed exampleBella Ciaospectrogramshummed clip\nGiven the image on the left, a model needs to locate the audio corresponding to the right-hand image from a collection of over 50M similar-looking images (corresponding to segments of studio recordings of other songs). To achieve this, the model has to learn to focus on the dominant melody, and ignore background vocals, instruments, and voice timbre, as well as differences stemming from background noise or room reverberations. To find by eye the dominant melody that might be used to match these two spectrograms, a person might look for similarities in the lines near the bottom of the above images.\nPrior efforts to enable discovery of music, in particular in the context of recognizing recorded music being played in an environment such as a cafe or a club, demonstrated how machine learning might be applied to this problem. Now Playing, released to Pixel phones in 2017, uses an on-device deep neural network to recognize songs without the need for a server connection, and Sound Search further developed this technology to provide a server-based recognition service for faster and more accurate searching of over 100 million songs. The next challenge then was to leverage what was learned from these releases to recognize hummed or sung music from a similarly large library of songs.Now PlayingSound Search\nMachine Learning Setup\nThe first step in developing Hum to Search was to modify the music-recognition models used in Now Playing and Sound Search to work with hummed recordings. In principle, many such retrieval systems (e.g., image recognition) work in a similar way. A neural network is trained with pairs of input (here pairs of hummed or sung audio with recorded audio) to produce embeddings for each input, which will later be used for matching to a hummed melody.\nTo enable humming recognition, the network should produce embeddings for which pairs of audio containing the same melody are close to each other, even if they have different instrumental accompaniment and singing voices. Pairs of audio containing different melodies should be far apart. In training, the network is provided such pairs of audio until it learns to produce embeddings with this property.\nThe trained model can then generate an embedding for a tune that is similar to the embedding of the song\u2019s reference recording.  Finding the correct song is then only a matter of searching for similar embeddings from a database of reference recordings computed from audio of popular music.\nTraining Data\nBecause training of the model required song pairs (recorded and sung), the first challenge was to obtain enough training data. Our initial dataset consisted of mostly sung music segments (very few of these contained humming). To make the model more robust, we augmented the audio during training, for example by varying the pitch or tempo of the sung input randomly. The resulting model worked well enough for people singing, but not for people humming or whistling.\nTo improve the model\u2019s performance on hummed melodies we generated additional training data of simulated \u201chummed\u201d melodies from the existing audio dataset using SPICE, a pitch extraction model developed by our wider team as part of the FreddieMeter project. SPICE extracts the pitch values from given audio, which we then use to generate a melody consisting of discrete audio tones. The very first version of this system transformed this original clip into these tones.SPICEFreddieMeterthis original clipthese tones\nWe later refined this approach by replacing the simple tone generator with a neural network that generates audio resembling an actual hummed or whistled tune. For example, the network generates this humming example or whistling example from the above sung clip.humming examplewhistling examplesung clip\nAs a final step, we compared training data by mixing and matching the audio samples. For example, if we had a similar clip from two different singers, we\u2019d align those two clips with our preliminary models, and are therefore able to show the model an additional pair of audio clips that represent the same melody.\nMachine Learning Improvements\nWhen training the Hum to Search model, we started with a triplet loss function. This loss has been shown to perform well across a variety of classification tasks like images and recorded music. Given a pair of audio corresponding to the same melody (points R and P in embedding space, shown below), triplet loss would ignore certain parts of the training data derived from a different melody. This helps the machine improve learning behavior, either when it finds a different melody that is too \u2018easy\u2019 in that it is already far away from R and P (see point E) or because it is too hard in that, given the model's current state of learning, the audio ends up being too close to R \u2014 even though according to our data it represents a different melody (see point H).triplet losshas been shownrecorded music\nWe\u2019ve found that we could improve the accuracy of the model by taking these additional training data (points H and E) into account, namely by formulating a general notion of model confidence across a batch of examples: How sure is the model that all the data it has seen can be classified correctly, or has it seen examples that do not fit its current understanding? Based on this notion of confidence, we added a loss that drives model confidence towards 100% across all areas of the embedding space, which led to improvements in our model\u2019s precision and recall.precision and recall\nThe above changes, but in particular our variations, augmentations and superpositions of the training data, enabled the neural network model deployed in Google Search to recognize sung or hummed melodies. The current system reaches a high level of accuracy on a song database that contains over half a million  songs that we are continually updating. This song corpus still has room to grow to include more of the world\u2019s many melodies.\nTo try the feature, you can open the latest version of the Google app, tap the mic icon and say \u201cwhat's this song?\u201d or click the \u201cSearch a song\u201d button, after which you can hum, sing, or whistle away! We hope that Hum to Search can help with that earworm of yours, or maybe just help you in case you want to find and playback a song without having to type its name.\nAcknowledgements\nThe work described here was authored by Alex Tudor, Duc Dung Nguyen, Matej Kastelic\u200e, Mihajlo Velimirovi\u0107\u200e, Stefan Christoph, Mauricio Zuluaga, Christian Frank, Dominik Roblek, and Matt Sharifi. We would like to deeply thank Krishna Kumar, Satyajeet Salgar and Blaise Aguera y Arcas for their ongoing support, as well as all the Google teams we've collaborated with to build the full Hum to Search product.\nWe would also like to thank all our colleagues at Google who donated clips of themselves singing or humming and therefore laid a foundation for this work, as well as Nick Moukhine\u200e for building the Google-internal singing donation app. Finally, special thanks to Meghan Danks and Krishna Kumar for their feedback on earlier versions of this post.",
      "link": "http://ai.googleblog.com/2020/11/the-machine-learning-behind-hum-to.html",
      "author": "Posted by Christian Frank, Google Research, Z\u00fcrich"
    },
    {
      "title": "Improving On-Device Speech Recognition with VoiceFilter-Lite",
      "date": "Wednesday, November 11, 2020",
      "abstract": "Improving On-Device Speech Recognition with VoiceFilter-Lite\nVoice assistive technologies, which enable users to employ voice commands to interact with their devices, rely on accurate speech recognition to ensure responsiveness to a specific user. But in many real-world use cases, the input to such technologies often consists of overlapping speech, which poses great challenges to many speech recognition algorithms. In 2018, we published a VoiceFilter system, which leverages Google\u2019s Voice Match to personalize interaction with assistive technology by allowing people to enroll their voices.speech recognitionVoiceFilter systemVoice Matchenroll their voices\nWhile the VoiceFilter approach is highly successful, achieving a better source to distortion ratio (SDR) than conventional approaches, efficient on-device streaming speech recognition requires addressing restrictions such as model size, CPU and memory limitations, as well as battery usage considerations and latency minimization.source to distortion ratio\nIn \u201cVoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech Recognition\u201d, we present an update to VoiceFilter for on-device use that can significantly improve speech recognition in overlapping speech by leveraging the enrolled voice of a selected speaker. Importantly, this model can be easily integrated with existing on-device speech recognition applications, allowing the user to access voice assistive features under extremely noisy conditions even if an internet connection is unavailable. Our experiments show that a 2.2MB VoiceFilter-Lite model provides a 25.1% improvement to the word error rate (WER) on overlapping speech.VoiceFilter-Lite: Streaming Targeted Voice Separation for On-Device Speech RecognitionVoiceFilterenrolled voiceword error rate\nImproving On-Device Speech Recognition\nWhile the original VoiceFilter system was very successful at separating a target speaker's speech signal from other overlapping sources, its model size, computational cost and latency are not feasible for speech recognition on mobile devices.speech recognition on mobile devices\nThe new VoiceFilter-Lite system has been carefully designed to fit on-device applications. Instead of processing audio waveforms, VoiceFilter-Lite takes exactly the same input features as the speech recognition model (stacked log Mel-filterbanks), and directly enhances these features by filtering out components not belonging to the target speaker in real time. Together with several optimizations on network topologies, the number of runtime operations is drastically reduced. After quantizing the neural network with the TensorFlow Lite library, the model size is only 2.2 MB, which fits most on-device applications.VoiceFilter-LiteMel-filterbanksquantizing the neural networkTensorFlow Lite\nTo train the VoiceFilter-Lite model, the filterbanks of the noisy speech are fed as input to the network together with an embedding vector that represents the identity of the target speaker (i.e., a d-vector). The network predicts a mask that is element-wise multiplied to the input to produce enhanced filterbanks. A loss function is defined to minimize the difference between the enhanced filterbanks and the filterbanks from the clean speech during training.d-vector\nVoiceFilter-Lite is a plug-and-play model, which allows the application in which it\u2019s implemented to easily bypass it if the speaker did not enroll their voice. This also means that the speech recognition model and the VoiceFilter-Lite model can be separately trained and updated, which largely reduces engineering complexity in the deployment process.\nAddressing the Challenge of Over-Suppression\nWhen speech separation models are used for improving speech recognition, two types of error could occur: under-suppression, when the model fails to filter out noisy components from the signal; and over-suppression, when the model fails to preserve useful signal, resulting in some words being dropped from the recognized text. Over-suppression is especially problematic since modern speech recognition models are usually already trained with extensively augmented data (such as room simulation and SpecAugment), and thus are more robust to under-suppression.room simulationSpecAugment\nVoiceFilter-Lite addresses the over-suppression issue with two novel approaches. First, it uses an asymmetric loss during the training process, such that the model is less tolerant to over-suppression than under-suppression. Second, it predicts the type of noise at runtime, and adaptively adjusts the suppression strength according to this prediction.\nWith these two solutions, the VoiceFilter-Lite model retains great performance on streaming speech recognition for other scenarios, such as single-speaker speech under quiet or various noise conditions, while still providing significant improvement on overlapping speech. From our experiments, we observed a 25.1% improvement of word error rate after the 2.2MB VoiceFilter-Lite model is applied on additive overlapping speech. For reverberant overlapping speech, which is a more challenging task to simulate far-field devices such as smart home speakers, we also observed a 14.7% improvement of word error rate with VoiceFilter-Lite.\nFuture Work\nWhile VoiceFilter-Lite has shown great promise for various on-device speech applications, we are also exploring several other directions to make VoiceFilter-Lite more useful. First, our current model is trained and evaluated with English speech only. We are excited about adopting the same technology to improve speech recognition for more languages. Second, we would like to directly optimize the speech recognition loss during the training of VoiceFilter-Lite, which can potentially further improve speech recognition beyond overlapping speech.\nAcknowledgements\nThe research described in this post represents joint efforts from multiple teams within Google. Contributors include Quan Wang, Ignacio Lopez Moreno, Mert Saglam, Kevin Wilson, Alan Chiao, Renjie Liu, Yanzhang He, Wei Li, Jason Pelecanos, Philip Chao, Sinan Akay, John Han, Stephen Wu, Hannah Muckenhirn, Ye Jia, Zelin Wu, Yiteng Huang, Marily Nika, Jaclyn Konzelmann, Nino Tasca, and Alexander Gruenstein.",
      "link": "http://ai.googleblog.com/2020/11/improving-on-device-speech-recognition.html",
      "author": "Posted by Quan Wang, Software Engineer, Google Research"
    },
    {
      "title": "Announcing the Objectron Dataset",
      "date": "Monday, November 9, 2020",
      "abstract": "Announcing the Objectron Dataset\nThe state of the art in machine learning (ML) has achieved exceptional accuracy on many computer vision tasks solely by training models on photos. Building upon these successes and advancing 3D object understanding has great potential to power a wider range of applications, such as augmented reality, robotics, autonomy, and image retrieval. For example, earlier this year we released MediaPipe Objectron, a set of real-time 3D object detection models designed for mobile devices, which were trained on a fully annotated, real-world 3D dataset, that can predict objects\u2019 3D bounding boxes.MediaPipe Objectron\nYet, understanding objects in 3D remains a challenging task due to the lack of large real-world datasets compared to 2D tasks (e.g., ImageNet, COCO, and Open Images).  To empower the research community for continued advancement in 3D object understanding, there is a strong need for the release of object-centric video datasets, which capture more of the 3D structure of an object, while matching the data format used for many vision tasks (i.e., video or camera streams), to aid in the training and benchmarking of machine learning models.ImageNetCOCOOpen Images\nToday, we are excited to release the Objectron dataset, a collection of short, object-centric video clips capturing a larger set of common objects from different angles. Each video clip is accompanied by AR session metadata that includes camera poses and sparse point-clouds. The data also contain manually annotated 3D bounding boxes for each object, which describe the object\u2019s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images collected from a geo-diverse sample (covering 10 countries across five continents).Objectron dataset\nA 3D Object Detection Solution\nAlong with the dataset, we are also sharing a 3D object detection solution for four categories of objects \u2014 shoes, chairs, mugs, and cameras. These models are released in MediaPipe, Google's open source framework for cross-platform customizable ML solutions for live and streaming media, which also powers ML solutions like on-device real-time hand, iris and body pose tracking.3D object detection solutionMediaPipeML solutionshandirisbody pose tracking\nIn contrast to the previously released single-stage Objectron model, these newest versions utilize a two-stage architecture. The first stage employs the TensorFlow Object Detection model to find the 2D crop of the object. The second stage then uses the image crop to estimate the 3D bounding box while simultaneously computing the 2D crop of the object for the next frame, so that the object detector does not need to run every frame. The second stage 3D bounding box predictor runs at 83 FPS on Adreno 650 mobile GPU.single-stage Objectron modelTensorFlow Object Detection\nEvaluation Metric for 3D Object Detection\nWith ground truth annotations, we evaluate the performance of 3D object detection models using 3D intersection over union (IoU) similarity statistics, a commonly used metric for computer vision tasks, which measures how close the bounding boxes are to the ground truth.intersection over union\nWe propose an algorithm for computing accurate 3D IoU values for general 3D-oriented boxes. First, we compute the intersection points between faces of the two boxes using Sutherland-Hodgman Polygon clipping algorithm. This is similar to frustum culling, a technique used in computer graphics. The volume of the intersection is computed by the convex hull of all the clipped polygons. Finally, the IoU is computed from the volume of the intersection and volume of the union of two boxes. We are releasing the evaluation metrics source code along with the dataset.Sutherland-Hodgman Polygon clipping algorithmfrustum cullingconvex hullevaluation metrics source code\nDataset Format\nThe technical details of the Objectron dataset, including usage and tutorials, are available on the dataset website. The dataset includes bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes, and is stored in the objectron bucket on Google Cloud storage with the following assets:dataset websiteobjectron bucketGoogle Cloud storageTensorflowPyTorchJax\nWith the dataset, we are also open-sourcing a data-pipeline to parse the dataset in popular Tensorflow, PyTorch and Jax frameworks. Example colab notebooks are also provided.colab notebooks\nBy releasing this Objectron dataset, we hope to enable the research community to push the limits of 3D object geometry understanding. We also hope to foster new research and applications, such as view synthesis, improved 3D representation, and unsupervised learning. Stay tuned for future activities and developments by joining our mailing list and visiting our github page.view synthesisimproved 3D representationjoining our mailing listour github page\nAcknowledgements\nThe research described in this post was done by Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Mogan Shieh, Ryan Hickman, Buck Bourdon, Alexander Kanaukou, Chuo-Ling Chang, Matthias Grundmann, \u200eand Tom Funkhouser. We thank Aliaksandr Shyrokau, Sviatlana Mialik, Anna Eliseeva, and the annotation team for their high quality annotations. We also would like to thank Jonathan Huang and Vivek Rathod for their guidance on TensorFlow Object Detection API.",
      "link": "http://ai.googleblog.com/2020/11/announcing-objectron-dataset.html",
      "author": "Posted by Adel Ahmadyan and Liangkai Zhang, Software Engineers, Google Research"
    },
    {
      "title": "Background Features in Google Meet, Powered by Web ML",
      "date": "Friday, October 30, 2020",
      "abstract": "Background Features in Google Meet, Powered by Web ML\nVideo conferencing is becoming ever more critical in people's work and personal lives. Improving that experience with privacy enhancements or fun visual touches can help center our focus on the meeting itself. As part of this goal, we recently announced ways to blur and replace your background in Google Meet, which use machine learning (ML) to better highlight participants regardless of their surroundings. Whereas other solutions require installing additional software, Meet\u2019s features are powered by cutting-edge web ML technologies built with MediaPipe that work directly in your browser \u2014 no extra steps necessary. One key goal in developing these features was to provide real-time, in-browser performance on almost all modern devices, which we accomplished by combining efficient on-device ML models, WebGL-based rendering, and web-based ML inference via XNNPACK and TFLite.blurreplaceGoogle MeetMediaPipeWebGLXNNPACKTFLite\nOverview of Our Web ML Solution\nThe new features in Meet are developed with MediaPipe, Google's open source framework for cross-platform customizable ML solutions for live and streaming media, which also powers ML solutions like on-device real-time hand, iris and body pose tracking.MediaPipehandirisbody pose tracking\nA core need for any on-device solution is to achieve high performance. To accomplish this, MediaPipe\u2019s web pipeline leverages WebAssembly, a low-level binary code format designed specifically for web browsers that improves speed for compute-heavy tasks. At runtime, the browser converts WebAssembly instructions into native machine code that executes much faster than traditional JavaScript code. In addition, Chrome 84 recently introduced support for WebAssembly SIMD, which processes multiple data points with each instruction, resulting in a performance boost of more than 2x.web pipelineWebAssemblyJavaScriptChrome 84SIMD\nOur solution first processes each video frame by segmenting a user from their background (more about our segmentation model later in the post) utilizing ML inference to compute a low resolution mask. Optionally, we further refine the mask to align it with the image boundaries. The mask is then used to render the video output via WebGL2, with the background blurred or replaced.WebGL2\nIn the current version, model inference is executed on the client\u2019s CPU for low power consumption and widest device coverage. To achieve real-time performance, we designed efficient ML models with inference accelerated by the XNNPACK library, the first inference engine specifically designed for the novel WebAssembly SIMD specification. Accelerated by XNNPACK and SIMD, the segmentation model can run in real-time on the web.XNNPACKXNNPACKSIMD\nEnabled by MediaPipe's flexible configuration, the background blur/replace solution adapts its processing based on device capability. On high-end devices it runs the full pipeline to deliver the highest visual quality, whereas on low-end devices it continues to perform at speed by switching to  compute-light ML models and bypassing the mask refinement.\nSegmentation Model\nOn-device ML models need to be ultra lightweight for fast inference, low power consumption, and small download size. For models running in the browser, the input resolution greatly affects the number of floating-point operations (FLOPs) necessary to process each frame, and therefore needs to be small as well. We downsample the image to a smaller size before feeding it to the model. Recovering a segmentation mask as fine as possible from a low-resolution image adds to the challenges of model design.\nThe overall segmentation network has a symmetric structure with respect to encoding and decoding, while the decoder blocks (light green) also share a symmetric layer structure with the encoder blocks (light blue). Specifically, channel-wise attention with global average pooling is applied in both encoder and decoder blocks, which is friendly to efficient CPU inference.channel-wise attentionMobileNetV3\nWe modified MobileNetV3-small as the encoder, which has been tuned by network architecture search for the best performance with low resource requirements. To reduce the model size by 50%, we exported our model to TFLite using float16 quantization, resulting in a slight loss in weight precision but with no noticeable effect on quality. The resulting model has 193K parameters and is only 400KB in size.MobileNetV3-smallnetwork architecture search\nRendering Effects\nOnce segmentation is complete, we use OpenGL shaders for video processing and effect rendering, where the challenge is to render efficiently without introducing artifacts. In the refinement stage, we apply a joint bilateral filter to smooth the low resolution mask.bilateral filterbilateral filterLight wrapping\nThe blur shader simulates a bokeh effect by adjusting the blur strength at each pixel proportionally to the segmentation mask values, similar to the circle-of-confusion (CoC) in optics. Pixels are weighted by their CoC radii, so that foreground pixels will not bleed into the background. We implemented separable filters for the weighted blur, instead of the popular Gaussian pyramid, as it removes halo artifacts surrounding the person. The blur is performed at a low resolution for efficiency, and blended with the input frame at the original resolution.bokehcircle-of-confusionseparable filtersGaussian pyramid\nFor background replacement, we adopt a compositing technique, known as light wrapping, for blending segmented persons and customized background images. Light wrapping helps soften segmentation edges by allowing background light to spill over onto foreground elements, making the compositing more immersive. It also helps minimize halo artifacts when there is a large contrast between the foreground and the replaced background.light wrapping\nPerformance\nTo optimize the experience for different devices, we provide model variants at multiple input sizes (i.e., 256x144 and 160x96 in the current release), automatically selecting the best according to available hardware resources.\nWe evaluated the speed of model inference and the end-to-end pipeline on two common devices: MacBook Pro 2018 with 2.2 GHz 6-Core Intel Core i7, and Acer Chromebook 11 with Intel Celeron N3060. For 720p input, the MacBook Pro can run the higher-quality model at 120 FPS and the end-to-end pipeline at 70 FPS, while the Chromebook runs inference at 62 FPS with the lower-quality model and 33 FPS end-to-end.\nFor quantitative evaluation of model accuracy, we adopt the popular metrics of intersection-over-union (IOU) and boundary F-measure. Both models achieve high quality, especially for having such a lightweight network:intersection-over-unionboundary F-measure\nWe also release the accompanying Model Card for our segmentation models, which details our fairness evaluations. Our evaluation data contains images from 17 geographical subregions of the globe, with annotations for skin tone and gender. Our analysis shows that the model is consistent in its performance across the various regions, skin-tones, and genders, with only small deviations in IOU metrics.Model Card\nConclusion \nWe introduced a new in-browser ML solution for blurring and replacing your background in Google Meet. With this, ML models and OpenGL shaders can run efficiently on the web. The developed features achieve real-time performance with low power consumption, even on low-power devices.\nAcknowledgments\nSpecial thanks to those on the Meet team and others who worked on this project, in particular Sebastian Jansson, Rikard Lundmark, Stephan Reiter, Fabian Bergmark, Ben Wagner, Stefan Holmer, Dan Gunnarson, St\u00e9phane Hulaud and to all our team members who worked on the technology with us: Siargey Pisarchyk, Karthik Raveendran, Chris McClanahan, Marat Dukhan, Frank Barchard, Ming Guang Yong, Chuo-Ling Chang, Michael Hays, Camillo Lugaresi, Gregory Karpiak, Siarhei Kazakou, Matsvei Zhdanovich, and Matthias Grundmann.",
      "link": "http://ai.googleblog.com/2020/10/background-features-in-google-meet.html",
      "author": "Posted by Tingbo Hou and Tyler Mullen, Software Engineers, Google Research"
    },
    {
      "title": "Experimenting with Automatic Video Creation from a Web Page",
      "date": "Thursday, October 29, 2020",
      "abstract": "Experimenting with Automatic Video Creation from a Web Page\nAt Google, we're actively exploring how people can use creativity tools powered by machine learning and computational methods when producing multimedia content, from creating music and reframing videos, to drawing and more. One creative process in particular, video production, can especially benefit from such tools, as it requires a series of decisions about what content is best suited to a target audience, how to position the available assets within the field of view, and what temporal arrangement will yield the most compelling narrative. But what if one could leverage existing assets, such as a website, to get a jump-start on video creation? Businesses commonly host websites that contain rich visual representations about their services or products, all of which could be repurposed for other multimedia formats, such as videos, potentially enabling those without extensive resources the ability to reach a broader audience.creating musicreframing videosdrawing\nIn \u201cAutomatic Video Creation From a Web Page\u201d, published at UIST 2020, we introduce URL2Video, a research prototype pipeline to automatically convert a web page into a short video, given temporal and visual constraints provided by the content owner. URL2Video extracts assets (text, images, or videos) and their design styles (including fonts, colors, graphical layouts, and hierarchy) from HTML sources and organizes the visual assets into a sequence of shots, while maintaining a look-and-feel similar to the source page. Given a user-specified aspect ratio and duration, it then renders the repurposed materials into a video that is ideal for product and service advertising.Automatic Video Creation From a Web PageUIST 2020\nURL2Video Overview\nAssume a user provides an URL to a web page that illustrates their business. The URL2Video pipeline automatically selects key content from the page and decides the temporal and visual presentation of each asset, based on a set of heuristics derived from an interview study with designers who were familiar with web design and video ad creation. These designer-informed heuristics capture common video editing styles, including content hierarchy, constraining the amount of information in a shot and its time duration, providing consistent color and style for branding, and more. Using this information, the URL2Video pipeline parses a web page, analyzing the content and selecting visually salient text or images while preserving their design styles, which it organizes according to the video specifications provided by the user.\nWebpage Analysis\nGiven a webpage URL, URL2Video extracts document object model (DOM) information and multimedia materials. For the purposes of our research prototype, we limited the domain to static web pages that contain salient assets and headings preserved in an HTML hierarchy that follows recent web design principles, which encourage the use of prominent elements, distinct sections, and an order of visual focus that guides readers in perceiving information. URL2Video identifies such visually-distinguishable elements as a candidate list of asset groups, each of which may contain a heading, a product image, detailed descriptions, and call-to-action buttons, and  captures both the raw assets (text and multimedia files) and detailed design specifications (HTML tags, CSS styles, and rendered locations) for each element. It then ranks the asset groups by assigning each a priority score based on their visual appearance and annotations, including their HTML tags, rendered sizes, and ordering shown on the page. In this way, an asset group that occupies a larger area at the top of the page receives a higher score.document object modelweb design principles\nConstraints-Based Asset Selection\nWe consider two goals when composing a video: (1) each video shot should provide concise information, and (2) the visual design should be consistent with the source page. Based on these goals and the video constraints provided by the user, including the intended video duration (in seconds) and aspect ratio (commonly 16:9, 4:3, 1:1, etc.), URL2Video automatically selects and orders the asset groups to optimize the total priority score. To make the content concise, it presents only dominant elements from a page, such as a headline and a few multimedia assets. It constrains the duration of each visual element for viewers to perceive the content. In this way, a short video highlights the most salient information from the top of the page, and a longer video contains more campaigns or products.\nScene Composition & Video Rendering\nGiven an ordered list of assets based on the DOM hierarchy, URL2Video follows the design heuristics obtained from interview studies to make decisions about both the temporal and spatial arrangement to present the assets in individual shots. It transfers the graphical layout of elements into the video\u2019s aspect ratio, and applies the style choices including fonts and colors. To make a video more dynamic and engaging, it adjusts the presentation timing of assets. Finally, it renders the content into a video in the MPEG-4 container format.\nUser Control\nThe interface to the research prototype allows the user to review the design attributes in each video shot extracted from the source page, reorder the materials, change the detailed design, such as colors and fonts, and adjust the constraints to generate a new video.\nURL2Video Use Cases\nWe demonstrate the performance of the end-to-end URL2Video pipeline on a variety of existing web pages. Below we highlight an example result where URL2Video converts a page that embeds multiple short video clips into a 12-second output video. Note how the pipeline makes automatic editing decisions on font and color choices, timing, and content ordering in a video captured from the source page.Google Search\nThe video below provides further demonstration:\nTo evaluate the automatically-generated videos, we conducted a user study with designers at Google. Our results show that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process.\nNext steps\nWhile this current research focuses on the visual presentation, we are developing new techniques that support the audio track and a voiceover in video editing. All in all, we envision a future where creators focus on making high-level decisions and an ML model interactively suggests detailed temporal and graphical edits for a final video creation on multiple platforms.\nAcknowledgments\nWe greatly thank our paper co-authors, Zheng Sun (Research) and Katrina Panovich (YouTube). We would also like to thank our colleagues who contributed to URL2Video, (in alphabetical order of last name) Jordan Canedy, Brian Curless, Nathan Frey, Madison Le, Alireza Mahdian, Justin Parra, Emily Ryan, Mogan Shieh, Sandor Szego, and Weilong Yang. We are grateful to receive the support from our leadership, Tomas Izo, Rahul Sukthankar, and Jay Yagnik.",
      "link": "http://ai.googleblog.com/2020/10/experimenting-with-automatic-video.html",
      "author": "Posted by Peggy Chi, Senior Research Scientist and Irfan Essa, Senior Staff Research Scientist, Google Research"
    },
    {
      "title": "Estimating the Impact of Training Data with Reinforcement Learning",
      "date": "Wednesday, October 28, 2020",
      "abstract": "Estimating the Impact of Training Data with Reinforcement LearningRecent work suggests that not all data samples are equally useful for training, particularly for deep neural networks (DNNs). Indeed, if a dataset contains low-quality or incorrectly labeled data, one can often improve performance by removing a significant portion of training samples.  Moreover, in cases where there is a mismatch between the train and test datasets (e.g., due to difference in train and test location or time), one can also achieve higher performance by carefully restricting samples in the training set to those most relevant for the test scenario.  Because of the ubiquity of these scenarios, accurately quantifying the values of training samples has great potential for improving model performance on real-world datasets.Recent workdeep neural networksimprove performancerestricting samples\n  In addition to improving model performance, assigning a quality value to individual data can also enable new use cases. It can be used to suggest better practices for data collection, e.g., what kinds of additional data would benefit the most, and can be used to construct large-scale training datasets more efficiently, e.g., by web searching using the labels as keywords and filtering out less valuable data.\nIn \u201cData Valuation Using Deep Reinforcement Learning\u201d, accepted at ICML 2020, we address the challenge of quantifying the value of training data using a novel approach based on meta-learning. Our method integrates data valuation into the training procedure of a predictor model that learns to recognize samples that are more valuable for the given task, improving both predictor and data valuation performance. We have also launched four AI Hub Notebooks that exemplify the use cases of DVRL and are designed to be conveniently adapted to other tasks and datasets, such as\u00a0domain adaptation,\u00a0corrupted sample discovery and robust learning,\u00a0transfer learning on image data\u00a0and\u00a0data valuation.Data Valuation Using Deep Reinforcement LearningICML 2020AI Hubdomain adaptationcorrupted sample discovery and robust learningtransfer learning on image datadata valuation\nQuantifying the Value of Data\nNot all data are equal for a given ML model \u2014 some have greater relevance for the task at hand or are more rich in informative content than others. So how does one evaluate the value of a single datum? At the granularity of a full dataset, it is straightforward; one can simply train a model on the entire dataset and use its performance on a test set as its value. However, estimating the value of a single datum is far more difficult, especially for complex models that rely on large-scale datasets, because it is computationally infeasible to re-train and re-evaluate a model on all possible subsets.\nTo tackle this, researchers have explored permutation-based methods (e.g., influence functions), and game theory-based methods (e.g., data Shapley). However, even the best current methods are far from being computationally feasible for large datasets and complex models, and their data valuation performance is limited. Concurrently, meta learning-based adaptive weight assignment approaches have been developed to estimate the weight values using a meta-objective. But rather than prioritizing learning from high value data samples, their data value mapping is typically based on gradient descent learning or other heuristic approaches that alter the conventional predictor model training dynamics, which can result in performance changes that are unrelated to the value of individual data points.influence functionsdata Shapleymeta learning-based adaptive weight assignment approaches\nData Valuation Using Reinforcement Learning (DVRL)\nTo infer the data values, we propose a data value estimator (DVE) that estimates data values and selects the most valuable samples to train the predictor model. This selection operation is fundamentally non-differentiable and thus conventional gradient descent-based methods cannot be used. Instead, we propose to use reinforcement learning (RL) such that the supervision of the DVE is based on a reward that quantifies the predictor performance on a small (but clean) validation set. The reward guides the optimization of the policy towards the action of optimal data valuation, given the state and input samples. Here, we treat the predictor model learning and evaluation framework as the environment, a novel application scenario of RL-assisted machine learning.gradient descentreinforcement learning\nResults\nWe evaluate the data value estimation quality of DVRL on multiple types of datasets and use cases.Leave-One-OutWideResNet-28-10CIFAR-10 and CIFAR-100HAM 10000USPS datae-mail spam dataSMS dataset\nConclusions\nWe propose a novel meta learning framework for data valuation which determines how likely each training sample will be used in training of the predictor model. Unlike previous works, our method integrates data valuation into the training procedure of the predictor model, allowing the predictor and DVE to improve each other's performance. We model this data value estimation task using a DNN trained through RL with a reward obtained from a small validation set that represents the target task performance.  In a computationally-efficient way, DVRL can provide high quality ranking of training data that is useful for domain adaptation, corrupted sample discovery and robust learning.  We show that DVRL significantly outperforms alternative methods on diverse types of tasks and datasets.\nAcknowledgements\nWe gratefully acknowledge the contributions of Tomas Pfister.",
      "link": "http://ai.googleblog.com/2020/10/estimating-impact-of-training-data-with.html",
      "author": "Posted by Jinsung Yoon and Sercan O. Arik, Research Scientists, Cloud AI Team, Google Research"
    },
    {
      "title": "Rethinking Attention with Performers",
      "date": "Friday, October 23, 2020",
      "abstract": "Rethinking Attention with Performers\nTransformer models have achieved state-of-the-art results across a diverse range of domains, including natural language, conversation, images, and even music. The core block of every Transformer architecture is the attention module, which computes similarity scores for all pairs of positions in an input sequence. This however, scales poorly with the length of the input sequence, requiring quadratic computation time to produce all similarity scores, as well as quadratic memory size to construct a matrix to store these scores.Transformer modelsnatural languageconversationimagesmusicattention module\nFor applications where long-range attention is needed, several fast and more space-efficient proxies have been proposed such as memory caching techniques, but a far more common way is to rely on sparse attention. Sparse attention reduces computation time and the memory requirements of the attention mechanism by computing a limited selection of similarity scores from a sequence rather than all possible pairs, resulting in a sparse matrix rather than a full matrix. These sparse entries may be manually proposed, found via optimization methods, learned, or even randomized, as demonstrated by such methods as Sparse Transformers, Longformers, Routing Transformers, Reformers, and Big Bird. Since sparse matrices can also be represented by graphs and edges, sparsification methods are also motivated by the graph neural network literature, with specific relationships to attention outlined in Graph Attention Networks. Such sparsity-based architectures usually require additional layers to implicitly produce a full attention mechanism.memory caching techniquessparse attentionSparse TransformersLongformersRouting TransformersReformersBig Birdgraphs and edgesgraph neural networkGraph Attention NetworksEfficient Transformers: A Survey\nUnfortunately, sparse attention methods can still suffer from a number of limitations. (1) They require efficient sparse-matrix multiplication operations, which are not available on all accelerators; (2) they usually do not provide rigorous theoretical guarantees for their representation power; (3) they are optimized primarily for Transformer models and generative pre-training; and (4) they usually stack more attention layers to compensate for sparse representations, making them difficult to use with other pre-trained models, thus requiring retraining and significant energy consumption. In addition to these shortcomings, sparse attention mechanisms are often still not sufficient to address the full range of problems to which regular attention methods are applied, such as Pointer Networks. There are also some operations that cannot be sparsified, such as the commonly used softmax operation, which normalizes similarity scores in the attention mechanism and is used heavily in industry-scale recommender systems.retraining and significant energy consumptionPointer Networkssoftmax operationindustry-scale recommender systems\nTo resolve these issues, we introduce the Performer, a Transformer architecture with attention mechanisms that scale linearly, thus enabling faster training while allowing the model to process longer lengths, as required for certain image datasets such as ImageNet64 and text datasets such as PG-19. The Performer uses an efficient (linear) generalized attention framework, which allows a broad class of attention mechanisms based on different similarity measures (kernels). The framework is implemented by our novel Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm, which provides scalable low-variance and unbiased estimation of attention mechanisms that can be expressed by random feature map decompositions (in particular, regular softmax-attention). We obtain strong accuracy guarantees for this method while preserving linear space and time complexity, which can also be applied to standalone softmax operations.PerformerImageNet64PG-19Fast Attention Via Positive Orthogonal Random Features (FAVOR+) algorithm\nGeneralized Attention\nIn the original attention mechanism, the query and key inputs, corresponding respectively to rows and columns of a matrix, are multiplied together and passed through a softmax operation to form an attention matrix, which stores the similarity scores. Note that in this method, one  cannot decompose the query-key product back into its original query and key components after passing it into the nonlinear softmax operation. However, it is possible to decompose the attention matrix back to a product of random nonlinear functions of the original queries and keys, otherwise known as random features, which allows one to encode the similarity information in a more efficient manner.\nRegular softmax-attention can be seen as a special case with these nonlinear functions defined by exponential functions and Gaussian projections. Note that we can also reason inversely, by implementing more general nonlinear functions first, implicitly defining other types of similarity measures, or kernels, on the query-key product. We frame this as generalized attention, based on earlier work in kernel methods. Although for most kernels, closed-form formulae do not exist, our mechanism can still be applied since it does not rely on them.kernel methods\nTo the best of our knowledge, we are the first to show that any attention matrix can be effectively approximated in downstream Transformer-applications using random features. The novel mechanism enabling this is the use of positive random features, i.e., positive-valued nonlinear functions of the original queries and keys, which prove to be crucial for avoiding instabilities during training and provide more accurate approximation of the regular softmax attention mechanism.\nTowards FAVOR+: Fast Attention via Matrix Associativity\nThe decomposition described above allows one to store the implicit attention matrix with linear, rather than quadratic, memory complexity. One can also obtain a linear time attention mechanism using this decomposition. While the original attention mechanism multiplies the stored attention matrix with the value input to obtain the final result, after decomposing the attention matrix, one can rearrange matrix multiplications to approximate the result of the regular attention mechanism, without explicitly constructing the quadratic-sized attention matrix. This ultimately leads to FAVOR+.\nThe above analysis is relevant for so-called bidirectional attention, i.e., non-causal attention  where there is no notion of past and future. For unidirectional (causal) attention, where tokens do not attend to other tokens appearing later in the input sequence, we slightly modify the approach to use prefix-sum computations, which only store running totals of matrix computations rather than storing an explicit lower-triangular regular attention matrix.prefix-sum computations\nProperties\nWe first benchmark the space- and time-complexity of the Performer and show that the attention speedups and memory reductions are empirically nearly optimal, i.e., very close to simply not using an attention mechanism at all in the model.\nWe further show that the Performer, using our unbiased softmax approximation, is backwards compatible with pretrained Transformer models after a bit of fine-tuning, which could potentially lower energy costs by improving inference speed, without having to fully retrain pre-existing models.One Billion Word Benchmark\nExample Application: Protein Modeling\nProteins are large molecules with complex 3D structures and specific functions that are essential to life. Like words, proteins are specified as linear sequences where each character is one of 20 amino acid building blocks. Applying Transformers to large unlabeled corpora of protein sequences (e.g. UniRef) yields models that can be used to make accurate predictions about the folded, functional macromolecule. Performer-ReLU (which uses ReLU-based attention, an instance of generalized attention that is different from softmax) performs strongly at modeling protein sequence data, while Performer-Softmax matches the performance of the Transformer, as predicted by our theoretical results.UniRefmodelsReLUProGen (2019)\nBelow we visualize a protein Performer model, trained using the ReLU-based approximate attention mechanism. Using the Performer to estimate similarity between amino acids recovers similar structure to well-known substitution matrices obtained by analyzing evolutionary substitution patterns across carefully curated sequence alignments. More generally, we find local and global attention patterns consistent with Transformer models trained on protein data. The dense attention approximation of the Performer has the potential to capture global interactions across multiple protein sequences. As a proof of concept, we train models on long concatenated protein sequences, which overloads the memory of a regular Transformer model, but not the Performer due to its space efficiency.structuresubstitution matricesevolutionary substitution patternsTransformer models trained on protein dataglobal interactionsBPT1_BOVIN\nConclusion \nOur work contributes to the recent efforts on non-sparsity based methods and kernel-based interpretations of Transformers. Our method is interoperable with other techniques like reversible layers and we have even integrated FAVOR with the Reformer's code. We provide the links for the paper, Performer code, and the Protein Language Modeling code. We believe that our research opens up a brand new way of thinking about attention, Transformer architectures, and even kernel methods.non-sparsity based methodskernel-based interpretations of TransformersFAVOR with the Reformer's codepaperPerformer codeProtein Language Modeling code\nAcknowledgements\nThis work was performed by the core Performer designers Krzysztof Choromanski (Google Brain Team, Tech and Research Lead), Valerii Likhosherstov (University of Cambridge) and Xingyou Song (Google Brain Team), with contributions from David Dohan, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. We give special thanks to the Applied Science Team for jointly leading the research effort on applying efficient Transformer architectures to protein sequence data.Google Brain TeamApplied Science Team\nWe additionally wish to thank Joshua Meier, John Platt, and Tom Weingarten for many fruitful discussions on biological data and useful comments on this draft, along with Yi Tay and Mostafa Dehghani for discussions on comparing baselines. We further thank Nikita Kitaev and Wojciech Gajewski for multiple discussions on the Reformer, and Aurko Roy and Ashish Vaswani for multiple discussions on the Routing Transformer.",
      "link": "http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html",
      "author": "Posted by Krzysztof Choromanski and Lucy Colwell, Research Scientists, Google Research"
    },
    {
      "title": "Announcing the Recipients of the 2020 Award for Inclusion Research",
      "date": "Wednesday, October 21, 2020",
      "abstract": "Announcing the Recipients of the 2020 Award for Inclusion Research\nAt Google, it is our ongoing goal to support faculty who are conducting innovative research that will have positive societal impact. As part of that goal, earlier this year we launched the Award for Inclusion Research program, a global program that supports academic research in computing and technology addressing the needs of underrepresented populations. The Award for Inclusion Research program allows faculty and Google researchers an opportunity to partner on their research initiatives and build new and constructive long-term relationships.Award for Inclusion Research\nWe received 100+ applications from over 100 universities, globally, and today we are excited to announce the 16 proposals chosen for funding, focused on an array of topics around diversity and inclusion, algorithmic bias, education innovation, health tools, accessibility, gender bias, AI for social good, security, and social justice. The proposals include 25 principal investigators who focus on making the community stronger through their research efforts.\nCongratulations to announce this year\u2019s recipients:\n  \"Human Centred Technology Design for Social Justice in Africa\"\nAnicia Peters (University of Namibia) and Shaimaa Lazem (City for Scientific Research and Technological Applications, Egypt)\n  \"Modern NLP for Regional and Dialectal Language Variants\"\nAntonios Anastasopoulos (George Mason University)\n  \"Culturally Relevant Collaborative Health Tracking Tools for Motivating Heart-Healthy Behaviors Among African Americans\"\n  Aqueasha Martin-Hammond (Indiana University - Purdue University Indianapolis) and Tanjala S. Purnell (Johns Hopkins University)\n  \"Characterizing Energy Equity in the United States\"\n  Destenie Nock and Constantine Samaras (Carnegie Mellon University)\n  \"Developing a Dialogue System for a Culturally-Responsive Social Programmable Robot\"\n  Erin Walker (University of Pittsburgh) and Leshell Hatley (Coppin State University)\n  \"Eliminating Gender Bias in NLP Beyond English\"\n  Hinrich Schuetze (LMU Munich)\n  \"The Ability-Based Design Mobile Toolkit: Enabling Accessible Mobile Interactions through Advanced Sensing and Modeling\"\n  Jacob O. Wobbrock (University of Washington)\n  \"Mutual aid and community engagement: Community-based mechanisms against algorithmic bias\"\n  Jasmine McNealy (University of Florida)\n  \"Empowering Syrian Girls through Culturally Sensitive Mobile Technology and Media Literacy\n  Karen Elizabeth Fisher (University of Washington) and Yacine Ghamri-Doudane (University of La Rochelle)\n  \"Broadening participation in data science through examining the health, social, and economic impacts of gentrification\"\n  Latifa Jackson (Howard University) and Hasan Jackson (Howard University)\n  \"Understanding How Peer and Near Peer Mentors co-Facilitating the Active Learning Process of Introductory Data Structures Within an Immersive Summer Experience Effected Rising Sophomore Computer Science Student Persistence and Preparedness for Careers in Silicon Valley\"\n  Legand Burge (Howard University) and Marlon Mejias (University of North Carolina at Charlotte)\n  \"Who is Most Likely to Advocate for this Case? A Machine Learning Approach\"\n  Maria De-Arteaga (University of Texas at Austin)\n  \"Contextual Rendering of Equations for Visually Impaired Persons\"\n  Meenakshi Balakrishnan (Indian Institute of Technology Delhi, India) and Volker Sorge (University of Birmingham)\n  \"Measuring the Cultural Competence of Computing Students and Faculty Nationwide to Improve Diversity, Equity, and Inclusion\"\n  Nicki Washington (Duke University)\n  \"Designing and Building Collaborative Tools for Mixed-Ability Programming Teams\"\n  Steve Oney (University of Michigan)\n  \"Iterative Design of a Black Studies Research Computing Initiative through `Flipped Research\u2019\"\n  Timothy Sherwood and Sharon Tettegah (University of California, Santa Barbara)",
      "link": "http://ai.googleblog.com/2020/10/announcing-2020-award-for-inclusion.html",
      "author": "Posted by Negar Saei, Program Manager, Google Research"
    },
    {
      "title": "Recreating Historical Streetscapes Using Deep Learning and Crowdsourcing",
      "date": "Thursday, October 15, 2020",
      "abstract": "Recreating Historical Streetscapes Using Deep Learning and Crowdsourcing\nFor many, gazing at an old photo of a city can evoke feelings of both nostalgia and wonder \u2014 what was it like to walk through Manhattan in the 1940s? How much has the street one grew up on changed? While Google Street View allows people to see what an area looks like in the present day, what if you want to explore how places looked in the past?\nTo create a rewarding \u201ctime travel\u201d experience for both research and entertainment purposes, we are launching a browser-based toolset called\u00a0r\u01dd (pronounced as re\u201dturn\"), an open source, scalable system running on Google Cloud and Kubernetes that can reconstruct cities from historical maps and photos, which represents an implementation of our suite of open source tools launched earlier this year. Referencing the common prefix meaning again or anew, r\u01dd is meant to represent the themes of reconstruction, research, recreation and remembering behind this crowdsourced research effort, and consists of three components:browser-based toolset called\u00a0r\u01ddGoogle CloudKuberneteslaunched\nOur goal is for r\u01dd to become a compendium that allows history enthusiasts to virtually experience historical cities around the world, aids researchers, policy makers and educators, and provides a dose of nostalgia to everyday users.\nCrowdsourcing Data from Historical Maps\nReconstructing how cities used to look at scale is a challenge \u2014 historical image data is more difficult to work with than modern data, as there are far fewer images available and much less metadata captured from the images. To help with this difficulty, the r\u01dd maps module is a suite of open source tools that work together to create a map server with a time dimension, allowing users to jump back and forth between time periods using a slider. These tools allow users to upload scans of historical print maps, georectify them to match real world coordinates, and then convert them to vector format by tracing their geographic features. These vectorized maps are then served on a tile server and rendered as slippy maps, which lets the user zoom in and pan around.slippy maps\nThe entry point of the r\u01dd maps module is Warper, a web app that allows users to upload historical images of maps and georectify them by finding control points on the historical map and corresponding points on a base map. The next app, Editor, allows users to load the georectified historical maps as the background and then trace their geographic features (e.g., building footprints, roads, etc.). This traced data is stored in an OpenStreetMap (OSM) vector format. They are then converted to vector tiles and served from the Server app, a vector tile server. Finally, our map renderer, Kartta, visualizes the spatiotemporal vector tiles allowing the users to navigate space and time on historical maps. These tools were built on top of numerous open source resources including OpenStreetMap, and we intend for our tools and data to be completely open source as well.OpenStreetMap\n3D Experience\nThe 3D Models module aims to reconstruct the detailed full 3D structures of historical buildings using the associated images and maps data, organize these 3D models properly in one repository, and render them on the historical maps with a time dimension.\nIn many cases, there is only one historical image available for a building, which makes the 3D reconstruction an extremely challenging problem. To tackle this challenge, we developed a coarse-to-fine reconstruction-by-recognition algorithm.\nStarting with footprints on maps and fa\u00e7ade regions in historical images (both are annotated by crowdsourcing or detected by automatic algorithms), the footprint of one input building is extruded upwards to generate its coarse 3D structure. The height of this extrusion is set to the number of floors from the corresponding metadata in the maps database.\nIn parallel, instead of directly inferring the detailed 3D structures of each fa\u00e7ade as one entity, the 3D reconstruction pipeline recognizes all individual constituent components (e.g., windows, entries, stairs, etc.) and reconstructs their 3D structures separately based on their categories. Then these detailed 3D structures are merged with the coarse one for the final 3D mesh. The results are stored in a 3D repository and ready for 3D rendering.\nThe key technology powering this feature is a number of state-of-art deep learning models:region-based convolutional neural networksDeepLab\nKey Results\nConclusion\n  With r\u01dd, we have developed tools that facilitate crowdsourcing to tackle the main challenge of insufficient historical data when recreating virtual cities. The 3D experience is still a work-in-progress and we aim to improve it with future updates. We hope r\u01dd acts as a nexus for an active community of enthusiasts and casual users that not only utilizes our historical datasets and open source code, but actively contributes to both.\nAcknowledgements\nThis effort has been successful thanks to the hard work of many people, including, but not limited to the following (in alphabetical order of last name): Yale Cong, Feng Han, Amol Kapoor, Raimondas Kiveris, Brandon Mayer, Mark Phillips, Sasan Tavakkol, and Tim Waters (Waters Geospatial Ltd).",
      "link": "http://ai.googleblog.com/2020/10/recreating-historical-streetscapes.html",
      "author": "Posted by Raimondas Kiveris, Software Engineer, Google Research"
    },
    {
      "title": "Measuring Gendered Correlations in Pre-trained NLP Models",
      "date": "Wednesday, October 14, 2020",
      "abstract": "Measuring Gendered Correlations in Pre-trained NLP Models\nNatural language processing (NLP) has seen significant progress over the past several years, with pre-trained models like BERT, ALBERT, ELECTRA, and XLNet achieving remarkable accuracy across a variety of tasks. In pre-training, representations are learned from a large text corpus, e.g., Wikipedia, by repeatedly masking out words and trying to predict them (this is called masked language modeling). The resulting representations encode rich information about language and correlations between concepts, such as surgeons and scalpels. There is then a second training stage, fine-tuning, in which the model uses task-specific training data to learn how to use the general pre-trained representations to do a concrete task, like classification. Given the broad adoption of these representations in many NLP tasks, it is crucial to understand the information encoded in them and how any learned correlations affect performance downstream, to ensure the application of these models aligns with our AI Principles.Natural language processingBERTALBERTELECTRAXLNetWikipediamasked language modelingclassificationAI Principles\nIn \u201cMeasuring and Reducing Gendered Correlations in Pre-trained Models\u201d we perform a case study on BERT and its low-memory counterpart ALBERT, looking at correlations related to gender, and formulate a series of best practices for using pre-trained language models. We present experimental results over public model checkpoints and an academic task dataset to illustrate how the best practices apply, providing a foundation for exploring settings beyond the scope of this case study. We will soon release a series of checkpoints, Zari1, which reduce gendered correlations while maintaining state-of-the-art accuracy on standard NLP task metrics.Measuring and Reducing Gendered Correlations in Pre-trained ModelsZari1\nMeasuring Correlations\nTo understand how correlations in pre-trained representations can affect downstream task performance, we apply a diverse set of evaluation metrics for studying the representation of gender. Here, we\u2019ll discuss results from one of these tests, based on coreference resolution, which is the capability that allows models to understand the correct antecedent to a given pronoun in a sentence. For example, in the sentence that follows, the model should recognize his refers to the nurse, and not to the patient.coreference resolution\nThe standard academic formulation of the task is the OntoNotes test (Hovy et al., 2006), and we measure how accurate a model is at coreference resolution in a general setting using an F1 score over this data (as in Tenney et al. 2019). Since OntoNotes represents only one data distribution, we also consider the WinoGender benchmark that provides additional, balanced data designed to identify when model associations between gender and profession incorrectly influence coreference resolution. High values of the WinoGender metric (close to one) indicate a model is basing decisions on normative associations between gender and profession (e.g., associating nurse with the female gender and not male). When model decisions have no consistent association between gender and profession, the score is zero, which suggests that decisions are based on some other information, such as sentence structure or semantics.OntoNotesHovy et al., 2006F1 scoreTenney et al. 2019WinoGendernormative associations between gender and profession\nIn this study, we see that neither the (Large) BERT or ALBERT public model achieves zero score on the WinoGender examples, despite achieving impressive accuracy on OntoNotes (close to 100%). At least some of this is due to models preferentially using gendered correlations in reasoning. This isn\u2019t completely surprising: there are a range of cues available to understand text and it is possible for a general model to pick up on any or all of these. However, there is reason for caution, as it is undesirable for a model to make predictions primarily based on gendered correlations learned as priors rather than the evidence available in the input.BERTALBERT\nBest Practices\nGiven that it is possible for unintended correlations in pre-trained model representations to affect downstream task reasoning, we now ask: what can one do to mitigate any risk this poses when developing new NLP models?dropout regularizationoverfittingour paper\nWhat\u2019s Next\nWe believe these best practices provide a starting point for developing robust NLP systems that perform well across the broadest possible range of linguistic settings and applications.  Of course these techniques on their own are not sufficient to capture and remove all potential issues. Any model deployed in a real-world setting should undergo rigorous testing that considers the many ways it will be used, and implement safeguards to ensure alignment with ethical norms, such as Google's AI Principles. We look forward to developments in evaluation frameworks and data that are more expansive and inclusive to cover the many uses of language models and the breadth of people they aim to serve.\nAcknowledgements\nThis is joint work with Xuezhi Wang, Ian Tenney, Ellie Pavlick, Alex Beutel, Jilin Chen, Emily Pitler, and Slav Petrov. We benefited greatly throughout the project from discussions with Fernando Pereira, Ed Chi, Dipanjan Das, Vera Axelrod, Jacob Eisenstein, Tulsee Doshi, and James Wexler.\n1 Zari is an Afghan Muppet designed to show that \u2018a little girl could do as much as everybody else\u2019. [Edit 10/23/2020 \u2014 The Zari checkpoints are now available on the GitHub repository.]\u00a0\u21a91Afghan MuppetGitHub repository\u21a9",
      "link": "http://ai.googleblog.com/2020/10/measuring-gendered-correlations-in-pre.html",
      "author": "Posted by Kellie Webster, Software Engineer, Google Research"
    },
    {
      "title": "Announcing the 2020 Google PhD Fellows",
      "date": "Thursday, October 8, 2020",
      "abstract": "Announcing the 2020 Google PhD Fellows\nGoogle created the PhD Fellowship Program in 2009 to recognize and support outstanding graduate students who seek to influence the future of technology by pursuing exceptional research in computer science and related fields. Now in its twelfth year, these Fellowships have helped support approximately 500 graduate students globally in North America and Europe, Africa, Australia, East Asia, and India.PhD Fellowship Program\nIt is our ongoing goal to continue to support the academic community as a whole, and these Fellows as they make their mark on the world. We congratulate all of this year\u2019s awardees!\nAlgorithms, Optimizations and Markets\nJan van den Brand, KTH Royal Institute of Technology\nMahsa Derakhshan, University of Maryland, College Park\nSidhanth Mohanty, University of California, Berkeley\nComputational Neuroscience\nConnor Brennan, University of Pennsylvania\nHuman Computer Interaction\nAbdelkareem Bedri, Carnegie Mellon University\nBrendan David-John, University of Florida\nHiromu Yakura, University of Tsukuba\nManaswi Saha, University of Washington\nMuratcan Cicek, University of California, Santa Cruz\nPrashan Madumal, University of Melbourne\nMachine Learning\nAlon Brutzkus, Tel Aviv University\nChin-Wei Huang, Universite de Montreal\nEli Sherman, Johns Hopkins University\nEsther Rolf, University of California, Berkeley\nImke Mayer, Fondation Sciences Math\u00e9matique de Paris\nJean Michel Sarr, Cheikh Anta Diop University\nLei Bai, University of New South Wales\nNontawat Charoenphakdee, The University of Tokyo\nPreetum Nakkiran, Harvard University\nSravanti Addepalli, Indian Institute of Science\nTaesik Gong, Korea Advanced Institute of Science and Technology\nVihari Piratla, Indian Institute of Technology - Bombay\nVishakha Patil, Indian Institute of Science \nWilson Tsakane Mongwe, University of Johannesburg\nXinshi Chen, Georgia Institute of Technology\nYadan Luo, University of Queensland\nMachine Perception, Speech Technology and Computer Vision\nBenjamin van Niekerk, University of Stellenbosch\nEric Heiden, University of Southern California\nGyeongsik Moon, Seoul National University\nHou-Ning Hu, National Tsing Hua University\nNan Wu, New York University\nShaoshuai Shi, The Chinese University of Hong Kong\nYaman Kumar, Indraprastha Institute of Information Technology - Delhi\nYifan Liu, University of Adelaide\nYu Wu, University of Technology Sydney\nZhengqi Li, Cornell University\nMobile Computing\nXiaofan Zhang, University of Illinois at Urbana-Champaign\nNatural Language Processing\nAnjalie Field, Carnegie Mellon University\nMingda Chen, Toyota Technological Institute at Chicago\nShang-Yu Su, National Taiwan University\nYanai Elazar, Bar-Ilan\nPrivacy and Security\nJulien Gamba, Universidad Carlos III de Madrid\nShuwen Deng, Yale University\nYunusa Simpa Abdulsalm, Mohammed VI Polytechnic University\nProgramming Technology and Software Engineering\nAdriana Sejfia, University of Southern California\nJohn Cyphert, University of Wisconsin-Madison\nQuantum Computing\nAmira Abbas, University of KwaZulu-Natal\nMozafari Ghoraba Fereshte, EPFL\nStructured Data and Database Management\nYanqing Peng, University of Utah\nSystems and Networking\nHuynh Nguyen Van, University of Technology Sydney\nMichael Sammler, Saarland University, MPI-SWS\nSihang Liu, University of Virginia\nYun-Zhan Cai, National Cheng Kung University",
      "link": "http://ai.googleblog.com/2020/10/announcing-2020-google-phd-fellows.html",
      "author": "Posted by Susie Kim, Program Manager, University Relations"
    },
    {
      "title": "Massively Large-Scale Distributed Reinforcement Learning with Menger",
      "date": "Friday, October 2, 2020",
      "abstract": "Massively Large-Scale Distributed Reinforcement Learning with Menger\nIn the last decade, reinforcement learning (RL) has become one of the most promising research areas in machine learning and has demonstrated great potential for solving sophisticated real-world problems, such as chip placement and resource management, and solving challenging games (e.g., Go, Dota 2, and hide-and-seek). In simplest terms, an RL infrastructure is a loop of data collection and training, where actors explore the environment and collect samples, which are then sent to the learners to train and update the model.  Most current RL techniques require many iterations over batches of millions of samples from the environment to learn a target task (e.g., Dota 2  learns from batches of 2 million frames every 2 seconds). As such, an RL infrastructure should not only scale efficiently (e.g., increase the number of actors) and collect an immense number of samples, but also be able to swiftly iterate over these extensive amounts of samples during training.reinforcement learningchip placementresource managementGoDota 2hide-and-seekDota 2TF-AgentsIMPALA\n  Today we introduce Menger1, a massive large-scale distributed RL infrastructure with localized inference that scales up to several thousand actors across multiple processing clusters (e.g., Borg cells), reducing the overall training time in the task of chip placement. In this post we describe how we implement Menger using Google TPU accelerators for fast training iterations, and present its performance and scalability on the challenging task of chip placement. Menger reduces the training time by up to 8.6x compared to a baseline implementation.1Borg cellsGoogle TPU acceleratorschip placementimplementation\nMenger System Design\nThere are various distributed RL systems, such as Acme and SEED RL, each of which focus on optimizing a single particular design point in the space of distributed reinforcement learning systems. For example, while Acme uses local inference on each actor with frequent model retrieval from the learner, SEED RL benefits from a centralized inference design by allocating a portion of TPU cores for performing batched calls. The tradeoffs between these design points are (1) paying the communication cost of sending/receiving observations and actions to/from a centralized inference server or paying the communication cost of model retrieval from a learner and (2) the cost of inference on actors (e.g., CPUs) compared to accelerators (e.g., TPUs/GPUs). Because of the requirements of our target application (e.g., size of observations, actions, and model size), Menger uses local inference in a manner similar to Acme, but pushes the scalability of actors to virtually an unbounded limit. The main challenges to achieving massive scalability and fast training on accelerators include:AcmeSEED RLTPU Pod\nEfficient Model Retrieval\nTo address the first challenge, we introduce transparent and distributed caching components between the learner and the actors optimized in TensorFlow and backed by Reverb (similar approach used in Dota). The main responsibility of the caching components is to strike a balance between the large number of requests from actors and the learner job. Adding these caching components not only significantly reduces the pressure on the learner to service the read requests, but also further distributes the actors across multiple Borg cells with a marginal communication overhead. In our study, we show that for a 16 MB model with 512 actors, the introduced caching components reduce the average read latency by a factor of ~4.0x leading to faster training iterations, especially for on-policy algorithms such as PPO.ReverbDotaBorg cellsPPOgRPC\nHigh Throughput Input Pipeline\nTo deliver a high throughput input data pipeline, Menger uses Reverb, a recently open-sourced data storage system designed for machine learning applications that provides an efficient and flexible platform to implement experience replay in a variety of on-policy/off-policy algorithms. However, using a single Reverb replay buffer service does not currently scale well in a distributed RL setting with thousands of actors, and simply becomes inefficient in terms of write throughput from actors.Reverbexperience replay\nTo better understand the efficiency of the replay buffer in a distributed setting, we evaluate the average write latency for various payload sizes from 16 MB to 512 MB and a number of actors ranging from 16 to 2048. We repeat the experiment when the replay buffer and actors are placed on the same Borg cell. As the number of actors grows the average write latency also increases significantly. Expanding the number of actors from 16 to 2048, the average write latency increases by a factor of ~6.2x and ~18.9x for payload size 16 MB and 512 MB, respectively. This increase in the write latency negatively impacts the data collection time and leads to inefficiency in the overall training time.BorgReverb\nTo mitigate this, we use the sharding capability provided by Reverb to increase the throughput between actors, learner, and replay buffer services. Sharding balances the write load from the large number of actors across multiple replay buffer servers, instead of throttling a single replay buffer server, and also minimizes the average write latency for each replay buffer server (as fewer actors share the same server). This enables Menger to scale efficiently to thousands of actors across multiple Borg cells.shardingReverb\nCase Study: Chip Placement \nWe studied the benefits of Menger in the complex task of chip placement for a large netlist. Using 512 TPU cores, Menger achieves significant improvements in the training time (up to ~8.6x, reducing the training time from ~8.6 hours down to merely one hour in the fastest configuration) compared to a strong baseline. While Menger was optimized for TPUs, that the key factor for this performance gain is the architecture, and we would expect to see similar gains when tailored to use on GPUs.chip placementnetlistbaselinechip placement\nWe believe that Menger infrastructure and its promising results in the intricate task of chip placement demonstrate an innovative path forward to further shorten the chip design cycle and has the potential to not only enable further innovations in the chip design process, but other challenging real-world tasks as well.\nAcknowledgments\nMost of the work was done by Amir Yazdanbakhsh, Junchao Chen, and Yu Zheng. We would like to also thank Robert Ormandi, Ebrahim Songhori, Shen Wang, TF-Agents team, Albin Cassirer, Aviral Kumar, James Laudon, John Wilkes, Joe Jiang, Milad Hashemi, Sat Chatterjee, Piotr Stanczyk, Sabela Ramos, Lasse Espeholt, Marcin Michalski, Sam Fishman, Ruoxin Sang, Azalia Mirhosseini, Anna Goldie, and Eric Johnson for their help and support.\n\n1 A Menger cube is a three-dimensional fractal curve, and the inspiration for the name of this system, given that the proposed infrastructure can virtually scale ad infinitum. \u21a91Menger\u21a9",
      "link": "http://ai.googleblog.com/2020/10/massively-large-scale-distributed.html",
      "author": "Posted by Amir Yazdanbakhsh, Research Scientist and Junchao Chen, Software Engineer, Google Research"
    },
    {
      "title": "Developing Real-Time, Automatic Sign Language Detection for Video Conferencing",
      "date": "Thursday, October 1, 2020",
      "abstract": "Developing Real-Time, Automatic Sign Language Detection for Video Conferencing\nVideo conferencing should be accessible to everyone, including users who communicate using sign language. However, since most video conference applications transition window focus to those who speak aloud, it makes it difficult for signers to \u201cget the floor\u201d so they can communicate easily and effectively. Enabling real-time sign language detection in video conferencing is challenging, since applications need to perform classification using the high-volume video feed as the input, which makes the task computationally heavy. In part, due to these challenges, there is only limited research on sign language detection.\nIn \u201cReal-Time Sign Language Detection using Human Pose Estimation\u201d, presented at SLRTP2020 and demoed at ECCV2020, we present a real-time sign language detection model and demonstrate how it can be used to provide video conferencing systems a mechanism to identify the person signing as the active speaker.Real-Time Sign Language Detection using Human Pose EstimationSLRTP2020ECCV2020\nOur Model\nTo enable a real-time working solution for a variety of video conferencing applications, we needed to design a light weight model that would be simple to \u201cplug and play.\u201d Previous attempts to integrate models for video conferencing applications on the client side demonstrated the importance of a light-weight model that consumes fewer CPU cycles in order to minimize the effect on call quality. To reduce the input dimensionality, we isolated the information the model needs from the video in order to perform the classification of every frame.\nBecause sign language involves the user\u2019s body and hands, we start by running a pose estimation model, PoseNet. This reduces the input considerably from an entire HD image to a small set of landmarks on the user\u2019s body, including the eyes, nose, shoulders, hands, etc. We use these landmarks to calculate the frame-to-frame optical flow, which quantifies user motion for use by the model without retaining user-specific information. Each pose is normalized by the width of the person\u2019s shoulders in order to ensure that the model attends to the person signing over a range of distances from the camera. The optical flow is then normalized by the video\u2019s frame rate before being passed to the model.PoseNetoptical flow\nTo test this approach, we used the German Sign Language corpus (DGS), which contains long videos of people signing, and includes span annotations that indicate in which frames signing is taking place. As a na\u00efve baseline, we trained a linear regression model to predict when a person is signing using optical flow data. This baseline reached around 80% accuracy, using only ~3\u03bcs (0.000003 seconds) of processing time per frame. By including the 50 previous frames\u2019 optical flow as context to the linear model, it is able to reach 83.4%.German Sign Language corpus\nTo generalize the use of context, we used a long-short-term memory (LSTM) architecture, which contains memory over the previous timesteps, but no lookback. Using a single layer LSTM, followed by a linear layer, the model achieves up to 91.5% accuracy, with 3.5ms (0.0035 seconds) of processing time per frame.long-short-term memory\nProof of Concept\nOnce we had a functioning sign language detection model, we needed to devise a way to use it for triggering the active speaker function in video conferencing applications. We developed a lightweight, real-time, sign language detection web demo that connects to various video conferencing applications and can set the user as the \u201cspeaker\u201d when they sign. This demo leverages PoseNet fast human pose estimation and sign language detection models running in the browser using tf.js, which enables it to work reliably in real-time.PoseNettf.js\nWhen the sign language detection model determines that a user is signing, it passes an ultrasonic audio tone through a virtual audio cable, which can be detected by any video conferencing application as if the signing user is \u201cspeaking.\u201d The audio is transmitted at 20kHz, which is normally outside the hearing range for humans. Because video conferencing applications usually detect the audio \u201cvolume\u201d as talking rather than only detecting speech, this fools the application into thinking the user is speaking.virtual audio cable\nYou can try our experimental demo right now! By default, the demo acts as a sign language detector. The training code and models as well as the web demo\u00a0source code is available on GitHub.our experimental demotraining code and modelsweb demo\nDemo\nIn the following video, we demonstrate how the model might be used. Notice the yellow chart at the top left corner, which reflects the model\u2019s confidence in detecting that activity is indeed sign language. When the user signs, the chart values rise to nearly 100, and when she stops signing, it falls to zero. This process happens in real-time, at 30 frames per second, the maximum frame rate of the camera used.\nUser Feedback\nTo better understand how well the demo works in practice, we conducted a user experience study in which participants were asked to use our experimental demo during a video conference and to communicate via sign language as usual. They were also asked to sign over each other, and over speaking participants to test the speaker switching behavior. Participants responded positively that sign language was being detected and treated as audible speech, and that the demo successfully identified the signing attendee and triggered the conferencing system\u2019s audio meter icon to draw focus to the signing attendee.\nConclusions\nWe believe video conferencing applications should be accessible to everyone and hope this work is a meaningful step in this direction. We have demonstrated how our model could be leveraged to empower signers to use video conferencing more conveniently.\nAcknowledgements\nAmit Moryossef, Ioannis Tsochantaridis, Roee Aharoni, Sarah Ebling, Annette Rios, Srini Narayanan, George Sung, Jonathan Baccash, Aidan Bryant, Pavithra Ramasamy and Maayan Gazuli",
      "link": "http://ai.googleblog.com/2020/10/developing-real-time-automatic-sign.html",
      "author": "Posted by Amit Moryossef, Research Intern, Google Research"
    },
    {
      "title": "Audiovisual Speech Enhancement in YouTube Stories",
      "date": "Thursday, October 1, 2020",
      "abstract": "Audiovisual Speech Enhancement in YouTube Stories\nWhile tremendous efforts are invested in improving the quality of videos taken with smartphone cameras, the quality of audio in videos is often overlooked. For example, the speech of a subject in a video where there are multiple people speaking or where there is high background noise might be muddled, distorted, or difficult to understand. In an effort to address this, two years ago we introduced Looking to Listen, a machine learning (ML) technology that uses both visual and audio cues to isolate the speech of a video\u2019s subject. By training the model on a large-scale collection of online videos, we are able to capture correlations between speech and visual signals such as mouth movements and facial expressions, which can then be used to separate the speech of one person in a video from another, or to separate speech from background sounds. We showed that this technology not only achieves state-of-the-art results in speech separation and enhancement (a noticeable 1.5dB improvement over audio-only models), but in particular can improve the results over audio-only processing when there are multiple people speaking, as the visual cues in the video help determine who is saying what.Looking to Listenlarge-scale collection of online videosWe showed\nWe are now happy to make the Looking to Listen technology available to users through a new audiovisual Speech Enhancement feature in YouTube Stories (on iOS), allowing creators to take better selfie videos by automatically enhancing their voices and reducing background noise. Getting this technology into users\u2019 hands was no easy feat. Over the past year, we worked closely with users to learn how they would like to use such a feature, in what scenarios, and what balance of speech and background sounds they would like to have in their videos. We heavily optimized the Looking to Listen model to make it run efficiently on mobile devices, overall reducing the running time from 10x real-time on a desktop when our paper came out, to 0.5x real-time performance on the phone. We also put the technology through extensive testing to verify that it performs consistently across different recording conditions and for people with different appearances and voices.YouTube Storiespaper\nFrom Research to ProductOptimizing Looking to Listen to allow fast and robust operation on mobile devices required us to overcome a number of challenges. First, all processing needed to be done on-device within the client app in order to minimize processing time and to preserve the user\u2019s privacy; no audio or video information would be sent to servers for processing. Further, the model needed to co-exist alongside other ML algorithms used in the YouTube app in addition to the resource-consuming video recording itself. Finally, the algorithm needed to run quickly and efficiently on-device while minimizing battery consumption.other ML algorithms\nThe first step in the Looking to Listen pipeline is to isolate thumbnail images that contain the faces of the speakers from the video stream. By leveraging MediaPipe BlazeFace with GPU accelerated inference, this step is now able to be executed in just a few milliseconds. We then switched the model part that processes each thumbnail separately to a lighter weight MobileNet (v2) architecture, which outputs visual features learned for the purpose of speech enhancement, extracted from the face thumbnails in 10 ms per frame. Because the compute time to embed the visual features is short, it can be done while the video is still being recorded. This avoids the need to keep the frames in memory for further processing, thereby reducing the overall memory footprint. Then, after the video finishes recording, the audio and the computed visual features are streamed to the audio-visual speech separation model which produces the isolated and enhanced speech.MediaPipe BlazeFaceMobileNetv2\nWe reduced the total number of parameters in the audio-visual model by replacing \u201cregular\u201d 2D convolutions with separable ones (1D in the frequency dimension, followed by 1D in the time dimension) with fewer filters. We then optimized the model further using TensorFlow Lite \u2014 a set of tools that enable running TensorFlow models on mobile devices with low latency and a small binary size. Finally, we reimplemented the model within the Learn2Compress framework in order to take advantage of built-in quantized training and QRNN support.TensorFlow LiteLearn2CompressQRNN\nThese optimizations and improvements reduced the running time from 10x real-time on a desktop using the original formulation of Looking to Listen, to 0.5x real-time performance using only an iPhone CPU;  and brought the model size down from 120MB to 6MB now, which makes it easier to deploy. Since YouTube Stories videos are short \u2014 limited to 15 seconds \u2014 the result of the video processing is available within a couple of seconds after the recording is finished.Looking to Listen\nFinally, to avoid processing videos with clean speech (so as to avoid unnecessary computation), we first run our model only on the first two seconds of the video, then compare the speech-enhanced output to the original input audio. If there is sufficient difference (meaning the model cleaned up the speech), then we enhance the speech throughout the rest of the video.\nResearching User NeedsEarly versions of Looking to Listen were designed to entirely isolate speech from the background noise. In a user study conducted together with YouTube, we found that users prefer to leave in some of the background sounds to give context and to retain some the general ambiance of the scene. Based on this user study, we take a linear combination of the original audio and our produced clean speech channel: output_audio = 0.1 x original_audio + 0.9 x speech. The following video presents clean speech combined with different levels of the background sounds in the scene (10% background is the balance we use in practice).\nBelow are additional examples of the enhanced speech results from the new Speech Enhancement feature in YouTube Stories. We recommend watching the videos with good speakers or headphones.\nFairness AnalysisAnother important requirement is that the model be fair and inclusive. It must be able to handle different types of voices, languages and accents, as well as different visual appearances. To this end, we conducted a series of tests exploring the performance of the model with respect to various visual and speech/auditory attributes: the speaker\u2019s age, skin tone, spoken language, voice pitch, visibility of the speaker\u2019s face (% of video in which the speaker is in frame), head pose throughout the video, facial hair, presence of glasses, and the level of background noise in the (input) video.fair and inclusive\nFor each of the above visual/auditory attributes, we ran our model on segments from our evaluation set (separate from the training set) and measured the speech enhancement accuracy, broken down according to the different attribute values. Results for some of the attributes are summarized in the following plots. Each data point in the plots represents hundreds (in most cases thousands) of videos fitting the criteria.voice pitch\nUsing the FeatureYouTube creators who are eligible for YouTube Stories creation may record a video on iOS, and select \u201cEnhance speech\u201d from the volume controls editing tool. This will immediately apply speech enhancement to the audio track and will play back the enhanced speech in a loop. It is then possible to toggle the feature on and off multiple times to compare the enhanced speech with the original audio.\nIn parallel to this new feature in YouTube, we are also exploring additional venues for this technology. More to come later this year \u2014 stay tuned!\nAcknowledgementsThis feature is a collaboration across multiple teams at Google. Key contributors include: from Research-IL: Oran Lang; from VisCAM: Ariel Ephrat, Mike Krainin, JD Velasquez, Inbar Mosseri, Michael Rubinstein; from Learn2Compress: Arun Kandoor; from MediaPipe: Buck Bourdon, Matsvei Zhdanovich, Matthias Grundmann; from YouTube: Andy Poes, Vadim Lavrusik, Aaron La Lau, Willi Geiger, Simona De Rosa, and Tomer Margolin.",
      "link": "http://ai.googleblog.com/2020/10/audiovisual-speech-enhancement-in.html",
      "author": "Posted by Inbar Mosseri, Software Engineer and Michael Rubinstein, Research Scientist, Google Research"
    },
    {
      "title": "Advancing Instance-Level Recognition Research",
      "date": "Friday, September 25, 2020",
      "abstract": "Advancing Instance-Level Recognition Research\nInstance-level recognition (ILR) is the computer vision task of recognizing a specific instance of an object, rather than simply the category to which it belongs. For example, instead of labeling an image as \u201cpost-impressionist painting\u201d, we\u2019re interested in instance-level labels like \u201cStarry Night Over the Rhone by Vincent van Gogh\u201d, or \u201cArc de Triomphe de l'\u00c9toile, Paris, France\u201d, instead of simply \u201carch\u201d. Instance-level recognition problems exist in many domains, like landmarks, artwork, products, or logos, and have applications in visual search apps, personal photo organization, shopping and more. Over the past several years, Google has been contributing to research on ILR with the Google Landmarks Dataset and Google Landmarks Dataset v2 (GLDv2), and novel models such as DELF and Detect-to-Retrieve.Google Landmarks DatasetGoogle Landmarks Dataset v2DELFDetect-to-Retrieve\nToday, we highlight some results from the Instance-Level Recognition Workshop at ECCV\u201920. The workshop brought together experts and enthusiasts in this area, with many fruitful discussions, some of which included our ECCV\u201920 paper \u201cDEep Local and Global features\u201d (DELG), a state-of-the-art image feature model for instance-level recognition, and a supporting open-source codebase for DELG and other related ILR techniques. Also presented were two new landmark challenges (on recognition and retrieval tasks) based on GLDv2, and future ILR challenges that extend to other domains: artwork recognition and product retrieval. The long-term goal of the workshop and challenges is to foster advancements in the field of ILR and push forward the state of the art by unifying research workstreams from different domains, which so far have mostly been tackled as separate problems.Instance-Level Recognition WorkshopECCV\u201920DEep Local and Global featuresopen-source codebaserecognitionretrieval\nDELG: DEep Local and Global Features\nEffective image representations are the key components required to solve instance-level recognition problems. Often, two types of representations are necessary: global and local image features. A global feature summarizes the entire contents of an image, leading to a compact representation but discarding information about spatial arrangement of visual elements that may be characteristic of unique examples. Local features, on the other hand, comprise descriptors and geometry information about specific image regions; they are especially useful to match images depicting the same objects.\nCurrently, most systems that rely on both of these types of features need to separately adopt each of them using different models, which leads to redundant computations and lowers overall efficiency. To address this, we proposed DELG, a unified model for local and global image features.\nThe DELG model leverages a fully-convolutional neural network with two different heads: one for global features and the other for local features. Global features are obtained using pooled feature maps of deep network layers, which in effect summarize the salient features of the input images making the model more robust to subtle changes in input. The local feature branch leverages intermediate feature maps to detect salient image regions, with the help of an attention module, and to produce descriptors that represent associated localized contents in a discriminative manner.convolutional neural networkwith the help of an attention module\nThis novel design allows for efficient inference since it enables extraction of global and local features within a single model. For the first time, we demonstrated that such a unified model can be trained end-to-end and deliver state-of-the-art results for instance-level recognition tasks. When compared to previous global features, this method outperforms other approaches by up to 7.5% mean average precision; and for the local feature re-ranking stage, DELG-based results are up to 7% better than previous work. Overall, DELG achieves 61.2% average precision on the recognition task of GLDv2, which outperforms all except two methods of the 2019 challenge. Note that all top methods from that challenge used complex model ensembles, while our results use only a single model.2019 challenge\nTensorflow 2 Open-Source Codebase\nTo foster research reproducibility, we are also releasing a revamped open-source codebase that includes DELG and other techniques relevant to instance-level recognition, such as DELF and Detect-to-Retrieve. Our code adopts the latest Tensorflow 2 releases, and makes available reference implementations for model training & inference, besides image retrieval and matching functionalities. We invite the community to use and contribute to this codebase in order to develop strong foundations for research in the ILR field.open-source codebaseDELFDetect-to-RetrieveTensorflow 2contribute\nNew Challenges for Instance Level Recognition\nFocused on the landmarks domain, the Google Landmarks Dataset v2 (GLDv2) is the largest available dataset for instance-level recognition, with 5 million images spanning 200 thousand categories. By training landmark retrieval models on this dataset, we have demonstrated improvements of up to 6% mean average precision, compared to models trained on earlier datasets. We have also recently launched a new browser interface for visually exploring the GLDv2 dataset.Google Landmarks Dataset v2we have demonstratednew browser interface\nThis year, we also launched two new challenges within the landmark domain, one focusing on recognition and the other on retrieval. These competitions feature newly-collected test sets, and a new evaluation methodology: instead of uploading a CSV file with pre-computed predictions, participants have to submit models and code that are run on Kaggle servers, to compute predictions that are then scored and ranked. The compute restrictions of this environment put an emphasis on efficient and practical solutions.recognitionretrieval\nThe challenges attracted over 1,200 teams, a 3x increase over last year, and participants achieved significant improvements over our strong DELG baselines. On the recognition task, the highest scoring submission achieved a relative increase of 43% average precision score and on the retrieval task, the winning team achieved a 59% relative improvement of the mean average precision score. This latter result was achieved via a combination of more effective neural networks, pooling methods and training protocols (see more details on the Kaggle competition site).on the Kaggle competition site\nIn addition to the landmark recognition and retrieval challenges, our academic and industrial collaborators discussed their progress on developing benchmarks and competitions in other domains. A large-scale research benchmark for artwork recognition is under construction, leveraging The Met\u2019s Open Access image collection, and with a new test set consisting of guest photos exhibiting various photometric and geometric variations. Similarly, a new large-scale product retrieval competition will capture various challenging aspects, including a very large number of products, a long-tailed class distribution and variations in object appearance and context. More information on the ILR workshop, including slides and video recordings, is available on its website.The MetOpen Access image collectionwebsite\nWith this research, open source code, data and challenges, we hope to spur progress in instance-level recognition and enable researchers and machine learning enthusiasts from different communities to develop approaches that generalize across different domains.\nAcknowledgements\nThe main Google contributors of this project are Andr\u00e9 Araujo, Cam Askew, Bingyi Cao, Jack Sim and Tobias Weyand. We\u2019d like to thank the co-organizers of the ILR workshop Ondrej Chum, Torsten Sattler, Giorgos Tolias (Czech Technical University), Bohyung Han (Seoul National University), Guangxing Han (Columbia University), Xu Zhang (Amazon), collaborators on the artworks dataset Nanne van Noord, Sarah Ibrahimi (University of Amsterdam), Noa Garcia (Osaka University), as well as our collaborators from the Metropolitan Museum of Art: Jennie Choi, Maria Kessler and Spencer Kiser. For the open-source Tensorflow codebase, we\u2019d like to thank the help of recent contributors: Dan Anghel, Barbara Fusinska, Arun Mukundan, Yuewei Na and Jaeyoun Kim. We are grateful to Will Cukierski, Phil Culliton, Maggie Demkin for their support with the landmarks Kaggle competitions. Also we\u2019d like to thank Ralph Keller and Boris Bluntschli for their help with data collection.Czech Technical UniversitySeoul National UniversityColumbia UniversityAmazonUniversity of AmsterdamOsaka UniversityMetropolitan Museum of Art",
      "link": "http://ai.googleblog.com/2020/09/advancing-instance-level-recognition.html",
      "author": "Posted by Cam Askew and Andr\u00e9 Araujo, Software Engineers, Google Research"
    },
    {
      "title": "Advancing NLP with Efficient Projection-Based Model Architectures",
      "date": "Monday, September 21, 2020",
      "abstract": "Advancing NLP with Efficient Projection-Based Model Architectures\nDeep neural networks have radically transformed natural language processing (NLP) in the last decade, primarily through their application in data centers using specialized hardware. However, issues such as preserving user privacy, eliminating network latency, enabling offline functionality, and reducing operation costs have rapidly spurred the development of NLP models that can be run on-device rather than in data centers. Yet mobile devices have limited memory and processing power, which requires models running on them to be small and efficient \u2014 without compromising quality.\nLast year, we published a neural architecture called PRADO, which at the time achieved state-of-the-art performance on many text classification problems, using a model with less than 200K parameters. While most models use a fixed number of parameters per token, the PRADO model used a network structure that required extremely few parameters to learn the most relevant or useful tokens for the task.PRADO\nToday we describe a new extension to the model, called pQRNN, which advances the state of the art for NLP performance with a minimal model size. The novelty of pQRNN is in how it combines a simple projection operation with a quasi-RNN encoder for fast, parallel processing. We show that the pQRNN model is able to achieve BERT-level performance on a text classification task with orders of magnitude fewer number of parameters.quasi-RNNBERT\nWhat Makes PRADO Work?\nWhen developed a year ago, PRADO exploited NLP domain-specific knowledge on text segmentation to reduce the model size and improve the performance. Normally, the text input to NLP models is first processed into a form that is suitable for the neural network, by segmenting text into pieces (tokens) that correspond to values in a predefined universal dictionary (a list of all possible tokens). The neural network then uniquely identifies each segment using a trainable parameter vector, which comprises the embedding table. However, the way in which text is segmented has a significant impact on the model performance, size, and latency. The figure below shows the spectrum of approaches used by the NLP community and their pros and cons.universal dictionary\nSince the number of text segments is such an important parameter for model performance and compression, it raises the question of whether or not an NLP model needs to be able to distinctly identify every possible text segment. To answer this question we look at the inherent complexity of NLP tasks.\nOnly a few NLP tasks (e.g., language models and machine translation) need to know subtle differences between text segments and thus need to be capable of uniquely identifying all possible text segments. In contrast, the majority of other tasks can be solved by knowing a small subset of these segments. Furthermore, this subset of task-relevant segments will likely not be the most frequent, as a significant fraction of segments will undoubtedly be dedicated to articles, such as a, an, the, etc., which for many tasks are not necessarily critical. Hence, allowing the network to determine the most relevant segments for a given task results in better performance. In addition, the network does not need to be able to uniquely identify these segments, but only needs to recognize clusters of text segments. For example, a sentiment classifier just needs to know segment clusters that are strongly correlated to the sentiment in the text.sentiment classifier\nLeveraging these insights, PRADO was designed to learn clusters of text segments from words rather than word pieces or characters, which enabled it to achieve good performance on low-complexity NLP tasks. Since word units are more meaningful, and yet the most relevant words for most tasks are reasonably small, many fewer model parameters are needed to learn such a reduced subset of relevant word clusters.\nImproving PRADO\nBuilding on the success of PRADO, we developed an improved NLP model, called pQRNN. This model is composed of three building blocks, a projection operator that converts tokens in text to a sequence of ternary vectors, a dense bottleneck layer and a stack of QRNN encoders.\nThe implementation of the projection layer in pQRNN is identical to that used in PRADO and helps the model learn the most relevant tokens without a fixed set of parameters to define them. It first fingerprints the tokens in the text and converts it to a ternary feature vector using a simple mapping function. This results in a ternary vector sequence with a balanced symmetric distribution that uniquely represents the text. This representation is not directly useful since it does not have any information needed to solve the task of interest and the network has no control over this representation. We combine it with a dense bottleneck layer to allow the network to learn a per word representation that is relevant for the task at hand. The representation resulting from the bottleneck layer still does not take the context of the word into account. We learn a contextual representation by using a stack of bidirectional QRNN encoders. The result is a network that is capable of learning a contextual representation from just text input without employing any kind of preprocessing.\nPerformance\nWe evaluated pQRNN on the civil_comments dataset and compared it with the BERT model on the same task. Simply because the model size is proportional to the number of parameters, pQRNN is much smaller than BERT. But in addition, pQRNN is quantized, further reducing the model size by a factor of 4x. The public pretrained version of BERT performed poorly on the task hence the comparison is done to a BERT version that is pretrained on several different relevant multilingual data sources to achieve the best possible performance.civil_commentsBERTquantizedarea under the curve\nConclusion\nUsing our previous generation model PRADO, we have demonstrated how it can be used as the foundation for the next generation of state-of-the-art light-weight text classification models. We present one such model, pQRNN, and show that this new architecture can nearly achieve BERT-level performance, despite being 300x smaller and being trained on only supervised data. To stimulate further research in this area, we have open-sourced the PRADO model and encourage the community to use it as a jumping off point for new model architectures.open-sourced the PRADO model\nAcknowledgements\nWe thank Yicheng Fan, M\u00e1rius \u0160ajgal\u00edk, Peter Young and Arun Kandoor for contributing to the open sourcing effort and helping improve the models. We would also like to thank Amarnag Subramanya, Ashwini Venkatesh, Benoit Jacob, Catherine Wah, Dana Movshovitz-Attias, Dang Hien, Dmitry Kalenichenko, Edgar Gonz\u00e0lez i Pellicer, Edward Li, Erik Vee, Evgeny Livshits, Gaurav Nemade, Jeffrey Soren, Jeongwoo Ko, Julia Proskurnia, Rushin Shah, Shirin Badiezadegan, Sidharth KV, Victor C\u0103rbune and the Learn2Compress team for their support. We would like to thank Andrew Tomkins and Patrick Mcgregor for sponsoring this research project.Learn2Compress",
      "link": "http://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html",
      "author": "Posted by Prabhu Kaliamoorthi, Software Engineer, Google Research"
    },
    {
      "title": "Improving the Accuracy of Genomic Analysis with DeepVariant 1.0",
      "date": "Friday, September 18, 2020",
      "abstract": "Improving the Accuracy of Genomic Analysis with DeepVariant 1.0\nSequencing genomes involves sampling short pieces of the DNA from the ~6 billion pairs of nucleobases \u2014 i.e., adenine (A), thymine (T), guanine (G), and cytosine (C) \u2014 we inherit from our parents. Genome sequencing is enabled by two key technologies: DNA sequencers (hardware) that \"read\" relatively small fragments of DNA, and variant callers (software) that combine the reads to identify where and how an individual's genome differs from a reference genome, like the one assembled in the Human Genome Project. Such variants may be indicators of genetic disorders, such as an elevated risk for breast cancer, pulmonary arterial hypertension, or neurodevelopmental disorders.Human Genome Projectelevated risk for breast cancerpulmonary arterial hypertensionneurodevelopmental disorders\nIn 2017, we released DeepVariant, an open-source tool which identifies genome variants in sequencing data using a convolutional neural network (CNN). The sequencing process begins with a physical sample being sequenced by any of a handful of instruments, depending on the end goal of the sequencing. The raw data, which consists of numerous reads of overlapping fragments of the genome, are then mapped to a reference genome. DeepVariant analyzes these mappings to identify variant locations and distinguish them from sequencing errors.we released DeepVariantconvolutional neural network\nSoon after it was first published in 2018, DeepVariant underwent a number of updates and improvements, including significant changes to improve accuracy for whole exome sequencing and polymerase chain reaction (PCR) sequencing.publishedupdates and improvementsexome sequencingpolymerase chain reaction\nWe are now releasing DeepVariant v1.0, which incorporates a large number of improvements for all sequencing types. DeepVariant v1.0 is an improved version of our submission to the PrecisionFDA v2 Truth Challenge, which achieved Best Overall accuracy for 3 of 4 instrument categories. Compared to previous state-of-the-art models, DeepVariant v1.0 significantly reduces the errors for widely-used sequencing data types, including Illumina and Pacific Biosciences. In addition, through a collaboration with the UCSC Genomics Institute, we have also released a model that combines DeepVariant with the UCSC\u2019s PEPPER method, called PEPPER-DeepVariant, which extends coverage to Oxford Nanopore data for the first time.DeepVariant v1.0PrecisionFDA v2 Truth ChallengeIlluminaPacific BiosciencesUCSC Genomics Institute,PEPPERPEPPER-DeepVariantOxford Nanopore\nSequencing Technologies and DeepVariant\nFor the last decade, the majority of sequence data were generated using Illumina instruments, which produce short (75-250 bases) and accurate sequences. In recent years, new technologies have become available that can sequence much longer pieces, including Pacific Biosciences, which can produce long and accurate sequences up to ~15,000 bases in length, and Oxford Nanopore, which can produce reads up to 1 million bases long, but with higher error rates. The particular type of sequencing data a researcher might use depends on the ultimate use-case.IlluminaPacific Bioscienceslong and accurate sequences1 million bases long, but with higher error rates\nBecause DeepVariant is a deep learning method, we can quickly re-train it for these new instrument types, ensuring highly accurate sequence identification. Accuracy is important because a missed variant call could mean missing the causal variant for a disorder, while a false positive variant call could lead to identifying an incorrect one. Earlier\u00a0state-of-the-art methods could reach ~99.1% accuracy (~73,000 errors) on a 35-fold coverage Illumina whole genome, whereas an early version of DeepVariant (v0.10) had ~99.4% accuracy (46,000 errors), corresponding to a 38% error reduction. DeepVariant v1.0 reduces Illumina errors by another ~22% and PacBio errors by another ~52% relative to the last DeepVariant release (v0.10).\nDeepVariant Overview\nDeepVariant is a convolutional neural network (CNN) that treats the task of identifying genetic variants as an image classification problem. DeepVariant constructs tensors, essentially multi-channel images, where each channel represents an aspect of the sequence, such as the bases in the sequence (called read base), the quality of alignment between different reads (mapping quality), whether a given read supports an alternate allele (read supports variant),  etc. It then analyzes these data and outputs three genotype likelihoods, corresponding to how many copies (0, 1, or 2) of a given alternate allele are present.allele\nTechnical Improvements in DeepVariant v1.0\nBecause DeepVariant uses the same codebase for each data type, improvements apply to each of Illumina, PacBio, and Oxford Nanopore. Below, we show the numbers for Illumina and PacBio for two types of small variants: SNPs (single nucleotide polymorphisms, which change a single base without changing sequence length) and INDELs (insertions and deletions).SNPsINDELs\nThe Genome in a Bottle consortium from the National Institute of Standards and Technology (NIST) creates gold-standard samples with known variants covering the regions of the genome. These are used as labels to train DeepVariant. Using long-read technologies the Genome in a Bottle expanded the set of confident variants, increasing the regions described by the standard set from 85% of the genome to 92% of it. These more difficult regions were already used in training the PacBio models, and including them in the Illumina models reduced errors by 11%. By relaxing the filter for reads of lower mapping quality, we further reduced errors by 4% for Illumina and 13% for PacBio.Genome in a BottleNational Institute of Standards and Technologyexpanded the set of confident variants\nWe inherit one copy of DNA from our mother and another from our father. PacBio and Oxford Nanopore sequences are long enough to separate sequences by parental origin, which is called a haplotype. By providing this information to the neural network, DeepVariant improves its identification of random sequence errors and can better determine whether a variant has a copy from one or both parents.\nDeepVariant uses input sequence fragments that have been aligned to a reference genome. The optimal alignment for variants that include insertions or deletions could be different if the aligner knew they were present. To capture this information, we implemented an additional alignment step relative to the candidate variant.  The figure below shows an additional second row where the reads are aligned to the candidate variant, which is a large insertion. You can see sequences that abruptly stop in the first row can now be fully aligned, providing additional information.\nVariants can have multiple alleles, with a different base inherited from each parent. DeepVariant\u2019s classifier only generates a probability for one potential variant at a time. In previous versions, simple hand-written rules converted the probabilities into a composite call, but these rules failed in some edge cases. In addition, it also separated the way a final call was made from the backpropagation to train the network. By adding a small, fully-connected neural network to the post-processing step, we are able to better handle these tricky multi-allelic cases.\nThe timeframe for the competition was compressed, so we trained only with data similar to the challenge data (PCR-Free NovaSeq) to speed model training. In our production releases, we seek high accuracy for multiple instruments as well as PCR+ preparations. Training with data from these diverse classes helps the model generalize, so our DeepVariant v1.0 release model outperforms the one submitted.\nThe charts below show the error reduction achieved by each improvement.\nTraining a Hybrid model\nDeepVariant v1.0 also includes a hybrid model for PacBio and Illumina reads. In this case, the model leverages the strengths of both input types, without needing new logic.\nWe observed no change in SNP errors, suggesting that PacBio reads are strictly superior for SNP calling. We observed a further 49% reduction in Indel errors relative to the PacBio model, suggesting that the Indel error modes of Illumina and PacBio HiFi can be used in a complementary manner.PEPPER-Deepvariant: A Pipeline for Oxford Nanopore Data Using DeepVariant\nUntil the PrecisionFDA competition, a DeepVariant model was not available for Oxford Nanopore data, because the higher base error rate created too many candidates for DeepVariant to classify. We partnered with the UC Santa Cruz Genomics Institute, which has extensive expertise with Nanopore data. They had previously trained a deep learning method called PEPPER, which could narrow down the candidates to a more tractable number. The larger neural network of DeepVariant can then accurately characterize the remaining candidates with a reasonable runtime.UC Santa Cruz Genomics InstitutePEPPER\nThe combined PEPPER-DeepVariant pipeline with the Oxford Nanopore model is open-source and available on GitHub. This pipeline was able to achieve a superior SNP calling accuracy to DeepVariant Illumina on the PrecisionFDA challenge, which is the first time anyone has shown Nanopore outperforming Illumina in this way.available on GitHub\nConclusion\nDeepVariant v1.0 isn\u2019t the end of development. We look forward to working with the genomics community to further maximize the value of genomic data to patients and researchers.",
      "link": "http://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html",
      "author": "Posted by Andrew Carroll, Product Lead and Pi-Chuan Chang, Technical Lead, Google Health"
    },
    {
      "title": "Improving Sparse Training with RigL",
      "date": "Wednesday, September 16, 2020",
      "abstract": "Improving Sparse Training with RigL\nModern deep neural network architectures are often highly redundant [1, 2, 3], making it possible to remove a significant fraction of connections without harming performance. The sparse neural networks that result have been shown to be more parameter and compute efficient compared to dense networks, and, in many cases, can significantly decrease wall clock inference times.123parametercompute\nBy far the most popular method for training sparse neural networks is pruning, (dense-to-sparse training) which usually requires first training a dense model, and then \u201csparsifying\u201d it by cutting out the connections with negligible weights. However, this process has two limitations.pruning inferiorperformance\nIn \u201cRigging the Lottery: Making All Tickets Winners\u201d, presented at ICML 2020, we introduce RigL, an algorithm for training sparse neural networks that uses a fixed parameter count and computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The algorithm identifies which neurons should be active during training, which helps the optimization process to utilize the most relevant connections and results in better sparse solutions. An example of this is shown below, where, during the training of a multilayer perceptron (MLP) network on MNIST, our sparse network trained with RigL learns to focus on the center of the images, discarding the uninformative pixels from the edges. A Tensorflow implementation of our method along with three other baselines (SET, SNFS, SNIP) can be found at github.com/google-research/rigl.Rigging the Lottery: Making All Tickets WinnersICML 2020multilayer perceptronMNISTTensorflowSETSNFSSNIPgithub.com/google-research/rigl\nRigL Overview\nThe RigL method starts with a network initialized with a random sparse topology. At regularly spaced intervals we remove a fraction of the connections with the smallest weight magnitudes. Such a strategy has been shown to have very little effect on the loss. RigL then activates new connections using instantaneous gradient information, i.e., without using past gradient information. After updating the connectivity, training continues with the updated network until the next scheduled update. Next, the system activates connections with large gradients, since these connections are expected to decrease the loss most quickly.has been shown\nEvaluating Performance\nBy changing the connectivity of the neurons dynamically during training, RigL helps optimize to find better solutions. To demonstrate this, we restart training from a bad solution that exhibits poor accuracy and show that RigL's mask updates help the optimization achieve better loss compared to static training, in which connectivity of the sparse network remains the same.\nThe figure below summarizes the performance of various methods on training an 80% sparse ResNet-50 architecture. We compare RigL with two recent sparse training methods, SET and SNFS and three baseline training methods: Static, Small-Dense and Pruning. Two of these methods (SNFS and Pruning) require dense resources as they need to either train a large network or store the gradients of it. Overall, we observe that the performance of all methods improves with additional training time; thus, for each method we run extended training with up to 5x the training steps of the original 100 epochs.ResNet-50SETSNFS\nAs noted in a number of studies [4, 5, 6, 7], training a network with fixed sparsity from scratch (Static) leads to inferior performance compared to solutions found by pruning. Training a small, dense network (Small-Dense) with the same number of parameters gets better results than Static, but fails to match the performance of dynamic sparse models. Similarly, SET improves the performance over Small-Dense, but saturates at around 75% accuracy, revealing the limits of growing new connections randomly. Methods that use gradient information to grow new connections (RigL and SNFS) obtain higher accuracy in general, but RigL achieves the highest accuracy, while also consistently requiring fewer FLOPs (and memory footprint) than the other methods.4567\nObserving the trend between extended training and performance, we compare the results using longer training runs. Within the interval considered (i.e., 1x-100x) RigL's performance constantly improves with additional training. RigL achieves state of art performance of 68.07% Top-1 accuracy at training with a 99% sparse ResNet-50 architecture. Similarly extended training of a 90% sparse MobileNet-v1 architecture with RigL achieves 70.55% Top-1 accuracy. Obtaining the same results with fewer training iterations is an exciting future research direction.MobileNet-v1\nOther experiments include image classification on CIFAR-10 datasets and character-based language modelling using RNNs with the WikiText-103 dataset and can be found in the full paper.CIFAR-10WikiText-103 datasetfull paper\nFuture Work\nRigL is useful in three different scenarios:89101112\nAcknowledgements\nWe would like to thank Eleni Triantafillou, Hugo Larochelle, Bart van Merrienboer, Fabian Pedregosa, Joan Puigcerver, Danny Tarlow, Nicolas Le Roux, Karen Simonyan for giving feedback on the preprint of the paper; Namhoon Lee for helping us verify and debug our SNIP implementation; Chris Jones for helping us discover and solve the distributed training bug; and Tom Small for creating the visualization of the algorithm.",
      "link": "http://ai.googleblog.com/2020/09/improving-sparse-training-with-rigl.html",
      "author": "Posted by Utku Evci and Pablo Samuel Castro, Research Engineers, Google Research, Montreal"
    },
    {
      "title": "Imitation Learning in the Low-Data Regime",
      "date": "Tuesday, September 15, 2020",
      "abstract": "Imitation Learning in the Low-Data Regime\nReinforcement Learning (RL) is a paradigm for using trial-and-error to train agents to sequentially make decisions in complex environments, which has had great success in a number of domains, including games, robotics manipulation and chip design. Agents typically aim at maximizing the sum of the reward they collect in an environment, which can be based on a variety of parameters, including speed, curiosity, aesthetics and more. However, designing a specific RL reward function is a challenge since it can be hard to specify or too sparse. In such cases, imitation learning (IL) methods offer an alternative as they learn how to solve a task from expert demonstrations, rather than a carefully designed reward function. However, state-of-the-art IL methods rely on adversarial training, which uses min/max optimization procedures, making them algorithmically unstable and difficult to deploy.gamesrobotics manipulationchip designcuriosityimitation learningadversarial training\nIn \u201cPrimal Wasserstein Imitation Learning\u201d (PWIL), we introduce a new IL method, based on the primal form of the Wasserstein distance, also known as the earth mover\u2019s distance, which does not rely on adversarial training. Using the MuJoCo suite of tasks, we demonstrate the efficacy of the PWIL method by imitating a simulated expert with a limited number of demonstrations (even a single example) and limited interactions with the environment.Primal Wasserstein Imitation LearningprimalWasserstein distanceMuJoCo suite of tasksHumanoid\nAdversarial Imitation Learning\nState-of-the-art adversarial IL methods operate similarly to generative adversarial networks (GANs) in which a generator (the policy) is trained to maximize the confusion of a discriminator (the reward) that itself is trained to differentiate between the agent\u2019s state-action pairs and the expert\u2019s. Adversarial IL methods boil down to a distribution matching problem, i.e., the problem of minimizing a distance between probability distributions in a\u00a0metric space. However, just as GANs, adversarial IL methods rely on a min/max optimization problem and hence come with a number of training stability challenges.generative adversarial networksdistribution matching problemmetric spacenumber of training stability challenges\nImitation Learning as Distribution Matching\nThe PWIL method is based on the formulation of IL as a distribution matching problem, in this case, the Wasserstein distance. The first step consists of inferring from the demonstrations a state-action distribution of the expert, the collection of relationships between the actions taken by the expert and the corresponding state of the environment. The goal is then to minimize the distance between the agent\u2019s and the expert\u2019s state-action distributions, through interactions with the environment. In contrast, PWIL is a non-adversarial method, enabling it to bypass the min/max optimization problem and directly minimize the Wasserstein distance between the agent\u2019s and the expert\u2019s state-action pair distributions.\nPrimal Wasserstein Imitation Learning\nComputing the exact Wasserstein distance can be restrictive since one must wait until the end of a trajectory of the agent to calculate it, meaning that the rewards can be computed only when the agent is done interacting with the environment. To avoid this restriction, we use an upper bound on the distance instead, from which we can define a reward that we optimize using RL. We show that by doing so, we indeed recover expert behaviour and minimize the Wasserstein distance between the agent and the expert on a number of locomotion tasks of the MuJoCo simulator. While adversarial IL methods use a reward function from a neural network that must be optimized and re-estimated continuously as the agent interacts with the environment, PWIL defines a reward function offline from demonstrations, which does not change and is based on substantially fewer hyperparameters than adversarial IL approaches.Humanoid\nA Measure of Similarity for the True Imitation Learning Setting\nAs in numerous challenges in ML, a number of IL methods are evaluated on synthetic tasks, where one usually has access to the underlying reward function of the task and can measure similarity between the expert\u2019s and the agent\u2019s behaviour in terms of performance, which is the expected sum of rewards. A byproduct of PWIL is the creation of a metric that can compare expert behavior to an agent\u2019s behavior for any IL method, without access to the true reward of the task. In this sense, we can use the Wasserstein distance in the true IL setting, not only on synthetic tasks.\nConclusion\nIn environments where interacting is costly (e.g., a real robot or a complex simulator), PWIL is a prime candidate not only because it can recover expert behaviour, but also because the reward function it defines is easy to tune and is defined without interactions with the environment. This opens multiple opportunities for future exploration, including deployment to real systems, extending PWIL to the setup where we have only access to demonstration states (rather than states and actions), and finally applying PWIL to visual based observations.\nAcknowledgements\nWe thank our co-authors, Matthieu Geist and Olivier Pietquin; as well as Zafarali Ahmed, Adrien Ali Ta\u00efga, Gabriel Dulac-Arnold, Johan Ferret, Alexis Jacq and Saurabh Kumar for their feedback on the manuscript.",
      "link": "http://ai.googleblog.com/2020/09/imitation-learning-in-low-data-regime.html",
      "author": "Posted by Robert Dadashi, Research Software Engineer and L\u00e9onard Hussenot, Student Researcher, Google Research"
    },
    {
      "title": "The Technology Behind our Recent Improvements in Flood Forecasting",
      "date": "Thursday, September 3, 2020",
      "abstract": "The Technology Behind our Recent Improvements in Flood Forecasting\nFlooding is the most common natural disaster on the planet, affecting the lives of hundreds of millions of people around the globe and causing around $10 billion in damages each year. Building on our work in previous years, earlier this week we announced some of our recent efforts to improve flood forecasting in India and Bangladesh, expanding coverage to  more than 250 million people, and providing unprecedented lead time, accuracy and clarity.our work in previous yearsimprove flood forecasting in India and BangladeshTo enable these breakthroughs, we have devised a new approach for inundation modeling, called a morphological inundation model, which combines physics-based modeling with machine learning (ML) to create more accurate and scalable inundation models in real-world settings. Additionally, our new alert-targeting model allows identifying areas at risk of flooding at unprecedented scale using end-to-end machine learning models and data that is publicly available globally. In this post, we also describe developments for the next generation of flood forecasting systems, called\u00a0HydroNets (presented at ICLR AI for Earth Sciences and EGU this year), which is a new architecture specially built for hydrologic modeling across multiple basins, while still optimizing for accuracy at each location.HydroNetsICLR AI for Earth SciencesEGU\nForecasting Water Levels\nThe first step in a flood forecasting system is to identify whether a river is expected to flood. Hydrologic models (or gauge-to-gauge models) have long been used by governments and disaster management agencies to improve the accuracy and extend the lead time of their forecasts. These models receive inputs like precipitation or upstream gauge measurements of water level (i.e., the absolute elevation of the water above sea level) and output a forecast for the water level (or discharge) in the river at some time in the future.Hydrologic modelsgauge measurements\nThe hydrologic model component of the\u00a0flood forecasting system described in this week\u2019s Keyword post doubled the lead time of flood alerts for areas covering more than 75 million people. These models not only increase lead time, but also provide unprecedented accuracy, achieving an R2 score of more than 99% across all basins we cover, and predicting the water level within a 15 cm error bound more than 90% of the time.\u00a0Once a river is predicted to reach flood level, the next step in generating actionable warnings is to convert the river level forecast into a prediction for how the floodplain will be affected.Keyword postR2 score\nMorphological Inundation ModelingIn prior work, we developed high quality elevation maps based on satellite imagery, and ran physics-based models to simulate water flow across these digital terrains, which allowed  warnings with unprecedented resolution and accuracy in data-scarce regions. In collaboration with our satellite partners, Airbus, Maxar and Planet, we have now expanded the elevation maps to cover hundreds of millions of square kilometers. However, in order to scale up the coverage to such a large area while still retaining high accuracy, we had to re-invent how we develop inundation models.In prior workunprecedented resolution and accuracy in data-scarce regionsAirbusMaxarPlanet\nInundation modeling at scale suffers from three significant challenges. Due to the large areas involved and the resolution required for such models, they necessarily have high computational complexity. In addition, most global elevation maps don\u2019t include riverbed bathymetry, which is important for accurate modeling. Finally, the errors in existing data, which may include gauge measurement errors, missing features in the elevation maps, and the like, need to be understood and corrected. Correcting such problems may require collecting additional high-quality data or fixing erroneous data manually, neither of which scale well.\nOur new approach to inundation modeling, which we call a morphological model, addresses these issues by using several innovative tricks. Instead of modeling the complex behaviors of water flow in real time, we compute modifications to the morphology of the elevation map that allow one to simulate the inundation using simple physical principles, such as those describing hydrostatic systems.hydrostatic systems\nFirst, we train a pure-ML model (devoid of physics-based information) to estimate the one-dimensional river profile from gauge measurements. The model takes as input the water level at a specific point on the river (the stream gauge) and outputs the river profile, which is the water level at all points in the river. We assume that if the gauge increases, the water level increases monotonically, i.e., the water level at other points in the river increases as well. We also assume that the absolute elevation of the river profile decreases downstream (i.e., the river flows downhill).\nWe then use this learned model and some heuristics to edit the elevation map to approximately \u201ccancel out\u201d the pressure gradient that would exist if that region were flooded. This new synthetic elevation map provides the foundation on which we model the flood behavior using a simple flood-fill algorithm. Finally, we match the resulting flooded map to the satellite-based flood extent with the original stream gauge measurement.flood-fill algorithm\nThis approach abandons some of the realistic constraints of classical physics-based models, but in data scarce regions where existing methods currently struggle, its flexibility allows the model to automatically learn the correct bathymetry and fix various errors to which physics-based models are sensitive. This morphological model improves accuracy by 3%, which can significantly improve forecasts for large areas, while also allowing for much more rapid model development by reducing the need for manual modeling and correction.physics-based models\nAlert targeting\nMany people reside in areas that are not covered by the morphological inundation models, yet access to accurate predictions are still urgently needed. To reach this population and to increase the impact of our flood forecasting models, we designed an end-to-end ML-based approach, using almost exclusively data that is globally publicly available, such as stream gauge measurements, public satellite imagery, and low resolution elevation maps. We train the model to use the data it is receiving to directly infer the inundation map in real time.\nThis approach works well \u201cout of the box\u201d when the model only needs to forecast an event that is within the range of events previously observed. Extrapolating to more extreme conditions is much more challenging. Nevertheless, proper use of existing elevation maps and real-time measurements can enable alerts that are more accurate than presently available for those in areas not covered by the more detailed morphological inundation models. Because this model is highly scalable, we were able to launch it across India after only a few months of work, and we hope to roll it out to many more countries soon.\nImproving Water Levels Forecasting\nIn an effort to continue improving flood forecasting, we have developed HydroNets \u2014 a specialized deep neural network architecture built specifically for water levels forecasting \u2014 which allows us utilize some exciting recent advances in ML-based hydrology in a real-world operational setting. Two prominent features distinguish it from standard hydrologic models. First, it is able to differentiate between model components that generalize well between sites, such as the modeling of rainfall-runoff processes, and those that are specific to a given site, like the rating curve, which converts a predicted discharge volume into an expected water level. This enables the model to generalize well to different sites, while still fine-tuning its performance to each location. Second, HydroNets takes into account the structure of the river network being modeled, by training a large architecture that is actually a web of smaller neural networks, each representing a different location along the river. This allows neural networks that are modeling upstream sites to pass information encoded in embeddings to models of downstream sites, so that every model can know everything it needs without a drastic increase in parameters.HydroNetsrecent advancesrating curve\nThe animation below illustrates the structure and flow of information in HydroNets. The output from the modeling of upstream sub-basins is combined into a single representation of a given basin state. It is then processed by the shared model component, which is informed by all basins in the network, and passed on to the label prediction model, which calculates the water level (and the loss function). The output from this iteration of the network is then passed on to inform downstream models, and so on.\nWe\u2019re incredibly excited about this progress, and are working hard on improving our systems further.\nAcknowledgementsThis work is a collaboration between the Google Flood Forecasting Initiative, the Google Geo and Crisis Response teams, Google.org and many other research teams at Google, and is part of our AI for Social Good efforts. We would also like to thank the Partnerships and Policy teams.Crisis ResponseGoogle.orgAI for Social Good",
      "link": "http://ai.googleblog.com/2020/09/the-technology-behind-our-recent.html",
      "author": "Posted by Sella Nevo, Senior Software Engineer, Google Research, Tel Aviv"
    },
    {
      "title": "KeyPose: Estimating the 3D Pose of Transparent Objects from Stereo",
      "date": "Wednesday, September 2, 2020",
      "abstract": "KeyPose: Estimating the 3D Pose of Transparent Objects from Stereo\nEstimating the position and orientation of 3D objects is one of the core problems in computer vision applications that involve object-level perception, such as augmented reality and robotic manipulation. In these applications, it is important to know the 3D position of objects in the world, either to directly affect them, or to place simulated objects correctly around them. While there has been much research on this topic using machine learning (ML) techniques, especially Deep Nets, most have relied on the use of depth sensing devices, such as the Kinect, which give direct measurements of the distance to an object. For objects that are shiny or transparent, direct depth sensing does not work well. For example, the figure below includes a number of objects (left), two of which are transparent stars. A depth device does not find good depth values for the stars, and gives a very poor reconstruction of the actual 3D points (right).augmented realityrobotic manipulationDeep NetsKinect\nOne solution to this problem, such as that proposed by ClearGrasp, is to use a deep neural network to inpaint the corrupted depth map of the transparent objects. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries, which it uses to refine the initial depth estimates for all transparent surfaces in the scene (far right\u00a0in the figure above). This approach is very promising, and allows scenes with transparent objects to be processed by pose-estimation methods that rely on depth.\u00a0 But inpainting can be tricky, especially when trained completely with synthetic images, and can still result in errors in depth.ClearGraspinpaint\nIn \u201cKeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent Objects\u201d, presented at CVPR 2020 in collaboration with the Stanford AI Lab, we describe an ML system that estimates the depth of transparent objects by directly predicting 3D keypoints. To train the system we gather a large real-world dataset of images of transparent objects in a semi-automated way, and efficiently label their pose using 3D keypoints selected by hand. We then train deep models (called KeyPose) to estimate the 3D keypoints end-to-end from monocular or stereo images, without explicitly computing depth. The models work on objects both seen and unseen during training, for both individual objects and categories of objects. While KeyPose can work with monocular images, the extra information available from stereo images allows it to improve its results by a factor of two over monocular image input, with typical errors from 5 mm to 10 mm, depending on the objects. It substantially improves over state-of-the-art in pose estimation for these objects, even when competing methods are provided with ground truth depth. We are releasing the dataset of keypoint-labeled transparent objects for use by the research community.KeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent ObjectsCVPR 2020Stanford AI Labdataset of keypoint-labeled transparent objects\nReal-World Transparent Object Dataset with 3D Keypoint Labels\nTo facilitate gathering large quantities of real-world images, we set up a robotic data-gathering system in which a robot arm moves through a trajectory while taking video with two devices, a stereo camera and the Kinect Azure depth camera.Kinect AzureAzure Kinect\nThe AprilTags on the target enable accurate tracing of the pose of the cameras. By hand-labelling only a few images in each video with 2D keypoints, we can extract 3D keypoints for all frames of the video using multi-view geometry, thus increasing the labelling efficiency by a factor of 100.AprilTags\nWe captured imagery for 15 different transparent objects in five categories, using 10 different background textures and four different poses for each object, yielding a total of 600 video sequences comprising 48k stereo and depth images. We also captured the same images with an opaque version of the object, to provide accurate ground truth depth images. All the images are labelled with 3D keypoints. We are releasing this dataset of real-world images publicly, complementing the synthetic ClearGrasp dataset with which it shares similar objects.dataset of real-world images\nKeyPose Algorithm Using Early Fusion Stereo\nThe idea of using stereo images directly for keypoint estimation was developed independently for this project; it has also appeared recently in the context of hand-tracking. The diagram below shows the basic idea: the two images from a stereo camera are cropped around the object and fed to the KeyPose network, which predicts a sparse set of 3D keypoints that represent the 3D pose of the object. The network is trained using supervision from the labelled 3D keypoints.appeared recently\nOne of the key aspects of stereo KeyPose is the use of early fusion to intermix the stereo images, and allow the network to implicitly compute disparity, in contrast to late fusion, in which keypoints are predicted for each image separately, and then combined. As shown in the diagram below, the output of KeyPose is a 2D keypoint heatmap in the image plane along with a disparity (i.e., inverse depth) heatmap for each keypoint. The combination of these two heatmaps yields the 3D coordinate of the keypoint, for each keypoint.\nWhen compared to late fusion or to monocular input, early fusion stereo typically is twice as accurate.\nResults\nThe images below show qualitative results of KeyPose on individual objects. On the left is one of the original stereo images; in the middle are the predicted 3D keypoints projected onto the image. On the right, we visualize points from a 3D model of the bottle, placed at the pose determined by the predicted 3D keypoints. The network is efficient and accurate, predicting keypoints with an MAE of 5.2 mm for the bottle and 10.1 mm for the mug using just 5 ms on a standard GPU.\nThe following table shows results for KeyPose on category-level estimation. The test set used a background texture not seen by the training set. Note that the MAE varies from 5.8 mm to 9.9 mm, showing the accuracy of the method.DenseFusionFor a complete accounting of quantitative results, as well as, ablation studies, please see the paper and supplementary materials and the KeyPose website.paper and supplementary materialsKeyPose website\nConclusion\nThis work shows that it is possible to accurately estimate the 3D pose of transparent objects from RGB images without reliance on depth images. It validates the use of stereo images as input to an early fusion deep net, where the network is trained to extract sparse 3D keypoints directly from the stereo pair. We hope the availability of an extensive, labelled dataset of transparent objects will help to advance the field. Finally, while we used semi-automatic methods to efficiently label the dataset, we hope to employ self-supervision methods in future work to do away with manual labelling.self-supervision methods\nAcknowledgementsI want to thank my co-authors, Xingyu Liu of Stanford University, and Rico Jonschkowski and Anelia Angelova; as well the many who helped us through discussions during the project and paper writing, including Andy Zheng, Shuran Song, Vincent Vanhoucke, Pete Florence, and Jonathan Tompson.",
      "link": "http://ai.googleblog.com/2020/09/keypose-estimating-3d-pose-of.html",
      "author": "Posted by Kurt Konolige, Software Engineer, Robotics at Google"
    },
    {
      "title": "Using Machine Learning to Detect Deficient Coverage in Colonoscopy Screenings",
      "date": "Friday, August 28, 2020",
      "abstract": "Using Machine Learning to Detect Deficient Coverage in Colonoscopy Screenings\nColorectal cancer (CRC) is a global health problem and the second deadliest cancer in the United States, resulting in an estimated 900K deaths per year. While deadly, CRC can be prevented by removing small precancerous lesions in the colon, called polyps, before they become cancerous. In fact, it is estimated that a 1% increase in the adenoma detection rate (ADR, defined as the fraction of procedures in which a physician discovers at least one polyp) can lead to a 6% decrease in the rate of interval CRCs (a CRC that is diagnosed within 60 months of a negative colonoscopy).Colorectal cancersecond deadliest cancerestimated 900K deaths per year6% decreaseinterval CRCs\nColonoscopy is considered the gold standard procedure for the detection and removal of polyps. Unfortunately, the literature indicates that endoscopists miss on average 22%-28% of polyps during colonoscopies; furthermore, 20% to 24% of polyps that have the potential to become cancerous (adenomas) are missed. Two major factors that may cause an endoscopist to miss a polyp are (1) the polyp appears in the field of view, but the endoscopist misses it, perhaps due to its small size or flat shape; and (2) the polyp does not appear in the field of view, as the endoscopist has not fully covered the relevant area during the procedure.miss on average 22%-28% of polyps during colonoscopiesadenomas\nIn \u201cDetecting Deficient Coverage in Colonoscopies\u201d, we introduce the Colonoscopy Coverage Deficiency via Depth algorithm, or C2D2, a machine learning-based approach to improving colonoscopy coverage. The C2D2 algorithm performs a local 3D reconstruction of the colon as images are captured during the procedure, and on that basis, identifies which areas of the colon were covered and which remained outside of the field of view. C2D2 can then indicate in real time whether a particular area of the colon has suffered from deficient coverage so the endoscopist can return to that area. Our work proposes a novel approach to compute coverage in real time, for which 3D reconstruction is done using a calibration-free, unsupervised learning method, and evaluate it in a large scale way.Detecting Deficient Coverage in Colonoscopiesunsupervised learning\nThe C2D2 Algorithm\nWhen considering colon coverage, it is important to estimate the coverage fraction \u2014 what percentage of the relevant regions were covered by a complete procedure. While a retrospective analysis is useful for the physician and could provide general guidance for future procedures, it is more useful to have real-time estimation of coverage fraction, on a segment by segment basis, i.e. knowledge of what fraction of the current segment has been covered while traversing the colon. The helpfulness of such functionality is clear: during the procedure itself, a physician may be alerted to segments with deficient coverage, and can immediately return to review these areas. Higher coverage will result in a higher proportion of polyps being seen.\nThe C2D2 algorithm is designed to compute such a segment-by-segment coverage in two phases: computing depth maps for each frame of the colonoscopy video, followed by computation of coverage based on these depth maps.\nDepth map creation consists of both depth estimation as well as pose estimation \u2014 the localization of where the endoscope is in space, as well as the direction it is pointing. In addition to the detection of deficient coverage, depth and pose estimation are useful for a variety of other interesting tasks. For example, depth can be used for improved detection of flat polyps, while pose estimation can be used for relocalizing areas of the colon (including polyps) that the endoscopist wishes to revisit, and both together can be used for visualization and navigation.Haustral ridges\nIn order to compute coverage fractions from these depth maps, we trained C2D2 on two sources of data: synthetic sequences and real sequences. We generated the synthetic videos using a graphical model of a colon. For each synthetic video, ground truth coverage is available in the form of a number between 0 (completely uncovered) and 1 (completely covered). For real sequences, we analyzed de-identified colonoscopy videos, for which ground truth coverage is unavailable.\nPerformance on Synthetic Videos\nWhen using synthetic videos, the availability of ground truth coverage enables the direct measurement of C2D2\u2019s performance. We quantify this using the mean absolute error (MAE), which indicates how much the algorithm\u2019s prediction differs, on average, from the ground truth. We find that C2D2\u2019s MAE = 0.075; meaning that, on average, the prediction of C2D2 is within 7.5% of the ground truth. By contrast, a group of physicians given the same task achieved MAE = 0.177, i.e., within 17.7% of the ground truth. Thus, the C2D2 attained an accuracy rate 2.4 times higher on synthetic sequences.mean absolute error\nPerformance on Real Videos\nOf course, what matters most is performance on videos of real colonoscopies. The challenge in this case is the absence of ground truth labelling: we don\u2019t know what the actual coverage is. Additionally, one cannot use labels provided by experts directly as they are not always accurate, due to the challenges described earlier. However, C2D2 can still perform inference on real colonoscopy videos. Indeed, the learning pipeline is designed to perform equally well on synthetic and real colonoscopy videos.\nTo verify performance on real sequences, we used a variant of a technique common in the generative modelling literature, which involves providing video sequences to human experts along with C2D2\u2019s coverage scores for those sequences. We then ask the experts to assess whether C2D2\u2019s score is correct. The idea is that while it is difficult for experts to assign a score directly, the task of verifying a given score is considerably easier. (This is similar to the fact that verifying a proposed solution to an algorithmic problem is generally much easier than computing that solution.) Using this methodology, experts verified C2D2\u2019s score 93% of the time. And in a more qualitative sense, C2D2\u2019s output seems to pass the \u201ceyeball test\u201d, see the figure below.lumen\nNext steps\nBy alerting physicians to missed regions of the colon wall, C2D2 promises to lead to the discovery of more adenomas, thereby increasing the ADR and concomitantly decreasing the rate of interval CRC. This would be of tremendous benefit to patients.interval CRC\nIn addition to this work that addresses colonoscopy coverage, we are concurrently conducting research to improve polyp detection by combining C2D2 with an automatic, real-time polyp detection algorithm. This study adds to the mounting evidence that physicians may use machine learning methods to augment their efforts, especially during procedures, to improve the quality of care for patients.\nAcknowledgements\nThis research was conducted by Daniel Freedman, Yochai Blau, Liran Katzir, Amit Aides, Ilan Shimshoni, Danny Veikherman, Tomer Golany, Ariel Gordon, Greg Corrado, Yossi Matias, and Ehud Rivlin, with support from Verily. We would like to thank all of our team members and collaborators who worked on this project with us, including: Nadav Rabani, Chen Barshai, Nia Stoykova, David Ben-Shimol, Jesse Lachter, and Ori Segol, 3D-Systems and many others. We'd also like to thank Yossi Matias for support and guidance. The research was conducted by teams from Google Health and Google Research, Israel.Verily3D-Systems",
      "link": "http://ai.googleblog.com/2020/08/using-machine-learning-to-detect.html",
      "author": "Posted by Daniel Freedman and Ehud Rivlin, Research Scientists, Google Health"
    },
    {
      "title": "Scaling Up Fundamental Quantum Chemistry Simulations on Quantum Hardware",
      "date": "Thursday, August 27, 2020",
      "abstract": "Scaling Up Fundamental Quantum Chemistry Simulations on Quantum Hardware\nAccurate computational prediction of chemical processes from the quantum mechanical laws that govern them is a tool that can unlock new frontiers in chemistry, improving a wide variety of industries. Unfortunately, the exact solution of quantum chemical equations for all but the smallest systems remains out of reach for modern classical computers, due to the exponential scaling in the number and statistics of quantum variables. However, by using a quantum computer, which by its very nature takes advantage of unique quantum mechanical properties to handle calculations intractable to its classical counterpart, simulations of complex chemical processes can be achieved. While today\u2019s quantum computers are powerful enough for a clear computational advantage at some tasks, it is an open question whether such devices can be used to accelerate our current quantum chemistry simulation techniques. computational advantage at some tasks,\nIn \u201cHartree-Fock on a Superconducting Qubit Quantum Computer\u201d, appearing today in Science, the Google AI Quantum team explores this complex question by performing the largest chemical simulation performed on a quantum computer to date. In our experiment, we used a noise-robust variational quantum eigensolver (VQE) to directly simulate a chemical mechanism via a quantum algorithm. Though the calculation focused on the Hartree-Fock approximation of a real chemical system, it was twice as large as previous chemistry calculations on a quantum computer, and contained ten times as many quantum gate operations. Importantly, we validate that algorithms being developed for currently available quantum computers can achieve the precision required for experimental predictions, revealing pathways towards realistic simulations of quantum chemical systems. Furthermore, we have released the code for the experiment, which uses OpenFermion, our open source repository for quantum computations of chemistry.Hartree-Fock on a Superconducting Qubit Quantum ComputerScienceGoogle AI Quantum teamvariational quantum eigensolverHartree-Fockreleased the codeOpenFermion\nDeveloping an Error Robust Quantum Algorithm for Chemistry\nThere are a number of ways to use a quantum computer to simulate the ground state energy of a molecular system.  In this work we focused on a quantum algorithm \u201cbuilding block\u201d, or circuit primitive, and perfect its performance through a VQE (more on that later).  In the classical setting this circuit primitive is equivalent to the Hartree-Fock model and is an important circuit component of an algorithm we previously developed for optimal chemistry simulations.  This allows us to focus on scaling up without incurring exponential simulation costs to validate our device.  Therefore, robust error mitigation on this component is crucial for accurate simulations when scaling to the \u201cbeyond classical\u201d regime.previously developed\nErrors in quantum computation emerge from interactions of the quantum circuitry with the environment, causing erroneous logic operations \u2014 even minor temperature fluctuations can cause qubit errors. Algorithms for simulating chemistry on near-term quantum devices must account for these errors with low overhead, both in terms of the number of qubits or additional quantum resources, such as implementing a quantum error correcting code. The most popular method to account for errors (and why we used it for our experiment) is to use a VQE. For our experiment, we selected the VQE we developed a few years ago, which treats the quantum processor like an neural network and attempts to optimize a quantum circuit\u2019s parameters to account for noisy quantum logic by minimizing a cost function. Just like how classical neural networks can tolerate imperfections in data by optimization, a VQE dynamically adjusts quantum circuit parameters to account for errors that occur during the quantum computation.qubitquantum error correcting codeVQE we developed a few years ago\nEnabling High Accuracy with Sycamore\nThe experiment was run on the Sycamore processor that was recently used to demonstrate quantum supremacy. Though our experiment required fewer qubits, even higher quantum gate fidelity was needed to resolve chemical bonding. This led to the development of new, targeted calibration techniques that optimally amplify errors so they can be diagnosed and corrected.demonstrate quantum supremacy\nErrors in the quantum computation can originate from a variety of sources in the quantum hardware stack. Sycamore has 54-qubits and consists of over 140 individually tunable elements, each controlled with high-speed, analog electrical pulses. Achieving precise control over the whole device requires fine tuning more than 2,000 control parameters, and even small errors in these parameters can quickly add up to large errors in the total computation.\nTo accurately control the device, we use an automated framework that maps the control problem onto a graph with thousands of nodes, each of which represent a physics experiment to determine a single unknown parameter. Traversing this graph takes us from basic priors about the device to a high fidelity quantum processor, and can be done in less than a day.  Ultimately, these techniques along with the algorithmic error mitigation enabled orders of magnitude reduction in the errors.\nPathways Forward\nWe hope that this experiment serves as a blueprint for how to run chemistry calculations on quantum processors, and as a jumping off point on the path to physical simulation advantage. One exciting prospect is that it is known how to modify the quantum circuits used in this experiment in a simple way such that they are no longer efficiently simulable, which would determine new directions for improved quantum algorithms and applications. We hope that the results from this experiment can be used to explore this regime by the broader research community. To run these experiments, you can find the code here.here",
      "link": "http://ai.googleblog.com/2020/08/scaling-up-fundamental-quantum.html",
      "author": "Posted by Nicholas Rubin and Charles Neill, Research Scientists, Google AI Quantum"
    },
    {
      "title": "Axial-DeepLab: Long-Range Modeling in All Layers for Panoptic Segmentation",
      "date": "Wednesday, August 26, 2020",
      "abstract": "Axial-DeepLab: Long-Range Modeling in All Layers for Panoptic Segmentation\nThe success of convolutional neural networks (CNNs) mainly comes from two properties of convolution: translation equivariance and locality. Translation equivariance, although not exact, ensures that the model functions well for objects at different positions in an image or for images of different sizes. Locality ensures efficient computation, but at the cost of making the modeling of long-range spatial relations challenging for panoptic segmentation of large images. For example, segmenting a large object requires modeling the shape of it, which could potentially cover a very large pixel area, and context that could be helpful for segmenting the object may come from farther away. In such cases, the inability to inform the model from context far from the convolution kernel could negatively impact the performance.convolutional neural networksconvolutionnot exactpanoptic segmentation\nA rich set of literature has discussed approaches to solving the limitation of locality and enabling long-range interactions in CNNs. Some employ atrous convolutions, or image pyramids, which expand the receptive field somewhat, but it is still limited to a small local region. Another line of work adopts self-attention mechanisms, e.g., non-local neural networks, which allow the receptive field to cover the entire input image, as opposed to local convolutions. Unfortunately, such approaches are computationally expensive, especially for large inputs. Recent works enable building fully attentional models, but at a cost of applying local constraints to non-local neural networks. These restrictions limit the model receptive field, which is harmful to tasks such as segmentation, especially on high-resolution inputs.atrous convolutionsimage pyramidsself-attentionnon-local neural networksfully attentional modelssegmentation\nIn our recent ECCV 2020 paper, \u201cAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\u201d, we propose to adopt axial-attention (or criss-cross attention), which recovers large receptive field in fully attentional models. The core idea is to separate 2D attention into two steps that apply 1D attention in the height and width axes sequentially. The efficiency of this approach enables attention over large regions, allowing models that learn long-range, or even global, interactions. Additionally, we propose a novel formulation for self-attention modules, which is more sensitive to the position of relevant context in a large receptive field with marginal costs. We evaluate our position-sensitive axial-attention method on panoptic segmentation by applying it to Panoptic-DeepLab, a simple and efficient method for panoptic segmentation. The effectiveness of our model is demonstrated on ImageNet, COCO, and Cityscapes. Axial-DeepLab achieves state-of-the-art results on panoptic segmentation and semantic segmentation, outperforming Panoptic-DeepLab by a large margin.ECCV 2020Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentationaxial-attentioncriss-cross attentionPanoptic-DeepLabImageNetCOCOCityscapespanoptic segmentationsemantic segmentationPanoptic-DeepLab\nAxial-Attention Architecture\nAxial-DeepLab consists of an Axial-ResNet backbone and Panoptic-DeepLab output heads, which produce panoptic segmentation results. Our Axial-ResNet is built on a ResNet architecture, in which all the 3\u00d73 local convolutions in the ResNet bottleneck blocks are replaced by our proposed global position-sensitive axial-attention, thus enabling both a large receptive field and precise positional information.Panoptic-DeepLabResNet architecture\nThe Axial-DeepLab height axial attention layer provides 1-dimensional self-attention globally, propagating information within individual columns \u2014 it does not transfer information between columns. The second 1D attention layer operating in the horizontal direction allows one to capture both column-wise and row-wise information. This separation reduces the complexity of self-attention from quadratic (2D) to linear (1D), which enables using a much larger (65\u00d765 vs. previously 3\u00d73) or even global context in all layers for long-range modeling in panoptic segmentation.\nNote that a message or feature vector at (x1, y1) can always be passed globally on a 2D lattice to any position (x2, y2), with one hop on the height-axis (x1, y1 \u2192x1, y2), followed by another hop on the width axis (x1, y2 \u2192 x2, y2). In this way, we are able to model 2D long-range relations in a single residual block. This axial-attention design also reduces the complexity from quadratic to linear and enables global receptive fields in all layers of a model.\nPosition-Sensitive Self-Attention\nAdditionally, we propose a position-sensitive formulation for self-attention. Previous self-attention formulations enabled a given pixel A to aggregate long-range context B, but provided no information about where in the receptive field the context originated. For example, perhaps the feature at pixel A represents the eye of a cat, and the context B might be the nose and another eye. In this case, the aggregated feature at pixel A would be a nose and two eyes, regardless of the geometric structure of a face. This could cause a false indication of the presence of a face when the two eyes are on the bottom-left of an image and the nose is on the top-right. A recently proposed solution is to impose a positional bias on where in the receptive field the context can originate. This bias depends on the feature at A only, (an eye), but not the feature at B, which contains important contextual information.recently proposed solution\nIn this work, we let this bias also depend on the context feature at B (i.e., the nose and another eye). This change enables a more accurate positional bias when a pixel and the context informing it are far away from one another and thus contains different information about the bias. In addition, when pixel A aggregates the context feature B, we also include a feature that indicates the relative position from A to B. This change enables A to know precisely where B originated. These two changes make self-attention position-sensitive, especially in the situation of long-range modeling.\nResults\nWe have tested Axial-DeepLab on COCO, and Cityscapes for panoptic segmentation. Improvements over the state-of-the-art Panoptic-DeepLab for each dataset can be seen in the table below. In particular, our Axial-DeepLab outperforms Panoptic-DeepLab by 2.8% Panoptic Quality (PQ) on the COCO test-dev set. Our single-scale small model performs better than multi-scale Panoptic-DeepLab while improving computational efficiency by 27x and using only 1/4 the number of parameters. We also show state-of-the-art results on Cityscapes. Moreover, we find that the performance increases as the block receptive field increases from 5 \u00d7 5 to 65 \u00d7 65. Our model is also more robust to out-of-distribution scales, on which the model was not trained.COCOCityscapespanoptic segmentationPanoptic-DeepLab\nBesides our main results on panoptic segmentation, our full axial-attention model, Axial-ResNet, also performs better than the previous best stand-alone self-attention model on ImageNet.stand-alone self-attention model\nConclusion\nWe have proposed and demonstrated the effectiveness of position-sensitive axial-attention on image classification and panoptic segmentation. On ImageNet, our Axial-ResNet, formed by stacking axial-attention blocks, achieves state-of-the-art results among stand-alone self-attention models. We further convert Axial-ResNet to Axial-DeepLab for bottom-up panoptic segmentation, and also show state-of-the-art performance on several benchmarks, including COCO, and Cityscapes. We hope our promising results could establish that axial-attention is an effective building block for modern computer vision models.\nAcknowledgements\nThis post reflects the work of the authors as well as Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. We also thank Niki Parmar for discussion and support; Ashish Vaswani, Xuhui Jia, Raviteja Vemulapalli, Zhuoran Shen for their insightful comments and suggestions; Maxwell Collins and Blake Hechtman for technical support.",
      "link": "http://ai.googleblog.com/2020/08/axial-deeplab-long-range-modeling-in.html",
      "author": "Posted by Huiyu Wang, Student Researcher and Yukun Zhu, Software Engineer, Google Research"
    },
    {
      "title": "An Analysis of Online Datasets Using Dataset Search (Published, in Part, as a Dataset)",
      "date": "Tuesday, August 25, 2020",
      "abstract": "An Analysis of Online Datasets Using Dataset Search (Published, in Part, as a Dataset)\nThere are tens of millions of datasets on the web, with content ranging from sensor data and government records, to results of scientific experiments and business reports. Indeed, there are datasets for almost anything one can imagine, be it diets of emperor penguins or where remote workers live. More than two years ago, we undertook an effort to design a search engine that would provide a single entry point to these millions of datasets and thousands of repositories.  The result is Dataset Search, which we launched in beta in 2018 and fully launched in January 2020. In addition to facilitating access to data, Dataset Search reconciles and indexes datasets using the metadata descriptions that come directly from the dataset web pages using schema.org structure.diets of emperor penguinswhere remote workers liveDataset Searchlaunched in betafully launchedschema.org\nAs of today, the complete Dataset Search corpus contains more than 31 million datasets from more than 4,600 internet domains. About half of these datasets come from .com domains, but .org and governmental domains are also well represented. The graph below shows the growth of the corpus over the last two years, and while we still don\u2019t know what fraction of datasets on the web are currently in Dataset Search, the number continues to grow steadily.\nTo better understand the breadth and utility of the datasets made available through Dataset Search, we published \u201cGoogle Dataset Search by the Numbers\u201d, accepted at the 2020 International Semantic Web Conference. Here we provide an overview of the available datasets, present metrics and insights originating from their analysis, and suggest best practices for publishing future scientific datasets. In order to enable other researchers to build analysis and tools using the metadata, we are also making a subset of the data publicly available.Google Dataset Search by the Numbers2020 International Semantic Web Conferencesubset of the data\nA Range of Dataset Topics\nIn order to determine the distribution of topics covered by the datasets, we infer the research category based on dataset titles and descriptions, as well as other text on the dataset Web pages. The two most common topics are geosciences and social sciences, which account for roughly 45% of the datasets. Biology is a close third at ~15%, followed by a roughly even distribution for other topics, including computer science, agriculture, and chemistry, among others.\nIn our initial efforts to launch Dataset Search, we reached out to specific communities, which was key to bootstrapping widespread use of the corpus. Initially, we focused on geosciences and social sciences, but since then, we have allowed the corpus to grow organically. We were surprised to see that the fields associated with the communities we reached out to early on are still dominating the corpus. While their early involvement certainly contributes to their prevalence, there may be other factors involved, such as differences in culture across communities. For instance, geosciences have been particularly successful in making their data findable, accessible, interoperable, and reusable (FAIR), a core component to reducing barriers for access.initial effortsparticularly successfulFAIR\nMaking Data Easily Citable and Reusable\nThere is a growing consensus among researchers across scientific disciplines that it is important to make datasets available, to publish details relevant to their use, and to cite them when they are used. Many funding agencies and academic publishers require proper publication and citation of data.consensus\nPeer-reviewed journals such as Nature Scientific Data are dedicated to publishing valuable datasets, and efforts such as DataCite provide digital object identifiers (DOIs) for them. Resolution services (e.g., identifiers.org) also provide persistent, de-referenceable identifiers, allowing for easy citation, which is key to making datasets widely available in scientific discourse. Unfortunately, we found that only about 11% of the datasets in the corpus (or ~3M) have DOIs. We chose this subset from the dataset corpus to be included in our open-source release. From this collection, about 2.3M datasets come from two sites, datacite.org and figshare.com:Nature Scientific DataDataCitedigital object identifiersidentifiers.orgopen-source releasedatacite.orgfigshare.com\nPublishers can specify access requirements for a dataset via schema.org metadata properties, including details of the license and information indicating whether or not the dataset is accessible for free. Only 34% of datasets specify license information, but when no license is specified, users cannot make any assumptions on whether or not they are allowed to reuse the data. Thus, adding licensing information, and, ideally, adding as open a license as possible, will greatly improve the reusability of the data.licenseis accessible for free\nAmong the datasets that did specify a license, we were able to recognize a known license in 72% of cases. Those licenses include Open Government licenses for the UK and Canada, Creative Commons licenses, and several Public Domain licenses (e.g., Public Domain Mark 1.0). We found 89.5% of these datasets to either be accessible for free or use a license that allows redistribution, or both. And of these open datasets, 5.6M (91%) allow commercial reuse.UKCanadaCreative Commons licensesPublic Domain Mark 1.0\nAnother critical component of data reusability is providing downloadable data, yet only 44% of datasets specify download information in their metadata. A possible explanation for this surprisingly low value is that webmasters (or dataset-hosting platforms) fear that exposing the data download link through schema.org metadata may lead search engines or other applications to give their users direct access to download the data, thus \u201cstealing\u201d traffic from their website. Another concern may be that data needs the proper context to be used appropriately (e.g., methodology, footnotes, and license information), and providers feel that only their web pages can give the complete picture. In Dataset Search, we do not show download links as part of dataset metadata so that users must go to the publisher\u2019s website to download the data, where they will see the full context for the dataset.\nWhat Do Users Access?\nFinally, we examine how Dataset Search is being used. Overall, 2.1M unique datasets from 2.6K domains appeared in the top 100 Dataset Search results over 14 days in May 2020. We find that the distribution of topics being queried is different from that of the corpus as a whole. For instance, geoscience takes up a much smaller fraction, and conversely, biology and medicine represent a larger fraction relative to their share of the corpus. This result is likely explained by the timing of our analysis, as it was performed during the first weeks of the COVID-19 pandemic.\nBest Practices for Publishing Scientific Datasets\nBased on our analysis, we have identified a set of best practices that can improve how datasets are discovered, reused and cited.Discoverability\nDataset metadata should be on pages that are accessible to web crawlers and that provide metadata in machine-readable formats in order to improve discoverability.Persistence\nPublishing metadata on sites that are likely to be more persistent than personal web pages will facilitate data reuse and citation. Indeed, during our analysis of Dataset Search, we noted a very high rate of turnover \u2014 many URLs that hosted a dataset one day did not have it a few weeks or months later. Data repositories, such as Figshare, Zenodo, DataDryad, Kaggle Datasets and many others, are a good way to ensure dataset persistence. Many of these repositories have agreements with libraries to preserve data in perpetuity.FigshareZenodoDataDryadKaggle Datasetsagreements with librariesProvenance\nWith datasets often published in multiple repositories, it would be useful for repositories to describe the provenance information more explicitly in the metadata. The provenance information helps users understand who collected the data, where the primary source of the dataset is, or how it might have changed.Licensing \nDatasets should include licensing information, ideally in a machine-readable format. Our analysis indicates that when dataset providers select a license, they tend to choose a fairly open one. So, encouraging and enabling scientists to choose licenses for their data will result in many more datasets being openly available.Assigning persistent identifiers (such as DOIs)\nDOIs are critical for long-term tracking and useability. Not only do these identifiers allow for much easier citation of datasets and version tracking, they are also dereferenceable: if a dataset moves, the identifier can point to a different location.\nReleasing Metadata for Datasets with Persistent Identifiers\nAs part of the announcement today, we are also releasing a subset of our corpus for others to use. It contains the metadata for more than three million datasets that have DOIs and other types of persistent identifiers \u2013- these are the datasets that are the most easily citable. Researchers can use this metadata to perform deeper analysis or to build their own applications using this data. For example, much of the growth of DOI usage appears to have been within the last decade. How does this timeframe relate to the datasets covered in the corpus? Is the DOI usage distribution uniform across datasets, or are there significant differences between research communities?releasing a subset of our corpus\nWe will update the dataset on a regular basis. Finally, we hope that focusing this data release on datasets with persistent citable identifiers will encourage more data providers to describe their datasets in more detail and to make them more easily citable.\nIn conclusion, we hope that having data more discoverable through tools such as Google's Dataset Search will encourage scientists to share their data more broadly and do it in a way that makes data truly FAIR.FAIR\nAcknowledgments\nThis post reflects the work of the entire Dataset Search team. We are grateful to Shiyu Chen, Dimitris Paparas, Katrina Sostek, Yale Cong, Marc Najork, and Chris Gorgolewski for their contributions. We would also like to thank Hal Varian for suggesting this analysis and for many helpful ideas.",
      "link": "http://ai.googleblog.com/2020/08/an-analysis-of-online-datasets-using.html",
      "author": "Posted by Natasha Noy, Research Scientist and Omar Benjelloun, Software Engineer, Google Research"
    },
    {
      "title": "Google at ECCV 2020",
      "date": "Monday, August 24, 2020",
      "abstract": "Google at ECCV 2020\nThis week, the 16th European Conference on Computer Vision (ECCV2020) begins, a premier forum for the dissemination of research in computer vision and related fields. Being held virtually for the first time this year, Google is proud to be an ECCV2020 Platinum Partner and is excited to share our research with the community with nearly 50 accepted publications, alongside several tutorials and workshops.16th European Conference on Computer VisionECCV2020 Platinum Partner\nIf you are registered for ECCV this year, please visit our virtual booth in the Platinum Exhibition Hall to learn more about the research we\u2019re presenting at ECCV 2020, including some demos and opportunities to connect with our researchers. You can also learn more about our contributions below (Google affiliations in bold).\nOrganizing Committee\nGeneral Chairs:\u00a0Vittorio Ferrari, Bob Fisher, Cordelia Schmid, Emanuele Truco\nAcademic Demonstrations Chair:\u00a0Thomas Mensink\nAccepted Publications\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis (Honorable Mention Award)\nBen Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren NgNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\nQuaternion Equivariant Capsule Networks for 3D Point Clouds\nYongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti, Leonidas Guibas, Federico TombariQuaternion Equivariant Capsule Networks for 3D Point Clouds\nSoftpoolNet: Shape Descriptor for Point Cloud Completion and Classification\nYida Wang, David Joseph Tan, Nassir Navab, Federico TombariSoftpoolNet: Shape Descriptor for Point Cloud Completion and Classification\nCombining Implicit Function Learning and Parametric Models for 3D Human Reconstruction\nBharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt, Gerard Pons-MollCombining Implicit Function Learning and Parametric Models for 3D Human Reconstruction\nCoReNet: Coherent 3D scene reconstruction from a single RGB image\nStefan Popov, Pablo Bauszat, Vittorio FerrariCoReNet: Coherent 3D scene reconstruction from a single RGB image\nAdversarial Generative Grammars for Human Activity Prediction\nAJ Piergiovanni, Anelia Angelova, Alexander Toshev, Michael S. RyooAdversarial Generative Grammars for Human Activity Prediction\nSelf6D: Self-Supervised Monocular 6D Object Pose Estimation\nGu Wang, Fabian Manhardt, Jianzhun Shao, Xiangyang Ji, Nassir Navab, Federico TombariSelf6D: Self-Supervised Monocular 6D Object Pose Estimation\nDu2Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels\nYinda Zhang, Neal Wadhwa, Sergio Orts-Escolano, Christian H\u00e4ne, Sean Fanello, Rahul GargDu2Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels\nWhat Matters in Unsupervised Optical Flow\nRico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon, Kurt Konolige, Anelia AngelovaWhat Matters in Unsupervised Optical Flow\nAppearance Consensus Driven Self-Supervised Human Mesh Recovery\nJogendra N. Kundu, Mugalodi Rakesh, Varun Jampani, Rahul M. Venkatesh, R. Venkatesh BabuAppearance Consensus Driven Self-Supervised Human Mesh Recovery\nFashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset\nMenglin Jia, Mengyun Shi, Mikhail Sirotenko, Yin Cui, Claire Cardie, Bharath Hariharan, Hartwig Adam, Serge BelongieFashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset\nPointMixup: Augmentation for Point Clouds\nYunlu Chen, Vincent Tao Hu, Efstratios Gavves, Thomas Mensink, Pascal Mettes1, Pengwan Yang, Cees SnoekPointMixup: Augmentation for Point Clouds\nConnecting Vision and Language with Localized Narratives (see our blog post)\nJordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, Vittorio FerrariConnecting Vision and Language with Localized Narrativesblog post\nBig Transfer (BiT): General Visual Representation Learning (see our blog post)\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil HoulsbyBig Transfer (BiT): General Visual Representation Learningblog post\nView-Invariant Probabilistic Embedding for Human Pose\nJennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Ting LiuView-Invariant Probabilistic Embedding for Human Pose\nAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, Liang-Chieh ChenAxial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation\nMask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve\nWeicheng Kuo, Anelia Angelova, Tsung-Yi Lin, Angela DaiMask2CAD: 3D Shape Prediction by Learning to Segment and Retrieve\nA Generalization of Otsu's Method and Minimum Error Thresholding\nJonathan T. BarronA Generalization of Otsu's Method and Minimum Error Thresholding\nLearning to Factorize and Relight a City\nAndrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, Noah SnavelyLearning to Factorize and Relight a City\nWeakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows\nAndrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, Bill Freeman, Rahul Sukthankar, Cristian SminchisescuWeakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows\nMulti-modal Transformer for Video Retrieval\nValentin Gabeur, Chen Sun, Karteek Alahari, Cordelia SchmidMulti-modal Transformer for Video Retrieval\nGenerative Latent Textured Proxies for Category-Level Object Modeling\nRicardo Martin Brualla, Sofien Bouaziz, Matthew Brown, Rohit Pandey, Dan B GoldmanGenerative Latent Textured Proxies for Category-Level Object Modeling\nNeural Design Network: Graphic Layout Generation with Constraints\nHsin-Ying Lee*, Lu Jiang, Irfan Essa, Phuong B Le, Haifeng Gong, Ming-Hsuan Yang, Weilong YangNeural Design Network: Graphic Layout Generation with Constraints\nNeural Articulated Shape Approximation\nBoyang Deng, Gerard Pons-Moll, Timothy Jeruzalski, JP Lewis, Geoffrey Hinton, Mohammad Norouzi, Andrea TagliasacchiNeural Articulated Shape Approximation\nUncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos\nAnurag Arnab, Arsha Nagrani, Chen Sun, Cordelia SchmidUncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos\nBeyond Controlled Environments: 3D Camera Re-Localization in Changing Indoor Scenes\nJohanna Wald, Torsten Sattler, Stuart Golodetz, Tommaso Cavallari, Federico TombariBeyond Controlled Environments: 3D Camera Re-Localization in Changing Indoor Scenes\nConsistency Guided Scene Flow Estimation\nYuhua Chen, Luc Van Gool, Cordelia Schmid, Cristian SminchisescuConsistency Guided Scene Flow Estimation\nContinuous Adaptation for Interactive Object Segmentation by Learning from Corrections\nTheodora Kontogianni*, Michael Gygli, Jasper Uijlings, Vittorio FerrariContinuous Adaptation for Interactive Object Segmentation by Learning from Corrections\nSimPose: Effectively Learning DensePose and Surface Normal of People from Simulated Data\nTyler Lixuan Zhu, Per Karlsson, Christoph BreglerSimPose: Effectively Learning DensePose and Surface Normal of People from Simulated Data\nLearning Data Augmentation Strategies for Object Detection\nBarret Zoph, Ekin Dogus Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, Quoc V LeLearning Data Augmentation Strategies for Object Detection\nStreaming Object Detection for 3-D Point Clouds\nWei Han, Zhengdong Zhang, Benjamin Caine, Brandon Yang, Christoph Sprunk, Ouais Alsharif, Jiquan Ngiam, Vijay Vasudevan, Jonathon Shlens, Zhifeng ChenStreaming Object Detection for 3-D Point Clouds\nImproving 3D Object Detection through Progressive Population Based Augmentation\nShuyang Cheng, Zhaoqi Leng, Ekin Dogus Cubuk, Barret Zoph, Chunyan Bai, Jiquan Ngiam, Yang Song, Benjamin Caine, Vijay Vasudevan, Congcong Li, Quoc V. Le, Jonathon Shlens, Dragomir AnguelovImproving 3D Object Detection through Progressive Population Based Augmentation\nAn LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds\nRui Huang, Wanyue Zhang, Abhijit Kundu, Caroline Pantofaru, David A Ross, Thomas Funkhouser, Alireza FathiAn LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds\nBigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models\nJiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc LeBigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models\nMemory-Efficient Incremental Learning Through Feature Adaptation\nAhmet Iscen, Jeffrey Zhang, Svetlana Lazebnik, Cordelia SchmidMemory-Efficient Incremental Learning Through Feature Adaptation\nVirtual Multi-view Fusion for 3D Semantic Segmentation\nAbhijit Kundu, Xiaoqi Yin, Alireza Fathi, David A Ross, Brian E Brewington, Thomas Funkhouser, Caroline PantofaruVirtual Multi-view Fusion for 3D Semantic Segmentation\nEfficient Scale-permuted Backbone with Learned Resource Distribution\nXianzhi Du, Tsung-Yi Lin, Pengchong Jin, Yin Cui, Mingxing Tan, Quoc V Le, Xiaodan SongEfficient Scale-permuted Backbone with Learned Resource Distribution\nRetrieveGAN: Image Synthesis via Differentiable Patch Retrieval\nHung-Yu Tseng*, Hsin-Ying Lee*, Lu Jiang, Ming-Hsuan Yang, Weilong YangRetrieveGAN: Image Synthesis via Differentiable Patch Retrieval\nGraph convolutional networks for learning with few clean and many noisy labels\nAhmet Iscen, Giorgos Tolias, Yannis Avrithis, Ondrej Chum, Cordelia SchmidGraph convolutional networks for learning with few clean and many noisy labels\nDeep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis\nRuixuan Yu, Xin Wei, Federico Tombari, Jian SunDeep Positional and Relational Feature Learning for Rotation-Invariant Point Cloud Analysis\nFederated Visual Classification with Real-World Data Distribution\nTzu-Ming Harry Hsu, Hang Qi, Matthew BrownFederated Visual Classification with Real-World Data Distribution\nJoint Bilateral Learning for Real-time Universal Photorealistic Style Transfer\nXide Xia, Meng Zhang, Tianfan Xue, Zheng Sun, Hui Fang, Brian Kulis, Jiawen ChenJoint Bilateral Learning for Real-time Universal Photorealistic Style Transfer\nAssembleNet++: Assembling Modality Representations via Attention Connections\nMichael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, Anelia AngelovaAssembleNet++: Assembling Modality Representations via Attention Connections\nNaive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation\nLiang-Chieh Chen, Raphael Gontijo-Lopes, Bowen Cheng, Maxwell D. Collins, Ekin D. Cubuk, Barret Zoph, Hartwig Adam, Jonathon ShlensNaive-Student: Leveraging Semi-Supervised Learning in Video Sequences for Urban Scene Segmentation\nAttentionNAS: Spatiotemporal Attention Cell Search for Video Classification\nXiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S. Ryoo, Anelia Angelova, Kris M. Kitani, Wei HuaAttentionNAS: Spatiotemporal Attention Cell Search for Video Classification\nUnifying Deep Local and Global Features for Image Search\nBingyi Cao, Andre Araujo, Jack SimUnifying Deep Local and Global Features for Image Search\nPillar-based Object Detection for Autonomous Driving\nYue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Caroline Pantofaru, Tom Funkhouser, Justin SolomonPillar-based Object Detection for Autonomous Driving\nImproving Object Detection with Selective Self-supervised Self-training\nYandong Li, Di Huang, Danfeng Qin, Liqiang Wang, Boqing GongImproving Object Detection with Selective Self-supervised Self-training\nEnvironment-agnostic Multitask Learning for Natural Language Grounded Navigation\nXin Eric Wang*, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, Sujith RaviEnvironment-agnostic Multitask Learning for Natural Language Grounded Navigation\nSimAug: Learning Robust Representations from Simulation for Trajectory Prediction\nJunwei Liang, Lu Jiang, Alex HauptmannSimAug: Learning Robust Representations from Simulation for Trajectory Prediction\nTutorials\nNew Frontiers for Learning with Limited Labels or Data\nOrganizers: Shalini De Mello, Sifei Liu, Zhiding Yu, Pavlo Molchanov, Varun Jampani, Arash Vahdat, Animashree Anandkumar, Jan KautzNew Frontiers for Learning with Limited Labels or Data\nWeakly Supervised Learning in Computer Vision\t\nOrganizers: Seong Joon Oh, Rodrigo Benenson, Hakan BilenWeakly Supervised Learning in Computer Vision\nWorkshops\nJoint COCO and LVIS Recognition Challenge\nOrganizers: Alexander Kirillov, Tsung-Yi Lin, Yin Cui, Matteo Ruggero Ronchi, Agrim Gupta, Ross Girshick, Piotr DollarJoint COCO and LVIS Recognition Challenge\n4D Vision\nOrganizers: Anelia Angelova, Vincent Casser, J\u00fcrgen Sturm, Noah Snavely, Rahul Sukthankar4D Vision\nGigaVision: When Gigapixel Videography Meets Computer Vision\nOrganizers: Lu Fang, Shengjin Wang, David J. Brady, Feng YangGigaVision: When Gigapixel Videography Meets Computer Vision\nAdvances in Image Manipulation Workshop and Challenges\nOrganizers: Radu Timofte, Andrey Ignatov, Luc Van Gool, Wangmeng Zuo, Ming-Hsuan Yang, Kyoung Mu Lee, Liang Lin, Eli Shechtman, Kai Zhang, Dario Fuoli, Zhiwu Huang, Martin Danelljan, Shuhang Gu, Ming-Yu Liu, Seungjun Nah, Sanghyun Son, Jaerin Lee, Andres Romero, ETH Zurich, Hannan Lu, Ruofan Zhou, Majed El Helou, Sabine S\u00fcsstrunk, Roey Mechrez, BeyondMinds & Technion, Pengxu Wei, Evangelos Ntavelis, Siavash BigdeliAdvances in Image Manipulation Workshop and Challenges\nRobust Vision Challenge 2020\nOrganizers:Oliver Zendel, Hassan Abu Alhaija, Rodrigo Benenson, Marius Cordts, Angela Dai, Xavier Puig Fernandez, Andreas Geiger, Niklas Hanselmann, Nicolas Jourdan, Vladlen Koltun, Peter Kontschider, Alina Kuznetsova, Yubin Kang, Tsung-Yi Lin, Claudio Michaelis, Gerhard Neuhold, Matthias Niessner, Marc Pollefeys, Rene Ranftl, Carsten Rother, Torsten Sattler, Daniel Scharstein, Hendrik Schilling, Nick Schneider, Jonas Uhrig, Xiu-Shen Wei, Jonas Wulff, Bolei ZhouRobust Vision Challenge 2020\n\u201cDeep Internal Learning\u201d: Training with no prior examples\nOrganizers: Michal Irani,Tomer Michaeli, Tali Dekel, Assaf Shocher, Tamar Rott Shaham\u201cDeep Internal Learning\u201d: Training with no prior examples\nInstance-Level Recognition\nOrganizers: Andre Araujo, Cam Askew, Bingyi Cao, Ondrej Chum, Bohyung Han, Torsten Sattler, Jack Sim, Giorgos Tolias, Tobias Weyand, Xu ZhangInstance-Level Recognition\nWomen in Computer Vision Workshop (WiCV) \n  (Platinum Sponsor)\nPanel Participation: Dina Damen, Sanja Fiddler, Zeynep Akata, Grady Booch, Rahul SukthankarWomen in Computer Vision Workshop (WiCV)\n  *Work performed while at Google",
      "link": "http://ai.googleblog.com/2020/08/google-at-eccv-2020.html",
      "author": "Posted by Melody Pound and Lauren Jones"
    },
    {
      "title": "Understanding View Selection for Contrastive Learning",
      "date": "Friday, August 21, 2020",
      "abstract": "Understanding View Selection for Contrastive Learning\nMost people take for granted the ability to view an object from several different angles, but still recognize that it's the same object\u2014 a dog viewed from the front is still a dog when viewed from the side. While people do this naturally, computer scientists need to explicitly enable machines to learn representations that are view-invariant, with the goal of seeking robust data representations that retain information that is useful to downstream tasks.learn representations\nOf course, in order to learn these representations, manually annotated training data can be used. However, as in many cases such annotations aren\u2019t available, which gives rise to a series of self- and crossmodal supervised approaches that do not require manually annotated training data. Currently, a popular paradigm for training with such data is contrastive multiview learning, where two views of the same scene (for example, different image channels, augmentations of the same image, and video and text pairs) will tend to converge in representation space while two views of different scenes diverge. Despite their success, one important question remains: \u201cIf one doesn\u2019t have annotated labels readily available, how does one select the views to which the representations should be invariant?\u201d In other words, how does one identify an object using information that resides in the pixels of the image itself, while still remaining accurate when that image is viewed from disparate viewpoints?self-crossmodalcontrastive multiview learningdifferent image channelsaugmentations of the same imagevideo and text pairs\nIn \u201cWhat makes for good views for contrastive learning\u201d, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that one should reduce the mutual information between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their mutual information. We also consider data augmentation as a way to reduce mutual information, and show that increasing data augmentation indeed leads to decreasing mutual information while improving downstream classification accuracy. To encourage further research in this space, we have open-sourced the code and pre-trained models.What makes for good views for contrastive learningmutual informationcode and pre-trained models\nThe InfoMin Hypothesis\nThe goal of contrastive multiview learning is to learn a parametric encoder, whose output representations can be used to discriminate between pairs of views with the same identities, and pairs with different identities. The amount and type of information shared between the views determines how well the resulting model performs on downstream tasks. We hypothesize that the views that yield the best results should discard as much information in the input as possible except for the task relevant information (e.g., object labels), which we call the InfoMin principle.\nConsider the example below in which two patches of the same image represent the different \u201cviews\u201d. The training objective is to identify that the two views belong to the same image. It is undesirable to have views that share too much information, for example, where low-level color and texture cues can be exploited as \u201cshortcuts\u201d (left), or to have views that share too little information to identify that they belong to the same image (right). Rather, views at the \u201csweet spot\u201d share the information related to downstream tasks, such as patches corresponding to different parts of the panda for an object classification task (center).\nA Unified View on Contrastive Learning\nWe design several sets of experiments to verify the InfoMin hypothesis, motivated by the fact that there are simple ways to control the mutual information shared between views without any supervision. For example, we can sample different patches from the same images, and reduce their mutual information simply by increasing the distance between the patches. Here, we estimate the mutual information using InfoNCE (INCE), which is a quantitative measure of the mutual information lower bound.. Indeed, we observe a reverse U-shape curve: as mutual information is reduced, the downstream task accuracy first increases and then begins to decrease.InfoNCESTL-10CIFAR-10\nFurthermore, we demonstrate that several state-of-the-art contrastive learning methods (InstDis, MoCo, CMC, PIRL, SimCLR and CPC) can be unified through the perspective of view selection: despite the differences in architecture, objective and engineering details, all recent contrastive learning methods create two views that implicitly follow the InfoMin hypothesis, where the information shared between views are controlled by the strength of data augmentation. Motivated by this, we propose a new set of data augmentations, which outperforms the prior state of the art, SimCLR, by nearly 4% on the ImageNet linear readout benchmark. We also found that transferring our unsupervised pre-trained models to object detection and instance segmentation consistently outperforms ImageNet pre-training.InstDisMoCoCMCPIRLSimCLRCPCSimCLRlinear readout benchmarkobject detectioninstance segmentation\nLearning to Generate Views\nIn our work, we design unsupervised and semi-supervised methods that synthesize novel views following the InfoMin hypothesis. We learn flow-based models that transfer natural color spaces into novel color spaces, from which we split the channels to get views. For the unsupervised setup, the view generators are optimized to minimize the InfoNCE bound between views. As shown in the results below, we observe a similar reverse U-shape trend while minimizing the InfoNCE bound.flow-based models\nTo reach the sweet spot without overly minimizing mutual information, we can use the semi-supervised setup and guide the view generator to retain label information. As expected, all learned views are now centered around the sweet spot, no matter what the input color space is.\nCode and Pretrained Models\nTo accelerate research in self-supervised contastive learning, we are excited to share the code and pretrained models of InfoMin with the academic community. They can be found here.here\nAcknowledgements\nThe core team includes Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid and Phillip Isola. We would like to thank Kevin Murphy for insightful discussion; Lucas Beyer for feedback on the manuscript; and the Google Cloud team for computation support.",
      "link": "http://ai.googleblog.com/2020/08/understanding-view-selection-for.html",
      "author": "Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research"
    },
    {
      "title": "Tackling Open Challenges in Offline Reinforcement Learning",
      "date": "Thursday, August 20, 2020",
      "abstract": "Tackling Open Challenges in Offline Reinforcement LearningOver the past several years, there has been a surge of interest in reinforcement learning (RL) driven by its high-profile successes in game playing and robotic control. However, unlike supervised learning methods, which learn from massive datasets that are collected once and then reused, RL algorithms use a trial-and-error feedback loop that requires active interaction during learning, collecting data every time a new policy is learned. This approach is prohibitive in many real-world settings, such as healthcare, autonomous driving, and dialogue systems, where trial-and-error data collection can be costly, time consuming, or even irresponsible. Even for problems where some active data collection can be used, the requirement for interactive collection limits dataset size and diversity.reinforcement learninggame playingrobotic controlsupervised learning\nOffline RL (also called batch RL or fully off-policy RL) relies solely on a previously collected dataset without further interaction. It provides a way to utilize previously collected datasets \u2014 from previous RL experiments, from human demonstrations, and from hand-engineered exploration strategies \u2014 in order to automatically learn decision-making strategies. In principle, while off-policy RL algorithms can be used in the offline setting (fully off-policy), they are generally only successful when used with active environment interaction \u2014 without receiving this direct feedback, they often exhibit undesirable performance in practice. Consequently, while offline RL has enormous potential, that potential cannot be reached without resolving significant algorithmic challenges.off-policy RL algorithms can be used in the offline settingundesirable performance in practice\nIn \u201cOffline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems\u201d, we provide a comprehensive tutorial on approaches for tackling the challenges of offline RL and discuss the many issues that remain. To address these issues, we have designed and released an open-source benchmarking framework, Datasets for Deep Data-Driven Reinforcement Learning (D4RL), as well as a new, simple, and highly effective offline RL algorithm, called conservative Q-learning (CQL).Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open ProblemsDatasets for Deep Data-Driven Reinforcement Learningconservative Q-learning\nBenchmarks for Offline RL\nIn order to understand the capabilities of current approaches and to guide future progress, it is first necessary to have effective benchmarks. A common choice in prior work was to simply use data generated by a successful online RL run. However, while simple, this data collection approach is artificial because it involves training an online RL agent which is prohibitive in many real-world settings as we discussed previously. One wishes to learn a policy that is better than the current best from diverse data sources that provides good coverage of the task. For example, one might have data collected from a hand-designed controller of a robot arm, and use offline RL to train an improved controller. To enable progress in this field under realistic settings, one needs a benchmark suite that accurately reflects these settings, while being simple and accessible enough to enable rapid experimentation.data generated by a successful online RL run\nD4RL provides standardized environments, datasets and evaluation protocols, as well as reference scores for recent algorithms to help accomplish this. This is a \u201cbatteries-included\u201d resource, making it ideal for anyone to jump in and get started with minimal fuss.D4RLThe key design goal for D4RL was to develop tasks that reflect both real-world dataset challenges as well as real-world applications. Previous datasets used data collected either from random agents or agents trained with RL. Instead, by thinking through potential applications in autonomous driving, robotics, and other domains, we considered how real-world applications of offline RL might require handling of data generated from human demonstrations or hard-coded controllers, data collected from heterogeneous sources, and data collected by agents with a variety of different goals.trained with RL\nAside from the widely used MuJoCo locomotion tasks, D4RL includes datasets for more complex tasks. The Adroit domain, which requires manipulating a realistic robotic hand to use a hammer, for example, illustrates the challenges of working with limited human demonstrations, without which these tasks are extremely challenging. Previous work found that existing datasets could not distinguish between competing methods, whereas the Adroit domain reveals clear deficiencies between them.MuJoCoAdroit domainPrevious work\nAnother common scenario for real-world tasks is one in which the dataset used for training is collected from agents performing a wide range of other activities that are related to, but not specifically targeted towards, the task of interest. For example, data from human drivers may illustrate how to drive a car well, but do not necessarily show how to reach a specific desired destination. In this case, one might like offline RL methods to \u201cstitch\u201d together parts of routes in the driving dataset to accomplish a task that was not actually seen in the data (i.e., navigation). As an illustrative example, given paths labeled \u201cA\u201d and \u201cB\u201d in the picture below, offline RL should be able to \u201cremix\u201d them to produce path C.\nWe constructed a series of increasingly difficult tasks to exercise this \u201cstitching\u201d ability. The maze environments, shown below, require two robots (a simple ball or an \u201cAnt\u201d robot) to navigate to locations in a series of mazes.\nA more complex \u201cstitching\u201d scenario is provided by the Franka kitchen domain (based on the Adept environment), where demonstrations from humans using a VR interface comprise a multi-task dataset, and offline RL methods must again \u201cremix\u201d this data.Adept environment\nFinally, D4RL includes two tasks that are meant to more accurately reflect potential realistic applications of offline RL, both based on existing driving simulators. One is a first-person driving dataset that utilizes the widely used CARLA simulator developed at Intel, which provides photo-realistic images in realistic driving domains, and the other is a dataset from the Flow traffic control simulator (from UC Berkeley), which requires controlling autonomous vehicles to facilitate effective traffic flow.CARLAFlow\nWe have packaged these tasks and standardized datasets into an easy-to-use Python package to accelerate research. Furthermore, we provide benchmark numbers for all tasks using relevant prior methods (BC, SAC, BEAR, BRAC, AWR, BCQ), in order to  baseline new approaches. We are not the first to propose a benchmark for offline RL: a number of prior works have proposed simple datasets based on running RL algorithms, and several more recent works have proposed datasets with image observations and other features. However, we believe that the more realistic dataset composition in D4RL makes it an effective way to drive progress in the field.Python packagebenchmark numbersBCSACBEARBRACAWRBCQpriorworksmore recent\nAn Improved Algorithm for Offline RL\nAs we developed the benchmark tasks, we found that existing methods could not solve the more challenging tasks. The central challenge arises from a distributional shift: in order to improve over the historical data, offline RL algorithms must learn to make decisions that differ from the decisions taken in the dataset. However, this can lead to problems when the consequences of a seemingly good decision cannot be deduced from the data \u2014 if no agent has taken this particular turn in the maze, how does one know if it leads to the goal or not? Without handling this distributional shift problem, offline RL methods can extrapolate erroneously, making over-optimistic conclusions about the outcomes of rarely seen actions. Contrast this with the online setting, where reward bonuses modeled after curiosity and surprise optimistically bias the agent to explore all potentially rewarding paths. Because the agent receives interactive feedback, if the action turns out to be unrewarding, then it can simply avoid the path in the future.curiosity and surprise\nTo address this, we developed conservative Q-learning (CQL), an offline RL algorithm designed to guard against overestimation while avoiding explicit construction of a separate behavior model and without using importance weights. While standard Q-learning (and actor-critic) methods bootstrap from previous estimates, CQL is unique in that it is fundamentally a pessimistic algorithm: it assumes that if a good outcome was not seen for a given action, that action is likely to not be a good one. The central idea of CQL is to learn a lower bound on the policy\u2019s expected return (called the Q-function), instead of learning to approximate the expected return. If we then optimize our policy under this conservative Q-function, we can be confident that its value is no lower than this estimate, preventing errors from overestimation.conservative Q-learningQ-learningbootstrap from previous estimatesQ-function\nWe found that CQL attains state-of-the-art results on many of the harder D4RL tasks: CQL outperformed other approaches on the AntMaze, Kitchen tasks, and 6 out of 8 Adroit tasks. In particular, on the AntMaze tasks, which require navigating through a maze with an \u201cAnt\u201d robot, CQL is often the only algorithm that is able to learn non-trivial policies. CQL also performs well on other tasks, including Atari games. On the Atari tasks from Agarwal et al., CQL outperforms prior methods when data is limited (\u201c1%\u201d dataset). Moreover, CQL is simple to implement on top of existing algorithms (e.g., QR-DQN and SAC), without training additional neural networks.Agarwal et al.QR-DQNSACAgarwal et al.\nFuture Thoughts\nWe are excited about the fast-moving field of offline RL. While we took a first step towards a standard benchmark, there is clearly still room for improvement. We expect that as algorithms improve, we will need to reevaluate the tasks in the benchmark and develop more challenging tasks. We look forward to working with the community to evolve the benchmark and evaluation protocols. Together, we can bring the rich promises of offline RL to real-world applications.\nAcknowledgements\nThis work was carried out in collaboration with UC Berkeley PhD students Aviral Kumar, Justin Fu, and Aurick Zhou, with contributions from Ofir Nachum from Google Research.",
      "link": "http://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html",
      "author": "Posted by George Tucker, Research Scientist and Sergey Levine, Faculty Advisor, Google Research"
    },
    {
      "title": "Understanding Deep Learning on Controlled Noisy Labels",
      "date": "Wednesday, August 19, 2020",
      "abstract": "Understanding Deep Learning on Controlled Noisy Labels\nThe success of deep neural networks depends on access to high-quality labeled training data, as the presence of label errors (label noise) in training data can greatly reduce the accuracy of models on clean test data. Unfortunately, large training datasets almost always contain examples with inaccurate or incorrect labels. This leads to a paradox: on one hand, large datasets are necessary to train better deep networks, while on the other hand, deep networks tend to memorize training label noise, resulting in poorer model performance in practice.can greatly reduce the accuracy\nThe research community has recognized the importance of this problem, introducing works attempting to understand noisy training labels, e.g., by Arpit et al., as well as mitigation strategies, such as MentorNet or co-teaching, to overcome them. Controlled experiments play a crucial role in understanding noisy labels by studying the impact of the noise level \u2014 the percentage of examples with incorrect labels in the dataset \u2014 on model performance. However, current experiments have only been performed on synthetic labels, in which noisy examples have randomly assigned labels, not real-world label noise, which follows a different noise distribution. Such studies may then result in very different or even contradictory findings about noisy labels compared to practical experience. In addition, methods that perform well on synthetic noise may not work as well on real-world noisy labels.Arpit et al.MentorNetco-teachingControlled experiments\nIn \u201cBeyond Synthetic Noise: Deep Learning on Controlled Noisy Labels\u201d, published at ICML 2020, we make three contributions towards better understanding deep learning on non-synthetic noisy labels. First, we establish the first controlled dataset and benchmark of realistic, real-world label noise sourced from the web (i.e., web label noise). Second, we propose a simple but highly effective method to overcome both synthetic and real-world noisy labels. Finally, we conduct the largest study to date that compares synthetic and web label noise across a wide variety of settings.Beyond Synthetic Noise: Deep Learning on Controlled Noisy LabelsICML 2020\nProperties of Synthetic vs Real-World (Web) Label Noise\nThere are a number of differences between the distribution of images with synthetic versus real-world (web) label noise. First, images with web label noise tend to be more consistent, visually or semantically, with the true positive images. Second, synthetic label noise is at class-level (all examples in the same class are equally noisy), whereas real-world label noise is at instance-level (certain images are more likely to be mislabelled than others, regardless of the associated class). For example, images of \u201cHonda Civic\u201d and \u201cHonda Accord\u201d are more often confused when the images are taken from the side than when the vehicles are imaged from the front. Third, images with real-world label noise come from an open class vocabulary that may not overlap with the class vocabulary of a specific dataset. For example, the web noisy images of \u201cladybug\u201d include classes such as \u201cfly\u201d and other bugs that are not included in the class list of the dataset being used. The benchmark for controlled label noise will help provide better quantitative understanding of the differences between synthetic and real-world web label noise.\nBenchmark for Controlled Label Noise from the Web\nThe benchmark in this work is built on two public datasets: Mini-ImageNet, for coarse-grained image classification, and Stanford Cars, for fine-grained image classification. We gradually replace clean images in these datasets with incorrectly labeled images gathered from the web, following standard methods for the construction of synthetic datasets.Mini-ImageNetStanford Carsstandard methods\nTo do this, we collect images from the web using the class name (e.g., \u201cladybug\u201d) as a keyword \u2014 an automatic approach to collect noisy labeled images from the web without manual annotations. Each retrieved image is then examined by 3-5 annotators using Google Cloud Labeling Service who identify whether or not the web label given is correct, yielding nearly 213k annotated images. We use these web images with incorrect labels to replace a percentage of the clean training images in the original Mini-ImageNet and Stanford Cars datasets. We create 10 different datasets with progressively higher levels of label noise (from 0% clean data to 80% data with erroneous labels). The datasets have been open-sourced at our Controlled Noisy Web Labels website.Google Cloud Labeling ServiceControlled Noisy Web Labels website\nMentorMix:  A Simple Robust Learning Method\nGiven a dataset of some unknown noise level, our goal is to train a robust model that can generalize well on the clean test data. We introduce a simple yet effective method for dealing with both synthetic and real-world noisy labels, called MentorMix, which we developed on the Controlled Noisy Web Labels dataset.\nMentorMix is an iterative approach built on two existing techniques, MentorNet and Mixup, that comprises four steps: weight, sample, mixup, and weight again. In the first step, a weight is computed for every example in a mini-batch by a MentorNet network, which can be tailored to the task at hand, and the weights are normalized into a distribution. In practice, the goal is to assign high weights for correctly labeled examples and zero weights for incorrectly labeled examples. In reality, we don't know which are correct and which are incorrect, so MentorNet weights are based on approximations. In the example here, MentorNet uses the StudentNet training loss to determine the weights in the distribution.MentorNetMixupNext, for each example, we use importance sampling to select another example in the same mini-batch according to the distribution. As examples with higher weights tend to have the correct label, they are favored in the sampling procedure. We then use Mixup to mix the original and sampled examples so that the model interpolates between the two and avoids over-fitting the noisy training examples. Finally, we may compute another weight for the mixed example to scale the final loss. The impact of this second weighting strategy becomes more pronounced for high noise levels.importance samplingConceptually, the above steps implement a new robust loss, which turns out to be more resilient to noisy training labels. More discussion on this topic can be found in our paper. The animation below illustrates the four key steps in MentorMix, where StudentNet is the model to be trained on noisy labeled data. We employ a very simple version of MentorNet, as described by Jiang et al., to compute the weight for each example.our paperJiang et al.\nEvaluation\nWe evaluate MentorMix on five datasets including CIFAR 10/100 with synthetic label noise, and WebVision 1.0, a large dataset of 2.2 million images with real-world noisy labels. MentorMix consistently yields improved results on the CIFAR 10/100 datasets and achieves the best published result on the WebVision dataset, improving the previous best method by a significant ~3% in terms of the top-1 classification accuracy on the ImageNet ILSVRC12 validation set.CIFAR 10/100WebVision 1.0ImageNet ILSVRC12Lee et al. 2018MentorNet 2018Guo et al. 2018\nNew Findings on Noisy Labels from the Web\nThis work represents the largest study to date into understanding deep neural networks trained on noisy labels. We propose three new findings on web label noise:\n    While it is well known that deep neural networks generalize poorly on synthetic label noise, our results suggest that deep neural networks generalize much better on web label noise. For example, the classification accuracy of a network trained on the Stanford Cars dataset using the 60% web label noise level is 0.66, much higher than that for the same network trained at the same 60% level of synthetic noise, which achieves only 0.09. This pattern is consistent across our two datasets using both fine-tuning and training from scratch.generalize poorly on synthetic label noise\n    Our common understanding is that deep neural networks learn patterns first \u2014 an interesting property in which DNNs are able to automatically capture generalizable \u201cpatterns\u201d in the early training stage before memorizing noisy training labels. Because of this, early stopping is commonly used for training on noisy data. However, our results suggest deep neural networks may not learn patterns first when trained using datasets with web label noise, at least for the fine-grained classification task, suggesting that early stopping may not be effective on real-world label noise from the web.deep neural networks learn patterns firstearly stopping\nKornblith et al. (2019) found that fine-tuning more advanced architectures trained on ImageNet tend to perform better on downstream tasks that have clean training labels. Our results extend this finding to noisy training data, showing that a better pre-trained architecture that exhibits better performance when pre-trained on ImageNet is likely to perform better even when it is fine-tuned on noisy training labels.Kornblith et al. (2019)\nSummary\nBased on our findings, we have the following practical recommendations for training deep neural networks on noisy data.our findingsEarly stopping\nThe code of MentorMix is available on GitHub, the datasets are on our Dataset website.GitHubDataset website\nAknowledgementsThis research was conducted by Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. We'd like to thank Boqing Gong and Fei Sha for constructive feedback. Additional thanks go to the leadership Andrew Moore for supporting our data labeling effort, along with Tomas Izo and Rahul Sukthankar for help in releasing the dataset.",
      "link": "http://ai.googleblog.com/2020/08/understanding-deep-learning-on.html",
      "author": "Posted by Lu Jiang, Senior Research Scientist and Weilong Yang, Senior Staff Software Engineer, Google Research"
    },
    {
      "title": "Language-Agnostic BERT Sentence Embedding",
      "date": "Tuesday, August 18, 2020",
      "abstract": "Language-Agnostic BERT Sentence Embedding\nA multilingual embedding model is a powerful tool that encodes text from different languages into a shared embedding space, enabling it to be applied to a range of downstream tasks, like text classification, clustering, and others, while also leveraging semantic information for language understanding. Existing approaches for generating such embeddings, like LASER or m~USE,  rely on parallel data, mapping a sentence from one language directly to another language in order to encourage consistency between the sentence embeddings. While these existing multilingual approaches yield good overall performance across a number of languages, they often underperform on high-resource languages compared to dedicated bilingual models, which can leverage approaches like translation ranking tasks with translation pairs as training data to obtain more closely aligned representations. Further, due to limited model capacity and the often poor quality of training data for low-resource languages, it can be difficult to extend multilingual models to support a larger number of languages while maintaining good performance.text classificationclusteringLASERm~USEtranslation ranking tasks\nRecent efforts to improve language models include the development of masked language model (MLM) pre-training, such as that used by BERT, ALBERT and RoBERTa. This approach has led to exceptional gains across a wide range of languages and a variety of natural language processing tasks since it only requires monolingual text. In addition, MLM pre-training has been extended to the multilingual setting by modifying MLM training to include concatenated translation pairs, known as translation language modeling (TLM), or by simply introducing pre-training data from multiple languages. However, while the internal model representations learned during MLM and TLM training are helpful when fine-tuning on downstream tasks, without a sentence level objective, they do not directly produce sentence embeddings, which are critical for translation tasks.masked language modelBERTALBERTRoBERTa.translation language modeling\nIn \u201cLanguage-agnostic BERT Sentence Embedding\u201d, we present a multilingual BERT embedding model, called LaBSE, that produces language-agnostic cross-lingual sentence embeddings for 109 languages. The model is trained on 17 billion monolingual sentences and 6 billion bilingual sentence pairs using MLM and TLM pre-training, resulting in a model that is effective even on low-resource languages for which there is no data available during training. Further, the model establishes a new state of the art on multiple parallel text (a.k.a. bitext) retrieval tasks. We have released the pre-trained model to the community through tfhub, which includes modules that can be used as-is or can be fine-tuned using domain-specific data.Language-agnostic BERT Sentence EmbeddingBERTparallel textbitexttfhub\nThe Model\nIn previous work, we proposed the use of a translation ranking task to learn a multilingual sentence embedding space. This approach tasks the model with ranking the true translation over a collection of sentences in the target language, given a sentence in the source language. The translation ranking task is trained using a dual encoder architecture with a shared transformer encoder. The resulting bilingual models achieved state-of-the-art performance on multiple parallel text retrieval tasks (including United Nations and BUCC). However, the model suffered when the bi-lingual models were extended to support multiple languages (16 languages, in our test case) due to limitations in model capacity, vocabulary coverage, training data quality and more.previous worktransformerUnited NationsBUCC\nFor LaBSE, we leverage recent advances on language model pre-training, including MLM and TLM, on a BERT-like architecture and follow this with fine-tuning on a translation ranking task. A 12-layer transformer with a 500k token vocabulary pre-trained using MLM and TLM on 109 languages is used to increase the model and vocabulary coverage. The resulting LaBSE model offers extended support to 109 languages in a single model.BERTtransformer\nPerformance on Cross-lingual Text Retrieval\nWe evaluate the proposed model using the Tatoeba corpus, a dataset consisting of up to 1,000 English-aligned sentence pairs for 112 languages. For more than 30 of the languages in the dataset, the model has no training data. The model is tasked with finding the nearest neighbor translation for a given sentence, which it calculates using the cosine distance.Tatoeba corpuscosine distance\nTo understand the performance of the model for languages at the head or tail of the training data distribution, we divide the set of languages into several groups and compute the average accuracy for each set. The first 14-language group is selected from the languages supported by m~USE, which cover the languages from the head of the distribution (head languages). We also evaluate a second language group composed of 36 languages from the XTREME benchmark. The third 82-language group, selected from the languages covered by the LASER training data, includes many languages from the tail of the distribution (tail languages). Finally, we compute the average accuracy for all languages.XTREME benchmark\nThe table below presents the average accuracy achieved by LaBSE, compared to the m~USE and LASER models, for each language group. As expected, all models perform strongly on the 14-language group that covers most head languages. With more languages included, the averaged accuracy for both LASER and LaBSE declines. However, the reduction in accuracy from the LaBSE model with increasing numbers of languages is much less significant, outperforming LASER significantly, particularly when the full distribution of 112 languages is included (83.7% accuracy vs. 65.5%).Tatoeba Datasetsconvolutional neural networkTransformer\nSupport to Unsupported Languages\nThe average performance of all languages included in Tatoeba is very promising. Interestingly, LaBSE even performs relatively well for many of the 30+ Tatoeba languages for which it has no training data (see below). For one third of these languages the LaBSE accuracy is higher than 75% and only 8 have accuracy lower than 25%, indicating very strong transfer performance to languages without training data. Such positive language transfer is only possible due to the massively multilingual nature of LaBSE.ISO 639-1/639-2\nMining Parallel Text from Web\nLaBSE can be used for mining parallel text (bi-text) from web-scale data. For example, we applied LaBSE to CommonCrawl, a large-scale monolingual corpus, to process 560 million Chinese and 330 million German sentences for the extraction of parallel text. Each Chinese and German sentence pair is encoded using the LaBSE model and then the encoded embedding is used to find a potential translation from a pool of 7.7 billion English sentences pre-processed and encoded by the model. An approximate nearest neighbor search is employed to quickly search through the high-dimensional sentence embeddings. After a simple filtering, the model returns 261M and 104M potential parallel pairs for English-Chinese and English-German, respectively. The trained NMT model using the mined data reaches BLEU scores of 35.7 and 27.2 on the WMT translation tasks (wmt17 for English-to-Chinese and wmt14 for English-to-German). The performance is only a few points away from current state-of-art-models trained on high quality parallel data.CommonCrawlapproximate nearest neighbor searchWMT translation tasks\nConclusion\nWe're excited to share this research, and the model, with the community.  The pre-trained model is released at tfhub to support further research on this direction and possible downstream applications. We also believe that what we're showing here is just the beginning, and there are more important research problems to be addressed, such as building better models to support all languages.tfhub\nAcknowledgements\nThe core team includes Wei Wang, Naveen Arivazhagan, Daniel Cer. We would like to thank the Google Research Language team, along with our partners in other Google groups for their feedback and suggestions. Special thanks goes to Sidharth Mudgal, and Jax Law for help with data processing; as well as Jialu Liu, Tianqi Liu, Chen Chen, and Anosh Raj for help on BERT pre-training.",
      "link": "http://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html",
      "author": "Posted by Yinfei Yang and Fangxiaoyu Feng, Software Engineers, Google Research"
    },
    {
      "title": "On-device, Real-time Body Pose Tracking with MediaPipe BlazePose",
      "date": "Thursday, August 13, 2020",
      "abstract": "On-device, Real-time Body Pose Tracking with MediaPipe BlazePose\nPose estimation from video plays a critical role enabling the overlay of digital content and information on top of the physical world in augmented reality, sign language recognition, full-body gesture control, and even quantifying physical exercises, where it can form the basis for yoga, dance, and fitness applications. Pose estimation for fitness applications is particularly challenging due to the wide variety of possible poses (e.g., hundreds of yoga asanas), numerous degrees of freedom, occlusions (e.g. the body or other objects occlude limbs as seen from the camera), and a variety of appearances or outfits.augmented reality sign languagefull-body gesture controlquantifying physical exercisesasanas\nToday we are announcing the release of a new approach to human body pose perception, BlazePose, which we presented at the CV4ARVR workshop at CVPR 2020. Our approach provides human pose tracking by employing machine learning (ML) to infer 33, 2D landmarks of a body from a single frame. In contrast to current pose models based on the standard COCO topology, BlazePose accurately localizes more keypoints, making it uniquely suited for fitness applications. In addition, current state-of-the-art approaches rely primarily on powerful desktop environments for inference, whereas our method achieves real-time performance on mobile phones with CPU inference. If one leverages GPU inference, BlazePose achieves super-real-time performance, enabling it to run subsequent ML models, like face or hand tracking.BlazePosepresentedCV4ARVR workshopCVPR 2020COCO topology\nTopology \nThe current standard for human body pose is the COCO topology, which consists of 17 landmarks across the torso, arms, legs, and face. However, the COCO keypoints only localize to the ankle and wrist points, lacking scale and orientation information for hands and feet, which is vital for practical applications like fitness and dance. The inclusion of more keypoints is crucial for the subsequent application of domain-specific pose estimation models, like those for hands, face, or feet.COCO topology\nWith BlazePose, we present a new topology of 33 human body keypoints, which is a superset of COCO, BlazeFace and BlazePalm topologies. This allows us to determine body semantics from pose prediction alone that is consistent with face and hand models.BlazeFaceBlazePalm\nOverview: An ML Pipeline for Pose Tracking\nFor pose estimation, we utilize our proven two-step detector-tracker ML pipeline. Using a detector, this pipeline first locates the pose region-of-interest (ROI) within the frame. The tracker subsequently predicts all 33 pose keypoints from this ROI. Note that for video use cases, the detector is run only on the first frame. For subsequent frames we derive the ROI from the previous frame\u2019s pose keypoints as discussed below.provendetector-tracker ML pipeline\nPose Detection by extending BlazeFace\nFor real-time performance of the full ML pipeline consisting of pose detection and tracking models, each component must be very fast, using only a few milliseconds per frame. To accomplish this, we observe that the strongest signal to the neural network about the position of the torso is the person's face (due to its high-contrast features and comparably small variations in appearance). Therefore, we achieve a fast and lightweight pose detector by making the strong (yet for many mobile and web applications valid) assumption that the head should be visible for our single-person use case.\nConsequently, we trained a face detector, inspired by our sub-millisecond BlazeFace model, as a proxy for a pose detector. Note, this model only detects the location of a person within the frame and can not be used to identify individuals. In contrast to the Face Mesh and MediaPipe Hand tracking pipelines, where we derive the ROI from predicted keypoints, for the human pose tracking we explicitly predict two additional virtual keypoints that firmly describe the human body center, rotation and scale as a circle. Inspired by Leonardo\u2019s Vitruvian man, we predict the midpoint of a person's hips, the radius of a circle circumscribing the whole person, and the incline angle of the line connecting the shoulder and hip midpoints. This results in consistent tracking even for very complicated cases, like specific yoga asanas. The figure below illustrates the approach.BlazeFaceFace MeshMediaPipe HandLeonardo\u2019s Vitruvian man\nTracking Model\nThe pose estimation component of the pipeline predicts the location of all 33 person keypoints with three degrees of freedom each (x, y location and visibility) plus the two virtual alignment keypoints described above. Unlike current approaches that employ compute-intensive heatmap prediction, our model uses a regression approach that is supervised by a combined heat map/offset prediction of all keypoints, as shown below.heatmap\nSpecifically, during training we first employ a heatmap and offset loss to train the center and left tower of the network. We then remove the heatmap output and train the regression encoder (right tower), thus, effectively using the heatmap to supervise a lightweight embedding.heatmap and offset loss\nThe table below shows an ablation study of the model quality resulting from different training strategies. As an evaluation metric, we use the Percent of Correct Points with 20% tolerance (PCK@0.2) (where we assume the point to be detected correctly if the 2D Euclidean error is smaller than 20% of the  corresponding person\u2019s torso size).  To obtain a human baseline, we asked annotators to annotate several samples redundantly and obtained an average PCK@0.2 of 97.2. The training and validation have been done on a geo-diverse dataset of various poses, sampled uniformly.\nTo cover a wide range of customer hardware, we present two pose tracking models: lite and full, which are differentiated in the balance of speed versus quality. For performance evaluation on CPU, we use XNNPACK; for mobile GPUs, we use the TFLite GPU backend.XNNPACKTFLite GPU\nApplications \nBased on human pose, we can build a variety of applications, like fitness or yoga trackers. As an example, we present squats and push up counters, which can automatically count user statistics, or verify the quality of performed exercises. Such use cases can be implemented either using an additional classifier network or even with a simple joint pairwise distance lookup algorithm, which matches the closest pose in normalized pose space.\nConclusionWe have released a version of BlazePose targeting upper body use cases in MediaPipe running on Android, iOS and Python. BlazePose will also be made available to the broader mobile developer community via the Pose detection API in the upcoming release of ML Kit. Apart from the mobile domain, we preview our web-based in-browser version as well. We hope that providing this human pose perception functionality to the broader research and development community will result in an emergence of creative use cases, stimulating new applications, and new research avenues.BlazePoseMediaPipePose detection APIML Kitweb-based in-browser version\nWe plan to extend this technology with more robust and stable tracking to an even larger variety of human poses and activities. In the accompanying Model Card, we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with Google\u2019s AI Principles. We believe that publishing this technology can provide an impulse to new creative ideas and applications by the members of the research and developer community at large. We are excited to see what you can build with it!Model CardGoogle\u2019s AI Principles.\nAcknowledgments\nSpecial thanks to all our team members who worked on the tech with us: Fan Zhang, Artsiom Ablavatski, Yury Kartynnik, Tyler Zhu, Karthik Raveendran, Andrei Vakunov, Andrei Tkachenka, Marat Dukhan, Raman Sarokin, Tyler Mullen, Gregory Karpiak, Suril Shah, Buck Bourdon, Jiuqiang Tang, Ming Guang Yong, Chuo-Ling Chang, Juhyun Lee, Michael Hays, Camillo Lugaresi, Esha Uboweja, Siarhei Kazakou, Andrei Kulik, Matsvei Zhdanovich, and Matthias Grundmann.",
      "link": "http://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html",
      "author": "Posted by Valentin Bazarevsky and Ivan Grishchenko, Research Engineers, Google Research"
    },
    {
      "title": "REALM: Integrating Retrieval into Language Representation Models",
      "date": "Wednesday, August 12, 2020",
      "abstract": "REALM: Integrating Retrieval into Language Representation Models\nRecent advances in natural language processing have largely built upon the power of unsupervised pre-training, which trains general purpose language representation models using a large amount of text, without human annotations or labels. These pre-trained models, such as BERT and RoBERTa, have been shown to memorize a surprising amount of world knowledge, such as \u201cthe birthplace of Francesco Bartolomeo Conti\u201d, \u201cthe developer of JDK\u201d and \u201cthe owner of Border TV\u201d. While the ability to encode knowledge is especially important for certain natural language processing tasks such as question answering, information retrieval and text generation, these models memorize knowledge implicitly \u2014 i.e., world knowledge is captured in an abstract way in the model weights \u2014 making it difficult to determine what knowledge has been stored and where it is kept in the model. Furthermore, the storage space, and hence the accuracy of the model, is limited by the size of the network. To capture more world knowledge, the standard practice is to train ever-larger networks, which can be prohibitively slow or expensive.BERTRoBERTamemorize a surprising amount of world knowledgeFrancesco Bartolomeo ContiJDKBorder TVInstead, what if there was a method for pre-training that could access knowledge explicitly, e.g., by referencing an additional large external text corpus, in order to achieve accurate results without increasing the model size or complexity?\u00a0 For example, a sentence found in an external document collection, \"Francesco Bartolomeo Conti was born in Florence,\" could be referenced by the model to determine the birthplace of the musician, rather than relying on the model's opaque ability to access the knowledge stored in its own parameters. The ability to retrieve text containing explicit knowledge such as this would improve the efficiency of pre-training while enabling the model to perform well on knowledge-intensive tasks without using billions of parameters.\nIn \u201cREALM: Retrieval-Augmented Language Model Pre-Training\u201d, accepted at the 2020 International Conference on Machine Learning, we share a novel paradigm for language model pre-training, which augments a language representation model with a knowledge retriever, allowing REALM models to retrieve textual world knowledge explicitly from raw text documents, instead of memorizing all the knowledge in the model parameters. We have also open sourced the REALM codebase to demonstrate how one can train the retriever and the language representation jointly.REALM: Retrieval-Augmented Language Model Pre-Training2020 International Conference on Machine LearningREALM codebase\nBackground: Pre-training Language Representation Models\nTo understand how standard language representation models memorize world knowledge, one should first review how these models are pre-trained. Since the invention of BERT, the fill-in-the-blank task, called masked language modeling, has been widely used for pre-training language representation models.  Given any text with certain words masked out, the task is to fill back the missing words. An example of this task looks like:BERTmasked language modeling\nDuring pre-training, a model will go over a large number of examples and adjust the parameters in order to predict the missing words (answer: drink, in the above example). Interestingly, the fill-in-the-blank task makes the model memorize certain facts about the world. For example, the knowledge of Einstein's birthplace is required to fill the missing word in the following example:\nEinstein was a __-born scientist. (answer: German)\nHowever, because the world knowledge captured by the model is stored in the model weights, it is abstract, making it difficult to understand what information is stored.\nOur Proposal: Retrieval-Augmented Language Representation Model Pre-training\nIn contrast to standard language representation models, REALM augments the language representation model with a knowledge retriever that first retrieves another piece of text from an external document collection as the supporting knowledge \u2014 in our experiments, we use the Wikipedia text corpus \u2014 and then feeds this supporting text as well as the original text into a language representation model.Wikipedia text corpus\nThe key intuition of REALM is that a retrieval system should improve the model's ability to fill in missing words. Therefore, a retrieval that provides more context for filling the missing words should be rewarded. If the retrieved information does not help the model make its predictions, it should be discouraged, making room for better retrievals.\nHow does one train a knowledge retriever, given that only unlabeled text is available during pre-training? It turns out that one can use the task of filling words to train the knowledge retriever indirectly, without any human annotations. Assume the input of the query is:\nFilling the missing word (answer:pounds) in this sentence without retrieval can be tricky, as the model would need to have implicitly stored knowledge of the country in which the Buckingham Palace is located and the associated currency, as well as make the connection between the two. It would be easier for the model to fill in the missing word if it was presented with a passage that explicitly connects some of the necessary knowledge, retrieved from an external corpus.\n In this example, the retriever would be rewarded for retrieving the following sentence.\nSince the retrieval step needs to add more context, there may be multiple retrieval targets that could be helpful in filling the missing word, for example, \u201cThe official currency of the United Kingdom is the Pound.\u201d  The whole process is demonstrated in the next figure:\nComputational Challenges for REALM\nScaling REALM pre-training such that models can retrieve knowledge from millions of documents is challenging. In REALM, the selection of the best document is formulated as maximum inner product search (MIPS). To perform retrieval, MIPS models need to first encode all of the documents in the collection, such that each document has a corresponding document vector. When an input arrives, it is encoded as a query vector. In MIPS, given a query, the document in the collection that has the maximum inner product value between its document vector and the query vector is retrieved, as shown in the following figure:maximum inner product searchScaNN\nApplying REALM to Open-domain Question Answering\nWe evaluate the effectiveness of REALM by applying it to open-domain question answering (Open-QA), one of the most knowledge-intensive tasks in natural language processing. The goal of the task is to answer questions, such as \u201cWhat is the angle of the equilateral triangle?\u201dopen-domain question answering\nIn standard question answering tasks (e.g.,  SQuAD or Natural Questions), the supporting document is provided as part of input, so a model only needs to look up the answer in the given document. In Open-QA, there are no given documents, so that Open-QA models need to look up the knowledge by themselves \u2014 this makes Open-QA an excellent task to examine the effectiveness of REALM.SQuADNatural Questions\nThe following figure shows the results on the OpenQA version of Natural Question. We mainly compared our results with T5, another approach that trains models without annotated supporting documents. From the figure, one can clearly see that REALM pre-training generates very powerful Open-QA models, and even outperforms the much larger T5 (11B) model by almost 4 points, using only a fraction of the parameters (300M).Natural QuestionT5\nConclusion\nThe release of REALM has helped drive interest in developing end-to-end retrieval-augmented models, including a recent retrieval-augmented generative model. We look forward to the possibility of extending this line of work in several ways, including 1) applying REALM-like methods to new applications that require knowledge-intensive reasoning and interpretable provenance (beyond Open-QA), and 2) exploring the benefits of retrieving other forms of knowledge, such as images, knowledge graph structures, or even text in other languages. We are also excited to see what the research community does with the open source REALM codebase!retrieval-augmented generative modelREALM codebase\nAcknowledgements\nThis work has been a collaborative effort involving Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.",
      "link": "http://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html",
      "author": "Posted by Ming-Wei Chang and Kelvin Guu, Research Scientists, Google Research"
    },
    {
      "title": "A Simulation Suite for Tackling Applied Reinforcement Learning Challenges",
      "date": "Wednesday, August 12, 2020",
      "abstract": "A Simulation Suite for Tackling Applied Reinforcement Learning Challenges\nReinforcement Learning (RL) has proven to be effective in solving numerous complex problems ranging from Go, StarCraft and Minecraft to robot locomotion and chip design. In each of these cases, a simulator is available or the real environment is quick and inexpensive to access. Yet, there are still considerable challenges to deploying RL to real-world products and systems. For example, in physical control systems, such as robotics and autonomous driving, RL controllers are trained to solve tasks like grasping objects or driving on a highway. These controllers are susceptible to effects such as sensor noise, system delays, or normal wear-and-tear that can reduce the quality of input to the controller, leading to incorrect decision-making and potentially catastrophic failures.Reinforcement LearningGo,StarCraftMinecraftrobot locomotionchip designEveryday Robot ProjectX\nIn \u201cChallenges of Real-World Reinforcement Learning\u201d, we identify and discuss nine different challenges that hinder the application of current RL algorithms to applied systems. We then follow up this work with an empirical investigation in which we simulated versions of these challenges on state-of-the-art RL algorithms, and benchmark the effects of each. We have open-sourced these simulated challenges in the Real-World RL (RWRL) task suite to help draw attention to these important issues, as well as accelerate research toward solving them.Challenges of Real-World Reinforcement Learningempirical investigationReal-World RL\nThe RWRL Suite \nThe RWRL suite is a set of simulated tasks inspired by applied reinforcement learning challenges, the goal of which is to enable fast algorithmic iterations for both researchers and practitioners, without having to run slow, expensive experiments on real-systems. While there will be additional challenges transitioning from RL algorithms that were trained in simulation to real-world applications, this suite intends to close some of the more fundamental, algorithmic gaps. At present, RWRL supports a subset of the DeepMind Control Suite domains, but the goal is to broaden the suite to support an even more diverse domain set.DeepMind Control Suite\nEasy-to-Use & Flexible\nWe designed the suite with two main goals in mind. (1) It should be easy to use \u2014 a user should be able to start running experiments within minutes of downloading the suite, simply by changing a few lines of code. (2) It should be flexible \u2014 a user should be able to incorporate any combination of challenges into the environment with very little effort.\nA Delayed Action Example\nTo illustrate the ease of use of the RWRL suite, imagine a researcher or practitioner wants to implement action delays (i.e., temporal delays on actions being sent to the environment). To use the RWRL suite, simply import the rwrl module. Next, load an environment (e.g., cartpole) with the delay_spec argument. This optional argument is specified as a dictionary configuring delay applied to actions, observations, or rewards and the number of timesteps the corresponding element is delayed (e.g., 20 timesteps). Once the environment is loaded, the effects of actions are automatically delayed without any other changes to the experiment. This makes it easy to test an RL algorithm with action delays in a range of different environments supported by the RWRL suite.\nA user can combine different challenges or choose from a set of predefined benchmark challenges by simply adding additional arguments to the load function, all of which are specified in the open-source RWRL suite codebase.codebase\nSupported Challenges\nThe RWRL suite provides functionality to support experiments related to eight of the nine different challenges that make applying current RL algorithms on applied systems difficult: sample efficiency; system delays; high-dimensional state and action spaces; constraints; partial observability, stochasticity and non-stationarity; multiple objectives; real-time inference; and training from offline logs. RWRL excludes the explainability challenge, which is abstract and non-trivial to define. The supported experiments are non-exhaustive and provide researchers and practitioners with the ability to analyze the capabilities of their agent with respect to each challenge dimension. Examples of the supported challenges include:D4PGMuJoCo\nAs can be seen in the graphs, a researcher or practitioner can quickly gain insights as to which type of delay affects their agent\u2019s performance. These delays can also be combined together to observe their combined effect.codebaseoffline dataset releasenotebook\nConclusion\nMost systems rarely manifest only a single challenge, and we are excited to see how algorithms can deal with an environment in which there are multiple challenges combined with increasing levels of difficulty (\u2018Easy\u2019, \u2018Medium\u2019 and \u2018Hard\u2019). We highly encourage the research community to try and solve these challenges, as we believe that solving them will facilitate more widespread applications of RL to products and real-world systems.\nWhile the initial set of RWRL suite features and experiments provide a starting point for closing the gap between the current state of RL and the challenges of applied systems, there is still much work to do. The supported experiments are not exhaustive and we welcome new ideas from the wider community to better evaluate the capabilities of our RL agents. Our main goal with this suite is to highlight and encourage research on the core problems that limit the effectiveness of RL algorithms in applied products and systems and to accelerate progress towards enabling future RL applications.\nAcknowledgements\nWe would like to thank our core contributor and co-author Nir Levine for his invaluable help. We would also like to thank our co-authors Jerry Li, Sven Gowal, Todd Hester and Cosmin Paduraru as well as Robert Dadashi, the ACME team, Dan A. Calian, Juliet Rothenberg and Timothy Mann for their contributions.",
      "link": "http://ai.googleblog.com/2020/08/a-simulation-suite-for-tackling-applied.html",
      "author": "Posted by Daniel J. Mankowitz, Research Scientist, DeepMind and Gabriel Dulac-Arnold, Research Scientist, Google Research"
    },
    {
      "title": "On-device Supermarket Product Recognition",
      "date": "Tuesday, August 11, 2020",
      "abstract": "On-device Supermarket Product RecognitionMnasNetMobileNetsLookoutMediaPipeSIFToptical character recognitionN-GramsJaccard similarity coefficientTF-IDFIMUMnasNetMediaPipe Box trackingIntersection over UnionNASNetprincipal component analysisKNN searchScaNNk-meansproduct quantization",
      "link": "http://ai.googleblog.com/2020/07/on-device-supermarket-product.html",
      "author": "Posted by Chao Chen, Software Engineer, Google Research"
    },
    {
      "title": "MediaPipe Iris: Real-time Iris Tracking & Depth Estimation",
      "date": "Thursday, August 6, 2020",
      "abstract": "MediaPipe Iris: Real-time Iris Tracking & Depth Estimation\nA wide range of real-world applications, including computational photography (e.g., portrait mode and glint reflections) and augmented reality effects (e.g., virtual avatars) rely on estimating eye position by tracking the iris. Once accurate iris tracking is available, we show that it is possible to determine the metric distance from the camera to the user \u2014 without the use of a dedicated depth sensor. This, in-turn, can improve a variety of use cases, ranging from computational photography, over virtual try-on of properly sized glasses and hats to usability enhancements that adopt the font size depending on the viewer\u2019s distance.portrait modeaugmented reality effects\nIris tracking is a challenging task to solve on mobile devices, due to limited computing resources, variable light conditions and the presence of occlusions, such as hair or people squinting. Often, sophisticated specialized hardware is employed, limiting the range of devices on which the solution could be applied.FaceMesh\nToday, we announce the release of MediaPipe Iris, a new machine learning model for accurate iris estimation. Building on our work on MediaPipe Face Mesh, this model is able to track landmarks involving the iris, pupil and the eye contours using a single RGB camera, in real-time, without the need for specialized hardware. Through use of iris landmarks, the model is also able to determine the metric distance between the subject and the camera with relative error less than 10% without the use of depth sensor. Note that iris tracking does not infer the location at which people are looking, nor does it provide any form of identity recognition. Thanks to the fact that this system is implemented in MediaPipe \u2014 an open source cross-platform framework for researchers and developers to build world-class ML solutions and applications \u2014 it can run on most modern mobile phones, desktops, laptops and even on the web.MediaPipe IrisMediaPipe Face MeshMediaPipeon the web\nAn ML Pipeline for Iris Tracking\nThe first step in the pipeline leverages our previous work on 3D Face Meshes, which uses high-fidelity facial landmarks to generate a mesh of the approximate face geometry. From this mesh, we isolate the eye region in the original image for use in the iris tracking model. The problem is then divided into two parts: eye contour estimation and iris location. We designed a multi-task model consisting of a unified encoder with a separate component for each task, which allowed us to use task-specific training data.3D Face Meshes\nTo train the model from the cropped eye region, we manually annotated ~50k images, representing a variety of illumination conditions and head poses from geographically diverse regions, as shown below.Depth-from-Iris: Depth Estimation from a Single Image\nOur iris-tracking model is able to determine the metric distance of a subject to the camera with less than 10% error, without requiring any specialized hardware. This is done by relying on the fact that the horizontal iris diameter of the human eye remains roughly constant at 11.7\u00b10.5 mm across a wide population [1, 2, 3, 4], along with some simple geometric arguments. For illustration, consider a pinhole camera model projecting onto a sensor of square pixels. The distance to a subject can be estimated from facial landmarks by using the focal length of the camera, which can be obtained using camera capture APIs or directly from the EXIF metadata of a captured image, along with other camera intrinsic parameters. Given the focal length, the distance from the subject to the camera is directly proportional to the physical size of the subject\u2019s eye, as visualized below.1234focal lengthEXIF metadata\nIn order to quantify the accuracy of the method, we compared it to the depth sensor on an iPhone 11 by collecting front-facing, synchronized video and depth images on over 200 participants. We experimentally verified the error of the iPhone 11 depth sensor to be < 2% for distances up to 2 meters, using a laser ranging device. Our evaluation shows that our approach for depth estimation using iris size has a mean relative error of 4.3% and standard deviation of 2.4%. We tested our approach on participants with and without eyeglasses (not accounting for contact lenses on participants) and found that eyeglasses increase the mean relative error slightly to 4.8% (standard deviation 3.1%). We did not test this approach on participants with any eye diseases (like arcus senilis or pannus). Considering MediaPipe Iris requires no specialized hardware, these results suggest it may be possible to obtain metric depth from a single image on devices with a wide range of cost-points.arcus senilispannus\nRelease of MediaPipe Iris\nWe are releasing the iris and depth estimation models as a cross-platform MediaPipe pipeline that can run on desktop, mobile and the web. As described in our recent Google Developer Blog post on MediaPipe on the web, we leverage WebAssembly and XNNPACK to run our Iris ML pipeline locally in the browser, without any data being sent to the cloud.Google Developer Blog post on MediaPipe on the webWebAssemblyXNNPACKMediaPipe\u2019s WASMherehere\nFuture Directions\nWe plan to extend our MediaPipe Iris model with even more stable tracking for lower error and deploy it for accessibility use cases. We strongly believe in sharing code that enables reproducible research, rapid experimentation, and development of new ideas in different areas. In our documentation and the accompanying Model Card, we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with Google\u2019s AI Principles. Note, that any form of surveillance or identification is explicitly out of scope and not enabled by this technology. We hope that providing this iris perception functionality to the wider research and development community will result in an emergence of creative use cases, stimulating responsible new applications and new research avenues.documentationModel CardGoogle\u2019s AI Principlesproviding this iris perception functionality\nFor more ML solutions from MediaPipe, please see our solutions page and Google Developer blog for the latest updates.solutions pageGoogle Developer blog\nAcknowledgements\nWe would like to thank Artsiom Ablavatski, Andrei Tkachenka, Buck Bourdon, Ivan Grishchenko and Gregory Karpiak for support in model evaluation and data collection; Yury Kartynnik, Valentin Bazarevsky, Artsiom Ablavatski for developing the mesh technology; Aliaksandr Shyrokau and the annotation team for their diligence to data preparation; Vidhya Navalpakkam, Tomer Shekel, Kai Kohlhoff for their domain expertise, Fan Zhang, Esha Uboweja, Tyler Mullen, Michael Hays and Chuo-Ling Chang for help to integrate the model to MediaPipe; Matthias Grundmann, Florian Schroff and Ming Guang Yong for continuous help for building this technology.",
      "link": "http://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html",
      "author": "Posted by Andrey Vakunov and Dmitry Lagun, Research Engineers, Google Research"
    },
    {
      "title": "Live HDR+ and Dual Exposure Controls on Pixel 4 and 4a",
      "date": "Monday, August 3, 2020",
      "abstract": "Live HDR+ and Dual Exposure Controls on Pixel 4 and 4aHigh dynamic range (HDR) imagingengine behind HDR imagingHDR+ burst photographyWYSIWYGherereduce noiseGPUsHDRnetdeep neural networkexposure compensationherehere",
      "link": "http://ai.googleblog.com/2020/08/live-hdr-and-dual-exposure-controls-on.html",
      "author": "Posted by Jiawen Chen and Sam Hasinoff, Software Engineers, Google Research"
    },
    {
      "title": "Introducing the Model Card Toolkit for Easier Model Transparency Reporting",
      "date": "Wednesday, July 29, 2020",
      "abstract": "Introducing the Model Card Toolkit for Easier Model Transparency ReportingModel Cardslaunched Model Cards publiclyMediaPipetheir GitHub repositoryModel Card ToolkitColab tutorialUCI Census Income datasetJSON schemaML Metadataone UI templateTensorFlow ExtendedGoogle Cloud Platformget started herethe Colab tutorialmodel-cards@google.comTensorFlow Responsible AI page",
      "link": "http://ai.googleblog.com/2020/07/introducing-model-card-toolkit-for.html",
      "author": "Posted by Huanming Fang and Hui Miao, Software Engineers, Google Research"
    },
    {
      "title": "Announcing ScaNN: Efficient Vector Similarity Search",
      "date": "Tuesday, July 28, 2020",
      "abstract": "Announcing ScaNN: Efficient Vector Similarity SearchICML 2020Accelerating Large-Scale Inference with Anisotropic Vector Quantization,\u201dvector similarity search libraryann-benchmarks.comnearest neighbor searchinner productmaximum inner-product searchSeveral state-of-the-art solutionslearned quantizationglove-100-angular benchmarkann-benchmarks.com*GitHubstate-of-the-art performance\n* ScaNN performs similarly well on the other datasets of ann-benchmarks.com, but the website currently shows outdated, lower numbers. See this pull request for more representative performance figures on other datasets. \u21a9*ann-benchmarks.compull request\u21a9",
      "link": "http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html",
      "author": "Posted by Philip Sun, Software Engineer, Google Research"
    },
    {
      "title": "Improving Holistic Scene Understanding with Panoptic-DeepLab",
      "date": "Tuesday, July 21, 2020",
      "abstract": "Improving Holistic Scene Understanding with Panoptic-DeepLabinstance segmentationsemantic segmentationsemantic segmentationPanoptic segmentationMask R-CNNPanoptic-DeepLab: a simple, fast and strong system for panoptic segmentationCVPR 2020DeepLabbounding box predictionsUPSNetImageNetatrous spatial pyramid poolingDeepLabdecoder modulesatrous convolutionDeepLabV3+CityscapesMapillary VistasCOCOCityscapesMapillary Panoptic Segmentation trackICCV 2019 Joint COCO and Mapillary Recognition Challenge WorkshopCOCOMask R-CNN",
      "link": "http://ai.googleblog.com/2020/07/improving-holistic-scene-understanding.html",
      "author": "Posted by Bowen Cheng, Student Researcher and Liang-Chieh Chen, Research Scientist, Google Research"
    },
    {
      "title": "Exploring Faster Screening with Fewer Tests via Bayesian Group Testing",
      "date": "Tuesday, July 14, 2020",
      "abstract": "Exploring Faster Screening with Fewer Tests via Bayesian Group TestingRobert Dorfmanpaperinformation theorycombinatoricscompressive sensingbinary splittingindividualinfection probabilityNoisy Adaptive Group Testing using Bayesian Sequential Experimental Designour GitHub reposensitivityspecificityextremely large numbersequential Monte CarloBayesian optimal experimental designinformation gainmutual informationarea under the ROC curvegreedyorigami assaysinformative Dorfman approachcurrentPCR machinesvarious institutions",
      "link": "http://ai.googleblog.com/2020/07/exploring-faster-screening-with-fewer.html",
      "author": "Posted by Marco Cuturi and Jean-Philippe Vert, Research Scientists, Google Research, Brain Team"
    },
    {
      "title": "Google at ICML 2020",
      "date": "Monday, July 13, 2020",
      "abstract": "Google at ICML 2020Platinum SponsorInternational Conference on Machine LearningGoogle Dataset Search: Building an Open Ecosystem for Dataset DiscoveryEnd-to-end Bayesian inference workflows in TensorFlow ProbabilityPopulation-Based Black-Box Optimization for Biological Sequence DesignPredictive Coding for Locally-Linear ControlFedBoost: A Communication-Efficient Algorithm for Federated LearningFaster Graph Embeddings via CoarseningRevisiting Fundamentals of Experience ReplayBoosting for Control of Dynamical SystemsNeural Clustering ProcessesThe Tree Ensemble Layer: Differentiability Meets Conditional ComputationRepresentations for Stable Off-Policy Reinforcement LearningREALM: Retrieval-Augmented Language Model Pre-TrainingContext Aware Local Differential PrivacyScalable Deep Generative Modeling for Sparse GraphsDeep k-NN for Noisy LabelsRevisiting Spatial Invariance with Low-Rank Local ConnectivitySCAFFOLD: Stochastic Controlled Averaging for Federated LearningIncremental Sampling Without Replacement for Sequence ModelsSoftSort: A Continuous Relaxation for the argsort OperatorXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisationblog postLearning to Stop While Learning to PredictBandits with Adversarial ScalingSimGANs: Simulator-Based Generative Adversarial Networks for ECG Synthesis to Improve Deep ECG ClassificationStochastic Frank-Wolfe for Constrained Finite-Sum MinimizationImplicit differentiation of Lasso-type models for hyperparameter optimizationInfinite attention: NNGP and NTK for deep attention networksLogarithmic Regret for Learning Linear Quadratic Regulators EfficientlyAdversarial Learning Guarantees for Linear Hypotheses and Neural NetworksRandom Hypervolume Scalarizations for Provable Multi-Objective Black Box OptimizationGenerating Programmatic Referring Expressions via Program SynthesisOptimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching ApproachAutoML-Zero: Evolving Machine Learning Algorithms From Scratchblog postHow Good is the Bayes Posterior in Deep Neural Networks Really?Which Tasks Should Be Learned Together in Multi-task Learning?Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit ProblemsDisentangling Trainability and Generalization in Deep Neural NetworksThe Many Shapley Values for Model ExplanationNeural Contextual Bandits with UCB-based ExplorationAutomatic Shortcut Removal for Self-Supervised Representation LearningFederated Learning with Only Positive LabelsHow Recurrent Networks Implement Contextual Processing in Sentiment AnalysisSupervised Learning: No Loss No CryReady Policy One: World Building Through Active LearningWeakly-Supervised Disentanglement Without CompromisesFast Differentiable Sorting and RankingDebiased Sinkhorn barycentersInterpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device FailureAccelerating Large-Scale Inference with Anisotropic Vector QuantizationAn Optimistic Perspective on Offline Reinforcement Learningblog postThe Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of GeneralizationPrivate Query Release Assisted by Public DataLearning and Evaluating Contextual Embedding of Source CodeEvaluating Machine Accuracy on ImageNetImputer: Sequence Modelling via Imputation and Dynamic ProgrammingDomain Aggregation Networks for Multi-Source Domain AdaptationPlanning to Explore via Self-Supervised World ModelsContext-Aware Dynamics Model for Generalization in Model-Based Reinforcement LearningRetro*: Learning Retrosynthetic Planning with Neural Guided A* SearchOn the Consistency of Top-k Surrogate LossesDual Mirror Descent for Online Allocation ProblemsEfficient and Scalable Bayesian Neural Nets with Rank-1 FactorsBatch Stationary Distribution EstimationSmall-GAN: Speeding Up GAN Training Using Core-SetsData Valuation Using Reinforcement LearningA Game Theoretic Perspective on Model-Based Reinforcement LearningEncoding Musical Style with Transformer AutoencodersThe Shapley Taylor Interaction IndexMultidimensional Shape ConstraintsPrivate Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication OverheadLearning to Score Behaviors for Guided Policy OptimizationFundamental Tradeoffs between Invariance and Sensitivity to Adversarial PerturbationsOptimizing Black-Box Metrics with Adaptive SurrogatesCircuit-Based Intrinsic Methods to Detect OverfittingAutomatic Reparameterisation of Probabilistic ProgramsStochastic Flows and Geometric Optimization on the Orthogonal GroupBlack-Box Variational Inference as a Parametric Approximation to Langevin DynamicsConcise Explanations of Neural Networks Using Adversarial Trainingp-Norm Flow Diffusion for Local Graph ClusteringEmpirical Study of the Benefits of Overparameterization in Learning Latent Variable ModelsRobust Pricing in Dynamic Mechanism DesignDifferentiable Product Quantization for Learning Compact Embedding LayersAdaptive Region-Based Active LearningCountering Language Drift with Seeded Iterated LearningDoes Label Smoothing Mitigate Label Noise?Acceleration Through Spectral Density EstimationMomentum Improves Normalized SGDConQUR: Mitigating Delusional Bias in Deep Q-LearningOnline Learning with Imperfect HintsGo Wide, Then Narrow: Efficient Training of Deep Thin NetworksOn Implicit Regularization in \u03b2-VAEsIs Local SGD Better than Minibatch SGD?A Simple Framework for Contrastive Learning of Visual RepresentationsUniversal Average-Case Optimality of Polyak MomentumAn Imitation Learning Approach for Cache ReplacementCollapsed Amortized Variational Inference for Switching Nonlinear Dynamical SystemsBeyond Synthetic Noise: Deep Learning on Controlled Noisy LabelsOptimizing Data Usage via Differentiable RewardsSparse Sinkhorn AttentionOne Policy to Control Them All: Shared Modular Policies for Agent-Agnostic ControlOn Thompson Sampling with Langevin AlgorithmsGood Subnetworks Provably Exist: Pruning via Greedy Forward SelectionOn the Global Convergence Rates of Softmax Policy Gradient MethodsConcept Bottleneck ModelsSupervised Quantile Normalization for Low-Rank Matrix ApproximationMissing Data Imputation Using Optimal TransportLearning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention Over ModulesStochastic Optimization for Regularized Wasserstein EstimatorsLow-Rank Bottleneck in Multi-head Attention ModelsRigging the Lottery: Making All Tickets WinnersOnline Learning with Dependent Stochastic Feedback GraphsCalibration, Entropy Rates, and Memory in Language ModelsComposable Sketches for Functions of Frequencies: Beyond the Worst CaseEnergy-Based Processes for Exchangeable DataNear-Optimal Regret Bounds for Stochastic Shortest PathPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarizationblog postThe Complexity of Finding Stationary Points with Stochastic Gradient DescentThe k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural NetworksRegularized Optimal Transport is Ground Cost AdversarialNew In MLLatinX in AIWomen in Machine Learning Un-WorkshopQueer in AIWorkshop on Continual Learning5th ICML Workshop on Human Interpretability in Machine Learning (WHI)Self-supervision in Audio and SpeechWorkshop on eXtreme Classification: Theory and ApplicationsHealthcare Systems, Population Health, and the Role of Health-techTheoretical Foundations of Reinforcement LearningUncertainty and Robustness in Deep Learning Workshop (UDL)Beyond First Order Methods in Machine Learning SystemsObject-Oriented Learning: Perception, Representation, and ReasoningGraph Representation Learning and Beyond (GRL+)ML Interpretability for Scientific DiscoveryNegative Dependence and Submodularity for Machine Learning7th ICML Workshop on Automated Machine Learning (AutoML)Federated Learning for User Privacy and Data ConfidentialityMLRetrospectives: A Venue for Self-Reflection in ML ResearchMachine Learning for Media DiscoveryINNF+: Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models4th Lifelong Learning Workshop2nd ICML Workshop on Human in the Loop Learning (HILL)Machine Learning for Global Health",
      "link": "http://ai.googleblog.com/2020/07/google-at-icml-2020.html",
      "author": "Posted by Jaqui Herman and Cat Armato, Program Managers"
    },
    {
      "title": "Grounding Natural Language Instructions to Mobile UI Actions",
      "date": "Friday, July 10, 2020",
      "abstract": "Grounding Natural Language Instructions to Mobile UI ActionsMapping Natural Language Instructions to Mobile UI Action SequencesACL 2020GitHub repositorylanguage groundingTransformerarea attentionpublic android UI corpusgraph convolutional networkfeedforward network",
      "link": "http://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html",
      "author": "Posted by Yang Li, Research Scientist, Google Research"
    },
    {
      "title": "AutoML-Zero: Evolving Code that Learns",
      "date": "Thursday, July 9, 2020",
      "abstract": "AutoML-Zero: Evolving Code that Learnsdeep neural networksAutoMLneural architecture searchtopicmuchresearchlearning rulesince the early 90sour researchrecent paperbackpropagationlinear regressionvariantevolutionary methodsproved usefulsince the 80smakes them especially suitable for the discovery of learning algorithmsmigrationcodestochastic gradient descentnoise injection as data augmentationbilinear modelgradient normalizationweight averagingnoisy ReLU** The electricity consumption for the experiments (run in 2019) was matched with the purchase of renewable energy. \u21a9*\u21a9",
      "link": "http://ai.googleblog.com/2020/07/automl-zero-evolving-code-that-learns.html",
      "author": "Posted by Esteban Real, Staff Software Engineer and Chen Liang, Software Engineer, Google Research, Brain Team"
    },
    {
      "title": "Duality \u2014 A New Approach to Reinforcement Learning",
      "date": "Wednesday, July 8, 2020",
      "abstract": "Duality \u2014 A New Approach to Reinforcement LearningReinforcement learningQ-learningactor-criticconstraint-satisfaction problemmathematical issuesReinforcement Learning via Fenchel-Rockafellar Dualityconvex dualityconvex regularizerf-divergenceoff-policyofflinepolicy optimizationpolicy evaluationimitation learning",
      "link": "http://ai.googleblog.com/2020/07/duality-new-approach-to-reinforcement.html",
      "author": "Posted by Ofir Nachum and Bo Dai, Research Scientists, Google Research"
    },
    {
      "title": "Google at ACL 2020",
      "date": "Monday, July 6, 2020",
      "abstract": "Google at ACL 2020ACL 2020Diamond Level sponsor of ACL 2020Google virtual boothCross-modal Language Generation using Pivot Stabilization for Web-scale Language CoverageAutomatic Detection of Generated Text is Easiest when Humans are FooledOn Faithfulness and Factuality in Abstractive SummarizationMobileBERT: a Compact Task-Agnostic BERT for Resource-Limited DevicesBabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby StepsDynamic Programming Encoding for Subword Segmentation in Neural Machine TranslationGoEmotions: A Dataset of Fine-Grained EmotionsTaPas: Weakly Supervised Table Parsing via Pre-trainingblog postToxicity Detection: Does Context Really Matter?(Re)construing Meaning in NLPPretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language ModelsProbabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question AnsweringAdvAug: Robust Adversarial Augmentation for Neural Machine TranslationNamed Entity Recognition as Dependency ParsingCross-modal Coherence Modeling for Caption GenerationRepresentation Learning for Information Extraction from Form-like Documentsblog postLow-Dimensional Hyperbolic Knowledge Graph EmbeddingsWhat Question Answering can Learn from Trivia NerdsLearning a Multi-Domain Curriculum for Neural Machine Translationblog postTranslationese as a Language in \"Multilingual\" NMTMapping Natural Language Instructions to Mobile UI Action Sequencesblog postBLEURT: Learning Robust Metrics for Text Generationblog postExploring Unexplored Generalization Challenges for Cross-Database Semantic ParsingFrugal Paradigm CompletionReverse Engineering Configurations of Neural Text Generation ModelsSyntactic Data Augmentation Increases Robustness to Inference HeuristicsLeveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine TranslationSocial Biases in NLP Models as Barriers for Persons with DisabilitiesToward Better Storylines with Sentence-Level Language ModelsTYDI QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languagesblog post)Phonotactic Complexity and Its Trade-offsMultilingual Universal Sentence Encoder for Semantic Retrievalblog postIWPT - The 16th International Conference on Parsing TechnologiesALVR - Workshop on Advances in Language and Vision ResearchWNGT - The 4th Workshop on Neural Generation and TranslationNLPMC - NLP for Medical ConversationsAutoSimTrans - The 1st Workshop on Automatic Simultaneous TranslationInterpretability and Analysis in Neural NLP (cutting-edge)Commonsense Reasoning for Natural Language Processing (Introductory)",
      "link": "http://ai.googleblog.com/2020/07/google-at-acl-2020.html",
      "author": "Posted by Cat Armato and Emily Knapp, Program Managers"
    },
    {
      "title": "SmartReply for YouTube Creators",
      "date": "Wednesday, July 1, 2020",
      "abstract": "SmartReply for YouTube CreatorsSmartReplyGmail launchAndroid MessagesAndroid WearPlay Developer ConsoleMLKitTFLiteSmartReply for Inboxrecurrent neural networkwe foundmaximum inner product searchexpand SmartReply to Gmailour recent workresearchTransformerself-attention layersWaveNetcontrastive objectiveaverage poolingcontrastive objective",
      "link": "http://ai.googleblog.com/2020/07/smartreply-for-youtube-creators.html",
      "author": "Posted by Rami Al-Rfou, Research Scientist, Google Research"
    },
    {
      "title": "SpineNet: A Novel Architecture for Object Detection Discovered with Neural Architecture Search",
      "date": "Tuesday, June 30, 2020",
      "abstract": "SpineNet: A Novel Architecture for Object Detection Discovered with Neural Architecture SearchConvolutional neural networksobject detectionsegmentationFPNDeepLabv3+ResNetCVPR 2020SpineNet: Learning Scale-Permuted Backbone for Recognition and Localizationneural architecture searchTensorflow TPU GitHub repositoryTensorFlow Model Garden GitHub repositoryCOCO datasetResNet-50ResNet-50-FPNaverage precisionFLOPsresidual blockbottleneck blockResNetCOCOiNaturalistCOCOTensorBoard.devCOCO",
      "link": "http://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html",
      "author": "Posted by Xianzhi Du, Software Engineer and Jaeyoun Kim, Technical Program Manager, Google Research"
    },
    {
      "title": "Leveraging Temporal Context for Object Detection",
      "date": "Friday, June 26, 2020",
      "abstract": "Leveraging Temporal Context for Object Detectionmountain pass road conditionsecosystem phenologyleading to poor generalization to novel deploymentsLILA BCWildlife InsightsSnapshot SerengetiContext R-CNN: Long Term Temporal Context for Per-Camera Object DetectionFaster R-CNNTF Object Detection APIFaster R-CNNattentionFaster R-CNNsimilarity-based attentionSnapshot SerengetiSnapshot SerengetiCaltech Camera Trapsmean average precisionS3Dmean average precisionaverage recallWildlife InsightsiWildCamCVPR Fine-Grained Visual Recognition WorkshopCaltech Computational Vision Lab",
      "link": "http://ai.googleblog.com/2020/06/leveraging-temporal-context-for-object.html",
      "author": "Posted by Sara Beery, Student Researcher and Jonathan Huang, Research Scientist, Google Research"
    },
    {
      "title": "Sensing Force-Based Gestures on the Pixel 4",
      "date": "Wednesday, June 24, 2020",
      "abstract": "Sensing Force-Based Gestures on the Pixel 4recent update to the Pixel 4dielectricconvolutionalrecurrentTensorFlow LiteGestureDetectorViewMotionEvent classification",
      "link": "http://ai.googleblog.com/2020/06/sensing-force-based-gestures-on-pixel-4.html",
      "author": "Posted by Philip Quinn and Wenxin Feng, Research Scientists, Android UX"
    },
    {
      "title": "Presenting a Challenge and Workshop in Efficient Open-Domain Question Answering",
      "date": "Tuesday, June 23, 2020",
      "abstract": "Presenting a Challenge and Workshop in Efficient Open-Domain Question Answeringknowledge graphWikipediaT5EfficientQA competition and workshopNeurIPS 2020Princeton UniversityUniversity of Washingtonquestion answeringNatural Questions datasetcompetition track at NeurIPS 20202017 NeurIPS Human-Computer competitioncompetition site",
      "link": "http://ai.googleblog.com/2020/06/presenting-challenge-and-workshop-in.html",
      "author": "Posted by Eunsol Choi, Visiting Faculty Researcher and Tom Kwiatkowski, Research Scientist, Google Research"
    },
    {
      "title": "RepNet: Counting Repetitions in Videos",
      "date": "Monday, June 22, 2020",
      "abstract": "RepNet: Counting Repetitions in VideosCounting Out Time: Class Agnostic Video Repetition Counting in the WildRepNetour previous work, whichdatasetColab notebookIn the pastResNetResNetimagevideoTransformersPERTUBEaffine motionKinetics datasetCountixHimawari satelliteimgur.comleftrightNatureprincipal component analysisannotationsColab notebook",
      "link": "http://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html",
      "author": "Posted by Debidatta Dwibedi, Research Scientist, Robotics at Google"
    },
    {
      "title": "Improving Speech Representations and Personalized Models Using Self-Supervision",
      "date": "Thursday, June 18, 2020",
      "abstract": "Improving Speech Representations and Personalized Models Using Self-Supervisionautomatic speech recognitionBERTALBERTInception layersSimCLRT5Visual Task Adaptation BenchmarkTowards Learning a Universal Non-Semantic Representation of SpeechThese datasetsTensorFlow DatasetsTRIpLet Loss networkopen-source the codeinternet of thingsadded the six datasets in our benchmark to TensorFlow Datasetsopen sourced the evaluation frameworkpersonalizing speech recognizersvoice imitation text-to-speech from few samplesAudioSetpreviousworkBERTMobileNetResNet50OpenSMILEstrong baselinesInterspeech 2020 Computational Paralinguistics Challenge (ComParE)baseline modelGitHubTensorFlow DatasetsAI Hub",
      "link": "http://ai.googleblog.com/2020/06/improving-speech-representations-and.html",
      "author": "Posted by Joel Shor, Software Engineer and Oran Lang, Software Engineer, Google Research, Israel"
    },
    {
      "title": "Using Selective Attention in Reinforcement Learning Agents",
      "date": "Thursday, June 18, 2020",
      "abstract": "Using Selective Attention in Reinforcement Learning AgentsInattentional blindnessselective attentiondeepreinforcement learningpredictingfuturesequencesGECCO 2020Neuroevolution of Self-Interpretable AgentsCarRacingDoomTakeCoverseveralworksabilitiesself-attentionlong short-term memorybackpropagationdifferentiablederivative-free optimizationVizDoomTakeCoverDoomTakeCoverCarRacing-v0CarRacingvideovideoCarRacingExtension",
      "link": "http://ai.googleblog.com/2020/06/using-selective-attention-in.html",
      "author": "Posted by Yujin Tang, Research Software Engineer and David Ha, Staff Research Scientist, Google Research, Tokyo"
    },
    {
      "title": "Machine Learning-based Damage Assessment for Disaster Relief",
      "date": "Tuesday, June 16, 2020",
      "abstract": "Machine Learning-based Damage Assessment for Disaster ReliefBuilding Damage Detection in Satellite Imagery Using Convolutional Neural NetworksUnited Nations World Food Program (WFP) Innovation Acceleratorconvolutional neural networkhistogram equalizationUNOSATREACHGoogle Earth EngineROCtrue positive and false positive ratesUnited Nations World Food Programme (WFP) Innovation Accelerator",
      "link": "http://ai.googleblog.com/2020/06/machine-learning-based-damage.html",
      "author": "Posted by Joseph Xu, Senior Software Engineer and Pranav Khaitan, Engineering Lead, Google Research"
    },
    {
      "title": "Google at CVPR 2020",
      "date": "Monday, June 15, 2020",
      "abstract": "Google at CVPR 20202020 Conference on Computer Vision and Pattern Recognitionmain conferenceworkshopstutorialsSupporter Level Virtual Sponsorvirtual boothmachine perceptionEvolving Losses for Unsupervised Video Representation LearningCvxNet: Learnable Convex DecompositionNeural SDE: Stabilizing Neural ODE Networks with Stochastic NoiseScalability in Perception for Autonomous Driving: Waymo Open DatasetDeep Implicit Volume CompressionNeural Networks Are More Productive Teachers Than Human Raters: Active Mixup for Data-Efficient Knowledge Distillation from a Blackbox ModelGoogle Landmarks Dataset v2 - A Large-Scale Benchmark for Instance-Level Recognition and Retrievalblog postCycleISP: Real Image Restoration via Improved Data SynthesisDynamic Graph Message Passing NetworksLocal Deep Implicit Functions for 3D ShapeGHUM & GHUML: Generative 3D Human Shape and Articulated Pose ModelsSearch to Distill: Pearls are Everywhere but not the EyesSemantic Pyramid for Image GenerationFlow Contrastive Estimation of Energy-Based ModelsRethinking Class-Balanced Methods for Long-Tailed Visual Recognition from A Domain Adaptation PerspectiveCategory-Level Articulated Object Pose EstimationAdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency LossSpeedNet: Learning the Speediness in VideosBSP-Net: Generating Compact Meshes via Binary Space PartitioningSAPIEN: A SimulAted Part-based Interactive ENvironmentSurfelGAN: Synthesizing Realistic Sensor Data for Autonomous DrivingFilter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural NetworksRL-CycleGAN: Reinforcement Learning Aware Simulation-To-RealOpen Compound Domain AdaptationSingle-view view synthesis with multiplane imagesAdversarial Examples Improve Image RecognitionAdversarial Texture Optimization from RGB-D ScansSingle-Image HDR Reconstruction by Learning to Reverse the Camera PipelineCollaborative Distillation for Ultra-Resolution Universal Style TransferLearning to AutofocusMulti-Scale Boosted Dehazing Network with Dense Feature FusionComposing Good Shots by Exploiting Mutual RelationsPatchVAE: Learning Local Latent Codes for RecognitionNeural Voxel Renderer: Learning an Accurate and Controllable Rendering ToolLocal Implicit Grid Representations for 3D ScenesLarge Scale Video Representation Learning via Relational Graph ClusteringDeep Homography Estimation for Dynamic ScenesC-Flow: Conditional Generative Flow Models for Images and 3D Point CloudsLighthouse: Predicting Lighting Volumes for Spatially-Coherent IlluminationScale-space flow for end-to-end optimized video compressionStructEdit: Learning Structural Shape Variations3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance SegmentationSequential mastery of multiple tasks: Networks naturally learn to learn and forget to forgetDistilling Effective Supervision from Severe Label NoiseViewAL: Active Learning With Viewpoint Entropy for Semantic SegmentationAttribution in Scale and SpaceWeakly-Supervised Semantic Segmentation via Sub-category ExplorationSpeech2Action: Cross-modal Supervision for Action RecognitionCounting Out Time: Class Agnostic Video Repetition Counting in the WildThe Garden of Forking Paths: Towards Multi-Future Trajectory PredictionSelf-training with Noisy Student improves ImageNet classificationEfficientDet: Scalable and Efficient Object Detectionblog postACNe: Attentive Context Normalization for Robust Permutation-Equivariant LearningVectorNet: Encoding HD Maps and Agent Dynamics from Vectorized RepresentationSpineNet: Learning Scale-Permuted Backbone for Recognition and LocalizationKeyPose: Multi-View 3D Labeling and Keypoint Estimation for Transparent ObjectsStructured Multi-Hashing for Model CompressionDOPS: Learning to Detect 3D Objects and Predict their 3D ShapesPanoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic SegmentationContext R-CNN: Long Term Temporal Context for Per-Camera Object DetectionDistortion Agnostic Deep WatermarkingCan weight sharing outperform random architecture search? An investigation with TuNASGIFnets: Differentiable GIF Encoding FrameworkYour Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative ModelsFast Sparse ConvNetsRetinaTrack: Online Single Stage Joint Detection and TrackingLearning to See Through ObstructionsSelf-Supervised Learning of Video-Induced Visual Invariances3rd Workshop and Challenge on Learned Image CompressionCLVISION 1st Workshop on Continual Learning in Computer VisionEmbodied AIThe 1st International Workshop and Prize Challenge on Agriculture-Vision: Challenges & Opportunities for Computer Vision in AgricultureEmbodied AINew Trends in Image Restoration and Enhancement workshop and challenges on image and video restoration and enhancement (NTIRE)The End-of-End-to-End A Video Understanding Pentathlon4th Workshop on Media Forensics4th Workshop on Visual Understanding by Learning from Web DataAI for Content CreationFourth Workshop on Computer Vision for AR/VRLow-Power Computer Vision Competition (LPCVC)Sight and SoundWorkshop on Efficient Deep Learning for Computer VisionExtreme classification in computer visionImage Matching: Local Features and Beyondblog postThe DAVIS Challenge on Video Object Segmentation2nd Workshop on Precognition: Seeing through the FutureComputational Cameras and Displays (CCD)2nd Workshop on Learning from Unlabeled Videos (LUV)7th Workshop on Fine Grained Visual Categorization (FGVC7)blog postLanguage & Vision with applications to Video UnderstandingNeural Architecture Search and Beyond for Representation LearningDisentangled 3D Representations for Relightable Performance Capture of HumansLearning Representations via Graph-Structured NetworksNovel View Synthesis: From Depth-Based Warping to Multi-Plane Images and BeyondHow to Write a Good ReviewNeural RenderingFairness Accountability Transparency and Ethics and Computer Vision",
      "link": "http://ai.googleblog.com/2020/06/google-at-cvpr-2020.html",
      "author": "Posted by Emily Knapp, Program Manager and Benjamin H\u00fctteroth, Program Specialist"
    },
    {
      "title": "Extracting Structured Data from Templatic Documents",
      "date": "Friday, June 12, 2020",
      "abstract": "Extracting Structured Data from Templatic DocumentsRepresentation Learning for Information Extraction from Form-like DocumentsACL 2020tablesGoogle Knowledge GraphserviceReLUmax-poolingcosine similarityF1 scoreDocument AIBERT",
      "link": "http://ai.googleblog.com/2020/06/extracting-structured-data-from.html",
      "author": "Posted by Sandeep Tata, Software Engineer, Google Research"
    },
    {
      "title": "Unlocking the \"Chemome\" with DNA-Encoded Chemistry and Machine Learning",
      "date": "Thursday, June 11, 2020",
      "abstract": "Unlocking the \"Chemome\" with DNA-Encoded Chemistry and Machine LearningMachine learning on DNA-encoded libraries: A new paradigm for hit-findingJournal of Medicinal ChemistryX-Chem PharmaceuticalsChemome initiativeAccelerated Science teamZebiAI4% of human proteins have a known chemical probe availableRobotic assisted high throughput screening1.2x10910201060DNA-encoded small molecule librariesNobel Prize winning phage display technologygraph convolutional neural networksEHER\u03b1c-KITMculeX-Chemstandard chemical fingerprintsIC50ZebiAI TherapeuticsX-Chem PharmaceuticalsChemome InitiativeAccelerated Science TeamX-Chem Pharmaceuticals",
      "link": "http://ai.googleblog.com/2020/06/unlocking-chemome-with-dna-encoded.html",
      "author": "Posted by Patrick Riley, Principal Engineer, Accelerated Science Team, Google Research"
    },
    {
      "title": "PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization",
      "date": "Tuesday, June 9, 2020",
      "abstract": "PEGASUS: A State-of-the-Art Model for Abstractive Text Summarizationabstractive text summarizationsequence-to-sequencerecurrent neural networksTransformerBERTGPT-2RoBERTaXLNetALBERTT5ELECTRAPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization2020 International Conference on Machine LearningGitHubROUGEn-gramT5ROUGETuring testXSumCNN/Dailymaiexample articleGitHub",
      "link": "http://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html",
      "author": "Posted by Peter J. Liu and Yao Zhao, Software Engineers, Google Research"
    },
    {
      "title": "Recent Advances in Google Translate",
      "date": "Monday, June 8, 2020",
      "abstract": "Recent Advances in Google TranslateGNMT neural translation modelWMT Evaluation Campaignsupported languagesM4 modelingBLEU scoreBLEU scoreGNMT modelour work decoupling different aspects of model performancetransformerLingvoTensorFlowmore effective at machine translation than RNN modelsphrase-based machine translationmore sensitive to data qualityprecision than recalldictionary-based modelembedding based modeldenoising NMT trainingcurriculum learning problemstate-of-the-artmachine translation systemsM4transitioning from phrase-based translation to NMTsource sidetarget sidemachine translation hallucination",
      "link": "http://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html",
      "author": "Posted by Isaac Caswell and Bowen Liang, Software Engineers, Google Research"
    },
    {
      "title": "DADS: Unsupervised Reinforcement Learning for Skill Discovery",
      "date": "Friday, May 29, 2020",
      "abstract": "DADS: Unsupervised Reinforcement Learning for Skill Discoverysupervisedreinforcement learninggrasping arbitrary objectslearning agile locomotionsensorsmanual-labelling of \u201cgoal\u201d statesunsupervised learningcuriosityresearchhas recentlyfocusedDynamics-Aware Unsupervised Discovery of SkillsEmergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement LearningHumanoidAntD\u2019ClawROBELheremodel-based planningEmergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement LearningD\u2019KittyROBELemergence of complex behaviorsROBEL",
      "link": "http://ai.googleblog.com/2020/05/dads-unsupervised-reinforcement.html",
      "author": "Posted by Archit Sharma, AI Resident, Google Research"
    },
    {
      "title": "Federated Analytics: Collaborative Data Science without Data Collection",
      "date": "Wednesday, May 27, 2020",
      "abstract": "Federated Analytics: Collaborative Data Science without Data CollectionFederated learningintroduced in 2017next wordsexpressionsGboard for Androidimproving the quality of smart repliesAndroid MessagesNow Playing featureUnder the hoodfederated learning and analytics serversecure aggregation protocolparticular songfederated learning comic booktraining a character-level recurrent neural networkwhat patterns in the data are difficult for my model to recognize?user-level differentially private model trainingprivacy principlesdifferential privacy in the data centerduring data collectionintroduced in 2014Advances and Open Problems in Federated LearningFederated Heavy Hitters Discovery with Differential Privacy",
      "link": "http://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html",
      "author": "Posted by Daniel Ramage, Research Scientist and Stefano Mazzocchi, Software Engineer, Google Research"
    },
    {
      "title": "Evaluating Natural Language Generation with BLEURT",
      "date": "Tuesday, May 26, 2020",
      "abstract": "Evaluating Natural Language Generation with BLEURTtranslate textsummarize articlesengage in conversationcomment on picturesincreasinglyof sophisticationBLEUunreliable substitutesBLEURT: Learning Robust Metrics for Text GenerationACL 2020transfer learningGithubprecisionWMT Metrics Shared TaskBERTExperimentsthe original BERT paperLanguage team",
      "link": "http://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html",
      "author": "Posted by Thibault Sellam, Software Engineer and Ankur P. Parikh, Research Scientist, Google Research"
    },
    {
      "title": "Open-Sourcing BiT: Exploring Large-Scale Pre-training for Computer Vision",
      "date": "Thursday, May 21, 2020",
      "abstract": "Open-Sourcing BiT: Exploring Large-Scale Pre-training for Computer VisionOpenImagesPlacesthis amount of labeled dataImageNetBERTT5Big Transfer (BiT): General Visual Representation LearningILSVRC-2012ObjectNetshare the best BiT modelscode in TF2, Jax, and PyTorchImageNet-21kJFTResNetbatch normalizationstabilizes training by normalizing activationsgroup normalizationweight standardizationBERTVTAB-1kthe previous state-of-the-artPetsFlowersCIFARRetinaNetMSCOCO-2017ObjectNetprevious state-of-the-artreleaseTensorFlow2 tutorialNoisy StudentExploring the Limits of Weakly Supervised Pretraining",
      "link": "http://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html",
      "author": "Posted by Lucas Beyer and Alexander Kolesnikov, Research Engineers, Google Research, Z\u00fcrich"
    },
    {
      "title": "Announcing the 7th Fine-Grained Visual Categorization Workshop",
      "date": "Wednesday, May 20, 2020",
      "abstract": "Announcing the 7th Fine-Grained Visual Categorization WorkshopDanaus plexippusLimenitis archippusFGVCCUB7th Workshop on Fine-Grained Visual CategorizationIEEEComputer Vision and Pattern RecognitionFungi challengeDanish Mycological Societya team from Czech Technical University and the University of West Bohemiaa new workflowconference paperapp for AndroidiOSopen access modelTensorFlow HubSvampeatlas appiCassava Disease ChallengeMakerere UniversityNational Crops Resources Research InstituteiWildCammaking generalization difficultusing informationbeyondthe image itselfiNaturalist competition datasetsHerbarium ChallengeNew York Botanical GardeniMat Fashioninstance segmentationiMetMetropolitan Museum of ArtSemi-Supervised AvesiNaturalistsemi-supervised learningPlant PathologyiCassava",
      "link": "http://ai.googleblog.com/2020/05/announcing-7th-fine-grained-visual.html",
      "author": "Posted by Christine Kaeser-Chen, Software Engineer and Serge Belongie, Visiting Faculty, Google Research"
    },
    {
      "title": "Enabling E-Textile Microinteractions: Gestures and Light through Helical Structures",
      "date": "Friday, May 15, 2020",
      "abstract": "Enabling E-Textile Microinteractions: Gestures and Light through Helical StructuresE-textile MicrointeractionsACM CHI 2020previously introduced E-textile architectureACM UIST 2018video about E-textile microinteractionsvideo about the E-textile architecturecapacitive sensingLikert scale",
      "link": "http://ai.googleblog.com/2020/05/enabling-e-textile-microinteractions.html",
      "author": "Posted by Alex Olwal, Research Scientist, Google Research"
    },
    {
      "title": "Announcing Meta-Dataset: A Dataset of Datasets for Few-Shot Learning",
      "date": "Wednesday, May 13, 2020",
      "abstract": "Announcing Meta-Dataset: A Dataset of Datasets for Few-Shot Learningmanually annotated training datasparked interestMeta-Dataset: A Dataset of Datasets for Learning to Learn from Few ExamplesICLR 2020ImageNetCUB-200-2011FungipublicnotebookTensorFlowPyTorchmini-ImageNetImageNetRecentworksrecentpapersnearest-neighbor comparisonssurgeofattentionOmniglotQuickdrawProtoNetfo-Proto-MAMLtaskconditioninghyperparameter tuningmeta-baselinefeature selection",
      "link": "http://ai.googleblog.com/2020/05/announcing-meta-dataset-dataset-of.html",
      "author": "Posted by Eleni Triantafillou, Student Researcher and Vincent Dumoulin, Research Scientist, Google Research"
    },
    {
      "title": "Speeding Up Neural Network Training with Data Echoing",
      "date": "Tuesday, May 12, 2020",
      "abstract": "Speeding Up Neural Network Training with Data EchoingMoore's lawGPUsTPUsWe know there are limitsstochastic gradient descentFaster Neural Network Training with Data Echoingshuffle bufferimage classificationlanguage modelingobject detectionResNet-50ImageNetperplexity",
      "link": "http://ai.googleblog.com/2020/05/speeding-up-neural-network-training.html",
      "author": "Posted by Dami Choi, Student Researcher and George Dahl, Senior Research Scientist, Google Research"
    },
    {
      "title": "Agile and Intelligent Locomotion via Deep Reinforcement Learning",
      "date": "Wednesday, May 6, 2020",
      "abstract": "Agile and Intelligent Locomotion via Deep Reinforcement Learningusing off-policy dataimitating animal behaviorsperforming meta learningData Efficient Reinforcement Learning for Legged RobotsHierarchical Reinforcement Learning for Quadruped LocomotionSoft Actor-Criticmore than an hour of datadifficult to collectwe presentmodel predictive controltrajectory generatorour second paperPPOSACaugmented random searchAI Residency program",
      "link": "http://ai.googleblog.com/2020/05/agile-and-intelligent-locomotion-via.html",
      "author": "Posted by Yuxiang Yang and Deepali Jain, AI Residents, Robotics at Google"
    },
    {
      "title": "Understanding the Shape of Large-Scale Data",
      "date": "Tuesday, May 5, 2020",
      "abstract": "Understanding the Shape of Large-Scale DatagraphInternet graphlink together friendsJust SLaQ When You Approximate: Accurate Spectral Distances for Web-Scale GraphsWWW'20DDGK: Learning Graph Representations for Deep Divergence Graph KernelsWWW\u201919graph embeddingsIn our 2019 paper,t-SNEproteinsspectrumdrum3D shapesgraphsgeneral high-dimensional dataAutoMLanomaly detection in dynamic graphschemical molecule characterizationOur more recent paperVon Neumann Graph EntropyEstrada Indexgraph energyNetLSDWikipediaKarateSLaQDDGKrecommendation systemsGraph Mining teamAlgorithm and Optimization",
      "link": "http://ai.googleblog.com/2020/05/understanding-shape-of-large-scale-data.html",
      "author": "Posted by Anton Tsitsulin, Research Intern and Bryan Perozzi, Senior Research Scientist, Graph Mining Team, Google Research"
    },
    {
      "title": "An NLU-Powered Tool to Explore COVID-19 Scientific Literature",
      "date": "Monday, May 4, 2020",
      "abstract": "An NLU-Powered Tool to Explore COVID-19 Scientific LiteratureCOVID-19 Research ExplorerCOVID-19 Open Research Datasetsemantic searchWhat regulates ACE2 expression?regulatesinformation retrievalquery expansionangiotensin converting enzyme-2n-gramBERTrecently been deployedGoogle SearchBioASQlarge synthetic corpus of questions and relevant documentsencoder-decoder modelmachine translationmemorization-generalization continuumtf-idfvector space modelk-nearest neighborlot of research and engineeringCOVID-19 Research Explorer",
      "link": "http://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html",
      "author": "Posted by Keith Hall, Research Scientist, Natural Language Understanding, Google Research"
    },
    {
      "title": "Using Neural Networks to Find Answers in Tables",
      "date": "Thursday, April 30, 2020",
      "abstract": "Using Neural Networks to Find Answers in TablesManyrecentapproachessemantic parsingSQLTAPAS: Weakly Supervised Table Parsing via Pre-trainingACL 2020BERTquestion-answeringour GitHub repoSQAWTQWikiSQLMin et al (2019)Wang et al. (2019)Mueller et al., 2019",
      "link": "http://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html",
      "author": "Posted by Thomas M\u00fcller, Software Engineer, Google Research"
    },
    {
      "title": "Applying Machine Learning to\u2026..Yeast?",
      "date": "Wednesday, April 29, 2020",
      "abstract": "Applying Machine Learning to\u2026..Yeast?sequence homologCalico Life SciencesLearning causal networks using inducible transcription factors and transcriptome-wide time seriesMolecular Systems Biologygene expressionInduction Dynamics gene Expression AtlaspythonGitHubGene Expression OmnibusCalico Life SciencesDNARNAribosomesproteinmicroarraystranscription factorsL1-normcommentaryGoogle Accelerated Science",
      "link": "http://ai.googleblog.com/2020/04/applying-machine-learning-toyeast.html",
      "author": "Posted by Ted Baltz, Senior Staff Software Engineer, Google Research, Accelerated Science Team"
    },
    {
      "title": "Yet More Google Compute Cluster Trace Data",
      "date": "Tuesday, April 28, 2020",
      "abstract": "Yet More Google Compute Cluster Trace DataBorg cluster management systempublishedMapReduceGoogle BigQueryThis sitethis paperlet us know",
      "link": "http://ai.googleblog.com/2020/04/yet-more-google-compute-cluster-trace.html",
      "author": "Posted by John Wilkes, Principal Software Engineer, Google Cloud"
    },
    {
      "title": "Optimizing Multiple Loss Functions with Loss-Conditional Training",
      "date": "Monday, April 27, 2020",
      "abstract": "Optimizing Multiple Loss Functions with Loss-Conditional Trainingimage compressionICLR 2020You Only Train Once: Loss-Conditional Training of Deep Networksvariational autoencodersAdjustable Real-time Style Transferstyle transferlearned image compressionClassic image compression algorithmsleading learned compression methodsBalle et alartistic style transferRecent methodswebpageinteractive demoMS-COCOunsplash.com",
      "link": "http://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html",
      "author": "Posted by Alexey Dosovitskiy, Research Scientist, Google Research"
    },
    {
      "title": "Google at ICLR 2020",
      "date": "Sunday, April 26, 2020",
      "abstract": "Google at ICLR 2020ICLR 2020machine learningdeep learningDiamond Sponsor of ICLR 2020SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inferenceblog postDifferentiable Reasoning Over a Virtual Knowledge BaseDynamics-Aware Unsupervised Discovery of SkillsGenDICE: Generalized Offline Estimation of Stationary ValuesMathematical Reasoning in Latent SpaceYour Classifier is Secretly an Energy Based Model and You Should Treat it Like OneAdjustable Real-time Style Transferblog postAre Transformers Universal Approximators of Sequence-to-sequence Functions?AssembleNet: Searching for Multi-Stream Neural Connectivity in Video ArchitecturesAugMix: A Simple Data Processing Method to Improve Robustness and UncertaintyBatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong LearningBlack-box Off-policy Estimation for Infinite-Horizon Reinforcement Learningblog postCan Gradient Clipping Mitigate Label Noise?CAQL: Continuous Action Q-LearningChameleon: Adaptive Code Optimization for Expedited Deep Neural Network CompilationCoherent Gradients: An Approach to Understanding Generalization in Gradient Descent-based OptimizationConsistency Regularization for Generative Adversarial NetworksContrastive Representation DistillationDeep Audio Priors Emerge from Harmonic Convolutional NetworksDetecting and Diagnosing Adversarial Images with Class-Conditional Capsule ReconstructionsDetecting Extrapolation with Local EnsemblesDisentangling Factors of Variations Using Few LabelsDistance-Based Learning from Errors for Confidence CalibrationELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generatorsblog postES-MAML: Simple Hessian-Free Meta Learningblog postExploration in Reinforcement Learning with Deep Covering OptionsExtreme Tensoring for Low-Memory PreconditioningFantastic Generalization Measures and Where to Find ThemGeneralization Bounds for Deep Convolutional Neural NetworksGeneralized Convolutional Forest Networks for Domain Generalization and Visual RecognitionGenerative Models for Effective ML on Private, Decentralized DatasetsGenerative Ratio Matching NetworksGlobal Relational Models of Source CodeHierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal GenerationIdentity Crisis: Memorization and Generalization Under Extreme OverparameterizationImitation Learning via Off-Policy Distribution MatchingLanguage GANs Falling ShortLarge Batch Optimization for Deep Learning: Training BERT in 76 MinutesLearning Execution through Neural Code FusionLearning Heuristics for Quantified Boolean Formulas through Reinforcement LearningLearning to Learn by Zeroth-Order OracleLearning to Represent Programs with Property SignaturesMACER: Attack-free and Scalable Robust Training via Maximizing Certified RadiusMeasuring Compositional Generalization: A Comprehensive Method on Realistic DataMeta Reinforcement Learning with Autonomous Inference of Subtask DependenciesMeta-Dataset: A Dataset of Datasets for Learning to Learn from Few ExamplesModel-based Reinforcement Learning for Biological Sequence DesignNetwork Randomization: A Simple Technique for Generalization in Deep Reinforcement LearningObservational Overfitting in Reinforcement LearningOn Bonus-based Exploration Methods In The Arcade Learning EnvironmentOn Identifiability in TransformersOn Mutual Information Maximization for Representation LearningOn the Global Convergence of Training Deep Linear ResNetsPhase Transitions for the Information Bottleneck in Representation LearningPre-training Tasks for Embedding-based Large-scale RetrievalPrediction, Consistency, Curvature: Representation Learning for Locally-Linear ControlProvable Benefit of Orthogonal Initialization in Optimizing Deep Linear NetworksRapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAMLReinforced Genetic Algorithm Learning for Optimizing Computation GraphsReMixMatch: Semi-Supervised Learning with Distribution Alignment and Augmentation AnchoringScalable Model Compression by Entropy Penalized ReparameterizationScalable Neural Methods for Reasoning With a Symbolic Knowledge BaseSemi-Supervised Generative Modeling for Controllable Speech SynthesisSpan Recovery for Deep Neural Networks with Applications to Input ObfuscationThieves on Sesame Street! Model Extraction of BERT-based APIsThinking While Moving: Deep Reinforcement Learning with Concurrent ControlVideoFlow: A Conditional Flow-Based Model for Stochastic Video GenerationWatch, Try, Learn: Meta-Learning from Demonstrations and RewardsWeakly Supervised Disentanglement with GuaranteesYou Only Train Once: Loss-Conditional Training of Deep NetworksA Mutual Information Maximization Perspective of Language Representation LearningALBERT: A Lite BERT for Self-supervised Learning of Language Representationsblog postAsymptotics of Wide Networks from Feynman DiagramsDDSP: Differentiable Digital Signal ProcessingDoubly Robust Bias Reduction in Infinite Horizon Off-Policy EstimationDream to Control: Learning Behaviors by Latent Imaginationblog postEmergent Tool Use From Multi-Agent AutocurriculaGradientless Descent: High-Dimensional Zeroth-Order OptimizationHOPPITY: Learning Graph Transformations to Detect and Fix Bugs in ProgramsLearning to Plan in High Dimensions via Neural Exploration-Exploitation TreesModel Based Reinforcement Learning for Atariblog postNeural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading ComprehensionSUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable ModelsMeasuring the Reliability of Reinforcement Learning AlgorithmsMeta-Learning without MemorizationNeural Tangents: Fast and Easy Infinite Neural Networks in PythonScaling Autoregressive Video ModelsThe Intriguing Role of Module Criticality in the Generalization of Deep NetworksReformer: The Efficient Transformerblog postComputer Vision for Global ChallengesPractical ML for Developing Countries: Learning under limited/low resource scenariosTackling Climate Change with Machine LearningTowards Trustworthy ML: Rethinking Security and Privacy for ML",
      "link": "http://ai.googleblog.com/2020/04/google-at-iclr-2020.html",
      "author": "Posted by Christian Howard, Google Research"
    },
    {
      "title": "Chip Design with Deep Reinforcement Learning",
      "date": "Thursday, April 23, 2020",
      "abstract": "Chip Design with Deep Reinforcement LearningMoore\u2019s LawDennard scalingChip Placement with Deep Reinforcement LearningTPUsASICgraph neural networkfeedforward neural networkforce-directedopen-source RISC-V",
      "link": "http://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html",
      "author": "Posted by Anna Goldie, Senior Software Engineer and Azalia Mirhoseini, Senior Research Scientist, Google Research, Brain Team"
    },
    {
      "title": "A Scalable Approach to Reducing Gender Bias in Google Translate",
      "date": "Wednesday, April 22, 2020",
      "abstract": "A Scalable Approach to Reducing Gender Bias in Google TranslateAI Principlesannouncedthree-step approachneural machine translationrecallsyntactic parserlanguage modeltransformer-basedsequence-to-sequence",
      "link": "http://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html",
      "author": "Posted by Melvin Johnson, Senior Software Engineer, Google Research"
    },
    {
      "title": "Exploring Evolutionary Meta-Learning in Robotics",
      "date": "Tuesday, April 21, 2020",
      "abstract": "Exploring Evolutionary Meta-Learning in Roboticssimulator engines imitation learningoffline reinforcement learningdomain randomizationRapidly Adaptable Legged Robots via Evolutionary Meta-Learningevolutionary strategiespolicy gradientsmeta-learningmodel-agnostic meta-learningpolicy gradientbeforeES-MAMLefficient compact policiesIt also worksguided ESprevious workhill-climbingPG-MAML",
      "link": "http://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html",
      "author": "Posted by Xingyou (Richard) Song, Software Engineer and Yuxiang Yang, AI Resident, Robotics at Google"
    },
    {
      "title": "Off-Policy Estimation for Infinite-Horizon Reinforcement Learning",
      "date": "Friday, April 17, 2020",
      "abstract": "Off-Policy Estimation for Infinite-Horizon Reinforcement Learningreinforcement learninglearn good policiesroboticsrecommendation systemsmore criticalPongbiasBlack-Box Off-Policy Estimation for Infinite-Horizon Reinforcement LearningICLR 2020often grows exponentiallyimportance samplingprobability distributionour paperModelWinIPSroot-mean-square errorCartPolePendulumMountainCarCartpolePendulumMountaincarpaperQiang LiuDenny Zhou",
      "link": "http://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html",
      "author": "Posted by Ali Mousavi, AI Resident and Lihong Li, Research Scientist, Google Research"
    },
    {
      "title": "EfficientDet: Towards Scalable and Efficient Object Detection",
      "date": "Wednesday, April 15, 2020",
      "abstract": "EfficientDet: Towards Scalable and Efficient Object Detectionroboticsdriverless carshigh-accuracy detectorsEfficientDet: Scalable and Efficient Object DetectionCVPR 2020EfficientNetEfficientDetEfficientNetResNetsResNeXtAmoebaNetEfficientNetFPNPANetNAS-FPNFPNPANetNAS-FPNdepthwise separable convolutionsprevious workCOCO datasetmean average precisionmAPprior state of the artsemantic segmentationPascal VOC 2012Pascal VOC 2012DeepLabV3+GitHub",
      "link": "http://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html",
      "author": "Posted by Mingxing Tan, Software Engineer and Adams Yu, Research Scientist, Google Research"
    },
    {
      "title": "An Optimistic Perspective on Offline Reinforcement Learning",
      "date": "Tuesday, April 14, 2020",
      "abstract": "An Optimistic Perspective on Offline Reinforcement LearningSutton & Bartoreinforcement learningan agentroboticsautonomous drivingunintendedbehaviorchallengingpreviouslycollectedoffline RLoff-policyAn Optimistic Perspective on Offline RLAtari 2600 gamesDQNdata-driven RLparadigm.DQN Replay Datasetcodeoffline-rl.github.ioDQNAtari 2600 gamesdistributional RLQR-DQNrecentworkpresentsremediesregularizingDQNAtari 2600 gamessticky actionsImageNetQR-DQNQR-DQNgeneralizationensemble of modelsQ-valueDropoutweighted combinationdistributionalfractionQ-functionC51distributionalpriorworkthatreportsclaimscontinuous controlTD3sophisticated offline agentoffline policy evaluationmodel-based RLself-supervised learningcontributed talk",
      "link": "http://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html",
      "author": "Posted by Rishabh Agarwal, AI Resident and Mohammad Norouzi, Research Scientist, Google Research"
    },
    {
      "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
      "date": "Monday, April 13, 2020",
      "abstract": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual GeneralizationpostpositionsmBERTXLMXLM-RXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual GeneralizationDravidian languagesNiger-Congo languagesheresentence classificationstructured predictionsentence retrievalquestion answeringXNLIXQuADmultilingual BERTXLMXLM-RM4ISO 639-1 codesMLQATyDi QApart-of-speech taggingnamed entity recognitionSemi-supervised methodsGLUESuperGLUEBERTRoBERTaXLNetAlBERTTwitter account",
      "link": "http://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html",
      "author": "Posted by Melvin Johnson, Senior Software Engineer, Google Research and Sebastian Ruder, Research Scientist, DeepMind"
    },
    {
      "title": "uDepth: Real-time 3D Depth Sensing on the Pixel 4",
      "date": "Friday, April 10, 2020",
      "abstract": "uDepth: Real-time 3D Depth Sensing on the Pixel 4portrait modeARtransparent object detectionface unlockCamera2Pixel Neural Core,DEPTH16improved depth capabilities to selfiesSlanted O(1) Stereoportrait modedropout schemea volumetric capture systembokehhere",
      "link": "http://ai.googleblog.com/2020/04/udepth-real-time-3d-depth-sensing-on.html",
      "author": "Posted by Michael Schoenberg, uDepth Software Lead and Adarsh Kowdle, uDepth Hardware/Systems Lead, Google Research"
    },
    {
      "title": "Advancing Self-Supervised and Semi-Supervised Learning with SimCLR",
      "date": "Wednesday, April 8, 2020",
      "abstract": "Advancing Self-Supervised and Semi-Supervised Learning with SimCLRBERTT5Exemplar-CNNInstance DiscriminationCPCAMDIMCMCMoCoA Simple Framework for Contrastive Learning of Visual RepresentationsImageNetsophisticated transformation policyconvolutional neural networkResNetfully-connected networkMLPstochastic gradient descentCPC v2Cloud TPUour GitHub repository",
      "link": "http://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html",
      "author": "Posted by Ting Chen, Research Scientist and Geoffrey Hinton, VP & Engineering Fellow, Google Research"
    },
    {
      "title": "Exploring Nature-Inspired Robot Agility",
      "date": "Friday, April 3, 2020",
      "abstract": "Exploring Nature-Inspired Robot Agilityreinforcement learningLearning Agile Robotic Locomotion Skills by Imitating AnimalsPyBulletQT-OptLearning to Walk in the Real World with Minimal Human Effortany human interventionskills from videos",
      "link": "http://ai.googleblog.com/2020/04/exploring-nature-inspired-robot-agility.html",
      "author": "Posted by Xue Bin (Jason) Peng, Student Researcher and Sehoon Ha, Research Scientist, Robotics at Google"
    },
    {
      "title": "Announcing the 2020 Image Matching Benchmark and Challenge",
      "date": "Thursday, April 2, 2020",
      "abstract": "Announcing the 2020 Image Matching Benchmark and ChallengeStructure-from-Motionphotographycultural heritage preservationin a browserStreetView and aerial imageryImage Matching across Wide Baselines: From Paper to PracticeUVICCTUEPFLImage Matching: Local Features and BeyondCVPR 2019publicly availableopen-sourced benchmark2020 Image Matching ChallengeCVPR 20201images donated by userslocal featurestriangulationjointly estimatedImage SearchGoogle Lensmixed realityGoogle Maps' Live ViewOpenImages datasetmachine learning techniques with human annotatorsHPatchesImage Matching BenchmarkYFCC100m datasetSIFTHardNetLogPolarDescR2D2our website2020 Image Matching Challenge1 Please note that as of April 2, 2020, CVPR is currently on track, despite the COVID-19 pandemic. Challenge information will be updated as the situation develops. Please see the 2020 Image Matching Challenge website for details.\u21a912020 Image Matching Challenge\u21a9",
      "link": "http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html",
      "author": "Posted by Eduard Trulls, Research Scientist, Google Maps"
    },
    {
      "title": "A Step Towards Protecting Patients from Medication Errors",
      "date": "Thursday, April 2, 2020",
      "abstract": "A Step Towards Protecting Patients from Medication Errorsresearch showsIOM reportBakar Computational Health Sciences InstitutePredicting Inpatient Medication Orders in Electronic Health Record DataClinical Pharmacology and Therapeuticsprior workde-identifiedFast Healthcare Interoperability Resources (FHIR)make healthcare data more effective for machine learninglong short-term memoryrecurrent neural networklogistic model,",
      "link": "http://ai.googleblog.com/2020/04/a-step-towards-protecting-patients-from.html",
      "author": "Posted by Kathryn Rough, Research Scientist and Alvin Rajkomar, MD, Google Health"
    },
    {
      "title": "Improving Audio Quality in Duo with WaveNetEQ",
      "date": "Wednesday, April 1, 2020",
      "abstract": "Improving Audio Quality in Duo with WaveNetEQGoogle Duopacket loss concealmentgenerative modelDeepMind\u2019sWaveRNNWebRTC open source projectrecurrent neural networkautoregressiveWaveNettext-to-speechphonemesprosodyspectrogramteacher forcingMEL spectrogramjitter bufferLibriTTSGoogle Cloud Speech-to-Text APIword error rateDuo calls on Pixel 4 phones",
      "link": "http://ai.googleblog.com/2020/04/improving-audio-quality-in-duo-with.html",
      "author": "Posted by Pablo Barrera, Software Engineer, Google Research and Florian Stimberg, Research Engineer, DeepMind"
    },
    {
      "title": "Exploring New Ways to Support Faculty Research",
      "date": "Thursday, March 26, 2020",
      "abstract": "Exploring New Ways to Support Faculty ResearchGoogle Faculty Research Award ProgramResearch Scholar ProgramLatin America Research AwardsPhD Fellowship ProgramVisiting Researcher Programthis pagepublication database",
      "link": "http://ai.googleblog.com/2020/03/exploring-new-ways-to-support-faculty.html",
      "author": "Posted by Maggie Johnson, VP, Google Research"
    },
    {
      "title": "A Neural Weather Model for Eight-Hour Precipitation Forecasting",
      "date": "Wednesday, March 25, 2020",
      "abstract": "A Neural Weather Model for Eight-Hour Precipitation Forecastingdeep neural networksTPUsprevious researchnowcastingMetNet: A Neural Weather Model for Precipitation ForecastingNOAAmulti-radar/multi-sensor systemGeostationary Operational Environmental Satelliteconvolutional LSTMaxial self-attentionprecipitation rate forecasting benchmarkHigh Resolution Rapid Refreshbaseline modeloptical flowTPUsF1-score",
      "link": "http://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html",
      "author": "Posted by Nal Kalchbrenner and Casper S\u00f8nderby, Research Scientists, Google Research, Amsterdam"
    },
    {
      "title": "Massively Scaling Reinforcement Learning with SEED RL",
      "date": "Monday, March 23, 2020",
      "abstract": "Massively Scaling Reinforcement Learning with SEED RLGoDota 2SEED RL: Scalable and Efficient Deep-RL with Accelerated Central InferenceacceleratorsGPUsTPUsGoogle Research FootballArcade Learning EnvironmentDeepMind LabGithubGPUsIMPALAIMPALAnetwork librarygRPCTensorFlow 2TPUsV-tracepolicy gradientR2D2Q-learningDeepMind Labpaper",
      "link": "http://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html",
      "author": "Posted by Lasse Espeholt, Research Engineer, Google Research, Amsterdam"
    },
    {
      "title": "Visual Transfer Learning for Robotic Manipulation",
      "date": "Friday, March 20, 2020",
      "abstract": "Visual Transfer Learning for Robotic Manipulationaffordancesaffordance-based manipulationgraspingpushingthrowingdata collectiontrial and errorrecentdiscoveriestransfer learningnow availableLearning to See before Learning to Act: Visual Pre-training for Manipulationresearchers from MITICRA 2020exploration processgrippersdeep modelResNetImageNetCOCOsurface normalsMaskRCNN",
      "link": "http://ai.googleblog.com/2020/03/visual-transfer-learning-for-robotic.html",
      "author": "Posted by Yen-Chen Lin, Research Intern and  Andy Zeng, Research Scientist, Robotics at Google"
    },
    {
      "title": "Introducing Dreamer: Scalable Reinforcement Learning Using World Models",
      "date": "Wednesday, March 18, 2020",
      "abstract": "Introducing Dreamer: Scalable Reinforcement Learning Using World Modelsartificial agentsreinforcement learningDQNAlphaStarreal-world scenariosDeep Planning NetworkDeepMindDreamerbackpropagationsource codetypicalPlaNetDeepMind Control SuiteDeepMind LabPlaNetA3CD4PGD4PGPlaNet",
      "link": "http://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html",
      "author": "Posted by Danijar Hafner, Student Researcher, Google Research"
    },
    {
      "title": "Fast and Easy Infinitely Wide Networks with Neural Tangents",
      "date": "Friday, March 13, 2020",
      "abstract": "Fast and Easy Infinitely Wide Networks with Neural Tangentsnatural language processingconversational agentsconnectomicsconverge toGaussian processesBayesian inferencegradient descentconvolutional neural networkuseful models in their own rightNeural Tangentsnew open-source software libraryJAXis described bycan be computed throughout trainingbe found hereclosed-formCIFAR-10wide residual networkspapertutorial Colab notebookGithub repoICLR 2020",
      "link": "http://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html",
      "author": "Posted by Samuel S. Schoenholz, Senior Research Scientist and Roman Novak, Research Engineer, Google Research"
    },
    {
      "title": "Soli Radar-Based Perception and Interaction in Pixel 4",
      "date": "Thursday, March 12, 2020",
      "abstract": "Soli Radar-Based Perception and Interaction in Pixel 4Motion Senseinteract with their Pixelface unlockSoliradaralmost a centuryfrom the ground upbandwidthbeamwidthfrequency-modulatedDoppler frequencyTensorFlowdigital signal processorRFICAdvanced Technology and Projects",
      "link": "http://ai.googleblog.com/2020/03/soli-radar-based-perception-and.html",
      "author": "Posted by Jaime Lien, Research Engineer and Nicholas Gillian, Software Engineer, Google Advanced Technology and Projects"
    },
    {
      "title": "Real-Time 3D Object Detection on Mobile Devices with MediaPipe",
      "date": "Wednesday, March 11, 2020",
      "abstract": "Real-Time 3D Object Detection on Mobile Devices with MediaPipe 2D object predictionMediaPipeLIDARaugmented realityARCoreARKithundreds of millions3D point cloudsbuilt a single-stage modelMobileNetv2EPnPAdreno 650 mobile GPU2D object detection and trackinginstant motion trackingMotion StillsMediaPipeend-to-end demo mobile applicationshoeschairs",
      "link": "http://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html",
      "author": "Posted by Adel Ahmadyan and Tingbo Hou, Software Engineers, Google Research"
    },
    {
      "title": "More Efficient NLP Model Pre-training with ELECTRA",
      "date": "Tuesday, March 10, 2020",
      "abstract": "More Efficient NLP Model Pre-training with ELECTRABERTRoBERTaXLNetALBERTT5sentiment analysisquestion answeringGPTELECTRA: Pre-training Text Encoders as Discriminators Rather Than GeneratorsGLUESQuADreleasedgenerative adversarial networksdifficultytransformerFLOPsT5SQuAD 2.0T5-11bcode",
      "link": "http://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html",
      "author": "Posted by Kevin Clark, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team"
    },
    {
      "title": "Announcing TensorFlow Quantum: An Open Source Library for Quantum Machine Learning",
      "date": "Monday, March 9, 2020",
      "abstract": "Announcing TensorFlow Quantum: An Open Source Library for Quantum Machine LearningRichard Feynmanimage processing for cancer detectionforecasting earthquake aftershockspredicting extreme weather patternsdetecting new exoplanetsprogress in the development of quantum computingprofound impact on the world\u2019s biggest problemsUniversity of WaterlooXVolkswagenTensorFlow QuantumNoisy Intermediate Scale QuantumCirqTensorFlowsuperpositionentanglementsimulation of chemicalsquantum matterquantum controlquantum communication networksquantum metrologyTFQ white paperresearch repositoryTensorFlow OpsCirqHello Many-Worldsquantum convolutional neural networksguidegithub linkquantum circuit simulator(n1-ultramem-160this papern2-highcpu-80TFQ white paperSycamorewhite paperTensorFlow Quantum website",
      "link": "http://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html",
      "author": "Posted by Alan Ho, Product Lead and Masoud Mohseni, Technical Lead, Google Research"
    },
    {
      "title": "Measuring Compositional Generalization",
      "date": "Friday, March 6, 2020",
      "abstract": "Measuring Compositional GeneralizationLake and Baronithe CLEVR datasetChomsky saidone approachAnother approachanother methodMeasuring Compositional Generalization: A Comprehensive Method on Realistic DataCompositional Freebase QuestionsFreebase knowledge basesemantic parsingWikiSQLComplex Web Questionsparse treeLSTMattentionTransformerUniversal Transformersyntactic attentionCLEVR",
      "link": "http://ai.googleblog.com/2020/03/measuring-compositional-generalization.html",
      "author": "Posted by Marc van Zee, Software Engineer, Google Research"
    },
    {
      "title": "Toward Human-Centered Design for ML Frameworks",
      "date": "Tuesday, March 3, 2020",
      "abstract": "Toward Human-Centered Design for ML FrameworksAPIsTensorFlow.jsJavaScriptSoftware Developers Learning Machine Learning: Motivations, Hurdles, and DesiresBest Paper AwardIEEEVisual Languages and Human-Centric ComputingTensorFlow.jsMNISTTensorBoardtfjs-visPAIR",
      "link": "http://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html",
      "author": "Posted by Carrie J. Cai, Senior Research Scientist, Google Research and Philip J. Guo, Assistant Professor, UC San Diego"
    },
    {
      "title": "Ultra-High Resolution Image Analysis with Mesh-TensorFlow",
      "date": "Friday, February 28, 2020",
      "abstract": "Ultra-High Resolution Image Analysis with Mesh-TensorFlowdatamodelconvolutional neural networkcomputed tomographyHigh Resolution Medical Image Analysis with Spatial PartitioningMayo ClinicMesh-TensorFlowU-Netopen-sourcedtensorsconvolution operationLiTS benchmarkS\u00f8rensen\u2013Dice coefficientwith the released code",
      "link": "http://ai.googleblog.com/2020/02/ultra-high-resolution-image-analysis.html",
      "author": "Posted by Le Hou and Youlong Cheng, Software Engineers, Google Research"
    },
    {
      "title": "Open Images V6 \u2014 Now Featuring Localized Narratives",
      "date": "Wednesday, February 26, 2020",
      "abstract": "Open Images V6 \u2014 Now Featuring Localized NarrativesOpen Imagesdeep convolutional neural networksintroduction of version 5object detectioninstance segmentationvisual relationship detectionOpen Images V51969 Camaro RS/SSD. Millerthe houseanita kluskaCat Cafe Shinjuku calicoAri HelminenRadiofiera - Villa Cordellina Lombardi, Montecchio Maggiore (VI) - agosto 2010Andrea SartoratiCC BY 2.0 licenseOpen Images V6localized narrativesCOCO datasetSpring is here:-)Kasiasome previous worksCOCOFlickr30k EntitiesCOCOFlickr30k EntitiesSapa, VietnamRamaLocalized narrativesFreepikVia Guglielmo Marconi, Positano - Hotel Le Agavi - boatElliott Brownair framevivek jenaCL P1050512Virginia State ParksIMG_5678.jpgJames BuckDSC_0494Quentin MeulepasDSC06464sally9258_DSCs1341 (2)Boo PhRichard Wagner Spiele 2015Johannes G\u00e4rtner",
      "link": "http://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html",
      "author": "Posted by Jordi Pont-Tuset, Research Scientist, Google Research"
    },
    {
      "title": "Enhancing the Research Community\u2019s Access to Street View Panoramas for Language Grounding Tasks",
      "date": "Tuesday, February 25, 2020",
      "abstract": "Enhancing the Research Community\u2019s Access to Street View Panoramas for Language Grounding Tasksnatural language processingcomputer visionincluding robotsStreet ViewTouchdownvision-and-language navigationspatial description resolutionRetouchdown: Adding Touchdown to StreetLearn as a Shareable Resource for Language Grounding Tasks in Street ViewStreetLearnopen source TensorFlow implementations for the Touchdown tasksVALANRoom-to-Roombaseline models in TouchdownGoogle Maps/Google Earth Terms of ServiceDeepMindStreetLearnthe StreetNav task suiteRoom-to-Room;the current imagery available for this locationRetouchdownVALANChen et al. (2019)SDTW metricsmall differences in models and processingChen et al. (2019)StreetLearn interest formTouchdown github repository",
      "link": "http://ai.googleblog.com/2020/02/enhancing-research-communitys-access-to.html",
      "author": "Posted by Harsh Mehta, Software Engineer and Jason Baldridge, Research Scientist, Google Research"
    },
    {
      "title": "Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer",
      "date": "Monday, February 24, 2020",
      "abstract": "Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformernatural language processingGPTULMFiTELMoBERTXLNetRoBERTaALBERTReformerMT-DNNExploring the Limits of Transfer Learning with a Unified Text-to-Text TransformerColossal Clean Crawled Corpuscodepre-trained modelsColab NotebookWikipediaCommon CrawlTensorFlow DatasetsGoogle Cloud TPU acceleratorsGLUESuperGLUESQuADCNN/Daily MailHurricane ConnieColabpaperopen-domain question answeringTriviaQAWebQuestionsNatural QuestionsGPT-2Talk To TransformerAI Dungeonfindingscodepre-trained modelsColab Notebook",
      "link": "http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html",
      "author": "Posted by Adam Roberts, Staff Software Engineer and Colin Raffel, Senior Research Scientist, Google Research"
    },
    {
      "title": "Announcing the 2019 Google Faculty Research Award Recipients",
      "date": "Monday, February 24, 2020",
      "abstract": "Announcing the 2019 Google Faculty Research Award RecipientsGoogle Faculty Research Awardsresearch philosophyMachine LearningSystemsHuman Computer Interactionwell-deserving recipientsour website",
      "link": "http://ai.googleblog.com/2020/02/announcing-2019-google-faculty-research.html",
      "author": "Posted by Maggie Johnson, VP and Negar Saei, Program Manager, Google Research"
    },
    {
      "title": "Setting Fairness Goals with the TensorFlow Constrained Optimization Library",
      "date": "Friday, February 21, 2020",
      "abstract": "Setting Fairness Goals with the TensorFlow Constrained Optimization Libraryloss functionfairnessTensorFlow Constrained Optimizationprecisionstrue positive ratesrecall ratesequalized oddspredictive parityAI PrinciplesHardt et al.classifierpredictiondatasetlabelobjective functionfairness constraintslabelsdecision boundary\u201cequal opportunity\u201d principledecision boundarycontradictory constraintsfalse positive ratestrue positive ratesequalized oddsdecision boundarystochastic gradientsWiki toxicity exampleCelebA examplefalse positive rateALT\u201919JMLR\u201919AAAI\u201920NeurIPS\u201919aICML\u201919TFCOtoxicity classificationsmile detection",
      "link": "http://ai.googleblog.com/2020/02/setting-fairness-goals-with-tensorflow.html",
      "author": "Posted by Andrew Zaldivar, Responsible AI Advocate, Google Research, on behalf of the TFCO Team"
    },
    {
      "title": "Generating Diverse Synthetic Medical Image Data for Training Machine Learning Models",
      "date": "Wednesday, February 19, 2020",
      "abstract": "Generating Diverse Synthetic Medical Image Data for Training Machine Learning ModelsprogressDermGAN: Synthetic Generation of Clinical Skin Images with PathologyMachine Learning for HealthNeurIPS 2019dermatology imagesFitzpatrick skin typesrelative few cases at the \u201cboundaries\u201dpix2pixgenerative adversarial networkU-Netcheckerboard artifacts.ReLUbasal cell carcinomamelanocytic nevuspapermetastatic breast cancer detection algorithmsWhole-slide image focus quality: Automatic assessment and impact on AI cancer detectionJournal of Pathology InformaticsbokehJPEG compressionmodeling each step is essentialinverse probability weightingour work on ML for chest X-rays",
      "link": "http://ai.googleblog.com/2020/02/generating-diverse-synthetic-medical.html",
      "author": "Posted by Timo Kohlberger and Yuan Liu, Software Engineers, Google Health"
    },
    {
      "title": "AutoFlip: An Open Source Framework for Intelligent Video Reframing",
      "date": "Thursday, February 13, 2020",
      "abstract": "AutoFlip: An Open Source Framework for Intelligent Video Reframingincreasing number of usersAutoFlipMediaPipefaceobject detection modelsTensorFlow LiteEuclidean-normlow-degree polynomialEuclidean-normletterbox effectdeep uncrop",
      "link": "http://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html",
      "author": "Posted by Nathan Frey, Senior Software Engineer, Google Research, Los Angeles and Zheng Sun, Senior Software Engineer, Google Research, Mountain View"
    },
    {
      "title": "Learning to See Transparent Objects",
      "date": "Wednesday, February 12, 2020",
      "abstract": "Learning to See Transparent ObjectsRGB-D camerasLIDARautonomous manipulatorsLambertian3D depth imagerypoint cloudsSynthesis AIColumbia UniversityClearGrasplarge-scale synthetic datasetprevious methodspick and place robotImageNetWikipediaBERTMatterport3DScanNetlarge-scale dataset of transparent objectssurface normalsClearGrasp synthetic datasetRGB-D 3D cameraMatterport3DScanNetMatterport3DScanNetquantitative experimentsalternativemethodsproject webpagegrasping algorithmUR5 robot armcausticscausticsClearGraspdatasetproject websiteGitHub repository",
      "link": "http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html",
      "author": "Posted by Shreeyak Sajjan, Research Engineer, Synthesis AI and Andy Zeng, Research Scientist, Robotics at Google"
    },
    {
      "title": "TyDi QA: A Multilingual Question Answering Benchmark",
      "date": "Thursday, February 6, 2020",
      "abstract": "TyDi QA: A Multilingual Question Answering Benchmarkreceive an answerNatural QuestionschallengeTyDi QATyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languagestypological diversitysapodillaleaderboardquestion answering system",
      "link": "http://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html",
      "author": "Posted by Jonathan Clark, Research Scientist, Google Research"
    },
    {
      "title": "ML-fairness-gym: A Tool for Exploring Long-Term Impacts of Machine Learning Systems",
      "date": "Wednesday, February 5, 2020",
      "abstract": "ML-fairness-gym: A Tool for Exploring Long-Term Impacts of Machine Learning Systemscriminal sentencingchild welfare assessmentswho receives medical attentionAIF360fairlearnfairness-indicatorsfairness-comparisonML-fairness-gymFairness is not Static: Deeper Understanding of Long Term Fairness via Simulation Studieslending problemLiu et alequality of opportunityTPRtest setbiasesOpen AI\u2019s GymSimpson\u2019s paradoxSimpson's paradoxtrue positivefalse negativetrue positive ratepaperFair treatment allocations in social networkssocial networkML-fairness-gym Github repository",
      "link": "http://ai.googleblog.com/2020/02/ml-fairness-gym-tool-for-exploring-long.html",
      "author": "Posted by Hansa Srinivasan, Software Engineer, Google Research"
    },
    {
      "title": "Encode, Tag and Realize: A Controllable and Efficient Approach for Text Generation",
      "date": "Friday, January 31, 2020",
      "abstract": "Encode, Tag and Realize: A Controllable and Efficient Approach for Text GenerationSequence-to-sequencesummarizationsentence fusionTransformerunsupervised pre-traininghallucinationEncode, Tag, Realize: High-Precision Text Editingopen sourcedBERTWikiSplitBERTSARI scorethey grow longer, become more complex, and come as part of a dialogue discourseGitHub repo",
      "link": "http://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html",
      "author": "Posted by Eric Malmi and Sebastian Krause, Software Engineers, Google Research"
    },
    {
      "title": "Announcing the Third Workshop and Challenge on Learned Image Compression",
      "date": "Thursday, January 30, 2020",
      "abstract": "Announcing the Third Workshop and Challenge on Learned Image Compressionneural network-based image compressionresolutiongood quality and high compression speed20182019Third Workshop and Challenge On Learned Image CompressionCVPR 2020image compressionP-Framevideo compressionPSNRMS-SSIMhuman evaluated rating taskcompression.ccGoogle Groups page",
      "link": "http://ai.googleblog.com/2020/01/announcing-third-workshop-and-challenge.html",
      "author": "Posted by Nick Johnston, Software Engineer, Google Research"
    },
    {
      "title": "Towards a Conversational Agent that Can Chat About\u2026Anything",
      "date": "Tuesday, January 28, 2020",
      "abstract": "Towards a Conversational Agent that Can Chat About\u2026AnythingTowards a Human-like Open-Domain Chatbotneural conversational modelperplexityEvolved Transformerseq2seqneural architecture searchOpenAI GPT-2MitsukuCleverbotXiaoIceDialoGPTMitsukuCleverbotXiaoIceDialoGPTchallengingperplexityseq2seqdistillationpersonalityfactuality",
      "link": "http://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html",
      "author": "Posted by Daniel Adiwardana, Senior Research Engineer, and Thang Luong, Senior Research Scientist, Google Research, Brain Team"
    },
    {
      "title": "Releasing the Drosophila Hemibrain Connectome \u2014 The Largest Synapse-Resolution Map of Brain Connectivity",
      "date": "Wednesday, January 22, 2020",
      "abstract": "Releasing the Drosophila Hemibrain Connectome \u2014 The Largest Synapse-Resolution Map of Brain Connectivitygene networkssocial networksanalyzingannouncedin collaborationFlyEMJanelia Research Campusreleasedinteractive versionmushroom bodystaindividefocused ion beam scanning electron microscopesiterationsflood-filling networksflood-filling networkteamautomated synaptic detectionHHMI\u2019s press releasevisualizeddownloadedneuPrintdownloadableexplainingpre-printring neuronsellipsoid bodymushroom bodyinteractive visualization",
      "link": "http://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html",
      "author": "Posted by Michal Januszewski, Software Engineer and Viren Jain, Research Scientist and Technical Lead, Connectomics at Google"
    },
    {
      "title": "Reformer: The Efficient Transformer",
      "date": "Thursday, January 16, 2020",
      "abstract": "Reformer: The Efficient Transformerlong short-term memorytranslate sentence-by-sentenceTransformer modelentire Wikipedia articlesgenerate musicimagesthousands of layersReformerlocality-sensitive-hashingreversible residual layershash functiongradient descentactivationsreversible layersthis colab Imagenet64 datasetCrime and Punishmentthis colabresearch in the openeven longer sequencesimprove handling of positional encodingsReformer paperour codethis colabchat with usTraxJAX",
      "link": "http://ai.googleblog.com/2020/01/reformer-efficient-transformer.html",
      "author": "Posted by Nikita Kitaev, Student Researcher, UC Berkeley and \u0141ukasz Kaiser, Research Scientist, Google Research"
    },
    {
      "title": "Can You Trust Your Model\u2019s Uncertainty?",
      "date": "Wednesday, January 15, 2020",
      "abstract": "Can You Trust Your Model\u2019s Uncertainty?deep learningit was recently observedCan you trust your model\u2019s uncertainty? Evaluating Predictive Uncertainty Under Dataset ShiftNeurIPS 2019related postproper scoring rulesBrier ScoreLog Likelihoodexpected calibration errorImageNetCorrupted ImagenetCorrupted Imagenetdropouttemperature scalingtemperature scalingPlatt scalingDeep ensemblescode and model predictions",
      "link": "http://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html",
      "author": "Posted by Jasper Snoek, Research Scientist and Zachary Nado, Research Engineer, Google Research"
    },
    {
      "title": "Using Machine Learning to \u201cNowcast\u201d Precipitation in High Resolution",
      "date": "Monday, January 13, 2020",
      "abstract": "Using Machine Learning to \u201cNowcast\u201d Precipitation in High ResolutionMachine Learning for Precipitation Nowcasting from Radar ImagesDoppler radarNOAANWSNSSLNational Oceanic and Atmospheric AdministrationadvectionconvectionU-NetHigh Resolution Rapid Refresh1-hour total accumulated surface precipitationpersistence modelprecision and recallPrecision and recall (PR) curves",
      "link": "http://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html",
      "author": "Posted by Jason Hickey, Senior Software Engineer, Google Research"
    },
    {
      "title": "Google Research: Looking Back at 2019, and Forward to 2020 and Beyond",
      "date": "Thursday, January 9, 2020",
      "abstract": "Google Research: Looking Back at 2019, and Forward to 2020 and BeyondGoogle Research201820172016research publications in 2019AI Principlesone-year updateresearch paper about a new transparency toolModel Cardsexample model card for the Cloud AI Vision API Object Detection featureActivation AtlasesTensorFlow Privacydifferential privacyFairness IndicatorsKDD'19 paperAIES'19 paperAIES'19 papernew dataset to help with research to identify deepfakesFloods are the most common and the most deadly natural disaster on the planetmachine learning, computation and better sources of data to make significantly more accurate flood forecastsworkshop that brought together researchers with expertise in flood forecasting, hydrology and machine learninguse machine learning to help analyze wildlife camera dataNOAAidentify whale species and locationscreated and released a set of tools for enabling new kinds of machine-learning-oriented biodiversity research6th Fine-Grained Visual Categorization WorkshopMakerere University AI & Data Science research groupcassava plant diseasesGoogle Earth Timelapseaggregate data on human mobility617 million children do not have basic literacyBolo app uses speech-recognition technologyhelped 800,000 children read storiesthree-month pilotSocratic apphelp high schoolersSocratic methodannouncedAI Impact ChallengeFondation M\u00e9decins Sans Fronti\u00e8rescreating a free smartphone application that uses image recognition tools to help clinical staffdevastate their crop yieldsWadhwani AIRainforest Connectionherehave published a number of papers inAn Interactive, Automated 3D Reconstruction of a Fly BrainLearning Better Simulation Methods for Partial Differential EquationsBurgers\u2019 equationLearning to Smell: Using Deep Learning to Predict the Olfactory Properties of Moleculeskernel-density estimateframework for molecule optimizationcollaborate with AI and ARdancing with a machinecreating new melodiesML-powered DoodleLookoutLive TranscribeProject Euphoniathis researchParrotronGet Image Descriptions from GoogleLens for Google Gospeech recognition modelsvision modelshandwriting recognition modelson-device captioning with Live Captiontranscribing Recorder appGoogle Translate\u2019s camera translationAugmented Faces APIARCorereal-time AR self-expressionon-device, real-time hand trackingImproved, RNN-based on-device handwriting recognitionnew global localization approach using your smart phone\u2019s cameraFederated learningonline comicpowerful machine learning approach invented by Google researchers in 2015large-scale learning systemssurvey article on Federated Learningtake great selfiesprofessional-looking shallow depth of field images and portraitsthe Night Sight feature on Pixel Phones to take some stunning astrophotography picturesmulti-frame super resolutionmobile photography in very low-light conditionsGoogle Healthpublishing research papersbuilding toolsdeep learning model for mammographydeep learning model for differential diagnoses of skin diseasesmachine learning model can predict the onset of acute kidney injuryexpanded the application of deep learning to electronic health records2018 blog posta promising step forward for predicting lung cancerexpand and evaluate our deployment of machine learning tools for detection and prevention of eye diseaseVerilyan augmented reality microscope for cancer diagnosisherehuman-centric, similar-image search toolcomputational taskFull Res VersionFull Res Versionearly exampleeasier to expresseasier to controlclassical machine learning techniques like deep reinforcement learningwhat our quantum computing milestone meansalgorithms and theorygraph miningmarket algorithmssummarizing some of our work in graph learning algorithmsVLDB\u201919Cache-aware load balancing of data center applicationsbalanced partitioning of graphsCache-aware load balancing of data center applicationsICLR\u20192019A new dog learns old tricks: RL finds classic optimization algorithmsFOCS\u201919 papertheorypracticedensity clusteringvocabulary compressionSODA\u201919 paperFOCS 2019 paperbipartite matchingITCS\u201919cachingSODA\u201920market algorithmsinnovations in experimental designNeurIPS\u201919 oral paperWINE2019KDD'19 paperNeurIPS'19 paperexperimental powerRandomized Experimental Design via Geographic ClusteringMeasuring the Limits of Data Parallel Training for Neural NetworkspaperGPipeEvaluating the Unsupervised Learning of Disentangled RepresentationsPredicting the Generalization Gap in Deep Neural NetworksImproving Out-of-Distribution Detection in Machine Learning ModelsOff-Policy ClassificationLearning to Generalize from Sparse and Underspecified RewardsEfficientNet: Improving Accuracy and Efficiency through AutoML and Model ScalingAutoML MNASEfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoMLVideo Architecture SearchAutoML TablesAn End-to-End AutoML Solution for Tabular Data at KaggleDaysExploring Weight Agnostic Neural NetworksApplying AutoML to Transformer Architecturesour paperSpecAugment: A New Data Augmentation Method for Automatic Speech Recognitionkeyword spotting and spoken language identification using AutoMLExploring Massively Multilingual, Massive Neural Machine TranslationBLEU scoreLarge-Scale Multilingual Speech Recognition with a Streaming End-to-End ModelTranslatotron: An End-to-End Speech-to-Speech Translation ModelMultilingual Universal Sentence Encoder for Semantic RetrievalGoogle Talk to BooksWhat fragrance brings back memories?Robust Neural Machine Translationseq2seqTransformerBERTTransformer-XLALBERTGoogle TranslateSmart ComposeGoogle Searchlaunch of BERT in our core search and ranking algorithmsvisual searchNest Hub MaxLensdepth prediction from videosfine-grained temporal understanding of videos using temporal cycle-consistency learningrepresentations across text, speech and video that are temporally consistent from unlabeled videospredict future visual inputs from observations of the pastbetter recall special video moments like \u201cblowing out candles\u201d or \u201csliding down a slide\u201dGoogle Photossignificant research areaLong-Range Robotic Navigation via Automated Reinforcement LearningPlaNet: A Deep Planning Network for Reinforcement LearningUnifying Physics and Deep Learning with TossingBotSoft Actor-Critic: Deep Reinforcement Learning for RoboticsLearning to Assemble and to Generalize from Self-Supervised DisassemblyROBEL: Robotics Benchmarks for Learning with Low-Cost Robotslaunched TensorFlow 2.0mobile GPU inferenceTensorFlow Litelaunched Teachable Machine 2.0MLIRJAXNeurIPS 2019neural tangent kernelsBayesian inferencemolecular dynamicsa preview of JAX on Cloud TPUsMediaPipeXNNPACKCloud TPUsTensorFlow Research CloudIntro To TensorFlow at Courserataking TensorFlow on the roadTensorFlow Worlddiscovered two new planetsreminiscent of African maskscreate the Farmers Companiondetermine safe road conditionsidentify pot holes and dangerous road crackslearns how to add color to black-and-white photosGoogle Dataset Searchshare open data responsiblyOpen Images V5Open ImagesNatural questionsData for deepfake detectionFaceForensics benchmarkGoogle Research FootballGoogle-Landmarks-v2YouTube-8M SegmentsAtomic Visual Actions (AVA) Spoken ActivityAVA action recognition and AVA: Spoken ActivityPAWS and PAWS-XNatural language dialog datasetsThe Visual Task Adaptation BenchmarkSchema-Guided Dialogue DatasetCVPRICMLICLRACLInterspeechICCVNeurIPSGoogle Faculty Research Awards 2018Google AI Residency Programmentored AI-focused startupsResearch office in Bangalorewe\u2019re hiringa nice overview of important advances of the last decadeTPUv1TPUv2 and TPUv3Edge TPUsdeep learning revolution will continue to reshape how we think about computing and computers",
      "link": "http://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html",
      "author": "Posted by Jeff Dean, Senior Fellow and SVP of Google Research and Health, on behalf of the entire Google Research community"
    },
    {
      "title": "ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations",
      "date": "Friday, December 20, 2019",
      "abstract": "ALBERT: A Lite BERT for Self-Supervised Learning of Language RepresentationsBERTnatural language processingALBERT: A Lite BERT for Self-supervised Learning of Language RepresentationsICLR 2020Stanford Question Answering DatasetSATRACEreleasedTensorFlowSQuAD2.0RACETransformerBERTXLNetRoBERTaattention-feedforwardSQuAD2.0RACESQuAD2.0RACESAT Reading TestRACE datasetXLNetRoBERTaopen-sourcing ALBERT",
      "link": "http://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html",
      "author": "Posted by Radu Soricut and Zhenzhong Lan, Research Scientists, Google Research"
    },
    {
      "title": "The On-Device Machine Learning Behind Recorder",
      "date": "Wednesday, December 18, 2019",
      "abstract": "The On-Device Machine Learning Behind RecorderRecorderimprovements announced earlier this yearusing CNNs to classify audio soundsdatasets for audio event detectionunigram and bigram",
      "link": "http://ai.googleblog.com/2019/12/the-on-device-machine-learning-behind.html",
      "author": "Posted by Itay Inbar and Nir Shemy, Software Engineers, Google Research"
    },
    {
      "title": "Improving Out-of-Distribution Detection in Machine Learning Models",
      "date": "Tuesday, December 17, 2019",
      "abstract": "Improving Out-of-Distribution Detection in Machine Learning ModelsLikelihood Ratios for Out-of-Distribution DetectionNeurIPS 2019releasedlikelihoodNCBI catalog of prokaryotic genome sequencesearlierworkPixelCNN++Fashion-MNISTMNISTnucleotidesAUROC scorepaperGitHub repository",
      "link": "http://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html",
      "author": "Posted by Jie Ren, Research Scientist, Google Research and Balaji Lakshminarayanan, Research Scientist, DeepMind"
    },
    {
      "title": "Improvements to Portrait Mode on the Google Pixel 4 and Pixel 4 XL",
      "date": "Monday, December 16, 2019",
      "abstract": "Improvements to Portrait Mode on the Google Pixel 4 and Pixel 4 XLPortrait Modemachine learning to estimate depthdual-pixel auto-focusbokehSLRparallaxcorrespondingaperture problemblog postlast yearTensorflowconvolutional neural networkencodersskip connectionsresidual blocksdrop-outherebokehHDR+album",
      "link": "http://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html",
      "author": "Posted by Neal Wadhwa, Software Engineer and Yinda Zhang, Research Scientist, Google Research"
    },
    {
      "title": "Fairness Indicators: Scalable Infrastructure for Fair ML Systems",
      "date": "Wednesday, December 11, 2019",
      "abstract": "Fairness Indicators: Scalable Infrastructure for Fair ML Systemshighlighting AI principlesTensorFlow WorldFairness Indicatorsmodel cardsinteractive case studyJigsaw\u2019sUnintended Bias in Toxicity datasetnow available in betarepresentationsconfidence intervalsWhat-If Toolpip packageTensorflow Model AnalysisTensorflow Data ValidationFairness Indicators Example ColabFairness Indicators for TensorBoardTensorBoardFairness Indicators with TFHub EmbeddingsFairness Indicators with Cloud Vision API's Face Detection Modelmodel cardsGitHub repositoryGuidance for usageCase StudyToxic Comment Classification datasetTensorFlow ExtendedEvaluatorTensorBoardstandalone toolModel Agnostic TFMAcase studyintroductory videoUnintended Bias in Toxicity datasetactive learningmin-diffFairness Indicators GitHub repothis linktfx@tensorflow.org",
      "link": "http://ai.googleblog.com/2019/12/fairness-indicators-scalable.html",
      "author": "Posted by Catherina Xu and Tulsee Doshi, Product Managers, Google Research"
    },
    {
      "title": "Lessons Learned from Developing ML for Healthcare",
      "date": "Tuesday, December 10, 2019",
      "abstract": "Lessons Learned from Developing ML for HealthcareTIMI Risk Scorea variety of medical applicationscomplex medical recordsimproving the accuracy of genomic sequencingcardiovascular risk factorsrefractive errorgrading diabetic eye diseasediagnosing metastatic breast cancerleverage ML-based tools in an interactive fashionNature MaterialsJournal of the American Medical AssociationHow to develop machine learning models for healthcarepdfNature MaterialsUsers\u2019 Guide to the Medical Literature: How to Read Articles that use Machine LearningJAMAmemorize the training dataset",
      "link": "http://ai.googleblog.com/2019/12/lessons-learned-from-developing-ml-for.html",
      "author": "Posted by Yun Liu, Research Scientist and Po-Hsuan Cameron Chen, Research Engineer, Google Health"
    },
    {
      "title": "Google at NeurIPS 2019",
      "date": "Monday, December 9, 2019",
      "abstract": "Google at NeurIPS 2019Conference on Neural Information Processing SystemsML-based Flood ForecastingAI for Social GoodGoogle Research FootballGoogle Dataset SearchTF-AgentsSocial IntelligenceMemory Efficient Adaptive OptimizationStand-Alone Self-Attention in Vision ModelsHigh Fidelity Video Prediction with Large Neural NetsUnsupervised Learning of Object Structure and Dynamics from VideosGPipe: Efficient Training of Giant Neural Networks using Pipeline ParallelismQuadratic Video InterpolationOnline Stochastic Shortest Path with Bandit Feedback and Unknown Transition FunctionIndividual Regret in Cooperative Nonstochastic Multi-Armed BanditsLearning to ScreenDualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution CorrectionsA Kernel Loss for Solving the Bellman EquationAccurate Uncertainty Estimation and Decomposition in Ensemble LearningSaccader: Improving Accuracy of Hard Attention Models for VisionInvertible Convolutional FlowHypothesis Set Stability and GeneralizationBandits with Feedback Graphs and Switching CostsRegularized Gradient BoostingLogarithmic Regret for Online ControlSampled Softmax with Random Fourier FeaturesMultilabel Reductions: What is My Loss Optimising?MetaInit: Initializing Learning by Learning to InitializeGeneralization Bounds for Neural Networks via Approximate Description LengthVariance Reduction of Bipartite Experiments through Correlation ClusteringLikelihood Ratios for Out-of-Distribution DetectionCan You Trust Your Model\u2019s Uncertainty? Evaluating Predictive Uncertainty Under Dataset ShiftSurrogate Objectives for Batch Policy Optimization in One-step Decision MakingGlobally Optimal Learning for Structured Elliptical LossesDPPNet: Approximating Determinantal Point Processes with Deep NetworksGraph Normalizing FlowsWhen Does Label Smoothing Help?On the Role of Inductive Bias From Simulation and the Transfer to the Real World: a new Disentanglement DatasetOn the Fairness of Disentangled RepresentationsAre Disentangled Representations Helpful for Abstract Visual Reasoning?Don\u2019t Blame the ELBO! A Linear VAE Perspective on Posterior CollapseStabilizing Off-Policy Q-Learning via Bootstrapping Error ReductionOptimizing Generalized Rate Metrics with Game EquilibriumOn Making Stochastic Classifiers DeterministicDiscrete Flows: Invertible Generative Models of Discrete DataGraph Agreement Models for Semi-Supervised LearningA Robust Non-Clairvoyant Dynamic Mechanism for Contextual AuctionsAdversarial Robustness through Local LinearizationA Geometric Perspective on Optimal Representations for Reinforcement LearningOnline Learning via the Differential Privacy LensReducing the Variance in Online Optimization by Transporting Past GradientsUniversality and Individuality in Neural Dynamics Across Large Populations of Recurrent NetworksReverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor DynamicsStrategizing Against No-Regret LearnersPrior-Free Dynamic Auctions with Low Regret BuyersPrivate Stochastic Convex Optimization with Optimal RatesComputational Separations between Sampling and OptimizationMomentum-Based Variance Reduction in Non-Convex SGDKernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise AccelerationFast and Flexible Multi-Task Classification using Conditional Neural Adaptive ProcessesIcebreaker: Element-wise Active Information Acquisition with Bayesian Deep Latent Gaussian ModelMultiview Aggregation for Learning Category-Specific Shape ReconstructionVisualizing and Measuring the Geometry of BERTLocality-Sensitive Hashing for f-Divergences: Mutual Information Loss and BeyondA Benchmark for Interpretability Methods in Deep Neural NetworksPractical and Consistent Estimation of f-DivergencesTree-Sliced Variants of Wasserstein DistancesGame Design for Eliciting Distinguishable BehaviorDifferentially Private Anonymized HistogramsLocally Private Gaussian EstimationExponential Family Estimation via Adversarial Dynamics EmbeddingLearning to Predict Without Looking Ahead: World Models Without Forward PredictionAdaptive Density Estimation for Generative ModelsWeight Agnostic Neural NetworksRetrosynthesis Prediction with Conditional Graph Logic NetworkLarge Scale Structure of Neural Network Loss LandscapesOff-Policy Evaluation via Off-Policy ClassificationDomes to Drones: Self-Supervised Active Triangulation for 3D Human Pose ReconstructionEnergy-Inspired Models: Learning with Sampler-Induced DistributionsFrom Deep Learning to Mechanistic Understanding in Neuroscience: The Structure of Retinal PredictionLanguage as an Abstraction for Hierarchical Deep Reinforcement LearningBayesian Layers: A Module for Neural Network UncertaintyAdaptive Temporal-Difference Learning for Policy Evaluation with Per-State Uncertainty EstimatesA Unified Framework for Data Poisoning Attack to Graph-based Semi-Supervised LearningMixMatch: A Holistic Approach to Semi-Supervised LearningSMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional PoliciesLimits of Private Learning with Access to Public DataRegularized Weighted Low Rank ApproximationUnsupervised Curricula for Visual Meta-Reinforcement LearningSecretary Ranking with Minimal InversionsMixtape: Breaking the Softmax Bottleneck EfficientlyBudgeted Reinforcement Learning in Continuous State SpaceFrom Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox OptimizationFlattening a Hierarchical Clustering through Active LearningRobust Attribution RegularizationRobustness Verification of Tree-based ModelsMeta Architecture SearchContextual Bandits with Cross-LearningDynamic Incentive-Aware Learning: Robust Pricing in Contextual AuctionsOptimizing Generalized Rate Metrics with Three PlayersNoise-Tolerant Fair ClassificationTowards Automatic Concept-based ExplanationsLocally Private Learning without Interaction Requires SeparationLearning GANs and Ensembles Using DiscrepancyCondConv: Conditionally Parameterized Convolutions for Efficient InferenceA Fourier Perspective on Model Robustness in Computer VisionRobust Bi-Tempered Logistic Loss Based on Bregman DivergencesWhich Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic ModelWide Neural Networks of Any Depth Evolve as Linear Models Under Gradient DescentAbstract Reasoning with Distracting FeaturesSearch on the Replay Buffer: Bridging Planning and Reinforcement LearningDifferentiable Ranking and Sorting Using Optimal TransportXLNet: Generalized Autoregressive Pretraining for Language UnderstandingPrivate Learning Implies Online Learning: An Efficient ReductionEvaluating Protein Transfer Learning with TAPETight Dimensionality Reduction for Sketching Low Degree Polynomial KernelsNo Pressure! Addressing the Problem of Local Minima in Manifold Learning AlgorithmsSubspace Detours: Building Transport Plans that are Optimal on Subspace ProjectionsOn the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement DatasetStacked Capsule AutoencodersWasserstein Dependency Measure for Representation LearningSampling Sketches for Concave Sublinear Functions of FrequenciesHamiltonian Neural NetworksComputational Mirrors: Blind Inverse Light Transport by Deep Matrix FactorizationTransfusion: Understanding Transfer Learning for Medical ImagingDifferentially Private Covariance EstimationLearning Transferable Graph ExplorationNeural Attribution for Semantic Bug-Localization in Student ProgramsPyTorch: An Imperative Style, High-Performance Deep Learning LibraryBreaking the Glass Ceiling for Embedding-Based Classifiers for Large Output SpacesEfficient Rematerialization for Deep Networks3rd Conversational AI: Today's Practice and Tomorrow's PotentialAI for Humanitarian Assistance and Disaster Response WorkshopBayesian Deep LearningBeyond First Order Methods in Machine Learning SystemsBiological and Artificial Reinforcement LearningContext and Compositionality in Biological and Artificial Neural SystemsDeep Reinforcement LearningDocument IntelligenceFederated Learning for Data Privacy and ConfidentialityGraph Representation LearningHuman-Centric Machine LearningInformation Theory and Machine LearningKR2ML - Knowledge Representation and Reasoning Meets Machine LearningLearning Meaningful Representations of LifeLearning Transferable SkillsMachine Learning for Creativity and DesignMachine Learning for Health (ML4H): What Makes Machine Learning in Medicine Different?Machine Learning and the Physical SciencesML for SystemsOptimal Transport for Machine LearningThe Optimization Foundations of Reinforcement LearningPrivacy in Machine LearningProgram Transformations for MLReal Neurons & Hidden Units:\u00a0Future Directions at the Intersection of Neuroscience and Artificial IntelligenceRobot Learning: Control and Interaction in the Real WorldSafety and Robustness in Decision MakingScience Meets Engineering of Deep LearningSets and PartitionsTackling Climate Change with MLVisually Grounded Interaction and LanguageWorkshop on Machine Learning with GuaranteesRepresentation Learning and Fairness",
      "link": "http://ai.googleblog.com/2019/12/google-at-neurips-2019.html",
      "author": "Posted by Andrew Helton, Editor, Google Research Communications"
    },
    {
      "title": "Understanding Transfer Learning for Medical Imaging",
      "date": "Friday, December 6, 2019",
      "abstract": "Understanding Transfer Learning for Medical Imagingtransfer learningImageNetinterpreting chest x-raysidentifying eye diseasesearly detection of Alzheimer\u2019s diseasethe effects on performance improvementcontribution of the underlying architecturepretraining dataset type and sizeNeurIPS 2019Transfusion: Understanding Transfer Learning for Medical Imagingdiabetic retinopathyfundus photographsfive different diseases from chest x-raysResNet50Inception-v3convolutional neural networksbatchnormReLUrecentmethodsingular vector canonical correlation analysiscode and tutorialscanonical correlation analysisGabor-likeAUCGabor filters",
      "link": "http://ai.googleblog.com/2019/12/understanding-transfer-learning-for.html",
      "author": "Posted by Maithra Raghu and Chiyuan Zhang, Research Scientists, Google Research"
    },
    {
      "title": "Developing Deep Learning Models for Chest X-rays with Adjudicated Image Labels",
      "date": "Tuesday, December 3, 2019",
      "abstract": "Developing Deep Learning Models for Chest X-rays with Adjudicated Image Labelsearly-stage lung cancerspneumothoraceslung cancer detectionprostate cancer gradingdifferential diagnosesvaluableeffortsChest Radiograph Interpretation with Deep Learning Models: Assessment with Radiologist-adjudicated Reference Standards and Population-adjusted EvaluationRadiologyhereApollo HospitalsChestX-ray14National Institutes of Healthhas limitationssharing with the research community",
      "link": "http://ai.googleblog.com/2019/12/developing-deep-learning-models-for.html",
      "author": "Posted by Dave Steiner, MD, Research Scientist and Shravya Shetty, Technical Lead, Google Health"
    },
    {
      "title": "Astrophotography with Night Sight on Pixel Phones",
      "date": "Tuesday, November 26, 2019",
      "abstract": "Astrophotography with Night Sight on Pixel Phonesexperimentspleasing resultsprevious blog postSIGGRAPH Asia 2019shot noiseDark currentCMOS image sensorselectronic viewfindersAutofocusconvolutional neural networknoise reductionherehere",
      "link": "http://ai.googleblog.com/2019/11/astrophotography-with-night-sight-on.html",
      "author": "Posted by Florian Kainz and  Kiran Murthy, Software Engineers, Google Research"
    },
    {
      "title": "RecSim: A Configurable Simulation Platform for Recommender Systems",
      "date": "Tuesday, November 19, 2019",
      "abstract": "RecSim: A Configurable Simulation Platform for Recommender Systemsrecommender systemsReinforcement learningroboticsRecSimhereMovieLens 1Mresearchchoice modela novel decomposition techniqueQ-valuesTensorFlow\u2019s probabilistic APIspreference elicitationwhite paperhere",
      "link": "http://ai.googleblog.com/2019/11/recsim-configurable-simulation-platform.html",
      "author": "Posted by Martin Mladenov, Research Scientist and Chih-wei Hsu, Software Engineer, Google Research"
    },
    {
      "title": "New Solutions for Quantum Gravity with TensorFlow",
      "date": "Friday, November 15, 2019",
      "abstract": "New Solutions for Quantum Gravity with TensorFlowteaching robots how to throwolfactory properties of moleculesTensorFlowquantum mechanicselectromagneticstrongweakgravitationquantum gravityM-TheoryThe Theory formerly known as Strings,\u201dEdward WittenarticleJournal of High Energy Physicsto 194tachyon-freeGoogle colabPython library",
      "link": "http://ai.googleblog.com/2019/11/new-solutions-for-quantum-gravity-with.html",
      "author": "Posted by Thomas Fischbacher, Software Engineer, Google Research, Z\u00fcrich"
    },
    {
      "title": "SPICE: Self-Supervised Pitch Estimation",
      "date": "Thursday, November 14, 2019",
      "abstract": "SPICE: Self-Supervised Pitch EstimationpYINSWIPECREPEpaperunsupervisedConstant-Q transformvideoconvolutional encoderconstant-Q transformharmonicMIR-1k datasetFreddieMeter",
      "link": "http://ai.googleblog.com/2019/11/spice-self-supervised-pitch-estimation.html",
      "author": "Posted by Marco Tagliasacchi, Research Scientist, Google Research"
    },
    {
      "title": "Introducing the Next Generation of On-Device Vision Models: MobileNetV3 and MobileNetEdgeTPU",
      "date": "Wednesday, November 13, 2019",
      "abstract": "Introducing the Next Generation of On-Device Vision Models: MobileNetV3 and MobileNetEdgeTPUalgorithmically-efficientGoogle Pixel 4Pixel Neural CoreEdge TPUedge computingMobileNetsMobileNetV3MobileNetEdgeTPUAutoMLMobileNetV2previous version of MobileNetMnasNetNetAdaptreinforcement learningImageNetSwishsqueeze-and-excitationsigmoid functioninverted bottleneck structureReLUCOCO datasetsemantic segmentationCityscapes DatasetCoralaccelerator-aware AutoMLDSPsGPUsEfficientNet-EdgeTPUImageNetCOCO14 minival datasetMobileNet github pageTensorflow Object Detection APIDeepLab",
      "link": "http://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html",
      "author": "Posted by Andrew Howard, Software Engineer and Suyog Gupta, Silicon Engineer, Google Research"
    },
    {
      "title": "New Insights into Human Mobility with Privacy Preserving Aggregation",
      "date": "Tuesday, November 12, 2019",
      "abstract": "New Insights into Human Mobility with Privacy Preserving Aggregationour latest work published in Nature Human Behaviorpredicting epidemicsinfrastructure planningnatural disastersimportantdomainsbased on cell carrier logslocation \"check-ins\"Hierarchical Organization of Urban Mobility and Its Connection with City LivabilityNature CommunicationsAI principlesdifferential privacyk-anonymityimplemented in 2014Location History, in order toS2 cellslive trafficparking availabilityChristaller\u2019s workpopulation densitysprawl20 different variablesdebateoptimalAI for Social Goodon-device federated learningsecure aggregation protocolsrandomized responsessecure Chrome from malicious attacksInstitute for Cross-Disciplinary Physics and Complex SystemsComputer Science DepartmentGhoshal LabBruno Kessler Foundationdifferential privacy libraryGitHub repo",
      "link": "http://ai.googleblog.com/2019/11/new-insights-into-human-mobility-with.html",
      "author": "Posted by Adam Sadilek, Software Engineer and Xerxes Dotiwalla, Product Manager, Google Research"
    },
    {
      "title": "Highlights from the 3rd Cohort of the Google AI Residency Program",
      "date": "Thursday, November 7, 2019",
      "abstract": "Highlights from the 3rd Cohort of the Google AI Residency ProgramGoogle AI Residency Program machine perception algorithms and optimization language understanding healthcarelarge-scale studythis postgenerative modelaid designersA method to learn GANslikelihood ratio methodFashion-MNISTstudyCIFAR100workshopQueer in AIDeep Learning for Natural Language ProcessingAdaNetCoconetAI-powered Doodleg.co/airesidency/applyg.co/airesidency",
      "link": "http://ai.googleblog.com/2019/11/highlights-from-2019-google-ai.html",
      "author": "Posted by Katie Meckley, Program Manager, Google AI Residency"
    },
    {
      "title": "The Visual Task Adaptation Benchmark",
      "date": "Wednesday, November 6, 2019",
      "abstract": "The Visual Task Adaptation BenchmarkDeep learningcomputer visionrepresentationsTensorFlow HubPyTorch HubThe Visual Task Adaptation Benchmarkavailable on GitHubImageNetGLUEAtarimedical imagesremote sensingDeepMind LabCLEVRdSpritesdisentangled representationsGANsVAEsImageNetS4LGitHubpublic leaderboardTF HubTPUGPU",
      "link": "http://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html",
      "author": "Posted by Neil Houlsby, Research Scientist and Xiaohua Zhai, Research Engineer, Google Research, Z\u00fcrich"
    },
    {
      "title": "Learning to Assemble and to Generalize from Self-Supervised Disassembly",
      "date": "Thursday, October 31, 2019",
      "abstract": "Learning to Assemble and to Generalize from Self-Supervised DisassemblyForm2Fittime-reversed disassemblyt-SNEt-SNEGitHub repository",
      "link": "http://ai.googleblog.com/2019/10/learning-to-assemble-and-to-generalize.html",
      "author": "Posted by Kevin Zakka, Research Intern and Andy Zeng, Research Scientist, Robotics at Google"
    },
    {
      "title": "On-Device Captioning with Live Caption",
      "date": "Tuesday, October 29, 2019",
      "abstract": "On-Device Captioning with Live Caption\n  Recently we introduced Live Caption, a new Android feature that automatically captions media playing on your phone. The captioning happens in real time, completely on-device, without using network resources, thus preserving privacy and lowering latency. The feature is currently available on Pixel 4 and Pixel 4 XL, will roll out to Pixel 3 models later this year, and will be more widely available on other Android devices soon.Live Captionrecurrent neural networksequence transductionRNN-Tunspoken punctuationconvolutional neural networksound events classificationsound events detectionAudioSetautomatic speech recognitionoptimized for edge-devicesperformswellTensorFlow Litemulti-speaker content",
      "link": "http://ai.googleblog.com/2019/10/on-device-captioning-with-live-caption.html",
      "author": "Posted by Michelle Tadmor-Ramanovich and Nadav Bar, Senior Software Engineers, Google Research, Tel-Aviv"
    },
    {
      "title": "Introducing the Schema-Guided Dialogue Dataset for Conversational Assistants",
      "date": "Monday, October 28, 2019",
      "abstract": "Introducing the Schema-Guided Dialogue Dataset for Conversational Assistantsvirtual assistantsGoogle AssistantTowards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue DatasetSchema-Guided Dialogue datasetintent predictionslot fillinglanguage generationBERTopen sourceAPIsWizard-of-Ozprobabilistic automatonopen-sourced dialogue state tracking modelDialog System Technology ChallengesSchema-Guided Dialogue State Tracking8th DSTCDSTC8 workshopAAAI-20",
      "link": "http://ai.googleblog.com/2019/10/introducing-schema-guided-dialogue.html",
      "author": "Posted by Abhinav Rastogi, Software Engineer and Pranav Khaitan, Engineering Lead, Google Research"
    },
    {
      "title": "Google at ICCV 2019",
      "date": "Monday, October 28, 2019",
      "abstract": "Google at ICCV 2019International Conference on Computer Vision 2019workshopstutorialsMediaPipeOpen ImagesGoogle LensBuilding Rome in a DaySinGAN: Learning a Generative Model from a Single Natural ImageLearning Single Camera Depth Estimation using Dual-PixelsRIO: 3D Object Instance Re-Localization in Changing Indoor EnvironmentsShapeMask: Learning to Segment Novel Objects by Refining Shape PriorsPuppetGAN: Cross-Domain Image Manipulation by DemonstrationCOCO-GAN: Generation by Parts via Conditional CoordinatingTowards Unconstrained End-to-End Text SpottingSinGAN: Learning a Generative Model from a Single Natural ImageGenerative Modeling for Small-Data Object DetectionSearching for MobileNetV3S\u2074L: Self-Supervised Semi-supervised LearningSampling-Free Epistemic Uncertainty Estimation Using Approximated Variance PropagationLinearized Multi-sampling for Differentiable Image TransformationELF: Embedded Localisation of Features in Pre-trained CNNDepth from Videos in the Wild: Unsupervised Monocular Depth Learning from Unknown CamerasForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth ImageA Learned Representation for Scalable Vector GraphicsFrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single RGB ImagePrior-Aware Neural Network for Partially-Supervised Multi-Organ SegmentationBoundless: Generative Adversarial Networks for Image ExtensionCap2Det: Learning to Amplify Weak Caption Supervision for Object DetectionNOTE-RCNN: NOise Tolerant Ensemble RCNN for Semi-supervised Object DetectionObject-Driven Multi-Layer Scene Decomposition from a Single ImageImproving Adversarial Robustness via Guided Complement EntropyXRAI: Better Attributions Through Regions SegSort: Segment Sorting for Semantic SegmentationSelf-Supervised Learning with Geometric Constraints in Monocular Video: Connecting Flow, Depth, and CameraVideoBERT: A Joint Model for Video and Language Representation LearningExplaining the Ambiguity of Object Detection and 6D Pose from Visual DataConstructing Self-Motivated Pyramid Curriculums for Cross-Domain Semantic SegmentationLearning Shape Templates Using Structured Implicit FunctionsTransferable Representation Learning in Vision-and-Language NavigationControllable Attention for Structured Layered Video DecompositionPixel2Mesh++: Multi-view 3D Mesh Generation via DeformationBeyond Cartesian Representations for Local DescriptorsDomain Randomization and Pyramid Consistency: Simulation-to-Real Generalization without Accessing Target Domain DataEvolving Space-Time Neural Architectures for VideosMoulding Humans: Non-parametric 3D Human Shape Estimation from Single ImagesMulti-view Image FusionEvalNorm: Estimating Batch Normalization Statistics for EvaluationAttention Augmented Convolutional NetworksPatchwork: A Patch-wise Attention Network for Efficient Object Detection and Segmentation in Video StreamsLow-Power Computer VisionNeural ArchitectsThe 3rd YouTube-8M Large-Scale Video Understanding WorkshopShould We Pre-register Experiments in Computer Vision?Extreme Vision ModelingJoint COCO and Mapillary Recognition ChallengeOpen Images Challenge",
      "link": "http://ai.googleblog.com/2019/10/google-at-iccv-2019.html",
      "author": "Posted by Andrew Helton, Editor, Google Research Communications"
    },
    {
      "title": "A New Workflow for Collaborative Machine Learning Research in Biodiversity",
      "date": "Friday, October 25, 2019",
      "abstract": "A New Workflow for Collaborative Machine Learning Research in BiodiversityspeciesidentificationFGVCLifeCLEFwildlife camera trap imagesherbarium sheetsscientific research publicationsdesigned with these considerations in mindBiodiversity NextGBIFiNaturalistVisipediaannouncingbillion+ species occurrence countDigital Object IdentifierTensorFlow HubDanish Mycological SocietyTF Hub model pagelive, interactive demonstrationMushroom Recognizerproject page",
      "link": "http://ai.googleblog.com/2019/10/a-new-workflow-for-collaborative.html",
      "author": "Posted by Serge Belongie, Visiting Faculty and Hartwig Adam, Engineering Director, Google Research"
    },
    {
      "title": "Learning to Smell: Using Deep Learning to Predict the Olfactory Properties of Molecules",
      "date": "Thursday, October 24, 2019",
      "abstract": "Learning to Smell: Using Deep Learning to Predict the Olfactory Properties of Moleculestrigger vivid memoriesolfactory receptorsolfactory sensory neuronsolfactory epitheliumolfactory bulbVanillinmulti-label classificationMachine Learning for Scent: Learning Generalizable Perceptual Representations of Small Moleculesgraphsnatural model of choicemessage passingstereoisomers(R)- and (S)-carvonerandom forestsstrong baselineAUROCAUPRC, recall, precisioncolor spacecommon chemoinformatic representationkernel-density estimatefundamental methodology in GNNs",
      "link": "http://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html",
      "author": "Posted by Alexander B Wiltschko, Senior Research Scientist, Google Research"
    },
    {
      "title": "Quantum Supremacy Using a Programmable Superconducting Processor",
      "date": "Wednesday, October 23, 2019",
      "abstract": "Quantum Supremacy Using a Programmable Superconducting Processorquantum computingquantum supremacyQuantum Supremacy Using a Programmable Superconducting Processorquantum logic gatesFull Res VersionFull Res Versionquantum interferenceSchr\u00f6dinger-Feynmanalgorithmextended Church-Turing thesistwo-qubit gatesforward compatiblePauli errorsPrior experimentsstate-spaceNISQ processors",
      "link": "http://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html",
      "author": "Posted by John Martinis, Chief Scientist Quantum Hardware and Sergio Boixo, Chief Scientist Quantum Computing Theory, Google AI Quantum"
    },
    {
      "title": "Audio and Visual Quality Measurement Using Fr\u00e9chet Distance",
      "date": "Tuesday, October 22, 2019",
      "abstract": "Audio and Visual Quality Measurement Using Fr\u00e9chet DistanceWilliam ThomsonPopular Lectures Vol. I, p. 73deep learningImageNetPenn Treebankgenerative modelsStarCraft Video (SCV)Fr\u00e9chet Audio Distance: A Metric for Evaluating Music Enhancement AlgorithmsTowards Accurate Generative Models of Video: A New Metric & ChallengesFVDFADvery difficultgenerative adversarial networksFr\u00e9chet Inception DistanceInception object-recognition networkFr\u00e9chet distancepeak signal-to-noise ratiostructural similarity indexsource-to-distortion ratiomultivariate Gaussian distributionsPlackett-Luce model",
      "link": "http://ai.googleblog.com/2019/10/audio-and-visual-quality-measurement.html",
      "author": "Posted by Kevin Kilgour, Software Engineer and Thomas Unterthiner, Research Software Engineer, Google Research, Z\u00fcrich"
    },
    {
      "title": "Video Architecture Search",
      "date": "Thursday, October 17, 2019",
      "abstract": "Video Architecture Searchfeature representationConvolutional neural networksInceptionResNetZoph et alReal et alwidely explored for imagesEvaNetAssembleNetTinyVideoNetEvolving Space-Time Neural Architectures for VideosICCV 2019evolutionary algorithm(2+1)DcodeAssembleNet: Searching for Multi-Stream Neural Connectivity in Video ArchitecturesRGBoptical flowdirected graphstwo-stream modelsconnection weight learningCharadesMoments-in-TimeTiny Video Networks2D poolinggating layerssqueeze-and-excitation",
      "link": "http://ai.googleblog.com/2019/10/video-architecture-search.html",
      "author": "Posted by Michael S. Ryoo, Research Scientist and AJ Piergiovanni, Student Researcher, Robotics at Google"
    },
    {
      "title": "Exploring Massively Multilingual, Massive Neural Machine Translation",
      "date": "Friday, October 11, 2019",
      "abstract": "Exploring Massively Multilingual, Massive Neural Machine TranslationWarren Weaverneural machine translationautomatic speech recognitiontext-to-speech123scaling up the number of languagesMassively Multilingual Neural Machine Translation in the Wild: Findings and Challenges4567great challenge in NMTtransferBLEU scoreBLEUalready knownrepresentational capacity5linguistically similar languageslong-standing intuitionsexploit these similarities6linguistic familyrecognizedbetter learningcapacity controlour studyGPipe4TransformersBLEUdepth-width trade-offsparsely-gated mixture of experts7** The Cambridge Handbook of Endangered Languages (Austin and Sallabank, 2011). \u21a9*\u21a9",
      "link": "http://ai.googleblog.com/2019/10/exploring-massively-multilingual.html",
      "author": "Posted by Ankur Bapna, Software Engineer and Orhan Firat, Research Scientist, Google Research"
    },
    {
      "title": "ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots",
      "date": "Wednesday, October 9, 2019",
      "abstract": "ROBEL: Robotics Benchmarks for Learning with Low-Cost Robotsdm_controlOpenAI-GymDDPGQT-OptSoft Actor-CriticPR2Kuka-armsShadowHandBaxterROBEL: Robotics Benchmarks for Learning with Low-Cost RobotsCoRL 2019optical tableD'ClawD'KittyD\u2019KittyD\u2019ClawD\u2019Lanternheretechnical reportSACcomprehensive galleryOn-hardware trainingDAPGNatural Policy GradientMuJoCoNatural Policy GradientMuJoCoSACNatural Policy GradientMuJoCoNatural Policy GradientMuJoCoNatural Policy GradientMuJoCoHierarchical Sim2RealHierarchical Sim2RealHierarchical Sim2Realroboticsbenchmarks.org",
      "link": "http://ai.googleblog.com/2019/10/robel-robotics-benchmarks-for-learning.html",
      "author": "Posted by Michael Ahn, Software Engineer and Vikash Kumar, Research Scientist, Robotics at Google"
    },
    {
      "title": "Improving Quantum Computation with Classical Machine Learning",
      "date": "Thursday, October 3, 2019",
      "abstract": "Improving Quantum Computation with Classical Machine Learningqubitenergyphotonsphononsclassicalwhite noisedigital-to-analogUniversal Quantum Control through Deep Reinforcement LearningNature Partner Journal (npj) Quantum Informationcost functionquantum logic gatedeep reinforcement learningtrusted-region RLoff-policy RL methodsQ-learning",
      "link": "http://ai.googleblog.com/2019/10/improving-quantum-computation-with.html",
      "author": "Posted by Murphy Yuezhen Niu and Sergio Boixo, Research Scientists"
    },
    {
      "title": "Releasing PAWS and PAWS-X: Two New Datasets to Improve Natural Language Understanding Models",
      "date": "Wednesday, October 2, 2019",
      "abstract": "Releasing PAWS and PAWS-X: Two New Datasets to Improve Natural Language Understanding ModelsBERTParaphrase Adversaries from Word ScramblingPAWS-Xlexical overlapQuora Question PairsWikipediaPAWS: Paraphrase Adversaries from Word Scramblinglanguage modelback-translationwe extended itneural machine translation (NMT) serviceBERTDIINQuora Question PairsBag-of-Wordsmultilingual BERT",
      "link": "http://ai.googleblog.com/2019/10/releasing-paws-and-paws-x-two-new.html",
      "author": "Posted by Yuan Zhang, Research Scientist and Yinfei Yang, Software Engineer, Google Research"
    },
    {
      "title": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model",
      "date": "Monday, September 30, 2019",
      "abstract": "Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Modelautomatic speech recognitionLarge-Scale Multilingual Speech Recognition with a Streaming End-to-End ModelInterspeech 20191234combine all three components into a single neural network12Voice SearchGBoard dictationrecent researchRecurrent Neural Network Transducertyping in real timeone-hotresidual adapter modulespaper",
      "link": "http://ai.googleblog.com/2019/09/large-scale-multilingual-speech.html",
      "author": "Posted by Arindrima Datta and Anjuli Kannan, Software Engineers, Google Research"
    },
    {
      "title": "Contributing Data to Deepfake Detection Research",
      "date": "Tuesday, September 24, 2019",
      "abstract": "Contributing Data to Deepfake Detection Researchmore accessible through text-to-speechgenerate training data for medical imagingdeepfakesAI Principlesdataset of synthetic speechinternational challengeJigsawFaceForensics benchmarkProf. Matthias NiessnerProf. Luisa VerdolivaFaceForensics teamFaceForensics github page",
      "link": "http://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html",
      "author": "Posted by Nick Dufour, Google Research and Andrew Gully, Jigsaw"
    },
    {
      "title": "An Inside Look at Flood Forecasting",
      "date": "Wednesday, September 18, 2019",
      "abstract": "An Inside Look at Flood Forecastingflood forecasting pilotexpanded our flood forecasting coverageAI for Social GoodIndian Central Water Commissionstream gaugesdigital elevation modelsembankmentsSaint-Venant equationsTensor Processing Unitsdata-driven discretizationEuropean Space AgencySentinel-1Synthetic-Aperture Radarmulti-task learningJKU Institute For Machine LearningKratzert et al.LSTMsNSE",
      "link": "http://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html",
      "author": "Sella Nevo, Senior Software Engineer, Google Research, Tel Aviv"
    },
    {
      "title": "Project Ihmehimmeli: Temporal Coding in Spiking Neural Networks",
      "date": "Wednesday, September 18, 2019",
      "abstract": "Project Ihmehimmeli: Temporal Coding in Spiking Neural Networksvideogamesfine-grained understanding of videoartificial spiking neural networkshimmelipublishedopen-sourcedbiologically-inspiredtransfer functionMNISTrecover representations",
      "link": "http://ai.googleblog.com/2019/09/project-ihmehimmeli-temporal-coding-in.html",
      "author": "Posted by Iulia-Maria Com\u0219a and Krzysztof Potempa, Research Engineers, Google Research, Z\u00fcrich"
    },
    {
      "title": "Google at Interspeech 2019",
      "date": "Sunday, September 15, 2019",
      "abstract": "Google at Interspeech 201920th Annual Conference of the International Speech Communication AssociationParrotronNeural Machine TranslationBuilding Large-Vocabulary ASR Systems for Languages Without Any Audio Training DataMulti-Microphone Adaptive Noise Cancellation for Robust Hotword DetectionDirect Speech-to-Speech Translation with a Sequence-to-Sequence ModelImproving Keyword Spotting and Language Identification via Neural Architecture Search at ScaleShallow-Fusion End-to-End Contextual Biasing VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking SpecAugment: A Simple Data Augmentation Method for Automatic Speech RecognitionTwo-Pass End-to-End Speech RecognitionOn the Choice of Modeling Unit for Sequence-to-Sequence Speech RecognitionContextual Recovery of Out-of-Lattice Named Entities in Automatic Speech RecognitionJoint Speech Recognition and Speaker Diarization via Sequence TransductionPersonalizing ASR for Dysarthric and Accented Speech with Limited DataAn Investigation Into On-Device Personalization of End-to-End Automatic Speech Recognition ModelsSalient Speech Representations Based on Cloned NetworksCross-Lingual Consistency of Phonological Features: An Empirical StudyLibriTTS: A Corpus Derived from LibriSpeech for Text-to-SpeechImproving Performance of End-to-End ASR on Numeric SequencesDeveloping Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across LanguagesPhoneme-Based Contextualization for Cross-Lingual Speech Recognition in End-to-End ModelsFr\u00e9chet Audio Distance: A Reference-free Metric for Evaluating Music Enhancement AlgorithmsLearning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice CloningSampling from Stochastic Finite Automata with Applications to CTC DecodingLarge-Scale Multilingual Speech Recognition with a Streaming End-to-End ModelA Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNetLow-Dimensional Bottleneck Features for On-Device Continuous Speech RecognitionUnified Verbalization for Speech Recognition & Synthesis Across LanguagesBetter Morphology Prediction for Better Speech SystemsDual Encoder Classifier Models as Constraints in Neural Text NormalizationLarge-Scale Visual Speech RecognitionParrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation",
      "link": "http://ai.googleblog.com/2019/09/google-at-interspeech-2019.html",
      "author": "Andrew Helton, Editor, Google Research Communications"
    },
    {
      "title": "Using Deep Learning to Inform Differential Diagnoses of Skin Diseases",
      "date": "Thursday, September 12, 2019",
      "abstract": "Using Deep Learning to Inform Differential Diagnoses of Skin DiseasespublishedNature Medicine37%more than half of those patients are seen by non-dermatologists24%70%7796malignant or benignwhether a lesion is melanomaare not malignantA Deep Learning System for Differential Diagnosis of Skin Diseasesstasis dermatitiscellulitisdifferential diagnosisInception-v4teledermatologyFitzpatrick skin type",
      "link": "http://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html",
      "author": "Posted by Yuan Liu, PhD, Software Engineer and Peggy Bui, MD, Technical Program Manager, Google Health"
    },
    {
      "title": "Learning Cross-Modal Temporal Representations from Unlabeled Videos",
      "date": "Wednesday, September 11, 2019",
      "abstract": "Learning Cross-Modal Temporal Representations from Unlabeled Videostemporal localizationaction detectionself-driving carsnaturally resides in the data itselfVideoBERT: A Joint Model for Video and Language Representation LearningContrastive Bidirectional Transformer for Temporal Representation Learningautomatic speech recognitioncross-modal learningBidirectional Encoder Representations from Transformersnatural language processingTransformercloze testtokensTransformer\u201czero-shot\u201d classificationContrastive Bidirectional Transformerstransfer learningcontrastive lossmutual informationaverage poolingLSTMsAvgPoolLSTM",
      "link": "http://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html",
      "author": "Posted by Chen Sun and Cordelia Schmid, Research Scientists, Google Research"
    },
    {
      "title": "Recursive Sketches for Modular Deep Learning",
      "date": "Tuesday, September 10, 2019",
      "abstract": "Recursive Sketches for Modular Deep Learningsmallefficientrobustthe foundational work of Alon, Matias, and SzegedyRecursive Sketches for Modular Deep LearningICML 2019variety of efficient algorithmsquantilesinterquartile rangefrequent elementssupport sizenormsentropy estimationlinear regressiondeep neural networksWord2VecImage EmbeddingsGloveDeepWalkBERTmodularmodular deep networkNeural Modular NetworksCapsule Neural NetworksPathNetconvolutional neural networksconvolution kernelsdictionary learningmodel interpretabilityknowledge grapharchitecture search",
      "link": "http://ai.googleblog.com/2019/09/recursive-sketches-for-modular-deep.html",
      "author": "Posted by Badih Ghazi and  Joshua R. Wang, Research Scientists, Google Research"
    },
    {
      "title": "Assessing the Quality of Long-Form Synthesized Speech",
      "date": "Monday, September 9, 2019",
      "abstract": "Assessing the Quality of Long-Form Synthesized Speech assistantssmart speaker devicesgenerating speech for low-resource\u00a0languagescreating human-like speech with Tacotron 2Evaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and ParagraphsSSW10t-test",
      "link": "http://ai.googleblog.com/2019/09/assessing-quality-of-long-form.html",
      "author": "Posted by Tom Kenter, Google Research, London"
    },
    {
      "title": "Announcing Two New Natural Language Dialog Datasets",
      "date": "Friday, September 6, 2019",
      "abstract": "Announcing Two New Natural Language Dialog DatasetsCoached Conversational Preference ElicitationTaskmaster-1Wizard-of-Ozresearch paper2019 Annual Conference of the Special Interest Group on Discourse and Dialogueresearch paper2019 Conference on Empirical Methods in Natural Language ProcessingCCPETaskmaster-1",
      "link": "http://ai.googleblog.com/2019/09/announcing-two-new-natural-language.html",
      "author": "Posted by Bill Byrne and Filip Radlinski, Research Scientists, Google Research"
    },
    {
      "title": "Announcement of the 2019 Fellowship Awardees and Highlights from the Google PhD Fellowship Summit",
      "date": "Thursday, September 5, 2019",
      "abstract": "Announcement of the 2019 Fellowship Awardees and Highlights from the Google PhD Fellowship SummitPhD Fellowship ProgramFLIPPeter NorvigPeggy ChiJeff DeanVinodkumar Prabhakaranhere",
      "link": "http://ai.googleblog.com/2019/09/announcement-of-2019-fellowship.html",
      "author": "Posted by Susie Kim, Program Manager, University Relations"
    },
    {
      "title": "Giving Lens New Reading Capabilities in Google Go",
      "date": "Wednesday, September 4, 2019",
      "abstract": "Giving Lens New Reading Capabilities in Google GoGoogle LensKnowledge Graphwe announcedGoogle Go available globally in the Google Play StoreCameraXJetpackAndroid Camera2 APIoptical character recognitionregion proposal networkHough TransformText FlowKnowledge Graphconvolutional neural networkslong short-term memoryReCaptchaGoogle Bookshomographic distortionsneural machine translation (NMT) algorithmsGoogle Text-to-Speech (TTS)WaveNet",
      "link": "http://ai.googleblog.com/2019/09/giving-lens-new-reading-capabilities-in.html",
      "author": "Posted by Rajan Patel, Director, Augmented Reality"
    },
    {
      "title": "Exploring Weight Agnostic Neural Networks",
      "date": "Tuesday, August 27, 2019",
      "abstract": "Exploring Weight Agnostic Neural Networksneural networkimage classificationreinforcement learningneural architecture searchconvolutional network componentstransformer blocksconvolutional networksinductive biasesrandomly initializedprecocial speciesanti-predator behaviorsWeight Agnostic Neural NetworkscodeBipedalWalker-v2topologysearch algorithmmulti-objective optimizationswing-up cartpole taskhow animals learnCar RacingensembleMNISTBaldwin effectevolutionarypressureinteractive articlepdfPrettyNEAT",
      "link": "http://ai.googleblog.com/2019/08/exploring-weight-agnostic-neural.html",
      "author": "Posted by Adam Gaier, Student Researcher and David Ha, Staff Research Scientist, Google Research, Tokyo"
    },
    {
      "title": "Bi-Tempered Logistic Loss for Training Neural Nets with Noisy Data",
      "date": "Monday, August 26, 2019",
      "abstract": "Bi-Tempered Logistic Loss for Training Neural Nets with Noisy Dataflerkenloss functionlogistic losssensitive to outliersdecision boundaryactivationmarginsoftmax transfer functionrecent papertail-heavinessinteractive visualizationfeed-forward neural networkvisualizationour paperManfred WarmuthTomer Korenhere",
      "link": "http://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html",
      "author": "Posted by Ehsan Amid, Student Researcher and Rohan Anil, Software Engineer, Google Research"
    },
    {
      "title": "Turbo, An Improved Rainbow Colormap for Visualization",
      "date": "Tuesday, August 20, 2019",
      "abstract": "Turbo, An Improved Rainbow Colormap for Visualizationdepth imagesimage differencinglinked to lower accuracysuch as medical imagingDisparity imageperceptually uniformProtanopiamany modern alternativesmatplotlibherehereheresRGBcubic splineC2 continuousit does affect people\u2019s choiceLightnessCIECAM02-UCSachromatopsiaour attention system prioritizes huedepth estimationdivergingdifference imagescolor blindness simulatorProtanomalyProtanopiaDeuteranomalyDeuteranopiaTritanomalyTritanopiaBlue cone monochromacyAchromatopsiaherehereherevisualizing disparity maps",
      "link": "http://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html",
      "author": "Posted by Anton Mikhailov, Senior Software Engineer, Daydream"
    },
    {
      "title": "On-Device, Real-Time Hand Tracking with MediaPipe",
      "date": "Monday, August 19, 2019",
      "abstract": "On-Device, Real-Time Hand Tracking with MediaPipesign languageaugmented realityCVPR 2019MediaPipeproviding this hand perception functionalityMediaPipeface meshML pipelinepose estimationsingle-shot detector modelBlazeFaceavailablenon-maximum suppression algorithmanchorsRetinaNetfocal lossaverage precisionMediaPipedirected graphwide variety of devices and platformsTFLite GPU inferenceMediaPipehere",
      "link": "http://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html",
      "author": "Posted by Valentin Bazarevsky and Fan Zhang, Research Engineers, Google Research"
    },
    {
      "title": "Joint Speech Recognition and Speaker Diarization via Sequence Transduction",
      "date": "Friday, August 16, 2019",
      "abstract": "Joint Speech Recognition and Speaker Diarization via Sequence TransductionThis basic multi-stage approachrecurrent neural network transducerpresented recentlyJoint Speech Recognition and Speaker Diarization via Sequence TransductionInterspeech 2019Gaussian mixture modelsrecent blog postloss functionforward-backward algorithmTPU friendly implementationefficient implementation of the RNN-T lossunderstanding medical conversations",
      "link": "http://ai.googleblog.com/2019/08/joint-speech-recognition-and-speaker.html",
      "author": "Posted by Laurent El Shafey, Software Engineer and Izhak Shafran, Research Scientist, Google Health"
    },
    {
      "title": "Project Euphonia\u2019s Personalized Speech Recognition for Non-Standard Speech",
      "date": "Tuesday, August 13, 2019",
      "abstract": "Project Euphonia\u2019s Personalized Speech Recognition for Non-Standard Speechword error ratesPersonalizing ASR for Dysarthric and Accented Speech with Limited DataInterspeech 2019Project EuphoniaParrotronRNN-TransducerListen, Attend, and SpellgraphemesPrabhavalkar et al. 2017ALS Therapy Development Institutefiller wordsL2 Arctic datasetCMU Arctic promptsALS Functional Rating ScalepreviousspeechworkSAMPA phonemesSAMPA phonemeLibrispeechthis form",
      "link": "http://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html",
      "author": "Posted by Joel Shor and Dotan Emanuel, Research Engineers, Google Research, Tel Aviv"
    },
    {
      "title": "Video Understanding Using Temporal Cycle-Consistency Learning",
      "date": "Thursday, August 8, 2019",
      "abstract": "Video Understanding Using Temporal Cycle-Consistency LearningpowerfuldeeplearningmodelsTemporal Cycle-Consistency LearningcodebasecomputervisionResNetvideos from the Penn Action Datasetself-supervisedlearningcodebasePennAction",
      "link": "http://ai.googleblog.com/2019/08/video-understanding-using-temporal.html",
      "author": "Posted by Debidatta Dwibedi, Research Associate, Google Research"
    },
    {
      "title": "EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML",
      "date": "Tuesday, August 6, 2019",
      "abstract": "EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoMLMoore\u2019s Lawhardware acceleratorsedge computingEfficientNet-EdgeTPUEfficientNetsEdge TPUCoral Dev BoardUSB AcceleratorEfficientNetsAutoML MNAS frameworkreinforcement learningdepthwise-separable convolutionsswish non-linearitysqueeze-and-excitation blockResNetInceptiongithub repositorypost-training quantization toolherethis post on the Google Developer\u2019s BlogCoral website",
      "link": "http://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html",
      "author": "Posted by Suyog Gupta, Machine Learning Accelerator Architect and Mingxing Tan, Software Engineer, Google Research"
    },
    {
      "title": "An Interactive, Automated 3D Reconstruction of a Fly Brain",
      "date": "Monday, August 5, 2019",
      "abstract": "An Interactive, Automated 3D Reconstruction of a Fly Brain connectomicsfruit flyfruit fly researchJanelia Research CampusCambridge UniversityAutomated Reconstruction of a Serial-Section EM Drosophila Brain with Flood-Filling Networks and Local Realignmentdownloadbrowse onlineinteractive viewing20162018resultingCloud TPUsFlood-Filling Networksautomatically traced each individual neuronSegmentation-Enhanced CycleGANgenerative adversarial networkcomplicated shapesnew visualization technologiesThe result is Neuroglancergithubavailable hereconnectomesynapsesFlyEM teamimagesFIB-SEM",
      "link": "http://ai.googleblog.com/2019/08/an-interactive-automated-3d.html",
      "author": "Posted by Peter H. Li, Research Scientist and Jeremy Maitin-Shepard, Software Engineer, Connectomics at Google"
    },
    {
      "title": "Robust Neural Machine Translation",
      "date": "Monday, July 29, 2019",
      "abstract": "Robust Neural Machine Translationneural machine translationTransformer modelsdeep neural networksTransformerRobust Neural Machine Translation with Doubly Adversarial InputsACL 2019generative adversarial networksBLEUMiyao et al.Cheng et al.",
      "link": "http://ai.googleblog.com/2019/07/robust-neural-machine-translation.html",
      "author": "Posted by Yong Cheng, Software Engineer, Google Research"
    },
    {
      "title": "Google at ACL 2019",
      "date": "Monday, July 29, 2019",
      "abstract": "Google at ACL 20192019 Annual Meeting of the Association for Computational LinguisticsNatural Questions corpusMultilingual Universal Sentence EncoderA Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag HierarchyDo Neural Dialog Systems Use the Conversation History Effectively? An Empirical StudyGenerating Logical Forms from Graph Representations of Text and EntitiesExtracting Symptoms and their Status from Clinical ConversationsStay on the Path: Instruction Fidelity in Vision-and-Language NavigationMeaning to Form: Measuring Systematicity as InformationMatching the Blanks: Distributional Similarityfor Relation LearningTransformer-XL: Attentive Language Models Beyond a Fixed-Length ContextHighRES: Highlight-based Reference-less Evaluation of SummarizationZero-Shot Entity Linking by Reading Entity DescriptionsRobust Neural Machine Translation with Doubly Adversarial InputsNatural Questions: a Benchmark for Question Answering ResearchLike a Baby: Visually Situated Neural Language AcquisitionWhat Kind of Language Is Hard to Language-Model?How Multilingual is Multilingual BERT?Handling Divergent Reference Texts when Evaluating Table-to-Text GenerationBAM! Born-Again Multi-Task Networks for Natural Language UnderstandingDynamically Composing Domain-Data Selection with Clean-Data Selection by \u201cCo-Curricular Learning\" for Neural Machine TranslationMonotonic Infinite Lookback Attention for Simultaneous Machine TranslationOn the Robustness of Self-Attentive ModelsNeural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear BHow Large Are Lions? Inducing Distributions over Quantitative AttributesBERT Rediscovers the Classical NLP PipelineCan You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language ModelingRobust Zero-Shot Cross-Domain Slot Filling with Example ValuesLatent Retrieval for Weakly Supervised Open Domain Question AnsweringOn-device Structured and Context Partitioned Projection NetworksIncorporating Priors with Feature Attribution on Text ClassificationInformative Image Captioning with External Sources of InformationReducing Word Omission Errors in Neural\u00a0Machine Translation: A Contrastive Learning ApproachSynthetic QA Corpora Generation with Roundtrip ConsistencyUnsupervised Paraphrasing without TranslationWidening NLP 2019NLP for Conversational AIThe Fourth Arabic Natural Language Processing WorkshopThe Third Workshop on Abusive Language OnlineTyP-NLP, Typology for Polyglot NLPGender Bias in Natural Language ProcessingWikipedia as a Resource for Text Analysis and Retrieval",
      "link": "http://ai.googleblog.com/2019/07/google-at-acl-2019.html",
      "author": "Andrew Helton, Editor, Google Research Communications"
    },
    {
      "title": "Learning Better Simulation Methods for Partial Differential Equations",
      "date": "Tuesday, July 23, 2019",
      "abstract": "Learning Better Simulation Methods for Partial Differential Equationsimpacts of climate changeairplanessimulate a fusion reactorpartial differential equationsMoore\u2019s law has been slowingfaster hardwareLearning Data Driven Discretizations for Partial Differential EquationsProceedings of the National Academy of Sciencesclosed-form solutionsdiscretizations\u201dfinite differencesstate-of-the-art weather modelcrude approximationskey source of uncertainty NOAAexisting schemesBurgers\u2019 equationfinite volume methodsconservation of momentumblending machine learning",
      "link": "http://ai.googleblog.com/2019/07/learning-better-simulation-methods-for.html",
      "author": "Posted by Stephan Hoyer, Software Engineer, Google Research"
    },
    {
      "title": "Building SMILY, a Human-Centric, Similar-Image Search Tool for Pathology",
      "date": "Friday, July 19, 2019",
      "abstract": "Building SMILY, a Human-Centric, Similar-Image Search Tool for Pathologydiabetic eye diseasemetastatic breast canceranatomic pathologycontent-based image retrievalreverse image searchGoogle Imagessimilar image searchSimilar Image Search for Histopathology: SMILYNature Partner Journal (npj) Digital MedicineHuman-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Makingherehonorable mention2019 ACM CHI Conference on Human Factors in Computing SystemsThe Cancer Genome Atlasiterative diagnosissecond paperGoogle HealthPAIRherehere",
      "link": "http://ai.googleblog.com/2019/07/building-smily-human-centric-similar.html",
      "author": "Posted by Narayan Hegde, Software Engineer, Google Health and Carrie J. Cai, Research Scientist, Google Research"
    },
    {
      "title": "Parrotron: New Research into Improving Verbal Communication for People with Speech Impairments",
      "date": "Wednesday, July 17, 2019",
      "abstract": "Parrotron: New Research into Improving Verbal Communication for People with Speech Impairmentsautomatic speech recognitiontext-to-speech synthesisProject EuphoniaParrotronend-to-enddeep neural networkattention-based sequence-to-sequence modelParallel WaveNetphonotacticProject Euphoniamultitask objectivespectrogramsprofoundly deaftransliterationpeoplemuscular dystrophyoutcomesProject Euphoniaour papergithub repositorythis short formALS-TDI",
      "link": "http://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html",
      "author": "Posted by Fadi Biadsy, Research Scientist and Ron Weiss, Software Engineer, Google Research"
    },
    {
      "title": "Multilingual Universal Sentence Encoder for Semantic Retrieval",
      "date": "Friday, July 12, 2019",
      "abstract": "Multilingual Universal Sentence Encoder for Semantic Retrievalintroduced last yearUniversal Sentence Encoder (USE) for EnglishTensorflow Hubsentiment classifierthree new USE multilingual modulesquestion-answer retrievalmulti-task dual-encoder frameworkdual-encoder with additive margin softmax approachUniversal Sentence Encodernearest neighbor problemprecision and recallsemantically similar textfind comparable sentences from 50 million sentences in wikipediaChidambaram et al. (2018)question-answer retrieval applicationsGoogle Talk to BooksWhat fragrance brings back memories?semantic similarity for natural language",
      "link": "http://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html",
      "author": "Posted by Yinfei Yang and Amin Ahmad, Software Engineers, Google Research"
    },
    {
      "title": "Advancing Semi-supervised Learning with Unsupervised Data Augmentation",
      "date": "Wednesday, July 10, 2019",
      "abstract": "Advancing Semi-supervised Learning with Unsupervised Data AugmentationGPUTPUImageNetnatural language processingvisionspeechsupervised learningUnsupervised Data Augmentation (UDA) for Consistency Trainingsemi-supervised learningrevival of semi-supervised learningBERTgithubloss functionGaussian noiseadversarial noiseback translationAutoAugmentTF-IDFIMDbsentiment analysisCIFAR-10VATICTPyramidNet+ShakeDropSVHNImageNetcodebasepreprintMixMatchS4L",
      "link": "http://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html",
      "author": "Posted by Qizhe Xie, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team"
    },
    {
      "title": "Predicting the Generalization Gap in Deep Neural Networks",
      "date": "Tuesday, July 9, 2019",
      "abstract": "Predicting the Generalization Gap in Deep Neural NetworksDeepneural networksimage recognitionimage segmentationmachine translationVC-dimensionRademacher complexitygeneralize poorlyrecent workdecision boundarysupport-vector machinestheoretical upper boundsICLR 2019Predicting the Generalization Gap in Deep Networks with Margin DistributionsGithub repositorya convolutional neural networkCIFAR-10our paperlinear regressionResNet-32Deep Model GeneralizationNetwork-in-NetworkregularizationGithub repository",
      "link": "http://ai.googleblog.com/2019/07/predicting-generalization-gap-in-deep.html",
      "author": "Posted by Yiding Jiang, Google AI Resident"
    },
    {
      "title": "Announcing the YouTube-8M Segments Dataset",
      "date": "Friday, June 28, 2019",
      "abstract": "Announcing the YouTube-8M Segments DatasetFirstSecond YouTube-8M Large-Scale Video Understanding Challenge and WorkshopYouTube-8M SegmentsKaggle video understanding challenge3rd Workshop on YouTube-8M Large-Scale Video Understanding2019 International Conference on Computer Visioncapturing special video momentsYouTube-8M releaseKaggle competition pageworkshop page",
      "link": "http://ai.googleblog.com/2019/06/announcing-youtube-8m-segments-dataset.html",
      "author": "Posted by Joonseok Lee and Joe Yue-Hei Ng, Software Engineers, Google Research"
    },
    {
      "title": "Predicting Bus Delays with Machine Learning",
      "date": "Thursday, June 27, 2019",
      "abstract": "Predicting Bus Delays with Machine Learningintroduced live traffic delays for busesfirst launched in India three weeks agospeech processingmachine translationpopular times at businessesparking difficulty",
      "link": "http://ai.googleblog.com/2019/06/predicting-bus-delays-with-machine.html",
      "author": "Posted by Alex Fabrikant, Research Scientist, Google Research"
    },
    {
      "title": "Innovations in Graph Representation Learning",
      "date": "Tuesday, June 25, 2019",
      "abstract": "Innovations in Graph Representation Learningfriendship relationsgraph analysisclusteringlink predictionprivacyothersgraph embeddingDeepWalkKarateDeepWalkIs a Single Embedding Enough? Learning Node Representations that Capture Multiple Social ContextsWWW\u201919Watch Your Step: Learning Node Embeddings via Graph AttentionNeurIPS\u201918graph embeddingsWWW\u201919 paperego-network analysispersona graph conceptup to 90% reductionnode2veclink prediction and node classificationsecond paperDeepWalkbackpropagationAutoMLcodeGraph Mining teamAlgorithm and Optimization",
      "link": "http://ai.googleblog.com/2019/06/innovations-in-graph-representation.html",
      "author": "Posted by Alessandro Epasto, Senior Research Scientist and Bryan Perozzi, Senior Research Scientist, Graph Mining Team"
    },
    {
      "title": "Off-Policy Classification - A New Reinforcement Learning Model Selection Method",
      "date": "Wednesday, June 19, 2019",
      "abstract": "Off-Policy Classification - A New Reinforcement Learning Model Selection MethodReinforcement learningwalkinggraspingAutoMLOff-Policy Evaluation via Off-Policy Classificationimportance samplingQ-functionQ-learningpositive-unlabeled learningsimulated datatransfer learning techniquessample complexityrandom textures and lightingRCANtemporal-difference errortemporal differencearXiv",
      "link": "http://ai.googleblog.com/2019/06/off-policy-classification-new.html",
      "author": "Unknown"
    },
    {
      "title": "Google at CVPR 2019",
      "date": "Monday, June 17, 2019",
      "abstract": "Google at CVPR 20192019 Conference on Computer Vision and Pattern Recognitionworkshopstutorialsmachine perceptionpredicting pedestrian motionOpen Images V5 datasetRelational Action ForecastingPushing the Boundaries of View Extrapolation With Multiplane ImagesAuto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image SegmentationAutoAugment: Learning Augmentation Strategies From DataDeepView: View Synthesis With Learned Gradient DescentNormalized Object Coordinate Space for Category-Level 6D Object Pose and Size EstimationDo Better ImageNet Models Transfer Better?TextureNet: Consistent Local Parametrizations for Learning From High-Resolution Signals on MeshesDiverse Generation for Multi-Agent Sports GamesOccupancy Networks: Learning 3D Reconstruction in Function SpaceA General and Adaptive Robust Loss FunctionLearning the Depths of Moving People by Watching Frozen PeopleComposing Text and Image for Image Retrieval - an Empirical OdysseyLearning to Synthesize Motion BlurNeural Rerendering in the WildNeural Illumination: Lighting Prediction for Indoor EnvironmentsUnprocessing Images for Learned Raw DenoisingCo-Occurrent Features in Semantic SegmentationCrDoCo: Pixel-Level Domain Transfer With Cross-Domain ConsistencyIm2Pencil: Controllable Pencil Illustration From PhotographsMode Seeking Generative Adversarial Networks for Diverse Image SynthesisRevisiting Self-Supervised Visual Representation LearningScene Graph Generation With External Knowledge and Image ReconstructionScene Memory Transformer for Embodied Agents in Long-Horizon TasksSpatially Variant Linear Representation Models for Joint FilteringTarget-Aware Deep TrackingTemporal Cycle-Consistency LearningDepth-Aware Video Frame InterpolationMnasNet: Platform-Aware Neural Architecture Search for MobileA Compact Embedding for Facial Expression SimilarityContrastive Adaptation Network for Unsupervised Domain AdaptationDeepLight: Learning Illumination for Unconstrained Mobile Mixed RealityDetect-To-Retrieve: Efficient Regional Aggregation for Image SearchFast Object Class Labelling via SpeechLearning Independent Object Motion From Unlabelled Stereoscopic VideosPeeking Into the Future: Predicting Future Person Activities and Locations in VideosSpotTune: Transfer Learning Through Adaptive Fine-TuningNAS-FPN: Learning Scalable Feature Pyramid Architecture for Object DetectionClass-Balanced Loss Based on Effective Number of SamplesFEELVOS: Fast End-To-End Embedding Learning for Video Object SegmentationInserting Videos Into VideosVolumetric Capture of Humans With a Single RGBD Camera via Semi-Parametric LearningYou Look Twice: GaterNet for Dynamic Filter Selection in CNNsInteractive Full Image Segmentation by Considering All Regions JointlyLarge-Scale Interactive Object Segmentation With Human AnnotatorsSelf-Supervised GANs via Auxiliary Rotation LossSim-To-Real via Sim-To-Sim: Data-Efficient Robotic Grasping via Randomized-To-Canonical Adaptation NetworksUsing Unknown Occluders to Recover Hidden ScenesComputer Vision for Global ChallengesDeep Vision 2019Landmark RecognitionImage Matching: Local Features and Beyond3D-WiDGET: Deep GEneraTive Models for 3D UnderstandingFine-Grained Visual CategorizationLow-Power Image Recognition Challenge (LPIRC)New Trends in Image Restoration and Enhancement Workshop and Associated ChallengesSpatio-temporal Action Recognition (AVA) @ ActivityNet ChallengeThird Workshop on Computer Vision for AR/VRDAVIS Challenge on Video Object SegmentationEfficient Deep Learning for Computer VisionFairness Accountability Transparency and Ethics in Computer VisionPrecognition Seeing through the FutureWorkshop and Challenge on Learned Image CompressionWhen Blockchain Meets Computer Vision & AIApplications of Computer Vision and Pattern Recognition to Media ForensicsTowards Relightable Volumetric Performance Capture of HumansLearning Representations via Graph-structured Networks",
      "link": "http://ai.googleblog.com/2019/06/google-at-cvpr-2019.html",
      "author": "Posted by"
    },
    {
      "title": "Applying AutoML to Transformer Architectures",
      "date": "Friday, June 14, 2019",
      "abstract": "Applying AutoML to Transformer ArchitecturesTransformer architecturegenerating fantasy fictionwriting musical harmoniesfeed forward neural networksrecurrent neural networksother feed forward modelsAutoMLstate-of-the-art modelsevolution-basedEvolved Transformernatural language processingreleasing this new modelTensor2TensorWMT\u201914 English-GermanCIFAR-10convolutional layersour paperBLEUperplexity performanceour paperLM1Bopen sourcedTensor2Tensorsearch space we used for our searchColab",
      "link": "http://ai.googleblog.com/2019/06/applying-automl-to-transformer.html",
      "author": "Posted by David So, Software Engineer, Google AI"
    },
    {
      "title": "Google at ICML 2019",
      "date": "Monday, June 10, 2019",
      "abstract": "Google at ICML 2019International Conference on Machine LearningInternational Machine Learning SocietyGoogle Research Football EnvironmentAdaNetRobotics at GoogleChallenging Common Assumptions in the Unsupervised Learning of Disentangled RepresentationsLearning to Groove with Inverse Sequence TransformationsMetric-Optimized Example WeightsHOList: An Environment for Machine Learning of Higher Order Logic Theorem ProvingLearning to Clear the MarketShape Constraints for Set FunctionsSelf-Attention Generative Adversarial NetworksHigh-Fidelity Image Generation With Fewer LabelsLearning Optimal Linear RegularizersDeepMDP: Learning Continuous Latent Space Models for Representation LearningkernelPSI: a Post-Selection Inference Framework for Nonlinear Variable SelectionLearning from a LearnerRate Distortion For Model Compression:From Theory To PracticeAn Investigation into Neural Net Optimization via Hessian Eigenvalue DensityGraph Matching Networks for Learning the Similarity of Graph Structured ObjectsSubspace Robust Wasserstein DistancesTraining Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent ConstraintsThe Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical StudyA Theory of Regularized Markov Decision ProcessesArea AttentionEfficientNet: Rethinking Model Scaling for Convolutional Neural NetworksStatic Automatic Batching In TensorFlowThe Evolved TransformerPolicy Certificates: Towards Accountable Reinforcement LearningSelf-similar Epochs: Value in ArrangementThe Value Function Polytope in Reinforcement LearningAdversarial Examples Are a Natural Consequence of Test Error in NoiseSOLAR: Deep Structured Representations for Model-Based Reinforcement LearningGarbage In, Reward Out: Bootstrapping Exploration in Multi-Armed BanditsImperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech RecognitionDirect Uncertainty Prediction for Medical Second OpinionsA Large-Scale Study on Regularization and Normalization in GANsLearning a Compressed Sensing Measurement Matrix via Gradient UnrollingNATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural NetworksDistributed Weighted Matching via Randomized Composable CoresetsMonge blunts Bayes: Hardness Results for Adversarial TrainingGeneralized Majorization-MinimizationNAS-Bench-101: Towards Reproducible Neural Architecture SearchVariational Russian Roulette for Deep Bayesian NonparametricsSurrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex OptimizationImproved Parallel Algorithms for Density-Based Network ClusteringThe Advantages of Multiple Classes for Reducing Overfitting from Test Set ReuseSubmodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive ComplexityHiring Under UncertaintyA Tree-Based Method for Fast Repeated Sampling of Determinantal Point ProcessesStatistics and Samples in Distributional Reinforcement LearningProvably Efficient Maximum Entropy ExplorationActive Learning with Disagreement GraphsMixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood MixingUnderstanding the Impact of Entropy on Policy OptimizationMatrix-Free Preconditioning in Online LearningState-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden RepresentationsOnline Convex Optimization in Adversarial Markov Decision ProcessesBounding User Contributions: A Bias-Variance Trade-off in Differential PrivacyComplementary-Label Learning for Arbitrary Losses and ModelsLearning Latent Dynamics for Planning from PixelsUnifying Orthogonal Monte Carlo MethodsDifferentially Private Learning of Geometric ConceptsOnline Learning with Sleeping Experts and Feedback GraphsAdaptive Scale-Invariant Online Algorithms for Learning Linear ModelsTensorFuzz: Debugging Neural Networks with Coverage-Guided FuzzingOnline Control with Adversarial DisturbancesAdversarial Online Learning with NoiseEscaping Saddle Points with Adaptive Gradient MethodsFairness Risk MeasuresDBSCAN++: Towards Fast and Scalable Density ClusteringLearning Linear-Quadratic Regulators Efficiently with only \u221aT RegretUnderstanding and correcting pathologies in the training of learned optimizersParameter-Efficient Transfer Learning for NLPEfficient Full-Matrix Adaptive RegularizationEfficient On-Device Models Using Neural ProjectionsFlexibly Fair Representation Learning by DisentanglementRecursive Sketches for Modular Deep LearningPOLITEX: Regret Bounds for Policy Iteration Using Expert PredictionAnytime Online-to-Batch, Optimism and AccelerationInsertion Transformer: Flexible Sequence Generation via Insertion OperationsRobust Inference via Generative Classifiers for Handling Noisy LabelsA Better k-means++ Algorithm via Local SearchAnalyzing and Improving Representations with the Soft Nearest Neighbor LossLearning to Generalize from Sparse and Underspecified RewardsMeanSum: A Neural Model for Unsupervised Multi-Document Abstractive SummarizationCHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational NetworkSimilarity of Neural Network Representations RevisitedOnline Algorithms for Rent-Or-Buy with Expert AdviceBreaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearitiesNon-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query ComplexityAgnostic Federated LearningCategorical Feature Compression via Submodular OptimizationCross-Domain 3D Equivariant Image EmbeddingsFaster Algorithms for Binary Matrix FactorizationOn Variational Bounds of Mutual InformationGuided Evolutionary Strategies: Augmenting Random Search with Surrogate GradientsSemi-Cyclic Stochastic Gradient DescentStochastic Deep Networks1st Workshop on Understanding and Improving Generalization in Deep LearningClimate Change: How Can AI Help?Generative Modeling and Model-Based Reasoning for Robotics and AIHuman In the Loop Learning (HILL)ICML 2019 Time Series WorkshopJoint Workshop on On-Device Machine Learning & Compact Deep Neural Network Representations (ODML-CDNNR)Negative Dependence: Theory and Applications in Machine LearningReinforcement Learning for Real LifeUncertainty and Robustness in Deep LearningTheoretical Physics for Deep LearningWorkshop on the Security and Privacy of Machine LearningExploration in Reinforcement Learning WorkshopICML Workshop on Imitation, Intent, and Interaction (I3)Identifying and Understanding Deep Learning PhenomenaWorkshop on Multi-Task and Lifelong Reinforcement LearningWorkshop on Self-Supervised LearningInvertible Neural Networks and Normalizing Flows",
      "link": "http://ai.googleblog.com/2019/06/google-at-icml-2019.html",
      "author": "Posted by Andrew Helton, Editor, Google AI Communications"
    },
    {
      "title": "Introducing Google Research Football: A Novel Reinforcement Learning Environment",
      "date": "Friday, June 7, 2019",
      "abstract": "Introducing Google Research Football: A Novel Reinforcement Learning Environmentreinforcement learningroboticsself-driving carsmoreAtari console gamesgame of GoDota 2Starcraft 2Google Research Football Environmentopen-source code on GithubGameplay FootballstochasticOpenAI Gymrule-basedDQNIMPALAsample-efficient RLsparse rewardsmodel based RLcurriculum learning",
      "link": "http://ai.googleblog.com/2019/06/introducing-google-research-football.html",
      "author": "Posted by Karol Kurach, Research Lead and Olivier Bachem, Research Scientist, Google Research, Z\u00fcrich"
    },
    {
      "title": "An Inside Look at Google Earth Timelapse",
      "date": "Wednesday, June 5, 2019",
      "abstract": "An Inside Look at Google Earth TimelapseGoogle Earth TimelapseTime MachineCREATE Labintroduced several updatesvisual upgradesEarth Enginewhat we did in 2013LandsatSentinelmap pyramiding techniqueopen source Time Machine softwareYouTube playlistmobile browser vendors have re-enabled video autoplayMaterial DesignabundanceWorld Database on Protected Areas (WDPA) boundariesshifts in the city of Pittsburgh's racial makeupVisual evidence",
      "link": "http://ai.googleblog.com/2019/06/an-inside-look-at-google-earth-timelapse.html",
      "author": "Posted by Paul Dille, Senior Software Developer, Carnegie Mellon University CREATE Lab, and Chris Herwig, Geo Data Engineer, Google Earth Outreach"
    },
    {
      "title": "Introducing TensorNetwork, an Open Source Library for Efficient Tensor Calculations",
      "date": "Tuesday, June 4, 2019",
      "abstract": "Introducing TensorNetwork, an Open Source Library for Efficient Tensor Calculationshigh-temperature superconductors true nature of space and timequantum statestensor networksaccelerated hardwareTensorNetworkPerimeter Institute for Theoretical PhysicsXTensorFlowGPUfirstAPIsecond paperTensorsscalarvectormatrixtensor contractionmatrix tracediagrammatic notationone-hot-encodedpetabytesmatrix product stateStoudenmire and SchwabfirstTensorNetworksecond papertree tensor networkMNISTFashion-MNIST",
      "link": "http://ai.googleblog.com/2019/06/introducing-tensornetwork-open-source.html",
      "author": "Posted by Chase Roberts, Research Engineer, Google AI and Stefan Leichenauer, Research Scientist, X"
    },
    {
      "title": "EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling",
      "date": "Wednesday, May 29, 2019",
      "abstract": "EfficientNet: Improving Accuracy and Efficiency through AutoML and Model ScalingConvolutional neural networksResNetGPipeImageNetICML 2019EfficientNet: Rethinking Model Scaling for Convolutional Neural NetworksAutoMLgrid searchFLOPSMobileNetResNetneural architecture searchAutoML MNAS frameworkMobileNetV2MnasNetImageNetGpipeResNet-50AutoML MNASCIFAR-100FlowersTPUhere",
      "link": "http://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html",
      "author": "Posted by Mingxing Tan, Staff Software Engineer and Quoc V. Le, Principal Scientist, Google AI"
    },
    {
      "title": "Moving Camera, Moving People: A Deep Learning Approach to Depth Prediction",
      "date": "Thursday, May 23, 2019",
      "abstract": "Moving Camera, Moving People: A Deep Learning Approach to Depth Predictioncomputer visiontriangulationGoogle\u2019s JumpLearning the Depths of Moving People by Watching Frozen Peopledeep learning-baseddepth mapsaugmented realityregular videopeople imitate mannequinsmulti-view-stereopeople imitating mannequinsmotion parallax2D optical flowhuman-segmentation networkvideo clipDORNChen et al.DeMoNWind Walk Travel Videos",
      "link": "http://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html",
      "author": "Posted by Tali Dekel, Research Scientist and Forrester Cole, Software Engineer, Machine Perception"
    },
    {
      "title": "Introducing Translatotron: An End-to-End Speech-to-Speech Translation Model",
      "date": "Wednesday, May 15, 2019",
      "abstract": "Introducing Translatotron: An End-to-End Speech-to-Speech Translation Modelpast several decadesautomatic speech recognitionmachine translationtext-to-speech synthesisGoogle TranslateDirect speech-to-speech translation with a sequence-to-sequence modelattentive sequence-to-sequence modelthe feasibilityoutperform cascade modelsleveraging weakly supervised dataspectrogramsvocodermultitask objectiveBLEU scoreherespeaker verificationspeaker adaptation for TTShere",
      "link": "http://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html",
      "author": "Posted by Ye Jia and Ron Weiss, Software Engineers, Google AI"
    },
    {
      "title": "An End-to-End AutoML Solution for Tabular Data at KaggleDays",
      "date": "Thursday, May 9, 2019",
      "abstract": "An End-to-End AutoML Solution for Tabular Data at KaggleDaysfeature engineeringhyper-parameter tuningneural architecture searchNasNetAmoebaNetMNasNetKaggleDays SF HackathonKaggleDays eventKaggle progression systemfinishing second place by a narrow marginfinal leaderboardarchitecture searchbootstrap aggregatingpublic kernelXGBoostfeature engineeringTFBTGoogle Cloud AutoML TablesGoogle Cloud Next \u201819",
      "link": "http://ai.googleblog.com/2019/05/an-end-to-end-automl-solution-for.html",
      "author": "Posted by Yifeng Lu, Software Engineer, Google AI"
    },
    {
      "title": "Announcing Open Images V5 and the ICCV 2019 Open Images Challenge",
      "date": "Wednesday, May 8, 2019",
      "abstract": "Announcing Open Images V5 and the ICCV 2019 Open Images ChallengeOpen ImagesupdatesOpen Images V4bounding-boxesOpen Images V5Open Images Challengeinteractive segmentation processTea and cake at the Fitzwilliam MuseumTim ReganPilota IIEuskal kultur erakundea Institut culturel basqueRheasDag PeakWuxi science park, 1995Gary StevensCat Cafe Shinjuku calicoAri HelminenUntitledTodd HuffmanCC BY 2.0segmentation masks on the training setinteractive segmentation processintersection-over-unionmasks on the validation and test setsthistle flowerssophiestill life with axliz westFischkutter KO\u0141-180 in Kolobrzeg (PL)zeesenbootCC BY 2.0Open Images Challenge2019 International Conference on Computer Vision2018 editionavailable now",
      "link": "http://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html",
      "author": "Posted by Vittorio Ferrari, Research Scientist, Machine Perception"
    },
    {
      "title": "Google at ICLR 2019",
      "date": "Monday, May 6, 2019",
      "abstract": "Google at ICLR 2019ICLR 2019machine learningdeep learningneural networksGenerating High Fidelity Images with Subscale Pixel Networks and Multidimensional UpscalingEnabling Factorized Piano Music Modeling and Generation with the MAESTRO DatasetMeta-Learning Update Rules for Unsupervised Representation LearningA Data-Driven and Distributed Approach to Sparse Signal Representation and RecoveryBayesian Deep Convolutional Networks with Many Channels are Gaussian ProcessesDiversity-Sensitive Conditional Generative Adversarial NetworksDiversity and Depth in Per-Example Routing ModelsEidetic 3D LSTM: A Model for Video Prediction and BeyondGANSynth: Adversarial Neural Audio SynthesisK for the Price of 1: Parameter-efficient Multi-task and Transfer LearningLearning to Describe Scenes with ProgramsLearning to Infer and Execute 3D Shape ProgramsThe Singular Values of Convolutional LayersUnsupervised Discovery of Parts, Structure, and DynamicsAdversarial Reprogramming of Neural NetworksDiscriminator Rejection SamplingOn Self Modulation for Generative Adversarial NetworksTowards GAN Benchmarks Which Require GeneralizationUnderstanding and Improving Interpolation in Autoencoders via an Adversarial RegularizerA new dog learns old tricks: RL finds classic optimization algorithmsContingency-Aware Exploration in Reinforcement LearningDiscriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation LearningDiversity is All You Need: Learning Skills without a Reward FunctionEpisodic Curiosity through ReachabilityLearning to Navigate the WebMeta-Learning Probabilistic Inference for PredictionMulti-step Retriever-Reader Interaction for Scalable Open-domain Question AnsweringNear-Optimal Representation Learning for Hierarchical Reinforcement LearningNeural Logic MachinesNeural Program Repair by Jointly Learning to Localize and RepairOptimal Completion Distillation for Sequence LearningRecall Traces: Backtracking Models for Efficient Reinforcement LearningSample Efficient Adaptive Text-to-SpeechSynthetic Datasets for Neural Program SynthesisThe Laplacian in RL: Learning Representations with Efficient ApproximationsA Mean Field Theory of Batch NormalizationEfficient Training on Very Large Corpora via Gramian EstimationPredicting the Generalization Gap in Deep Networks with Margin DistributionsInfoBot: Transfer and Exploration via the Information BottleneckAntisymmetricRNN: A Dynamical System View on Recurrent Neural NetworksComplement Objective TrainingDOM-Q-NET: Grounded RL on Structured LanguageFrom Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction FollowingHarmonic Unpaired Image-to-image TranslationHierarchical Generative Modeling for Controllable Speech SynthesisLearning Finite State Representations of Recurrent Policy NetworksLearning to Screen for Fast Softmax Inference on Large Vocabulary Neural NetworksMusic Transformer: Generating Music with Long-Term StructureUniversal TransformersWhat do you learn from context? Probing for sentence structure in contextualized word representationsDoubly Reparameterized Gradient Estimators for Monte Carlo ObjectivesHow Important Is a Neuron?Integer Networks for Data Compression with Latent-Variable ModelsModeling Uncertainty with Hedged Instance EmbeddingsPreventing Posterior Collapse with delta-VAEsSpectral Inference Networks: Unifying Deep and Spectral LearningStochastic Prediction of Multi-Agent Interactions from Partial ObservationsLearning from Limited Labeled DataDeep Reinforcement Learning Meets Structured PredictionDebugging Machine Learning ModelsStructure & Priors in Reinforcement Learning (SPiRL)Task-Agnostic Reinforcement Learning (TARL)AI for Social GoodSafe Machine Learning Specification, Robustness and AssuranceRepresentation Learning on Graphs and Manifolds",
      "link": "http://ai.googleblog.com/2019/05/google-at-iclr-2019.html",
      "author": "Posted by Andrew Helton, Editor, Google AI Communications"
    },
    {
      "title": "Announcing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition & Retrieval",
      "date": "Friday, May 3, 2019",
      "abstract": "Announcing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition & RetrievalGoogle-Landmarksimage retrievalKaggle challengesLandmark Recognition 2018Landmark Retrieval 2018machine learningGoogle-Landmarks-v2Landmark Recognition 2019Landmark Retrieval 2019source code and modelDetect-to-Retrievelast year\u2019s datasetNeuschwanstein Castle,Golden Gate BridgeKiyomizu-deraBurj khalifaGreat Sphinx of GizaMachu PicchuWikimedia CommonsWiki Loves MonumentsLandmark Recognition 2019Landmark Retrieval 2019Second Landmark Recognition WorkshopCVPR 2019open-source codeDetect-to-RetrieveCVPR 2019bounding boxeshereLandmark Recognition 2019Landmark Retrieval 2019Second Landmark Recognition WorkshopCVPR 2019Common Visual Data Foundation",
      "link": "http://ai.googleblog.com/2019/05/announcing-google-landmarks-v2-improved.html",
      "author": "Posted by Bingyi Cao and Tobias Weyand, Software Engineers, Google AI"
    },
    {
      "title": "Announcing the 6th Fine-Grained Visual Categorization Workshop",
      "date": "Monday, April 29, 2019",
      "abstract": "Announcing the 6th Fine-Grained Visual Categorization WorkshopiNaturalistiMaterialistindigo buntinglazuli buntingmorethan500FGVC5 at CVPR 2018domain-specific transfer learningestimating test-time priors6th annual workshop on Fine-Grained Visual CategorizationCVPR 2019updated iNaturalist challengefashionproductswildlife camera trapsfoodbutterflies & mothsfashion designcassava leaf diseaseThe Metropolitan Museum of ArtiMet Collection challengeNew York Botanical GardenHerbarium challengeiMet Collection challengepublic domain datasetKernels-only competitionHerbarium challengeSteere HerbariumKaggle leaderboardsSeek by iNaturalistMerlin Bird ID",
      "link": "http://ai.googleblog.com/2019/04/announcing-6th-fine-grained-visual.html",
      "author": "Posted by Christine Kaeser-Chen, Software Engineer and Serge Belongie, Visiting Faculty, Google AI"
    },
    {
      "title": "Evaluating the Unsupervised Learning of Disentangled Representations",
      "date": "Wednesday, April 24, 2019",
      "abstract": "Evaluating the Unsupervised Learning of Disentangled Representationsunsuperviseddeep learningcuriosity driven explorationabstract reasoningvisual concept learningdomain adaptation for reinforcement learningChallenging Common Assumptions in the Unsupervised Learning of Disentangled RepresentationsICML 2019disentanglement_libShapes3DShapes3D data setvariational autoencodersBetaVAEAnnealedVAEFactorVAEDIP-VAE I/IIBeta-TCVAEBetaVAE scoreFactorVAE scoreMIGSAPModularityDCI Disentanglementinductive biasesviolin plotsCars3D data setdisentanglement_libdisentanglement_libdisentanglement_lib>10,000 pretrained disentanglement_lib modelsdisentanglement_lib",
      "link": "http://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html",
      "author": "Posted by Olivier Bachem, Research Scientist, Google AI Z\u00fcrich"
    },
    {
      "title": "SpecAugment: A New Data Augmentation Method for Automatic Speech Recognition",
      "date": "Monday, April 22, 2019",
      "abstract": "SpecAugment: A New Data Augmentation Method for Automatic Speech Recognitionthe ongoing development of deep neural networksdata augmentationimage classificationSpecAugment: A Simple Data Augmentation Method for Automatic Speech RecognitionLibriSpeech 960hSwitchboard 300hspectrogramthis articlewarpingListen Attend and Spell (LAS) networksWord Error RateLibriSpeech 960hSwitchboard 300hLi et. al (2019)Yang et. al (2018)Zeyer et. al (2018)HMMCTCASGLanguage models",
      "link": "http://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html",
      "author": "Posted by Daniel S. Park, AI Resident and William Chan, Research Scientist"
    },
    {
      "title": "MorphNet: Towards Faster and Smaller Neural Networks",
      "date": "Wednesday, April 17, 2019",
      "abstract": "MorphNet: Towards Faster and Smaller Neural Networksimage classificationtext recognitionspeech transcriptionNeural Architecture Search and AdaNetMorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep NetworksTensorFlow implementation of MorphNetsparsifying regularizerloss functionFLOPswidth multiplierResNet-101JFTResNet architectureInception-style architecturesJFTInception V2ImageNethereMorphNet paper",
      "link": "http://ai.googleblog.com/2019/04/morphnet-towards-faster-and-smaller.html",
      "author": "Posted by Andrew Poon, Senior Software Engineer and Dhyanesh Narayanan, Product Manager, Google AI Perception"
    },
    {
      "title": "Take Your Best Selfie Automatically, with Photobooth on Pixel 3",
      "date": "Tuesday, April 16, 2019",
      "abstract": "Take Your Best Selfie Automatically, with Photobooth on Pixel 3automatic photography with Google Clipsnewest release of Pixel CameraTop ShotPortrait modeImage Content ModelMobileNetsattention model",
      "link": "http://ai.googleblog.com/2019/04/take-your-best-selfie-automatically.html",
      "author": "Posted by Navid Shiee, Senior Software Engineer and Aseem Agarwala, Staff Research Scientist, Google AI"
    },
    {
      "title": "Capturing Special Video Moments with Google Photos",
      "date": "Wednesday, April 3, 2019",
      "abstract": "Capturing Special Video Moments with Google PhotosRethinking the Faster R-CNN Architecture for Temporal Action LocalizationFaster R-CNNobject detectionfaster R-CNNfeatureconvolutional neural networkregion proposal networkTHUMOS'14ActivityNetprecision and recall",
      "link": "http://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html",
      "author": "Posted by Sudheendra Vijayanarasimhan and David Ross, Software Engineers"
    },
    {
      "title": "Using Deep Learning to Improve Usability on Mobile Devices",
      "date": "Tuesday, April 2, 2019",
      "abstract": "Using Deep Learning to Improve Usability on Mobile Devicesgraphical user interfacesfalse affordancesCHI'19Modeling Mobile Interface Tappability Using Crowdsourcing and Deep Learningthe blue color and underline of a linkdeep neural networkconvolutional neural networkprecisionrecall",
      "link": "http://ai.googleblog.com/2019/04/using-deep-learning-to-improve.html",
      "author": "Posted by Yang Li, Research Scientist, Google AI"
    },
    {
      "title": "Unifying Physics and Deep Learning with TossingBot",
      "date": "Tuesday, March 26, 2019",
      "abstract": "Unifying Physics and Deep Learning with TossingBotgrasp objects efficientlyvisually self adaptlearn from real-world experiencespicking robotprevious systemspreprintpushgrasppreprint",
      "link": "http://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html",
      "author": "Posted by Andy Zeng, Student Researcher, Robotics at Google"
    },
    {
      "title": "Simulated Policy Learning in Video Models",
      "date": "Monday, March 25, 2019",
      "abstract": "Simulated Policy Learning in Video Modelsreinforcement learningclassic Atari 2600 gamesRecent workexploration regimesMontezuma's RevengeModel-Based Reinforcement Learning for Ataricodetensor2tensorwell establishedrecentmodel-basedreinforcementlearningmethodsfeedforward convolutional networkPongpreviousworkKung Fu MasterProximal Policy Optimization (PPO)FreewayRainbowFreewayBreakoutAtlantisBattlezonePrivate Eyerepositorycolab",
      "link": "http://ai.googleblog.com/2019/03/simulated-policy-learning-in-video.html",
      "author": "Posted by \u0141ukasz Kaiser and Dumitru Erhan, Research Scientists, Google AI"
    },
    {
      "title": "Reducing the Need for Labeled Data in Generative Adversarial Networks",
      "date": "Wednesday, March 20, 2019",
      "abstract": "Reducing the Need for Labeled Data in Generative Adversarial NetworksGenerative adversarial networksGANshigh-fidelity natural image synthesisimproving learned image compressionconditional GANsHigh-Fidelity Image Generation With Fewer LabelsCompare GAN libraryrecently introducedpreviouslyFr\u00e9chet Inception DistanceCompare GANGinexamplesTensorFlow datasetsincreasingly importantself-supervised learningSelf-Supervised Generative Adversarial Networks",
      "link": "http://ai.googleblog.com/2019/03/reducing-need-for-labeled-data-in.html",
      "author": "Posted by Mario Lu\u010di\u0107, Research Scientist and Marvin Ritter, Software Engineer, Google AI Z\u00fcrich"
    },
    {
      "title": "Measuring the Limits of Data Parallel Training for Neural Networks",
      "date": "Tuesday, March 19, 2019",
      "abstract": "Measuring the Limits of Data Parallel Training for Neural Networksimage classificationmachine translationspeech recognitionCloud TPU Podsdistribute computationsmodel parallelismdata parallelismstochastic gradient descentMeasuring the Effects of Data Parallelism in Neural Network Trainingshare our raw data our paperout-of-sample errorCloud TPU PodsResNet-8CIFAR-10ResNet-50ImageNettransformerLSTMLM1BCommon Crawlmomentumour paper",
      "link": "http://ai.googleblog.com/2019/03/measuring-limits-of-data-parallel.html",
      "author": "Posted by Chris Shallue, Senior Software Engineer and George Dahl, Senior Research Scientist, Google AI"
    },
    {
      "title": "A Summary of the Google Flood Forecasting Meets Machine Learning Workshop",
      "date": "Monday, March 18, 2019",
      "abstract": "A Summary of the Google Flood Forecasting Meets Machine Learning WorkshopGoogle Flood Forecasting Meets Machine Learningour beliefProf. Paolo BurlandoDr. Tyler EricksonDr. Peter SalamonProf. Dawei HanYossi Matiasrecentmachinelearningflood forecastingcrisis responseAI for Social GoodProf. Peter MolnarProf. Yishay MansourGooglefascinating talkspostersDr. Dhanya C. T.Adarsh M. S.Central Water Commission'sProf. Andras BardossyFrederik KratzertLSTMsProf. Paul BatesProf. Emmanouil AnagnostouProf. Efrat MorinDr. Zachary Flamigglobal flash flood prediction projectremote discharge estimationdata-driven discretization approach to solving partial differential equationspreviousprojectsinGoogleflood forecasting effortsProf. Paolo BurlandoProf. Dawei HanDr. Peter SalamonDr. Tyler EricksonAI for Social Goodour continued engagement",
      "link": "http://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html",
      "author": "Posted by Sella Nevo, Senior Software Engineer and Rainier Aliment, Program Manager"
    },
    {
      "title": "Google Faculty Research Awards 2018",
      "date": "Friday, March 15, 2019",
      "abstract": "Google Faculty Research Awards 2018Google Faculty Research Awardshuman computer interactionmachine learningmachine perceptionsystemsrecipientsour websitehere",
      "link": "http://ai.googleblog.com/2019/03/google-faculty-research-awards-2018.html",
      "author": "Posted by Maggie Johnson, VP, Education and Negar Saei, Program Manager, University Relations"
    },
    {
      "title": "Harnessing Organizational Knowledge for Machine Learning",
      "date": "Thursday, March 14, 2019",
      "abstract": "Harnessing Organizational Knowledge for Machine LearningSnorkel Drybell: A Case Study in Deploying Weak Supervision at Industrial ScaleSnorkelnamed-entity recognitiongenerative modeling techniquecovariance matrixusing a new matrix completion-style approachTensorFlowMapReducegenerative modeling techniquestatistically estimatedour papersnorkel.stanford.edu",
      "link": "http://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html",
      "author": "Posted by Alex Ratner, Stanford University and Cassandra Xia, Google AI"
    },
    {
      "title": "An All-Neural On-Device Speech Recognizer",
      "date": "Tuesday, March 12, 2019",
      "abstract": "An All-Neural On-Device Speech Recognizer significant accuracy improvements with deep learningVoice Searchdeep neural networksrecurrent neural networkslong short-term memory networksconvolutional networksStreaming End-to-End Speech Recognition for Mobile DevicesRNN transducerattention-basedlisten-attend-spellgreat promiseconnectionist temporal classification (CTC)halve the latencypapertraining techniqueparallel implementationTPU v2search graphFinite State Transducerbeam search through a single neural networkparameter quantization and hybrid kernel techniquesmodel optimization toolkitTensorFlow Lite",
      "link": "http://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html",
      "author": "Posted by Johan Schalkwyk, Google Fellow, Speech Team"
    },
    {
      "title": "Real-Time AR Self-Expression with Machine Learning",
      "date": "Friday, March 8, 2019",
      "abstract": "Real-Time AR Self-Expression with Machine LearningAugmented realityAR features coming to Google MapsPlayground - a creative mode in the Pixel cameraYouTube StoriesARCoreAugmented Faces APITensorFlow Liteits new mobile GPU functionalityYouTube Stories'latest ARCore SDKML Kit Face Contour Detection APItransfer learningMLKitTensorFlow Litenewly introduced GPU back-endlatest ARCore SDK",
      "link": "http://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html",
      "author": "Posted by Artsiom Ablavatski and Ivan Grishchenko, Research Engineers, Google AI"
    },
    {
      "title": "RNN-Based Handwriting Recognition in Gboard",
      "date": "Thursday, March 7, 2019",
      "abstract": "RNN-Based Handwriting Recognition in GboardGoogle Handwriting Inputinitial launchGboard for AndroidFast Multi-language LSTM-based Online Handwriting RecognitionB\u00e9zier curvesrecurrent neural networkprevious modelsquasi-recurrent neural networksConnectionist Temporal Classificationweighted finite-state acceptorTensorFlowTensorFlow Litequantizing",
      "link": "http://ai.googleblog.com/2019/03/rnn-based-handwriting-recognition-in.html",
      "author": "Posted by Sandro Feuz and Pedro Gonnet, Senior Software Engineers, Handwriting Team"
    },
    {
      "title": "Exploring Neural Networks with Activation Atlases",
      "date": "Wednesday, March 6, 2019",
      "abstract": "Exploring Neural Networks with Activation AtlasesOpenAIExploring Neural Networks with Activation Atlasesjupyter notebooksInceptionv1ImageNetUMAPjupyter notebookscolabLucid",
      "link": "http://ai.googleblog.com/2019/03/exploring-neural-networks.html",
      "author": "Posted by Shan Carter, Software Engineer, Google AI"
    },
    {
      "title": "Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models",
      "date": "Monday, March 4, 2019",
      "abstract": "Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network ModelsDeep neural networksBigGanBertGPT2.0ImageNet visual recognition challengeGoogleNetSqueeze-and-Excitation Networksstate-of-the-art image modelsTPUv2sGPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelismpipeline parallelismgradient descentAmoebaNetCIFARGPipe libraryLingvomini-batchTransformerbetter ImageNet models transfer betterGPipe",
      "link": "http://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html",
      "author": "Posted by Yanping Huang, Software Engineer, Google AI"
    },
    {
      "title": "Long-Range Robotic Navigation via Automated Reinforcement Learning",
      "date": "Thursday, February 28, 2019",
      "abstract": "Long-Range Robotic Navigation via Automated Reinforcement Learninggrasp objectsrobot locomotionagentsLearning Navigation Behaviors End-to-End with AutoRLPRM-RL: Long-Range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based PlanningLong-Range Indoor Navigation with PRM-RLlidarfirst paperDDPGcatastrophic forgetfulnesslarge-scale hyperparameter optimizationDDPGartificial potential fieldsdynamic window approachbehavior cloningSampling-based plannersprobabilistic roadmapssecond paperBest Paper in Service RoboticsICRA 2018Monte Carlothird paperSimultaneous Localization and MappingFirst paperoriginal PRMspath-guided artificial potential fieldssecond paperthird paper",
      "link": "http://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html",
      "author": "Aleksandra Faust, Senior Research Scientist and Anthony Francis, Senior Software Engineer, Robotics at Google"
    },
    {
      "title": "Learning to Generalize from Sparse and Underspecified Rewards",
      "date": "Friday, February 22, 2019",
      "abstract": "Learning to Generalize from Sparse and Underspecified RewardsReinforcement learningplaying video gamescontinuous controlrobotic learninglearning agentlanguage understandingreward hackingLearning to Generalize from Sparse and Underspecified Rewardsexplorationsemantic parsinglogical formsSQLweakly-superviseda relevant Wikipedia tableWikiTableQuestionsWikiSQLprior workground-truthpreviousrewardlearninggeneralization performancevalidation setmeta learningOur paperthe two directionsKullback\u2013Leiblerbimodalmodelling reward functionscredit-assignment problem",
      "link": "http://ai.googleblog.com/2019/02/learning-to-generalize-from-sparse-and.html",
      "author": "Posted by Rishabh Agarwal, Google AI Resident and Mohammad Norouzi, Research Scientist"
    },
    {
      "title": "On the Path to Cryogenic Control of Quantum Processors",
      "date": "Thursday, February 21, 2019",
      "abstract": "On the Path to Cryogenic Control of Quantum Processorspractical problemsGoogle AI Quantum teamCurrent thresholds suggestBristleconekelvinwe presentedInternational Solid State Circuits Conferenceour lab in Santa BarbaraUniversity of Massachusetts Professor Joseph BardinT1Rabi oscillationssingle qubit gatesGoogle Visiting Researcher Program",
      "link": "http://ai.googleblog.com/2019/02/on-path-to-cryogenic-control-of-quantum.html",
      "author": "Posted by Joseph Bardin, Visiting Faculty Researcher and Erik Lucero, Staff Research Scientist and Hardware Lead, Google AI Quantum Team"
    },
    {
      "title": "Introducing PlaNet: A Deep Planning Network for Reinforcement Learning",
      "date": "Friday, February 15, 2019",
      "abstract": "Introducing PlaNet: A Deep Planning Network for Reinforcement Learningartificial agentsreinforcement learningDeepMind's DQNcontrolrobotsAlphaGoDeepMindDeep Planning Network (PlaNet)source codeplan over imagesworld modelsonline research paperPDF versionA3CD4PGDeepMind Control Suite3D environmentsTPUs",
      "link": "http://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html",
      "author": "Posted by Danijar Hafner, Student Researcher, Google AI"
    },
    {
      "title": "Using Global Localization to Improve Navigation",
      "date": "Monday, February 11, 2019",
      "abstract": "Using Global Localization to Improve NavigationVisual Positioning ServiceStreet ViewlocalizationGPSStreet ViewARCoreLocal GuidesGoogle Lens",
      "link": "http://ai.googleblog.com/2019/02/using-global-localization-to-improve.html",
      "author": "Posted by Tilman Reinhardt\u200e, Software Engineer, Google Maps"
    },
    {
      "title": "Announcing the Second Workshop and Challenge on Learned Image Compression",
      "date": "Wednesday, February 6, 2019",
      "abstract": "Announcing the Second Workshop and Challenge on Learned Image Compressionwe announcedWorkshop and Challenge on Learned Image CompressionCVPR 201823 accepted workshop papersthe competitionmean opinion scoreBetter Portable GraphicsWorkshop and Challenge on Learned Image CompressionCVPR 2019PSNRMS-SSIMhuman evaluated rating taskcompression.cc",
      "link": "http://ai.googleblog.com/2019/02/announcing-second-workshop-and.html",
      "author": "Posted by Nick Johnston, Software Engineer, Machine Perception"
    },
    {
      "title": "Real-time Continuous Transcription with Live Transcribe",
      "date": "Monday, February 4, 2019",
      "abstract": "Real-time Continuous Transcription with Live TranscribeWorld Health Organizationestimatesautomatic speech recognitionGoogle's ASRautomated captions in Youtubepresentations in Slidesmultiple improvementsCARTPalantypistSTTRLive TranscribeGoogle Cloudour previous workAudioSetpublished VGGish modelGallaudet Universityeven small projectors1UX research in this spacecocktail party problemon-device recognitionspeaker-separationspeech enhancementThe Keyword1\u21a9",
      "link": "http://ai.googleblog.com/2019/02/real-time-continuous-transcription-with.html",
      "author": "Posted by Sagar Savla, Product Manager, Machine Perception"
    },
    {
      "title": "Transformer-XL: Unleashing the Potential of Attention Models",
      "date": "Tuesday, January 29, 2019",
      "abstract": "Transformer-XL: Unleashing the Potential of Attention ModelsGating-based RNNsthe gradient clipping techniquenot sufficientTransformers,a fixed-length contextdo not respect the sentence boundariesTransformer-XLother relative positional encoding schemesour paperperplexityenwiki8text8WikiText-103One Billion WordPenn Treebanklanguage model pretrainingBERTpaperGitHub",
      "link": "http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html",
      "author": "Posted by Zhilin Yang and Quoc Le, Google AI"
    },
    {
      "title": "Natural Questions: a New Corpus and Challenge for Question Answering Research",
      "date": "Wednesday, January 23, 2019",
      "abstract": "Natural Questions: a New Corpus and Challenge for Question Answering ResearchOpen-domain question answeringnatural language understandingthis Wikipedia pageNatural Questions1quite easy for computers to solvechallengeNatural Questions: a Benchmark for Question Answering ResearchTransactions of the Association for Computational LinguisticsNQ websitechallenge website\n1 A reader just alerted us to DuReader, a dataset from Baidu that contains real queries and full documents. We will add this to our paper, in which we discuss NQ in relation to previous work.\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html",
      "author": "Posted by Tom Kwiatkowski and Michael Collins, Research Scientists, Google AI Language"
    },
    {
      "title": "Expanding the Application of Deep Learning to Electronic Health Records",
      "date": "Tuesday, January 22, 2019",
      "abstract": "Expanding the Application of Deep Learning to Electronic Health Recordspublished a paperour work aims to further improve potential clinical benefitIntermountain HealthcareTeleHealth critical care unit",
      "link": "http://ai.googleblog.com/2019/01/expanding-application-of-deep-learning.html",
      "author": "Posted by Alvin Rajkomar, MD and Eyal Oren, PhD, Google AI, Healthcare"
    },
    {
      "title": "Soft Actor-Critic: Deep Reinforcement Learning for Robotics",
      "date": "Friday, January 18, 2019",
      "abstract": "Soft Actor-Critic: Deep Reinforcement Learning for Roboticswe recently released Soft Actor-Critic BAIR blog post tutorialQ-functiontechnical reportGhost RoboticsDynamixel Clawproject websiteprior workBAIR blog postearly preprint of the locomotion experimentmore complete description of the algorithmGitHub",
      "link": "http://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html",
      "author": "Posted by Tuomas Haarnoja, Student Researcher and Sergey Levine, Faculty Advisor, Robotics at Google"
    },
    {
      "title": "Looking Back at Google\u2019s Research Efforts in 2018",
      "date": "Tuesday, January 15, 2019",
      "abstract": "Looking Back at Google\u2019s Research Efforts in 2018publications in 2018Google AI Principlesresponsible AI practicesevaluate our own development of AIML fairnessmodel interpretabilityreducing gender biasesexplorationreleaseFairness ModuleMachine Learning Crash Coursework on flood predictionearthquake aftershock predictionTensorFlowusing convolutional neural networks to identify humpback whale callsdetecting new exoplanetsidentifying diseased cassava plantsGoogle AI for Social Impact ChallengeGoogle.orgGoogle DuplexSmart ComposeSound SearchNow PlayingSmart Linkifysupport more languagesbetter understanding of semantic similarityspeech synthesistext-to-speech for languages without much training data availablequantum supremacya number of exciting new resultsBristleconeCirqquantum computers could be used for neural networksunderstanding performance fluctuations in quantum processorsquantum computers might be useful as a computational substrate for neural networksmix of basic research as well as product-focused collaborationsTransformer work from 2017Universal TransformerBERTGLUE benchmarkhandle multilingual use cases better, with the goal of makingpetslearn about the natural worldanswer questions in real-timedo more with Lens in Google Imagesnew capabilities in vision and video in Cloud ML APIsface-related on-device building blocksthis blog poststereo magnificationa fast bottom-up model for joint pose estimation and person instance segmentationsystem for visualizing complex motionsystemdistillation3D convolutionsunsupervised learning of semantic audio representationsexpressive and human-like speech synthesisLooking to ListenMobileNetV2next-generation mobile computer vision modelMorphNetautomatic generation of mobile network architecturesHDR+Motion Photos in Pixel 2Augmented Reality mode in Motion StillsGoogle Photos albumNight Sightpressusersusing ML to provide better portrait mode shotsSuper Res ZoomTop ShotGoogle Clipsherehererouting algorithms behind Google tripsconsistent hashing for Google cloudalgorithms and theorygraph miningour work on studying convergence of stochastic optimization algorithms for training neural networksICLR 2018ADAMdistributed optimizationvia round compressionvia core-setssubmodular maximizationk-core decompositionset cover at scale via sketchingbalanced partitioning and hierarchical clusteringonline delivery servicesWWW'18OR-tools platform2018 Minizinc new models reconstruction learning a mixture of multinomial logits learnable by neural networksimprove classiconline algorithmsby iterationby shufflingincentive-aware learningnew researchmarket algorithmstest incentive compatibilityoptimizing ad refresh for in-app advertisinglack of prediction of futurenoisy forecastsheterogenous buyer behaviourdynamic double auctionsnew online allocation algorithms for stochastic input with traffic spikesnew bandit algorithms robust to corrupted datadynamic control flowMesh TensorFlowreleasedJAXXLACleverHansTensorFlow Privacyplacement of computations onto deviceslearning memory access patternslearned indicesHierarchical Planner'sSpectreMeltdownProject Zerotoolthe evolution of Google's Software Defined Networking WANstand-alone, federated query processing platforma report on our extensive use of code reviewconsistent hashing schemedeployed itGoogle Cloud Pub/Subearlier version of our paperVimeohaproxyalmost 8evolutionary algorithms can be used to automatically discover state-of-the-art neural network architecturesautomatically generate image transformation sequencesfind new symbolic optimization expressions that are more effective than the commonly used optimization update rulesAdaNet showed how to have a fast and flexible AutoML algorithm with learning guaranteescombining the accuracy of a model with its inference computation time in the reward function for a reinforcement learning architecture searchexplored using ML to learn to automatically compress ML modelsTensor Processing UnitsBERTvia ColabTensorFlow Research CloudCloud TPUslarge-scale ML trainingCloud TPU v3TensorFlowreleased in November 2015TensorFlow LiteTensorFlow.jsTensorFlow ProbabilityContent Delivery Network10,000 stars on Githubintroduced a new framework for flexible and reproducible reinforcement learningnew visualization tools to rapidly understand the characteristics of a dataseta high-level library for expressing machine learning problems that involve learning-to-ranka framework for fast and flexible AutoML solutions with learning guaranteeslibrary for doing in-browser realtime t-SNE visualizationsFHIR tools and software for working with electronic healthcare dataYou can find a live demo hereGoogle Dataset Searchcurated and released many new, novel datasetsOpen Images ExtendedOpen Images V4extended this dataset to add more diversity of people and scenes from all over the worldcrowdsource.google.comAVAhuman actionsspeech in video.announced an updated YouTube-8M, and the 2nd YouTube-8M Large-Scale Video Understanding Challenge and WorkshopHDR+ Burst Photography DatasetGoogle-LandmarksFluid Annotationgameneoriginal imageInclusive Images ChallengeiNaturalist 2018 Challenge Kaggle \"Quick, Draw!\" Doodle Recognition ChallengeConceptual Captionsteach robots to grasp novel objectsto learn about objects without human supervisioncombining ML and sampling-based methodsICRA'18learning robot geometryperceive the structure of the worlddeep reinforcement learning models online on real robotslearn stable approacheshigh-precision automated reconstruction of neuronsimproves the accuracy of automated interpretation of connectomics data by an order of magnitudeFinding new planets outside our solar system by data mining light curves of starsRecognizing the origin or function of short DNA sequencesAutomatically detecting out-of-focus microscope imagesDigitally creating images of the same cells with multiple stainsAutomatically mapping mass spectrometry output to peptide chainsFiji (ImageJ)applying ML to healthpublished workis on-par with retinal specialistspairing ophthalmologists and this ML model allow them to make more accurate decisionsVerilyAravind Eye HospitalsRajavithi Hospital affiliated with the Ministry of Health in Thailandmedical and eye specialists found quite remarkablecan assess cardiovascular risk from retinal imageshow to improve the grading of prostate cancer using MLdetect metastatic breast cancer with deep learningprototype for an augmented-reality microscopeUniversity of Chicago MedicineUCSFStanford Medicinepublished work in Nature Digital Medicineopen sourced softwareFast Healthcare Interoperability ResourcesGitHub repositoryimproved the accuracy, speed and utility of our deep learning-based variant caller, DeepVariantNature Biotechnologyto proactively advance health equityproviding multi-year Ph.D. fellowshipsannual Google Ph.D. Fellowship SummitGoogle AI Residencyits third yearglobal officesmachine learningperceptionalgorithms and optimizationlanguage understandinghealthcaremuch moreGoogle Faculty Research Awards programworkshop on AI/ML Research and PracticeAlgorithms & Optimization Workshopmuch of our research is published openlyICLR 2018NAACL 2018ICML 2018CVPR 2018NeurIPS 2018ECCV 2018EMNLP 2018ASPLOSHPCAICSEIEEE Security & PrivacyOSDISIGCOMMannounced our first AI research office in AfricaParisTokyoAmsterdamPrincetonhere",
      "link": "http://ai.googleblog.com/2019/01/looking-back-at-googles-research.html",
      "author": "Posted by Jeff Dean, Senior Fellow and Google AI Lead, on behalf of the entire Google Research Community"
    },
    {
      "title": "Top Shot on Pixel 3",
      "date": "Thursday, December 20, 2018",
      "abstract": "Top Shot on Pixel 3Google ClipsTop ShotPixel 3HDR+optical flowGoogle\u2019s Visual CoreMotion PhotoMobileNetsingle shot detectorknowledge distillationquantizationGeneralized Additive ModelVizierPrecision and RecallMean Reciprocal Rank",
      "link": "http://ai.googleblog.com/2018/12/top-shot-on-pixel-3.html",
      "author": "Posted by Li Zhang and Wei (Alex) Hong, Software Engineers"
    },
    {
      "title": "Google AI Princeton: Current and Future Research",
      "date": "Tuesday, December 18, 2018",
      "abstract": "Google AI Princeton: Current and Future Researchpartnered with academialarge-scale machine learningcontrol theoryreinforcement learninggradient descent methodstochastic gradient descentAdaGradAdaGradAdamdeveloped a new methodtheoretical computer sciencespectral filteringseveralrecentpublicationsHankel matrices",
      "link": "http://ai.googleblog.com/2018/12/google-ai-princeton-current-and-future.html",
      "author": "Posted by Elad Hazan and Yoram Singer, Research Scientists, Google AI and Princeton University"
    },
    {
      "title": "Exploring Quantum Neural Networks",
      "date": "Monday, December 17, 2018",
      "abstract": "Exploring Quantum Neural NetworksGoogle AI Quantum teammachine learningglobal optimizationexperimental quantum computerscomplex patterns in physical systemsClassification with Quantum Neural Networks on Near Term ProcessorsBarren Plateaus in Quantum Neural Network Training Landscapesoften involves randomizationdifficulties",
      "link": "http://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html",
      "author": "Posted by Jarrod McClean, Senior Research Scientist and Hartmut Neven, Director of Engineering, Google AI Quantum Team"
    },
    {
      "title": "Improving the Effectiveness of Diabetic Retinopathy Models",
      "date": "Thursday, December 13, 2018",
      "abstract": "Improving the Effectiveness of Diabetic Retinopathy Modelsour inaugural work in training deep learning modelsapply our technology to improve health outcomes in the worldannouncing a new partnera paperOphthalmologymicroaneurysmsour latest studyVerilyRajavithi Hospital",
      "link": "http://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html",
      "author": "Posted by Rory Sayres PhD and Jonathan Krause PhD, Google AI, Healthcare"
    },
    {
      "title": "Grasp2Vec: Learning Object Representations from Self-Supervised Grasping",
      "date": "Tuesday, December 11, 2018",
      "abstract": "Grasp2Vec: Learning Object Representations from Self-Supervised Graspingcognitive developmental researchobject permanenceGrasp2Vecour prior collaboration with X Roboticsreinforcement learningdiverse grasping skills from scratchbutton that supplies rewards directly to a robot when it is pushedcan be compressed into a low-dimensional spaceframes in a video can be predicted from previous framesspatial feature mapN-Pairs objectivecosine distanceour papergraspingpushingmanipulating",
      "link": "http://ai.googleblog.com/2018/12/grasp2vec-learning-object.html",
      "author": "Posted by Eric Jang, Software Engineer, Robotics at Google and Coline Devin, Berkeley PhD Student and former Research Intern"
    },
    {
      "title": "Providing Gender-Specific Translations in Google Translate",
      "date": "Monday, December 10, 2018",
      "abstract": "Providing Gender-Specific Translations in Google TranslateGoogle Translateend-to-end neural network-based systemwe announcedmorphologically complexCloud Natural Language APINeural Machine Translationmultilingual NMT systemsfairness in machine learning",
      "link": "http://ai.googleblog.com/2018/12/providing-gender-specific-translations.html",
      "author": "Posted by  Melvin Johnson, Senior Software Engineer, Google Translate"
    },
    {
      "title": "Adding Diversity to Images with Open Images Extended",
      "date": "Friday, December 7, 2018",
      "abstract": "Adding Diversity to Images with Open Images Extendedwe introducedInclusive Images Kaggle competitionNeurIPS 2018 Competition TrackOpen Images ExtendedCrowdsource apphereCrowdsource Android appcontact us",
      "link": "http://ai.googleblog.com/2018/12/adding-diversity-to-images-with-open.html",
      "author": "Posted by Anurag Batra and Parker Barnes, Product Managers, Google AI"
    },
    {
      "title": "TF-Ranking: A Scalable TensorFlow Library for Learning-to-Rank",
      "date": "Wednesday, December 5, 2018",
      "abstract": "TF-Ranking: A Scalable TensorFlow Library for Learning-to-Rankmachine translationdialogue systemscomputational biology.learning-to-rankTensorFlowTF-Rankingpaperpairwise or listwise loss functionsmulti-item scoringranking metric optimizationunbiased learning-to-rankloss functionprior workembeddingsMean Reciprocal RankNormalized Discounted Cumulative GainTensorBoardscored jointlyrecent workLETORArea Under the Curvediscontinuous or flatrecent work,expectation-maximizationPrior researchunbiased learning-to-rankTensorFlow EstimatorTensorboardTensorFlow ServingGitHub repotutorial examples",
      "link": "http://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html",
      "author": "Posted by Xuanhui Wang and Michael Bendersky, Software Engineers, Google AI"
    },
    {
      "title": "The NeurIPS 2018 Test of Time Award: The Trade-Offs of Large Scale Learning",
      "date": "Tuesday, December 4, 2018",
      "abstract": "The NeurIPS 2018 Test of Time Award: The Trade-Offs of Large Scale LearningThe Trade-Offs of Large Scale LearningNEC LabsFacebook AI ResearchGoogle AI, Z\u00fcrich)stochastic gradient descentKernel MachinesSVMsquadratic programmingLarge Scale Kernel Machines",
      "link": "http://ai.googleblog.com/2018/12/the-neurips-2018-test-of-time-award.html",
      "author": "Posted by Anna Ukhanova, Program Manager, Google AI Z\u00fcrich"
    },
    {
      "title": "Google at NeurIPS 2018",
      "date": "Monday, December 3, 2018",
      "abstract": "Google at NeurIPS 2018Conference on Neural Information Processing Systemsdefine a new framework3D-Aware Scene Manipulation via Inverse GraphicsA Retrieve-and-Edit Framework for Predicting Structured OutputsAdversarial Attacks on Stochastic BanditsAdversarial Examples that Fool both Computer Vision and Time-Limited HumansAdversarially Robust Generalization Requires More DataAre GANs Created Equal? A Large-Scale StudyCollaborative Learning for Deep Neural NetworksCompleting State Representations using Spectral LearningContent Preserving Text Generation with Attribute ControlsContext-aware Synthesis and Placement of Object InstancesCo-regularized Alignment for Unsupervised Domain AdaptationcpSGD: Communication-efficient and differentially-private distributed SGDData Center Cooling Using Model-Predictive ControlData-Efficient Hierarchical Reinforcement LearningDeep Attentive Tracking via Reciprocative LearningGeneralizing Point Embeddings Using the Wasserstein Space of Elliptical DistributionsGLoMo: Unsupervised Learning of Transferable Relational GraphsGroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model ShrinkingInterpreting Neural Network Judgments via Minimal, Stable, and Symbolic CorrectionsLearning Hierarchical Semantic Image Manipulation through Structured RepresentationsLearning Temporal Point Processes via Reinforcement LearningLearning Towards Minimum Hyperspherical EnergyMesh-TensorFlow: Deep Learning for SupercomputersMiME: Multilevel Medical Embedding of Electronic Health Records for Predictive HealthcareSearching for Efficient Multi-Scale Architectures for Dense Image PredictionSplineNets: Continuous Neural Decision GraphsTask-Driven Convolutional Recurrent Models of the Visual SystemTo Trust or Not to Trust a ClassifierTransfer Learning from Speaker Verification to Multispeaker Text-To-Speech SynthesisAlgorithms and Theory for Multiple-Source AdaptationA Lyapunov-based Approach to Safe Reinforcement LearningAdaptive Methods for Nonconvex OptimizationAssessing Generative Models via Precision and RecallA Loss Framework for Calibrated Anomaly DetectionBlockwise Parallel Decoding for Deep Autoregressive ModelsBreaking the Curse of Horizon: Infinite-Horizon Off-Policy EstimationContextual Pricing for Lipschitz BuyersCoupled Variational Bayes via Optimization EmbeddingData Amplification: A Unified and Competitive Approach to Property EstimationDeep Network for the Integrated 3D Sensing of Multiple People in Natural ImagesDeep Non-Blind Deconvolution via Generalized Low-Rank ApproximationDiminishing Returns Shape Constraints for Interpretability and RegularizationDropBlock: A Regularization Method for Convolutional NetworksGeneralization Bounds for Uniformly Stable AlgorithmsGeometrically Coupled Monte Carlo SamplingGILBO: One Metric to Measure Them AllInsights on Representational Similarity in Neural Networks with Canonical CorrelationImproving Online Algorithms via ML PredictionsLearning to Exploit Stability for 3D Scene ParsingMaximizing Induced Cardinality Under a Determinantal Point ProcessMemory Augmented Policy Optimization for Program Synthesis and Semantic ParsingPCA of High Dimensional Random Walks with Comparison to Neural Network TrainingPredictive Approximate Bayesian Computation via Saddle PointsRecurrent World Models Facilitate Policy EvolutionSanity Checks for Saliency MapsSimple, Distributed, and Accelerated Probabilistic ProgrammingTangent: Automatic Differentiation Using Source-Code Transformation for Dynamically Typed Array ProgrammingThe Emergence of Multiple Retinal Cell Types Through Efficient Coding of Natural MoviesThe Everlasting Database: Statistical Validity at a Fair PriceThe Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural NetworkA Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial AttacksAutoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific LanguageA Bayesian Nonparametric View on Count-Min SketchAutomatic Differentiation in ML: Where We are and Where We Should be GoingAssessing the Scalability of Biologically-Motivated Deep Learning Algorithms and ArchitecturesDeep Generative Models for Distribution-Preserving Lossy CompressionDeep Structured Prediction with Nonlinear Output TransformationsDiscovery of Latent 3D Keypoints via End-to-end Geometric ReasoningTransfer Learning with Neural AutoMLEfficient Gradient Computation for Structured Output Learning with Rational and Tropical LossesCooperative neural networks (CoNN): Exploiting prior independence structure for improved classificationGraph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic OptimizationHierarchical Reinforcement Learning for Zero-shot Generalization with Subtask DependenciesHuman-in-the-Loop Interpretability PriorJoint Autoregressive and Hierarchical Priors for Learned Image CompressionLarge-Scale Computation of Means and Clusters for Persistence Diagrams Using Optimal TransportLearning to Reconstruct Shapes from Unseen ClassesLarge Margin Deep Networks for ClassificationMallows Models for Top-k ListsMeta-Learning MCMC ProposalsNon-delusional Q-Learning and Value-IterationOnline Learning of Quantum StatesOnline Reciprocal Recommendation with Theoretical Performance GuaranteesOptimal Algorithms for Continuous Non-monotone Submodular and DR-Submodular MaximizationPolicy Regret in Repeated GamesProvable Variational Inference for Constrained Log-Submodular ModelsRealistic Evaluation of Deep Semi-Supervised Learning AlgorithmsSample-Efficient Reinforcement Learning with Stochastic Ensemble Value ExpansionVisual Object Networks: Image Generation with Disentangled 3D RepresentationsWatch Your Step: Learning Node Embeddings via Graph Attention2nd Workshop on Machine Learning on the Phone and Other Consumer DevicesBayesian Deep LearningContinual LearningThe Second Conversational AI Workshop \u2013 Today's Practice and Tomorrow's PotentialVisually Grounded Interaction and LanguageWorkshop on Ethical, Social and Governance Issues in AIAI for Social GoodBlack in AIInterpretability and Robustness in Audio, Speech, and LanguageLatinX in AIMachine Learning for SystemsQueer in AISecond Workshop on Machine Learning for Creativity and DesignWorkshop on Security in Machine LearningVisualization for Machine Learning",
      "link": "http://ai.googleblog.com/2018/12/google-at-neurips-2018.html",
      "author": "Posted by Slav Petrov, Principal Scientist, Google"
    },
    {
      "title": "Highlights from the 2018 Google PhD Fellowship Summit",
      "date": "Friday, November 30, 2018",
      "abstract": "Highlights from the 2018 Google PhD Fellowship SummitPhD Fellowship ProgramAustraliaChina and East AsiaIndiaNorth America, Europe, the Middle EastAfricaMaggie JohnsonMaya GuptaAndrew TomkinsRahul SukthankarSai Teja PeddintiAmin VahdatMartin StumpeEd ChiCiera JaspanJeff Dean",
      "link": "http://ai.googleblog.com/2018/11/highlights-from-2018-google-phd.html",
      "author": "Posted by Susie Kim, Program Manager, University Relations"
    },
    {
      "title": "Learning to Predict Depth on the Pixel 3 Phones",
      "date": "Thursday, November 29, 2018",
      "abstract": "Learning to Predict Depth on the Pixel 3 Phonesdual-pixel autofocustraditional non-learnedstereo algorithmHDR+last year\u2019s blog postparallaxthe aperture problemconvolutional neural networkTensorFlowskip connectionsresidual blocksstructure from motionmulti-view stereoTensorFlow LiteGoogle Photos depth editorthirdpartydepthextractorsalbum",
      "link": "http://ai.googleblog.com/2018/11/learning-to-predict-depth-on-pixel-3.html",
      "author": "Posted by Rahul Garg, Research Scientist and Neal Wadhwa, Software Engineer"
    },
    {
      "title": "A Structured Approach to Unsupervised Depth Learning from Monocular Videos",
      "date": "Tuesday, November 27, 2018",
      "abstract": "A Structured Approach to Unsupervised Depth Learning from Monocular VideosLIDARego-motionZhou et al.prior researchDepth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular VideosAAAI 2019open sourced the code in TensorFlowneural network to learn depth directlyKITTICityscapessolveFetch robot",
      "link": "http://ai.googleblog.com/2018/11/a-structured-approach-to-unsupervised.html",
      "author": "Posted by Anelia Angelova, Research Scientist, Robotics at Google"
    },
    {
      "title": "Improved Grading of Prostate Cancer Using Deep Learning",
      "date": "Friday, November 16, 2018",
      "abstract": "Improved Grading of Prostate Cancer Using Deep LearningpublishedNature Partner Journal (npj) Digital Medicine1 in 9 menmost common cancer in malesprostatectomyrisk stratificationGleason gradedeep learningGoogleothersDevelopment and Validation of a Deep Learning Algorithm for Improving Gleason Scoring of Prostate Canceroverall Gleason grade groupGleason patternsNational Institutes of Healthneedle core biopsiesgenitourinaryInterobserver Variability in Histologic Evaluation of Radical Prostatectomy Between Central and Local Pathologists: Findings of TAX 3501 Multinational Clinical TrialPhase 3 Study of Adjuvant Radiotherapy Versus Wait and See in pT3 Prostate Cancer: Impact of Pathology Review on AnalysisUtility of Quantitative Gleason Grading in Prostate Biopsies and Prostatectomy Specimens",
      "link": "http://ai.googleblog.com/2018/11/improved-grading-of-prostate-cancer.html",
      "author": "Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Healthcare, Google AI"
    },
    {
      "title": "Night Sight: Seeing in the Dark on Pixel Phones",
      "date": "Wednesday, November 14, 2018",
      "abstract": "Night Sight: Seeing in the Dark on Pixel PhonesHDR+herehereimage noiseshot noiseread noisesignal-to-noise ratioHDR+zero-shutter-lagHDR+Super Res Zoomoptical flow1herehereexposure stackingSynthcamsteadily became cleaner as you watchedHDR+'s merging algorithmSuper Res Zoomcolor constancycolor temperatureauto white balancingill-posed problemlearning-based AWB algorithmhereherecone cellshere2WikidataS-curveherehyperfocal distanceblog postexamplesA/B comparisonsexposure sliderGoogle\u2019s Photos editorbrace your phoneA/B album1\u21a92On the relation of optics to painting\u21a9",
      "link": "http://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html",
      "author": "Posted by Marc Levoy, Distinguished Engineer and Yael Pritch, Staff Software Engineer"
    },
    {
      "title": "Accurate Online Speaker Diarization with Supervised Learning",
      "date": "Monday, November 12, 2018",
      "abstract": "Accurate Online Speaker Diarization with Supervised LearningSpeaker diarizationunderstanding medical conversationsvideo captioningFully Supervised Speaker DiarizationNIST SRE 2000 CALLHOMEprevious clustering-based methoddeep neural network embedding methodsopen sourcing the core algorithmsk-meansspectral clusteringrecurrent neural networkChinese restaurant processstochastic gradient descentour paperGithub page",
      "link": "http://ai.googleblog.com/2018/11/accurate-online-speaker-diarization.html",
      "author": "Posted by Chong Wang, Research Scientist, Google AI"
    },
    {
      "title": "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing",
      "date": "Friday, November 2, 2018",
      "abstract": "Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processingnatural language processingquestion answeringsentiment analysisopen sourcedTransformersCloud TPUTensorFlowIn our associated paperStanford Question Answering DatasetSemi-supervised Sequence LearningGenerative Pre-TrainingELMoULMFitWikipediaword2vecGloVeword embeddingvery long timeCloud TPUsTransformer model architectureopen source releasetensor2tensor librarySQuAD v1.1GLUE benchmarkimproves upon the state-of-the-arthttp://goo.gl/language/bertColabBERT FineTuning with Cloud TPUsBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "link": "http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html",
      "author": "Posted by Jacob Devlin and Ming-Wei Chang, Research Scientists, Google AI Language"
    },
    {
      "title": "Google at EMNLP 2018",
      "date": "Wednesday, October 31, 2018",
      "abstract": "Google at EMNLP 2018Empirical Methods in Natural Language ProcessingComputational Natural Language Learningnatural language processingNoun-Verb Ambiguity in POS Tagging DatasetQuery Wellformedness DatasetParalex corpusWikiSplit: Split and Rephrase Dataset Extracted from Wikipedia EditsWikiAtomicEdits: A Multilingual Corpus of Atomic Wikipedia EditsConceptual CaptionsGAP Coreference Resolutionpast contributionsLinguistically-Informed Self-Attention for Semantic Role LabelingBest Long Paper awardsA Challenge Set and Methods for Noun-Verb AmbiguityA Fast, Compact, Accurate Model for Language Identification of Codemixed TextAirDialogue: An Environment for Goal-Oriented Dialogue ResearchContent Explorer: Recommending Novel Entities for a Document WriterDeep Relevance Ranking using Enhanced Document-Query InteractionsHotpotQA: A Dataset for Diverse, Explainable Multi-hop Question AnsweringIdentifying Well-formed Natural Language QuestionsLearning To Split and Rephrase From Wikipedia Edit HistoryLinguistically-Informed Self-Attention for Semantic Role LabelingOpen Domain Question Answering Using Early Fusion of Knowledge Bases and TextNoise Contrastive Estimation for Conditional Models: Consistency and Statistical EfficiencyPart-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language IdentificationPhrase-Indexed Question Answering: A New Challenge for Scalable Document ComprehensionPolicy Shaping and Generalized Update Equations for Semantic Parsing from DenotationsRevisiting Character-Based Neural Machine Translation with Capacity and CompressionSelf-governing neural networks for on-device short text classificationSemi-Supervised Sequence Modeling with Cross-View TrainingState-of-the-art Chinese Word Segmentation with Bi-LSTMsSubgoal Discovery for Hierarchical Dialogue Policy LearningSwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine TranslationThe Importance of Generation Order in Language ModelingTraining Deeper Neural Machine Translation Models with Transparent AttentionUnderstanding Back-Translation at ScaleUnsupervised Natural Language Generation with Denoising AutoencodersWikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and DiscourseWikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative CommunitySentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text ProcessingUniversal Sentence Encoder for EnglishMultilingual Parsing from Raw Text to Universal DependenciesUniversal Dependency Parsing with Multi-Treebank ModelsUniversal POS TaggingMorphological Taggingopen-sourced Meta-BiLSTM taggerSentence-Level Fluency Evaluation: References Help, But Can Be Spared!",
      "link": "http://ai.googleblog.com/2018/10/google-at-emnlp-2018.html",
      "author": "Posted by Manaal Faruqui, Senior Research Scientist and Emily Pitler, Staff Research Scientist, Google AI Language"
    },
    {
      "title": "Introducing AdaNet: Fast and Flexible AutoML with Learning Guarantees",
      "date": "Tuesday, October 30, 2018",
      "abstract": "Introducing AdaNet: Fast and Flexible AutoML with Learning GuaranteesEnsemble learningNetflix PrizeKaggle competitionsbecome more readily availablewill grow largerAdaNetTensorFlowreinforcement learningevolutionaryselecting optimal neural network architecturesTensorFlow EstimatorTensorFlow Hub modulesTensorFlow Model AnalysisGoogle Cloud\u2019s Hyperparameter TunerCIFAR-100TensorBoardTensorFlow ServingAdaNet: Adaptive Structural Learning of Artificial Neural NetworksICML 2017our tutorial about the AdaNet objectiveadanet.subnetwork.Buildertf.layersadanet.Estimatoropen-source implementationNASNet-A CIFAR architectureZoph et al., 2018CIFAR-10tf.contrib.estimator.Headsadanet.subnetwork.GeneratorGithub repotutorial notebooksworking examples using dense layers and convolutions",
      "link": "http://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html",
      "author": "Posted by Charles Weill, Software Engineer, Google AI, NYC"
    },
    {
      "title": "Acoustic Detection of Humpback Whales Using a Convolutional Neural Network",
      "date": "Monday, October 29, 2018",
      "abstract": "Acoustic Detection of Humpback Whales Using a Convolutional Neural NetworkGoogle AI Perceptionnon-speech captionsAudioSetmodel codeAI for Social GoodPacific Islands Fisheries Science CenterNational Oceanic and Atmospheric Administrationhumpback whalePassive acoustic monitoringhydrophonesWiggins and Hildebrand, 2007full text PDFdecimationspectrogramtonal unitResNet-50success at classifying non-speech audiosupervised learningfrequency-modulatedtonalper-channel energy normalizationwhale songUnsupervised Learning of Semantic Audio RepresentationsPrincipal component analysisTensorFlow Embedding Projectort-SNENOAA Fisheries blog post",
      "link": "http://ai.googleblog.com/2018/10/acoustic-detection-of-humpback-whales.html",
      "author": "Posted by Matt Harvey, Software Engineer, Google AI Perception"
    },
    {
      "title": "Curiosity and Procrastination in Reinforcement Learning",
      "date": "Wednesday, October 24, 2018",
      "abstract": "Curiosity and Procrastination in Reinforcement LearningReinforcement learningcarrot-and-stickDQNAlphaGoZeroOpenAI-Fivegrasp new objectsstruggleEpisodic Curiosity through ReachabilityGoogle Brain teamDeepMindETH Z\u00fcrichCuriosity-driven Exploration by Self-supervised PredictionIndira PaskoCC BY-NC-ND 4.0Deepak PathakCC BY 2.0Large-Scale Study of Curiosity-Driven LearningOpenAIEpisodic Curiosity through Reachabilitydeep neural networkViZDoomDMLabresearch paperCount-Based Exploration with Neural Density Models#Exploration: A Study of Count-Based Exploration for Deep Reinforcement LearningUnsupervised Learning of Goal Spaces for Intrinsically Motivated Goal ExplorationVIME: Variational Information Maximizing Exploration",
      "link": "http://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html",
      "author": "Posted by Nikolay Savinov, Research Intern, Google Brain Team and Timothy Lillicrap, Research Scientist, DeepMind"
    },
    {
      "title": "Fluid Annotation: An Exploratory Machine Learning\u2013Powered Interface for Faster Image Annotation",
      "date": "Monday, October 22, 2018",
      "abstract": "Fluid Annotation: An Exploratory Machine Learning\u2013Powered Interface for Faster Image AnnotationTensorFlow Object Detection APIOpen Imagessemantic segmentationtraditional manual labelingCOCOStuffFlorida Memoryoriginal imageFluid Annotation: A Human-Machine Collaboration Interface for Full Image AnnotationBrave New Ideas2018 ACM Multimedia Conferencegameneoriginal imageMask-RCNNtry out the demosneakaoriginal imageDan Hurtoriginal imageMelodie Mesianooriginal image",
      "link": "http://ai.googleblog.com/2018/10/fluid-annotation-exploratory-machine.html",
      "author": "Posted by Jasper Uijlings and Vittorio Ferrari, Research Scientists, Machine Perception"
    },
    {
      "title": "See Better and Further with Super Res Zoom on the Pixel 3",
      "date": "Monday, October 15, 2018",
      "abstract": "See Better and Further with Super Res Zoom on the Pixel 3(Updated August 6, 2020: The work described in this blogpost was presented at SIGGRAPH 2019, and has been published in the ACM Transactions on Graphics.)SIGGRAPH 2019publishedACM Transactions on GraphicsDSLR cameraslinear interpolation methodsRAISRdemosaicingcolor filter arrayBayer*burst photographyHDR+ algorithmknown for more than a decadedrizzlesuper-resolutionAliasingMoir\u00e9 patternsunfortunate choice of wardrobeOptical Image StabilizationedgesPortrait modeHDR+album*cone cells\u21a9",
      "link": "http://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html",
      "author": "Posted by Bartlomiej Wronski, Software Engineer and Peyman Milanfar, Lead Scientist, Computational Imaging"
    },
    {
      "title": "Applying Deep Learning to Metastatic Breast Cancer Detection",
      "date": "Friday, October 12, 2018",
      "abstract": "Applying Deep Learning to Metastatic Breast Cancer DetectionmetastasizedTNM cancer stagingabout 1 in 4as low as 38%deep learning\u2013based approach to improve diagnostic accuracy2016 ISBI Camelyon Challengesignificantly higher cancer detection ratesArtificial Intelligence Based Breast Cancer Nodal Metastasis Detection: Insights into the Black Box for PathologistsArchives of Pathology and Laboratory MedicineImpact of Deep Learning Assistance on the Histopathologic Review of Lymph Nodes for Metastatic Breast CancerThe American Journal of Surgical Pathologyfirst paperthe Naval Medical Center San Diegosecond paper",
      "link": "http://ai.googleblog.com/2018/10/applying-deep-learning-to-metastatic.html",
      "author": "Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Healthcare, Google AI"
    },
    {
      "title": "Open Sourcing Active Question Reformulation with Reinforcement Learning",
      "date": "Wednesday, October 10, 2018",
      "abstract": "Open Sourcing Active Question Reformulation with Reinforcement LearningNatural language understandingongoing focusmachine translationsyntacticsemanticmuch morebuilding blockTensorFlow package for Active Question Answeringreinforcement learningICLR 2018Ask the Right Questions: Active Question Reformulation with Reinforcement Learningsupervised learning techniquesreinforcement learningour papertf-idf query term re-weightingword stemmingsequence to sequenceTensorFlow Neural Machine Translation Tutorial code*convolutional neural networkGloVeBiDAFSeo et al. (2017)\n* The system we reported on in our paper used the TensorFlow sequence-to-sequence code used in Britz et al. (2017). Later, an open source version of the Google Translation model (GNMT) was published as a tutorial. The ActiveQA version released today is based on this more recent, and actively developed implementation. For this reason the released system varies slightly from the paper\u2019s. Nevertheless, the performance and behavior are qualitatively and quantitatively comparable.\u21a9*TensorFlow sequence-to-sequence codeBritz et al. (2017)open source versionGoogle Translation model (GNMT)published\u21a9",
      "link": "http://ai.googleblog.com/2018/10/open-sourcing-active-question.html",
      "author": "Posted by Michelle Chen Huebscher, Software Engineer and Rodrigo Nogueira, New York University PhD Student and Software Engineering Intern, Google AI Language"
    },
    {
      "title": "Highlights from the Google AI Residency Program",
      "date": "Tuesday, October 9, 2018",
      "abstract": "Highlights from the Google AI Residency Programinaugural class of the Google Brain ResidencyGoogle AI ResidencyGoogle AI teams2017 Neural Information Processing Systems Conferencemachine learningroboticshealthcareA studyAn algorithmInitialization methodsA methodResNet-50ImageNetmany moreAdversarial Examples that Fool both Computer Vision and Time-Limited HumansNIPS 2018Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement LearningICLR 2018Delta-OrthogonalDynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural NetworksICML 2018SGDDon\u2019t Decay the Learning Rate, Increase the Batch SizeICLR 2018global officesperceptionalgorithms and optimizationlanguagehealthcaremuch moreg.co/airesidency/applyg.co/airesidency",
      "link": "http://ai.googleblog.com/2018/10/highlights-from-google-ai-residency.html",
      "author": "Posted by Phing Lee, Program Manager, Google AI Residency"
    },
    {
      "title": "Introducing the Kaggle \u201cQuick, Draw!\u201d Doodle Recognition Challenge",
      "date": "Friday, September 28, 2018",
      "abstract": "Introducing the Kaggle \u201cQuick, Draw!\u201d Doodle Recognition ChallengeOnline handwriting recognitionTranslateKeepHandwriting Inputimprove your drawing abilitiesbuild virtual worldsQuick, Draw!dataset of 50M drawings1B that were drawnmany different new projectsKaggle \"Quick, Draw!\" Doodle Recognition Challenge\u201cQuick, Draw!\u201d datasettutorialchallenge websitekernels",
      "link": "http://ai.googleblog.com/2018/09/introducing-kaggle-quick-draw-doodle.html",
      "author": "Posted by Thomas Deselaers, Senior Staff Software Engineer and Jake Walker, Product Manager, Machine Perception"
    },
    {
      "title": "Building Google Dataset Search and Fostering an Open Data Ecosystem",
      "date": "Wednesday, September 26, 2018",
      "abstract": "Building Google Dataset Search and Fostering an Open Data EcosystemGoogle Dataset Searchlaunchadding structured metadata on their sitesschema.org/Datasetguidelinesscholarly discussionsschema.org/sameAsDigital Object IdentifierKnowledge GraphGoogle Scholarschema.orgW3C DCATJSON-LDStructured Data Testing TooladdDataset Search",
      "link": "http://ai.googleblog.com/2018/09/building-google-dataset-search-and.html",
      "author": "Posted by Matthew Burgess and Natasha Noy, Google AI"
    },
    {
      "title": "Google\u2019s Next Generation Music Recognition",
      "date": "Friday, September 14, 2018",
      "abstract": "Google\u2019s Next Generation Music RecognitionNow Playingdeep neural networksSound SearchNow Playing miniaturized music recognitionhybrid of spatial partitioning and vector quantization",
      "link": "http://ai.googleblog.com/2018/09/googles-next-generation-music.html",
      "author": "Posted by James Lyon, Google AI, Z\u00fcrich"
    },
    {
      "title": "Introducing the Unrestricted Adversarial Examples Challenge",
      "date": "Thursday, September 13, 2018",
      "abstract": "Introducing the Unrestricted Adversarial Examples Challengemedicinechemistryagricultureadversarial examplesprevious research on adversarial examplesimproved modelsnot subject to the \u201csmall modification\u201d constraintconfident errors when faced with an adversaryUnrestricted Adversarial Examples Challengesmall modifications to the input pixelsspatial transformationssimple guess-and-checkfull two-sided challenge with prizes for both attacks and defenses1234our paperthe project on githubleaderboard",
      "link": "http://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html",
      "author": "Posted by Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team"
    },
    {
      "title": "The What-If Tool: Code-Free Probing of Machine Learning Models",
      "date": "Tuesday, September 11, 2018",
      "abstract": "The What-If Tool: Code-Free Probing of Machine Learning ModelsGoogle AI PAIR initiativeWhat-If ToolTensorBoardFacetsUCI census datasetnumerical fairness criteriaCelebA datasetROC curveconfusion matrixequal opportunitymulticlass classification modelUCI iris datasetimage classification modelA regression modelUCI census dataset",
      "link": "http://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html",
      "author": "Posted by James Wexler, Software Engineer, Google AI"
    },
    {
      "title": "Text-to-Speech for Low-Resource Languages (Episode 4): One Down, 299 to Go",
      "date": "Friday, September 7, 2018",
      "abstract": "Text-to-Speech for Low-Resource Languages (Episode 4): One Down, 299 to Gofirst episodesecond episodethird episodeinitial investigationnew modelInternational Phonetic Alphabetlanguages of IndonesiaStandard IndonesianJavaneseSundanesephonologiesphonemeGoogle TranslateAndroidIndo-AryanDravidianSanskritTeluguKannadaWest BengaliOdiaGujaratiMarathiphonologyorthographyin our recent paperIndian BengaliGujaratiKannadaMalayalamMarathiTamilTeluguUrduNepaliSinhalaHindiBangladeshi BengaliGoogle for IndiaNepaliSinhalaBengaliKhmerJavaneseSundaneseSLTUInterspeech",
      "link": "http://ai.googleblog.com/2018/09/text-to-speech-for-low-resource.html",
      "author": "Posted by Alexander Gutkin, Software Engineer, Google AI"
    },
    {
      "title": "Introducing the Inclusive Images Competition",
      "date": "Thursday, September 6, 2018",
      "abstract": "Introducing the Inclusive Images CompetitionImageNetOpen ImagesConceptual Captionsfound to be geographically skewedInclusive Images Competition on KaggleConference on Neural Information Processing Systems Competition TrackOpen ImagesCrowdsource projectInclusive Images Competition websiteConference on Neural Information Processing Systemsthis page",
      "link": "http://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html",
      "author": "Posted by Tulsee Doshi, Product Manager, Google AI"
    },
    {
      "title": "Conceptual Captions: A New Dataset and Challenge for Image Captioning",
      "date": "Wednesday, September 5, 2018",
      "abstract": "Conceptual Captions: A New Dataset and Challenge for Image CaptioningAlt-text HTMLtext-to-speech systemsautomatic image captioningFrancis Vallance (Heritage Warrior)CC BY 2.0Conceptual Captionsa paperACL 2018MS-COCO datasetConceptual Captions ChallengeJonny HunterSigNote CloudTony HisgettResoluteSupportMediaCC BY 2.0our paperimage classification modelsword variations1RockoleandoCC BY 2.0RNNTransformerTensor2TensorMS-COCOour paperFlickr30KConceptual Captions Challenge\n1 In our paper, we posit that if automatic determination of names, locations, brands, etc. from the image is needed, it should be done as a separate task that may leverage image meta-information (e.g. GPS info), or complementary techniques such as OCR.\u21a91OCR\u21a9",
      "link": "http://ai.googleblog.com/2018/09/conceptual-captions-new-dataset-and.html",
      "author": "Posted by Piyush Sharma, Software Engineer and Radu Soricut, Research Scientist, Google AI"
    },
    {
      "title": "Understanding Performance Fluctuations in Quantum Processors",
      "date": "Friday, August 31, 2018",
      "abstract": "Understanding Performance Fluctuations in Quantum ProcessorsGoogle AI Quantum teamsuperconducting electrical circuitsquantum bitsmodest processor sizesFluctuations of Energy-Relaxation Times in Superconducting QubitsPhysical Review Letters energy relaxation timesstill puzzles researchersthermodynamics arguments",
      "link": "http://ai.googleblog.com/2018/08/understanding-performance-fluctuations.html",
      "author": "Posted by Paul V. Klimov, Research Scientist, Google AI Quantum Team"
    },
    {
      "title": "Teaching the Google Assistant to be Multilingual",
      "date": "Thursday, August 30, 2018",
      "abstract": "Teaching the Google Assistant to be Multilingual123launching145recurrent neural networksspeech recognition systemsrandom forest\n1 It is typically acknowledged that spoken language recognition is remarkably more challenging than text-based language identification where, relatively simple techniques based on dictionaries can do a good job. The time/frequency patterns of spoken words are difficult to compare, spoken words can be more difficult to delimit as they can be spoken without pause and at different paces and microphones may record background noise in addition to speech.\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2018/08/Multilingual-Google-Assistant.html",
      "author": "Posted by Johan Schalkwyk, VP and Ignacio Lopez Moreno, Engineer, Google Speech"
    },
    {
      "title": "Introducing a New Framework for Flexible and Reproducible Reinforcement Learning Research",
      "date": "Monday, August 27, 2018",
      "abstract": "Introducing a New Framework for Flexible and Reproducible Reinforcement Learning ResearchReinforcement learningDQNAlphaGoAlphaGo ZeroOpen AI Fivelarge-scale distributed trainindistributional methodsrobotic manipulationteaching robots to visually self-adaptTensorflow-based frameworkreward-motivated behaviour in the brainset of colabswell-documentedArcade Learning EnvironmentDQNC51Rainbow agentImplicit Quantile Network agentInternational Conference on Machine LearningMachado et al. (2018)full training dataJSON data filesa websiteTensorboarddownloads sectiongithub repo",
      "link": "http://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html",
      "author": "Posted by Pablo Samuel Castro, Research Software Developer and Marc G. Bellemare, Research Scientist, Google Brain Team"
    },
    {
      "title": "Moving Beyond Translation with the Universal Transformer",
      "date": "Wednesday, August 15, 2018",
      "abstract": "Moving Beyond Translation with the Universal Transformerthe Transformerrecurrent neural networksNeural GPUNeural Turing MachineUniversal TransformersTuring completeadaptive computation mechanismbAbI linguistic reasoning taskLAMBADA language modeling taskBLEU1hereMostafa DehghaniStephan GouwsOriol VinyalsJakob Uszkoreit\u0141ukasz KaiserAshish VaswaniDouglas EckDavid Dohan\n1 A translation quality benchmark widely used in the machine translation community, computed on the standard WMT newstest2014 English to German translation test data set.\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2018/08/moving-beyond-translation-with.html",
      "author": "Posted by Stephan Gouws, Research Scientist, Google Brain Team and Mostafa Dehghani, University of Amsterdam PhD student and Google Research Intern"
    },
    {
      "title": "The Machine Learning Behind Android Smart Linkify",
      "date": "Thursday, August 9, 2018",
      "abstract": "The Machine Learning Behind Android Smart Linkifywe launched Android 9 PieSmart LinkifySmart Text SelectionTextClassifier APITensorFlowTensorFlow LiteFlatBuffersherefeedforward neural networkshereSchema.orgopen-source as part of Android framework",
      "link": "http://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html",
      "author": "Posted by Lukas Zilka, Software Engineer, Google AI, Z\u00fcrich"
    },
    {
      "title": "MnasNet: Towards Automating the Design of Mobile Machine Learning Models",
      "date": "Tuesday, August 7, 2018",
      "abstract": "MnasNet: Towards Automating the Design of Mobile Machine Learning ModelsConvolutional neural networksMobileNetMobileNetV2AutoML neural architecture searchMnasNet: Platform-Aware Neural Architecture Search for Mobilereinforcement learningMobileNetV2NASNetFLOPSRNNTensorFlow Litemulti-objective optimizationPareto optimalImageNetCOCOMobileNetV2NASNetsqueeze-and-excitationResNet-50SSD300",
      "link": "http://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html",
      "author": "Posted by Mingxing Tan, Software Engineer, Google Brain Team"
    },
    {
      "title": "Machine Learning in Google BigQuery",
      "date": "Wednesday, July 25, 2018",
      "abstract": "Machine Learning in Google BigQueryGoogle BigQuerySQLBigQuery MLgradient descent*stochasticperform poorly when the data is suboptimally orderedfartrickierregularizationpreconditioningpaperBigQuery consoleuser guide\n* For example, a gradient vector can be computed using the SUM and GROUP BY operators, and the weights of a model can be updated using an INNER JOIN.\u21a9*gradient vectorSUMGROUP BYINNER JOIN\u21a9",
      "link": "http://ai.googleblog.com/2018/07/machine-learning-in-google-bigquery.html",
      "author": "Posted by Umar Syed and Sergei Vassilvitskii, Research Scientists, Google AI, NYC"
    },
    {
      "title": "Announcing Cirq: An Open Source Framework for NISQ Algorithms",
      "date": "Wednesday, July 18, 2018",
      "abstract": "Announcing Cirq: An Open Source Framework for NISQ Algorithmsquantum computingquantum algorithmsNoisy Intermediate Scale Quantum*First International Workshop on Quantum Software and Quantum Machine LearningGoogle AI Quantum teampublic alpha of Cirqcomputational problems of practical importanceApache 2installedOpenFermion-CirqOpenFermionlatest advancesZapata Computingquantum autoencoderexample codevideo tutorialQC Wareexample codevideo tutorialQuantum Benchmarkvideo tutorialHeisenberg Quantum SimulationsAnderson ModelCambridge Quantum Computingvideo tutorialNASAtemporal-planning for QAOAslidesslidesBristlecone processorCirqOpenFermion-Cirq\n* An analogous situation is how early classical programmers needed to run complex programs in very small memory spaces by paying careful attention to the lowest level details of the hardware.\u21a9*\u21a9",
      "link": "http://ai.googleblog.com/2018/07/announcing-cirq-open-source-framework.html",
      "author": "Posted by Alan Ho, Product Lead and Dave Bacon, Software Lead, Google AI Quantum Team"
    },
    {
      "title": "Improving Connectomics by an Order of Magnitude",
      "date": "Monday, July 16, 2018",
      "abstract": "Improving Connectomics by an Order of Magnitudeconnectomicselectron microscopyneuritessynapticcolleagues at the Max Planck Institute of NeurobiologyHigh-Precision Automated Reconstruction of Neurons with Flood-Filling NetworksNature Methodsrecurrent neural networkbiorXivimage segmentationedge detectorclassifierwatershedgraph cutrecurrent neural networksconvolutional neural networkmean-time-between-failurebiologically relevant quantitieszebra finchserial block-face scanning electron microscopyprevious deep learning pipelineszebra finchpreviously published approachtest theories related to how they learn their songopen-sourced the TensorFlow codeWebGL visualization software for 3d datasets",
      "link": "http://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html",
      "author": "Posted by Viren Jain, Research Scientist and Technical Lead and Michal Januszewski, Software Engineer, Connectomics at Google"
    },
    {
      "title": "Accelerated Training and Inference with the Tensorflow Object Detection API",
      "date": "Friday, July 13, 2018",
      "abstract": "Accelerated Training and Inference with the Tensorflow Object Detection APIwe announcedTensorFlow Object Detection APINeural Architecture Searchinstance segmentation supportOpen Imagesfinding scofflaws on the streets of NYCdiagnosing diseases on cassava plants in TanzaniaCloud TPUsTensorFlow LiteLin et al., 2017MobileNetPooling Pyramid NetworkCOCO datasetsingle shot detectorRetinaNetmean Average PrecisionTensorFlow Litemodel quantizationmodel quantizationJacob et al.whitepaper by Krishnamoorthicheck out our new tutorial",
      "link": "http://ai.googleblog.com/2018/07/accelerated-training-and-inference-with.html",
      "author": "Posted by Jonathan Huang, Research Scientist and Vivek Rathod, Software Engineer, Google AI Perception"
    },
    {
      "title": "Automating Drug Discoveries Using Computer Vision",
      "date": "Thursday, July 12, 2018",
      "abstract": "Automating Drug Discoveries Using Computer VisionPatrick CharbonneauMARCO initiativeProtein crystallizationMAchine Recognition of Crystallization OutcomesClassification of Crystallization Outcomes using Deep Convolutional Neural NetworksPLOS OneArXiv preprintopen-sourced our modelCloud ML Enginea large repository of curated crystallography imagesInception V3hereDuke Research Blog post",
      "link": "http://ai.googleblog.com/2018/07/automating-drug-discoveries-using.html",
      "author": "Vincent Vanhoucke, Principal Scientist, Google Brain Team"
    },
    {
      "title": "Google at ICML 2018",
      "date": "Monday, July 9, 2018",
      "abstract": "Google at ICML 2018International Conference on Machine LearningInternational Machine Learning SocietyTensorFlow HubMagenta projectGoogle AI ResidencPredict and Constrain: Modeling Cardinality in Deep Structured PredictionQuickshift++: Provably Good Initializations for Sample-Based Mean ShiftLearning a Mixture of Two Multinomial LogitsStructured Evolution with Compact Architectures for Scalable Policy OptimizationFixing a Broken ELBOHierarchical Long-term Video Prediction without SupervisionSelf-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory EmbeddingsWell Tempered LassoProgrammatically Interpretable Reinforcement LearningDynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural NetworksOn the Optimization of Deep Networks: Implicit Acceleration by OverparameterizationScalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness ConstraintsData Summarization at Scale: A Two-Stage Submodular ApproachMachine Theory of MindLearning to Optimize Combinatorial FunctionsProportional Allocation: Simple, Distributed, and Diverse Matching with High EntropyPath Consistency Learning in Tsallis Entropy Regularized MDPsEfficient Neural Architecture Search via Parameters SharingAdafactor: Adaptive Learning Rates with Sublinear Memory CostLearning Memory Access PatternsSBEED: Convergent Reinforcement Learning with Nonlinear Function ApproximationScalable Bilinear Pi Learning Using State and Action FeaturesDistributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?Shampoo: Preconditioned Stochastic Tensor OptimizationParallel and Streaming Algorithms for K-Core DecompositionCan Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?Is Generator Conditioning Causally Related to GAN Performance?The Mirage of Action-Dependent Baselines in Reinforcement LearningMentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted LabelsLoss Decomposition for Fast Learning in Large Output SpacesA Hierarchical Latent Vector Model for Learning Long-Term Structure in MusicSmoothed Action Value Functions for Learning Gaussian PoliciesFast Decoding in Sequence Models Using Discrete Latent VariablesAccelerating Greedy Coordinate Descent MethodsApproximate Leave-One-Out for Fast Parameter Tuning in High DimensionsImage TransformerTowards End-to-End Prosody Transfer for Expressive Speech Synthesis with TacotronDynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural NetworksStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech SynthesisConstrained Interacting Submodular GroupingsReinforcing Adversarial Robustness using Model Confidence Induced by Adversarial TrainingInterpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)Online Learning with AbstentionOnline Linear Quadratic ControlCompetitive Caching with Machine Learned AdviceEfficient Neural Audio SynthesisGradient Descent with Identity Initialization Efficiently Learns Positive Definite Linear Transformations by Deep Residual NetworksUnderstanding and Simplifying One-Shot Architecture SearchApproximation Algorithms for Cascading Prediction ModelsLearning Longer-term Dependencies in RNNs with Auxiliary LossesSelf-Imitation LearningAdaptive Sampled Softmax with Kernel Based Sampling2018 Workshop on Human Interpretability in Machine Learning (WHI)Exploration in Reinforcement LearningTheoretical Foundations and Applications of Deep Generative Models",
      "link": "http://ai.googleblog.com/2018/07/google-at-icml-2018.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Google AI Communications"
    },
    {
      "title": "Scalable Deep Reinforcement Learning for Robotic Manipulation",
      "date": "Thursday, June 28, 2018",
      "abstract": "Scalable Deep Reinforcement Learning for Robotic Manipulationdeep learningreinforcement learningprevious worklarge-scale distributed optimizationQ-learningarXivthat sharing experience across robots can accelerate learningdomain adaptationrecent work on learning how to self-calibrateXarXiv",
      "link": "http://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html",
      "author": "Posted Alex Irpan, Software Engineer, Google Brain Team and Peter Pastor, Senior Roboticist, X"
    },
    {
      "title": "Self-Supervised Tracking via Video Colorization",
      "date": "Wednesday, June 27, 2018",
      "abstract": "Self-Supervised Tracking via Video Colorizationactivity recognitionobject interactionvideo stylizationTracking Emerges by Colorizing VideosDAVIS 2017Kinetics datasetthe DAVIS 2017 datasetKinetics datasetPrincipal Component AnalysisDAVIS 2017 datasetJHMDBlatest methodsoptical flowthe paper",
      "link": "http://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html",
      "author": "Posted by Carl Vondrick, Research Scientist, Machine Perception"
    },
    {
      "title": "Teaching Uncalibrated Robots to Visually Self-Adapt",
      "date": "Friday, June 22, 2018",
      "abstract": "Teaching Uncalibrated Robots to Visually Self-AdaptSim2Real Viewpoint Invariant Visual Servoing by Recurrent ControlCVPR 2018fully convolutional networkslong short-term memorydegrees of freedomdistributing the data collection and trials to multiple robotsSadeghi & Levineindoor navigationobject localizationpick and placingreinforcement learningpybullet",
      "link": "http://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html",
      "author": "Posted by Fereshteh Sadeghi, Student Researcher, Google Brain Team"
    },
    {
      "title": "How Can Neural Network Similarity Help Us Understand Training and Generalization?",
      "date": "Thursday, June 21, 2018",
      "abstract": "How Can Neural Network Similarity Help Us Understand Training and Generalization?previous postCanonical Correlation Analysisconvolutional neural networksInsights on Representational Similarity in Neural Networks with Canonical Correlationrecurrent neural networkscode used for applying CCA on neural networksZhang et al., 2017our papersoftmaxprevious workcode",
      "link": "http://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html",
      "author": "Posted by Maithra Raghu, Google Brain Team and Ari S. Morcos, DeepMind"
    },
    {
      "title": "Google at CVPR 2018",
      "date": "Monday, June 18, 2018",
      "abstract": "Google at CVPR 20182018 Conference on Computer Vision and Pattern Recognitionworkshopstutorialsmachine perceptionportrait mode on the Pixel 2 and Pixel 2 XL smartphonesOpen Images V4 datasetUnsupervised Discovery of Object Landmarks as Structural RepresentationsDoubleFusion: Real-time Capture of Human Performances with Inner Body Shapes from a Single Depth SensorNeural Kinematic Networks for Unsupervised Motion RetargettingBurst Denoising with Kernel Prediction NetworksQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only InferenceAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual ActionsFocal Visual-Text Attention for Visual Question AnsweringInferring Light Fields from ShadowsModifying Non-Local Variations Across Multiple ViewsIterative Visual Reasoning Beyond ConvolutionsUnsupervised Training for 3D Morphable Model RegressionLearning Transferable Architectures for Scalable Image RecognitionThe iNaturalist Species Classification and Detection DatasetLearning Intrinsic Image Decomposition from Watching the WorldLearning Intelligent Dialogs for Bounding Box AnnotationRevisiting Knowledge Transfer for Training Object Class DetectorsRethinking the Faster R-CNN Architecture for Temporal Action LocalizationHierarchical Novelty Detection for Visual Object RecognitionCOCO-Stuff: Thing and Stuff Classes in ContextAppearance-and-Relation Networks for Video ClassificationMorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep NetworksDeformable Shape Completion with Graph Convolutional AutoencodersMegaDepth: Learning Single-View Depth Prediction from Internet PhotosUnsupervised Discovery of Object Landmarks as Structural RepresentationsBurst Denoising with Kernel Prediction NetworksPix3D: Dataset and Methods for Single-Image 3D Shape ModelingSparse, Smart Contours to Represent and Edit ImagesMaskLab: Instance Segmentation by Refining Object Detection with Semantic and Direction FeaturesLarge Scale Fine-Grained Categorization and Domain-Specific Transfer LearningImproved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent NetworksMobileNetV2: Inverted Residuals and Linear BottlenecksScanComplete: Large-Scale Scene Completion and Semantic Segmentation for 3D ScansSim2Real View Invariant Visual Servoing by Recurrent ControlAlternating-Stereo VINS: Observability Analysis and Performance EvaluationSoccer on Your TabletopUnsupervised Learning of Depth and Ego-Motion from Monocular Video Using 3D Geometric ConstraintsAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual ActionsInferring Light Fields from ShadowsModifying Non-Local Variations Across Multiple ViewsAperture Supervision for Monocular Depth EstimationInstance Embedding Transfer to Unsupervised Video Object SegmentationFrame-Recurrent Video Super-ResolutionWeakly Supervised Action Localization by Sparse Temporal Pooling NetworkIterative Visual Reasoning Beyond ConvolutionsLearning and Using the Arrow of TimeHydraNets: Specialized Dynamic Architectures for Efficient InferenceThoracic Disease Identification and Localization with Limited SupervisionInferring Semantic Layout for Hierarchical Text-to-Image SynthesisDeep Semantic Face DeblurringUnsupervised Training for 3D Morphable Model RegressionLearning Transferable Architectures for Scalable Image RecognitionLearning Intrinsic Image Decomposition from Watching the WorldPiCANet: Learning Pixel-wise Contextual Attention for Saliency DetectionMobile Video Object Detection with Temporally-Aware Feature MapsComputer Vision for Robotics and DrivingUnsupervised Visual LearningUltraFast 3D Sensing, Reconstruction and Understanding of People, Objects and EnvironmentsGenerative Adversarial Networks",
      "link": "http://ai.googleblog.com/2018/06/google-at-cvpr-2018.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Google AI Communications"
    },
    {
      "title": "Google at NAACL",
      "date": "Friday, June 8, 2018",
      "abstract": "Google at NAACLNorth American Association of Computational LinguisticsWidening Natural Language Processing WorkshopTest of Time AwardBLEU: a Method for Automatic Evaluation of Machine TranslationDiscriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron AlgorithmsThumbs up?: Sentiment Classification using Machine Learning TechniquesGoogle AI Language Team pageGoogle Assistant or My Assistant? Towards Personalized Situated Conversational AgentsBootstrapping a Neural Conversational Agent with Dialogue Self-Play, Crowdsourcing and On-Line Reinforcement LearningSHAPED: Shared-Private Encoder-Decoder for Text Style AdaptationOlive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds Using Paraphrases in a Neural ModelAre All Languages Equally Hard to Language-Model?Self-Attention with Relative Position RepresentationsDialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue SystemsSubword & Character Level Models in NLPStorytelling WorkshopEthics in NLPCareers in IndustryEthics in NLP",
      "link": "http://ai.googleblog.com/2018/06/google-at-naacl.html",
      "author": "Posted by Kenton Lee, Research Scientist and Slav Petrov, Principal Scientist, Language Team, Google AI"
    },
    {
      "title": "Realtime tSNE Visualizations with TensorFlow.js",
      "date": "Thursday, June 7, 2018",
      "abstract": "Realtime tSNE Visualizations with TensorFlow.jst-distributed Stochastic Neighbor EmbeddingTensorFlow Embedding ProjectorTensorBoardLinear tSNE Optimization for the WebWebGLreleasing this work as an open source libraryTensorFlow.jsYou can find a live demo hereobjective functionN-body simulationBarnes-HutMNISTTensorFlow.jsan open source libraryN-body simulations",
      "link": "http://ai.googleblog.com/2018/06/realtime-tsne-visualizations-with.html",
      "author": "Posted by Nicola Pezzotti, Software Engineering Intern, Google Z\u00fcrich"
    },
    {
      "title": "Announcing an updated YouTube-8M, and the 2nd YouTube-8M Large-Scale Video Understanding Challenge and Workshop",
      "date": "Tuesday, June 5, 2018",
      "abstract": "Announcing an updated YouTube-8M, and the 2nd YouTube-8M Large-Scale Video Understanding Challenge and WorkshopYouTube-8M Large-Scale Video Understanding ChallengeKaggleYouTube-8M datasetworkshop at CVPR\u201917update to the YouTube-8M dataseta new Kaggle video understanding challenge2nd Workshop on YouTube-8M Large-Scale Video Understanding2018 European Conference on Computer Visionground truthstarter codeTensorFlow2nd YouTube-8M Video Understanding ChallengeKaggle competition pageECCV\u201918workshop page",
      "link": "http://ai.googleblog.com/2018/06/announcing-updated-youtube-8m-and-2nd.html",
      "author": "Posted by Joonseok Lee, Software Engineer, Google AI"
    },
    {
      "title": "Improving Deep Learning Performance with AutoAugment",
      "date": "Monday, June 4, 2018",
      "abstract": "Improving Deep Learning Performance with AutoAugmentdesign neural network architecturesoptimizersAutoAugment: Learning Augmentation Policies from Datareinforcement learningmixupImageNetstreet view of house numbersSVHNCIFAR-10ImageNetImageNetStanford CarsFGVC-Aircraftappendix of the paper",
      "link": "http://ai.googleblog.com/2018/06/improving-deep-learning-performance.html",
      "author": "Posted by Ekin Dogus Cubuk, Google AI Resident and Barret Zoph, Research Scientist, Google Brain Team"
    },
    {
      "title": "Advances in Semantic Textual Similarity",
      "date": "Thursday, May 17, 2018",
      "abstract": "Advances in Semantic Textual SimilaritySmart ComposeTalk to BooksTensorFlow HubLearning Semantic Textual Similarity from ConversationsSNLIentailmentSTSBenchmarkCQA task BUniversal Sentence Encoderskip-thoughtdeep average networkTransformerUniversal Sentence EncodermodelTensorFlow HubUniversal Sentence Encoder - LargeUniversal Sentence Encoder - LiteTransformerSentence Piece",
      "link": "http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html",
      "author": "Posted by Yinfei Yang, Software Engineer and Chris Tar,  Engineering Manager, Google AI"
    },
    {
      "title": "Smart Compose: Using Neural Networks to Help Write Emails",
      "date": "Wednesday, May 16, 2018",
      "abstract": "Smart Compose: Using Neural Networks to Help Write EmailsGoogle I/OSmart ComposeSmart Replyngramneural bag-of-wordsRNN languagesequence-to-sequenceword embeddingsTPUv2 PodFairness in machine learningSemantics derived automatically from language corpora contain human-like biasesthis paperTransformer,RNMT+",
      "link": "http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html",
      "author": "Posted by Yonghui Wu, Principal Engineer, Google Brain Team"
    },
    {
      "title": "Automatic Photography with Google Clips",
      "date": "Friday, May 11, 2018",
      "abstract": "Automatic Photography with Google ClipsHenri Cartier-BressonGoogle ClipsWe then hiredrecognize over 27,000 different labelsMobileNetpiecewise linear regression modelmost recent releasefairness  in ML algorithms",
      "link": "http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html",
      "author": "Posted by Aseem Agarwala, Research Scientist, Clips Content Team Lead"
    },
    {
      "title": "Custom On-Device ML Models with Learn2Compress",
      "date": "Wednesday, May 9, 2018",
      "abstract": "Custom On-Device ML Models with Learn2CompressOn-device machine learningMobileNetsProjectionNetsML KitTensorFlow LitehereProjectionNeton-device modelsQuantizationJoint trainingdistillationtransfer learningMobileNetsNASNetInceptionProjectionNetCIFAR-10ImageNetNASNetMobileNetImageNetCIFAR-10CIFAR-10NASNetFishbrain",
      "link": "http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html",
      "author": "Posted by Sujith Ravi, Senior Staff Research Scientist, Google Expander Team"
    },
    {
      "title": "Google Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone",
      "date": "Tuesday, May 8, 2018",
      "abstract": "Google Duplex: An AI System for Accomplishing Real-World Tasks Over the PhoneGoogle voice searchWaveNetrecurrent neural networkTensorFlow ExtendedTacotronWaveNetGoogle Assistant",
      "link": "http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html",
      "author": "Posted by Yaniv Leviathan, Principal Engineer and Yossi Matias, Vice President, Engineering, Google"
    },
    {
      "title": "Deep Learning for Electronic Health Records",
      "date": "Tuesday, May 8, 2018",
      "abstract": "Deep Learning for Electronic Health Recordsdata wranglingUC San FranciscoStanford MedicineThe University of Chicago MedicineScalable and Accurate Deep Learning with Electronic Health RecordsNature Partner Journals: Digital MedicineFast Healthcare Interoperability Resourcesin an earlier blog postrecurrent neural networksfeedforwardFHIRarea-under-the-receiver-operator curvepaper",
      "link": "http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html",
      "author": "Posted by Alvin Rajkomar MD, Research Scientist and Eyal Oren PhD, Product Manager, Google AI"
    },
    {
      "title": "Introducing Google AI",
      "date": "Monday, May 7, 2018",
      "abstract": "Introducing Google AIcomputer visionhealthcare researchAutoMLproductsplatformsGoogle AI websiteTwitterGoogle+",
      "link": "http://ai.googleblog.com/2018/05/introducing-google-ai.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Google AI Communications"
    },
    {
      "title": "The Question of Quantum Supremacy",
      "date": "Friday, May 4, 2018",
      "abstract": "The Question of Quantum SupremacyQuantum computinginformation technologyquantum mechanicsintractablecomputational tasksCharacterizing quantum supremacy in near-term devicesherebutterfly effectqubitArguablyexponentialseparationcomputationalimprovementsclassicalalgorithmssimulate quantum circuitsincrease the sizesimulation costherefault-toleranthighly complexSpace-time volumeeasier to simulateA blueprint for demonstrating quantum supremacy with superconducting qubitsheredigital quantum processorsurface code error correctionquantum processorsquantum algorithmsapplications",
      "link": "http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html",
      "author": "Posted by Sergio Boixo, Research Scientist and Theory Team Lead, and Charles Neill, Quantum Electronics Engineer, Quantum A.I. Lab"
    },
    {
      "title": "Announcing Open Images V4 and the ECCV 2018 Open Images Challenge",
      "date": "Monday, April 30, 2018",
      "abstract": "Announcing Open Images V4 and the ECCV 2018 Open Images ChallengeOpen ImagesupdatingrefiningOpen Images V4visualizerMark Paul Gosselaar plays the guitarRhys A.CivilizationPaul DowneyCC BY 2.0Open Images Challenge2018 European Conference on Computer VisionPASCAL VOCImageNetCOCOcrowdsource.google.com",
      "link": "http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html",
      "author": "Posted by Vittorio Ferrari, Research Scientist, Machine Perception"
    },
    {
      "title": "Google at ICLR 2018",
      "date": "Sunday, April 29, 2018",
      "abstract": "Google at ICLR 20186th International Conference on Learning Representationsmachine learningneural networksdeep learningWasserstein Auto-EncodersOn the Convergence of Adam and BeyondAsk the Right Questions: Active Question Reformulation with Reinforcement LearningBeyond Word Importance: Contextual Decompositions to Extract Interactions from LSTMsBoosting the Actor with Dual CriticMaskGAN: Better Text Generation via Filling in the _______Scalable Private Learning with PATEDeep Gradient Compression: Reducing the Communication Bandwidth for Distributed TrainingFlipout: Efficient Pseudo-Independent Weight Perturbations on Mini-BatchesLatent Constraints: Learning to Generate Conditionally from Unconditional Generative ModelsMulti-Mention Learning for Reading Comprehension with Neural CascadesQANet: Combining Local Convolution with Global Self-Attention for Reading ComprehensionSensitivity and Generalization in Neural Networks: An Empirical StudyAction-dependent Control Variates for Policy Optimization via Stein IdentityAn Efficient Framework for Learning Sentence RepresentationsFidelity-Weighted LearningGenerating Wikipedia by Summarizing Long SequencesMatrix Capsules with EM RoutingTemporal Difference Models: Model-Free Deep RL for Model-Based ControlDeep Neural Networks as Gaussian ProcessesMany Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence at Every StepInitialization Matters: Orthogonal Predictive State Recurrent Neural NetworksLearning Differentially Private Recurrent Language ModelsLearning Latent Permutations with Gumbel-Sinkhorn NetworksLeave no Trace: Learning to Reset for Safe and Autonomous Reinforcement LearningMeta-Learning for Semi-Supervised Few-Shot ClassificationThermometer Encoding: One Hot Way to Resist Adversarial ExamplesA Hierarchical Model for Device PlacementMonotonic Chunkwise AttentionTraining Confidence-calibrated Classifiers for Detecting Out-of-Distribution SamplesTrust-PCL: An Off-Policy Trust Region Method for Continuous ControlEnsemble Adversarial Training: Attacks and DefensesStochastic Variational Video PredictionDepthwise Separable Convolutions for Neural Machine TranslationDon\u2019t Decay the Learning Rate, Increase the Batch SizeGenerative Models of Visually Grounded ImaginationLarge Scale Distributed Neural Network Training through Online DistillationLearning a Neural Response Metric for Retinal ProsthesisNeumann Optimizer: A Practical Optimization Algorithm for Deep Neural NetworksA Neural Representation of Sketch DrawingsDeep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson SamplingGeneralizing Hamiltonian Monte Carlo with Neural NetworksLeveraging Grammar and Reinforcement Learning for Neural Program Synthesis On the Discrimination-Generalization Tradeoff in GANsA Bayesian Perspective on Generalization and Stochastic Gradient DescentLearning how to Explain Neural Networks: PatternNet and PatternAttributionSkip RNN: Learning to Skip State Updates in Recurrent Neural NetworksTowards Neural Phrase-based Machine TranslationUnsupervised Cipher Cracking Using Discrete GANsVariational Image Compression With A Scale HyperpriorLocal Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter ValuesStoachastic Gradient Langevin Dynamics that Exploit Neural Network StructureTowards Mixed-initiative generation of multi-channel sequential structureCan Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?GILBO: One Metric to Measure Them AllHoME: a Household Multimodal EnvironmentLearning to Learn without LabelsLearning via Social Awareness: Improving Sketch Representations with Facial FeedbackNegative Eigenvalues of the Hessian in Deep Neural NetworksRealistic Evaluation of Semi-Supervised Learning AlgorithmsWinner's Curse? On Pace, Progress, and Empirical RigorMeta-Learning for Batch Mode Active LearningTo Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model CompressionAdversarial SpheresClustering Meets Implicit Generative ModelsDecoding Decoders: Finding Optimal Representation Spaces for Unsupervised Similarity TasksLearning Longer-term Dependencies in RNNs with Auxiliary LossesGraph Partition Neural Networks for Semi-Supervised ClassificationSearching for Activation FunctionsTime-Dependent Representation for Neural Event Sequence PredictionFaster Discovery of Neural Architectures by Searching for Paths in a Large ModelIntriguing Properties of Adversarial ExamplesPPP-Net: Platform-aware Progressive Search for Pareto-optimal Neural ArchitecturesThe Mirage of Action-Dependent Baselines in Reinforcement LearningLearning to Organize Knowledge with N-Gram MachinesOnline variance-reducing optimization",
      "link": "http://ai.googleblog.com/2018/04/google-at-iclr-2018.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow, Head of Google Research and Machine Intelligence"
    },
    {
      "title": "Announcing the Google Cloud Platform Research Credits Program",
      "date": "Thursday, April 26, 2018",
      "abstract": "Announcing the Google Cloud Platform Research Credits Programmachine learning tools to understand and address student questionsstudy robotic interactions with humansCloud Platform (GCP) research creditsmade the transitionopen-source platform to manage the complex data sets of climate sciencevariant analysis pipelineeligiblewebsiteapply now",
      "link": "http://ai.googleblog.com/2018/04/announcing-google-cloud-platform.html",
      "author": "Posted by Steven Butschi, Head of Higher Education, Google"
    },
    {
      "title": "Google\u2019s Workshop on AI/ML Research and Practice in India",
      "date": "Tuesday, April 24, 2018",
      "abstract": "Google\u2019s Workshop on AI/ML Research and Practice in IndiaWorkshop on Artificial Intelligence and Machine LearninghereJeff DeanPrabhakar Raghavandeep learning to solve challenging problemsreinventing productivity using AIRajen ShethRoberto Bayardopanel discussionPankaj GuptaAshish TendulkaragendaBalaraman Ravindranstructure in deep reinforcement learningSunita Sarawagidomain generalization via cross-gradient trainingTanuja Ganucreating intelligent and sustainable energy systemsRushi Bhattmaintaining professional quality of the LinkedIn feed using deep learningSourangshu Bhattacharyatask specific representation learning for web-scale entity disambiguationMausamhierarchical pointer memory network for task-oriented dialoguePartha Talukdarcanonicalizing open knowledge graphsManish Guptaapplying machine learning to support human learningPankaj Guptasmart customer supportML/data science algorithms that power OlaPrateek Jainresource-efficient ML in 2 KB RAM for the Internet of ThingsAnimesh Mukherjeeranking state-of-the-art papers via incomplete tournaments induced by citations from performance tablesSrujana Merugupractitioner's perspective on building ML SystemsVineeth Balsubramaniangeneralized gradient-based visual explanations for deep convolutional networks (Grad-CAM++)Vinay Namboodiriadversarial machine learningVishnu Makkapatigeneration of fashion designs using generative adversarial networksVarun Gulshanusing ML in medical imagingSoma Biswaspreserving semantic similarity for zero-shot learningAditya Gopalanonline and reinforcement learning in complex environmentsGanesh Ramakrishnanhuman assisted machine learningLavanya Tekumallausing Bayesian models for product size recommendationKabir Rustogiaddressing geocoding using graph based machine learning modelsMohit Kumarfraudulent user prediction in ratings platformML challenges in Google Maps auto-moderationresearch awards round",
      "link": "http://ai.googleblog.com/2018/04/googles-workshop-on-aiml-research-and.html",
      "author": "Posted by Pankaj Gupta and Anand Rangarajan, Engineering Directors, Google India"
    },
    {
      "title": "Introducing the CVPR 2018 On-Device Visual Intelligence Challenge",
      "date": "Friday, April 20, 2018",
      "abstract": "Introducing the CVPR 2018 On-Device Visual Intelligence ChallengeMobileNet model familyinteger quantizationOn-device Visual Intelligence ChallengeLow-Power Image Recognition Challenge WorkshopCVPR20181multiply-accumulate operationsdeep neural networkoptimized kernels for mobile CPUslight-weight portable model formatsTOCOTensorFlowTensorFlow Lite inference engineA benchmarking SDKSample modelstutorialhere1\u21a9",
      "link": "http://ai.googleblog.com/2018/04/introducing-cvpr-2018-on-device-visual.html",
      "author": "Posted by Bo Chen, Software Engineer and Jeffrey M. Gilbert, Member of Technical Staff, Google Research"
    },
    {
      "title": "DeepVariant Accuracy Improvements for Genetic Datatypes",
      "date": "Thursday, April 19, 2018",
      "abstract": "DeepVariant Accuracy Improvements for Genetic Datatypesreleased DeepVariantgreater accuracy than previous methodsDeepVariant v0.6whole exome sequencingpolymerase chain reactionDeep learningGenome in a BottleNational Institute of Standards and Technologyfirst benchmarking genome released by GIABInternational HapMap Projectv0.5 releasecodes for proteinsDNAnexustraining dataindelsingle nucleotide polymorphismv0.6polymerase chain reactionfigure 10bcbioDNAnexusbcbiolet us know",
      "link": "http://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html",
      "author": "Posted by Pi-Chuan Chang, Software Engineer and Lizzie Dorfman, Technical Program Manager, Google Brain Team"
    },
    {
      "title": "An Augmented Reality Microscope for Cancer Detection",
      "date": "Monday, April 16, 2018",
      "abstract": "An Augmented Reality Microscope for Cancer DetectionpublishedNature Medicinedeep learningophthalmologydermatologyradiologypathologypublished resultsconvolutional neural networktalkAACRAn Augmented Reality Microscope for Real-time Automated Detection of CancerTensorFlow",
      "link": "http://ai.googleblog.com/2018/04/an-augmented-reality-microscope.html",
      "author": "Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Google Brain Team"
    },
    {
      "title": "Introducing Semantic Experiences with Talk to Books and Semantris",
      "date": "Friday, April 13, 2018",
      "abstract": "Introducing Semantic Experiences with Talk to Books and SemantrisNatural language understandingword vectorsmake improvements to Smart Reply for GmailSemantic ExperiencesTalk to BooksSemantrisUniversal Sentence Encoderpretrained semantic TensorFlow moduleEfficient Natural Language Response for Smart ReplyTalk to BooksSemantrisTry it out yourselfTensorFlow models",
      "link": "http://ai.googleblog.com/2018/04/introducing-semantic-experiences-with.html",
      "author": "Posted by Ray Kurzweil, Director of Engineering and Rachel Bernstein, Product Manager, Google Research"
    },
    {
      "title": "Seeing More with In Silico Labeling of Microscopy Images",
      "date": "Thursday, April 12, 2018",
      "abstract": "Seeing More with In Silico Labeling of Microscopy Imagesmicroscopyautomatically assess the quality of imagesassist pathologists diagnosing cancerous tissueIn Silico Labeling: Predicting Fluorescent Labels in Unlabeled Imagesopen sourced our networkphase-contrastphase-shiftedmotor neuron cultureinduced pluripotent stem cellsneuritesFinkbeiner labprogrammed cell deathdendritesaxonsneuritesInceptionGoogle HypertuneSteve Finkbeiner's lab at the Gladstone InstitutesRubin Lab at Harvardbright-fieldphase-contrastdifferential interference contrastmotor neuronsinduced pluripotent stem cellscortical culturesopen sourced our modeltransfer learningcheck out the codeGoogle Accelerated Science team",
      "link": "http://ai.googleblog.com/2018/04/seeing-more-with-in-silico-labeling-of.html",
      "author": "Eric Christiansen, Senior Software Engineer, Google Research"
    },
    {
      "title": "Looking to Listen: Audio-Visual Speech Separation",
      "date": "Wednesday, April 11, 2018",
      "abstract": "Looking to Listen: Audio-Visual Speech Separationcocktail party effectLooking to Listen at the Cocktail PartySIGGRAPH 2018AudioSetspectrogramour paperproject web pagethis work from UC Berkeleythis work from MIT",
      "link": "http://ai.googleblog.com/2018/04/looking-to-listen-audio-visual-speech.html",
      "author": "Posted by Inbar Mosseri and Oran Lang, Software Engineers, Google Research"
    },
    {
      "title": "Announcing the 2018 Google PhD Fellows for North America, Europe and the Middle East",
      "date": "Thursday, April 5, 2018",
      "abstract": "Announcing the 2018 Google PhD Fellows for North America, Europe and the Middle EastPhD Fellowship program",
      "link": "http://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html",
      "author": "Posted by Susie Kim, Program Manager, University Relations"
    },
    {
      "title": "MobileNetV2: The Next Generation of On-Device Computer Vision Networks",
      "date": "Tuesday, April 3, 2018",
      "abstract": "MobileNetV2: The Next Generation of On-Device Computer Vision NetworksMobileNetV1MobileNetV2TensorFlow-Slim Image Classification LibraryColaboratorydownloadJupytermoduleson github1MobileNet V2: Inverted Residuals and Linear BottlenecksTensorflow Object Detection APIMobileNetV2 + SSDLiteannounced recentlyPASCAL VOC 2012MobileNetV2 + DeepLabV3MobileNets: Efficient Convolutional Neural Networks for Mobile Vision ApplicationsMobileNetV2: Inverted Residuals and Linear BottlenecksRethinking Atrous Convolution for Semantic Image SegmentationSpeed/accuracy trade-offs for modern convolutional object detectorsDeep Residual Learning for Image Recognition1\u21a9",
      "link": "http://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html",
      "author": "Posted by Mark Sandler and Andrew Howard, Google Research"
    },
    {
      "title": "Investing in France\u2019s AI Ecosystem",
      "date": "Wednesday, March 28, 2018",
      "abstract": "Investing in France\u2019s AI Ecosystemannounced the launch of a new AI research team in our Paris officeDeepMind has also announcedFields MedalistC\u00e9dric Villanireport on AI\u00c9cole PolytechniqueINRIAFaculty Research AwardsPhD FellowshipFocused Research AwardsFrancis BachINRIAENSAlexandre d\u2019AspremontCNRSENSCIFRECordelia SchmidINRIAVisiting Faculty",
      "link": "http://ai.googleblog.com/2018/03/investing-in-frances-ai-ecosystem.html",
      "author": "Posted by Olivier Bousquet, Principal Engineer, Google Z\u00fcrich"
    },
    {
      "title": "Using Machine Learning to Discover Neural Network Optimizers",
      "date": "Wednesday, March 28, 2018",
      "abstract": "Using Machine Learning to Discover Neural Network OptimizersDeep learningSearchTranslatePhotosstochastic gradient descentmore advanced optimizersnon-convexGoogle Brain teamAutoMLNeural Optimizer Search with Reinforcement LearningPowerSignAddSignImageNetTensorflowrecurrent neural networkconvolutional neural networkCIFAR10reinforcement learningPowerSignlinear cosine decaylinear cosine decaycosine decayConvNetPowerSignAddSignNeural Machine TranslationBLEUNeural Optimizer Searchopen sourcing these optimizers in Tensorflow",
      "link": "http://ai.googleblog.com/2018/03/using-machine-learning-to-discover.html",
      "author": "Posted by Irwan Bello, Research Associate, Google Brain Team"
    },
    {
      "title": "Expressive Speech Synthesis with Tacotron",
      "date": "Tuesday, March 27, 2018",
      "abstract": "Expressive Speech Synthesis with TacotronTacotronprosodyTowards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotronthe paperTowards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotronthis web pageStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesisthe paperStyle Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesisthis web page",
      "link": "http://ai.googleblog.com/2018/03/expressive-speech-synthesis-with.html",
      "author": "Posted by Yuxuan Wang, Research Scientist and RJ Skerry-Ryan, Software Engineer, on behalf of the Machine Perception, Google Brain and TTS Research teams"
    },
    {
      "title": "Reformulating Chemistry for More Efficient Quantum Computation",
      "date": "Thursday, March 22, 2018",
      "abstract": "Reformulating Chemistry for More Efficient Quantum ComputationAntikythera mechanismclassically intractableAntikythera mechanismBristlecone quantum processor Quantum AI teamfirst quantum chemistry experiment on a superconducting quantum computing devicePhysical Review Xexperimentally simulated exotic phases of matterreleasedOpenFermionBristlecone quantum processorQuantum Information ProcessingIBM ThinkQLow-Depth Quantum Simulation of MaterialsPhysical Review XProfessor Garnet Chan at CaltechQuArC group at Microsoftbasis setsHamiltonianplane wavescrystal of lithium hydridecathodesheterogeneous catalystsgraphenesuperconductorsQuantum Simulation of Electronic Structure with Linear Depth and ConnectivityPhysical Review LettersAspuru-Guzik group at Harvarderror-correctioneighty years",
      "link": "http://ai.googleblog.com/2018/03/reformulating-chemistry-for-more.html",
      "author": "Posted by Ryan Babbush, Senior Research Scientist, Quantum AI Team"
    },
    {
      "title": "Google Faculty Research Awards 2017",
      "date": "Tuesday, March 20, 2018",
      "abstract": "Google Faculty Research Awards 2017Google Faculty Research Awardshuman computer interactionmachine learningmachine perceptionsystemsrecipientsour websitehere",
      "link": "http://ai.googleblog.com/2018/03/google-faculty-research-awards-2017.html",
      "author": "Posted by Maggie Johnson, Vice President of Education and University Relations, Google"
    },
    {
      "title": "Using Deep Learning to Facilitate Scientific Image Analysis",
      "date": "Friday, March 16, 2018",
      "abstract": "Using Deep Learning to Facilitate Scientific Image Analysisrobotic microscopy applicationsdistinguish cellular phenotypesseparate signal from noiseAssessing Microscope Image Focus Quality with Deep LearningTensorFlowFiji (ImageJ)CellProfilerFiji (ImageJ)publicationTensorFlowFijiCellProfilerdatasets and modelsAnne CarpenterKevin Eliceiri",
      "link": "http://ai.googleblog.com/2018/03/using-deep-learning-to-facilitate.html",
      "author": "Posted by Samuel Yang, Research Scientist, Google Accelerated Science Team"
    },
    {
      "title": "Using Evolutionary AutoML to Discover Neural Network Architectures",
      "date": "Thursday, March 15, 2018",
      "abstract": "Using Evolutionary AutoML to Discover Neural Network Architectures500 million years agoartificial neural networksfind what's in a photographcall a genetic varianthelp diagnose a diseaseStanley and Miikkulainen 2002OpenAIUber LabsSentient LabsDeepMindGoogle Brain teamAutoMLreinforcement learningLarge-Scale Evolution of Image ClassifiersICML 2017Regularized Evolution for Image Classifier Architecture SearchTPUv2 chipsCIFAR-10ImageNetCIFAR-10Inception-ResNetZoph et al.Zoph et al.Regularized Evolution for Image Classifier Architecture SearchZoph et al.the paperGoogle Brain team",
      "link": "http://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html",
      "author": "Posted by Esteban Real, Senior Software Engineer, Google Brain Team"
    },
    {
      "title": "Balanced Partitioning and Hierarchical Clustering at Scale",
      "date": "Wednesday, March 14, 2018",
      "abstract": "Balanced Partitioning and Hierarchical Clustering at Scalegraph partitioningNP-hardbest approximation algorithmssemidefinite programmingwe developedWSDM 2016 paperNIPS 2017 paperaffinity clusteringBor\u016fvka\u2019s classic Maximum-cost Spanning Tree algorithmNIPS\u201917 paperDHTsThis videoFENNELSpinnerMETISlabel propagation-based algorithmTwitter followership graphUgander and Backstrom, 2013UB13SpinnerGoogle Mapstreatment groups for experimental designshardsHilbert embeddingNYC Algorithms and Optimization research team",
      "link": "http://ai.googleblog.com/2018/03/balanced-partitioning-and-hierarchical.html",
      "author": "Posted by Hossein Bateni, Research Scientist and Kevin Aydin, Software Engineer, NYC Algorithms and Optimization Research Team"
    },
    {
      "title": "Behind the Motion Photos Technology in Pixel 2",
      "date": "Tuesday, March 13, 2018",
      "abstract": "Behind the Motion Photos Technology in Pixel 2new camera featureMotion Stills for AndroidGoogle Photos albumMotion StillsFused Video StabilizationGoogle Photos albumaffine transformationhomographyscene at infinityparallaxmixture homographiesrolling shutterlinear programming techniquesearlier posts",
      "link": "http://ai.googleblog.com/2018/03/behind-motion-photos-technology-in.html",
      "author": "Posted by Matthias Grundmann, Research Scientist and Jianing Wei, Software Engineer, Google Research"
    },
    {
      "title": "Semantic Image Segmentation with DeepLab in TensorFlow",
      "date": "Monday, March 12, 2018",
      "abstract": "Semantic Image Segmentation with DeepLab in TensorFlowportrait mode of the Pixel 2 and Pixel 2 XL smartphonesmobile real-time video segmentationimage-level classificationbounding box-level detectionDeepLab-v3+*TensorFlowconvolutional neural networkPascal VOC 2012Cityscapesdepthwise separable convolutionEncoder-Decoder with Atrous Separable Convolution for Semantic Image SegmentationXception: Deep Learning with Depthwise Separable ConvolutionsDeformable Convolutional Networks \u2014 COCO Detection and Segmentation Challenge 2017 EntrySemantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFsDeeplab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFsRethinking Atrous Convolution for Semantic Image Segmentation*\u21a9",
      "link": "http://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html",
      "author": "Posted by Liang-Chieh Chen and Yukun Zhu, Software Engineers, Google Research"
    },
    {
      "title": "Introducing the iNaturalist 2018 Challenge",
      "date": "Friday, March 9, 2018",
      "abstract": "Introducing the iNaturalist 2018 Challengedeep learningself-driving carsvirtual realityinstance-level landmark recognition challengelong tail2018 iNaturalist ChallengeiNaturalistVisipediaFGVC5CVPR\u00a02018iNat-2017KaggleGitHub repothe following photolong-tailed distributionGrant Van Horn and Oisin Mac AodhaFGVC5furniture categorization challengeFGVCxFGVC5CVPR\u00a02018competitionsiNaturalistVisipediaFGVC5",
      "link": "http://ai.googleblog.com/2018/03/introducing-inaturalist-2018-challenge.html",
      "author": "Posted by Yang Song, Staff Software Engineer and Serge Belongie, Visiting Faculty, Google Research"
    },
    {
      "title": "Open Sourcing the Hunt for Exoplanets",
      "date": "Thursday, March 8, 2018",
      "abstract": "Open Sourcing the Hunt for ExoplanetsGoogle Open Source Blogtwo exoplanetsneural networkKepler space telescopehealthcarequantum chemistryfusion researchrelease our codeK2Transiting Exoplanet Survey SatelliteKepler\u2019s photometerbinary starstarspotscosmic rayKepler data processing pipelinesignal-to-noise ratiosGoogle Brain teamhuman genomessketchesformal mathematical logicAndrew Vanderburgconvolutional neural networkaudio generationimage classificationthis paperKepler-90 i and Kepler-80 gKepler-90 ihome pageBox Least SquaresVARTOOLSLcToolsPlanet HuntersKepler 90 starour paperour paperThis Week In Machine Learning & AI",
      "link": "http://ai.googleblog.com/2018/03/open-sourcing-hunt-for-exoplanets.html",
      "author": "Posted by Chris Shallue, Senior Software Engineer, Google Brain Team"
    },
    {
      "title": "The Building Blocks of Interpretability",
      "date": "Tuesday, March 6, 2018",
      "abstract": "The Building Blocks of InterpretabilityGoogle Open Source Blogpsychedelic imagesDeepDreamsmall art movementallsortsofDistillwhat individual neurons in a network doThe Building Blocks of InterpretabilityLucidcolab notebooks",
      "link": "http://ai.googleblog.com/2018/03/the-building-blocks-of-interpretability.html",
      "author": "Posted by Chris Olah, Research Scientist and Arvind Satyanarayan, Visiting Researcher, Google Brain Team"
    },
    {
      "title": "A Preview of Bristlecone, Google\u2019s New Quantum Processor",
      "date": "Monday, March 5, 2018",
      "abstract": "A Preview of Bristlecone, Google\u2019s New Quantum ProcessorGoogle Quantum AI labAmerican Physical Societyour qubit technologysimulationoptimizationmachine learning.12quantum supremacysurface codequantum supremacy",
      "link": "http://ai.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html",
      "author": "Posted by Julian Kelly, Research Scientist, Quantum AI Lab"
    },
    {
      "title": "Making Healthcare Data Work Better with Machine Learning",
      "date": "Friday, March 2, 2018",
      "abstract": "Making Healthcare Data Work Better with Machine Learningalmost completely digitizedFast Healthcare Interoperability Resourcesde-facto standardbulk-data accesslarge-scale machine learningopen sourceprotocol bufferpartneringBigQueryTensorFlowour recent worktoolsGrahame GrieveEwout KramerJosh MandelDeepMindGoogle Brain teamacademic collaborators",
      "link": "http://ai.googleblog.com/2018/03/making-healthcare-data-work-better-with.html",
      "author": "Posted by Patrik Sundberg, Software Engineer and Eyal Oren, Product Manager, Google Brain Team"
    },
    {
      "title": "Mobile Real-time Video Segmentation",
      "date": "Thursday, March 1, 2018",
      "abstract": "Mobile Real-time Video Segmentationrotoscopingchroma keyingstoriesconvolutional neural networksIntersection-Over-UnionRGBLSTMsGRUshourglass segmentation network architecturelarge stridesU-NetResNetedgesDenseNetneural matting",
      "link": "http://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html",
      "author": "Valentin Bazarevsky and Andrei Tkachenka, Software Engineers, Google Research"
    },
    {
      "title": "Google-Landmarks: A New Dataset and Challenge for Landmark Recognition",
      "date": "Thursday, March 1, 2018",
      "abstract": "Google-Landmarks: A New Dataset and Challenge for Landmark RecognitionImagenetdrop substantially every yearLandmark RecognitionLandmark RetrievalCVPR\u201918 Landmarks workshopopen-sourcingDELFartwork recognitionrecognitionretrievalBig BenSacre Coeur Basilicarock sculpture of DecebalusMegyeri BridgeCVPRCVPR\u201918 Landmarks workshop",
      "link": "http://ai.googleblog.com/2018/03/google-landmarks-new-dataset-and.html",
      "author": "Posted by Andr\u00e9 Araujo and Tobias Weyand, Software Engineers, Google Research"
    },
    {
      "title": "A Summary of the Google Z\u00fcrich Algorithms & Optimization Workshop",
      "date": "Friday, February 23, 2018",
      "abstract": "A Summary of the Google Z\u00fcrich Algorithms & Optimization Workshopworkshop on Algorithms and OptimizationacademiaGoogleNYC algorithms research teamhereVahab Mirroknimarket algorithms projectPaul Duettingrecent advancement in stochastic optimization for pricingRenato Paes LemeStefano LeonardiReservation Exchange MarketsRadu JurcaOlivier Bousquetagnostic learning of distributionAmin KarbasiAndreas Krausesubmodular optimizationlearning submodular modelsMartin JaggiNicol\u00f2 Cesa-Bianchinew results on banditsSilvio LattanziGraph Mining teamPiotr Sankowskiinteresting model to explain the size of cascades in real-world graphsThomas SauerwaldCoalescing Random WalksPeter Sandersalgorithm engineering for large datasetsrecent paperSergei VassilvitskiiElisa CelisFlorin Ciocanfair allocationGraham Cormodealgorithms for private release of marginal statisticsMorteza Zadimoghaddamdynamic consistent hashingRobert Krauthgamerrecent results on graph sketching and combinatorial optimizationSayan BhattacharyaDynamic Algorithms via Primal-Dual MethodPino Italianonew efficient algorithms for network analysishere",
      "link": "http://ai.googleblog.com/2018/02/a-summary-of-google-zurich-algorithms.html",
      "author": "Posted by Silvio Lattanzi, Research Scientist, Google Z\u00fcrich and Vahab Mirrokni, Research Scientist, Google New York"
    },
    {
      "title": "Assessing Cardiovascular Risk Factors with Computer Vision",
      "date": "Monday, February 19, 2018",
      "abstract": "Assessing Cardiovascular Risk Factors with Computer Visiondiabetic eye diseasePrediction of Cardiovascular Risk Factors from Retinal Fundus Photographs via Deep Learningattention techniquesDevelopment and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus PhotographsDevelopment and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With DiabetesDermatologist-level classification of skin cancer with deep neural networksDiagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer",
      "link": "http://ai.googleblog.com/2018/02/assessing-cardiovascular-risk-factors.html",
      "author": "Posted by Lily Peng MD PhD, Product Manager, Google Brain Team"
    },
    {
      "title": "Introducing the HDR+ Burst Photography Dataset",
      "date": "Monday, February 12, 2018",
      "abstract": "Introducing the HDR+ Burst Photography Datasetcomputational photographyPixelPixel 2portrait modearchive of image burstsresults of Google's HDR+ softwaredatasetfirst releasedSIGGRAPH Asia paperQualcomm Hexagon DSPPixel Visual Coreresearch paperMiddlebury archiveCreative Commons license (CC-BY-SAdetailed description",
      "link": "http://ai.googleblog.com/2018/02/introducing-hdr-burst-photography.html",
      "author": "Posted by Sam Hasinoff, Software Engineer, Machine Perception"
    },
    {
      "title": "The Instant Motion Tracking Behind Motion Stills AR",
      "date": "Tuesday, February 6, 2018",
      "abstract": "The Instant Motion Tracking Behind Motion Stills ARMotion Stills on Androidmotion photos feature in Pixel 2Motion Stills for AndroidMotion Textprivacy blur on YouTubeaccelerometerMotion Textroll, pitch and yawdownload the new release of Motion StillsLullabyPoly",
      "link": "http://ai.googleblog.com/2018/02/the-instant-motion-tracking-behind.html",
      "author": "Posted by Jianing Wei and Tyler Mullen, Software Engineers, Google Research"
    },
    {
      "title": "The Google Brain Team \u2014 Looking Back on 2017 (Part 2 of 2)",
      "date": "Friday, January 12, 2018",
      "abstract": "The Google Brain Team \u2014 Looking Back on 2017 (Part 2 of 2)Google Brain teamGoogle AIPart 1 of this blog postassisting pathologists in detecting cancerunderstanding medical conversationshighly accurate variant calling system based on deep learningour algorithm correctly identifies the tumorresearch paperJAMAVerilyNikon's line of Optos ophthalmology camerasAravind Eye HospitalsStanfordUCSFUniversity of Chicagomachine learning to predict medical outcomes from de-identified medical recordscomputer-based simulations of robotic tasks with physical robotic experiencedeveloped robotic learning algorithms that can learn by observing human demonstrationsConference on Robot Learningsummary of the eventpredicting molecular propertiesfinding new exoplanetsdeep learning to guide automated proof systemsA Message Passing Neural Network predicts quantum properties of an organic moleculeFinding a new exoplanetAI piano duet toolcreate new musicNat & Friendshow to teach machines to drawSketchRNN modelavailablehow to control deep generative models running in the browser to create new musicNIPS 2017 Best Demo AwardMagenta projectNIPS 2016 Best Demo AwardInteractive musical improvisation with MagentaMusicVAEPeople + AI Research (PAIR)public symposiumdeeplearn.jsFacetsFacetsPAIRhow to avoid discrimination in ML systemsgeodiversity in open datasetsan analysis of an open dataset to understand diversity and cultural differencesPartnership on AICultural differencesgeo-location biaseswe posted about our general approach to conducting researchhalfway throughjust after the endalready done some exciting researchpublished at numerous research venuesGoogle AI Residency programg.co/airesidency/applyICLRICMLNIPSresearch papersvideoAsk Me Anything (AMA)r/MachineLearning2016\u2019s AMAg.co/brain@GoogleResearch@JeffDean",
      "link": "http://ai.googleblog.com/2018/01/the-google-brain-team-looking-back-on_12.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"
    },
    {
      "title": "The Google Brain Team \u2014 Looking Back on 2017  (Part 1 of 2)",
      "date": "Thursday, January 11, 2018",
      "abstract": "The Google Brain Team \u2014 Looking Back on 2017  (Part 1 of 2)Google Brain teamGoogle AIsharedGoogleAlphabetmachine learningsecond postnew approaches for designing neural network architecturesstate-of-the-art results on ImageNet classification and detectionoptimization algorithmseffective activation functionsCloud AIConvolutional architecture discovered by Neural Architecture SearchObject detection with a network discovered by AutoMLdevelop a number of improvements for an end-to-end approach to speech recognition123456789Listen-Attend-Spell end-to-end modelMachine Perceptionlisten for yourselfTacotron 2\u2019s model architecturecapsulessparsely-gated mixtures of expertshypernetworksnew kinds of multi-modal modelsattention-based mechanismssymbolicnon-symbolicback-propagate through discrete variablesreinforcementlearningreinforcement learning to make placement decisions for mapping computational graphs onto a set of computational devicesThe Case for Learned Index StructuresMachine Learning for Systems and Systems for Machine LearningLearned Models as Index Structuresa paperICLR 2017adversarial examplesdemonstrating adversarial examples in the physical worldhow to harness adversarial examples at scale during the training processdeep learningone of the best paper awards\u201cflatness\u201d of minima found by optimization methods is not as closely linked to good generalization as initially thoughtrandommatricesrecent study comparing many GAN approachesmethods that allow better interpretability of machine learning systemsOpenAIDeepMindYC Researchlaunch of DistillDistillmanyilluminatingarticlesFeature VisualizationHow to Use t-SNE effectivelyMNISTCIFAR-10ImageNetSVHNWMTYouTube-8MYouTube-Bounding BoxesSpeech Commands DatasetAudioSetAtomic Visual Actions (AVA)Open ImagesOpen Images with Bounding BoxesYouTube-Bounding Boxes datasetsourceTensorFlowTensorFlow 1.0we released v1.4Eager executionXLATensorFlow Litepre-compiled TensorFlow binariessource code on GitHubTensorFlow Developer Summittalks were recordedSign up nowrock-paper-scissors science experimentautomating cucumber sortingfinding sea cows in aerial imagerysorting diced potatoes to make safer baby foodidentifying skin cancerhelping to interpret bird call recordings in a New Zealand bird sanctuaryidentifying diseased plants in the most popular root crop on Earth in Tanzaniaone of the top five repositoriesmany companies and organizationsmore than 24,500 distinct repositories on GitHubTF-GANTensorFlow LatticeTensorFlow Object Detection APImodel repositorydeeplearn.jsopen-source hardware-accelerated implementation of deep learning APIs right in the browserTeachable MachinePerformance RNNCloud TPUsGoogle SearchGoogle TranslateGoogle PhotosAlphaGoLee SedolKe JiepaperISCA 2017Cloud TPU PodsExperiments with ResNet-50 training on ImageNetsecond-generation TPUGoogle Cloud PlatformCloud TPUsTensorFlow Research Cloud (TFRC)presented workg.co/tpusignuppart 2",
      "link": "http://ai.googleblog.com/2018/01/the-google-brain-team-looking-back-on.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"
    },
    {
      "title": "Introducing the CVPR 2018 Learned Image Compression Challenge",
      "date": "Wednesday, January 10, 2018",
      "abstract": "Introducing the CVPR 2018 Learned Image Compression Challengecompression.ccJPEGBPGWebPend-to-end with machine learningcompression through superresolutionperceptually improved JPEG imagesETHTwitterWorkshop and Challenge on Learned Image Compression (CLIC)CVPR 2018Jim BankoskiJens OhmOren RippelRamin Zabihcompression.ccImageNetOpen Images DatasetToderici2016Ball\u00e92016Ball\u00e92017Theis2017Agustsson2017Santurkar2017Rippel2017]compression.cccompression.cc",
      "link": "http://ai.googleblog.com/2018/01/introducing-cvpr-2018-learned-image.html",
      "author": "Posted by Michele Covell, Research Scientist, Google Research"
    },
    {
      "title": "Evaluation of Speech for the Google Assistant",
      "date": "Thursday, December 21, 2017",
      "abstract": "Evaluation of Speech for the Google AssistantSearch Quality Rating Guidelinespublishing some of the first Google Assistant guidelinesweather this weekendexplicit linguistic knowledge and deep learning solutionsour previous workWaveNetTacotron 2here",
      "link": "http://ai.googleblog.com/2017/12/evaluation-of-speech-for-google.html",
      "author": "Posted by Enrique Alfonseca, Staff Research Scientist, Google Assistant"
    },
    {
      "title": "Tacotron 2: Generating Human-like Speech from Text",
      "date": "Tuesday, December 19, 2017",
      "abstract": "Tacotron 2: Generating Human-like Speech from TextTacotronWaveNetTacotron 2Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram PredictionsWaveNetthe paperTacotron 2 audio samplesdecorummerlot",
      "link": "http://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html",
      "author": "Posted by Jonathan Shen and Ruoming Pang, Software Engineers, on behalf of the Google Brain and Machine Perception Teams"
    },
    {
      "title": "Introducing NIMA: Neural Image Assessment",
      "date": "Monday, December 18, 2017",
      "abstract": "Introducing NIMA: Neural Image Assessmentconvolutional neural networksaddress the subjective nature of image qualityNIMA: Neural Image Assessmentobject recognitionPSNRSSIMImageNetpaperAVAphotography contestsAVATID2013TID2013paperMIT-Adobe FiveK dataset",
      "link": "http://ai.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html",
      "author": "Posted by Hossein Talebi, Software Engineer and Peyman Milanfar Research Scientist, Machine Perception"
    },
    {
      "title": "Improving End-to-End Models For Speech Recognition",
      "date": "Thursday, December 14, 2017",
      "abstract": "Improving End-to-End Models For Speech RecognitionphonemesState-of-the-art Speech Recognition With Sequence-to-Sequence Modelsword error rategraphemewordpieceLower Frame Rate Neural Network Acoustic ModelsListen, attend and spellA Comparison of Sequence-to-sequence Models for Speech RecognitionState-of-the-art Speech Recognition With Sequence-to-Sequence ModelsMinimum Word Error Rate Training for Attention-based Sequence-to-Sequence ModelsMulti-Dialect Speech Recognition With a Single Sequence-to-Sequence ModelEnd-to-End Multilingual Speech Recognition using Encoder-Decoder ModelsImproving the Performance of Online Neural Transducer ModelsMonotonic Chunkwise AttentionLearning Hard Alignments with Variational InferenceNo Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End ModelsAn Analysis of Incorporating an External Language Model into a Sequence-to-Sequence Model",
      "link": "http://ai.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html",
      "author": "Posted by Tara N. Sainath, Research Scientist, Speech Team and Yonghui Wu, Software Engineer, Google Brain Team"
    },
    {
      "title": "A Summary of the First Conference on Robot Learning",
      "date": "Wednesday, December 13, 2017",
      "abstract": "A Summary of the First Conference on Robot LearningConference on Robot Learning (CoRL)74 original papers5 keynotesconference websiteProfessor Aude Billard\u00c9cole Polytechnique F\u00e9d\u00e9rale de LausanneEidgen\u00f6ssische Technische Hochschule",
      "link": "http://ai.googleblog.com/2017/12/a-summary-of-first-conference-on-robot.html",
      "author": "Posted by Vincent Vanhoucke, Principal Scientist, Google Brain Team and Melanie Salda\u00f1a, Program Manager, University Relations"
    },
    {
      "title": "TFGAN: A Lightweight Library for Generative Adversarial Networks",
      "date": "Tuesday, December 12, 2017",
      "abstract": "TFGAN: A Lightweight Library for Generative Adversarial NetworksGoogle Open Source Blogimage compressiontext-to-speech systemsGenerative Adversarial Networksgenerating images from textsuperresolutionhelping robots learn to graspTFGANexamplestutorialimage compressionImageNet datasetTacotron",
      "link": "http://ai.googleblog.com/2017/12/tfgan-lightweight-library-for.html",
      "author": "Posted by Joel Shor, Senior Software Engineer, Machine Perception"
    },
    {
      "title": "Introducing Appsperiments: Exploring the Potentials of Mobile Photography",
      "date": "Monday, December 11, 2017",
      "abstract": "Introducing Appsperiments: Exploring the Potentials of Mobile PhotographyMotion Stillsobject recognitionperson segmentationAndroidiOSAndroidiOS",
      "link": "http://ai.googleblog.com/2017/12/introducing-appsperiments-exploring.html",
      "author": "Posted by Alex Kauffmann, Interaction Researcher, Google Research"
    },
    {
      "title": "Introducing a New Foveation Pipeline for Virtual/Mixed Reality",
      "date": "Tuesday, December 5, 2017",
      "abstract": "Introducing a New Foveation Pipeline for Virtual/Mixed RealityVirtual RealityMixed Realityresearchfovea centralissaccades,frustumsrasterizedsystem-on-a-chip",
      "link": "http://ai.googleblog.com/2017/12/introducing-new-foveation-pipeline-for.html",
      "author": "Posted by Behnam Bastani, Software Engineer Manager and Eric Turner, Software Engineer, Daydream"
    },
    {
      "title": "DeepVariant: Highly Accurate Genomes With Deep Neural Networks",
      "date": "Monday, December 4, 2017",
      "abstract": "DeepVariant: Highly Accurate Genomes With Deep Neural NetworksGoogle Open Source BlogSanger sequencingmicroarray technologieshealthagricultureecologyGenome in a Bottle ConsortiumprecisionFDAopen source release of DeepVariantGoogle Brain teamVerily Life Sciencesexisting technology and expertiseSNPsinsertions/deletionsDeepVariantSNPprecisionFDA Truth ChallengeGoogle Cloud PlatformDeepVariant workflowsPipelines APIhealthcarescientific applications",
      "link": "http://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html",
      "author": "Posted by Mark DePristo and Ryan Poplin, Google Brain Team"
    },
    {
      "title": "Google at NIPS 2017",
      "date": "Sunday, December 3, 2017",
      "abstract": "Google at NIPS 2017Conference on Neural Information Processing SystemsPowering the next 100 yearsA Meta-Learning Perspective on Cold-Start Recommendations for ItemsAdaGAN: Boosting Generative ModelsDeep Lattice Networks and Partial Monotonic FunctionsFrom which world is your graphHiding Images in Plain Sight: Deep SteganographyImproved Graph Laplacian via Geometric Self-ConsistencyModel-Powered Conditional Independence TestNonlinear random matrix theory for deep learningResurrecting the sigmoid in deep learning through dynamical isometry: theory and practiceSGD Learns the Conjugate Kernel Class of the NetworkSVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and InterpretabilityLearning Hierarchical Information Flow with Recurrent Neural ModulesOnline Learning with Transductive RegretAcceleration and Averaging in Stochastic Descent DynamicsParameter-Free Online Learning via Model SelectionDynamic Routing Between CapsulesModulating early visual processing by languageMarrNet: 3D Shape Reconstruction via 2.5D SketchesAffinity Clustering: Hierarchical Clustering at ScaleAsynchronous Parallel Coordinate Minimization for MAP InferenceCold-Start Reinforcement Learning with Softmax Policy GradientFiltering Variational ObjectivesMulti-Armed Bandits with Metric Movement CostsMultiscale Quantization for Fast Similarity SearchReducing Reparameterization Gradient VarianceStatistical Cost SharingThe Unreasonable Effectiveness of Structured Random Orthogonal EmbeddingsValue Prediction NetworkREBAR: Low-variance, unbiased gradient estimates for discrete latent variable modelsApproximation and Convergence Properties of Generative Adversarial LearningAttention is All you NeedPASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inferenceRepeated Inverse Reinforcement LearningFair Clustering Through FairletsAffine-Invariant Online Optimization and the Low-rank Experts ProblemBatch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized ModelsBridging the Gap Between Value and Policy Based Reinforcement LearningDiscriminative State Space ModelsDynamic Revenue SharingMulti-view Matrix Factorization for Linear Dynamical System EstimationOn Blackbox Backpropagation and Jacobian SensingOn the Consistency of Quick ShiftRevenue Optimization with Approximate Bid PredictionsShape and Material from SoundLearning to See Physics via Visual De-animationElectronic Screen Protector with Efficient and Robust Mobile VisionMagenta and deeplearn.js: Real-time Control of DeepGenerative Music Models in the Browser6th Workshop on Automated Knowledge Base ConstructionActing and Interacting in the Real World: Challenges in Robot LearningAdvances in Approximate Bayesian InferenceConversational AI - Today's Practice and Tomorrow's PotentialExtreme Classification: Multi-class and Multi-label Learning in Extremely Large Label SpacesLearning in the Presence of Strategic BehaviorLearning on Distributions, Functions, Graphs and GroupsMachine DeceptionMachine Learning and Computer SecurityMachine Learning for Creativity and DesignMachine Learning for Audio Signal Processing (ML4Audio)Machine Learning for Health (ML4H)NIPS Time Series Workshop 2017OPT 2017: Optimization for Machine LearningML Systems WorkshopAligned Artificial IntelligenceBayesian Deep LearningBigNeuro 2017Cognitively Informed Artificial Intelligence: Insights From Natural IntelligenceDeep Learning At Supercomputer ScaleDeep Learning: Bridging Theory and PracticeInterpreting, Explaining and Visualizing Deep LearningLearning Disentangled Features: from Perception to ControlLearning with Limited Labeled Data: Weak Supervision and BeyondMachine Learning on the Phone and other Consumer DevicesOptimal Transport and Machine LearningThe future of gradient-based machine learning software & techniquesWorkshop on Meta-LearningDeep Reinforcement Learning SymposiumInterpretable Machine LearningMetalearningAdversarial Attacks and DefencesCompetition IV: Classifying Clinically Actionable Genetic MutationsFairness in Machine Learning",
      "link": "http://ai.googleblog.com/2017/12/google-at-nips-2017.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Research Communications"
    },
    {
      "title": "Understanding Bias in Peer Review",
      "date": "Thursday, November 30, 2017",
      "abstract": "Understanding Bias in Peer ReviewMatthew effectMatilda effectstudyReviewer bias in single- versus double-blind peer review10th ACM Web Search and Data Mining Conferenceextended version of our paperfull paper",
      "link": "http://ai.googleblog.com/2017/11/understanding-bias-in-peer-review.html",
      "author": "Posted by Andrew Tomkins, Director of Engineering and William D. Heavlin, Statistician, Google Research"
    },
    {
      "title": "Interpreting Deep Neural Networks with SVCCA",
      "date": "Tuesday, November 28, 2017",
      "abstract": "Interpreting Deep Neural Networks with SVCCAvisionlanguage understandingspeech recognitionadversarial examplescatastrophic forgettingreinforcement learningmode collapsemodellingSVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretabilitythe codeCIFAR-10Discrete Fourier transformImagenet ResnetpaperNIPS 2017code",
      "link": "http://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html",
      "author": "Posted by Maithra Raghu, Google Brain Team"
    },
    {
      "title": "Understanding Medical Conversations",
      "date": "Tuesday, November 21, 2017",
      "abstract": "Understanding Medical ConversationsElectronic Health Records123Speech Recognition for Medical ConversationsAutomatic Speech Recognitionresearchreturn joy to practice1http://www.annfammed.org/content/15/5/419.full\u21a92http://www.mayoclinicproceedings.org/article/S0025-6196%2815%2900716-8/abstract\u21a93http://www.annfammed.org/content/15/5/427.full\u21a9",
      "link": "http://ai.googleblog.com/2017/11/understanding-medical-conversations.html",
      "author": "Posted by Katherine Chou, Product Manager and Chung-Cheng Chiu, Software Engineer, Google Brain Team"
    },
    {
      "title": "SLING: A Natural Language Frame Semantic Parser",
      "date": "Wednesday, November 15, 2017",
      "abstract": "SLING: A Natural Language Frame Semantic Parsernatural language understandingdependency parsingcoreference resolutionSLINGrecurrent neural networktechnical report1TensorFlowDRAGNNrelease1\u21a9",
      "link": "http://ai.googleblog.com/2017/11/sling-natural-language-frame-semantic.html",
      "author": "Posted by Michael Ringgaard, Software Engineer and Rahul Gupta, Research Scientist"
    },
    {
      "title": "On-Device Conversational Modeling with TensorFlow Lite",
      "date": "Tuesday, November 14, 2017",
      "abstract": "On-Device Conversational Modeling with TensorFlow LiteAndroid Wear 2.0\"on-device\" machine learningGmailInboxAlloTensorFlow Liteon-device conversational modeldemo appProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projectionsquantizationdistillationmodelcodedemo apphere1graphheregraph learning frameworksemi-supervised1\u21a9",
      "link": "http://ai.googleblog.com/2017/11/on-device-conversational-modeling-with.html",
      "author": "Posted by Sujith Ravi, Research Scientist, Google Expander Team"
    },
    {
      "title": "Fused Video Stabilization on the Pixel 2 and Pixel 2 XL",
      "date": "Friday, November 10, 2017",
      "abstract": "Fused Video Stabilization on the Pixel 2 and Pixel 2 XLhighest overall rating for a smartphone cameraCMOSrolling shutter distortionangle of viewbreathingOptical Image StabilizationElectronic Image Stabilization",
      "link": "http://ai.googleblog.com/2017/11/fused-video-stabilization-on-pixel-2.html",
      "author": "Posted by Chia-Kai Liang, Senior Staff Software Engineer and Fuhao Shi, Android Camera Team"
    },
    {
      "title": "Seamless Google Street View Panoramas",
      "date": "Thursday, November 9, 2017",
      "abstract": "Seamless Google Street View PanoramasGoogle Street Viewparallaxoptical flowPhotoScan blog postsplineCeres Solverpreviously published work",
      "link": "http://ai.googleblog.com/2017/11/seamless-google-street-view-panoramas.html",
      "author": "Posted by Mike Krainin, Software Engineer and Ce Liu, Research Scientist, Machine Perception"
    },
    {
      "title": "Feature Visualization",
      "date": "Tuesday, November 7, 2017",
      "abstract": "Feature Visualizationnew articleDistillDeepDreamevery neuron1Distill",
      "link": "http://ai.googleblog.com/2017/11/feature-visualization.html",
      "author": "Posted by Christopher Olah, Research Scientist, Google Brain Team and Alex Mordvintsev, Research Scientist, Google Research"
    },
    {
      "title": "Tangent: Source-to-Source Debuggable Derivatives",
      "date": "Monday, November 6, 2017",
      "abstract": "Tangent: Source-to-Source Debuggable DerivativesGoogle Open Source BlogEasily debug your backward passFast gradient surgeryForward mode automatic differentiationEfficient Hessian-vector productsCode optimizationsNeural networksreverse-mode automatic differentiationTF EagerPyTorch AutogradTensorFlowTensorFlow Eager functionsreverse-order processingUsing TensorFlow Eager functionsSubroutinesControl flowgithub.com/google/tangentreportGriewank and Walther 2000Gruslys et al., 2016",
      "link": "http://ai.googleblog.com/2017/11/tangent-source-to-source-debuggable.html",
      "author": "Posted by Alex Wiltschko, Research Scientist, Google Brain Team"
    },
    {
      "title": "AutoML for large scale image classification and object detection",
      "date": "Thursday, November 2, 2017",
      "abstract": "AutoML for large scale image classification and object detectionAutoMLImageNetLearning Transferable Architectures for Scalable Image RecognitionCOCOSlimObject DetectionLearning Transferable Architectures for Scalable Image RecognitionGoing Deeper with ConvolutionsRethinking the inception architecture for computer visionInception-v4, Inception-ResNet and the Impact of Residual Connections on LearningSqueeze-and-Excitation NetworksFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
      "link": "http://ai.googleblog.com/2017/11/automl-for-large-scale-image.html",
      "author": "Posted by Barret Zoph, Vijay Vasudevan, Jonathon Shlens and Quoc Le, Research Scientists, Google Brain Team"
    },
    {
      "title": "Latest Innovations in TensorFlow Serving",
      "date": "Thursday, November 2, 2017",
      "abstract": "Latest Innovations in TensorFlow ServingTensorFlow ServingFebruary 2016multi-model batch interleavingSavedModelclassificationregressionpredictSRETFXUC Berkeley RISE LabClipperCloud ML PredictionreleasesBatch/Unbatchgithub.com/tensorflow/serving",
      "link": "http://ai.googleblog.com/2017/11/latest-innovations-in-tensorflow-serving.html",
      "author": "Posted by Chris Olston, Research Scientist, and Noah Fiedel, Software Engineer, TensorFlow Serving"
    },
    {
      "title": "Eager Execution: An imperative, define-by-run interface to TensorFlow",
      "date": "Tuesday, October 31, 2017",
      "abstract": "Eager Execution: An imperative, define-by-run interface to TensorFlowofficial user guideCollatz conjectureautogradMNIST examplethis blog postthe documentation pagenightlyREADMEUser Guideexamples in GitHubchangelogFeedback",
      "link": "http://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html",
      "author": "Posted by Asim Shankar and Wolff Dobson, Google Brain Team"
    },
    {
      "title": "Closing the Simulation-to-Reality Gap for Deep Robotic Learning",
      "date": "Monday, October 30, 2017",
      "abstract": "Closing the Simulation-to-Reality Gap for Deep Robotic LearningShrivastava et al.Bousmalis et al.Ganin et al.Bousmalis and Trigeorgis et al.Tzeng et al.Sadeghi and LevineTobin et al.James et al.Christiano et al.PyBulletpostGoogle Brain teamX\u2019s roboticsShapeNetGoodfellow et alarXivhere",
      "link": "http://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html",
      "author": "Posted by Konstantinos Bousmalis,\u00a0Senior Research Scientist, and Sergey Levine, Faculty Advisor, Google Brain Team"
    },
    {
      "title": "Announcing OpenFermion: The Open Source Chemistry Package for Quantum Computers",
      "date": "Monday, October 23, 2017",
      "abstract": "Announcing OpenFermion: The Open Source Chemistry Package for Quantum ComputersPaul Dirac, Quantum Mechanics of Many-Electron SystemsRichard Feynmanquantum computingthis papercover of NatureOpenFermionOpenFermion: The Electronic Structure Package for Quantum Computers1classical quantum chemistry packagesOpenFermion-Psi4OpenFermion-PySCFPsi4PySCFOpenFermion-ProjectQForest-OpenFermionProjectQForestpast blog postbasis setspin multiplicityOpenFermion-Psi4OpenFermion-PySCFHartree-FockBravyi-Kitaev transformationOpenFermion-ProjectQhereherehereGitHub repository\n1 If we may be allowed one sentence for the experts: the primary function of OpenFermion is to encode the electronic structure problem in second quantization defined by various basis sets and active spaces and then to transform those operators into spin Hamiltonians using various isomorphisms between qubit and fermion algebras.\u21a91electronic structuresecond quantizationHamiltoniansqubitfermion\u21a9",
      "link": "http://ai.googleblog.com/2017/10/announcing-openfermion-open-source.html",
      "author": "Posted by Ryan Babbush and Jarrod McClean, Quantum Software Engineers, Quantum AI Team"
    },
    {
      "title": "Announcing AVA: A Finely Labeled Video Dataset for Human Action Understanding",
      "date": "Thursday, October 19, 2017",
      "abstract": "Announcing AVA: A Finely Labeled Video Dataset for Human Action Understandingclassifyingfinding objectsUCF101ActivityNetKineticswebsitearXiv paperVideo SourceJHMDB datasetmAPmailing list",
      "link": "http://ai.googleblog.com/2017/10/announcing-ava-finely-labeled-video.html",
      "author": "Posted by Chunhui Gu & David Ross, Software Engineers"
    },
    {
      "title": "Portrait mode on the Pixel 2 and Pixel 2 XL smartphones",
      "date": "Tuesday, October 17, 2017",
      "abstract": "Portrait mode on the Pixel 2 and Pixel 2 XL smartphonesmobile camera rankingwithoutwithdepth of fieldbokehtriangulationstereo algorithmsemanticsegmentationsynthetic shallow depth of fieldHDR+high dynamic rangewithoutwithchroma keyingTensorFlowconvolutional neural networkTensorFlow Mobileleftrightimagedual-pixel autofocusMarkus KohlpaintnerBayer color filterupperloweranimated gif JumpJump Assemblerbilateral solverLens Bluranother blog postLeftRightdisk-shaped bokehimagean albuman albumwithoutwithwithoutwithNat & Friendsalbum",
      "link": "http://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html",
      "author": "Posted by Marc Levoy, Principal Engineer and Yael Pritch, Software Engineer"
    },
    {
      "title": "TensorFlow Lattice: Flexibility Empowered by Prior Knowledge",
      "date": "Wednesday, October 11, 2017",
      "abstract": "TensorFlow Lattice: Flexibility Empowered by Prior KnowledgeGoogle Open Source BlogDNNsrandom forestsTensorFlow LatticeTensorFlow EstimatorsTensorFlowmonotonic relationshipsTensorFlow Estimatorsnonlinear interactionsL1L2GitHub repositorytutorialsLattice RegressionOptimized Regression for Efficient Function EvaluationMonotonic Calibrated Interpolated Look-Up TablesFast and Flexible Monotonic Functions with Ensembles of LatticesDeep Lattice Networks and Partial Monotonic Functions",
      "link": "http://ai.googleblog.com/2017/10/tensorflow-lattice-flexibility.html",
      "author": "Posted by Maya Gupta, Research Scientist, Jan Pfeifer, Software Engineer and Seungil You, Software Engineer"
    },
    {
      "title": "The Google Brain Team\u2019s Approach to Research",
      "date": "Wednesday, September 13, 2017",
      "abstract": "The Google Brain Team\u2019s Approach to ResearchGoogle Brain teamTranslateMapsblog postpaperhundreds of papersNIPSICMLICLRDistill.publaunched byTensorFlow PlaygroundPAIRdeeplearn.jsGoogle Brain Residency Programits inaugural yearBrain team membersGoogle Research",
      "link": "http://ai.googleblog.com/2017/09/the-google-brain-teams-approach-to.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow"
    },
    {
      "title": "Highlights from the Annual Google PhD Fellowship Summit, and Announcing the 2017 Google PhD Fellows",
      "date": "Tuesday, September 12, 2017",
      "abstract": "Highlights from the Annual Google PhD Fellowship Summit, and Announcing the 2017 Google PhD FellowsPhD Fellowship ProgramAustraliaChina and East AsiaIndiaNorth America, Europe and the Middle EastMaggie JohnsonEd ChiDouglas Eck\u00dalfar ErlingssonDina PapagiannakiViren JainIan GoodfellowKevin MurphyGalen AndrewDomagoj BabicKathryn McKinleyNina TaftRoy WantSunny Colsalvo",
      "link": "http://ai.googleblog.com/2017/09/highlights-from-annual-google-phd.html",
      "author": "Posted by Susie Kim, Program Manager, University Relations"
    },
    {
      "title": "Build your own Machine Learning Visualizations with the new TensorBoard API",
      "date": "Monday, September 11, 2017",
      "abstract": "Build your own Machine Learning Visualizations with the new TensorBoard APIwe open-sourced TensorFlow in 2015TensorBoardexploring clusters in high-dimensional spacesset of APIstensorboard/plugins directory on GitHubprecision-recall curvesGitHubGitHubGitHubprovides a demoTensorBoard \u201cGreeter\u201d pluginexisting pluginsBeholder,Chris Andersonhereissue tracker",
      "link": "http://ai.googleblog.com/2017/09/build-your-own-machine-learning.html",
      "author": "Posted by Chi Zeng and Justine Tunney, Software Engineers, Google Brain Team"
    },
    {
      "title": "Seminal Ideas from 2007",
      "date": "Wednesday, September 6, 2017",
      "abstract": "Seminal Ideas from 2007ICML Test-of-Time AwardSylvain GellyGoogle Brain teamZurich officeDavid SilverDeepMindAlphaGoCombining Online and Offline Knowledge in UCTGame of GoDeep BlueAlphaGoinnovations and technological advances123tree searchMonte-Carlo Tree Searchsearch tree*MoGoreinforcement learning*machine learning\u21a9",
      "link": "http://ai.googleblog.com/2017/09/seminal-ideas-from-2007.html",
      "author": "Posted by Anna Ukhanova, Technical Program Manager, Google Research Europe"
    },
    {
      "title": "Transformer: A Novel Neural Network Architecture for Language Understanding",
      "date": "Thursday, August 31, 2017",
      "abstract": "Transformer: A Novel Neural Network Architecture for Language Understandingrecurrent neural networkslanguage modelingmachinetranslationquestionansweringAttention Is All You Needhas shownTPUsByteNetConvS2STensor2Tensor libraryjust a few commands",
      "link": "http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html",
      "author": "Posted by Jakob Uszkoreit, Software Engineer, Natural Language Understanding"
    },
    {
      "title": "Exploring and Visualizing an Open Global Dataset",
      "date": "Friday, August 25, 2017",
      "abstract": "Exploring and Visualizing an Open Global DatasetQuick, Draw!group of GooglersFacetsPAIR",
      "link": "http://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html",
      "author": "Posted by Reena Jana, Creative Lead, Business Inclusion, and Josh Lovejoy, UX Designer, Google Research"
    },
    {
      "title": "Launching the Speech Commands Dataset",
      "date": "Thursday, August 24, 2017",
      "abstract": "Launching the Speech Commands DatasetKaldiTensorFlowAIYSpeech Commands Datasettraining*inferencecontributed by members of the public through the AIY websiteCreative Commons BY 4.0 licenseopen sourced tooprebuilt set of the TensorFlow Android demo applicationsnew audio recognition tutorial on TensorFlow.orglatest development version of the framework*Convolutional Neural Networks for Small-footprint Keyword SpottingInterspeech 2015\u21a9",
      "link": "http://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html",
      "author": "Posted by Pete Warden, Software Engineer, Google Brain Team"
    },
    {
      "title": "Google at KDD\u201917: Graph Mining and Beyond",
      "date": "Wednesday, August 23, 2017",
      "abstract": "Google at KDD\u201917: Graph Mining and Beyond23rd ACM conference on Knowledge Discovery and Data MiningthesisSteven Skienaclassificationclusteringanomaly detectionDeepWalk: Online Learning of Social Representationsword2vecLearning Edge Representations via Low-Rank Asymmetric ProjectionsLocal Modeling of Attributed Graphs: Algorithms and ApplicationsUser Behavior Modeling with Large-Scale Graph AnalysisEgo-Splitting Framework: from Non-Overlapping to Overlapping ClustersHyperLogLog Hyperextended: Sketches for Concave Sublinear Frequency StatisticsGoogle Vizier: A Service for Black-Box OptimizationQuick Access: Building a Smart Experience for Google DriveTFX: A TensorFlow\u00ad Based Production \u00adScale Machine Learning PlatformConstruction of Directed 2K GraphsA Practical Algorithm for Solving the Incoherence Problem of Topic Models In Industrial ApplicationsTrain and Distribute: Managing Simplicity vs. Flexibility in High-\u00adLevel Machine Learning FrameworksLearning to Count Mosquitoes for the Sterile Insect Technique13th International Workshop on Mining and Learning with GraphsHARP: Hierarchical Representation Learning for NetworksFairness, Accountability, and Transparency in Machine LearningFair Clustering Through FairletsData Decisions and Theoretical Implications when Adversarially Learning Fair RepresentationsTensorFlow",
      "link": "http://ai.googleblog.com/2017/08/google-at-kdd17-graph-mining-and-beyond.html",
      "author": "Posted by Bryan Perozzi, Research Scientist,  NYC Algorithms and Optimization Team"
    },
    {
      "title": "Announcing the NYC Algorithms and Optimization Site",
      "date": "Monday, August 21, 2017",
      "abstract": "Announcing the NYC Algorithms and Optimization Siteoptimizing infrastructureprotecting privacyimproving friend suggestionsNYC Algorithms and Optimization Team pageLarge-scale Graph Mining Groupego-networkscomputing similarity rankings in large-scale multi-categorical bipartite graphsour paperlocal clusteringconnected componentsresearchLarge-scale Optimization Groupdesigned memoryless balanced allocation algorithmsGoogle Cloud Pub/SubexternallyhaproxyComposable core-setsdistributed balanced clusteringdistributed submodular maximizationwhole-page optimizationauctions with intermediariesoptimal pricing strategiesoptimal yield managementonline stochastic matchingbudgeted allocationhandling traffic spikessubmodular welfare maximizationone paperanother paperpositive carryover effectsbudget optimization in search-based auctionsconcise bid optimization strategies with multiple budget constraintsdynamic mechanismsbank account mechanismsnon-clairvoyant dynamic mechanismsrecent market algorithms workshopGoogle NYC Algorithms and Optimization Team pagepublicationsseminars",
      "link": "http://ai.googleblog.com/2017/08/announcing-nyc-algorithms-and.html",
      "author": "Posted by Vahab Mirrokni, Principal Research Scientist and Xerxes Dotiwalla, Product Manager, NYC Algorithms and Optimization Team"
    },
    {
      "title": "Making Visible Watermarks More Effective",
      "date": "Thursday, August 17, 2017",
      "abstract": "Making Visible Watermarks More Effectivestock photographyOn The Effectiveness Of Visible Watermarks2017 Computer Vision and Pattern Recognition Conferenceproject pageAdobe Stock123RFvulnerabilitiesseveral minutes to remove a watermarkCOCO datasetCopyright logo123RFalpha mattesingle image mattingproject pageAdobe StockCan Stock Photo123RFFotoliaour paperproject page",
      "link": "http://ai.googleblog.com/2017/08/making-visible-watermarks-more-effective.html",
      "author": "Posted by Tali Dekel and Michael Rubinstein, Research Scientists"
    },
    {
      "title": "Harness the Power of Machine Learning in Your Browser with Deeplearn.js",
      "date": "Friday, August 11, 2017",
      "abstract": "Harness the Power of Machine Learning in Your Browser with Deeplearn.jsobject recognitionlanguage translationhealthPAIRdeeplearn.js 0.1.0Andrej Karpathy's convnetjsTensorFireTensorFlowNumPydeeplearn.js homepage",
      "link": "http://ai.googleblog.com/2017/08/harness-power-of-machine-learning-in.html",
      "author": "Posted by Nikhil Thorat and Daniel Smilkov, Software Engineers, Google Big Picture Team"
    },
    {
      "title": "Google at ICML 2017",
      "date": "Sunday, August 6, 2017",
      "abstract": "Google at ICML 2017International Conference on Machine LearningInternational Machine Learning SocietyFacetsNsynthGoogle Brain Residency programRobust Adversarial Reinforcement LearningTight Bounds for Approximate Carath\u00e9odory and BeyondSharp Minima Can Generalize For Deep NetsGeometry of Neural Network Loss Surfaces via Random Matrix TheoryConditional Image Synthesis with Auxiliary Classifier GANsLearning Deep Latent Gaussian Models with Markov Chain Monte CarloOn the Expressive Power of Deep Neural NetworksAdaNet: Adaptive Structural Learning of Artificial Neural NetworksLearned Optimizers that Scale and GeneralizeAdaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIPAlgorithms for \u2113p Low-Rank ApproximationConsistent k-ClusteringInput Switched Affine Networks: An RNN Architecture Designed for InterpretabilityOnline and Linear-Time Attention by Enforcing Monotonic AlignmentsGradient Boosted Decision Trees for High Dimensional Sparse OutputSequence Tutor: Conservative fine-tuning of sequence generation models with KL-controlUniform Convergence Rates for Kernel Density EstimationDensity Level Set Estimation on Manifolds with DBSCANMaximum Selection and Ranking under Noisy ComparisonsNeural Audio Synthesis of Musical Notes with WaveNet AutoencodersDistributed Mean Estimation with Limited CommunicationLearning to Generate Long-term Future via Hierarchical PredictionVariational Boosting: Iteratively Refining Posterior ApproximationsRobustFill: Neural Program Learning under Noisy I/OA Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete DistributionsAxiomatic Attribution for Deep NetworksDifferentiable Programs with Neural LibrariesLatent LSTM Allocation: Joint Clustering and Non-Linear Dynamic Modeling of Sequence DataDevice Placement Optimization with Reinforcement LearningCanopy \u2014 Fast Sampling with Cover TreesZero-Shot Task Generalization with Multi-Task Deep Reinforcement LearningProbabilistic Submodular Maximization in Sub-Linear TimeDeep Value Networks Learn to Evaluate and Iteratively Refine Structured OutputsStochastic Generative HashingAccelerating Eulerian Fluid Simulation With Convolutional NetworksLarge-Scale Evolution of Image ClassifiersNeural Message Passing for Quantum ChemistryNeural Optimizer Search with Reinforcement LearningImplicit Generative ModelsLearning to Generate Natural LanguageGenerating High-Quality and Informative Conversation Responses with Sequence-to-Sequence ModelsLifelong Learning: A Reinforcement Learning ApproachBridging the Gap Between Value and Policy Based Reinforcement LearningPrincipled Approaches to Deep LearningWorkshop on Human Interpretability in Machine Learning (WHI)ICML Workshop on TinyML: ML on a Test-time Budget for IoT, Mobiles, and Other ApplicationsDeep Structured PredictionFiltering Variational ObjectivesREBAR: Low-variance, unbiased gradient estimates for discrete latent variable modelsMachine Learning in Speech and Language ProcessingPicky Learners: Choosing Alternative Ways to Process DataPrivate and Secure Machine LearningReproducibility in Machine Learning ResearchTime Series WorkshopInterpretable Machine Learning",
      "link": "http://ai.googleblog.com/2017/08/google-at-icml-2017.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Research Communications"
    },
    {
      "title": "Google at ACL 2017",
      "date": "Sunday, July 30, 2017",
      "abstract": "Google at ACL 20172017 Annual Meeting of the Association for Computational Linguisticsnatural language understandingA Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion LimitCross-Sentence N-ary Relation Extraction with Graph LSTMsNeural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak SupervisionCoarse-to-Fine Question Answering for Long DocumentsAutomatic Compositor Attribution in the First Folio of ShakespeareA Nested Attention Neural Hybrid Model for Grammatical Error CorrectionGet To The Point: Summarization with Pointer-Generator NetworksIdentifying 1950s American Jazz Composers: Fine-Grained IsA Extraction via Modifier Composition*Learning to Skim Text2017 ACL Student Research WorkshopWiNLP: Women and Underrepresented Minorities in Natural Language ProcessingBUCC: 10th Workshop on Building and Using Comparable CorporaCLPsych: Computational Linguistics and Clinical Psychology \u2013 From Linguistic Signal to ClinicalRealityRepl4NLP: 2nd Workshop on Representation Learning for NLPRoboNLP: Language Grounding for RoboticsCoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal DependenciesCoNLL-SIGMORPHON-2017 Shared Task: Universal Morphological ReinflectionSemEval: 11th International Workshop on Semantic EvaluationALW1: 1st Workshop on Abusive Language OnlineEventStory: Events and Stories in the NewsNMT: 1st Workshop on Neural Machine TranslationNatural Language Processing for Precision MedicineDeep Learning for Dialogue Systems*\u21a9",
      "link": "http://ai.googleblog.com/2017/07/google-at-acl-2017.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Research Communications"
    },
    {
      "title": "Expressions in Virtual Reality",
      "date": "Friday, July 28, 2017",
      "abstract": "Expressions in Virtual RealityMachine PerceptionDaydream LabsYouTube Spacesheadset \u2018removal\u2019 for mixed realityfacial action unitshereInception architectureTensorFlowImagenetpaperheadset removalSteven HicksonTF-SlimSiggraph 2017 Emerging Technologies",
      "link": "http://ai.googleblog.com/2017/07/expressions-in-virtual-reality.html",
      "author": "Posted by Steven Hickson, Software Engineering Intern, and Nick Dufour, Avneesh Sud, Software Engineers, Machine Perception"
    },
    {
      "title": "So there I was, firing a megawatt plasma collider at work...",
      "date": "Tuesday, July 25, 2017",
      "abstract": "So there I was, firing a megawatt plasma collider at work...fusionlong enoughtokamakITERstellaratorWendelstein 7-XNational Ignition FacilityAchievement of Sustained Net Plasma Heating in a Fusion Experiment with the Optometrist AlgorithmTri Alpha Energyfield-reversed configurationsimplemagnetohydrodynamicsfluid dynamicsat least some of the trillion+ individual particlesexperimentsfield-reversed plasma configurationsMarkov Chain Monte Carlopaperlearning from human preferencesNorman RostokerGoogle Accelerated Science team",
      "link": "http://ai.googleblog.com/2017/07/so-there-i-was-firing-megawatt-plasma.html",
      "author": "Posted by Ted Baltz, Senior Staff Software Engineer, Google Accelerated Science Team"
    },
    {
      "title": "Teaching Robots to Understand Semantic Concepts",
      "date": "Friday, July 21, 2017",
      "abstract": "Teaching Robots to Understand Semantic Conceptsgraspingopening doorsUnsupervised Perceptual Rewards for Imitation LearningTime-Contrastive Networks: Self-Supervised Learning from Multi-View ObservationEnd-to-End Learning of Semantic Graspingprevious postprior workdorsal-ventral decomposition observed in the human visual cortexnatural language understandingrobotic perceptiongraspingimitation learningUnsupervised Perceptual Rewards for Imitation LearningRSS 2017Time-Contrastive Networks: Self-Supervised Learning from Multi-View ObservationCVPR Workshop on Deep Learning for Robotic Vision",
      "link": "http://ai.googleblog.com/2017/07/teaching-robots-to-understand-semantic.html",
      "author": "Posted by Sergey Levine, Faculty Advisor and Pierre Sermanet, Research Scientist, Google Brain Team"
    },
    {
      "title": "Google at CVPR 2017",
      "date": "Friday, July 21, 2017",
      "abstract": "Google at CVPR 20172017 Conference on Computer Vision and Pattern Recognitionmachine perceptionHeadset Removal for Virtual and Mixed RealityImage Compression with Neural NetworksJumpTensorFlow Object Detection APITraining object class detectors with click supervisionUnsupervised Pixel-Level Domain Adaptation With Generative Adversarial NetworksBranchOut: Regularization for Online Ensemble Tracking With Convolutional Neural NetworksEnhancing Video Summarization via Vision-Language EmbeddingLearning by Association \u2014 A Versatile Semi-Supervised Training Method for Neural NetworksContext-Aware Captions From Context-Agnostic Supervision Spatially Adaptive Computation Time for Residual NetworksXception: Deep Learning With Depthwise Separable Convolutions Deep Metric Learning via Facility LocationSpeed/Accuracy Trade-Offs for Modern Convolutional Object DetectorsSynthesizing Normalized Faces From Facial Identity FeaturesTowards Accurate Multi-Person Pose Estimation in the WildGuessWhat?! Visual Object Discovery Through Multi-Modal DialogueLearning discriminative and transformation covariant local feature detectorsFull Resolution Image Compression With Recurrent Neural NetworksLearning From Noisy Large-Scale Datasets With Minimal SupervisionUnsupervised Learning of Depth and Ego-Motion From VideoCognitive Mapping and Planning for Visual NavigationFast Fourier Color ConstancyOn the Effectiveness of Visible WatermarksYouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in VideoDeep Learning for Robotic VisionThe Fourth Workshop on Fine-Grained Visual CategorizationLanguage and Vision WorkshopThe First Workshop on Negative Results in Computer VisionVisual Understanding by Learning from Web DataYouTube-8M Large-Scale Video Understanding Challenge",
      "link": "http://ai.googleblog.com/2017/07/google-at-cvpr-2017.html",
      "author": "Posted by Christian Howard, Editor-in-Chief, Research Communications"
    },
    {
      "title": "An Update to Open Images - Now with Bounding-Boxes",
      "date": "Thursday, July 20, 2017",
      "abstract": "An Update to Open Images - Now with Bounding-BoxesOpen ImagesGoogle Cloud Vision APIOpen ImagesFAMILY MAKING A SNOWMANmwvchamberSTANZA STUDENTI.S.S. ANNUNZIATAersupalermoCC BY 2.0hereWe don't need no bounding-boxes: Training object class detectors using only human verification",
      "link": "http://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html",
      "author": "Posted by Vittorio Ferrari, Research Scientist, Machine Perception"
    },
    {
      "title": "Motion Stills \u2014 Now on Android",
      "date": "Thursday, July 20, 2017",
      "abstract": "Motion Stills \u2014 Now on AndroidMotion StillsiOS appThe VergeMashableMotion Stills on Androidtexture mapI-frame spacingDownload Motion Stills for Android from the Google Play store",
      "link": "http://ai.googleblog.com/2017/07/motion-stills-now-on-android.html",
      "author": "Posted by Karthik Raveendran and Suril Shah, Software Engineers, Google Research"
    },
    {
      "title": "Facets: An Open Source Visualization Tool for Machine Learning Training Data",
      "date": "Monday, July 17, 2017",
      "abstract": "Facets: An Open Source Visualization Tool for Machine Learning Training DataGoogle Open Source BlogPAIR initiativeFacetsJupyter notebooksFacets demo website\u201cQuick, Draw!\u201d Datasetconfusion matrixlet us know what you thinkBig Picture teamhttp://archive.ics.uci.edu/ml/datasets/Census+IncomeLearning Multiple Layers of Features from Tiny Images",
      "link": "http://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html",
      "author": "Posted by James Wexler, Senior Software Engineer, Google Big Picture Team"
    },
    {
      "title": "Using Deep Learning to Create Professional-Level Photographs",
      "date": "Thursday, July 13, 2017",
      "abstract": "Using Deep Learning to Create Professional-Level Photographsexperimental deep-learning systemAVAgenerative adversarial networkpapershowcase",
      "link": "http://ai.googleblog.com/2017/07/using-deep-learning-to-create.html",
      "author": "Posted by Hui Fang, Software Engineer, Machine Perception"
    },
    {
      "title": "Building Your Own Neural Machine Translation System in TensorFlow",
      "date": "Wednesday, July 12, 2017",
      "abstract": "Building Your Own Neural Machine Translation System in TensorFlowGoogle Translatedeep neural networks to capture sentence meaningsOpenNMTtf-seq2seqNeural Machine Translation (NMT) tutorialTensorFlowWMT\u201914TensorFlow 1.2tf.contrib.dataGitHubGoogle Brain teamSequence to sequence learning with neural networksLearning phrase representations using RNN encoder-decoder for statistical machine translationNeural machine translation by jointly learning to align and translateEffective approaches to attention-based neural machine translationGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
      "link": "http://ai.googleblog.com/2017/07/building-your-own-neural-machine.html",
      "author": "Posted by Thang Luong, Research Scientist, and Eugene Brevdo, Staff Software Engineer, Google Brain Team"
    },
    {
      "title": "Revisiting the Unreasonable Effectiveness of Data",
      "date": "Tuesday, July 11, 2017",
      "abstract": "Revisiting the Unreasonable Effectiveness of Datadeep learningAlexNetResNetImageNetRevisiting Unreasonable Effectiveness of Data in Deep Learning Erarepresentation learningCOCO object detection benchmarkAPOpen ImagesYouTube-8M",
      "link": "http://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html",
      "author": "Posted by Abhinav Gupta, Faculty Advisor, Machine Perception"
    },
    {
      "title": "The Google Brain Residency Program \u2014 One Year Later",
      "date": "Monday, July 10, 2017",
      "abstract": "The Google Brain Residency Program \u2014 One Year LaterGoogle Brain Residency ProgramOver the past yearpublished over 30 papersICLRICMLCVPREMNLPRSSGECCOISMIRISMBCosyneNIPSICCVBMVCNature MethodsDistilldeconvolution causes checkerboard artifactsvisualizing a generative model of handwritingDistill articleTime-Contrastive Networks: Self-Supervised Learning from Multi-View ObservationDevice Placement Optimization with Reinforcement LearningNeural Optimizer Search with Reinforcement Learninggeneral-purpose sequence-to-sequence modelsmusic synthesismimicking human sketchingsubsampling a sequence for model training,an efficient \u201cattention\u201d mechanism for modelstime series analysisg.co/brainresidency",
      "link": "http://ai.googleblog.com/2017/07/the-google-brain-residency-program-one.html",
      "author": "Posted by Luke Metz, Research Associate and Yun Liu, Software Engineer, 2016 Google Brain Resident Alumni"
    },
    {
      "title": "MultiModel: Multi-Task Machine Learning Across Domains",
      "date": "Wednesday, June 21, 2017",
      "abstract": "MultiModel: Multi-Task Machine Learning Across DomainsMultiModelGoogle\u2019s Multilingual Neural Machine Translation System1Tensor2TensorGoogle Brain team1 The 8 tasks were: (1) speech recognition (WSJ corpus), (2) image classification (ImageNet), (3) image captioning (MS COCO), (4) parsing (Penn Treebank), (5) English-German translation, (6) German-English translation, (7) English-French translation, (8) French-English translation (all using WMT data-sets).\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html",
      "author": "Posted by \u0141ukasz Kaiser, Senior Research Scientist, Google Brain Team and Aidan N. Gomez, Researcher, Department of Computer Science Machine Learning Group, University of Toronto"
    },
    {
      "title": "Accelerating Deep Learning Research with the Tensor2Tensor Library",
      "date": "Monday, June 19, 2017",
      "abstract": "Accelerating Deep Learning Research with the Tensor2Tensor Librarymachine translationspeech recognitionobject detectionTensor2TensorAttention Is All You NeedDepthwise Separable Convolutions for Neural Machine TranslationOne Model to Learn Them AllTransformerSliceNetGNMT + Mixture of ExpertsConvS2SGNMTByteNetMOSESGNMT+MoEGNMTphrase-based translationgithub repoinstructionsLSTM sequence-to-sequence modelfew dozen lines of codeMultiModelMS COCOWSJWMTPenn Treebank12sequence-to-sequence problemparsing data-set generatorTransformerDyer et al.Zhu et al.Socher et al.Vinyals & Kaiser et al.papergithub repositoryTensor2Tensor1\u21a92\u21a9",
      "link": "http://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html",
      "author": "Posted by \u0141ukasz Kaiser, Senior Research Scientist, Google Brain Team"
    },
    {
      "title": "Supercharge your Computer Vision models with the TensorFlow Object Detection API",
      "date": "Thursday, June 15, 2017",
      "abstract": "Supercharge your Computer Vision models with the TensorFlow Object Detection APIGoogle Open Source Blogspur progress in the research communityCOCOMichael Mileyoriginal imageCOCO detection challengeNestCamsimilar items and style ideasstreet number and name detectionTensorFlow Object Detection APITensorFlowSingle Shot Multibox DetectorMobileNetsInception V2Region-Based Fully Convolutional NetworksResnet 101Faster RCNNFaster RCNNInception Resnet v2COCO datasetJupyter notebookCVPR 2017 paperhereJupyter notebooktraining your own pet detector on Cloud ML engineSpeed/accuracy trade-offs for modern convolutional object detectorsTowards Accurate Multi-person Pose Estimation in the WildYouTube-BoundingBoxes: A Large High-Precision Human-Annotated Data Set for Object Detection in Videoblog postBeyond Skip Connections: Top-Down Modulation for Object DetectionSpatially Adaptive Computation Time for Residual NetworksAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual ActionsMobileNets: Efficient convolutional neural networks for mobile vision applications",
      "link": "http://ai.googleblog.com/2017/06/supercharge-your-computer-vision-models.html",
      "author": "Posted by Jonathan Huang, Research Scientist and Vivek Rathod, Software Engineer"
    },
    {
      "title": "MobileNets: Open-Source Models for Efficient On-Device Vision",
      "date": "Wednesday, June 14, 2017",
      "abstract": "MobileNets: Open-Source Models for Efficient On-Device VisionGoogle Open Source Blogfrontier of visual recognition technologyCloud Vision APIMobileNetsTensorFlowInceptionTF-SlimImageNetTensorFlow MobileMobileNet_v1_1.0_224MobileNet_v1_1.0_192MobileNet_v1_1.0_160MobileNet_v1_1.0_128MobileNet_v1_0.75_224MobileNet_v1_0.75_192MobileNet_v1_0.75_160MobileNet_v1_0.75_128MobileNet_v1_0.50_224MobileNet_v1_0.50_192MobileNet_v1_0.50_160MobileNet_v1_0.50_128MobileNet_v1_0.25_224MobileNet_v1_0.25_192MobileNet_v1_0.25_160MobileNet_v1_0.25_128ILSVRC datasetTensorFlow-Slim Image Classification LibraryTensorFlow MobileMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "link": "http://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html",
      "author": "Posted by Andrew G. Howard, Senior Software Engineer and Menglong Zhu, Software Engineer"
    },
    {
      "title": "The Machine Intelligence Behind Gboard",
      "date": "Wednesday, May 24, 2017",
      "abstract": "The Machine Intelligence Behind Gboardmany exciting improvementsGboard for Androidfinite state transducersVoice Search acoustic modelslong short-term memoryconnectionist temporal classificationTensorFlow* beam search*OpenFst\u21a9",
      "link": "http://ai.googleblog.com/2017/05/the-machine-intelligence-behind-gboard.html",
      "author": "Posted by Fran\u00e7oise Beaufays, Principal Scientist, Speech and Keyboard Team and Michael Riley, Principal Scientist, Speech and Languages Algorithms Team"
    },
    {
      "title": "Introducing the TensorFlow Research Cloud",
      "date": "Wednesday, May 17, 2017",
      "abstract": "Introducing the TensorFlow Research Cloudmedical imagingneural machine translationgame playingTensorFlow Research CloudCloud TPUsCloud TPUssign up heresign up hereTensorFlow Research Cloudsign up today",
      "link": "http://ai.googleblog.com/2017/05/introducing-tensorflow-research-cloud.html",
      "author": "Posted by Zak Stone, Product Manager for TensorFlow"
    },
    {
      "title": "Efficient Smart Reply, now for Gmail",
      "date": "Wednesday, May 17, 2017",
      "abstract": "Efficient Smart Reply, now for GmailInbox by Gmailmachine learning to suggest replies to emailSmart Reply for Gmailhierarchies of modules, each of which can learn, remember, and recognize a sequential patternlong-short-term-memory",
      "link": "http://ai.googleblog.com/2017/05/efficient-smart-reply-now-for-gmail.html",
      "author": "Posted by Brian Strope, Research Scientist, and Ray Kurzweil, Engineering Director, Google Research"
    },
    {
      "title": "Using Machine Learning to Explore Neural Network Architecture",
      "date": "Wednesday, May 17, 2017",
      "abstract": "Using Machine Learning to Explore Neural Network Architectureimage recognitionspeech recognitionmachine translationGoogleNetevolutionary algorithmsreinforcement learning algorithmsCIFAR-10Penn Treebankrecently suggestedLSTM RNNsRNNsLarge-Scale Evolution of Image ClassifiersNeural Architecture Search with Reinforcement Learning",
      "link": "http://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html",
      "author": "Posted by Quoc Le & Barret Zoph, Research Scientists, Google Brain team"
    },
    {
      "title": "Coarse Discourse: A Dataset for Understanding Online Discussions",
      "date": "Tuesday, May 16, 2017",
      "abstract": "Coarse Discourse: A Dataset for Understanding Online DiscussionsInformation RetrievalCoarse Discourse datasetreddit.comCoarse Discourse annotation task guidelinesEarly findingsGitHub repositoryICWSMCharacterizing Online Discussion Using Coarse Discourse Sequences",
      "link": "http://ai.googleblog.com/2017/05/coarse-discourse-dataset-for.html",
      "author": "Posted by Praveen Paritosh, Senior Research Scientist, Ka Wong, Senior Data Scientist"
    },
    {
      "title": "Neural Network-Generated Illustrations in Allo",
      "date": "Thursday, May 11, 2017",
      "abstract": "Neural Network-Generated Illustrations in AlloAlloSimply snap a selfiesurrounding visual contextDeepDreamLamar Abramsuncanny valleyLamar AbramsAllo today for AndroidMachine Perception",
      "link": "http://ai.googleblog.com/2017/05/neural-network-generated-illustrations.html",
      "author": "Posted by Jennifer Daniel, Expressions Creative Director, Allo"
    },
    {
      "title": "Updating Google Maps with Deep Learning and Street View",
      "date": "Wednesday, May 3, 2017",
      "abstract": "Updating Google Maps with Deep Learning and Street ViewAttention-based Extraction of Structured Information from Street View ImageryFrench Street Name Signspublicly availableOptical Character Recognitionneural networks to blur faces and license platesreading street numbersStreet View House NumbersIan Goodfellowreleased French Street Name Signsour paperLarge Scale Business Discovery from Street View Imagerypublicly announced earlier this year",
      "link": "http://ai.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html",
      "author": "Posted by Julian Ibarz, Staff Software Engineer, Google Brain Team and Sujoy Banerjee, Product Manager, Ground Truth Team"
    },
    {
      "title": "Experimental Nighttime Photography with Nexus and Pixel",
      "date": "Tuesday, April 25, 2017",
      "abstract": "Experimental Nighttime Photography with Nexus and PixelMarin HeadlandshereGcamcomputational photographythe image processing pipeline that enables the HDR+ modesurprisingly good picturesSeeInTheDarkICCV 2015 Extreme Imaging Workshopcontrast detectionphase detectionDNGPoint Reyes lighthousehereherethey appear to rotate around the celestial polesherehereherehereherehereRussian RiverScorpiusSagittariushereherehereHere\u2019s a Google Photos album",
      "link": "http://ai.googleblog.com/2017/04/experimental-nighttime-photography-with.html",
      "author": "Posted by Florian Kainz, Software Engineer, Google Daydream"
    },
    {
      "title": "Research at Google and ICLR 2017",
      "date": "Monday, April 24, 2017",
      "abstract": "Research at Google and ICLR 20175th International Conference on Learning RepresentationsMachine LearningNeural NetworksDeep LearningGoogle Brain teamGoogle Research EuropeUnderstanding Deep Learning Requires Rethinking GeneralizationSemi-Supervised Knowledge Transfer for Deep Learning from Private Training DataQ-Prop: Sample-Efficient Policy Gradient with An Off-Policy CriticNeural Architecture Search with Reinforcement LearningAdversarial Machine Learning at ScaleCapacity and Trainability in Recurrent Neural NetworksImproving Policy Gradient by Exploring Under-Appreciated RewardsOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts LayerUnrolled Generative Adversarial NetworksCategorical Reparameterization with Gumbel-SoftmaxDecomposing Motion and Content for Natural Video Sequence PredictionDensity Estimation Using Real NVPLatent Sequence DecompositionsLearning a Natural Language Interface with Neural ProgrammerDeep Information PropagationIdentity Matters in Deep LearningA Learned Representation For Artistic StyleAdversarial Training Methods for Semi-Supervised Text ClassificationHyperNetworksLearning to Remember Rare EventsDeep Learning with Dynamic Computation GraphsHolStep: A Machine Learning Dataset for Higher-order Logic Theorem ProvingHyperband: Bandit-based Configuration Evaluation for Hyperparameter OptimizationParticle Value FunctionsNeural Combinatorial Optimization with Reinforcement LearningShort and Deep: Sketching and Neural NetworksExplaining the Learning Dynamics of Direct Feedback AlignmentTraining a Subsampling Mechanism in ExpectationTuning Recurrent Neural Networks with Reinforcement LearningREBAR: Low-Variance, Unbiased Gradient Estimates for Discrete Latent Variable ModelsAdversarial Examples in the Physical WorldRegularizing Neural Networks by Penalizing Confident Output DistributionsUnsupervised Perceptual Rewards for Imitation LearningChanging Model Behavior at Test-time Using Reinforcement Learning",
      "link": "http://ai.googleblog.com/2017/04/research-at-google-and-iclr-2017.html",
      "author": "Posted by Ian Goodfellow, Staff Research Scientist, Google Brain Team"
    },
    {
      "title": "PhotoScan: Taking Glare-Free Pictures of Pictures",
      "date": "Thursday, April 20, 2017",
      "abstract": "PhotoScan: Taking Glare-Free Pictures of Picturesupdate to PhotoScaniOSAndroidSIGGRAPH 2015obstruction-free photographysparse feature pointsORB featuresHarris cornershomographiesoptical flowbilinear combinationbilinear interpolationSzeliski and Coughlan (1997)AndroidiOS",
      "link": "http://ai.googleblog.com/2017/04/photoscan-taking-glare-free-pictures-of.html",
      "author": "Posted by Ce Liu, Michael Rubinstein, Mike Krainin and Bill Freeman, Research Scientists"
    },
    {
      "title": "Teaching Machines to Draw",
      "date": "Thursday, April 13, 2017",
      "abstract": "Teaching Machines to DrawA Neural Representation of Sketch Drawingsrecurrent neural networkraster imagesGenerative Adversarial Networkssequence-to-sequencevariational inferencehypernetworksA Neural Representation of Sketch DrawingsGoogle Brain Residency",
      "link": "http://ai.googleblog.com/2017/04/teaching-machines-to-draw.html",
      "author": "Posted by David Ha, Google Brain Resident"
    },
    {
      "title": "Introducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlow",
      "date": "Tuesday, April 11, 2017",
      "abstract": "Introducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlowGoogle Open Source BlogGoogle Neural Machine Translationseq2seqtf-seq2seqMassive Exploration of Neural Machine Translation Architecturesmachine translationGitHub repositoryMassive Exploration of Neural Machine Translation ArchitecturesSequence to Sequence Learning with Neural NetworksNeural Machine Translation by Jointly Learning to Align and TranslateGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine TranslationAttention and Augmented Recurrent Neural NetworksNeural Machine Translation and Sequence-to-sequence Models: A TutorialSequence-to-Sequence Models",
      "link": "http://ai.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.html",
      "author": "Posted by Anna Goldie and Denny Britz, Research Software Engineer and Google Brain Resident, Google Brain Team"
    },
    {
      "title": "Announcing the 2017 Google PhD Fellows for North America, Europe and the Middle East",
      "date": "Monday, April 10, 2017",
      "abstract": "Announcing the 2017 Google PhD Fellows for North America, Europe and the Middle EastPhD Fellowship program",
      "link": "http://ai.googleblog.com/2017/04/announcing-2017-google-phd-fellows-for.html",
      "author": "Posted by Michael Rennaker, Program Manager"
    },
    {
      "title": "Predicting Properties of Molecules with Machine Learning",
      "date": "Friday, April 7, 2017",
      "abstract": "Predicting Properties of Molecules with Machine Learningdrug discoverybattery designOLEDscatalystsDensity Functional TheoryRupp et al.Behler and ParrinelloQM9benchmarkGoogle Brain teamGoogle Accelerated Science teamDeepMindUniversity of Baselfirst paperNeural Message Passing for Quantum Chemistryatoms as nodes and bonds as edgesLi et al.Duvenaud et al.Kearnes et al.Defferrard et al.Google Brain residents",
      "link": "http://ai.googleblog.com/2017/04/predicting-properties-of-molecules-with.html",
      "author": "Posted by George Dahl, Research Scientist, Google Brain Team"
    },
    {
      "title": "Federated Learning: Collaborative Machine Learning without Centralized Training Data",
      "date": "Thursday, April 6, 2017",
      "abstract": "Federated Learning: Collaborative Machine Learning without Centralized Training DataMobile Vision APIOn-Device Smart ReplyGboard on AndroidStochastic Gradient DescentFederated Averaging algorithmmuch slowercompressing updatesdesigned algorithmsTensorFlowSecure Aggregation protocolrecognize different dog breeds",
      "link": "http://ai.googleblog.com/2017/04/federated-learning-collaborative.html",
      "author": "Posted by Brendan McMahan and Daniel Ramage, Research Scientists"
    },
    {
      "title": "Keeping fake listings off Google Maps",
      "date": "Thursday, April 6, 2017",
      "abstract": "Keeping fake listings off Google MapsGoogle Security blogGoogle My Businesscharge exorbitant service fees for servicesPinning Down Abuse on Google MapsInternational World Wide Web Conferenceselling knock-off products onlinepiloting an advanced verification process",
      "link": "http://ai.googleblog.com/2017/04/keeping-fake-listings-off-google-maps.html",
      "author": "Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security & Anti-Abuse Research"
    },
    {
      "title": "And the award goes to...",
      "date": "Wednesday, April 5, 2017",
      "abstract": "And the award goes to...Andrei BroderRavi KumarPrabhakar RaghavanSridhar RajagopalanAndrew TomkinGraph Structure in the Webwas given to Google founders Larry Page and Sergey BrinGraph Structure in the Web3,500 citationsindegreeGraph Structure in the Web",
      "link": "http://ai.googleblog.com/2017/04/and-award-goes-to.html",
      "author": "Posted by Evgeniy Gabrilovich, Senior Staff Research Scientist, Google Research, and WWW 2017 Technical Program Co-Chair"
    },
    {
      "title": "Consistent Hashing with Bounded Loads",
      "date": "Monday, April 3, 2017",
      "abstract": "Consistent Hashing with Bounded Loadsload balancingMikkel ThorupGoogle Cloud Pub/SubConsistent Hashing with Bounded LoadsVimeohaproxyblog postconsistent hashingballs-to-bins stochastic processesfloor or ceilingour paperopen-sourcepaperNYC Algorithms TeamMikkel Thorup",
      "link": "http://ai.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html",
      "author": "Posted by Vahab Mirrokni, Principal Scientist, Morteza Zadimoghaddam, Research Scientist, NYC Algorithms Team"
    },
    {
      "title": "Announcing AudioSet: A Dataset for Audio Event Research",
      "date": "Thursday, March 30, 2017",
      "abstract": "Announcing AudioSet: A Dataset for Audio Event Researchadding sound effect information to automatic video captions,AudioSetour paperIEEE International Conference on Acoustics, Speech, and Signal Processinghow humans organize sound eventsChild speechUkuleleBoingUrban Sound Taxonomyhereontologyonomatopoeia wordsour paperupdated YouTube 8M DatasetAudioSet websiteViolin, fiddleDCASE challenge seriesGitHub",
      "link": "http://ai.googleblog.com/2017/03/announcing-audioset-dataset-for-audio.html",
      "author": "Posted by Dan Ellis, Research Scientist, Sound Understanding Team"
    },
    {
      "title": "Adding Sound Effect Information to YouTube Captions",
      "date": "Thursday, March 23, 2017",
      "abstract": "Adding Sound Effect Information to YouTube CaptionsYouTube has provided automatic caption trackswe announcedDeep Neural NetworkViterbi algorithm",
      "link": "http://ai.googleblog.com/2017/03/adding-sound-effect-information-to.html",
      "author": "Posted by Sourish Chaudhuri, Software Engineer, Sound Understanding"
    },
    {
      "title": "Distill: Supporting Clarity in Machine Learning",
      "date": "Monday, March 20, 2017",
      "abstract": "Distill: Supporting Clarity in Machine LearningDistillmanyincredibledemonstrationsofthiskindofworkOlah & Carter, 2016Distill is an ecosystem to support clarity in Machine Learningoverview pagelatest articles",
      "link": "http://ai.googleblog.com/2017/03/distill-supporting-clarity-in-machine.html",
      "author": "Posted by Shan Carter, Software Engineer and Chris Olah, Research Scientist, Google Brain Team"
    },
    {
      "title": "Announcing Guetzli: A New Open Source JPEG Encoder",
      "date": "Thursday, March 16, 2017",
      "abstract": "Announcing Guetzli: A New Open Source JPEG EncoderGoogle Open Source BlogGuetzlia new open source algorithmZopfliPNGgzipRNN-based image compressionRAISRWebPcolor space transformdiscrete cosine transformquantizationpsychovisualGuetzli\u2019s psychovisual modelexperiments",
      "link": "http://ai.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html",
      "author": "Posted by Robert Obryk and Jyrki Alakuijala, Software Engineers, Google Research Europe"
    },
    {
      "title": "An Upgrade to SyntaxNet, New Models and a Parsing Competition",
      "date": "Wednesday, March 15, 2017",
      "abstract": "An Upgrade to SyntaxNet, New Models and a Parsing Competitiongeneration of email responsestranslationSyntaxNetParsey McParsefaceParsey's Cousinsmajor upgrade to SyntaxNetnew technologyTensorFlowcharacter-based modelsoriginally coined by Andrew IngrahammorphologysyntaxParseySaurusParsey\u2019s Cousinsmorphologically-richagglutinative languagessentenceherebeam searchmultilingual parsing competitionConference on Computational Natural Language LearningUniversal Dependenciessince 2013DFKItutorialDockerGoogle Cloud Platformsign up to competeUD-Pipe",
      "link": "http://ai.googleblog.com/2017/03/an-upgrade-to-syntaxnet-new-models-and.html",
      "author": "Posted by David Weiss and Slav Petrov, Research Scientists"
    },
    {
      "title": "Quick Access in Drive: Using Machine Learning to Save You Time",
      "date": "Friday, March 10, 2017",
      "abstract": "Quick Access in Drive: Using Machine Learning to Save You Timelanguage translationsunderstanding imageshelping you respond to emailsstudiesQuick AccessWebAndroidiOSthis talk from Google Cloud Next \u201817",
      "link": "http://ai.googleblog.com/2017/03/quick-access-in-drive-using-machine.html",
      "author": "Posted by Sandeep Tata, Software Engineer, Google Research"
    },
    {
      "title": "Assisting Pathologists in Detecting Cancer with Deep Learning",
      "date": "Friday, March 3, 2017",
      "abstract": "Assisting Pathologists in Detecting Cancer with Deep Learningas low as 48%similarly lowRadboud University Medical Center2016 ISBI Camelyon Challenge1Inception (aka GoogLeNet) worked reasonably wellFROC2scores as high as 81%Detecting Cancer Metastases on Gigapixel Pathology ImagesmacrophagesFROC score is definedassociated with prognosis1Camelyon17 challenge\u21a92\u21a9",
      "link": "http://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html",
      "author": "Posted by Martin Stumpe, Technical Lead, and Lily Peng, Product Manager"
    },
    {
      "title": "Google Research Awards 2016",
      "date": "Thursday, February 23, 2017",
      "abstract": "Google Research Awards 2016Google Research Awardsmachine learningmachine perceptionnatural language processingsecuritymachine learningmachine perceptionnetworkingsystemsincreased research presence in Z\u00fcrichrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2017/02/google-research-awards-2016.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, Google"
    },
    {
      "title": "Preprocessing for Machine Learning with tf.Transform",
      "date": "Wednesday, February 22, 2017",
      "abstract": "Preprocessing for Machine Learning with tf.Transformtf.TransformApache BeamGoogle Cloud Dataflowother frameworksTensorflow ServingTensorFlowTensorFlow ServingCloud Dataflow",
      "link": "http://ai.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html",
      "author": "Posted by Kester Tong, David Soergel, and Gus Katsiapis, Software Engineers"
    },
    {
      "title": "Headset \u201cRemoval\u201d for Virtual and Mixed Reality",
      "date": "Tuesday, February 21, 2017",
      "abstract": "Headset \u201cRemoval\u201d for Virtual and Mixed Realitynew ways to view the worldrealimaginaryMixed RealityMachine PerceptionDaydream LabsYouTube Spacesenhancing Mixed RealityGoogle-VR blogCreating a Mixed Reality videoincorporate eye-trackinguncanny valleyheadset removal technology to enhance Mixed RealityGoogle Tilt Brush",
      "link": "http://ai.googleblog.com/2017/02/headset-removal-for-virtual-and-mixed.html",
      "author": "Posted by Vivek Kwatra, Research Scientist and Christian Frueh, Avneesh Sud, Software Engineers"
    },
    {
      "title": "The CS Capacity Program - New Tools and SIGCSE 2017",
      "date": "Thursday, February 16, 2017",
      "abstract": "The CS Capacity Program - New Tools and SIGCSE 2017CS Capacity programdramatic increase in undergraduate computer science enrollmentsSPARCMaGE Peer Mentor programentire online course curriculuma small-group tutoring programSIGCSE 2017 Technical SymposiumAutolabMy Digital HandSIGCSE conference in March",
      "link": "http://ai.googleblog.com/2017/02/the-cs-capacity-program-new-tools-and.html",
      "author": "Posted by Chris Stephenson, Head of Computer Science Education Strategy"
    },
    {
      "title": "An updated YouTube-8M, a video understanding challenge, and a CVPR workshop. Oh my!",
      "date": "Wednesday, February 15, 2017",
      "abstract": "An updated YouTube-8M, a video understanding challenge, and a CVPR workshop. Oh my!YouTube-8M datasetmillions of videos labeled with thousands of classesOpen ImagesYouTube-BoundingBoxesYouTube-8M datasetGoogle Cloud Machine Learningkaggle.comvideo understanding competitionCVPR\u201917 Workshopaudio modeling architecturehereGoogle Cloudkaggle.comKaggle competitionhereGoogle Cloud Machine LearningGithubREADMEgetting started guide on Kaggle1st YouTube-8M WorkshopCVPR 2017invite",
      "link": "http://ai.googleblog.com/2017/02/an-updated-youtube-8m-video.html",
      "author": "Posted by Paul Natsev, Software Engineer"
    },
    {
      "title": "Announcing TensorFlow 1.0",
      "date": "Wednesday, February 15, 2017",
      "abstract": "Announcing TensorFlow 1.0first yearlanguage translationearly detection of skin cancerpreventing blindness in diabetics6000 open-source repositories onlineTensorFlow Developer Summitlivestreamed around the worldTensorFlow 1.0XLAtensorflow.orgtips & tricksKerashereTensorFlow 1.0migration guideconversion scriptJavaGotf.contrib.learnskflowTF SlimXLAtfdbgAndroid demosInstallationTensorFlow Developer Summit talks on YouTubeXLAhereFoldEmbedding ProjectorTensorFlow ServingGitHub issuesStack Overflow@TensorFlowdiscuss@tensorflow.org",
      "link": "http://ai.googleblog.com/2017/02/announcing-tensorflow-10.html",
      "author": "Posted by Amy McDonald Sandjideh, Technical Program Manager, TensorFlow"
    },
    {
      "title": "On-Device Machine Intelligence",
      "date": "Thursday, February 9, 2017",
      "abstract": "On-Device Machine Intelligenceconversational understandingimage recognition deep neural networksgraph-based machine learningIoTAndroid Wear 2.0Smart ReplyAlloInboxsentiment classificationrecurrent neural networkLSTMgraph learningquantizationcharacter-level modelsword embeddingsencoder networkslocality sensitive hashingsemi-supervisedgraph learningAPI",
      "link": "http://ai.googleblog.com/2017/02/on-device-machine-intelligence.html",
      "author": "Posted by Sujith Ravi, Staff Research Scientist, Google Research"
    },
    {
      "title": "Announcing TensorFlow Fold: Deep Learning With Dynamic Computation Graphs",
      "date": "Tuesday, February 7, 2017",
      "abstract": "Announcing TensorFlow Fold: Deep Learning With Dynamic Computation GraphsTensorFlowcomputation graphSIMDparse treesabstract syntax treesDOM treesTensorFlow FoldDeep Learning with Dynamic Computation Graphsrecursive neural networkwords to vector representationsour papergithub site",
      "link": "http://ai.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html",
      "author": "Posted by Moshe Looks, Marcello Herreshoff and DeLesley Hutchins, Software Engineers"
    },
    {
      "title": "Advancing Research on Video Understanding with the YouTube-BoundingBoxes Dataset",
      "date": "Monday, February 6, 2017",
      "abstract": "Advancing Research on Video Understanding with the YouTube-BoundingBoxes DatasetYouTube-8MYouTube-BoundingBoxespreprintidentifylocalizetrack objectsassociated preprint",
      "link": "http://ai.googleblog.com/2017/02/advancing-research-on-video.html",
      "author": "Posted by Esteban Real, Vincent Vanhoucke, Jonathon Shlens, Google Brain Team and\nStefano Mazzocchi, Google Research"
    },
    {
      "title": "Using Machine Learning to Predict Parking Difficulty",
      "date": "Friday, February 3, 2017",
      "abstract": "Using Machine Learning to Predict Parking DifficultyBob Edwardsstuck in trafficlooking for parkingGoogle MapsWazelaunched a new feature for Google Maps for Androidwisdom of the crowdlive trafficpopular times and visit durationslogistic regression",
      "link": "http://ai.googleblog.com/2017/02/using-machine-learning-to-predict.html",
      "author": "Posted by James Cook, Yechen Li, Software Engineers and Ravi Kumar, Research Scientist"
    },
    {
      "title": "App Discovery with Google Play, Part 3: Machine Learning to Fight Spam and Abuse at Scale",
      "date": "Monday, January 30, 2017",
      "abstract": "App Discovery with Google Play, Part 3: Machine Learning to Fight Spam and Abuse at ScalePart 1Part 2Google Play Developer Policyt-Distributed Stochastic Neighbor Embeddingadversarial problemPart 1Part 2unsupervised learningsupervised learning",
      "link": "http://ai.googleblog.com/2017/01/app-discovery-with-google-play-part-3.html",
      "author": "Posted by Hsu-Chieh Lee, Xing Chen, Software Engineers, and Qian An, Analyst"
    },
    {
      "title": "Facilitating the discovery of public datasets",
      "date": "Tuesday, January 24, 2017",
      "abstract": "Facilitating the discovery of public datasetspublish datasocial sciencelife sciencehigh-energy physicsclimate sciencerelyschema.orgrecently published new guidelinesKnowledge GraphData Catalog VocabularydiscussionsExplore for Docs, Sheets and SlidesW3C standards for describing semantics of Web resources and linked dataotherpromisingeffortschallenging problemschema.orgDCATCSVWapproachexpanding",
      "link": "http://ai.googleblog.com/2017/01/facilitating-discovery-of-public.html",
      "author": "Posted by Natasha Noy, Google Research and Dan Brickley, Open Source Programs Office"
    },
    {
      "title": "A Large Corpus for Supervised Word-Sense Disambiguation",
      "date": "Wednesday, January 18, 2017",
      "abstract": "A Large Corpus for Supervised Word-Sense Disambiguationthe capital raised by a business or corporation through the issue and subscription of sharesNew Oxford American DictionaryAI-completeNavigli, 2009Ide and Veronis 1998Mallery 1988release of word-sense annotationsMASCSemCorEnglish Wordnetword-sense disambiguationentity disambiguationNavigli, 2009Krippendorff's alphaKrippendorff, 2004WordnetSemi-supervised Word Sense Disambiguation with Neural Models",
      "link": "http://ai.googleblog.com/2017/01/a-large-corpus-for-supervised-word.html",
      "author": "Posted by Colin Evans and Dayu Yuan, Software Engineers"
    },
    {
      "title": "The Google Brain Team \u2014 Looking Back on 2016",
      "date": "Thursday, January 12, 2017",
      "abstract": "The Google Brain Team \u2014 Looking Back on 2016Google Brain teamICMLNIPSICLRprogram synthesisknowledge transfer from one network to anotherdistributed training of machine learning modelsgenerative models for languageunsupervised learning for roboticsautomated theorem provingbetter theoretical understanding of neural networksalgorithms for improved reinforcement learningACLCoNNLICASSPCVPRISEROSDIICLR 2017hereSequence to Sequence Learning with Neural Networksgenerating captions for imagesparsing sentencessolving computational geometry problemsreplace the translation algorithms powering Google Translateresearch paperzero-shot translationresearch paperThe Great A.I. Awakening[X]learn hand-eye coordinationresearch paperwe explored three possible ways for robots to learn new skillsmade multiple robotics datasets publicly availablepaperJAMAthat machine learning can improve the quality and efficiency of the healthcare experience for doctors and patientsMagentaexplore the intersection of art and machine intelligenceone-day symposiumsupported an art exhibition of machine generated artmusic generationartistic style transferour jam session demo won the Best Demo Award at NIPS 2016white paper on Concrete Problems in AI Safetyblog post heredifferential privacy guaranteesknowledge transfer techniquesa paper on equality of opportunity in supervised learningblog post herecreated a visualization to help illustrate and interactively explore the concepts from the paperopen-sourced an initial version of TensorFlowthe most popular machine learning project on GitHubTensorFlow\u2019s repository of modelsmore than 5000 TensorFlow-related repositorieswell-known research groups and large companiesDeepMindfinding sea cows Down Undersorting cucumbers in Japannumerous performance improvementsadded support for distributed trainingiOSRaspberry PiWindowsbig data infrastructureTensorBoardcomputation graphsembeddingsGoRustHaskellstate-of-the-art image classification modelsWide and DeepGitHubStackOverflowTensorFlow mailing listTensorFlow ServingGoogle Cloud Machine Learningcelebrated TensorFlow\u2019s one year anniversary as an open-source projectpaper on the computer systems aspects of TensorFlowOSDIbackend compiler for TensorFlow called XLAadded to the open-source releasefree online deep learning courseblog announcementTensorFlow PlaygroundGoogle Brain Residentsin seven months they have already conducted significantly original research, helping to author 21 research papersGoogle Brain team Reddit AMA (Ask Me Anything) on r/MachineLearningcustom machine learning accelerator ASIC that was discussed at Google I/ORankBrainNeural Machine Translation systemAlphaGo",
      "link": "http://ai.googleblog.com/2017/01/the-google-brain-team-looking-back-on.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"
    },
    {
      "title": "Google Brain Residency Program - 7 months in and looking ahead",
      "date": "Thursday, January 5, 2017",
      "abstract": "Google Brain Residency Program - 7 months in and looking aheadGoogle Brain ResidencyGoogle Brain teamBayLearn ConferenceMean Field Neural NetworksRegularizing Neural Networks by Penalizing Their Output DistributionNIPS 2016 Adversarial Training workshopISMIR 2016ICLR 2017Neural Architecture Search with Reinforcement LearningDeep Information PropagationUnrolled Generative Adversarial NetworksConditional Image Synthesis with Auxiliary Classifier GANsRegularizing Neural Networks by Penalizing Their Output DistributionMean Field Neural NetworksLearning to RememberTowards Generating Higher Resolution Images with Generative Adversarial NetworksMulti-Task Convolutional Music ModelsAudio DeepDream: Optimizing Raw Audio With Convolutional NetworksLearning to Remember Rare EventsNeural Combinatorial Optimization with Reinforcement LearningHyperNetworksOutrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts LayerNeural Architecture Search with Reinforcement LearningDeep Information PropagationCapacity and Trainability in Recurrent Neural NetworksUnrolled Generative Adversarial NetworksConditional Image Synthesis with Auxiliary Classifier GANsGenerating Long and Diverse Responses with Neural Conversation ModelsIntelligible Language Modeling with Input Switched Affine NetworksRegularizing Neural Networks by Penalizing Confident Output DistributionsUnsupervised Perceptual Rewards for Imitation LearningImproving policy gradient by exploring under-appreciated rewardsProtein Secondary Structure Prediction Using Deep Multi-scale Convolutional Neural Networks and Next-Step Conditioningg.co/brainresidency",
      "link": "http://ai.googleblog.com/2017/01/google-brain-residency-program-7-months.html",
      "author": "Posted by Jeff Dean, Google Senior Fellow and Leslie Phillips, Google Brain Residency Program Manager"
    },
    {
      "title": "Get moving with the new Motion Stills",
      "date": "Thursday, December 15, 2016",
      "abstract": "Get moving with the new Motion Stillsreleased Motion StillsiOS appintegrated Motion Stills into Google Photos for iOSMotion Stills appcinemagraphs\u201cPrivacy Blur\u201dwe published the details of our state-of-the-art RAISR technologydownload the new release of Motion Stills",
      "link": "http://ai.googleblog.com/2016/12/get-moving-with-new-motion-stills.html",
      "author": "Posted by Matthias Grundmann and Ken Conley, Machine Perception"
    },
    {
      "title": "App Discovery with Google Play, Part 2: Personalized Recommendations with Related Apps",
      "date": "Wednesday, December 14, 2016",
      "abstract": "App Discovery with Google Play, Part 2: Personalized Recommendations with Related AppsPart 1 of this seriesGoogle Play Apps StoreGoogle Play Apps Storeunderstanding of the topics associated with an appnearest neighbor searchembeddings",
      "link": "http://ai.googleblog.com/2016/12/app-discovery-with-google-play-part-2.html",
      "author": "Posted by Ananth Balashankar & Levent Koc, Software Engineers, and Norberto Guimaraes, Product Manager"
    },
    {
      "title": "Open sourcing the Embedding Projector: a tool for visualizing high dimensional data",
      "date": "Wednesday, December 7, 2016",
      "abstract": "Open sourcing the Embedding Projector: a tool for visualizing high dimensional dataimage recognitionlanguage translationmedical diagnosisopen-sourcing the Embedding ProjectorA.I. ExperimentTensorFlowprojector.tensorflow.orgembeddingsthis language embeddingword2vec tutorialPCAt-SNEPCAt-SNEMNIST dataset35k frequently used phrases in emailswebsiteEmbedding Projectorhere",
      "link": "http://ai.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html",
      "author": "Posted by Daniel Smilkov and the Big Picture group"
    },
    {
      "title": "NIPS 2016 & Research at Google",
      "date": "Sunday, December 4, 2016",
      "abstract": "NIPS 2016 & Research at Google30th Annual Conference on Neural Information Processing SystemsMachine Intelligencedeep learningDynamic Legged RobotsBoosting with AbstentionCommunity Detection on Evolving GraphsLinear Relaxations for Finding Diverse Elements in Metric SpacesNearly Isometric Embedding by RelaxationOptimistic Bandit Convex OptimizationReward Augmented Maximum Likelihood for Neural Structured PredictionStochastic Gradient MCMC with Stale GradientsUnsupervised Learning for Physical Interaction through Video Prediction*Using Fast Weights to Attend to the Recent PastA Credit Assignment Compiler for Joint PredictionA Neural TransducerAttend, Infer, Repeat: Fast Scene Understanding with Generative ModelsBi-Objective Online Matching and Submodular AllocationsCombinatorial Energy Learning for Image SegmentationDeep Learning GamesDeepMath - Deep Sequence Models for Premise SelectionDensity Estimation via Discrepancy Based Adaptive Sequential PartitionDomain Separation NetworksFast Distributed Submodular Cover: Public-Private Data SummarizationSatisfying Real-world Goals with Dataset ConstraintsCan Active Memory Replace Attention?Fast and Flexible Monotonic Functions with Ensembles of LatticesLaunch and Iterate: Reducing Prediction ChurnOn Mixtures of Markov ChainsOrthogonal Random FeaturesPerspective Transformer Nets: Learning Single-View 3D Object Reconstruction without 3DSupervisionStructured Prediction Theory Based on Factor Graph ComplexityToward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on ExpressivityInteractive musical improvisation with MagentaContent-based Related Video RecommendationAdvances in Approximate Bayesian InferenceAdversarial TrainingBayesian Deep LearningBrains & Bits: Neuroscience Meets Machine LearningConnectomics II: Opportunities & Challanges for Machine LearningConstructive Machine LearningContinual Learning & Deep NetworksDeep Learning for Action & InteractionEnd-to-end Learning for Speech and Audio ProcessingExtreme Classification: Multi-class & Multi-label Learning in Extremely Large Label SpacesInterpretable Machine Learning for Complex SystemsLarge Scale Computer Vision SystemsMachine Learning SystemsNonconvex Optimization for Machine Learning: Theory & PracticeOptimizing the OptimizersReliable Machine Learning in the WildThe Future of Gradient-Based Machine Learning SoftwareTime Series WorkshopTheory and Algorithms for Forecasting Non-Stationary Time SeriesWomen in Machine Learning*\u21a9",
      "link": "http://ai.googleblog.com/2016/12/nips-2016-research-at-google.html",
      "author": "Posted by Doug Eck, Research Scientist, Google Brain Team"
    },
    {
      "title": "Deep Learning for Detection of Diabetic Eye Disease",
      "date": "Tuesday, November 29, 2016",
      "abstract": "Deep Learning for Detection of Diabetic Eye DiseaseDiabetic retinopathy415 million diabetic patientsDevelopment and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus PhotographsJAMAmicroaneurysms, hemorrhages, hard exudates, etcF-scoresensitivity and specificityled by our colleagues at DeepMindHealth Research efforts of the Brain teamGoogle Accelerated Science team",
      "link": "http://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html",
      "author": "Posted by Lily Peng MD PhD, Product Manager and Varun Gulshan PhD, Research Engineer"
    },
    {
      "title": "Zero-Shot Translation with Google\u2019s Multilingual Neural Machine Translation System",
      "date": "Tuesday, November 22, 2016",
      "abstract": "Zero-Shot Translation with Google\u2019s Multilingual Neural Machine Translation SystemGoogle Translatewe announcedGoogle Neural Machine Translation (GNMT)Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot TranslationGoogle Translate",
      "link": "http://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html",
      "author": "Posted by Mike Schuster (Google Brain Team), Melvin Johnson (Google Translate) and Nikhil Thorat (Google Brain Team)"
    },
    {
      "title": "Enhance! RAISR Sharp Images with Machine Learning",
      "date": "Monday, November 14, 2016",
      "abstract": "Enhance! RAISR Sharp Images with Machine LearningRAISR: Rapid and Accurate Image Super-ResolutionUpsamplingMasa Ushioda/Seapics/Solent Newsedge featuresLanczos interpolationOriginal imageAndrzej DraganMarc LevoyaliasingMoire patternsjaggiesImage sourceour paperIEEE Transactions on Computational Imaging",
      "link": "http://ai.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html",
      "author": "Posted by Peyman Milanfar, Research Scientist"
    },
    {
      "title": "Open Source Visualization of GPS Displacements for Earthquake Cycle Physics",
      "date": "Thursday, November 10, 2016",
      "abstract": "Open Source Visualization of GPS Displacements for Earthquake Cycle PhysicsEarth\u2019s surface is movingGlobal Navigation Satellite Systemstrain energyApache 2 licenseavailable on GitHubDepartment of Earth and Planetary SciencesMachine PerceptionBig PictureGEONET array in Japanearthquake.rc.fas.harvard.eduGoogle Maps JavaScript APITHREE.jsearthquake.rc.fas.harvard.eduMachine PerceptionBig Picture TeamGeodetic imaging of plate motions, slip rates, and partitioning of deformation in Japan",
      "link": "http://ai.googleblog.com/2016/11/open-source-visualization-of-gps.html",
      "author": "Posted by Jimbo Wilson, Software Engineer, Google Big Picture Team and Brendan Meade, Professor, Harvard Department of Earth and Planetary Sciences"
    },
    {
      "title": "Celebrating TensorFlow\u2019s First Year",
      "date": "Wednesday, November 9, 2016",
      "abstract": "Celebrating TensorFlow\u2019s First YearGoogle Open Source BlogGoogle Developers BlogGoogle Brain Teamopen-sourced TensorFlowmake technology work better for everyoneTensorFlowthe most popularnumerous performance improvementsadded support for distributed trainingbrought TensorFlow to iOSRaspberry Pibig data infrastructureGoRustHaskellreleased state-of-the-art image classification modelsGitHubStackOverflowTensorFlow mailing listmajor improvements to Google TranslateTensor Processing UnitsProject Magentaproduce melodiesautomatically interpolate between artistic stylesDeepMinddecided to use TensorFlowfascinating generative modelsfind sea cowssort cucumberssigns of Parkinson\u2019s diseasekeep track of the CaltrainTensorFlow ServingWide and DeepGoogle Cloud PlatformCloud Machine LearningTensorFlow\u2019s repository of modelsmore than 3000 TensorFlow-related repositories@tensorflowfind us on GitHubask and answer questions on StackOverflowcommunity discussion listcontributed directly",
      "link": "http://ai.googleblog.com/2016/11/celebrating-tensorflows-first-year.html",
      "author": "Posted by Zak Stone, Product Manager for TensorFlow, on behalf of the TensorFlow team"
    },
    {
      "title": "App Discovery with Google Play, Part 1: Understanding Topics",
      "date": "Tuesday, November 8, 2016",
      "abstract": "App Discovery with Google Play, Part 1: Understanding Topicsdeep neural networkSkip-gram modelGoogle Play Apps store",
      "link": "http://ai.googleblog.com/2016/11/app-discovery-with-google-play-part-1.html",
      "author": "Posted by Malay Haldar, Matt MacMahon, Neha Jha and Raj Arasu, Software Engineers"
    },
    {
      "title": "Research suggestions at your fingertips with Explore in Docs",
      "date": "Tuesday, November 1, 2016",
      "abstract": "Research suggestions at your fingertips with Explore in DocsExplore in Docsmonarch butterfliesGoogle TranslateExplore in Docs",
      "link": "http://ai.googleblog.com/2016/11/research-suggestions-at-your-fingertips.html",
      "author": "Posted by Kishore Papineni, Research Scientist, Google Research NY"
    },
    {
      "title": "Supercharging Style Transfer",
      "date": "Wednesday, October 26, 2016",
      "abstract": "Supercharging Style Transfer*Pasticheonline forumsA Learned Representation for Artistic StyleA Neural Algorithm of Artistic StyleT\u00fcbingen Neckarfront by Andreas PraefckeHead of a ClownGeorges RouaultA Neural Algorithm of Artistic StylePoppy FieldImpression, SunriseClaude MonetAndreas PraefckeRich Niewiroski Jr.J.-H. Jan\u00dfenWilliam GlackensPaul SignacGeorges RouaultEdvard MunchVincent van GoghT\u00fcbingenMagenta blogTensorFlowNat & Lo\u2019s fantastic video explanationImage quilting for texture synthesis and transferImage analogiesA Neural Algorithm of Artistic StyleTexture Networks: Feed-forward Synthesis of Textures and Stylized Imageserceptual Losses for Real-Time Style Transfer and Super-Resolution*Google Brain TeamVincent\u21a9",
      "link": "http://ai.googleblog.com/2016/10/supercharging-style-transfer.html",
      "author": "Posted by Vincent Dumoulin*, Jonathon Shlens and Manjunath Kudlur, Google Brain Team"
    },
    {
      "title": "Course Builder now supports scheduling, easier customization and more",
      "date": "Monday, October 17, 2016",
      "abstract": "Course Builder now supports scheduling, easier customization and moreCourse Builder v1.11deploying from WindowsStudent groupscalendar triggers and availabilityGraphQL serverDigital WorkshopNPTELreleasing our first experimental codeforum",
      "link": "http://ai.googleblog.com/2016/10/course-builder-now-supports-scheduling.html",
      "author": "Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"
    },
    {
      "title": "Equality of Opportunity in Machine Learning",
      "date": "Friday, October 7, 2016",
      "abstract": "Equality of Opportunity in Machine Learningsupervised learningBig Picture teaminteractive visualizationfull version of our paperNIPS",
      "link": "http://ai.googleblog.com/2016/10/equality-of-opportunity-in-machine.html",
      "author": "Posted by Moritz Hardt, Research Scientist, Google Brain Team"
    },
    {
      "title": "Graph-powered Machine Learning at Google",
      "date": "Thursday, October 6, 2016",
      "abstract": "Graph-powered Machine Learning at GoogleMachine Learninggraph-basedreminders in Inboxsmart messaging in AlloGoogle Photosdeep learningsupervised learningunsupervised learningsemi-supervisedKnowledge GraphPhotoReply in Allorelational datasparsethis research paperSmart Reply feature in Inboxnon-convexLarge Scale Distributed Semi-Supervised Learning Using Streaming Approximationstreaming algorithmmultimodalSmart Reply for Inboxsolvesremindersquestion answeringlanguage translationvisual object recognitiondialogue understandingrecent release of Alloannounced this past week on-device Smart Reply capability",
      "link": "http://ai.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html",
      "author": "Posted by Sujith Ravi, Staff Research Scientist, Google Research"
    },
    {
      "title": "How Robots Can Acquire New Skills from Their Shared Experience",
      "date": "Monday, October 3, 2016",
      "abstract": "How Robots Can Acquire New Skills from Their Shared Experiencenatural language understandingspeech recognitioncloud roboticspreviously wrotesuccesses with the Atari video game systemplaying GohereDeep Reinforcement Learning for Robotic ManipulationvideoDeep Visual Foresight for Planning Robot MotionvideodataCollective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy SearchvideoPath Integral Guided Policy Searchvideo",
      "link": "http://ai.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html",
      "author": "Posted by Sergey Levine (Google Brain Team), Timothy Lillicrap (DeepMind), Mrinal Kalakrishnan (X)"
    },
    {
      "title": "Introducing the Open Images Dataset",
      "date": "Friday, September 30, 2016",
      "abstract": "Introducing the Open Images DatasetComputer Visionautomatically caption imagesnatural language replies in response to shared photosImageNetCOCOYFCC100MOpen ImagesCreative Commons Attribution*Google Cloud Vision APIGhost ArchesKevin KrejciSome SilverwareJ BCC BY 2.0DeepDreamartistic style transferOpen Imagesrecently released YouTube-8M*\u21a9",
      "link": "http://ai.googleblog.com/2016/09/introducing-open-images-dataset.html",
      "author": "Posted by Ivan Krasin and Tom Duerig, Software Engineers"
    },
    {
      "title": "Image Compression with Neural Networks",
      "date": "Thursday, September 29, 2016",
      "abstract": "Image Compression with Neural NetworksFull Resolution Image Compression with Recurrent Neural Networksprevious researchimage recognitiontext summarizationreleasing our compression modelTensorFlowGated Recurrent UnitRNNDeep Residual Learning for Image RecognitionJapanese ChinMS-SSIMblocking artifactsour paper",
      "link": "http://ai.googleblog.com/2016/09/image-compression-with-neural-networks.html",
      "author": "Posted by Nick Johnston and David Minnen, Software Engineers"
    },
    {
      "title": "Announcing YouTube-8M: A Large and Diverse Labeled Video Dataset for Video Understanding Research",
      "date": "Wednesday, September 28, 2016",
      "abstract": "Announcing YouTube-8M: A Large and Diverse Labeled Video Dataset for Video Understanding ResearchImageNetdetecting and classifying objects in static imagesVideo analysishelped re-imagine the photos experienceYouTube-8MKnowledge GraphSports-1Mtechnical reportdataset browserdataset explorerfeaturesInception-V3 image annotation modelTensorFlowActivityNet,Sports-1Mtechnical report",
      "link": "http://ai.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html",
      "author": "Posted by Sudheendra Vijayanarasimhan and Paul Natsev, Software Engineers"
    },
    {
      "title": "A Neural Network for Machine Translation, at Production Scale",
      "date": "Tuesday, September 27, 2016",
      "abstract": "A Neural Network for Machine Translation, at Production Scalelaunch of Google TranslatePhrase-Based Machine Translationspeech recognitionimage recognitionGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine TranslationRecurrent Neural NetworkshereTensorFlowTensor Processing UnitsGoogle Brain teamGoogle Translate teamBig Picture teamGoogle\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine TranslationSequence to Sequence Learning with Neural NetworksAddressing the rare word problem in neural machine translationNeural Machine Translation by Jointly Learning to Align and TranslateJapanese and Korean voice searchNeural Machine Translation of Rare Words with Subword Units",
      "link": "http://ai.googleblog.com/2016/09/a-neural-network-for-machine.html",
      "author": "Posted by Quoc V. Le & Mike Schuster, Research Scientists, Google Brain Team"
    },
    {
      "title": "Show and Tell: image captioning open sourced in TensorFlow",
      "date": "Thursday, September 22, 2016",
      "abstract": "Show and Tell: image captioning open sourced in TensorFlowGoogle Brain teammachine learning system to automatically produce captions that accurately describe imagesMicrosoft COCO 2015 image captioning challengeavailable as an open source modelTensorFlowShow and Tell: Lessons learned from the 2015 MSCOCO Image Captioning ChallengeIEEE Transactions on Pattern Analysis and Machine IntelligenceInception V1Inception V2Inception V3hereDistBelief software frameworkhererecently released Inception-ResNet-v2 model",
      "link": "http://ai.googleblog.com/2016/09/show-and-tell-image-captioning-open.html",
      "author": "Posted by Chris Shallue, Software Engineer, Google Brain Team"
    },
    {
      "title": "The 280-Year-Old Algorithm Inside Google Trips",
      "date": "Tuesday, September 20, 2016",
      "abstract": "The 280-Year-Old Algorithm Inside Google TripsAlgorithms Engineeringannounced Google TripsLeonhard Eulerbeautiful mathematical paperWikipediaGraph TheoryWikipediaOrienteeringTravelling Salesman Problemtriangle inequalityalgorithm discovered by Christofidesminimum spanning treeestimate delays on highwayswhen restaurants are most busywhen places are popularChanging of the GuardAndroidiOS",
      "link": "http://ai.googleblog.com/2016/09/the-280-year-old-algorithm-inside.html",
      "author": "Posted by Bogdan Arsintescu, Software Engineer & Sreenivas Gollapudi, Kostas Kollias, Tamas Sarlos and Andrew Tomkins, Research Scientists"
    },
    {
      "title": "The 2016 Google Earth Engine User Summit: Turning pixels into insights",
      "date": "Monday, September 19, 2016",
      "abstract": "The 2016 Google Earth Engine User Summit: Turning pixels into insightsCloud to StreetGoogle Earth Engine User Summit 2016twenty-five hands-on workshopsTerra Bellasession on using SkySat imagerya time series sessionLandsat 8 NDVICHIRPS precipitationr-selectedworkshop sessionSentinel 2AAgriculture in the Sentinel era: scaling up with Earth Enginelood Vulnerability from the Cloud to the Street (and back!) powered by Google Earth EngineAccelerating Rangeland ConservationMonitoring Drought with Google Earth Engine: From Archives to AnswersAutomated methods for surface water detectionMapping the Behavior of RiversClimate Data for Crisis and Health ApplicationsAppalachian Communities at RiskWater, Wildlife and Working LandsStream-side NDVI and The Salmonid Population Viability ProjectMapping Evapotranspiration for Water Use and AvailabilityDynamic Wildfire Modeling in Earth EngineFishing at Scale, now also in Earth EngineMapping crop yields from field to national scales in Earth EngineMapping Pacific Wildfires Impacts with Earth EngineEarthEnv.org - Environmental layers for accessing status and trends in biodiversity, ecosystems and climateBuilding a Landsat 8 Mosaic of AntarcticaMonitoring Primary Production at Broad Spatial and Temporal ScalesAssessing Urbanization Trends for Public Health: Modelling Nighttime Lights Imagery in Africa with Earth EngineNational-scale mapping of forest carbonUtilizing Google Earth Engine to Enhance Decision-Making Capabilities",
      "link": "http://ai.googleblog.com/2016/09/the-2016-google-earth-engine-user.html",
      "author": "Posted by Chris Herwig, Program Manager, Google Earth Engine"
    },
    {
      "title": "Research from VLDB 2016: Improved Friend Suggestion using Ego-Net Analysis",
      "date": "Thursday, September 15, 2016",
      "abstract": "Research from VLDB 2016: Improved Friend Suggestion using Ego-Net Analysis42nd International Conference on Very Large Data BasesEgo-net Community Mining Applied to Friend SuggestionSilvio LattanziVahab MirrokniAhmed Taei,myself,graphdistributed systems",
      "link": "http://ai.googleblog.com/2016/09/research-from-vldb-2016-improved-friend.html",
      "author": "Posted by Alessandro Epasto, Research Scientist, Google Research NY"
    },
    {
      "title": "Computational Thinking from a Dispositions Perspective",
      "date": "Tuesday, September 13, 2016",
      "abstract": "Computational Thinking from a Dispositions PerspectiveGoogle for Education BlogValerie Barr and Ipersonal qualities or soft skills needed for employmenthereCuoco, Goldenberg, and Mark\u2019sPotter and VickersInternational Society for Technology in EducationComputer Science Teachers Associationrecent blog post",
      "link": "http://ai.googleblog.com/2016/09/computational-thinking-from.html",
      "author": "Posted by Chris Stephenson, Head of Computer Science Education Programs at Google, and Joyce Malyn-Smith, Managing Project Director at Education Development Center (EDC)"
    },
    {
      "title": "Announcing the First Annual Global PhD Fellowship Summit and the 2016 Google PhD Fellows",
      "date": "Wednesday, September 7, 2016",
      "abstract": "Announcing the First Annual Global PhD Fellowship Summit and the 2016 Google PhD FellowsPhD Fellowship ProgramAustraliaChina and East AsiaIndiaNorth AmericaEurope and the Middle EastJeff DeanFran\u00e7oise BeaufaysPeter NorvigMaya GuptaAmin VahdatDan RussellKristen LeFevreDouglas EckFran\u00e7oise BeaufaysMaggie Johnson",
      "link": "http://ai.googleblog.com/2016/09/announcing-first-annual-global-phd.html",
      "author": "Posted by Michael Rennaker, Program Manager, University Relations"
    },
    {
      "title": "Reproducible Science: Cancer Researchers Embrace Containers in the Cloud",
      "date": "Tuesday, September 6, 2016",
      "abstract": "Reproducible Science: Cancer Researchers Embrace Containers in the CloudReproducibleCancer MoonshotBreak down silos and bring all the cancer fighters togethercontainersOntario Institute for Cancer ResearchUniversity of California Santa CruzSage BionetworksOregon Health and Sciences UniversityDockerCancer Cloud Pilots funded by the National Cancer InstituteICGC-TCGA DREAM Somatic Mutation Calling (SMC) ChallengesGoogle Cloud PlatformDNA somatic mutationsproviding key insightscontainerizationSMC-Het ChallengeSMC-RNA ChallengeSMC-Het Challengetumor subpopulationsSMC-RNA ChallengeSMC-HetSMC-RNA",
      "link": "http://ai.googleblog.com/2016/09/reproducible-science-cancer-researchers.html",
      "author": "Posted by Dr. Kyle Ellrott, Oregon Health and Sciences University, Dr. Josh Stuart, University of California Santa Cruz, and Dr. Paul Boutros, Ontario Institute for Cancer Research"
    },
    {
      "title": "Improving Inception and Image Classification in TensorFlow",
      "date": "Wednesday, August 31, 2016",
      "abstract": "Improving Inception and Image Classification in TensorFlowwe announced the latest release of the TF-Slim libraryInception-ResNet-v2ILSVRC image classification benchmarkInception V3[1][2]Inception-v4, Inception-ResNet and the Impact of Residual Connections on LearningILSVRC 2012 image classification benchmarkInception-ResNet-v2Codeinception_resnet_v2_2016_08_30.tar.gzInception V3Codeinception_v3_2016_08_28.tar.gzResNet 152Coderesnet_v1_152_2016_08_28.tar.gzResNet V2 200CodeAlaskan MalamuteleftSiberian Huskyrightpre-trained instanceTF-Slim Image Model Libraryinstructions",
      "link": "http://ai.googleblog.com/2016/08/improving-inception-and-image.html",
      "author": "Posted by Alex Alemi, Software Engineer"
    },
    {
      "title": "TF-Slim: A high level library to define complex models in TensorFlow",
      "date": "Tuesday, August 30, 2016",
      "abstract": "TF-Slim: A high level library to define complex models in TensorFlowwe releasedInception-V3ImageNet classification datasetTensorFlowTF-Slimlayersloss functionsevaluation metricstrainingevaluatingTF-Slim Image Models librarytf.contrib.slimlayersAtrous ConvolutionDeconvolutionevaluation metricsdeployment libraryCodeInceptionVGGAlexNetResNetPre-trainedImageNet classification datasetfine-tuneToolsImageNetCIFAR10MNISTREADMEinstructionsJupyter notebookGoing deeper with convolutionsBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftRethinking the Inception Architecture for Computer VisionVery Deep Convolutional Networks for Large-Scale Image RecognitionImageNet Classification with Deep Convolutional Neural NetworksDeep Residual Learning for Image Recognition",
      "link": "http://ai.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html",
      "author": "Posted by Nathan Silberman and Sergio Guadarrama, Google Research"
    },
    {
      "title": "Text summarization with TensorFlow",
      "date": "Wednesday, August 24, 2016",
      "abstract": "Text summarization with TensorFlowGoogle Brain teamTensorFlow model codeAnnotated English Gigawordmetricinverse-document frequencysequence-to-sequence learningSmart Reply for Inboxthis release",
      "link": "http://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html",
      "author": "Posted by Peter Liu and Xin Pan, Software Engineers, Google Brain Team"
    },
    {
      "title": "Meet Parsey\u2019s Cousins: Syntax for 40 languages, plus new SyntaxNet capabilities",
      "date": "Monday, August 8, 2016",
      "abstract": "Meet Parsey\u2019s Cousins: Syntax for 40 languages, plus new SyntaxNet capabilitiesACL 2016released in May as part of SyntaxNetCloud Natural Language APIParsey\u2019s CousinsaccuracyText SegmentationMorphological AnalysisTensorFlowMapReduceheavily inflected languageUniversal Dependenciestreebanksinstrumental caseinceptionGitHubParsey McParsefaceSyntaxNet",
      "link": "http://ai.googleblog.com/2016/08/meet-parseys-cousins-syntax-for-40.html",
      "author": "Posted by Chris Alberti, Dave Orr & Slav Petrov, Google Natural Language Understanding Team"
    },
    {
      "title": "ACL 2016 & Research at Google",
      "date": "Sunday, August 7, 2016",
      "abstract": "ACL 2016 & Research at Google2016 Annual Meeting of the Association for Computational LinguisticsNatural Language Processingg.co/NLUTeamGeneralized Transition-based Dependency Parsing via Control ParametersLearning the Curriculum with Bayesian Optimization for Task-Specific Word Representation LearningMorpho-syntactic Lexicon Generation Using Graph-based Semi-supervised LearningTACLMany Languages, One ParserTACL*Latent Predictor Networks for Code GenerationCollective Entity Resolution with Multi-Focal AttentionPlato: A Selective Context Model for Entity ResolutionTACLWikiReading: A Novel Large-scale Language Understanding Task over WikipediaStack-propagation: Improved Representation Learning for SyntaxCross-lingual Models of Word Embeddings: An Empirical ComparisonGlobally Normalized Transition-Based Neural NetworksCross-lingual projection for class-based language modelsSynthesizing Compound Words for Machine Translation**Cross-Lingual Morphological Tagging for Low-Resource Languages1st Workshop on Representation Learning for NLP1st Workshop on Evaluating Vector-Space Representations for NLPProblems With Evaluation of Word Embeddings Using Word Similarity Tasks*Correlation-based Intrinsic Evaluation of Word Vector RepresentationsSIGFSM Workshop on Statistical NLP and Weighted AutomataDistributed representation and estimation of WFST-based n-gram modelsPynini: A Python library for weighted finite-state grammar compilation*\u21a9",
      "link": "http://ai.googleblog.com/2016/08/acl-2016-research-at-google.html",
      "author": "Posted by Slav Petrov, Research Scientist"
    },
    {
      "title": "Computational Thinking for All Students",
      "date": "Wednesday, August 3, 2016",
      "abstract": "Computational Thinking for All StudentsGoogle for Education BlogHuffington Postthe importance of teaching computational thinking to all K-12 studentsan online course for K-12 teachers to learn about computational thinkinglarge repository of lessonsgreat resources showing how to integrate computational thinking into existing curriculumProject Blokscode.orgcurriculum and materials",
      "link": "http://ai.googleblog.com/2016/08/computational-thinking-for-all-students.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, Google"
    },
    {
      "title": "Announcing an Open Source ADC board for BeagleBone",
      "date": "Wednesday, July 20, 2016",
      "abstract": "Announcing an Open Source ADC board for BeagleBoneGoogle Open Source Blogcarrieramplifierfilterenvelope detectorthresholdAnalog to Digital ConverterDigital Signal ProcessingData Acquisition SystemArduinoBeagleBoneRaspberry Pisystem-on-a-chipBeagleBone capePRUDAQsoftware-defined radioGitHubGroupGetsGoogle Summer of CodeBeagleLogicPRUDAQemail list",
      "link": "http://ai.googleblog.com/2016/07/announcing-open-source-adc-board-for.html",
      "author": "Posted by Jason Holt, Software Engineer"
    },
    {
      "title": "Towards an exact (quantum) description of chemistry",
      "date": "Monday, July 18, 2016",
      "abstract": "Towards an exact (quantum) description of chemistryRichard FeynmanSimulating Physics with ComputersAspuru-Guzik group at HarvardScalable Quantum Simulation of Molecular EnergiesPhysical Review Xmolecular electronic structure problemhighly entangled quantum superposition statesvariational quantum eigensolverphase estimation algorithmquantum error correctionbacteria produce fertilizerhumans produce fertilizerhigh temperature superconductivity",
      "link": "http://ai.googleblog.com/2016/07/towards-exact-quantum-description-of.html",
      "author": "Posted by Ryan Babbush, Quantum Software Engineer"
    },
    {
      "title": "Wide & Deep Learning: Better Together with TensorFlow",
      "date": "Wednesday, June 29, 2016",
      "abstract": "Wide & Deep Learning: Better Together with TensorFlowcategorical featuresWide & Deep LearningLinear ModelsWide & Deep Learningresearch paper*code on GitHubLinear ModelsWide & Deep Learning*\u21a9",
      "link": "http://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html",
      "author": "Posted by Heng-Tze Cheng, Senior Software Engineer, Google Research"
    },
    {
      "title": "CVPR 2016 & Research at Google",
      "date": "Tuesday, June 28, 2016",
      "abstract": "CVPR 2016 & Research at Google2016 Conference on Computer Vision and Pattern RecognitionAbhinav GuptaPAMI Young Researcher AwardLonguet-Higgins PrizeScalable Recognition with a Vocabulary TreeMotion StillsTensorFlow-SlimGeneration and Comprehension of Unambiguous Object DescriptionsDetecting Events and Key Actors in Multi-Person VideosDeepStereo: Learning to Predict New Views From the World\u2019s ImageryDiscovering the Physical Parts of an Articulated Object Class From Multiple VideosBlockout: Dynamic Model Selection for Hierarchical Deep NetworksRethinking the Inception Architecture for Computer VisionImproving the Robustness of Deep Neural Networks via Stability TrainingSemantic Image Segmentation With Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain TransformOptimization Algorithms for Subset Selection and Summarization in Large Data SetsPerceptual Organization in Computer Vision: The Role of Feedback in Recognition and ReorganizationVQA Challenge WorkshopWomen in Computer VisionComputational Models for Learning Systems and Educational AssessmentLarge-Scale Scene Understanding (LSUN) ChallengeLarge Scale Visual Recognition and Retrieval: BigVision 2016ChaLearn Looking at PeopleMedical Computer Vision",
      "link": "http://ai.googleblog.com/2016/06/cvpr-2016-research-at-google.html",
      "author": "Posted by Rahul Sukthankar, Research Scientist"
    },
    {
      "title": "Project Bloks: Making code physical for kids",
      "date": "Monday, June 27, 2016",
      "abstract": "Project Bloks: Making code physical for kidsCS4All programComputer Science in the UK National CurriculumBlocklyScratch BlocksCS FirstMade w/ CodeProject BloksPaulo BliksteinIDEOFriedrich FroebelMaria MontessoriJean Piaget\u2019sLOGO and TORTISwiderangeofresearchandplatformsBlocklyconductive inkRaspberry Pi Zerodrawing robotLEGO\u00ae Education WeDo 2.0sign upposition paper.",
      "link": "http://ai.googleblog.com/2016/06/project-bloks-making-code-physical-for.html",
      "author": "Posted by Steve Vranakis and Jayme Goldstein, Executive Creative Director and Project Lead, Google Creative Lab"
    },
    {
      "title": "Bringing Precision to the AI Safety Discussion",
      "date": "Tuesday, June 21, 2016",
      "abstract": "Bringing Precision to the AI Safety DiscussionConcrete Problems in AI Safetyin the paper",
      "link": "http://ai.googleblog.com/2016/06/bringing-precision-to-ai-safety.html",
      "author": "Posted by Chris Olah, Google Research"
    },
    {
      "title": "ICML 2016 & Research at Google",
      "date": "Monday, June 20, 2016",
      "abstract": "ICML 2016 & Research at Google2016 International Conference on Machine LearningInternational Machine Learning SocietyADIOS: Architectures Deep In Output SpaceAssociative Long Short-Term MemoryAsynchronous Methods for Deep Reinforcement LearningBinary embeddings with structured hashed projectionsDiscrete Distribution Estimation Under Local PrivacyDueling Network Architectures for Deep Reinforcement LearningExploiting Cyclic Symmetry in Convolutional Neural NetworksFast Constrained Submodular Maximization: Personalized Data SummarizationGreedy Column Subset Selection: New Bounds and Distributed AlgorithmsHorizontally Scalable Submodular MaximizationContinuous Deep Q-Learning with Model-based AccelerationMeta-Learning with Memory-Augmented Neural NetworksOne-Shot Generalization in Deep Generative ModelsPixel Recurrent Neural NetworksPricing a low-regret sellerPrimal-Dual Rates and CertificatesRecommendations as Treatments: Debiasing Learning and EvaluationRecycling Randomness with Structure for Sublinear Time Kernel ExpansionsTrain faster, generalize better: Stability of stochastic gradient descentVariational Inference for Monte Carlo ObjectivesAbstraction in Reinforcement LearningDeep Learning WorkshopNeural Networks Back To The FutureData-Efficient Machine LearningOn-Device IntelligenceOnline Advertising SystemsAnomaly Detection 2016Deep Reinforcement LearningRigorous Data Dredging: Theory and Tools for Adaptive Data Analysis",
      "link": "http://ai.googleblog.com/2016/06/icml-2016-research-at-google.html",
      "author": "Posted by Afshin Rostamizadeh, Research Scientist"
    },
    {
      "title": "Announcing Google Research, Europe",
      "date": "Thursday, June 16, 2016",
      "abstract": "Announcing Google Research, EuropeTranslatePhoto SearchSmart Reply for InboxZurich officeKnowledge GraphGoogle Assistant in AlloMachine IntelligenceNatural Language Processing & UnderstandingMachine Perceptionour publicationsacademic support",
      "link": "http://ai.googleblog.com/2016/06/announcing-google-research-europe.html",
      "author": "Posted by Emmanuel Mogenet, Head of Google Research, Europe"
    },
    {
      "title": "Quantum annealing with a digital twist",
      "date": "Wednesday, June 8, 2016",
      "abstract": "Quantum annealing with a digital twistcalculate protein foldingreaction catalysts and \u201cdesigner\u201d moleculesannealadiabatic quantum computingDigitized adiabatic quantum computing with a superconducting circuitNatureour superconducting architectureresonance frequencyquantum entanglementQUTIS groupProf. E. SolanoDr. L. Lamataquantum error correction techniques",
      "link": "http://ai.googleblog.com/2016/06/quantum-annealing-with-digital-twist.html",
      "author": "Posted by Rami Barends and Alireza Shabani, Quantum Electronics Engineers"
    },
    {
      "title": "Motion Stills \u2013 Create beautiful GIFs from Live Photos",
      "date": "Tuesday, June 7, 2016",
      "abstract": "Motion Stills \u2013 Create beautiful GIFs from Live PhotosMotion StillsApple Live Photosvideo stabilizationhundreds of millions of videosGIF animations from photo burstslinear programmingcustom linear solver, GLOP#motionstillsApp Store",
      "link": "http://ai.googleblog.com/2016/06/motion-stills-create-beautiful-gifs.html",
      "author": "Posted by Ken Conley and Matthias Grundmann, Machine Perception"
    },
    {
      "title": "\"Aw, so cute!\": Allo helps you respond to shared photos",
      "date": "Wednesday, May 18, 2016",
      "abstract": "\"Aw, so cute!\": Allo helps you respond to shared photosannounced Alloimage recognition technologyMachine PerceptionKnowledge GraphExpandersemi-supervised learningmultimodal learning",
      "link": "http://ai.googleblog.com/2016/05/so-cute-allo-helps-you-respond-to.html",
      "author": "by Ariel Fuxman, Research Scientist"
    },
    {
      "title": "Chat Smarter with Allo",
      "date": "Wednesday, May 18, 2016",
      "abstract": "Chat Smarter with AlloMachine LearningAlloneural networksSmart Reply for Inboxrecurrent neural networklong short term memorybeam searchL-BFGSmachine translationTranslate teamsemi-supervised learningpaper",
      "link": "http://ai.googleblog.com/2016/05/chat-smarter-with-allo.html",
      "author": "Posted by Pranav Khaitan, Google Research"
    },
    {
      "title": "Announcing SyntaxNet: The World\u2019s Most Accurate Parser Goes Open Source",
      "date": "Thursday, May 12, 2016",
      "abstract": "Announcing SyntaxNet: The World\u2019s Most Accurate Parser Goes Open Sourcecomputer systemsreadunderstandhuman languageto process itintelligent waysSyntaxNetTensorFlowNatural Language Understandingmost accurate such model in the worldsyntactic parserbeam searchpaperSyntaxNetTensorFlowUniversal DependenciesPenn Treebankbetter than any previous approachGoogle WebTreebankSyntaxNet",
      "link": "http://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html",
      "author": "Posted by Slav Petrov, Senior Staff Research Scientist"
    },
    {
      "title": "Research at Google and ICLR 2016",
      "date": "Sunday, May 1, 2016",
      "abstract": "Research at Google and ICLR 20164th International Conference on Learning RepresentationsMachine LearningNeural NetworksDeep LearningGoogle Brain teamGoogle DeepMindNeural Programmer-InterpretersNet2Net: Accelerating Learning via Knowledge TransferPrioritized Experience ReplayReasoning about Entailment with Neural AttentionNeural Programmer: Inducing Latent Programs With Gradient DescentMuProp: Unbiased Backpropagation For Stochastic Neural NetworksMulti-Task Sequence to Sequence LearningA Test of Relative Similarity for Model Selection in Generative ModelsContinuous control with deep reinforcement learningPolicy DistillationNeural Random-Access MachinesVariable Rate Image Compression with Recurrent Neural NetworksOrder Matters: Sequence to Sequence for SetsGrid Long Short-Term MemoryNeural GPUs Learn AlgorithmsACDC: A Structured Efficient Linear LayerRevisiting Distributed Synchronous SGDBlack Box Variational Inference for State Space ModelsA Minimalistic Approach to Sum-Product Network Learning for Real ApplicationsEfficient Inference in Occlusion-Aware Generative Models of ImagesInception-v4, Inception-ResNet and the Impact of Residual Connections on LearningDeep Autoresolution NetworksLearning visual groups from co-occurrences in space and timeAdding Gradient Noise Improves Learning For Very Deep NetworksAdversarial AutoencodersGenerating Sentences from a Continuous Space",
      "link": "http://ai.googleblog.com/2016/05/research-at-google-and-iclr-2016.html",
      "author": "Posted by Dumitru Erhan, Gentleman Scientist"
    },
    {
      "title": "DeepMind moves to TensorFlow",
      "date": "Friday, April 29, 2016",
      "abstract": "DeepMind moves to TensorFlowDeepMindresearchArtificial General IntelligenceTorch7TensorFlow",
      "link": "http://ai.googleblog.com/2016/04/deepmind-moves-to-tensorflow.html",
      "author": "Posted by Koray Kavukcuoglu, Research Scientist, Google DeepMind"
    },
    {
      "title": "Computer Science Education for All Students",
      "date": "Tuesday, April 26, 2016",
      "abstract": "Computer Science Education for All StudentsGoogle for Education Blogjoinopen letter to Congressdeveloping programs, resources, tools and community partnerships$23.5 millionwww.change.org/computerscience",
      "link": "http://ai.googleblog.com/2016/04/computer-science-education-for-all.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Helping webmasters re-secure their sites",
      "date": "Monday, April 18, 2016",
      "abstract": "Helping webmasters re-secure their sites10 million users encounter harmful websites\u2018this site may harm your computer\u2019hereInternational World Wide Web Conferencedrive-by downloadsSearch Consoleeasier to receive security notifications through Google AnalyticsSearch Consoleupdating your site\u2019s softwareadding additional authentication",
      "link": "http://ai.googleblog.com/2016/04/helping-webmasters-re-secure-their-sites.html",
      "author": "Posted by Kurt Thomas and Yuan Niu, Spam & Abuse Research"
    },
    {
      "title": "Announcing TensorFlow 0.8 \u2013 now with distributed computing support!",
      "date": "Wednesday, April 13, 2016",
      "abstract": "Announcing TensorFlow 0.8 \u2013 now with distributed computing support!TensorFlowgRPCGoogle Cloud Machine Learningdistributed trainernception image classificationKubernetesspeed up inference using TensorFlow Serving on Kubernetesnew librariesDistBeliefsynchronous SGD with backup workerson GitHub",
      "link": "http://ai.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html",
      "author": "Posted by Derek Murray, Software Engineer"
    },
    {
      "title": "All of Google\u2019s CS Education Programs and Tools in One Place",
      "date": "Tuesday, April 12, 2016",
      "abstract": "All of Google\u2019s CS Education Programs and Tools in One PlaceGoogle for Education Bloggiving every student an opportunity to learn computer scienceCS EDU websiteBureau of Labor StatisticsGoogle\u2019s researchCS EDUCS FirstExploring Computational ThinkingigniteCSBlocklyHour of CodeGoogle\u2019s Made with CodeCS EDU website",
      "link": "http://ai.googleblog.com/2016/04/all-of-googles-cs-education-programs.html",
      "author": "Posted by Chris Stephenson, Head of Computer Science Education Programs"
    },
    {
      "title": "Genomic Data Processing on Google Cloud Platform",
      "date": "Tuesday, April 5, 2016",
      "abstract": "Genomic Data Processing on Google Cloud Platformcollaboration with Google GenomicsBroad InstituteCromwellannounced a collaborationWhole Genome Sequencing PipelineGoogle Cloud PlatformGCP NEXTGenome Analysis ToolkitPreemptible VMsGoogle Cloud Storage Nearline",
      "link": "http://ai.googleblog.com/2016/04/genomic-data-processing-on-google-cloud.html",
      "author": "Posted by Dr. Stacey Gabriel, Director of the Genomics Platform at the Broad Institute of MIT and Harvard"
    },
    {
      "title": "Lessons learned while protecting Gmail",
      "date": "Tuesday, March 29, 2016",
      "abstract": "Lessons learned while protecting GmailUSENIXEnigma Conferenceexcited to help make this conference happenRon Rivestthe consequences of backdooring encryptionRob Joycedefending against state attackersGeorge \u201cGeohot\u201d Hotzstate of the art software debuggingshare the lessons we\u2019ve learnedalso availableEnigma returns in 2017",
      "link": "http://ai.googleblog.com/2016/03/lessons-learned-while-protecting-gmail.html",
      "author": "Posted by Elie Bursztein - anti-abuse & security research,  Nicolas Lidzborski - Gmail security engineering, and Vijay Eranti - Gmail anti-abuse engineering"
    },
    {
      "title": "Machine Learning in the Cloud, with TensorFlow",
      "date": "Wednesday, March 23, 2016",
      "abstract": "Machine Learning in the Cloud, with TensorFlowspeech recognition in the Google appsearch in Google PhotosSmart Reply feature in Inbox by GmailTensorFlowGCP NEXT 2016announced the alpha releaseCloud Machine LearningTensorFlowCloud Machine LearningCloud Machine LearningTensorFlowdeep learningCloud Translate APICloud Vision APICloud Speech APISDCADeCAFGoogle ResearchGoogle Cloud PlatformGoogle Cloud Platform blog",
      "link": "http://ai.googleblog.com/2016/03/machine-learning-in-cloud-with.html",
      "author": "Posted by Slaven Bilac, Software Engineer, Google Research"
    },
    {
      "title": "Announcing the 2016 Google PhD Fellows for North America, Europe and the Middle East",
      "date": "Thursday, March 10, 2016",
      "abstract": "Announcing the 2016 Google PhD Fellows for North America, Europe and the Middle EastPhD Fellowship program",
      "link": "http://ai.googleblog.com/2016/03/announcing-2016-google-phd-fellows-for.html",
      "author": "Posted by Michael Rennaker, Google PhD Fellowships Lead"
    },
    {
      "title": "Train your own image classifier with Inception in TensorFlow",
      "date": "Wednesday, March 9, 2016",
      "abstract": "Train your own image classifier with Inception in TensorFlowto classify images with TensorFlowImageNethuman performanceWikipediaspotted salamanderfire salamanderherpetologyWikipediaRethinking the Inception Architecture for Computer VisionILSVRC 20122015 ImageNet Challengetransfer learningobject detectionzero-shot learningimage captioningvideo analysiswe are releasing libraries and code for training Inception-v3TensorFlow-Sliminstructionstrainevaluatefine-tune",
      "link": "http://ai.googleblog.com/2016/03/train-your-own-image-classifier-with.html",
      "author": "Posted by Jon Shlens, Senior Research Scientist"
    },
    {
      "title": "Deep Learning for Robots: Learning from Large-Scale Interaction",
      "date": "Tuesday, March 8, 2016",
      "abstract": "Deep Learning for Robots: Learning from Large-Scale Interactionherethis robotKAISTDARPA robotics challengeconvolutional neural networkrecent work by Pinto and Guptaavailable on arXiv",
      "link": "http://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html",
      "author": "Posted by Sergey Levine, Research Scientist"
    },
    {
      "title": "An Update on fast Transit Routing with Transfer Patterns",
      "date": "Wednesday, March 2, 2016",
      "abstract": "An Update on fast Transit Routing with Transfer Patternsbeginnings in 2005while you drag the endpointsGoogle Focused Research AwardNext-Generation Route PlanningHannah BastUniversity of FreiburgPeter SandersDorothea WagneKarlsruhe Institute of TechnologyScalable Transfer PatternsFrequency-Based Search for Public TransitSabine StorandtClick hereClick herelist of publicationsproject\u2019s websiteFast Routing in Very Large Public Transportation Networks Using Transfer PatternsdoiScalable Transfer PatternsdoiFrequency-based Search for Public Transitdoi",
      "link": "http://ai.googleblog.com/2016/03/an-update-on-fast-transit-routing-with.html",
      "author": "Arno Eigenwillig, Software Engineer on Google Maps Directions"
    },
    {
      "title": "And the winner of the $1 Million Little Box Challenge is\u2026CE+T Power\u2019s Red Electrical Devils",
      "date": "Monday, February 29, 2016",
      "abstract": "And the winner of the $1 Million Little Box Challenge is\u2026CE+T Power\u2019s Red Electrical DevilsIEEELittle Box ChallengeIEEE Power Electronics Society18 finalists were selectedNational Renewable Energy LaboratoryCE+T PowerBelgium\u2019s national soccer teamSchneider ElectricVirginia Tech\u2019s Future Energy Electronics Center100 hours of testing at NRELARPA-E Energy Innovation SummitLittle Box Challenge",
      "link": "http://ai.googleblog.com/2016/02/and-winner-of-1-million-little-box.html",
      "author": "Posted by Ross Koningstein, Engineering Director Emeritus, Google Research"
    },
    {
      "title": "On the Personalities of Dead Authors",
      "date": "Wednesday, February 24, 2016",
      "abstract": "On the Personalities of Dead AuthorsNatural Language UnderstandingProject GutenbergDeep Neural Networkword vectorsparagraph vectorst-SNE algorithmhere's the complete listthis UPenn studySmart Reply feature",
      "link": "http://ai.googleblog.com/2016/02/on-personalities-of-dead-authors.html",
      "author": "Posted by Marc Pickett, Software Engineer, Chris Tar, Engineering Manager and Brian Strope, Research Scientist"
    },
    {
      "title": "Google Science Fair 2016: #howcanwe make things better with science?",
      "date": "Tuesday, February 23, 2016",
      "abstract": "Google Science Fair 2016: #howcanwe make things better with science?Google for Education blog2016 Google Science Fairsitefollow along on Google+Twitter2015 Grand Prize winner, Olivia HalliseyMake Better Generatorhelpful ideasconversationenter the Google Science Fair this year",
      "link": "http://ai.googleblog.com/2016/02/google-science-fair-2016-howcanwe-make.html",
      "author": "Posted by Olivia Hallisey, 2015 Grand Prize winner, Google Science Fair"
    },
    {
      "title": "Exploring the Intersection of Art and Machine Intelligence",
      "date": "Monday, February 22, 2016",
      "abstract": "Exploring the Intersection of Art and Machine Intelligencepublished a storystrange, wonderful and oddly compelling imageswe released the source codewitnessed a tremendous interestAmanda PetersonMemo AktenSamim WinigerKyle McDonaldGene KoganGCHQA Neural Algorithm of Artistic StyleexperimentingTwitter botsexplorationsexperimentsstyle transfer algorithmNeil deGrasse TysonKadinsky\u2019s Jane Rouge BleuGuillaume PiolleGitXivfurtherMachine learning and arta similar programArtificial IntelligenceGray Area Foundation for the ArtsDeepDream: The Art of Neural NetworksArt and Machine Learning Symposium",
      "link": "http://ai.googleblog.com/2016/02/exploring-intersection-of-art-and.html",
      "author": "Posted by Mike Tyka, Software Engineer"
    },
    {
      "title": "Text-to-Speech for Low-Resource Languages (Episode 3): But can it say \u201cGoogle\u201d?",
      "date": "Friday, February 19, 2016",
      "abstract": "Text-to-Speech for Low-Resource Languages (Episode 3): But can it say \u201cGoogle\u201d?first episodesecond episodegather sufficient datatrain a statistical parametric voiceInternational Phonetic AlphabetBengali scriptBengali phonologyan earlier versionin use at Google for several yearsBengali pronunciation dictionaryCreative Commons License (CC BY 4.0)",
      "link": "http://ai.googleblog.com/2016/02/text-to-speech-for-low-resource.html",
      "author": "Posted by Martin Jansche, Software Engineer, Google Research for Low Resource Languages"
    },
    {
      "title": "Running your models in production with TensorFlow Serving",
      "date": "Tuesday, February 16, 2016",
      "abstract": "Running your models in production with TensorFlow Servingspeech recognition in the Google appSmart Reply in Inboxsearch in Google Photosnew and interesting challengesTensorFlow ServingTensorFlowTensorFlow Servingrun experimentsvalidationremote procedure callgRPCmachinequestionsfeature requestsgithub.com/tensorflow/servingtutorial@googleresearch+ResearchatGoogleJeff DeanGCP Next 2016",
      "link": "http://ai.googleblog.com/2016/02/running-your-models-in-production-with.html",
      "author": "Posted by Noah Fiedel, Software Engineer"
    },
    {
      "title": "Google Research Awards: Fall 2015",
      "date": "Friday, February 12, 2016",
      "abstract": "Google Research Awards: Fall 2015Google Research Awardsrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2016/02/google-research-awards-fall-2015.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Announcing the Google Internet of Things (IoT) Technology Research Award Pilot",
      "date": "Wednesday, February 10, 2016",
      "abstract": "Announcing the Google Internet of Things (IoT) Technology Research Award PilotInternet of Things1Google beacon platformPhysical WebNearby Messages APIGoogle's proximity beacon serviceBrilloWeaveOnHub routerOn.HereGoogle Cloud Platform IoT SolutionsChrome BoxesKiosk AppsVanadiumUbiquity Dev Summit playlistPlease submit your proposal here1see here\u21a9",
      "link": "http://ai.googleblog.com/2016/02/announcing-google-internet-of-things.html",
      "author": "Posted Vint Cerf, Chief Internet Evangelist, and Max Senges, Google Research"
    },
    {
      "title": "AlphaGo: Mastering the ancient game of Go with Machine Learning",
      "date": "Wednesday, January 27, 2016",
      "abstract": "AlphaGo: Mastering the ancient game of Go with Machine Learningnoughts and crossescheckersDeep BlueWatson2014 our own algorithms learned to play dozens of Atari gamesraw pixel inputsGo40 million people worldwideConfuciusour essential artsgoogolsearch tree10 yearsNaturedeep neural networksDeep BlueMonte-Carlo tree search44%reinforcement learningbelieved to be impossibleGoogle Cloud PlatformTensorFlow4 free moves headstartElo ratingrank Lee Sedolgrand challenges of AI\u2018expert\u2019 system",
      "link": "http://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html",
      "author": "Posted by David Silver and Demis Hassabis, Google DeepMind"
    },
    {
      "title": "Teach Yourself Deep Learning with TensorFlow and Udacity",
      "date": "Thursday, January 21, 2016",
      "abstract": "Teach Yourself Deep Learning with TensorFlow and UdacityDeep learningTensorFlowrecently releasedGitHubenthusiasts around the globeDeep Learning CourseUdacityconvolutional networksrecurrent neural networksUdacity blog postregister for the course",
      "link": "http://ai.googleblog.com/2016/01/teach-yourself-deep-learning-with.html",
      "author": "Posted by Vincent Vanhoucke, Principal Research Scientist"
    },
    {
      "title": "Why attend USENIX Enigma?",
      "date": "Monday, January 11, 2016",
      "abstract": "Why attend USENIX Enigma?we announced USENIX EnigmaElectronic Frontier FoundationAccess NowAkamaiBlackberryFacebookLinkedInNetflixTwittersession topics and organizations representedProject Zero0dayDavid BrumleyTED conferenceUSENIX Enigma",
      "link": "http://ai.googleblog.com/2016/01/why-attend-usenix-enigma.html",
      "author": "Parisa Tabriz, Security Princess & Enigma Program Co-Chair"
    },
    {
      "title": "Four years of Schema.org - Recent Progress and Looking Forward",
      "date": "Thursday, December 17, 2015",
      "abstract": "Four years of Schema.org - Recent Progress and Looking Forwardwe announced schema.orgschema.orgrich snippetsGMailGoogle AppSchema.orgSchema.org: Evolution of Structured Data on the Webschema.org group at W3Ccommunity discussionto build a systemschema.orgschema.org",
      "link": "http://ai.googleblog.com/2015/12/four-years-of-schemaorg-recent-progress.html",
      "author": "Posted by Ramanathan Guha, Google Fellow"
    },
    {
      "title": "Text-to-Speech for Low-Resource Languages (Episode 2): Building a Parametric Voice",
      "date": "Tuesday, December 15, 2015",
      "abstract": "Text-to-Speech for Low-Resource Languages (Episode 2): Building a Parametric Voiceprevious episodeprevious episodeunit selectionHidden Markov Modelswell-established techniqueProf. Keiichi TokudaRecurrent Neural NetworksvocodersphonemesVocaineproposed a neural network-based modelLong Short-Term Memorydescribed the LSTM RNN architecturerecently mentioned in our blogBangla corpusHMM synthesizer outputresulting audiothis waveform",
      "link": "http://ai.googleblog.com/2015/12/text-to-speech-for-low-resource.html",
      "author": "Posted by Alexander Gutkin, Google Speech Team"
    },
    {
      "title": "Making online learning even easier with a re-envisioned Course Builder",
      "date": "Monday, December 14, 2015",
      "abstract": "Making online learning even easier with a re-envisioned Course BuilderGoogle for Education blogCourse Builder v1.10Google Open Online Educationrelease notes",
      "link": "http://ai.googleblog.com/2015/12/making-online-learning-even-easier-with.html",
      "author": "Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"
    },
    {
      "title": "When can Quantum Annealing win?",
      "date": "Tuesday, December 8, 2015",
      "abstract": "When can Quantum Annealing win?Quantum AI teamquantum annealersD-Wave 2X quantum annealersimulated annealingQuantum Monte Carloquantum hardware grouphttp://arxiv.org/abs/1512.02206",
      "link": "http://ai.googleblog.com/2015/12/when-can-quantum-annealing-win.html",
      "author": "Posted by Hartmut Neven, Director of Engineering"
    },
    {
      "title": "How to Classify Images with TensorFlow",
      "date": "Monday, December 7, 2015",
      "abstract": "How to Classify Images with TensorFlowJetpacdeep learningTensorFlowan annual competitionImagenetsource codelabel_imageREADMEWikipediathe TensorFlow Inception tutorialU.S History Imagesnematodefound that even someone who was trained at the job could only achieve a slightly-better 5.1% error doing the same task manuallythis simple tutorial on recognizing hand-drawn digits from the MNIST data set",
      "link": "http://ai.googleblog.com/2015/12/how-to-classify-images-with-tensorflow.html",
      "author": "Posted by Pete Warden, Software Engineer"
    },
    {
      "title": "NIPS 2015 and Machine Learning Research at Google",
      "date": "Sunday, December 6, 2015",
      "abstract": "NIPS 2015 and Machine Learning Research at Google29th Annual Conference on Neural Information Processing Systemsmachine learningdeep learningLearning Theory and Algorithms for Forecasting Non-stationary Time SeriesDistributed Submodular Cover: Succinctly Summarizing Massive DataSpatial Transformer NetworksPointer NetworksStructured Transforms for Small-Footprint Deep LearningSpherical Random Features for Polynomial KernelsLearning to Transduce with Unbounded MemoryDeep Knowledge TracingHidden Technical Debt in Machine Learning SystemsGrammar as a Foreign LanguageStochastic Variational Information MaximisationEmbedding Inference for Structured Multilabel PredictionOn the Convergence of Stochastic Gradient MCMC Algorithms with High-Order IntegratorsSpectral Norm Regularization of Orthonormal Representations for Graph TransductionDifferentially Private Learning of Structured Discrete DistributionsNearly Optimal Private LASSOLearning Continuous Control Policies by Stochastic Value GradientsGradient Estimation Using Stochastic Computation GraphsScheduled Sampling for Sequence Prediction with Recurrent Neural NetworksTeaching Machines to Read and ComprehendBayesian dark knowledgeGeneralization in Adaptive Data Analysis and Holdout ReuseSemi-supervised Sequence LearningNatural Neural NetworksRevenue Optimization against Strategic BuyersFeature Extraction: Modern Questions and ChallengesNIPS Time Series WorkshopNonparametric Methods for Large Scale Representation LearningMachine Learning for Spoken Language Understanding and InteractionAdaptive Data AnalysisDeep Reinforcement LearningAdvances in Approximate Bayesian InferenceCognitive Computation: Integrating Neural and Symbolic ApproachesTransfer and Multi-Task Learning: Trends and New PerspectivesLearning and privacy with incomplete data and weak supervisionBlack Box Learning and InferenceQuantum Machine LearningBayesian Nonparametrics: The Next GenerationBayesian Optimization: Scalability and FlexibilityReasoning, Attention, Memory (RAM)Extreme Classification 2015: Multi-class and Multi-label Learning in Extremely Large Label SpacesMachine Learning SystemsBrains, Mind and MachinesDeep Learning SymposiumAlgorithms Among Us: The Societal Impacts of Machine LearningNIPS 2015 Deep Learning TutorialLarge-Scale Distributed Systems for Training Neural Networks",
      "link": "http://ai.googleblog.com/2015/12/nips-2015-and-machine-learning-research.html",
      "author": "Posted by Sanjiv Kumar, Research Scientist"
    },
    {
      "title": "TensorFlow - Google\u2019s latest machine learning system, open sourced for everyone",
      "date": "Monday, November 9, 2015",
      "abstract": "TensorFlow - Google\u2019s latest machine learning system, open sourced for everyoneDistBeliefneural networksconcepts like \u201ccat\u201dthe Google appin Google PhotosLarge Scale Visual Recognition Challenge in 2014automated image captioningDeepDreamTensorFlowwhitepaperexamplesauto-differentiationexample model architecturessignals derived from deep neural networksmagic features of tomorrowwww.tensorflow.org",
      "link": "http://ai.googleblog.com/2015/11/tensorflow-googles-latest-machine.html",
      "author": "Posted by Jeff Dean, Senior Google Fellow, and Rajat Monga, Technical Lead"
    },
    {
      "title": "Computer, respond to this email.",
      "date": "Tuesday, November 3, 2015",
      "abstract": "Computer, respond to this email.*deep neural networksvoice searchYouTube thumbnailsTuring Testsequence-to-sequence learningEarly resultsfun to play withrecurrent neural networksthought vectorLSTMInbox for Android and iOS\n* This blog post may or may not have actually been written by a neural network.\u21a9*\u21a9",
      "link": "http://ai.googleblog.com/2015/11/computer-respond-to-this-email.html",
      "author": "Posted by Greg Corrado*, Senior Research Scientist"
    },
    {
      "title": "How to measure translation quality in your user interfaces",
      "date": "Friday, October 30, 2015",
      "abstract": "How to measure translation quality in your user interfaces200 languages that are spoken by at least 3 million peopleMeasuring user rated language quality: Development and validation of the user interface Language Quality Survey (LQS)International Journal of Human-Computer Studiesfull survey can be found in the publication",
      "link": "http://ai.googleblog.com/2015/10/how-to-measure-translation-quality-in.html",
      "author": "Posted by Javier Bargas-Avila, User Experience Research at Google"
    },
    {
      "title": "Improving YouTube video thumbnails with deep neural nets",
      "date": "Thursday, October 8, 2015",
      "abstract": "Improving YouTube video thumbnails with deep neural netsVideo thumbnailsdeep neural networksimagevideoGoogLeNetGrand Canyon Rock Squirrel",
      "link": "http://ai.googleblog.com/2015/10/improving-youtube-video-thumbnails-with.html",
      "author": "Posted by Weilong Yang and Min-hsuan Tsai, Video Content Analysis team and the YouTube Creator team"
    },
    {
      "title": "Google voice search: faster and more accurate",
      "date": "Thursday, September 24, 2015",
      "abstract": "Google voice search: faster and more accuratewe announcedDeep Neural NetworksConnectionist Temporal Classificationsequence discriminative training techniquesrecurrent neural networksphonetic notationLong Short-Term Memoryalready improved the qualityhereour new acoustic modelsGoogle app",
      "link": "http://ai.googleblog.com/2015/09/google-voice-search-faster-and-more.html",
      "author": "Posted by Ha\u015fim Sak, Andrew Senior, Kanishka Rao, Fran\u00e7oise Beaufays and Johan Schalkwyk \u2013 Google Speech Team"
    },
    {
      "title": "A Beginner\u2019s Guide to Deep Neural Networks",
      "date": "Tuesday, September 22, 2015",
      "abstract": "A Beginner\u2019s Guide to Deep Neural Networksabout the research that\u2019s gone into teaching computers to recognize speech and understand languagemachine translationimage recognition and descriptionGoogle Voice transcriptionthis projectGreg CorradoChristopher OlahAsk Me Anything on Reddithere",
      "link": "http://ai.googleblog.com/2015/09/a-beginners-guide-to-deep-neural.html",
      "author": "Posted by Natalie Hammel and Lorraine Yurshansky, creators of Nat & Lo\u2019s 20% Project"
    },
    {
      "title": "Information sharing for more efficient network utilization and management",
      "date": "Thursday, September 17, 2015",
      "abstract": "Information sharing for more efficient network utilization and managementGoogle Video Quality ReportpaperInternet Architecture BoardInternet Engineering Task ForceThroughput Guidance",
      "link": "http://ai.googleblog.com/2015/09/information-sharing-for-more-efficient.html",
      "author": "Andreas Terzis, Software Engineer"
    },
    {
      "title": "Crowdsourcing a Text-to-Speech Voice for Low-Resource Languages (Episode 1)",
      "date": "Tuesday, September 8, 2015",
      "abstract": "Crowdsourcing a Text-to-Speech Voice for Low-Resource Languages (Episode 1)articlesCrowd-sourcing projects for automatic speech recognitionpast researchPRAATthe publicly available TTS data from the Indian Institute of Information Technology",
      "link": "http://ai.googleblog.com/2015/09/crowdsourcing-text-to-speech-voice-for.html",
      "author": "Posted by Linne Ha, Senior Program Manager, Google Research for Low Resource Languages"
    },
    {
      "title": "VLDB 2015 and Database Research at Google",
      "date": "Monday, August 31, 2015",
      "abstract": "VLDB 2015 and Database Research at Google41st International Conference of Very Large Databasesstructured snippetstable searchF1MesaBigQueryKeys for GraphsIn-Memory Performance for Big DataThe Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data ProcessingResource Bricolage for Parallel Database SystemsAsterixDB: A Scalable, Open Source BDMSKnowledge-Based Trust: A Method to Estimate the Trustworthiness of Web SourcesEfficient Evaluation of Object-Centric Exploration Queries for VisualizationInterpretable and Informative Explanations of OutcomesTake me to your leader! Online Optimization of Distributed Storage ConfigurationsTreeScope: Finding Structural Anomalies In Semi-Structured Data",
      "link": "http://ai.googleblog.com/2015/08/vldb-2015-and-database-research-at.html",
      "author": "Posted by Corinna Cortes, Head of Google Research NY and Cong Yu, Research Scientist"
    },
    {
      "title": "Announcing Google\u2019s 2015 Global PhD Fellows",
      "date": "Friday, August 28, 2015",
      "abstract": "Announcing Google\u2019s 2015 Global PhD FellowsPhD Fellowship programAustraliaChina and East AsiaIndiaNorth AmericaEurope and the Middle East",
      "link": "http://ai.googleblog.com/2015/08/announcing-googles-2015-global-phd.html",
      "author": "Posted by Michael Rennaker, Google University Relations"
    },
    {
      "title": "Google Faculty Research Awards: Summer 2015",
      "date": "Friday, August 21, 2015",
      "abstract": "Google Faculty Research Awards: Summer 2015Google Faculty Research Awardslast roundrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2015/08/google-faculty-research-awards-summer.html",
      "author": "posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "The Next Chapter for Flu Trends",
      "date": "Thursday, August 20, 2015",
      "abstract": "The Next Chapter for Flu Trendslaunchuseful insightssearch trendseconomicsother fieldsupdatingColumbia University\u2019s Mailman School of Public HealthdashboardBoston Children\u2019s Hospital/HarvardCenters for Disease Control and Prevention (CDC) Influenza Division.affect millions of people every yearFlu Trends web page",
      "link": "http://ai.googleblog.com/2015/08/the-next-chapter-for-flu-trends.html",
      "author": "Posted by The Flu Trends Team"
    },
    {
      "title": "Pulling Back the Curtain on Google\u2019s Network Infrastructure",
      "date": "Tuesday, August 18, 2015",
      "abstract": "Pulling Back the Curtain on Google\u2019s Network Infrastructurewarehouse-scale computerACM SIGCOMM conferencepapertalkOpen Network Summitrecent paperSoftware Defined Networking (SDN)Bandwidth EnforcerCondorTIMELYGoogle Cloud Platform",
      "link": "http://ai.googleblog.com/2015/08/pulling-back-curtain-on-googles-network.html",
      "author": "Posted by Amin Vahdat, Google Fellow"
    },
    {
      "title": "Say hello to the Enigma conference",
      "date": "Tuesday, August 18, 2015",
      "abstract": "Say hello to the Enigma conferenceUSENIX Enigmahttp://enigma.usenix.org",
      "link": "http://ai.googleblog.com/2015/08/say-hello-to-enigma-conference.html",
      "author": "Posted by Elie Bursztein - Anti-abuse team, Parisa Tabriz - Chrome Security and Niels Provos - Security team"
    },
    {
      "title": "KDD 2015 Best Research Paper Award: \u201cAlgorithms for Public-Private Social Networks\u201d",
      "date": "Monday, August 17, 2015",
      "abstract": "KDD 2015 Best Research Paper Award: \u201cAlgorithms for Public-Private Social Networks\u201d21st ACM conference on Knowledge Discovery and Data Miningfreely availableACM Digital LibraryEfficient Algorithms for Public-Private Social NetworksRavi KumarSilvio LattanziVahab MirrokniAlessandro EpastoFlavio ChierichettiedgesIn a recent studygraphsPageRankEfficient Algorithms for Public-Private Social NetworksLarge-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMCTimeMachine: Timeline Generation for Knowledge-Base EntitiesAlgorithmic Cartography: Placing Points of Interest and Ads on MapsStream Sampling for Frequency Cap StatisticsDirichlet-Hawkes Processes with Applications to Clustering Continuous-Time Document StreamsAdaptation Algorithm and Theory Based on Generalized DiscrepancyEstimating Local Intrinsic DimensionalityUnified and Contrasting Cuts in Multiple Graphs: Application to Medical Imaging SegmentationGoing In-depth: Finding Longform on the WebAnnotating needles in the haystack without looking: Product information extraction from emailsFocusing on the Long-term: It's Good for Users and Business",
      "link": "http://ai.googleblog.com/2015/08/kdd-2015-best-research-paper-award.html",
      "author": "Posted by Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Google\u2019s Course Builder 1.9 improves instructor experience and takes Skill Maps to the next level",
      "date": "Thursday, August 13, 2015",
      "abstract": "Google\u2019s Course Builder 1.9 improves instructor experience and takes Skill Maps to the next levelGoogle for Education BlogCourse Builderskill mapping capabilitiesdownload it hereGoogle Open Online Educationrelease noteslet us knowSesame Street\u2019s Make Believe with MathComputational Thinking for Educators",
      "link": "http://ai.googleblog.com/2015/08/googles-course-builder-19-improves.html",
      "author": "Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"
    },
    {
      "title": "The neural networks behind Google Voice transcription",
      "date": "Tuesday, August 11, 2015",
      "abstract": "The neural networks behind Google Voice transcriptiondeep learningimage classification and captioningtranslationmodel visualization techniqueswe announcedLong Short-term Memory Recurrent Neural NetworksGaussian Mixture Modeladapting the modelsrevolutionized the field of speech recognitiondifferentiating phonetic unitsfirst launchedgarbage in, garbage out",
      "link": "http://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html",
      "author": "Posted by Fran\u00e7oise Beaufays, Research Scientist"
    },
    {
      "title": "The reusable holdout: Preserving validity in adaptive data analysis",
      "date": "Thursday, August 6, 2015",
      "abstract": "The reusable holdout: Preserving validity in adaptive data analysisXKCD cartoonp-valueXKCDThe Reusable Holdout: Preserving Validity in Adaptive Data AnalysisCynthia DworkVitaly FeldmanToniann PitassiOmer ReingoldAaron RothScienceFreedman\u2019s paradoxF-test\u201cgarden of the forking paths\u201ddifferential privacystabilityrelated work with Avrim Blumthis blog post",
      "link": "http://ai.googleblog.com/2015/08/the-reusable-holdout-preserving.html",
      "author": "Posted by Moritz Hardt, Research Scientist"
    },
    {
      "title": "Young people who are changing the world through science",
      "date": "Tuesday, August 4, 2015",
      "abstract": "Young people who are changing the world through scienceGoogle for Education BlogGoogle Science Fairannounce the 20 Global FinalistsSpotlight on a Young Scientist series on the Google for Education bloga panel of notable international scientists and scholarsother incredible prizes",
      "link": "http://ai.googleblog.com/2015/08/young-people-who-are-changing-world.html",
      "author": "Posted by Andrea Cohan, Google Science Fair Program Manager"
    },
    {
      "title": "See through the clouds with Earth Engine and Sentinel-1 Data",
      "date": "Monday, August 3, 2015",
      "abstract": "See through the clouds with Earth Engine and Sentinel-1 DataGoogle Earth EngineEuropean Geosciences Union General AssemblyIEEE Geoscience and Remote Sensing SocietyEuropean Commission Joint Research CentreWageningen UniversityUniversity of Paviautilizing the Earth Engine geospatial analysis platformCopernicus Sentinel-1Copernicussign up for Google Earth Engine",
      "link": "http://ai.googleblog.com/2015/08/see-through-clouds-with-earth-engine.html",
      "author": "Posted by Luc Vincent, Engineering Director, Geo Imagery"
    },
    {
      "title": "ICSE 2015 and Software Engineering Research at Google",
      "date": "Thursday, July 30, 2015",
      "abstract": "ICSE 2015 and Software Engineering Research at GoogleInternational Conference on Software EngineeringA Flexible and Non-intrusive Approach for Computing Complex Structural Coverage MetricsAutomated Decomposition of Build TargetsTricorder: Building a Program Analysis EcosystemComparing Software Architecture Recovery Techniques Using Accurate DependenciesSoftware Engineering for Privacy in-the-Large2nd International Workshop on Requirements Engineering and Testing (RET 2015)",
      "link": "http://ai.googleblog.com/2015/07/icse-2015-and-software-engineering.html",
      "author": "Posted by Mohsen Vakilian, Software Engineer"
    },
    {
      "title": "How Google Translate squeezes deep learning onto a phone",
      "date": "Wednesday, July 29, 2015",
      "abstract": "How Google Translate squeezes deep learning onto a phoneannouncedGoogle Translate appimage recognitiontrippy artour data centersSIMD",
      "link": "http://ai.googleblog.com/2015/07/how-google-translate-squeezes-deep.html",
      "author": "Posted by Otavio Good, Software Engineer, Google Translate"
    },
    {
      "title": "The Thorny Issue of CS Teacher Certification",
      "date": "Thursday, July 16, 2015",
      "abstract": "The Thorny Issue of CS Teacher CertificationGoogle for Education Bloggrowing demandComputer Science Teachers Association (CSTA)Bugs in the System: Computer Science Teacher Certification in the U.S.Dr. Aman YadavComputational Thinking for Educators",
      "link": "http://ai.googleblog.com/2015/07/the-thorny-issue-of-cs-teacher.html",
      "author": "Posted by Chris Stephenson, Head of Computer Science Education Programs"
    },
    {
      "title": "Should My Kid Learn to Code?",
      "date": "Tuesday, July 14, 2015",
      "abstract": "Should My Kid Learn to Code?Google for Education BlogHour of CodeMade with Codegrowth in developer bootcampscode.orgCS FirstKhan AcademyCodecademyBlockly GamesTechnovationGirls who CodeBlack Girls Code#YesWeCodemany more computing professionalsexperiencing capacity issuescomputational thinkingCivilizationBlocklycode.orgCS FirstKhan Academy",
      "link": "http://ai.googleblog.com/2015/07/should-my-kid-learn-to-code.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, Google"
    },
    {
      "title": "Simulating fermionic particles with superconducting quantum hardware",
      "date": "Monday, July 13, 2015",
      "abstract": "Simulating fermionic particles with superconducting quantum hardwarefermionic particlesPauli exclusion principlequantum stateanticommutationqubitsQuantum Error CorrectionDigital quantum simulation of fermionic models with a superconducting circuitNature CommunicationsDr. Lucas LamataM.Sc. Laura Garc\u00eda-\u00c1lvarezProf. Enrique SolanoQUTIS groupbosons",
      "link": "http://ai.googleblog.com/2015/07/simulating-fermionic-particles-with.html",
      "author": "Posted by Rami Barends and Julian Kelly, Quantum Electronics Engineers and John Martinis, Research Scientist"
    },
    {
      "title": "The Computer Science Pipeline and Diversity: Part 2 - Some positive signs, and looking towards the future",
      "date": "Thursday, July 9, 2015",
      "abstract": "The Computer Science Pipeline and Diversity: Part 2 - Some positive signs, and looking towards the futureGoogle for Education Blogrecent publicationsPCAST Stem Ed report, 2010Margolis 2003Margolis, 2010U.S. News, 2015Google ScholarHour of CodeMade with Codedeveloper bootcampscode.orgCS FirstKhan AcademyCodecademyBlockly GamesPencilCodelearn coding in their schoolsTechnovationGirls who CodeBlack Girls Code#YesWeCode2013 Taulbee SurveyComputing Research Associationlack of capacity and facultyGuzdial, 20143X3 award programNation\u2019s Report CardNation\u2019s Report Card",
      "link": "http://ai.googleblog.com/2015/07/the-computer-science-pipeline-and.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, Google"
    },
    {
      "title": "The Computer Science Pipeline and Diversity: Part 1 - How did we get here?",
      "date": "Wednesday, July 8, 2015",
      "abstract": "The Computer Science Pipeline and Diversity: Part 1 - How did we get here?Google for Education Blog2013 Taulbee SurveyComputing Research AssociationBureau of Labor Statistics Occupational Projection Report, 2012less than one-third of U.S. eighth graders show proficiency in science and mathematicsNSF: Higher Education in Science and EngineeringHyde 2006Margolis 2003Margolis 2003Mercier 2006Google white paper 2014Liston 2008Jane MargolisStuck in the Shallow EndHour of CodeMade with CodeCS FirstTechnovationGirls who CodeBlack Girls Code#YesWeCode",
      "link": "http://ai.googleblog.com/2015/07/the-computer-science-pipeline-and_8.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, Google"
    },
    {
      "title": "ICML 2015 and Machine Learning Research at Google",
      "date": "Sunday, July 5, 2015",
      "abstract": "ICML 2015 and Machine Learning Research at Google2015 International Conference on Machine LearningInternational Machine Learning SocietyLearning Program Embeddings to Propagate Feedback on Student CodeBilBOWA: Fast Bilingual Distributed Representations without Word AlignmentsAn Empirical Exploration of Recurrent Network ArchitecturesBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftDRAW: A Recurrent Neural Network For Image GenerationVariational Inference with Normalizing FlowsStructural Maxent ModelsWeight Uncertainty in Neural NetworkMADE: Masked Autoencoder for Distribution EstimationFictitious Self-Play in Extensive-Form GamesUniversal Value Function ApproximatorsExtreme Classification: Learning with a Very Large Number of LabelsMachine Learning for EducationWorkshop on Machine Learning Open Source Software 2015: Open EcosystemsMachine Learning for Music RecommendationLarge-Scale Kernel Learning: Challenges and New OpportunitiesJust-In-Time Kernel Regression for Expectation PropagationEuropean Workshop on Reinforcement Learning (EWRL)Workshop on Deep LearningA Neural Conversational ModelMassively Parallel Methods for Deep Reinforcement Learning",
      "link": "http://ai.googleblog.com/2015/07/icml-2015-and-machine-learning-research.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "DeepDream - a code example for visualizing Neural Networks",
      "date": "Wednesday, July 1, 2015",
      "abstract": "DeepDream - a code example for visualizing Neural Networkswe blogged about a visualization toolMIT Computer Science and AI LaboratoryIPython notebookCaffeNumPySciPyPILIPythonAnacondaCanopyCaffeInstallation instructions",
      "link": "http://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html",
      "author": "Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer"
    },
    {
      "title": "Google Computational Journalism Research Awards launch in Europe",
      "date": "Thursday, June 18, 2015",
      "abstract": "Google Computational Journalism Research Awards launch in EuropeDigital News InitiativeWalid MaalejWiebke LoosenIoana ManolescuXavier TannierNeil MaidenGeorge Brock",
      "link": "http://ai.googleblog.com/2015/06/google-computational-journalism.html",
      "author": "Posted by Andrea Held, Google University Relations & Matt Cooke, Google News Lab Europe"
    },
    {
      "title": "Inceptionism: Going Deeper into Neural Networks",
      "date": "Wednesday, June 17, 2015",
      "abstract": "Inceptionism: Going Deeper into Neural NetworksCreative Commons Attribution 4.0 International LicenseMIT Computer Science and AI LaboratoryArtificial Neural Networksimage classificationspeech recognitiongradually adjusting the network parameters[1][2][3][4]Zachi Evenoreorges Seuratneural net architectureInceptionism galleryMIT Computer Science and AI LaboratoryInceptionism gallery",
      "link": "http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html",
      "author": "Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer"
    },
    {
      "title": "New ways to add Reminders in Inbox by Gmail",
      "date": "Wednesday, June 17, 2015",
      "abstract": "New ways to add Reminders in Inbox by Gmailopened upnatural language understanding",
      "link": "http://ai.googleblog.com/2015/06/new-ways-to-add-reminders-in-inbox-by.html",
      "author": "Posted by Dave Orr, Google Research Product Manager"
    },
    {
      "title": "Google Computer Vision research at CVPR 2015",
      "date": "Sunday, June 7, 2015",
      "abstract": "Google Computer Vision research at CVPR 2015Google Photos2015 Conference on Computer Vision and Pattern Recognitioncomputer visionJumpGoogle CardboardApplied Deep Learning for Computer Vision with TorchDIY Deep Learning: a Hands-On Tutorial with CaffeImageNet Large Scale Visual Recognition Challenge TutorialFast Image Processing With HalideOpen Source Structure-from-MotionModeling Local and Global Deformations in Deep Learning: Epitomic Convolution, Multiple Instance Learning, and Sliding Window DetectionGoing Deeper with ConvolutionsDynamicFusion: Reconstruction and Tracking of Non-Rigid Scenes in Real-TimeShow and Tell: A Neural Image Caption GeneratorLong-Term Recurrent Convolutional Networks for Visual Recognition and DescriptionVisual Vibrometry: Estimating Material Properties from Small Motion in VideoFast Bilateral-Space Stereo for Synthetic DefocusLearning Semantic Relationships for Better Action Retrieval in ImagesFaceNet: A Unified Embedding for Face Recognition and ClusteringA Mixed Bag of Emotions: Model, Predict, and Transfer Emotion DistributionsBest-Buddies Similarity for Robust Template MatchingArticulated Motion Discovery Using Pairs of TrajectoriesReflection Removal Using Ghosting CuesP3.5P: Pose Estimation with Unknown Focal LengthMatchNet: Unifying Feature and Metric Learning for Patch-Based MatchingInferring 3D Layout of Building Facades from a Single ImageThe Aperture Problem for Refractive MotionVideo Magnification in Presence of Large MotionsRobust Video Segment Proposals with Painless Occlusion HandlingOntological Supervision for Fine Grained Classification of Street View StorefrontsVIP: Finding Important People in ImagesFusing Subcategory Probabilities for Texture ClassificationBeyond Short Snippets: Deep Networks for Video ClassificationTHUMOS Challenge 2015DeepVision: Deep Learning in Computer Vision 2015Large Scale Visual Commerce (LSVisCom)Large-Scale Video Search and Mining (LSVSM)Vision meets Cognition: Functionality, Physics, Intentionality and CausalityBig Data Meets Computer Vision: 3rd International Workshop on Large Scale Visual Recognition and Retrieval (BigVision 2015)Observing and Understanding Hands in Action (Hands 2015)Fine-Grained Visual Categorization (FGVC3)Large-scale Scene Understanding Challenge (LSUN)Looking from above: when Earth observation meets vision (EARTHVISION)Computer Vision in Vehicle Technology: Assisted Driving, Exploration Rovers, Aerial and Underwater VehiclesWomen in Computer Vision (WiCV)ChaLearn Looking at PeopleFine-Grained Visual Categorization (FGVC3)",
      "link": "http://ai.googleblog.com/2015/06/google-computer-vision-research-at-cvpr.html",
      "author": "Posted by Vincent Vanhoucke, Google Research Scientist"
    },
    {
      "title": "Announcing the 2015 Google European Doctoral Fellows",
      "date": "Friday, June 5, 2015",
      "abstract": "Announcing the 2015 Google European Doctoral FellowsPhD Fellowship programGoogle European Doctoral Fellowship programOfer MeshiCynthia LiemRoland AngstCarola DoerrYair AdatoPeter HosekNeil Houlsby",
      "link": "http://ai.googleblog.com/2015/06/announcing-2015-google-european.html",
      "author": "Posted by David Harper, University Relations and Beate List, Research Programs"
    },
    {
      "title": "A Multilingual Corpus of Automatically Extracted Relations from Wikipedia",
      "date": "Tuesday, June 2, 2015",
      "abstract": "A Multilingual Corpus of Automatically Extracted Relations from WikipediaNatural Language ProcessingQuestion AnsweringGoogle TranslateMultilingual Open Relation Extraction Using Cross-lingual Projection2015 Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language TechnologiestuplesWikipediareleasing a datasetCreative Commons Attribution-ShareAlike 3.0 License",
      "link": "http://ai.googleblog.com/2015/06/a-multilingual-corpus-of-automatically.html",
      "author": "Posted by Shankar Kumar, Google Research Scientist and Manaal Faruqui, Carnegie Mellon University PhD candidate"
    },
    {
      "title": "Sergey and Larry awarded the Seoul Test-of-Time Award from WWW 2015",
      "date": "Friday, May 22, 2015",
      "abstract": "Sergey and Larry awarded the Seoul Test-of-Time Award from WWW 201524th International World Wide Web ConferenceSeoul Test-of-Time AwardThe Anatomy of a Large-Scale Hypertextual Web Search Engine7th WWW conference in Brisbane, AustraliaProfessor Chin-Wan Chung13,000 citationssmall business owners to reach customershelp long lost friends to reuniteempower users to discover answersGmailGoogle MapsGoogle Earth EngineMachine IntelligenceComputer VisionNatural Language Understanding",
      "link": "http://ai.googleblog.com/2015/05/sergey-and-larry-awarded-seoul-test-of.html",
      "author": "Posted by Andrei Broder, Google Distinguished Scientist"
    },
    {
      "title": "Tone: An experimental Chrome extension for instant sharing over audio",
      "date": "Tuesday, May 19, 2015",
      "abstract": "Tone: An experimental Chrome extension for instant sharing over audioToneChromerickrollsDTMFTone extension for Chrome",
      "link": "http://ai.googleblog.com/2015/05/tone-experimental-chrome-extension-for.html",
      "author": "Posted by Alex Kauffmann, Interaction Researcher, and Boris Smus, Software Engineer"
    },
    {
      "title": "Paper to Digital in 200+ languages",
      "date": "Wednesday, May 6, 2015",
      "abstract": "Paper to Digital in 200+ languagesOptical Character RecognitionGoogle Drive200 languagesGoogle Drive APIDrive App for AndroidHidden Markov Modelsspeech recognition systemsorthographicGoogle Handwriting Inputminimum-error-rate traininglet us know",
      "link": "http://ai.googleblog.com/2015/05/paper-to-digital-in-200-languages.html",
      "author": "Posted by Dmitriy Genzel and Ashok Popat, Research Scientists and Dhyanesh Narayanan, Product Manager"
    },
    {
      "title": "Google Handwriting Input in 82 languages on your Android mobile device",
      "date": "Wednesday, April 15, 2015",
      "abstract": "Google Handwriting Input in 82 languages on your Android mobile deviceRNN-based handwriting recognition in GboardGboardthese instructionsGoogle Handwriting Inputlarge-scale language modelingrobust multi-language OCRlarge-scale neural-networksapproximate nearest neighbor searchTranslate AppsAndroidiOSMobile SearchGoogle Input ToolsChromeChromeOSGmail and Docstranslate.google.comDocs symbol pickerSimplifiedTraditionalCantoneseHindiGesture Searchherehere",
      "link": "http://ai.googleblog.com/2015/04/google-handwriting-input-in-82.html",
      "author": "Posted by Thomas Deselaers, Daniel Keysers, Henry Rowley, Li-Lun Wang, Victor C\u0103rbune, Ashok Popat, Dhyanesh Narayanan, Handwriting Team, Google Research"
    },
    {
      "title": "Beyond Short Snippets: Deep Networks for Video Classification",
      "date": "Wednesday, April 8, 2015",
      "abstract": "Beyond Short Snippets: Deep Networks for Video ClassificationConvolutional Neural Networksdetecting and classifying objects in static imagesautomatically learning complex features in picturesBeyond Short Snippets: Deep Networks for Video Classification2015 Computer Vision and Pattern Recognition1feature poolingrecurrent neural networksbest networkspreviously publishedSports 1 millionprevious workLong Short Term Memory unitsoptical flowshort video showing some example outputs1 Research carried out in collaboration with University of Maryland, College Park PhD student Joe Yue-Hei Ng and University of Texas at Austin PhD student Matthew Hausknecht, as part of a Google Software Engineering Internship\u21a91\u21a9",
      "link": "http://ai.googleblog.com/2015/04/beyond-short-snippets-deep-networks-for.html",
      "author": "Posted by Software Engineers George Toderici and Sudheendra Vijayanarasimhan"
    },
    {
      "title": "Skill maps, analytics and more with Google\u2019s Course Builder 1.8",
      "date": "Monday, April 6, 2015",
      "abstract": "Skill maps, analytics and more with Google\u2019s Course Builder 1.8Course Buildersustainable energycomic booksnew versionPower Searching with GooglevideoGoogle BigQueryconfiguringsending datainstructionslet us knowlist of coursestell us",
      "link": "http://ai.googleblog.com/2015/04/skill-maps-analytics-and-more-with.html",
      "author": "Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"
    },
    {
      "title": "Google Computer Science Capacity Awards",
      "date": "Monday, March 16, 2015",
      "abstract": "Google Computer Science Capacity Awardsdramatic increase in undergraduate CS enrollmentsMaGE program",
      "link": "http://ai.googleblog.com/2015/03/google-computer-science-capacity-awards.html",
      "author": "By Maggie Johnson, Director of Education and University Relations and Chris Busselle, Google.org"
    },
    {
      "title": "Announcing the Google MOOC Focused Research Awards",
      "date": "Monday, March 9, 2015",
      "abstract": "Announcing the Google MOOC Focused Research AwardsTsinghua University2014 APAC MOOC Focused Faculty Workshopideas generated at the workshopFocused Research Awardsonline learning at scale",
      "link": "http://ai.googleblog.com/2015/03/announcing-google-mooc-focused-research.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations, and Aimin Zhu, University Relations Manager, APAC"
    },
    {
      "title": "A step closer to quantum computation with Quantum Error Correction",
      "date": "Wednesday, March 4, 2015",
      "abstract": "A step closer to quantum computation with Quantum Error Correctionquantum computation1994qubitsState preservation by repetitive error detection in a superconducting quantum circuitNaturequantum error correctionentanglementsuperpositionXOR gates",
      "link": "http://ai.googleblog.com/2015/03/a-step-closer-to-quantum-computation.html",
      "author": "Posted by Julian Kelly, Rami Barends, and Austin Fowler, Quantum Electronics Engineers"
    },
    {
      "title": "Large-Scale Machine Learning for Drug Discovery",
      "date": "Monday, March 2, 2015",
      "abstract": "Large-Scale Machine Learning for Drug DiscoveryBharath Ramsundarhigh-throughput screensdeep learningvirtual drug screeningmultitaskPande LabStanford UniversityMassively Multitask Networks for Drug Discoverylarge-scale neural network training systemarea under the receiver operating characteristic curveDavid KonerdingSteven KearnesVijay PandeDeep Learning as an Opportunity in Virtual ScreeningMulti-task neural networks for QSAR predictionsDeep neural nets as a method for quantitative structure-activity relationshipsQuo Vadis, Virtual Screening? A Comprehensive Survey of Prospective Applications",
      "link": "http://ai.googleblog.com/2015/03/large-scale-machine-learning-for-drug.html",
      "author": "Posted by Patrick Riley and Dale Webster, Google Research and Bharath Ramsundar, Google Research Intern and Stanford Ph.D. candidate"
    },
    {
      "title": "From Pixels to Actions: Human-level control through Deep Reinforcement Learning",
      "date": "Wednesday, February 25, 2015",
      "abstract": "From Pixels to Actions: Human-level control through Deep Reinforcement LearningBreakoutHuman-level control through deep reinforcement learningNatureRiver RaidBoxingEnduroMnih et al. \u201cHuman-level control through deep reinforcement learning\"Deep Neural NetworksReinforcement Learningmachine learningMnih et al. \u201cHuman-level control through deep reinforcement learning\u201d,\u00a0Nature 26 Feb. 2015Richard Feynman",
      "link": "http://ai.googleblog.com/2015/02/from-pixels-to-actions-human-level.html",
      "author": "Posted by Dharshan Kumaran and Demis Hassabis, Google DeepMind, London"
    },
    {
      "title": "Google Faculty Research Awards: Winter 2015",
      "date": "Thursday, February 19, 2015",
      "abstract": "Google Faculty Research Awards: Winter 2015Google Faculty Research Awardslast roundResearch organizationrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2015/02/google-faculty-research-awards-winter.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Google Science Fair 2015: what will you try?",
      "date": "Wednesday, February 18, 2015",
      "abstract": "Google Science Fair 2015: what will you try?Google for Education BlogGoogle Science Fairsubmit projects onlinePrizesAnn MakosinskiKenneth ShinozukaHarine RavichandranSomething you love, you\u2019re good at, and want to try",
      "link": "http://ai.googleblog.com/2015/02/google-science-fair-2015-what-will-you.html",
      "author": "Posted by Miriam Schneider, Google for Education team"
    },
    {
      "title": "Announcing the 2015 North American Google PhD Fellows",
      "date": "Wednesday, February 18, 2015",
      "abstract": "Announcing the 2015 North American Google PhD FellowsPhD Fellowship programbuilding new intelligence modelschanging the way in which we interact with computersadvancing into faculty positions",
      "link": "http://ai.googleblog.com/2015/02/announcing-2015-north-american-google.html",
      "author": "Posted by Michael Rennaker, Google University Relations"
    },
    {
      "title": "Map of Life: A preview of how to evaluate species conservation with Google Earth Engine",
      "date": "Thursday, January 8, 2015",
      "abstract": "Map of Life: A preview of how to evaluate species conservation with Google Earth EngineMap of LifeGoogle Earth Engineconsensus range mapdifferent pictureevaluationconcerningWorld Parks Congressfollow Map of Life",
      "link": "http://ai.googleblog.com/2015/01/map-of-life-preview-of-how-to-evaluate.html",
      "author": "Posted by Walter Jetz, Dept. of Ecology and Evolutionary Biology, Yale University, and Dave Thau, Developer Advocate, Google Earth Engine, with support from Robert Guralnick, Dept. of Natural History, University of Florida"
    },
    {
      "title": "Little Box Challenge Academic Awards",
      "date": "Tuesday, December 16, 2014",
      "abstract": "Little Box Challenge Academic AwardsIEEE PELSLittle Box Challengeaward programincreasing the power density for DC-\u00adto\u00ad-AC power conversionKhurram K. AfridiHuang-Jen ChiuJos\u00e9 A. CobosPrasad EnjetiJohann W. KolarNeville McNeillTimothy PeshekRobert Pilawa-PodgurskiJ\u00f6rg Roth-StielowGeoff Walkerrequest for proposalsLittle Box Challenge",
      "link": "http://ai.googleblog.com/2014/12/little-box-challenge-academic-awards.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Call for Research Proposals to participate in the Open Web of Things Expedition",
      "date": "Friday, December 12, 2014",
      "abstract": "Call for Research Proposals to participate in the Open Web of Things Expeditiondefies the constraintsconnected systems, services, devices and \u201cthings\u201dthe realization is complexopen call for research proposals",
      "link": "http://ai.googleblog.com/2014/12/call-for-research-proposals-to.html",
      "author": "Posted Vint Cerf, Chief Internet Evangelist, Roy Want and Max Senges, Google Research"
    },
    {
      "title": "Learning Digital Skills online with Google Activate",
      "date": "Thursday, December 11, 2014",
      "abstract": "Learning Digital Skills online with Google ActivateEurostat dataSpanish Ministry of IndustryEOIUniversidad Complutense de MadridInteractive Advertising BureauGoogle ActivateCourse BuilderGoogle Faculty SummitUnimooc Aemprende",
      "link": "http://ai.googleblog.com/2014/12/learning-digital-skills-online-with.html",
      "author": "Posted by Michel Benard, University Relations Manager and Cova Soto, Product Marketing Manager, Business Marketing Madrid"
    },
    {
      "title": "MOOC Research and Innovation",
      "date": "Tuesday, December 9, 2014",
      "abstract": "MOOC Research and InnovationTsinghua University2014 APAC MOOC Focused Faculty Workshop37 professors from 12 countrieshybrid learninggamificationonline learning at scaleCourse Builder",
      "link": "http://ai.googleblog.com/2014/12/mooc-research-and-innovation.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "High Quality Object Detection at Scale",
      "date": "Monday, December 8, 2014",
      "abstract": "High Quality Object Detection at Scale*Scalable, High Quality Object DetectionILSVRCreported top resultsScalable, High Quality Object Detectionautomatically generated image captionsRich feature hierarchies for accurate object detection and semantic segmentationPrime Object Proposals with Randomized Prim\u2019s AlgorithmEdge boxes: Locating object proposals from edgesBING: Binarized normed gradients for objectness estimation at 300fpsScalable Object Detection using Deep Neural NetworksGoing deeper with convolutionsScalable, high quality object detectionOverFeat: Integrated Recognition, Localization and Detection using Convolutional Network*\u21a9",
      "link": "http://ai.googleblog.com/2014/12/high-quality-object-detection-at-scale.html",
      "author": "Google Engineers Christian Szegedy, Scott Reed*, Dumitru Erhan, and Dragomir Anguelov"
    },
    {
      "title": "What we can learn about effective, meaningful and diverse organizations",
      "date": "Thursday, December 4, 2014",
      "abstract": "What we can learn about effective, meaningful and diverse organizationsCatalystMcKinseyCedric Herringstrong correlation between having women on teams and innovationa large proportion of CEOs are taller than the average populationheight is strongly correlated with financial and career successrepeatable testslow representation of women and minorities in ScienceBen Barre\u2019s responseClaude Steeletwo groups of people can have similar or opposite reactions, depending on the way a situation is presentedMIT Science Faculty StudyHarvey Mudd collegeresearchNovember issue of Communications of the Association of Computing Machineryfull bibliography",
      "link": "http://ai.googleblog.com/2014/12/what-we-can-learn-about-effective.html",
      "author": "Posted by Beryl Nelson, Software Engineering Manager"
    },
    {
      "title": "Automatically making sense of data",
      "date": "Tuesday, December 2, 2014",
      "abstract": "Automatically making sense of dataProfessor Zoubin GhahramaniGoogle Focused Research AwardThe Automatic StatisticianSchwabe cycleGaussian processeskernel functionalgorithmreport for the solar irradiance dataThe Automatic Statistician",
      "link": "http://ai.googleblog.com/2014/12/automatically-making-sense-of-data.html",
      "author": "Posted by Kevin Murphy, Research Scientist and David Harper, Head of University Relations, EMEA"
    },
    {
      "title": "Advances in Variational Inference: Working Towards Large-scale Probabilistic Machine Learning at NIPS 2014",
      "date": "Monday, December 1, 2014",
      "abstract": "Advances in Variational Inference: Working Towards Large-scale Probabilistic Machine Learning at NIPS 2014Variational InferenceAdvances in Variational InferenceNeural Information Processing Systemsstochastic gradient methodsstreamtime-series modelsdeep neural networksreinforcement learningContributed talks from 6 speakers34 contributed papersDavid BleiNeil LawrenceShinichi NakajimaMatthias Seegerwww.variationalinference.orgStochastic Backpropagation and Approximate Inference in Deep Generative ModelsDeep AutoRegressive NetworksNeural Variational Inference and Learning in Belief NetworksAuto-Encoding Variational BayesStreaming Variational BayesStochastic Variational Inference",
      "link": "http://ai.googleblog.com/2014/12/advances-in-variational-inference.html",
      "author": "Posted by Shakir Mohamed and Charles Blundell, Google DeepMind, London"
    },
    {
      "title": "A picture is worth a thousand (coherent) words: building a natural description of images",
      "date": "Monday, November 17, 2014",
      "abstract": "A picture is worth a thousand (coherent) words: building a natural description of imagesobject detection, classification, and labelingcomputer visionnatural language processingcomplete image description approachmachine translationRecurrent Neural Networkvector representationConvolutional Neural NetworkSoftmaxBilingual Evaluation Understudyhere",
      "link": "http://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html",
      "author": "Posted by Google Research Scientists Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan"
    },
    {
      "title": "The World Parks Congress: Using technology to protect our natural environment",
      "date": "Wednesday, November 12, 2014",
      "abstract": "The World Parks Congress: Using technology to protect our natural environmentOfficial Google Australia BlogIUCN World Parks CongressGoogle Earth OutreachGoogle Earth EngineworkshopGoogle mapping toolsTrekkerStreet ViewOpen Data KitLive Sydney Seahorse HuntCatlin Seaview SurveySydney Institute of Marine ScienceTune in hereNetworking for nature: the future is coolpanel discussionGlobal Forest WatchGoogle Earth EngineThe Map of LifeThe Jane Goodall InstituteThe World Resources InstituteThe Map of Life",
      "link": "http://ai.googleblog.com/2014/11/the-world-parks-congress-using.html",
      "author": "Posted by Dave Thau, Developer Advocate for Google Earth Engine and Karin Tuxen-Bettman, Program Manager, Google Earth Outreach"
    },
    {
      "title": "Googler Shumin Zhai awarded with the ACM UIST Lasting Impact Award",
      "date": "Monday, November 3, 2014",
      "abstract": "Googler Shumin Zhai awarded with the ACM UIST Lasting Impact Award27th ACM User Interface Software and Technology SymposiumShumin ZhaiPer Ola KristenssonSHARK2: a large vocabulary shorthand writing system for pen-based computersShapeWriterSwypeSwiftKeySlideITTouchPalGoogle KeyboardIBM Almaden Research CenterShapeWriterpublishing prolificallyleading a groupGesture TypingACM Transactions on Computer- InteractionFellow of the ACMCHI Academy",
      "link": "http://ai.googleblog.com/2014/11/googler-shumin-zhai-awarded-with-acm.html",
      "author": "Posted by Alfred Spector, Vice President, Engineering"
    },
    {
      "title": "Google Flu Trends gets a brand new engine",
      "date": "Friday, October 31, 2014",
      "abstract": "Google Flu Trends gets a brand new engineGoogle Flu Trendsnicely complementDengue Trendsoverpredicted123since 2009",
      "link": "http://ai.googleblog.com/2014/10/google-flu-trends-gets-brand-new-engine.html",
      "author": "Posted by Christian Stefansen, Senior Software Engineer"
    },
    {
      "title": "Learning Statistics with Privacy, aided by the Flip of a Coin",
      "date": "Thursday, October 30, 2014",
      "abstract": "Learning Statistics with Privacy, aided by the Flip of a CoinChromium BlogGoogle Online Security Blogprotect our users' security and privacyrandomized responsedifferential privacystrongest form of privacyintense research in academiaon the Internet, nobody should know you\u2019re a dogunwanted softwarehijackingopen-source projectreportACM Conference on Computer and Communications Securityfeedbackrappor-discuss@googlegroups.com",
      "link": "http://ai.googleblog.com/2014/10/learning-statistics-with-privacy-aided.html",
      "author": "Posted by \u00dalfar Erlingsson, Tech Lead Manager, Security Research"
    },
    {
      "title": "HDR+: Low Light and High Dynamic Range photography in the Google Camera App",
      "date": "Monday, October 27, 2014",
      "abstract": "HDR+: Low Light and High Dynamic Range photography in the Google Camera AppProfessor Emeritus, Stanford Universityimage noisehigh dynamic rangeGoogle Camera appoptical image stabilizationAveraging multiple shotslucky imagingHDR+ offHDR+ ondynamic rangebracketingHDR+ off (left)HDR+ on (right)hereherealbumGoogle Camera app",
      "link": "http://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html",
      "author": "Posted by Marc Levoy, Google[x] Software Engineering Manager and Professor Emeritus, Stanford University"
    },
    {
      "title": "Helping teachers teach computer science",
      "date": "Friday, October 24, 2014",
      "abstract": "Helping teachers teach computer scienceGoogle for Education BlogCS4HS (Computer Science for High School)MOOCsJoyce & Showers (2002) and  Wiske, Stone, & Levinson (1993)demographics of CS educatorsCS teacher population",
      "link": "http://ai.googleblog.com/2014/10/helping-teachers-teach-computer-science.html",
      "author": "Unknown"
    },
    {
      "title": "Smart Autofill - Harnessing the Predictive Power of Machine Learning in Google Sheets",
      "date": "Monday, October 13, 2014",
      "abstract": "Smart Autofill - Harnessing the Predictive Power of Machine Learning in Google Sheetsmachine learningSmart Autofill Add-onPrediction APImore in-depth tutorialhere",
      "link": "http://ai.googleblog.com/2014/10/smart-autofill-harnessing-predictive.html",
      "author": "Posted by Konstantin Davydov, Software Engineer and Afshin Rostamizadeh, Research Scientist"
    },
    {
      "title": "All the News that's Fit to Read: A Study of Social Annotations for News Reading",
      "date": "Wednesday, October 8, 2014",
      "abstract": "All the News that's Fit to Read: A Study of Social Annotations for News ReadingAll the News that\u2019s Fit to Read: A Study of Social Annotations for News Reading2013 ACM SIGCHI Conference on Human Factors in Computing Systemsinfluential Google papers from 2013",
      "link": "http://ai.googleblog.com/2014/10/all-news-that-fit-to-read-study-of.html",
      "author": "Posted by Chinmay Kulkarni, Stanford University Ph.D candidate and former Google Intern, and Ed H. Chi, Google Research Scientist"
    },
    {
      "title": "Announcing the Google CS Engagement Small Awards Program",
      "date": "Monday, October 6, 2014",
      "abstract": "Announcing the Google CS Engagement Small Awards ProgramGoogle for Education blogmost enrolled course on campusHarvey Mudd\u2019s graduates2013 Taulbee SurveyRecent findingsResearch indicatesinstructordepartmentCS Engagement Small Grants ProgramCall for Proposals",
      "link": "http://ai.googleblog.com/2014/10/announcing-google-cs-engagement-small.html",
      "author": "Posted by Leslie Yeh Johnson, University Relations"
    },
    {
      "title": "Sudoku, Linear Optimization, and the Ten Cent Diet",
      "date": "Tuesday, September 30, 2014",
      "abstract": "Sudoku, Linear Optimization, and the Ten Cent DietGoogle Apps Developer blogGoogle Developers blogGeorge Stiglerlinear optimizationYouTube video stabilization systemGoogle\u00a0Docs Sudoku add-onSCIPLinear Optimization add-onAPIBruno de Backeror-tools suitefew exampleshereinstall the add-on firstSimplex algorithm",
      "link": "http://ai.googleblog.com/2014/09/sudoku-linear-optimization-and-ten-cent.html",
      "author": "Posted by Jon Orwant, Engineering Manager"
    },
    {
      "title": "Collaborative Mathematics with SageMathCloud and Google Cloud Platform",
      "date": "Monday, September 29, 2014",
      "abstract": "Collaborative Mathematics with SageMathCloud and Google Cloud PlatformGoogle for Education blogGoogle Cloud Platform blogWilliam SteinSageMathCloudPythonLaTeXRGoogle Cloud PlatformGoogle Compute EngineSagepeer-reviewed codeThe Sage NotebookSageMathCloudCoffeeScriptnode.jsPythonCassandraZFSbupCodeMirrorthree.jsdozens of courses",
      "link": "http://ai.googleblog.com/2014/09/collaborative-mathematics-with.html",
      "author": "Posted by Craig Citro, Software Engineer"
    },
    {
      "title": "Introducing Structured Snippets, now a part of Google Web Search",
      "date": "Monday, September 22, 2014",
      "abstract": "Introducing Structured Snippets, now a part of Google Web SearchKnowledge GraphResearch Tool found in Google Docs and Slides",
      "link": "http://ai.googleblog.com/2014/09/introducing-structured-snippets-now.html",
      "author": "Posted by Corinna Cortes, Boulos Harb, Afshin Rostamizadeh, Ken Wilder, and Cong Yu, Google Research"
    },
    {
      "title": "Sign in to edx.org with Google (and Facebook, and...)",
      "date": "Thursday, September 18, 2014",
      "abstract": "Sign in to edx.org with Google (and Facebook, and...)Course BuilderedXOpen edXedx.orgedx.orgOpenIDOAuth 2.0edx.orgauthentication moduleextensibleconfigurable",
      "link": "http://ai.googleblog.com/2014/09/sign-in-to-edxorg-with-google-and.html",
      "author": "Posted by John Cox, Software Engineer"
    },
    {
      "title": "Course Builder now supports the Learning Tools Interoperability (LTI) Specification",
      "date": "Thursday, September 11, 2014",
      "abstract": "Course Builder now supports the Learning Tools Interoperability (LTI) SpecificationCourse BuilderLearning Tools Interoperabilitynow available on GithubIMS Global",
      "link": "http://ai.googleblog.com/2014/09/course-builder-now-supports-learning.html",
      "author": "Posted by John Cox, Software Engineer"
    },
    {
      "title": "Building a deeper understanding of images",
      "date": "Friday, September 5, 2014",
      "abstract": "Building a deeper understanding of imagesILSVRCpaper by Krizhevsky et al.LeNetYann LeCunWei LiuYangqing JiaPierre SermanetScott ReedDrago AnguelovDumitru ErhanAndrew RabinovichmyselfDistBelief infrastructureHebbian principlescale invarianceR-CNN detector by Ross Girshick et al.multibox methodseveral ideas from the work of Andrew Howard\"Scalable Object Detection using Deep Neural Networks\"\"Rich feature hierarchies for accurate object detection and semantic segmentation\"\"Some Improvements on Deep Convolutional Neural Network Based Image Classification\"\"Imagenet classification with deep convolutional neural networks\"",
      "link": "http://ai.googleblog.com/2014/09/building-deeper-understanding-of-images.html",
      "author": "Posted by Christian Szegedy, Software Engineer"
    },
    {
      "title": "Working Together to Support Computer Science Education",
      "date": "Wednesday, September 3, 2014",
      "abstract": "Working Together to Support Computer Science EducationGoogle for Education blogmediapolicy makersStuck in the Shallow End: Education, Race, and ComputingCollege BoardNational Center for Women and Information Technologymore than 16 million U.S. children living in poverty",
      "link": "http://ai.googleblog.com/2014/09/working-together-to-support-computer.html",
      "author": "Posted by Chris Stephenson, Computer Science Education Program Manager"
    },
    {
      "title": "Hardware Initiative at Quantum Artificial Intelligence Lab",
      "date": "Tuesday, September 2, 2014",
      "abstract": "Hardware Initiative at Quantum Artificial Intelligence LabJohn Martinis and his team at UC Santa Barbarasuperconducting quantum electronic components of very high fidelityLondon PrizeD-Wavequantum annealing",
      "link": "http://ai.googleblog.com/2014/09/hardware-initiative-at-quantum.html",
      "author": "Posted by Hartmut Neven, Director of Engineering"
    },
    {
      "title": "Teaching machines to read between the lines (and a new corpus with entity salience annotations)",
      "date": "Monday, August 25, 2014",
      "abstract": "Teaching machines to read between the lines (and a new corpus with entity salience annotations)Penn Treebanklots of linguistic dataNew York Times Annotated Corpususe of the metadataa forum581 word articlemore like 5 in 330,000 wordsTFIDFBecky HammonKnowledge GraphFreebase entity IDsA New Entity Salience Task with Millions of Training Examplesword sense disambiguationpreviously touched onfrom Google Driveour Google Code site",
      "link": "http://ai.googleblog.com/2014/08/teaching-machines-to-read-between-lines.html",
      "author": "Posted by Dan Gillick, Research Scientist, and Dave Orr, Product Manager"
    },
    {
      "title": "Google Research Awards: Summer 2014",
      "date": "Wednesday, August 20, 2014",
      "abstract": "Google Research Awards: Summer 2014Google Research AwardsGoogle Glassrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2014/08/google-research-awards-summer-2014.html",
      "author": "posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Summer Games: Learn to Program",
      "date": "Monday, August 11, 2014",
      "abstract": "Summer Games: Learn to ProgramBlockly GamesComputer Science Education weekKlingon",
      "link": "http://ai.googleblog.com/2014/08/summer-games-learn-to-program.html",
      "author": "Posted by Jennifer Vaden Barth, Executive Assistant"
    },
    {
      "title": "Doing Data Science with coLaboratory",
      "date": "Friday, August 8, 2014",
      "abstract": "Doing Data Science with coLaboratorycoLaboratory projectMatthew TurkytIPythonJupytercoLaboratory Chrome AppPortable Native Client (PNaCl)collaborative data sciencezero dependency python",
      "link": "http://ai.googleblog.com/2014/08/doing-data-science-with-colaboratory.html",
      "author": "Posted by Kayur Patel, Kester Tong, Mark Sandler, and Corinna Cortes, Google Research"
    },
    {
      "title": "Facilitating Genomics Research with Google Cloud Platform",
      "date": "Wednesday, July 30, 2014",
      "abstract": "Facilitating Genomics Research with Google Cloud PlatformcomprehensivesequencingstudiesSingle Nucleotide VariantsStructural Variationsprimary causes of cancerOntario Institute for Cancer ResearchUniversity of California Santa CruzSage BionetworksIBM-DREAMOregon Health and Sciences UniversityDREAM Somatic Mutation Calling Challengewhole-genome sequencingGlobal Alliance for Genomics and Healthsimple web-based APIGenomics API Browserregister here",
      "link": "http://ai.googleblog.com/2014/07/facilitating-genomics-research-with.html",
      "author": "Posted by Paul C. Boutros, Ontario Institute for Cancer Research, Josh Stuart, UC Santa Cruz, Adam Margolin, Oregon Health & Science University; Nicole Deflaux and Jonathan Bingham, Google Cloud Platform and Google Genomics"
    },
    {
      "title": "Focus Areas for Policy & Standards Research Proposals",
      "date": "Thursday, July 24, 2014",
      "abstract": "Focus Areas for Policy & Standards Research ProposalsFaculty Research AwardsFaculty Research Awards websiteGoogle Policy Fellowshiprecent post on the Google Europe Blog",
      "link": "http://ai.googleblog.com/2014/07/focus-areas-for-policy-standards.html",
      "author": "Posted by Vint Cerf, VP & Chief Internet Evangelist"
    },
    {
      "title": "Academics and the Little Box Challenge",
      "date": "Tuesday, July 22, 2014",
      "abstract": "Academics and the Little Box ChallengeLittle Box Challengeover-a-century-long feudWide-bandgapLittle Box Challenge awards for academicsLittle Box Challengethese posters",
      "link": "http://ai.googleblog.com/2014/07/academics-and-little-box-challenge.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Simple is better - Making your web forms easy to use pays off",
      "date": "Monday, July 14, 2014",
      "abstract": "Simple is better - Making your web forms easy to use pays off20 guidelines to optimize web formsCHI 2014Designing usable web forms: empirical evaluation of web form improvement guidelineseye-tracking technologynumber of fixationssaccades",
      "link": "http://ai.googleblog.com/2014/07/simple-is-better-making-your-web-forms.html",
      "author": "Posted by Javier Bargas-Avila and Mirjam Seckler, User Experience Research at Google"
    },
    {
      "title": "Influential Papers for 2013",
      "date": "Monday, June 30, 2014",
      "abstract": "Influential Papers for 2013publicationsOnline Matching and Ad AllocationFast, Accurate Detection of 100,000 Object Classes on a Single MachinePhoton: Fault-tolerant and Scalable Joining of Continuous Data StreamsOmega: flexible, scalable schedulers for large compute clustersFFitts Law: Modeling Finger Touch with Fitts' LawTop-k Publish-Subscribe for Social Annotation of NewsAd Click Prediction: a View from the TrenchesLearning kernels using local rademacher complexityEfficient Estimation of Word Representations in Vector SpaceLarge-Scale Learning with Less RAM via RandomizationSource-Side Classifier Preordering for Machine TranslationToken and Type Constraints for Cross-Lingual Part-of-Speech TaggingUniversal Dependency Annotation for Multilingual ParsingB4: Experience with a Globally Deployed Software Defined WANWhen the Cloud Goes Local: The Global Problem with Data LocalizationCloud-based robot grasping with the google object recognition engineAlice in Warningland: A Large-Scale Field Study of Browser Security Warning EffectivenessArrival and departure dynamics in Social NetworksAll the news that's fit to read: a study of social annotations for news readingDoes Bug Prediction Support Human Developers? Findings from a Google Case StudyStatistical Parametric Speech Synthesis Using Deep Neural NetworksAccurate and Compact Large Vocabulary Speech Recognition on Mobile DevicesPay by the Bit: An Information-Theoretic Metric for Collective Human JudgmentF1: A Distributed SQL Database That Scales",
      "link": "http://ai.googleblog.com/2014/06/influential-papers-for-2013.html",
      "author": "Posted by Corinna Cortes and Alfred Spector, Google Research"
    },
    {
      "title": "2014 Google PhD Fellowships: Supporting the Future of Computer Science",
      "date": "Wednesday, June 18, 2014",
      "abstract": "2014 Google PhD Fellowships: Supporting the Future of Computer Science2014 Google PhD Fellowship recipientsCynthia LiemIan GoodfellowDelft University of TechnologyUniversit\u00e9 de Montr\u00e9al in Qu\u00e9becDr. Douglas EckACM MIRUM WorkshopJeff Deantranscribing the address numbers on houses from Google Street View photospublicationInternational Conference on Learning Representations",
      "link": "http://ai.googleblog.com/2014/06/2014-google-phd-fellowships-supporting.html",
      "author": "Posted by David Harper, Google University Relations & Beate List, Google Research Programs"
    },
    {
      "title": "A skill-based approach to creating open online courses",
      "date": "Tuesday, May 27, 2014",
      "abstract": "A skill-based approach to creating open online coursesresearchCarnegie MellonStanfordwhite paperEngineering Statics course",
      "link": "http://ai.googleblog.com/2014/05/a-skill-based-approach-to-creating-open.html",
      "author": "Posted by Sean Lip, Software Engineer, Open Online Education"
    },
    {
      "title": "A Billion Words: Because today's language modeling standard should be higher",
      "date": "Wednesday, April 30, 2014",
      "abstract": "A Billion Words: Because today's language modeling standard should be higherpronounce \u201cladder\u201d and \u201clatter\u201d identicallyIME keyboardslanguage modelswe are releasing scriptsarXiv paperhttp://www.statmt.org/lm-benchmark/",
      "link": "http://ai.googleblog.com/2014/04/a-billion-words-because-today-language.html",
      "author": "Posted by Dave Orr, Product Manager, and Ciprian Chelba, Research Scientist"
    },
    {
      "title": "Lens Blur in the new Google Camera app",
      "date": "Wednesday, April 16, 2014",
      "abstract": "Lens Blur in the new Google Camera appSLR camerasGoogle Cameratilt-shiftdepthbundle adjustmentMulti-View StereotriangulateMarkov Random Fieldthin lensPhoto ToursGoogle Earth",
      "link": "http://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html",
      "author": "Posted by Carlos Hern\u00e1ndez, Software Engineer"
    },
    {
      "title": "Sawasdeee ka Voice Search",
      "date": "Wednesday, April 2, 2014",
      "abstract": "Sawasdeee ka Voice Search",
      "link": "http://ai.googleblog.com/2014/04/sawasdeee-ka-voice-search.html",
      "author": "Posted by Keith Hall and Richard Sproat, Staff Research Scientists, Speech"
    },
    {
      "title": "Making Blockly Universally Accessible",
      "date": "Tuesday, April 1, 2014",
      "abstract": "Making Blockly Universally AccessibleQo'noSgharghtlhIngan maHherehere",
      "link": "http://ai.googleblog.com/2014/04/making-blockly-universally-accessible.html",
      "author": "Posted by Neil Fraser, Chief Interplanetary Liaison"
    },
    {
      "title": "Celebrating the First Set of Google Geo Education Awardees and Announcing Round Two",
      "date": "Monday, March 31, 2014",
      "abstract": "Celebrating the First Set of Google Geo Education Awardees and Announcing Round Twoapply for an awardremote sensing courseherehereapply for a GeoEDU award",
      "link": "http://ai.googleblog.com/2014/03/celebrating-first-set-of-google-geo.html",
      "author": "Posted by Dave Thau, Senior Developer Advocate"
    },
    {
      "title": "Making Sense of MOOC Data",
      "date": "Thursday, March 27, 2014",
      "abstract": "Making Sense of MOOC DataLearning@Scale conferencelearners\u2019 goals and activitiesself-evaluation as an assessment toolMaking Sense of Dataposted previouslyMapping with Google courseIntroduction to Web Accessibility courseprevious researchself-evaluationenhanced metacognitionMaking Sense of DatacbXOppia",
      "link": "http://ai.googleblog.com/2014/03/making-sense-of-mooc-data.html",
      "author": "Posted by Julia Wilkowski, Staff Instructional Designer"
    },
    {
      "title": "Berkeley Earth Maps Powered by Google Maps Engine now available in the Google Maps Gallery",
      "date": "Thursday, March 20, 2014",
      "abstract": "Berkeley Earth Maps Powered by Google Maps Engine now available in the Google Maps GalleryGoogle Maps EngineGoogle Maps GalleryBerkeley EarthBerkeley Earthscalable maps of climate and weather information",
      "link": "http://ai.googleblog.com/2014/03/berkeley-earth-maps-powered-by-google.html",
      "author": "Posted by Dr. Robert Rohde, Berkeley Earth"
    },
    {
      "title": "Computer Science Education Recharged!",
      "date": "Tuesday, March 11, 2014",
      "abstract": "Computer Science Education Recharged!SIGCSECS PrinciplesCSTAcode.orgHour of CodeExploring Computer ScienceCS FirstCS4HS",
      "link": "http://ai.googleblog.com/2014/03/computer-science-education-recharged.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Google joins the Global Alliance for Genomics and Health",
      "date": "Thursday, February 27, 2014",
      "abstract": "Google joins the Global Alliance for Genomics and HealthHuman Genome ProjectGlobal Alliance for Genomics and Healthsimple web-based API1,000 Genomes Projectopen-source sample projectsthe APIfill out this simple formcontact us",
      "link": "http://ai.googleblog.com/2014/02/google-joins-global-alliance-for.html",
      "author": "Posted by Jonathan Bingham, Product Manager"
    },
    {
      "title": "Making Sense of Data with Google",
      "date": "Tuesday, February 25, 2014",
      "abstract": "Making Sense of Data with GoogleedXMaking Sense of DatacbXg.co/datasense",
      "link": "http://ai.googleblog.com/2014/02/making-sense-of-data-with-google.html",
      "author": "Posted by John Atwood, Program Manager"
    },
    {
      "title": "Monitoring the World's Forests with Global Forest Watch",
      "date": "Thursday, February 20, 2014",
      "abstract": "Monitoring the World's Forests with Global Forest WatchCross-posted at the Google Lat Long BlogGlobal Forest WatchGoogle Earth EngineGoogle Maps EngineGlobal forest loss far exceeds forest gaindataThe United States\u2019 most heavily forested region is made up of production forestsanalysismultiple ecosystem services strategyForests are protected in Brazil\u2019s indigenous territoriesImazonMake Your Own Forest Mapmake your own forest map",
      "link": "http://ai.googleblog.com/2014/02/monitoring-world-forests-with-global.html",
      "author": "Posted by Crystal Davis, Director of Global Forest Watch, the World Resources Institute, and Dave Thau, Developer Advocate, Google Earth Engine"
    },
    {
      "title": "Google Award Program stimulates Journalism and CS collaboration",
      "date": "Wednesday, February 19, 2014",
      "abstract": "Google Award Program stimulates Journalism and CS collaborationLarry BirnbaumIrfan EssaSusan McGregorInfoScribePaul ResnickRumorLensRyan Thornburghttp://open-nc.org",
      "link": "http://ai.googleblog.com/2014/02/google-award-program-stimulates.html",
      "author": "Posted by Krishna Bharat, Distinguished Research Scientist"
    },
    {
      "title": "Google Research Awards: Winter 2014",
      "date": "Tuesday, February 18, 2014",
      "abstract": "Google Research Awards: Winter 2014Google Research Awardsrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2014/02/google-research-awards-winter-2014.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Explore the history of Pop -- and Punk, Jazz, and Folk -- with the Music Timeline",
      "date": "Thursday, January 16, 2014",
      "abstract": "Explore the history of Pop -- and Punk, Jazz, and Folk -- with the Music TimelineMusic TimelineMetalClassic MetalHair MetalAlt MetalR&BElectronicaresearch.google.com/bigpictureMusic Recommendations and Discovery team",
      "link": "http://ai.googleblog.com/2014/01/explore-history-of-pop-and-punk-jazz.html",
      "author": "Posted by Alison Cichowlas and Tony Lam, Google Research"
    },
    {
      "title": "Piloting after school clubs to ignite interest in Computer Science",
      "date": "Wednesday, January 15, 2014",
      "abstract": "Piloting after school clubs to ignite interest in Computer ScienceCS FirstSouth Carolina data centerApp InventorScratchBlocklyPython in CodacademyBootstrapFinch RobotsLittle BitsSpheroRaspberry PiGoogle Computer Science Teaching Fellows",
      "link": "http://ai.googleblog.com/2014/01/piloting-after-school-clubs-to-ignite.html",
      "author": "Posted by JamieSue Goodman, Program Lead, CS First"
    },
    {
      "title": "Groundbreaking simulations by Google Exacycle Visiting Faculty",
      "date": "Monday, December 16, 2013",
      "abstract": "Groundbreaking simulations by Google Exacycle Visiting FacultyannouncedGoogle Exacycle for Visiting Facultyenables massive parallelism for doing science in the cloudproposalsKai Kohlhoffsignalling proteinconformational changesbeta-2 adrenergic receptorGPCRNature Chemistrywebsite",
      "link": "http://ai.googleblog.com/2013/12/groundbreaking-simulations-by-google.html",
      "author": "Posted by David Konerding, Staff Software Engineer"
    },
    {
      "title": "Googler Moti Yung elected as 2013 ACM Fellow",
      "date": "Wednesday, December 11, 2013",
      "abstract": "Googler Moti Yung elected as 2013 ACM FellowreleasedResearch Scientist Moti Yungtraitor tracingthreshold cryptosystemszero knowledge proofs.",
      "link": "http://ai.googleblog.com/2013/12/googler-moti-yung-elected-as-2013-acm.html",
      "author": "Posted by Alfred Spector, VP of Engineering"
    },
    {
      "title": "Free Language Lessons for Computers",
      "date": "Tuesday, December 3, 2013",
      "abstract": "Free Language Lessons for ComputersWilliam Bruce Camerontell storiesvisualize informationmailing listWikipediahttps://code.google.com/p/relation-extraction-corpus/handy blog postFreebase concept IDsClueWeb09 FACCClueWeb12 FACCUCI machine learning repository (multiview video dataset)Google\u2019s repositoryhereWikiLinks corpusUmass Wiki-linksblog post announcing the releaseDownload from GoogleWikimedia DeutschlandpostedpaperGoogle BooksGoogle Ngram Viewerhttp://commondatastorage.googleapis.com/books/syntactic-ngrams/index.htmlblog postpaper about the releasehttp://nlp.stanford.edu/pubs/crosswikis-data.tar.bz2blog postassociated paperFreebase annotationsset of Freebase triples",
      "link": "http://ai.googleblog.com/2013/12/free-language-lessons-for-computers.html",
      "author": "Posted by Dave Orr, Google Research Product Manager"
    },
    {
      "title": "Released Data Set: Features Extracted From YouTube Videos for Multiview Learning",
      "date": "Tuesday, November 26, 2013",
      "abstract": "Released Data Set: Features Extracted From YouTube Videos for Multiview LearningThe \u201cduck test\u201dUCI machine learning repository (multiview video dataset)here",
      "link": "http://ai.googleblog.com/2013/11/released-data-set-features-extracted.html",
      "author": "Posted by Omid Madani, Senior Software Engineer"
    },
    {
      "title": "The MiniZinc Challenge",
      "date": "Monday, November 25, 2013",
      "abstract": "The MiniZinc Challengeor-toolsSATinteger programmingwikipedia pageor-toolshere",
      "link": "http://ai.googleblog.com/2013/11/the-minizinc-challenge.html",
      "author": "Posted by Jon Orwant, Engineering Manager"
    },
    {
      "title": "New Research Challenges in Language Understanding",
      "date": "Friday, November 22, 2013",
      "abstract": "New Research Challenges in Language UnderstandingagendaFaculty Research Awards program",
      "link": "http://ai.googleblog.com/2013/11/new-research-challenges-in-language.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Unique Strategies for Scaling Teacher Professional Development",
      "date": "Tuesday, November 19, 2013",
      "abstract": "Unique Strategies for Scaling Teacher Professional DevelopmentResearch showsCourse BuilderCreative ComputingNational GeographicAnnenberg LearnerWater: The Essential ResourceThe Friday InstituteDigital Learning Transitionspost-course surveycourse dataDigital Technology coursenew Australian curriculuma suite of coursescourseCommon Core State Standards",
      "link": "http://ai.googleblog.com/2013/11/unique-strategies-for-scaling-teacher.html",
      "author": "Posted by Candice Reimers, Senior Program Manager"
    },
    {
      "title": "Moore\u2019s Law Part 4: Moore's Law in other domains",
      "date": "Friday, November 15, 2013",
      "abstract": "Moore\u2019s Law Part 4: Moore's Law in other domainsfor Moore\u2019s LawEd ParsonsMetcalfe\u2019s LawInternet of ThingsSverre JarpCERNLEPHEPKaty B\u00f6rnerInformation Visualization Massive Open Online Course (MOOC)Professor Francesco StellacciRoss Koningsteincatalysishydrocarbonoil refinery processesGoogle VenturesCool Planet Energy SystemsResearch at Google Google+ page",
      "link": "http://ai.googleblog.com/2013/11/moores-law-part-4-moore-law-in-other.html",
      "author": "Unknown"
    },
    {
      "title": "The first detailed maps of global forest change",
      "date": "Thursday, November 14, 2013",
      "abstract": "The first detailed maps of global forest changeClick to enlargeLandsat 7Google Earth EngineClick to enlargehttp://earthenginepartners.appspot.com/science-2013-global-foresthttp://goo.gl/JbWWTkhttp://goo.gl/rhxK5XHigh-Resolution Global Maps of 21st-Century Forest Cover Change",
      "link": "http://ai.googleblog.com/2013/11/the-first-detailed-maps-of-global.html",
      "author": "Posted by Matt Hansen and Peter Potapov, University of Maryland; Rebecca Moore and Matt Hancher, Google"
    },
    {
      "title": "Moore\u2019s Law, Part 3: Possible extrapolations over the next 15 years and impact",
      "date": "Wednesday, November 13, 2013",
      "abstract": "Moore\u2019s Law, Part 3: Possible extrapolations over the next 15 years and impactOverall Roadmap Technology CharacteristicsORTC 2011 tablesDRAMmulti-level cellResearch at Google Google+ page",
      "link": "http://ai.googleblog.com/2013/11/moores-law-part-3-possible.html",
      "author": "Unknown"
    },
    {
      "title": "Moore\u2019s Law, Part 2: More Moore and More than Moore",
      "date": "Tuesday, November 12, 2013",
      "abstract": "Moore\u2019s Law, Part 2: More Moore and More than MooreInternational Technology Roadmap for Semiconductors 2012CMOSvery promising tunnel transistorsstack layers of transistorsBoolean logicquantum computingincreasing the number of statesMEMSRF/AMSITRS Overall Roadmap Technology Characteristics (ORTC) 2012Research at Google Google+ page",
      "link": "http://ai.googleblog.com/2013/11/moores-law-part-2-more-moore-and-more.html",
      "author": "Unknown"
    },
    {
      "title": "Moore\u2019s Law, Part 1: Brief history of Moore's Law and current state",
      "date": "Monday, November 11, 2013",
      "abstract": "Moore\u2019s Law, Part 1: Brief history of Moore's Law and current statehistory of computing hardwareWikipediaGordon E. Moore1965 paperpixels in digital camerasOther formulations and similar lawsworld economyNTRSITRSsources in 20052010 update to the ITRSMore than MooreSiPSoCCMOSResearch at Google Google+ page",
      "link": "http://ai.googleblog.com/2013/11/moores-law-part-1-brief-history-of.html",
      "author": "Unknown"
    },
    {
      "title": "Enhancing Linguistic Search with the Google Books Ngram Viewer",
      "date": "Thursday, October 17, 2013",
      "abstract": "Enhancing Linguistic Search with the Google Books Ngram Viewerwhat noun most often follows \u201cQueen\u201d in English fictionhis Atlantic articlethe phrase \u201cchanging roles\u201d has recently surged in popularity in English fictionwhen we add non-fiction into the mixcommon capitalizations of \u201cMother Earth\u201d",
      "link": "http://ai.googleblog.com/2013/10/enhancing-linguistic-search-with-google.html",
      "author": "Posted by Slav Petrov and Dipanjan Das, Research Scientists"
    },
    {
      "title": "Opening up Course Builder data",
      "date": "Wednesday, October 9, 2013",
      "abstract": "Opening up Course Builder dataCourse Builderwrote a postbuild data processing pipelineslearn from the courses we\u2019ve run",
      "link": "http://ai.googleblog.com/2013/10/opening-up-course-builder-data.html",
      "author": "Posted by John Cox and Pavel Simakov, Course Builder Team, Google Research"
    },
    {
      "title": "Projecting without a projector: sharing your smartphone content onto an arbitrary display",
      "date": "Thursday, September 26, 2013",
      "abstract": "Projecting without a projector: sharing your smartphone content onto an arbitrary displayDeep ShotOpen Project",
      "link": "http://ai.googleblog.com/2013/09/projecting-without-projector-sharing.html",
      "author": "Posted by Yang Li, Research Scientist, Google Research"
    },
    {
      "title": "Broadening Google Patents",
      "date": "Tuesday, September 17, 2013",
      "abstract": "Broadening Google PatentsUS Public Policy BlogEuropean Public Policy BlogInside Search BlogGoogle PatentsPrior Art FinderChinese dual-drive bicycleGerman valve for inflating bicycle tiresCanadian trailer to your bikeWIPO application for pedalling with one legGoogle Translate",
      "link": "http://ai.googleblog.com/2013/09/broadening-google-patents.html",
      "author": "Posted by Jon Orwant, Engineering Manager"
    },
    {
      "title": "We are joining the Open edX platform",
      "date": "Tuesday, September 10, 2013",
      "abstract": "We are joining the Open edX platformCourse BuilderIntroduction to Web AccessibilityedXthe findings of which",
      "link": "http://ai.googleblog.com/2013/09/we-are-joining-open-edx-platform.html",
      "author": "Posted by Dan Clancy, Director of Research"
    },
    {
      "title": "Make Your Websites More Accessible to More Users with Introduction to Web Accessibility",
      "date": "Tuesday, September 10, 2013",
      "abstract": "Make Your Websites More Accessible to More Users with Introduction to Web AccessibilityGoogle Developer's BlogEnglandGermanyJapanIntroduction to Web AccessibilityRegistration",
      "link": "http://ai.googleblog.com/2013/09/make-your-websites-more-accessible-to.html",
      "author": "Eve Andersson, Manager, Accessibility Engineering"
    },
    {
      "title": "A Comparison of Five Google Online Courses",
      "date": "Thursday, September 5, 2013",
      "abstract": "A Comparison of Five Google Online Courses",
      "link": "http://ai.googleblog.com/2013/09/a-comparison-of-five-google-online.html",
      "author": "Posted by Julia Wilkowski, Senior Instructional Designer"
    },
    {
      "title": "Google Research Awards: Summer 2013",
      "date": "Monday, August 12, 2013",
      "abstract": "Google Research Awards: Summer 2013Google Research AwardsAndroid-basedGoogle Glassrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2013/08/google-research-awards-summer-2013.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Computer Science Teaching Fellows Starting Up in Charleston, SC",
      "date": "Wednesday, August 7, 2013",
      "abstract": "Computer Science Teaching Fellows Starting Up in Charleston, SCSouth Carolina data centerhttp://www.cra.org/resources/NSFCS PrinciplesCSTAstandardsreportComputing in the CoreCode.orgElementary and Secondary School Actsupport CS educationMOOCsmachine learningKhan Academy",
      "link": "http://ai.googleblog.com/2013/08/computer-science-teaching-fellows.html",
      "author": "Posted by Cameron Fadjo, Program Lead, Computer Science Teaching Fellows"
    },
    {
      "title": "Under the hood of Croatian, Filipino, Ukrainian, and Vietnamese in Google Voice Search",
      "date": "Thursday, July 25, 2013",
      "abstract": "Under the hood of Croatian, Filipino, Ukrainian, and Vietnamese in Google Voice Searchtonal languagetonemescode switchingneural networkdiscovered cats",
      "link": "http://ai.googleblog.com/2013/07/under-hood-of-croatian-filipino.html",
      "author": "Posted by Eugene Weinstein and Pedro Moreno, Google Speech Team"
    },
    {
      "title": "11 Billion Clues in 800 Million Documents: A Web Research Corpus Annotated with Freebase Concepts",
      "date": "Wednesday, July 17, 2013",
      "abstract": "11 Billion Clues in 800 Million Documents: A Web Research Corpus Annotated with Freebase ConceptsPlatoKnowledge GraphFreebasedata to help with disambiguation$1.2M in research grantsClueWeb09 FACCClueWeb12 FACCFreebase MID\u2019sTREC query setsMillion Query TrackWeb TrackWikilinks CorpusClueWeb09 FACCClueWeb12 FACCdata release mailing list",
      "link": "http://ai.googleblog.com/2013/07/11-billion-clues-in-800-million.html",
      "author": "Posted by Dave Orr, Amar Subramanya, Evgeniy Gabrilovich, and Michael Ringgaard, Google Research"
    },
    {
      "title": "New research from Google shows that 88% of the traffic generated by mobile search ads is not replaced by traffic originating from mobile organic search",
      "date": "Tuesday, July 16, 2013",
      "abstract": "New research from Google shows that 88% of the traffic generated by mobile search ads is not replaced by traffic originating from mobile organic searchSearch Ads PauseIncremental Clicks Impact of Mobile Search Advertising",
      "link": "http://ai.googleblog.com/2013/07/new-research-from-google-shows-that-88.html",
      "author": "Posted by Shaun Lysen, Statistician at Google"
    },
    {
      "title": "Google Databoard: A new way to explore industry research",
      "date": "Tuesday, July 9, 2013",
      "abstract": "Google Databoard: A new way to explore industry researchDataboard for Research InsightsThe New Multi-screen WorldMobile In-store shopper researchMobile search momentsvisit the Databoard now",
      "link": "http://ai.googleblog.com/2013/07/google-databoard-new-way-to-explore.html",
      "author": "Posted by Adam Grunewald, Mobile Marketing Manager"
    },
    {
      "title": "Conference Report: USENIX Annual Technical Conference (ATC) 2013",
      "date": "Wednesday, July 3, 2013",
      "abstract": "Conference Report: USENIX Annual Technical Conference (ATC) 2013USENIX Annual Technical ConferenceUSENIX Federated Conference WeekAutonomic ComputingFeedback ComputingJanus: Optimal Flash Provisioning for Cloud Storage Workloadspacketdrill: Scriptable Network Stack Testing, from Sockets to PacketsTCPsource code and test scripts for packetdrillNicPic: Scalable and Accurate End-Host Rate LimitingAGILE: Elastic Distributed Resource Scaling for Infrastructure-as-a-ServiceGoogle PhD fellowshipMiG: Efficient Migration of Desktop VM Using Semantic Compressionopen accessconference website",
      "link": "http://ai.googleblog.com/2013/07/conference-report-usenix-annual.html",
      "author": "Posted by Murray Stokely, Google Storage Analytics Team"
    },
    {
      "title": "Natural Language Understanding-focused awards announced",
      "date": "Tuesday, July 2, 2013",
      "abstract": "Natural Language Understanding-focused awards announcedKnowledge Graph",
      "link": "http://ai.googleblog.com/2013/07/natural-language-understanding-focused.html",
      "author": "Posted by Massimiliano Ciaramita, Research Scientist and David Harper, Head University Relations (EMEA)"
    },
    {
      "title": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machine",
      "date": "Thursday, June 27, 2013",
      "abstract": "Fast, Accurate Detection of 100,000 Object Classes on a Single Machinehere",
      "link": "http://ai.googleblog.com/2013/06/fast-accurate-detection-of-100000.html",
      "author": "Posted by Tom Dean, Google Research"
    },
    {
      "title": "Some Innovative MOOCs",
      "date": "Tuesday, June 18, 2013",
      "abstract": "Some Innovative MOOCsMOOCsPower SearchingCourse Builderlarge number of coursesGivingWithPurposeLearning By Givinggiving it all awayevaluating and selecting soccer playerssearch for a job,develop digital learning opportunitiesGame TheoryInformation Visualization",
      "link": "http://ai.googleblog.com/2013/06/some-innovative-moocs.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Excellent Papers for 2012",
      "date": "Thursday, June 13, 2013",
      "abstract": "Excellent Papers for 2012publicationsset of paperssecond roundagain in 2011publications listOnline Matching with Stochastic RewardsMatching with our Eyes ClosedSimultaneous Approximations for Adversarial and Stochastic Online Budgeted AllocationPolyhedral Clinching Auctions and the Adwords PolytopeBacktracking Events as Indicators of Usability Problems in Creation-Oriented ApplicationsTalking in Circles: Selective Sharing in Google+Online selection of diverse resultsLarge Scale Distributed Deep NetworksOpen Problem: Better Bounds for Online Logistic RegressionSpectral Learning of General Weighted Automata via Constrained Matrix CompletionImproved Domain Adaptation for Statistical Machine TranslationReconstructing the World's MuseumsThe intervalgram: An audio feature for large-scale melody recognitionGeneral and Nested Wiberg MinimizationCalibration-Free Rolling Shutter RemovalVine Pruning for Efficient Multi-Pass Dependency ParsingCross-lingual Word Clusters for Direct Transfer of Linguistic Structure How to Split a FlowDeadline-Aware Datacenter TCP (D2TCP)Trickle: Rate Limiting YouTube Video StreamingLook Who I Found: Understanding the Effects of Sharing Curated Friend Groups AddressSanitizer: A Fast Address Sanity Checker Japanese and Korean Voice SearchGoogle's Cross-Dialect Arabic Voice SearchDeep Neural Networks for Acoustic Modeling in Speech RecognitionEmpowering Online Advertisements by Empowering Viewers with the Right to ChooseEfficient Spatial Sampling of Large Geographical TablesSpanner: Google's Globally-Distributed Database",
      "link": "http://ai.googleblog.com/2013/06/excellent-papers-for-2012.html",
      "author": "Posted by Corinna Cortes and Alfred Spector, Google Research"
    },
    {
      "title": "Improving Photo Search: A Step Across the Semantic Gap",
      "date": "Wednesday, June 12, 2013",
      "abstract": "Improving Photo Search: A Step Across the Semantic GapGoogle I/Omajor upgrade to the photos experiencesearch your own photosmany yearsImage Searchdeep learningconvolutional neural networksImageNet computer vision competitionwinning teamProfessor Geoffrey Hintonsoftware infrastructureJeff DeanAndrew Ngacquired the rights to the technologyphotos.google.comProfessor Yann LeCunreading handwritten letters and digitsImageNet Large Scale Visual RecognitionentitiesFreebase entitiesKnowledge Graph/m/0449p/m/012x34linear classifiermistaking a goat for a dog or a millipede for a snake",
      "link": "http://ai.googleblog.com/2013/06/improving-photo-search-step-across.html",
      "author": "Posted by Chuck Rosenberg, Image Search Team"
    },
    {
      "title": "2013 Google PhD Fellowships: 5 Years of Supporting the Future of Computer Science",
      "date": "Tuesday, June 11, 2013",
      "abstract": "2013 Google PhD Fellowships: 5 Years of Supporting the Future of Computer Science2013 Global Google PhD FellowsPhD Fellowship ProgramRoxana GeambasuComputer Science DepartmentRoland AngstMax Planck Center for Visual Computing and Communicationfaculty award opportunities",
      "link": "http://ai.googleblog.com/2013/06/2013-google-phd-fellowships-5-years-of.html",
      "author": "Posted by Michael Rennaker, Google University Relations"
    },
    {
      "title": "Building A Visual Planetary Time Machine",
      "date": "Monday, June 10, 2013",
      "abstract": "Building A Visual Planetary Time Machineto overturnzoomable and explorable time-lapse view of our planetEarth EngineCREATE LabTIME MagazineLandsatGoogle Earth Enginemonitor the Amazonestimate forest carbon in TanzaniaMODISvarious operational and technological reasonsopen-source \u201cTime Machine\u201d softwarestreamed as multiresolution, overlapping video tilesplant growthbee colony collapsevery-large-scale simulations of the universe",
      "link": "http://ai.googleblog.com/2013/06/building-visual-planetary-time-machine.html",
      "author": "Posted by Randy Sargent, Google/Carnegie Mellon University;  Matt Hancher and Eric Nguyen, Google; and Illah Nourbakhsh, Carnegie Mellon University"
    },
    {
      "title": "The Story Behind Course Builder",
      "date": "Monday, June 3, 2013",
      "abstract": "The Story Behind Course BuilderMOOCPower SearchingCourse Builderetclarge number of coursesCourseraUdacityInformation VisualizationGame TheoryMapping with GoogleDigital Learning in K12YouTube Creator AcademyGiving with Purpose",
      "link": "http://ai.googleblog.com/2013/06/the-story-behind-course-builder.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Distributing the Edit History of Wikipedia Infoboxes",
      "date": "Thursday, May 30, 2013",
      "abstract": "Distributing the Edit History of Wikipedia Infoboxesdisambiguation resourcesparallel corporastructured knowledgetraining systems to learn to extract data from documentsWikimedia Deutschland e.V.available for downloadWHAD: Wikipedia Historical Attributes DataLanguage Resources and Evaluation journalWikidataGuillermo GarridoAnselmo Pe\u00f1asUNEDWikipedia DeutschlandThomas HofmannFernando Pereira",
      "link": "http://ai.googleblog.com/2013/05/distributing-edit-history-of-wikipedia.html",
      "author": "Posted by Enrique Alfonseca, Google Research"
    },
    {
      "title": "Open Access for Publications",
      "date": "Wednesday, May 29, 2013",
      "abstract": "Open Access for PublicationsThe Association for Computing MachineryannouncedIEEEconsistently said",
      "link": "http://ai.googleblog.com/2013/05/open-access-for-publications.html",
      "author": "Posted by Alfred Spector, Vice President, Engineering"
    },
    {
      "title": "Explore more with Mapping with Google",
      "date": "Tuesday, May 28, 2013",
      "abstract": "Explore more with Mapping with GoogleCourse BuilderMOOCMapping with GoogleMapping with Googlenew Google Maps,Mapping with Googleg.co/mappingcourse",
      "link": "http://ai.googleblog.com/2013/05/explore-more-with-mapping-with-google.html",
      "author": "Posted by Tina Ornduff, Program Manager"
    },
    {
      "title": "Syntactic Ngrams over Time",
      "date": "Thursday, May 23, 2013",
      "abstract": "Syntactic Ngrams over Timesentence diagrammingngram-viewerscientific paperhere",
      "link": "http://ai.googleblog.com/2013/05/syntactic-ngrams-over-time.html",
      "author": "Posted by Yoav Goldberg, Professor at Bar Ilan University & Post-doc at Google 2011-2013"
    },
    {
      "title": "Launching the Quantum Artificial Intelligence Lab",
      "date": "Thursday, May 16, 2013",
      "abstract": "Launching the Quantum Artificial Intelligence LabD-Wave SystemsUSRA",
      "link": "http://ai.googleblog.com/2013/05/launching-quantum-artificial.html",
      "author": "Posted by Hartmut Neven, Director of Engineering"
    },
    {
      "title": "Two Googlers elected to the American Academy of Arts and Sciences",
      "date": "Thursday, April 25, 2013",
      "abstract": "Two Googlers elected to the American Academy of Arts and SciencesOfficial Google BlogAmerican Academy of Arts and Sciences2013 elected membersPeter NorvigArun MajumdarArtificial Intelligence: A Modern Approachnorvig.comAdvanced Research Projects Agency-Energy (ARPA-E)Environmental Energy Technologies Divisionmore about Arun",
      "link": "http://ai.googleblog.com/2013/04/two-googlers-elected-to-american.html",
      "author": "Posted by Alfred Spector, Vice President, Engineering"
    },
    {
      "title": "50,000 Lessons on How to Read: a Relation Extraction Corpus",
      "date": "Thursday, April 11, 2013",
      "abstract": "50,000 Lessons on How to Read: a Relation Extraction CorpusJim HensonJane HensonmanybelovedcharactersshowsWho created Kermit?which proteins interacthundreds of millions of entities and billions of relationsexplore the world\u2019s informationhuman-judged datasetWikipediahereFreebase MID\u2019sFreebase property/education/education/institution/m/01tdnyh/m/07tgnhttps://code.google.com/p/relation-extraction-corpus/JSONCreative Commons Attribution-Sharealike 3.0",
      "link": "http://ai.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html",
      "author": "Posted by Dave Orr, Product Manager, Google Research"
    },
    {
      "title": "Advanced Power Searching with Google: Lessons Learned",
      "date": "Tuesday, April 9, 2013",
      "abstract": "Advanced Power Searching with Google: Lessons Learned",
      "link": "http://ai.googleblog.com/2013/04/advanced-power-searching-with-google.html",
      "author": "Posted by Dan Russell, Uber Tech Lead, Search Quality & User Happiness and Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Education Awards on Google App Engine",
      "date": "Wednesday, March 27, 2013",
      "abstract": "Education Awards on Google App EngineGoogle Developers BloginvitedGoogle App Engine Education AwardJohn David N. DionisioXiaohui (Helen) GuAdvanced Distributed Systems ClassShriram KrishnamurthiWeSchemeFeifei LiMark LiffitonTeacherTapEni MustafarajManish ParasharOrit ShaerGreenTouchElliot SolowayJonathan WhiteDr. Jiaofei Zhong",
      "link": "http://ai.googleblog.com/2013/03/education-awards-on-google-app-engine.html",
      "author": "Posted by Andrea Held, Google University Relations"
    },
    {
      "title": "Scaling Computer Science Education",
      "date": "Wednesday, March 13, 2013",
      "abstract": "Scaling Computer Science EducationSIGCSENSFCS PrinciplesCSTAstandardsreportComputing in the CorehereLinkcode.orgACMNCWITCS4HSExploring Computational Thinkingstudentteacher",
      "link": "http://ai.googleblog.com/2013/03/scaling-computer-science-education.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Our Commitment to Social Computing Research: Social Interactions Focused Awards Announcement",
      "date": "Tuesday, March 12, 2013",
      "abstract": "Our Commitment to Social Computing Research: Social Interactions Focused Awards Announcementinfluences on our behavior from social networksour understanding of social belonging on healthhow conflicts and coordination play out in Wikipediasocial interactions underlie many fundamental learning mechanismspeer discussions are critical in conceptual learning in college classesGoogle+YouTubeHe Says, She Says: Conflict and Coordination in Wikipedia",
      "link": "http://ai.googleblog.com/2013/03/our-commitment-to-social-computing.html",
      "author": "Ed H. Chi, Staff Research Scientist"
    },
    {
      "title": "Learning from Big Data: 40 Million Entities in Context",
      "date": "Friday, March 8, 2013",
      "abstract": "Learning from Big Data: 40 Million Entities in ContextplanetgodcarelementFreddie89 other possibilitiesdisambiguationambiguousfruitgiant tech companyan idea we\u2019ve discussed beforeBentivogli et al.dataDay et al.Artiles et al.dataWikilinks CorpusACL paper on cross-document co-referenceGoogle\u2019s Wikilinks CorpusUMass Wiki-linksUMass siteUMass AmherstSameer SinghAndrew McCallum",
      "link": "http://ai.googleblog.com/2013/03/learning-from-big-data-40-million.html",
      "author": "Posted by Dave Orr, Amar Subramanya, and Fernando Pereira, Google Research"
    },
    {
      "title": "Applauding the White House Memorandum on Open Access",
      "date": "Monday, February 25, 2013",
      "abstract": "Applauding the White House Memorandum on Open AccessMemorandum",
      "link": "http://ai.googleblog.com/2013/02/applauding-white-house-memorandum-on.html",
      "author": "Posted by Alfred Spector, Vice President of Research and Special Initiatives"
    },
    {
      "title": "Google Research Awards: Winter, 2013",
      "date": "Friday, February 22, 2013",
      "abstract": "Google Research Awards: Winter, 2013Google Research AwardsUniversity Relationsfocused, longer-term projectsPhD Fellowship programrecipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2013/02/google-research-awards-winter-2013.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Mobile interaction research at Google",
      "date": "Friday, February 15, 2013",
      "abstract": "Mobile interaction research at Googlehybrid approach to researchGesture Typing in Android 4.2Bimanual gesture keyboard.Touch Behavior with Different Postures on Soft Smart Phone Keyboards",
      "link": "http://ai.googleblog.com/2013/02/mobile-interaction-research-at-google.html",
      "author": "Posted by Xiaojun Bi, Ciprian Chelba, Tom Ouyang, Kurt Partridge and Shumin Zhai"
    },
    {
      "title": "Research Projects on Google App Engine",
      "date": "Tuesday, February 12, 2013",
      "abstract": "Research Projects on Google App EngineGoogle Developers BlogannouncedGoogle App Engine Research AwardsGoogle App EngineK. Mani ChandyCommunity Seismic Network (CSN) projectLawrence ChungSoftware Benchmark and Simulation ForecasterJulian GoughRamesh RaskarErick Baptista PassosprototypeNorman SadehLivehoodsWilliam SteinSagenext stepEnrique VivoniHydrologic Science, Engineering & SustainabilitySample Mapwebsite",
      "link": "http://ai.googleblog.com/2013/02/research-projects-on-google-app-engine.html",
      "author": "By Andrea Held, Program Manager, Google University Relations"
    },
    {
      "title": "Advanced Power Searching with Google -- Registration Opens Today",
      "date": "Thursday, January 10, 2013",
      "abstract": "Advanced Power Searching with Google -- Registration Opens TodayInside Search BlogAdvanced Power Searching with GooglePower Searching with Googlebloga Google a Daywww.powersearchingwithgoogle.comPower Searching with Google Quick Reference Guide",
      "link": "http://ai.googleblog.com/2013/01/advanced-power-searching-with-google.html",
      "author": "Posted by Daniel Russell, \u00dcber Tech Lead for Search Quality and User Happiness"
    },
    {
      "title": "Conference Report: Workshop on Internet and Network Economics (WINE) 2012",
      "date": "Wednesday, December 19, 2012",
      "abstract": "Conference Report: Workshop on Internet and Network Economics (WINE) 2012WINEeconomics and computation groupBudget Optimization for Online Campaigns with Positive Carryover EffectsWWW \u201910 paperOn Fixed-Price Marketing for Goods with Positive Network ExternalitiesThe AND-OR game: Equilibrium Characterizationcoopetitive ad auctionsbranch-and-bound search algorithm",
      "link": "http://ai.googleblog.com/2012/12/conference-report-workshop-on-internet.html",
      "author": "Posted by Vahab Mirrokni, Research Scientist, Google Research New York"
    },
    {
      "title": "Using online courses in Spain to teach entrepreneurship",
      "date": "Tuesday, December 18, 2012",
      "abstract": "Using online courses in Spain to teach entrepreneurshipPolicy by the Numbers Blogroughly 25%recent researchrecent OECDUdacityCourseraedXUniMOOCCourse Builderwon an awardjoin today",
      "link": "http://ai.googleblog.com/2012/12/using-online-courses-in-spain-to-teach.html",
      "author": "Posted by Francisco Ruiz Anton, Policy Manager, Google Spain"
    },
    {
      "title": "Millions of Core-Hours Awarded to Science",
      "date": "Monday, December 17, 2012",
      "abstract": "Millions of Core-Hours Awarded to SciencelaunchedGoogle Exacycle for Visiting FacultyJeff GardnerAndrew ConnollyJohn PetersonThe Large Synoptic Survey TelescopeVijay PandeRuss AltmanGPCRsGromacsMapReduceDremelMarkov State Modelsthree dimensional structure of proteinsBaker LabRosetta Protein Modelling suitephysical modeldockingone-sided protein design",
      "link": "http://ai.googleblog.com/2012/12/millions-of-core-hours-awarded-to.html",
      "author": "Posted by Andrea Held, Program Manager, University Relations"
    },
    {
      "title": "Continuing the quest for future computer scientists with CS4HS",
      "date": "Thursday, December 13, 2012",
      "abstract": "Continuing the quest for future computer scientists with CS4HShow to applywebsitePower Searching with GoogleCourse Builder\u201cM\u201d in \u201cMOOC\u201dCourse Builder platformCS4HS websiteherecs4hs@google.com",
      "link": "http://ai.googleblog.com/2012/12/continuing-quest-for-future-computer.html",
      "author": "Erin Mindell, Program Manager, Google Education"
    },
    {
      "title": "Large Scale Language Modeling in Automatic Speech Recognition",
      "date": "Wednesday, October 31, 2012",
      "abstract": "Large Scale Language Modeling in Automatic Speech Recognitionsummary of results on Voice Search and a few YouTube speech transcription tasksCross-posted with the Research at Google G+ Page",
      "link": "http://ai.googleblog.com/2012/10/large-scale-language-modeling-in.html",
      "author": "Posted by Ciprian Chelba, Research Scientist"
    },
    {
      "title": "Ngram Viewer 2.0",
      "date": "Thursday, October 18, 2012",
      "abstract": "Ngram Viewer 2.0Google Books Ngram ViewerScience paperinfo page",
      "link": "http://ai.googleblog.com/2012/10/ngram-viewer-20.html",
      "author": "Posted by Jon Orwant, Engineering Manager"
    },
    {
      "title": "ReFr: A New Open-Source Framework for Building Reranking Models",
      "date": "Thursday, October 4, 2012",
      "abstract": "ReFr: A New Open-Source Framework for Building Reranking ModelsDan BikelKeith Hallhttp://code.google.com/p/refr/subject of one of the groupsReFr",
      "link": "http://ai.googleblog.com/2012/10/refr-new-open-source-framework-for.html",
      "author": "Posted by Dan Bikel and Keith Hall, Research Scientists at Google"
    },
    {
      "title": "EMEA Faculty Summit 2012",
      "date": "Tuesday, October 2, 2012",
      "abstract": "EMEA Faculty Summit 2012Science MuseumCodebreaker - Alan Turing\u2019s life and legacyAlfred SpectorUniversity Relations programsEd H. Chidata analysis to understand the ways users share content with their audiencesCircle feature in Google+Jens RiegelsbergerJohn Wilkes",
      "link": "http://ai.googleblog.com/2012/10/emea-faculty-summit-2012.html",
      "author": "Michel Benard, University Relations Manager"
    },
    {
      "title": "Running Continuous Geo Experiments to Assess Ad Effectiveness",
      "date": "Tuesday, September 18, 2012",
      "abstract": "Running Continuous Geo Experiments to Assess Ad Effectivenessprevious paperpaper",
      "link": "http://ai.googleblog.com/2012/09/running-continuous-geo-experiments-to.html",
      "author": "Posted by Jon Vaver, Research Scientist and Lizzy Van Alstine, Marketing Manager"
    },
    {
      "title": "Power Searching with Google is back",
      "date": "Tuesday, September 11, 2012",
      "abstract": "Power Searching with Google is backa few months agosign up againregister now for Power Searching with GoogleA Google a Day",
      "link": "http://ai.googleblog.com/2012/09/power-searching-with-google-is-back.html",
      "author": "Posted by Dan Russell, Uber Tech Lead, Search Quality & User Happiness"
    },
    {
      "title": "Helping the World to Teach",
      "date": "Tuesday, September 11, 2012",
      "abstract": "Helping the World to TeachResearch at GooglePower Searching with GooglesuccessfulCourse BuilderedXedXStanford UniversityIndiana UniversityUC San DiegoSaylor.orgLearningByGivingFoundation.orgSwiss Federal Institute of Technology in Lausanne (EPFL)UniversiaCRUEBanco Santander-UniversidadesUdacityPower SearchingCourse Builder Open Source Project PagePower Searching",
      "link": "http://ai.googleblog.com/2012/09/helping-world-to-teach.html",
      "author": "Posted by Peter Norvig, Director of Research"
    },
    {
      "title": "Users love simple and familiar designs \u2013 Why websites need to make a great first impression",
      "date": "Wednesday, August 29, 2012",
      "abstract": "Users love simple and familiar designs \u2013 Why websites need to make a great first impressionis crucialour studyRecent research",
      "link": "http://ai.googleblog.com/2012/08/users-love-simple-and-familiar-designs.html",
      "author": "Posted by Javier Bargas-Avila, Senior User Experience Researcher at YouTube UX Research"
    },
    {
      "title": "Google at UAI 2012",
      "date": "Tuesday, August 28, 2012",
      "abstract": "Google at UAI 2012Uncertainty in Artificial IntelligenceVideo In Sentences OutExploiting Compositionality to Explore a Large Space of Model Structuresinvited talkJudea PearlLatent Structured RankingHokusai - Sketching Streams in Real Timeorganizing committee",
      "link": "http://ai.googleblog.com/2012/08/google-at-uai-2012.html",
      "author": "Posted by Kevin Murphy, Research Scientist"
    },
    {
      "title": "Better table search through Machine Learning and Knowledge",
      "date": "Thursday, August 23, 2012",
      "abstract": "Better table search through Machine Learning and KnowledgeTable Searchsupport vector machinekernel functionsKnowledge GraphGoogle DriveFusion TablesFusion Tables APIAlgorithms for Learning Kernels Based on Centered AlignmentGeneralization Bounds for Learning Kernels",
      "link": "http://ai.googleblog.com/2012/08/better-table-search-through-machine.html",
      "author": "Posted By Johnny Chen, Product Manager, Google Research"
    },
    {
      "title": "Machine Learning Book for Students and Researchers",
      "date": "Wednesday, August 22, 2012",
      "abstract": "Machine Learning Book for Students and ResearchersThe Foundations of Machine LearningMehryar Mohri",
      "link": "http://ai.googleblog.com/2012/08/machine-learning-book-for-students-and.html",
      "author": "Posted by Afshin Rostamizadeh, Google Research"
    },
    {
      "title": "Faculty Summit 2012: Online Education Panel",
      "date": "Monday, August 20, 2012",
      "abstract": "Faculty Summit 2012: Online Education PanelPeter NorvigFaculty SummitEducation at ScaleCourseratalkIntro to AI classtalk",
      "link": "http://ai.googleblog.com/2012/08/faculty-summit-2012-online-education.html",
      "author": "Posted by Peter Norvig, Director of Research"
    },
    {
      "title": "Improving Google Patents with European Patent Office patents and the Prior Art Finder",
      "date": "Tuesday, August 14, 2012",
      "abstract": "Improving Google Patents with European Patent Office patents and the Prior Art FinderUS Public Policy BlogEuropean Public Policy BlogInside Search Blogthis onereleased an update to Google Translateour repository of USPTO bulk data",
      "link": "http://ai.googleblog.com/2012/08/improving-google-patents-with-european.html",
      "author": "Posted by Jon Orwant, Engineering Manager"
    },
    {
      "title": "Teaching the World to Search",
      "date": "Wednesday, August 8, 2012",
      "abstract": "Teaching the World to SearchMOOCpioneered by Stanford and MITcourse content",
      "link": "http://ai.googleblog.com/2012/08/teaching-world-to-search.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Speech Recognition and Deep Learning",
      "date": "Monday, August 6, 2012",
      "abstract": "Speech Recognition and Deep Learningan articleGoogle Compute EngineGoogle I/Ophilosophy of integrated research",
      "link": "http://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html",
      "author": "Posted by Vincent Vanhoucke, Research Scientist, Speech Team"
    },
    {
      "title": "Reflections on Digital Interactions: Thoughts from the 2012 NA Faculty Summit",
      "date": "Thursday, August 2, 2012",
      "abstract": "Reflections on Digital Interactions: Thoughts from the 2012 NA Faculty SummitComputer Science Faculty SummitprogramKnowledge GraphThis pagePower Searching Coursetheir blog",
      "link": "http://ai.googleblog.com/2012/08/reflections-on-digital-interactions.html",
      "author": "Posted by Alfred Spector, Vice President of Research and Special Initiatives"
    },
    {
      "title": "Natural Language in Voice Search",
      "date": "Tuesday, July 31, 2012",
      "abstract": "Natural Language in Voice SearchComputer Science Faculty SummitGoogle Voice SearchVoice actionsGoogle I/O 2012",
      "link": "http://ai.googleblog.com/2012/07/natural-language-in-voice-search.html",
      "author": "Posted by Jakob Uszkoreit, Software Engineer"
    },
    {
      "title": "New Challenges in Computer Science Research",
      "date": "Friday, July 27, 2012",
      "abstract": "New Challenges in Computer Science Research2012 Computer Science Faculty SummitVivek KwatraEd ChiJohn WilkesAlon HalevyFusion TablesKnowledge Graphlaunched",
      "link": "http://ai.googleblog.com/2012/07/new-challenges-in-computer-science.html",
      "author": "Posted by Jeff Walz, Head of University Relations"
    },
    {
      "title": "Education in the Cloud",
      "date": "Friday, July 27, 2012",
      "abstract": "Education in the CloudGoogle App Engine Education AwardsGoogle App Enginethis form",
      "link": "http://ai.googleblog.com/2012/07/education-in-cloud.html",
      "author": "Posted by Andrea Held, University Relations"
    },
    {
      "title": "Big Pictures with Big Messages",
      "date": "Thursday, July 26, 2012",
      "abstract": "Big Pictures with Big MessagesComputer Science Faculty Summitwind mapUS National Digital Forecast DatabasegalleryGoogle+ Rippleshow news spreadsFaculty Summit",
      "link": "http://ai.googleblog.com/2012/07/big-pictures-with-big-messages.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Site Reliability Engineers: \u201csolving the most interesting problems\u201d",
      "date": "Wednesday, July 25, 2012",
      "abstract": "Site Reliability Engineers: \u201csolving the most interesting problems\u201dMaps APIGoogle jobs site",
      "link": "http://ai.googleblog.com/2012/07/site-reliability-engineers-solving-most.html",
      "author": "Posted by Chris Reid, Sydney Staffing team"
    },
    {
      "title": "Google at SIGMOD/PODS 2012",
      "date": "Friday, July 13, 2012",
      "abstract": "Google at SIGMOD/PODS 2012Anish Das SarmaSIGMODACM SIGMOD/PODS conference (on Management of Data, and Principles of Database Systems)Pat HanrahanFusion TablesEfficient Spatial Sampling of Large Geographical TablesVizDeck: Self-Organizing Dashboards for Visual AnalyticsF1Kirsten LeFevreSurajit ChaudhuriDonald KossmannSam MaddenAnish Das SarmaAmin Vahdat'sAnish Das SarmaZoltan GyongyiAlon HalevyKristen LeFevreCong YuSymbiosis in Scale Out Networking and Data ManagementF1-The Fault-Tolerant Distributed RDBMS Supporting Google's Ad BusinessFinding Related TablesCloudRAMSort: Fast and Efficient Large-Scale Distributed RAM Sort on Shared-Nothing ClusterEfficient Spatial Sampling of Large Geographical Tables",
      "link": "http://ai.googleblog.com/2012/07/google-at-sigmodpods-2012.html",
      "author": "Posted by Anish Das Sarma, Research Scientist and Jeff Shute, Software Engineer"
    },
    {
      "title": "Reflections on the Google Faculty Institute",
      "date": "Thursday, July 12, 2012",
      "abstract": "Reflections on the Google Faculty Institute2011 Google Faculty Instituteten research initiativesApp Inventor",
      "link": "http://ai.googleblog.com/2012/07/reflections-on-google-faculty-institute.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Google Research Awards: Summer, 2012",
      "date": "Tuesday, July 3, 2012",
      "abstract": "Google Research Awards: Summer, 2012Google Research AwardsResearch @ Googlerecipients of this round\u2019s awardswebsite",
      "link": "http://ai.googleblog.com/2012/07/google-research-awards-summer-2012.html",
      "author": "Posted by Maggie Johnson, Director of Education, University Relations"
    },
    {
      "title": "Our Unique Approach to Research",
      "date": "Monday, July 2, 2012",
      "abstract": "Our Unique Approach to ResearchAlfred Spectorresearch projectPeter NorvigSlav Petrov\u201cGoogle\u2019s Hybrid Approach to Research,\u201dMapReduceVoice SearchAndroidChromeGoogle Translate\u201cGoogle\u2019s Hybrid Approach to Research\u201dResearch at GoogleOfficial Google Blog",
      "link": "http://ai.googleblog.com/2012/07/our-unique-approach-to-research.html",
      "author": "Posted by\u00a0Alfred Spector, Vice President of Research and Special Initiatives"
    },
    {
      "title": "Introducing new Fusion Tables API",
      "date": "Tuesday, June 26, 2012",
      "abstract": "Introducing new Fusion Tables APIFusion Tables APISQL APIGoogle APIs consolemigration guideAPI documentation",
      "link": "http://ai.googleblog.com/2012/06/introducing-new-fusion-tables-api.html",
      "author": "Posted by Warren Shen, Fusion Tables team"
    },
    {
      "title": "Become a Google Power Searcher",
      "date": "Tuesday, June 26, 2012",
      "abstract": "Become a Google Power SearcherOfficial Google BlogregistrationMOOCpioneered by Stanford and MITcourse homepage",
      "link": "http://ai.googleblog.com/2012/06/become-google-power-searcher.html",
      "author": "Posted by Terry Ednacot, Education Program Manager"
    },
    {
      "title": "Third Market Algorithms and Optimization Workshop at Google NYC",
      "date": "Friday, June 15, 2012",
      "abstract": "Third Market Algorithms and Optimization Workshop at Google NYCAd ExchangeThird Market Algorithms and Optimization Workshop at GoogleSTOC 2012\u00c9va TardosCostis DaskalakisPablo AzarTim RoughgardenBobby KleinbergGagan GoelAndrew McGregoradjacency matrixMohammad MahdianKendall-Tau distanceMukund SundararajanVarun GuptaMapReducePregelCorinna CortesAlfred Spectorblog post\u2018Muthu\u2019 Muthukrishnan",
      "link": "http://ai.googleblog.com/2012/06/third-market-algorithms-and.html",
      "author": "Posted by Nitish Korula and Vahab Mirrokni, Google Research, New York"
    },
    {
      "title": "Recap of NAACL-12 including two Best Paper awards for Googlers",
      "date": "Thursday, June 14, 2012",
      "abstract": "Recap of NAACL-12 including two Best Paper awards for GooglersConference of the North American Chapter of the Association for Computational Linguisticsmachine translationsentiment analysisMechanical Turkthe entire video on how a bill becomes a lawtwo keynote talksBrian RoarkRobert WilenskyRay MooneyCross-lingual Word Clusters for Direct Transfer of Linguistic StructureVine Pruning for Efficient Multi-Pass Dependency ParsingUnsupervised Translation Sense ClusteringComputational Linguistics for LiteratureAutomatic Knowledge Base Construction/Workshop on Web-scale Knowledge ExtractionWorkshop on Inducing Linguistic StructureWorkshop on Statistical Machine TranslationProgramWorkshop on the Future of Language Modeling for HLTFirst Workshop on Syntactic Analysis of Non-Canonical LanguageEvaluation Metrics and System Comparison for Automatic Summarization",
      "link": "http://ai.googleblog.com/2012/06/recap-of-naacl-12-including-two-best.html",
      "author": "Posted by Ryan McDonald, Research Scientist, Google Research"
    },
    {
      "title": "2012 Google PhD Fellowships",
      "date": "Monday, June 11, 2012",
      "abstract": "2012 Google PhD Fellowshipsclick here",
      "link": "http://ai.googleblog.com/2012/06/2012-google-phd-fellowships.html",
      "author": "Posted by Leslie Yeh Johnson, University Relations Manager"
    },
    {
      "title": "Hello science\u2014meet HR",
      "date": "Wednesday, June 6, 2012",
      "abstract": "Hello science\u2014meet HRwe apply science to organizational issues as wellGooglers save more for retirementdeveloping effective managersencouraging healthy food choices",
      "link": "http://ai.googleblog.com/2012/06/hello-sciencemeet-hr.html",
      "author": "Posted by Jennifer Kurkoski, Ph.D., Manager, People & Innovation Lab"
    },
    {
      "title": "Research at Google on G+: Featuring Excellent Papers for 2011",
      "date": "Monday, June 4, 2012",
      "abstract": "Research at Google on G+: Featuring Excellent Papers for 2011Excellent Papers for 2011Research at Google page on G+publicationsResearch at Googledeep dive",
      "link": "http://ai.googleblog.com/2012/06/research-at-google-on-g-featuring.html",
      "author": "Posted by Corinna Cortes, Google Research"
    },
    {
      "title": "From Words to Concepts and Back: Dictionaries for Linking Text, Entities and Ideas",
      "date": "Friday, May 18, 2012",
      "abstract": "From Words to Concepts and Back: Dictionaries for Linking Text, Entities and IdeasWikipedia articleWikipedia's groupings of articles into hierarchical categoriesEnglish Wikipedia article's canonical locationAssociation footballAmerican footballsoft drinkStanford UniversityStanford (disambiguation)Stanford, CaliforniaStanford Cardinal footballStanford CardinalStanford Cardinal men's basketballStanford prison experimentStanford, KentuckyStanford, NorfolkBank of the West ClassicStanford, IllinoisLeland StanfordCharles Villiers StanfordStanford, New YorkStanford, Bedfordshirenon-existent articlespaperpresentedLREC 2012datathis releaseAngel X. ChangValentin I. SpitkovskyUniversity of Basque Country's Ixa GroupEneko AgirreStanford's NLP GroupEric YehSRI InternationalChristopher D. ManningDaniel Jurafsky",
      "link": "http://ai.googleblog.com/2012/05/from-words-to-concepts-and-back.html",
      "author": "Posted by Valentin Spitkovsky and Peter Norvig, Research Team"
    },
    {
      "title": "2012 Google PhD Fellowships",
      "date": "Monday, June 11, 2012",
      "abstract": "2012 Google PhD FellowshipsRead Moreclick here",
      "link": "http://ai.googleblog.com/2012/06/2012-google-phd-fellowships.html",
      "author": "Posted by Leslie Yeh Johnson, University Relations Manager"
    },
    {
      "title": "Hello science\u2014meet HR",
      "date": "Wednesday, June 6, 2012",
      "abstract": "Hello science\u2014meet HRRead Morewe apply science to organizational issues as wellGooglers save more for retirementdeveloping effective managersencouraging healthy food choices",
      "link": "http://ai.googleblog.com/2012/06/hello-sciencemeet-hr.html",
      "author": "Posted by Jennifer Kurkoski, Ph.D., Manager, People & Innovation Lab"
    },
    {
      "title": "Research at Google on G+: Featuring Excellent Papers for 2011",
      "date": "Monday, June 4, 2012",
      "abstract": "Research at Google on G+: Featuring Excellent Papers for 2011Excellent Papers for 2011Read MoreExcellent Papers for 2011Research at Google page on G+publicationsResearch at Googledeep dive",
      "link": "http://ai.googleblog.com/2012/06/research-at-google-on-g-featuring.html",
      "author": "Posted by Corinna Cortes, Google Research"
    },
    {
      "title": "From Words to Concepts and Back: Dictionaries for Linking Text, Entities and Ideas",
      "date": "Friday, May 18, 2012",
      "abstract": "From Words to Concepts and Back: Dictionaries for Linking Text, Entities and IdeasRead MoreWikipedia articleWikipedia's groupings of articles into hierarchical categoriesEnglish Wikipedia article's canonical locationAssociation footballAmerican footballsoft drinkStanford UniversityStanford (disambiguation)Stanford, CaliforniaStanford Cardinal footballStanford CardinalStanford Cardinal men's basketballStanford prison experimentStanford, KentuckyStanford, NorfolkBank of the West ClassicStanford, IllinoisLeland StanfordCharles Villiers StanfordStanford, New YorkStanford, Bedfordshirenon-existent articlespaperpresentedLREC 2012datathis releaseAngel X. ChangValentin I. SpitkovskyUniversity of Basque Country's Ixa GroupEneko AgirreStanford's NLP GroupEric YehSRI InternationalChristopher D. ManningDaniel Jurafsky",
      "link": "http://ai.googleblog.com/2012/05/from-words-to-concepts-and-back.html",
      "author": "Posted by Valentin Spitkovsky and Peter Norvig, Research Team"
    },
    {
      "title": "Smart Pricing may increase average publisher revenue",
      "date": "Tuesday, May 15, 2012",
      "abstract": "Smart Pricing may increase average publisher revenueRead MoreSmart Pricingthis paper",
      "link": "http://ai.googleblog.com/2012/05/smart-pricing-may-increase-average.html",
      "author": "Posted by Guy Calvert, AdSense Sales"
    },
    {
      "title": "Is beautiful usable? What is the influence of beauty and usability on reactions to a product?",
      "date": "Monday, May 14, 2012",
      "abstract": "Is beautiful usable? What is the influence of beauty and usability on reactions to a product?Read Morewe conducted an experimental lab study",
      "link": "http://ai.googleblog.com/2012/05/is-beautiful-usable-what-is-influence.html",
      "author": "Posted by Javier Bargas-Avila, Senior User Experience Researcher at YouTube UX Research"
    },
    {
      "title": "Google, the World Wide Web and WWW conference: years of progress, prosperity and innovation",
      "date": "Monday, May 7, 2012",
      "abstract": "Google, the World Wide Web and WWW conference: years of progress, prosperity and innovationRead MoreWorld Wide Web conferenceWho Killed My Battery: Analyzing Mobile Browser Energy ConsumptionBuild Your Own Music Recommender by Modeling Internet Radio StreamsCounting beyond a Yottabyte, or how SPARQL 1.1 Property Paths will prevent adoption of the standardsee full schedule of booth talksGoogle Art ProjectRisk-Aware Revenue Maximization in Display AdvertisingSessionJuggler: Secure Web Login From an Untrusted Terminal Using Session HijackingSpotting Fake Reviewer Groups in Consumer ReviewsYour Two Weeks of Fame and Your Grandmother\u2019sYouTube Around the World: Geographic Popularity of VideosWho Killed My Battery: Analyzing Mobile Browser Energy ConsumptionA Multimodal Search Engine based on Rich Unified Content DescriptionEnabling on-the-fly Video Shot Detection on YouTubeFixing the Web one page at a time, or actually implementing xkcd #37Appification of the WebExtracting Unambiguous Keywords from Microposts Using Web and Query Logs DataMaking Sense of MicrospostsHuman Computation Must Be ReproducibleCrowdSearch: Crowdsourcing Web searchWebQuality 2012: The Anti-Social WebThe Role of Human-Generated and Automatically-Extracted Lexico-Semantic Resources in Web SearchGoogle Image SwirlWeb inventor Tim Berners-Lee on imagining worldsAdd Research at Google to your circles on G+",
      "link": "http://ai.googleblog.com/2012/05/google-world-wide-web-and-www.html",
      "author": "Posted by Prabhakar Raghavan, Vice President of Engineering"
    },
    {
      "title": "Video Stabilization on YouTube",
      "date": "Friday, May 4, 2012",
      "abstract": "Video Stabilization on YouTubeRead Moreearlier blog postAuto-Directed Video Stabilization with Robust L1 Optimal Camera PathsIEEE CVPR 2011YouTube editorvideo managerCalibration-Free Rolling Shutter RemovalIEEE ICCP 2012 narrated video descriptionYouTube stabilizer",
      "link": "http://ai.googleblog.com/2012/05/video-stabilization-on-youtube.html",
      "author": "Posted by Matthias Grundmann, Vivek Kwatra, and Irfan Essa, Research at Google"
    },
    {
      "title": "An Experiment in Music and Crowd-Sourcing",
      "date": "Friday, May 4, 2012",
      "abstract": "An Experiment in Music and Crowd-SourcingBodleian LibraryUniversity of OxfordRead MoreBodleian LibraryUniversity of OxfordWhat\u2019s-the-ScoreGoogle Focused Awards",
      "link": "http://ai.googleblog.com/2012/05/an-experiment-in-music-and-crowd.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "From Open Research to Open Flow",
      "date": "Wednesday, May 2, 2012",
      "abstract": "From Open Research to Open FlowOpen FlowRead MoreOpen FlowSoftware-Defined NetworkingOpen Networking Foundation",
      "link": "http://ai.googleblog.com/2012/05/from-open-research-to-open-flow.html",
      "author": "Posted by Jeff Walz, University Relations Team"
    },
    {
      "title": "Joining forces to support computer science majors",
      "date": "Thursday, April 26, 2012",
      "abstract": "Joining forces to support computer science majorsSIGCSENSFCS PrinciplesCollege BoardACMCSTAComputing in the Core CoalitionCSEdWeekCS4HSApp InventorExploring Computational Thinkingoutreach programs",
      "link": "http://ai.googleblog.com/2012/04/joining-forces-to-support-computer.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Working with your Data: Easier and More Fun",
      "date": "Thursday, April 12, 2012",
      "abstract": "Working with your Data: Easier and More Fun\u201cExperimental\u201d versionjoin this groupFusion Tables Layer wizardchart creatorFusion Tables open source library",
      "link": "http://ai.googleblog.com/2012/04/working-with-your-data-easier-and-more.html",
      "author": "Posted by Rebecca Shapley, Fusion Tables Team"
    },
    {
      "title": "Google App Engine Research Awards for scientific discovery",
      "date": "Thursday, March 29, 2012",
      "abstract": "Google App Engine Research Awards for scientific discoverylaunchApp EngineapplicationsUniversity RelationsRFPGoogle Research",
      "link": "http://ai.googleblog.com/2012/03/google-app-engine-research-awards-for.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations and Andrea Held, University Relations Program Manager"
    },
    {
      "title": "Impact of Organic Ranking on Ad Click Incrementality",
      "date": "Tuesday, March 27, 2012",
      "abstract": "Impact of Organic Ranking on Ad Click IncrementalitySearch Ads Pause research studygeo-based experimentsoriginal search ads pause studyhere",
      "link": "http://ai.googleblog.com/2012/03/impact-of-organic-ranking-on-ad-click.html",
      "author": "Posted by David Chan, Statistician and Lizzy Van Alstine, Research Evangelist"
    },
    {
      "title": "Excellent Papers for 2011",
      "date": "Thursday, March 22, 2012",
      "abstract": "Excellent Papers for 2011Theo Vassilakispublicationsset of paperssecond roundpublications listCascades of two-pole\u2013two-zero asymmetric resonators are good models of peripheral auditory functionRichard F. LyonOnline Vertex-Weighted Bipartite Matching and Single-bid Budgeted AllocationsGagan AggarwalGagan GoelChinmay KarandeAranyak MehtaMilgram-routing in social networksSilvio LattanziNon-Price Equilibria in Markets of Discrete GoodsFrom Basecamp to Summit: Scaling Field Research Across 9 LocationsJens RiegelsbergerUser-Defined Motion Gestures for Mobile InteractionYang LiReputation Systems for Open CollaborationA. KulshreshthaDomain adaptation in regressionCorinna CortesMehryar MohriOn the necessity of irrelevant variablesPhilip M. LongOnline Learning in the Manifold of Low-Rank MatricesGal ChechikTraining a Parser for Machine Translation ReorderingSlav PetrovRyan McDonaldFranz OchWatermarking the Outputs of Structured Prediction with an application in Statistical Machine TranslationJakob UszkoreitFranz OchInducing Sentence Structure from Parallel Corpora for ReorderingJohn DeNeroJakob UszkoreitKernelized Structural SVM Learning for Supervised Object SegmentationLuca BertelliTianli YuAuto-Directed Video Stabilization with Robust L1 Optimal Camera PathsMatthias GrundmannVivek Kwatrayoutube.com/editorThe Power of Comparative ReasoningJay YagnikDavid RossUnsupervised Part-of-Speech Tagging with Bilingual Graph-Based ProjectionsSlav PetrovTCP Fast OpenYuchung ChengJerry ChuArvind JainProportional Rate Reduction for TCPNandita DukkipatiYuchung ChengAutomated Analysis of Security-Critical JavaScript APIs\u00dalfar ErlingssonMark S. MillerApp Isolation: Get the Security of Multiple Browsers with Just OneCharles ReisImproving the speed of neural networks on CPUsVincent VanhouckeAndrew SeniorBayesian Language Model Interpolation for Mobile Speech InputCyril AllauzenMichael RileyLarge-Scale Parallel Statistical Forecasting Computations in RMurray StokelyDremel: Interactive Analysis of Web-Scale DatasetsTheo VassilakisRepresentative Skylines using Threshold-based Preference DistributionsAtish Das SarmaHyper-local, directions-based ranking of placesAlon Y. HalevyPower Management of Online Data-Intensive ServicesLuiz Andr\u00e9 BarrosoWolf-Dietrich WeberThe Impact of Memory Subsystem Resource Sharing on Datacenter ApplicationsRobert HundtLanguage-Independent Sandboxing of Just-In-Time Compilation and Self-Modifying Code\u00dalfar ErlingssonBrad ChenCliff L. BiffleThialfi: A Client Notification Service for Internet-Scale ApplicationsDaniel MyersMichael Piatek",
      "link": "http://ai.googleblog.com/2012/03/excellent-papers-for-2011.html",
      "author": "Posted by Corinna Cortes and Alfred Spector, Google Research"
    },
    {
      "title": "Google at INFOCOM 2012",
      "date": "Wednesday, March 21, 2012",
      "abstract": "Google at INFOCOM 2012INFOCOM 2012",
      "link": "http://ai.googleblog.com/2012/03/google-at-infocom-2012.html",
      "author": "Posted by Emilie Danna, Google Research & Michal Segalov,Networking Software"
    },
    {
      "title": "Gamification for Improved Search Ranking for YouTube Topics",
      "date": "Monday, March 19, 2012",
      "abstract": "Gamification for Improved Search Ranking for YouTube TopicsIn earlier postsYouTube Slamvideoemoatali99Whitney Houston covercute slammusic slamGamificationLatte Art SlamSpeed Painting Slamyoutube.com/topicscold startRomantic Proposals SlamthisthisthisthisChocolate BrowniePaper PlaneBush FlyingStealth TechnologyStencil GraffitiYosemite National ParkStealth Technology",
      "link": "http://ai.googleblog.com/2012/03/gamification-for-improved-search.html",
      "author": "Posted by Charles DuHadway and Sanketh Shetty, Google Research"
    },
    {
      "title": "Search Ads Pause Studies Update",
      "date": "Monday, March 12, 2012",
      "abstract": "Search Ads Pause Studies Update\"Incremental Clicks Impact of Search Advertising\"Click here for an infographic",
      "link": "http://ai.googleblog.com/2012/03/search-ads-pause-studies-update.html",
      "author": "Posted by Lizzy Van Alstine, Research Evangelist and David Chan, Statistician"
    },
    {
      "title": "Keeping an \u201cOER mind\u201d about shared resources for education",
      "date": "Monday, March 5, 2012",
      "abstract": "Keeping an \u201cOER mind\u201d about shared resources for educationOpenCourseWare ConsortiumOpen Education Week 2012OpenStaxConnexionsbelieveFlat World Knowledge+Google in Education pageopeneducationweek.org",
      "link": "http://ai.googleblog.com/2012/03/keeping-oer-mind-about-shared-resources.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Announcing Google-hosted workshop videos from NIPS 2011",
      "date": "Thursday, February 23, 2012",
      "abstract": "Announcing Google-hosted workshop videos from NIPS 201125th Neural Information Processing Systems (NIPS)NIPS 2011 blog postYouTube Tech Talks ChannelBig Learning: Algorithms, Systems, and Tools for Learning at ScaleDomain Adaptation Workshop: Theory and ApplicationLearning SemanticsSparse Representation and Low-rank ApproximationInternational Workshop on Music and Machine Learning: Learning from Musical StructureThe Domain AdaptationMachine Learning and MusicIntegrating Language and VisionDeep Learning and Unsupervised Feature Learning",
      "link": "http://ai.googleblog.com/2012/02/announcing-google-hosted-workshop.html",
      "author": "Posted by John Blitzer and Douglas Eck, Google Research"
    },
    {
      "title": "2011 EMEA Android Educational Outreach Program Awards Mobile Phones to Universities",
      "date": "Wednesday, February 22, 2012",
      "abstract": "2011 EMEA Android Educational Outreach Program Awards Mobile Phones to UniversitiesGeorge CandeaPocket CampusAndrew Ricesummer programmeLearn!Alan SmeatonNicolae TapusGerhard Tr\u00f6sterMartin Wirzwearable computing",
      "link": "http://ai.googleblog.com/2012/02/2011-emea-android-educational-outreach.html",
      "author": "Posted by David Harper, Head of University Relations, EMEA"
    },
    {
      "title": "Quantifying comedy on YouTube: why the number of o\u2019s in your LOL matter",
      "date": "Thursday, February 9, 2012",
      "abstract": "Quantifying comedy on YouTube: why the number of o\u2019s in your LOL matterpostYouTube Music SlamAI-CompleteCharlie bit my fingerDavid after dentistGive it a tryOpinion Mining and Sentiment AnalysisA Great Catchy Name: Semi-Supervised Recognition of Sarcastic Sentences in Online Product ReviewsThat\u2019s What She Said: Double Entendre Identi\ufb01cation",
      "link": "http://ai.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html",
      "author": "Posted by Sanketh Shetty, YouTube Slam Team, Google Research"
    },
    {
      "title": "Data and code open sourced from Google's Renewable Energy Cheaper than Coal project",
      "date": "Monday, January 30, 2012",
      "abstract": "Data and code open sourced from Google's Renewable Energy Cheaper than Coal projectCross-postedRE<CheliostatHOpSevaluate heliostat field layoutsREC_CSPRE<C wind data collection documenthereheliostat aerodynamic load dataappendixhereRE<C heliostat control demonstrationsaccelerometer sensingcontrol system designhere",
      "link": "http://ai.googleblog.com/2012/01/data-and-code-open-sourced-from-google.html",
      "author": "Posted by Ross Koningstein, Engineer, Google RE<C team"
    },
    {
      "title": "Open-sourcing Sky Map and collaborating with Carnegie Mellon University",
      "date": "Friday, January 20, 2012",
      "abstract": "Open-sourcing Sky Map and collaborating with Carnegie Mellon UniversityIn May 2009Pittsburgh officeUrban Sky Partyopen-sourced",
      "link": "http://ai.googleblog.com/2012/01/open-sourcing-sky-map-and-collaborating.html",
      "author": "Posted by John Taylor and Kevin Serafini"
    },
    {
      "title": "CDC Birth Vital Statistics in BigQuery",
      "date": "Friday, January 13, 2012",
      "abstract": "CDC Birth Vital Statistics in BigQueryBigQuery Servicelarge, public data setsnatalityDivision of Vital StatisticsCenters for Disease Control and Preventionsince 1969examples",
      "link": "http://ai.googleblog.com/2012/01/cdc-birth-vital-statistics-in-bigquery.html",
      "author": "Posted by Dan Vanderkam, Software Engineer"
    },
    {
      "title": "Google Correlate expands to 49 additional countries",
      "date": "Tuesday, January 3, 2012",
      "abstract": "Google Correlate expands to 49 additional countrieslaunchedThis systemconsumer spendingunemployment ratehousing inventorysociologymeteorologygas prices and search activity for fuel efficient carsFox News presidential debatecoveredpublishedbloody nose\"snorkeling\" in Australia\"cherry blossoms\" in Japan\"beer garden\" in Germany",
      "link": "http://ai.googleblog.com/2012/01/google-correlate-expands-to-49.html",
      "author": "Posted by Matt Mohebbi, Software Engineer"
    },
    {
      "title": "Academic Successes in Cluster Computing",
      "date": "Thursday, December 22, 2011",
      "abstract": "Academic Successes in Cluster ComputingCluster Exploratory Servicedonated by Google and IBMMapReduce",
      "link": "http://ai.googleblog.com/2011/12/academic-successes-in-cluster-computing.html",
      "author": "Posted by Alfred Spector, VP of Research"
    },
    {
      "title": "Measuring Ad Effectiveness Using Geo Experiments",
      "date": "Friday, December 9, 2011",
      "abstract": "Measuring Ad Effectiveness Using Geo Experimentsthis paper",
      "link": "http://ai.googleblog.com/2011/12/measuring-ad-effectiveness-using-geo.html",
      "author": "Posted by Lizzy Van Alstine and Jon Vaver, Quantitative Analysis Team"
    },
    {
      "title": "ACM Fellows for 2011",
      "date": "Thursday, December 8, 2011",
      "abstract": "ACM Fellows for 2011Official Google BlogAssociation for Computing MachineryannouncedAmit SinghalPeter S. MagnussonAmin Vahdatprior Googler ACM Fellows",
      "link": "http://ai.googleblog.com/2011/12/acm-fellows-for-2011.html",
      "author": "Posted by Alfred Spector, Google Research"
    },
    {
      "title": "Our second round of Google Research Awards for 2011",
      "date": "Tuesday, December 6, 2011",
      "abstract": "Our second round of Google Research Awards for 2011Google Research Awardsfocused awardswell-deserving recipients of this round\u2019s awardsour website",
      "link": "http://ai.googleblog.com/2011/12/our-second-round-of-google-research.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "2011 Google China Faculty Summit in Hangzhou",
      "date": "Friday, December 2, 2011",
      "abstract": "2011 Google China Faculty Summit in HangzhouHangzhou, Chinahere",
      "link": "http://ai.googleblog.com/2011/12/2011-google-china-faculty-summit-in.html",
      "author": "Posted by Aimin Zhu, University Relationship Manager, Google China"
    },
    {
      "title": "More Google Cluster Data",
      "date": "Tuesday, November 29, 2011",
      "abstract": "More Google Cluster Dataresearch blog on Google Cluster Datathis link",
      "link": "http://ai.googleblog.com/2011/11/more-google-cluster-data.html",
      "author": "Posted by John Wilkes, Principal Software Engineer"
    },
    {
      "title": "Discovering Talented Musicians with Acoustic Analysis",
      "date": "Wednesday, November 2, 2011",
      "abstract": "Discovering Talented Musicians with Acoustic Analysisearlier postMusic Beta by Googlelife-changing successYouTube Slamgive it a tryleaderboardscutebizarrecomedydanceVideo2Text: Learning to Annotate Video ContentHrishikesh AradhyeGeorge TodericiJay Yagnik",
      "link": "http://ai.googleblog.com/2011/11/discovering-talented-musicians-with.html",
      "author": "Posted by Charles DuHadway, YouTube Slam Team, Google Research"
    },
    {
      "title": "Fresh Perspectives about People and the Web from Think Quarterly",
      "date": "Wednesday, September 28, 2011",
      "abstract": "Fresh Perspectives about People and the Web from Think QuarterlyThink Quarterly\u201cPeople\u201d issueClick herehere",
      "link": "http://ai.googleblog.com/2011/09/fresh-perspectives-about-people-and-web.html",
      "author": "Posted by Allison Mooney, Christina Park, and Caroline McCarthy, The Think Quarterly Team"
    },
    {
      "title": "Trying on the new Dynamic Views from Blogger",
      "date": "Tuesday, September 27, 2011",
      "abstract": "Trying on the new Dynamic Views from BloggerBloggerLaunched todayBlogger Buzz post",
      "link": "http://ai.googleblog.com/2011/09/trying-on-new-dynamic-views-from-blogger.html",
      "author": "Unknown"
    },
    {
      "title": "Sorting Petabytes with MapReduce - The Next Episode",
      "date": "Wednesday, September 7, 2011",
      "abstract": "Sorting Petabytes with MapReduce - The Next Episoderesults of the first ever \"petasort\"Sort BenchmarkMapReducesoftware engineering position",
      "link": "http://ai.googleblog.com/2011/09/sorting-petabytes-with-mapreduce-next.html",
      "author": "Posted by Grzegorz Czajkowski, Mari\u00e1n Dvorsk\u00fd, Jerry Zhao, and Michael Conley, Systems Infrastructure"
    },
    {
      "title": "Google at the Joint Statistical Meetings in Miami",
      "date": "Monday, August 22, 2011",
      "abstract": "Google at the Joint Statistical Meetings in Miami\"How Google uses R to make online advertising more effective\"Statistical Plumbing: Effective use of classical statistical methods for large scale applicationsParallel Computations in R, with Applications for Statistical ForecastingConditional Regression ModelsThe Effectiveness of Display AdsMeasuring Ad Effectiveness Using Continuous Geo ExperimentsPost-Stratification and Network Sampling",
      "link": "http://ai.googleblog.com/2011/08/google-at-joint-statistical-meetings-in.html",
      "author": "Posted by Marianna Dizik, Statistician"
    },
    {
      "title": "A new MIT center for mobile learning, with support from Google",
      "date": "Tuesday, August 16, 2011",
      "abstract": "A new MIT center for mobile learning, with support from Googlelong-standing relationshipApp Inventor for AndroidGoogle University RelationslabScratch project",
      "link": "http://ai.googleblog.com/2011/08/a-new-mit-center-for-mobile-learning.html",
      "author": "Posted by Hal Abelson, Professor of Computer Science and Engineering, MIT"
    },
    {
      "title": "Our Faculty Institute brings faculty back to the drawing board",
      "date": "Friday, August 12, 2011",
      "abstract": "Our Faculty Institute brings faculty back to the drawing boardOfficial Google BlogTeach for AmericaThe New Teacher ProjectDepartment of EducationEdutopia",
      "link": "http://ai.googleblog.com/2011/08/our-faculty-institute-brings-faculty.html",
      "author": "Unknown"
    },
    {
      "title": "Culturomics, Ngrams and new power tools for Science",
      "date": "Wednesday, August 10, 2011",
      "abstract": "Culturomics, Ngrams and new power tools for ScienceculturomicsSciencepapereradicatedScience Hall of FameneurosciencenuclearBabelstone",
      "link": "http://ai.googleblog.com/2011/08/culturomics-ngrams-and-new-power-tools.html",
      "author": "Posted by Erez Lieberman Aiden and Jean-Baptiste Michel, Visiting Faculty at Google"
    },
    {
      "title": "President's Council Recommends Open Data for Federal Agencies",
      "date": "Thursday, July 28, 2011",
      "abstract": "President's Council Recommends Open Data for Federal AgenciesPublic Sector and Elections Lab BlogPCASTfull reportpress releaseGoogle Fusion Tables",
      "link": "http://ai.googleblog.com/2011/07/president-council-recommends-open-data.html",
      "author": "Posted by Alon Halevy, Senior Staff Research Scientist"
    },
    {
      "title": "Studies Show Search Ads Drive 89% Incremental Traffic",
      "date": "Thursday, July 21, 2011",
      "abstract": "Studies Show Search Ads Drive 89% Incremental Traffichere",
      "link": "http://ai.googleblog.com/2011/07/studies-show-search-ads-drive-89.html",
      "author": "Posted by David Chan and Lizzy Van Alstine, Quantitative Management Team"
    },
    {
      "title": "Faculty from across the Americas meet in New York for the Faculty Summit",
      "date": "Wednesday, July 20, 2011",
      "abstract": "Faculty from across the Americas meet in New York for the Faculty SummitOfficial Google BlogComputer Science Faculty SummitNew York City officeInternet of thingsFaculty Summit sitedownload a PDFcluster managementmobile searchcommercefaculty reflectionsUniversity Relations website",
      "link": "http://ai.googleblog.com/2011/07/faculty-from-across-americas-meet-in.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Google Americas Faculty Summit: Reflections from our attendees",
      "date": "Tuesday, July 19, 2011",
      "abstract": "Google Americas Faculty Summit: Reflections from our attendeesComputer Science Faculty Summit",
      "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit.html",
      "author": "Posted by Alfred Spector, Vice President, Research"
    },
    {
      "title": "Google Americas Faculty Summit Day 2: Shopping, Coupons and Data",
      "date": "Monday, July 18, 2011",
      "abstract": "Google Americas Faculty Summit Day 2: Shopping, Coupons and Data",
      "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-2.html",
      "author": "Posted by Andrew W. Moore, Director, Google Commerce and Site Director, Pittsburgh"
    },
    {
      "title": "Google Americas Faculty Summit Day 1: Cluster Management",
      "date": "Friday, July 15, 2011",
      "abstract": "Google Americas Faculty Summit Day 1: Cluster Management",
      "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-1.html",
      "author": "Posted by John Wilkes, Principal Software Engineer"
    },
    {
      "title": "Google Americas Faculty Summit Day 1: Mobile Search",
      "date": "Friday, July 15, 2011",
      "abstract": "Google Americas Faculty Summit Day 1: Mobile Search",
      "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-1_15.html",
      "author": "Posted by Johan Schalkwyk, Software Engineer"
    },
    {
      "title": "What You Capture Is What You Get: A New Way for Task Migration Across Devices",
      "date": "Tuesday, July 12, 2011",
      "abstract": "What You Capture Is What You Get: A New Way for Task Migration Across Deviceshere",
      "link": "http://ai.googleblog.com/2011/07/what-you-capture-is-what-you-get-new.html",
      "author": "Posted by Yang Li, Research Scientist"
    },
    {
      "title": "Languages of the World (Wide Web)",
      "date": "Thursday, July 7, 2011",
      "abstract": "Languages of the World (Wide Web)",
      "link": "http://ai.googleblog.com/2011/07/languages-of-world-wide-web.html",
      "author": "Posted by Daniel Ford and Josh Batson"
    },
    {
      "title": "Google Translate welcomes you to the Indic web",
      "date": "Tuesday, June 21, 2011",
      "abstract": "Google Translate welcomes you to the Indic webTranslate BlogOfficial Google BlogGoogle TranslateIndic languagesSubject Object Verb (SOV) orderingSubject Verb Object (SVO) orderingagglutinativestatistical machine translation approachcorrect usGoogle Translator Toolkitsee what it meansTamilTeluguBengaliGujaratiKannada",
      "link": "http://ai.googleblog.com/2011/06/google-translate-welcomes-you-to-indic.html",
      "author": "Posted by Ashish Venugopal, Research Scientist"
    },
    {
      "title": "Auto-Directed Video Stabilization with Robust L1 Optimal Camera Paths",
      "date": "Monday, June 20, 2011",
      "abstract": "Auto-Directed Video Stabilization with Robust L1 Optimal Camera PathsMatthias GrundmannVivek KwatraIrfan EssaannouncedYouTube Video Editorthis paperCVPR 2011Google\u2019s exhibition boothpaper",
      "link": "http://ai.googleblog.com/2011/06/auto-directed-video-stabilization-with.html",
      "author": "Posted by Matthias Grundmann, Vivek Kwatra, and Irfan Essa, Research Team"
    },
    {
      "title": "Google at CVPR 2011",
      "date": "Thursday, June 16, 2011",
      "abstract": "Google at CVPR 2011IEEE International Conference on Computer Vision and Pattern RecognitionImage SearchYouTubeStreet ViewPicasaGogglesAndrew SeniorWhere's Waldo: Matching People in Images of CrowdsVisual and Semantic Similarity in ImageNetMulticore Bundle AdjustmentA Hierarchical Conditional Random Field Model for Labeling and Segmenting Images of Street ScenesKernelized Structural SVM Learning for Supervised Object SegmentationDiscriminative Tag Learning on YouTube Videos with Latent Sub-tagsAuto-Directed Video Stabilization with Robust L1 Optimal Camera PathsImage Saliency: From Local to Global Context",
      "link": "http://ai.googleblog.com/2011/06/google-at-cvpr-2011.html",
      "author": "Posted by Mei Han and Sergey Ioffe, Research Team"
    },
    {
      "title": "Our first round of Google Research Awards for 2011",
      "date": "Thursday, June 9, 2011",
      "abstract": "Our first round of Google Research Awards for 2011Google Research Awardsrecipientswebsite",
      "link": "http://ai.googleblog.com/2011/06/our-first-round-of-google-research.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Instant Mix for Music Beta by Google",
      "date": "Wednesday, June 8, 2011",
      "abstract": "Instant Mix for Music Beta by GoogleMusic BetaDay One KeynoteGoogle I/O 2011Machine Hearing: An Emerging FieldSound Ranking Using Auditory Sparse-Code RepresentationsLarge-Scale Music Annotation and Retrieval: Learning to Rank in Joint  Semantic Spaces",
      "link": "http://ai.googleblog.com/2011/06/instant-mix-for-music-beta-by-google.html",
      "author": "Posted by Douglas Eck, Research Scientist"
    },
    {
      "title": "After the award: students and mentors",
      "date": "Monday, June 6, 2011",
      "abstract": "After the award: students and mentorsRobert Hundthere",
      "link": "http://ai.googleblog.com/2011/06/after-award-students-and-mentors.html",
      "author": "Posted by Leslie Yeh Johnson, University Relations Manager"
    },
    {
      "title": "Google Scribe: Now with automatic text for links and faster formatting options",
      "date": "Thursday, May 26, 2011",
      "abstract": "Google Scribe: Now with automatic text for links and faster formatting optionsGoogle Scribe'sGoogle Labsscribe.googlelabs.comlet us know what you think",
      "link": "http://ai.googleblog.com/2011/05/google-scribe-now-with-automatic-text.html",
      "author": "Posted by Kartik Singh and Kuntal Loya, Google Scribe team"
    },
    {
      "title": "Google at ACL 2011",
      "date": "Wednesday, May 18, 2011",
      "abstract": "Google at ACL 2011Association for Computational Linguisticsmachine translationspeechthis years conferenceDekang LinArea ChairsPlatinum SponsorUnsupervised Part-of-Speech Tagging with Bilingual Graph-Based ProjectionsDipanjan DasSlav Petrovsyntactic analysisMarius PascaWeb Search Queries as a CorpusKuzman GanchevRich Prior Knowledge in Learning for Natural Language ProcessingKatja FilippovaMonolingual Text-to-Text GenerationUnsupervised Part-of-Speech Tagging with Bilingual Graph-Based ProjectionsLarge-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical ModelsPiggyback: Using Search Engines for Robust Cross-Domain Named Entity RecognitionBeam-Width Prediction for Efficient Context-Free ParsingLanguage-independent compound splitting with morphological operationsModel-Based Aligner Combination Using Dual DecompositionBinarized Forest to String TranslationSemi-supervised Latent Variable Models for Fine-grained Sentiment Analysis",
      "link": "http://ai.googleblog.com/2011/05/google-at-acl-2011.html",
      "author": "Posted by Ryan McDonald and Fernando Pereira, Research Team"
    },
    {
      "title": "Make beautiful interactive maps even faster with new additions to the Fusion Tables API",
      "date": "Tuesday, May 10, 2011",
      "abstract": "Make beautiful interactive maps even faster with new additions to the Fusion Tables APIGoogle Fusion Tableslaunchedinnovative applicationsFusion Tables web appGuardian\u2019s map of deprivation in the UKBay Citizen\u2019s Bike Accident trackerTexas Tribune\u2019s Census 2010 interactive mapFusion Tables SQL APIGoogle APIs ConsoleOAuth 2.0Google Group",
      "link": "http://ai.googleblog.com/2011/05/make-beautiful-interactive-maps-even.html",
      "author": "Posted by Rebecca Shapley, Jayant Madhavan, Rod McChesney, and Kathryn Hurley, Fusion Tables team"
    },
    {
      "title": "Google at CHI 2011",
      "date": "Thursday, May 5, 2011",
      "abstract": "Google at CHI 2011Technical Programs and Events BlogACM CHI: Conference on Human Factors in Computing SystemsChromeAndroidgesture-based interfacesGesture Avatar: A Technique for Operating Mobile User Interfaces Using Gestures  User-Defined Motion Gestures for Mobile InteractionExperimental Analysis of Touch-Screen Gesture Designs in Mobile EnvironmentsMany Bills: Engaging Citizens through Visualizations of Congressional LegislationYouPivot: Improving Recall with Contextual SearchOops, I Did It Again: Mitigating Repeated Access Control Errors on FacebookDeep Shot: A Framework for Migrating Tasks Across Devices Using Mobile Phone CamerasDoubleFlip: A Motion Gesture Delimiter for Mobile InteractionCrowdsourcing and Human Computation: Systems, Studies and PlatformsDesigning for User Experience: Academia & IndustryFestschrift Panel in Honor of Stuart K. CardCHI Should be Replicating and Validating Results More: DiscussTransferability of Research Findings: Context-Dependent or Model-DrivenThe Future of Child-Computer InteractionFrom Basecamp to Summit: Scaling Field Research Across 9 LocationsDesign and Analysis of Large Scale Log StudiesParticipatory Culture in the Age of Social Media",
      "link": "http://ai.googleblog.com/2011/05/google-at-chi-2011.html",
      "author": "Posted by Yang Li, Research Scientist"
    },
    {
      "title": "Partnering with Tsinghua University to support education in Western China",
      "date": "Thursday, April 14, 2011",
      "abstract": "Partnering with Tsinghua University to support education in Western ChinaTsinghua UniversityQinghaiXinjiangGuizhouNingxiaYunnan",
      "link": "http://ai.googleblog.com/2011/04/partnering-with-tsinghua-university-to.html",
      "author": "Unknown"
    },
    {
      "title": "1 billion core-hours of computational capacity for researchers",
      "date": "Thursday, April 7, 2011",
      "abstract": "1 billion core-hours of computational capacity for researchersGoogle Exacycle for Visiting FacultyUniversity RelationsVisiting Faculty ProgramGoogle Exacycle for Visiting Faculty website",
      "link": "http://ai.googleblog.com/2011/04/1-billion-core-hours-of-computational.html",
      "author": "Posted by Dan Belov, Principal Engineer and David Konerding, Software Engineer"
    },
    {
      "title": "Overlapping Experiment Infrastructure: More, Better, Faster Experimentation",
      "date": "Monday, April 4, 2011",
      "abstract": "Overlapping Experiment Infrastructure: More, Better, Faster Experimentationthis paper",
      "link": "http://ai.googleblog.com/2011/04/overlapping-experiment-infrastructure.html",
      "author": "Posted by Deirdre O'Brien and Diane Tang, Adwords Team"
    },
    {
      "title": "Ig-pay Atin-lay Oice-vay Earch-say",
      "date": "Friday, April 1, 2011",
      "abstract": "Ig-pay Atin-lay Oice-vay Earch-sayGoogle Voice Searchtwo dozen languages and dialectsWikipedia describes itGoogle Search app",
      "link": "http://ai.googleblog.com/2011/04/ig-pay-atin-lay-oice-vay-earch-say.html",
      "author": "Posted by Martin Jansche and Alex Salcianu, Google Speech Team"
    },
    {
      "title": "Word of Mouth: Introducing Voice Search for Indonesian, Malaysian and Latin American Spanish",
      "date": "Wednesday, March 30, 2011",
      "abstract": "Word of Mouth: Introducing Voice Search for Indonesian, Malaysian and Latin American Spanish",
      "link": "http://ai.googleblog.com/2011/03/word-of-mouth-introducing-voice-search.html",
      "author": "Unknown"
    },
    {
      "title": "Reading tea leaves in the tourism industry: A Case Study in the Gulf Oil Spill",
      "date": "Thursday, March 24, 2011",
      "abstract": "Reading tea leaves in the tourism industry: A Case Study in the Gulf Oil Spillpredict the presentGoogle Insights for Searchheretry your own hand",
      "link": "http://ai.googleblog.com/2011/03/reading-tea-leaves-in-tourism-industry.html",
      "author": "Posted by Hyunyoung Choi and Paul Liu, Senior Economists"
    },
    {
      "title": "Games, auctions and beyond",
      "date": "Wednesday, March 16, 2011",
      "abstract": "Games, auctions and beyondHebrew UniversityTel Aviv UniversityTechnion",
      "link": "http://ai.googleblog.com/2011/03/games-auctions-and-beyond.html",
      "author": "Posted by Yossi Matias, Senior Director, Head of Israel R&D Center"
    },
    {
      "title": "Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings",
      "date": "Thursday, March 10, 2011",
      "abstract": "Large Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddingspaper",
      "link": "http://ai.googleblog.com/2011/03/large-scale-image-annotation-learning.html",
      "author": "Posted by Jason Weston and Samy Bengio, Research Team"
    },
    {
      "title": "Building resources to syntactically parse the web",
      "date": "Wednesday, March 9, 2011",
      "abstract": "Building resources to syntactically parse the webmachine translationquestion answeringinformation extractionsentiment analysisthis paperThis papersentimentanalysissystemLinguistic Data Consortium (LDC)LDC catalog",
      "link": "http://ai.googleblog.com/2011/03/building-resources-to-syntactically.html",
      "author": "Posted by Slav Petrov and Ryan McDonald, Research Team"
    },
    {
      "title": "Slicing and dicing data for interactive visualization",
      "date": "Monday, February 28, 2011",
      "abstract": "Slicing and dicing data for interactive visualizationGoogle Public Data Explorer31 datasetslabor productivityInternet speedgender balance in parliamentsgovernment debt levelspopulation density by municipalitydataset upload interfaceDataset Publishing Languageread morePublic Data Explorer",
      "link": "http://ai.googleblog.com/2011/02/slicing-and-dicing-data-for-interactive.html",
      "author": "Posted by Benjamin Yolken, Google Public Data Product Manager"
    },
    {
      "title": "Where does my data live?",
      "date": "Friday, February 25, 2011",
      "abstract": "Where does my data live?Availability in Globally Distributed Storage Systems",
      "link": "http://ai.googleblog.com/2011/02/where-does-my-data-live.html",
      "author": "Posted by Daniel Ford, Senior Mathematician"
    },
    {
      "title": "A Runtime Solution for Online Contention Detection and Response",
      "date": "Friday, February 25, 2011",
      "abstract": "A Runtime Solution for Online Contention Detection and ResponseContention Aware Execution: Online Contention Detection and Response2010 Proceedings of the International Symposium on Code Generation and Optimization (CGO)",
      "link": "http://ai.googleblog.com/2011/02/a-runtime-solution-for-online.html",
      "author": "Posted by Jason Mars, Software Engineering Intern"
    },
    {
      "title": "Congratulations to Ken Thompson",
      "date": "Tuesday, February 22, 2011",
      "abstract": "Congratulations to Ken ThompsonKen ThompsonJapan Prizehere",
      "link": "http://ai.googleblog.com/2011/02/congratulations-to-ken-thompson.html",
      "author": "Posted by Bill Coughran, Senior Vice President of Engineering"
    },
    {
      "title": "Query Language Modeling for Voice Search",
      "date": "Thursday, February 17, 2011",
      "abstract": "Query Language Modeling for Voice SearchQuery Language Modeling for Voice Searchopen finite state toolsdistributed language modeling infrastructure",
      "link": "http://ai.googleblog.com/2011/02/query-language-modeling-for-voice-search.html",
      "author": "Posted by Ciprian Chelba, Research Scientist"
    },
    {
      "title": "Julia meets HTML 5",
      "date": "Monday, January 31, 2011",
      "abstract": "Julia meets HTML 5Julia MapJulia setsGoogle Maps APIHTML 5 canvasWeb workers#juliamap",
      "link": "http://ai.googleblog.com/2011/01/julia-meets-html-5.html",
      "author": "Posted by Daniel Wolf, Software Engineer"
    },
    {
      "title": "Google at NIPS 2010",
      "date": "Thursday, January 27, 2011",
      "abstract": "Google at NIPS 2010Neural Information Processing Systems Conference (NIPS)Sam RoweisLabel Embedding Trees for Large Multi-Class TasksLearning Bounds for Importance WeightingOnline Learning in the Manifold of Low-Rank MatricesDeterministic Single\u2013Pass Algorithm for LDADistributed Dual Averaging In NetworksCoarse\u2013to\u2013Fine Learning and InferenceLow\u2013rank Methods for Large\u2013scale Machine LearningLearning on Cores, Clusters, and CloudsLearning Structural SparsityOnline Learning in the Manifold of Low\u2013Rank MatricesDistributed MAP Inference for Undirected Graphical ModelsMapReduce/Bigtable for Distributed OptimizationSelf-Pruning Prediction TreesWeb Scale Image Annotation: Learning to Rank with Joint Word-Image EmbeddingsCoarse\u2013to\u2013fine Decoding for Parsing and Machine TranslationNIPS 2011",
      "link": "http://ai.googleblog.com/2011/01/google-at-nips-2010.html",
      "author": "Posted by Slav Petrov, Doug Aberdeen, and Lisa McCracken, Google Research"
    },
    {
      "title": "More Google Contributions to the Broader Scientific Community",
      "date": "Tuesday, January 25, 2011",
      "abstract": "More Google Contributions to the Broader Scientific Communityset of papersRobust Mechanisms for Risk-Averse SellersMonitoring Algorithms for Negative Feedback SystemsChildren's Roles Using Keyword Search Interfaces in the HomeLarge Scale Image Annotation: Learning to Rank with Joint Word-Image EmbeddingsOverlapping Experiment Infrastructure: More, BetterProducts of Random Latent Variable GrammarsContention Aware Execution: Online Contention Detection and ResponseSay What? Why users choose to speak their web queriesQuery Language Modeling for Voice SearchDremel: Interactive Analysis of Web-Scale DatasetsLarge-scale Incremental Processing Using Distributed Transactions and NotificationsAvailability in Globally Distributed Storage SystemsImproved Consistent Sampling, Weighted Minhash and L1 Sketching",
      "link": "http://ai.googleblog.com/2011/01/more-google-contributions-to-broader.html",
      "author": "Posted by Corinna Cortes and Alfred Spector, Google Research"
    },
    {
      "title": "Supporting computer science education with CS4HS",
      "date": "Thursday, January 20, 2011",
      "abstract": "Supporting computer science education with CS4HSstatisticsCS4HSwww.cs4hs.comCS4HS curriculum modules",
      "link": "http://ai.googleblog.com/2011/01/supporting-computer-science-education.html",
      "author": "Posted by Terry Ednacot, Education Program Manager"
    },
    {
      "title": "More researchers dive into the digital humanities",
      "date": "Monday, December 20, 2010",
      "abstract": "More researchers dive into the digital humanitiesDigital Humanities Research AwardsGoogle Books Ngram ViewerEuropean Public Policy Blog",
      "link": "http://ai.googleblog.com/2010/12/more-researchers-dive-into-digital.html",
      "author": "Posted by  Jon Orwant, Engineering Manager for Google Books"
    },
    {
      "title": "Robot hackathon connects with Android, browsers and the cloud",
      "date": "Friday, December 17, 2010",
      "abstract": "Robot hackathon connects with Android, browsers and the cloudApp Inventor for AndroidCellbots for AndroidPython libraryScripting Layer 4 AndroidiRobotLEGO GroupVEX RoboticsBluetooth supporttight integration with LEGO MINDSTORMSJava appopen sourcethe popular Cellbots projectopen source code",
      "link": "http://ai.googleblog.com/2010/12/robot-hackathon-connects-with-android.html",
      "author": "Posted by Ryan Hickman and Mamie Rheingold, 20% Robotics Task Force"
    },
    {
      "title": "Find out what\u2019s in a word, or five, with the Google Books Ngram Viewer",
      "date": "Thursday, December 16, 2010",
      "abstract": "Find out what\u2019s in a word, or five, with the Google Books Ngram ViewerCross-posted from the Google Books BlogGoogle Books Ngram ViewerHarvard University\u2019spublished today in ScienceWorld War I, Great Warchild care, nursery school, kindergartenfax, phone, emaillook before you leap, he who hesitates is lostvirus, bacteriatofu, hot dogburnt, burnedflute, guitar, trumpet, drumParis, London, New York, Boston, Romelaptop, mainframe, microcomputer, minicomputerfry, bake, grill, roastGeorge Washington, Thomas Jefferson, Abraham LincolnsupercalifragilisticexpialidociousDigital Humanities Research Awards",
      "link": "http://ai.googleblog.com/2010/12/find-out-whats-in-word-or-five-with.html",
      "author": "Posted by Jon Orwant, Engineering Manager, Google Books"
    },
    {
      "title": "Letting everyone do great things with App Inventor",
      "date": "Wednesday, December 15, 2010",
      "abstract": "Letting everyone do great things with App InventorannouncedGoogle Labsvocabulary appstrack their favorite public transportation routesmarriage proposal appLabsGoogle accountApp Inventor home pagepp Inventor user forum",
      "link": "http://ai.googleblog.com/2010/12/letting-everyone-do-great-things-with.html",
      "author": "Unknown"
    },
    {
      "title": "$6 million to faculty in Q4 Research Awards",
      "date": "Wednesday, December 8, 2010",
      "abstract": "$6 million to faculty in Q4 Research AwardsPDFwebsite",
      "link": "http://ai.googleblog.com/2010/12/6-million-to-faculty-in-q4-research.html",
      "author": "Unknown"
    },
    {
      "title": "Four Googlers elected ACM Fellows this year",
      "date": "Tuesday, December 7, 2010",
      "abstract": "Four Googlers elected ACM Fellows this yearlike last yearannouncedLuiz BarrosoDick LyonMuthu MuthukrishnanFernando PereiraProfessor Christos FaloutsosVisiting Faculty Member",
      "link": "http://ai.googleblog.com/2010/12/four-googlers-elected-acm-fellows-this.html",
      "author": "Posted by Alfred Spector, VP of Research"
    },
    {
      "title": "Google Launches Cantonese Voice Search in Hong Kong",
      "date": "Thursday, December 2, 2010",
      "abstract": "Google Launches Cantonese Voice Search in Hong KongCangjie\u5009\u9821\u6ed8\u897f\u5ddeKau Sai Chau\u7835\u5178\u4e4d\u8857Pottinger StreetDataHoundJordan\u4f50\u6566",
      "link": "http://ai.googleblog.com/2010/12/google-launches-cantonese-voice-search.html",
      "author": "Posted by Posted by Yun-hsuan Sung (\u5b8b\u96f2\u8ed2) and Martin Jansche, Google Research"
    },
    {
      "title": "Voice Search in Underrepresented Languages",
      "date": "Tuesday, November 9, 2010",
      "abstract": "Voice Search in Underrepresented Languagesinformation access for everybodySouth Africa\u2019s North-West UniversityMeraka Research InstituteAndroid app",
      "link": "http://ai.googleblog.com/2010/11/voice-search-in-underrepresented.html",
      "author": "Posted by Pedro J. Moreno, Staff Research Scientist and Johan Schalkwyk, Senior Staff Engineer"
    },
    {
      "title": "Suggesting a Better Remote Control",
      "date": "Thursday, November 4, 2010",
      "abstract": "Suggesting a Better Remote ControlQuickSuggest.",
      "link": "http://ai.googleblog.com/2010/11/suggesting-better-remote-control.html",
      "author": "Posted by Ullas Gargi and Rich Gossweiler, Research Team"
    },
    {
      "title": "Exploring Computational Thinking",
      "date": "Monday, October 25, 2010",
      "abstract": "Exploring Computational Thinkingresourcescomputational thinkingExploring Computational ThinkingCS4HSGoogle Code Universitywww.google.com/edu/ect",
      "link": "http://ai.googleblog.com/2010/10/exploring-computational-thinking.html",
      "author": "Posted by Elaine Kao, Education Program Manager"
    },
    {
      "title": "Google at the Conference on Empirical Methods in Natural Language Processing (EMNLP '10)",
      "date": "Monday, October 18, 2010",
      "abstract": "Google at the Conference on Empirical Methods in Natural Language Processing (EMNLP '10)The Conference on Empirical Methods in Natural Language ProcessingAmit SinghalChallenges in running a commercial search engineDual Decomposition for Parsing with Non-Projective Head Automata\"Poetic\" Statistical Machine Translation: Rhyme and MeterhereDmitriy GenzelJakob UszkoreitFranz OchEfficient Graph-Based Semi-Supervised Learning of Structured Tagging ModelsAmarnag SubramanyaSlav PetrovFernando PereiraUptraining for Accurate Deterministic Question ParsingSlav PetrovMichael RinggaardHiyan AlshawiSelf-training with Products of Latent Variable GrammarsSlav Petrov",
      "link": "http://ai.googleblog.com/2010/10/google-at-conference-on-empirical.html",
      "author": "Posted by Slav Petrov, Research Scientist"
    },
    {
      "title": "Kuzman Ganchev Receives Presidential Award from the Republic of Bulgaria",
      "date": "Friday, October 15, 2010",
      "abstract": "Kuzman Ganchev Receives Presidential Award from the Republic of BulgariaKuzman GanchevJohn Atanasoff award",
      "link": "http://ai.googleblog.com/2010/10/kuzman-ganchev-receives-presidential.html",
      "author": "Posted by Slav Petrov, Research Scientist"
    },
    {
      "title": "Korean Voice Input -- Have you Dictated your E-Mails in Korean lately?",
      "date": "Thursday, October 14, 2010",
      "abstract": "Korean Voice Input -- Have you Dictated your E-Mails in Korean lately?EnglishMandarinJapaneseFrench, Italian, German and Spanishblog postKorean Voice Search system",
      "link": "http://ai.googleblog.com/2010/10/korean-voice-input-have-you-dictated.html",
      "author": "Posted by Mike Schuster & Kaisuke Nakajima, Google Research"
    },
    {
      "title": "Clustering Related Queries Based on User Intent",
      "date": "Wednesday, October 13, 2010",
      "abstract": "Clustering Related Queries Based on User Intentmarsthe planet Marsplanets in the solar systemMars candy barMars the Roman god of warpaperNASA\u2019s missions to the planetlife on MarsJapanese comic seriesgrocery chainInternational World Wide Web conference",
      "link": "http://ai.googleblog.com/2010/10/clustering-related-queries-based-on.html",
      "author": "Posted by Jayant Madhavan and Alon Halevy"
    },
    {
      "title": "Google at USENIX Symposium on Operating Systems Design and Implementation (OSDI \u201810)",
      "date": "Tuesday, October 12, 2010",
      "abstract": "Google at USENIX Symposium on Operating Systems Design and Implementation (OSDI \u201810)The 9th USENIX Symposium on Operating Systems Design and Implementation (OSDI \u201810) was recently held in Vancouver, B.C.  This biennial conference is one of the premiere forums for presenting innovative research in distributed systems from both academia and industry, and we were glad to be a part of it.9th USENIX Symposium on Operating Systems Design and ImplementationIn addition to sponsoring this conference since 2002, Googlers contributed to the exchange of scientific ideas through authoring or co-authoring 3 published papers, organizing workshops, and serving on the program committee.  A short summary of the contributions:Large-scale Incremental Processing Using Distributed Transactions and Notificationsindex individual documents at very low latencyAvailability in Globally Distributed Storage SystemsOnix: A Distributed Control Platform for Large-scale Production NetworksIn addition to the papers presented by current Googlers, we were also happy to see that the recipient of the 2009 Google Ph.D. Fellowship in Cloud Computing, Roxana Geambasu, presented her work on Comet: An active distributed key-value store.2009 Google Ph.D. Fellowship in Cloud ComputingRoxana GeambasuComet: An active distributed key-value storeVideos of all of the talks from OSDI are available on the conference website for attendees and current USENIX members.  There is also a USENIX YouTube channel with a growing subset of the conference videos open to everyone.conference websiteUSENIX YouTube channelGoogle is making substantial progress on many of the grand challenge problems in computer science and artificial intelligence as part of its mission to organize the worlds information and make it useful.  Given the continuing increase in the scale of our distributed systems it\u2019s fair to say we\u2019ll have some other exciting new work to share at the next OSDI.  Hope to see you in 2012.",
      "link": "http://ai.googleblog.com/2010/10/google-at-usenix-symposium-on-operating.html",
      "author": "Posted by Murray Stokely, Software Engineer"
    },
    {
      "title": "Making an Impact on a Thriving Speech Research Community",
      "date": "Monday, October 11, 2010",
      "abstract": "Making an Impact on a Thriving Speech Research CommunityVoice ActionsGoogle Search by VoiceInterspeech 2010Direct Construction of Compact Context-Dependency Transducers From DataMichael RileyVoice Search for DevelopmentPedro J. MorenoUnsupervised Discovery and Training of Maximally Dissimilar Cluster ModelsFran\u00e7oise BeaufaysVincent VanhouckeSearch by Voice in Mandarin ChineseMartin JanschePedro J. MorenoOn-Demand Language Model Interpolation for Mobile Speech InputCyril AllauzenAlexander GruensteinBuilding Transcribed Speech Corpora Quickly and Cheaply for Many LanguagesThad HughesSay What? Why Users Choose to Speak their Web QueriesMaryam KamvarStudy on Interaction between Entropy Pruning and Kneser-Ney SmoothingCiprian ChelbaDecision Tree State Clustering with Word and Syllable FeaturesHank Liao",
      "link": "http://ai.googleblog.com/2010/10/making-impact-on-thriving-speech.html",
      "author": "Posted by Vincent Vanhoucke, Google Research"
    },
    {
      "title": "Bowls and Learning",
      "date": "Thursday, October 7, 2010",
      "abstract": "Bowls and Learningconvex loss functionpaper",
      "link": "http://ai.googleblog.com/2010/10/bowls-and-learning.html",
      "author": "Posted by Phil Long, Research Team"
    },
    {
      "title": "Poetic Machine Translation",
      "date": "Tuesday, October 5, 2010",
      "abstract": "Poetic Machine Translationpaper on poetry translationEMNLPmeterrhymeVladimir NabokovDouglas Hofstadter arguesGoogle TranslatelimerickshaikusEssai monographique sur les Dianthus des Pyr\u00e9n\u00e9es fran\u00e7aisescoupletsiambic tetrameterLa Henriadedactylic tetrameterLe Miroir des simples \u00e2mesMarguerite Poretehaikupapercommentary",
      "link": "http://ai.googleblog.com/2010/10/poetic-machine-translation.html",
      "author": "Posted by Dmitriy Genzel, Software Engineer"
    },
    {
      "title": "Veni, Vidi, Verba Verti",
      "date": "Thursday, September 30, 2010",
      "abstract": "Veni, Vidi, Verba Verticonvertat nuntios electronicosepigrammata effigierum YouTubisphilosophiaphysicismathematicaLibris Googlishunc",
      "link": "http://ai.googleblog.com/2010/09/veni-vidi-verba-verti.html",
      "author": "Posted by Jakob Uszkoreit, Ingeniarius Programmandi"
    },
    {
      "title": "Remembering Fred Jelinek",
      "date": "Friday, September 17, 2010",
      "abstract": "Remembering Fred Jelineklifetechnical achievements",
      "link": "http://ai.googleblog.com/2010/09/remembering-fred-jelinek.html",
      "author": "Posted by Ciprian Chelba, Research Team"
    },
    {
      "title": "Frowns, Sighs, and Advanced Queries -- How does search behavior change as search becomes more difficult?",
      "date": "Friday, September 17, 2010",
      "abstract": "Frowns, Sighs, and Advanced Queries -- How does search behavior change as search becomes more difficult?How does search behavior change as search becomes more difficult?here",
      "link": "http://ai.googleblog.com/2010/09/frowns-sighs-and-advanced-queries-how.html",
      "author": "Posted by Anne Aula, Rehan Khan, and Zhiwei Guan, User Experience Team"
    },
    {
      "title": "Focusing on Our Users: The Google Health Redesign",
      "date": "Wednesday, September 15, 2010",
      "abstract": "Focusing on Our Users: The Google Health Redesignhealth topicswww.google.com/health",
      "link": "http://ai.googleblog.com/2010/09/focusing-on-our-users-google-health.html",
      "author": "Posted by Hendrik Mueller, User Experience Researcher"
    },
    {
      "title": "Discontinuous Seam Carving for Video Retargeting",
      "date": "Monday, September 13, 2010",
      "abstract": "Discontinuous Seam Carving for Video Retargetingpaperproject web-site",
      "link": "http://ai.googleblog.com/2010/09/discontinuous-seam-carving-for-video.html",
      "author": "Posted by Matthias Grundmann and Vivek Kwatra, Google Research"
    },
    {
      "title": "Google Search by Voice:  A Case Study",
      "date": "Thursday, September 9, 2010",
      "abstract": "Google Search by Voice:  A Case StudyGOOG-411Google Search by VoiceAdvances in Speech Recognition",
      "link": "http://ai.googleblog.com/2010/09/google-search-by-voice-case-study.html",
      "author": "Posted by Johan Schalkwyk, Google Research"
    },
    {
      "title": "Towards Energy-Proportional Datacenters",
      "date": "Wednesday, September 1, 2010",
      "abstract": "Towards Energy-Proportional Datacentersseriespaper",
      "link": "http://ai.googleblog.com/2010/09/towards-energy-proportional-datacenters.html",
      "author": "Posted by Dennis Abts, Michael R. Marty, Philip M. Wells, Peter Klausler, and Hong Liu"
    },
    {
      "title": "Google North American Faculty Summit - Day 2",
      "date": "Wednesday, August 4, 2010",
      "abstract": "Google North American Faculty Summit - Day 2Google Faculty Summit",
      "link": "http://ai.googleblog.com/2010/08/google-north-american-faculty-summit.html",
      "author": "Posted by Andrew Tomkins, Director of Engineering, Google Research"
    },
    {
      "title": "Google North American Faculty Summit - cloud computing",
      "date": "Tuesday, August 3, 2010",
      "abstract": "Google North American Faculty Summit - cloud computingsecurity in the cloudDremel: Interactive Analysis of Web-Scale DatasetsBigQuerylaunchedFlumeJava: easy, efficient data-parallel pipelinesMapReduce",
      "link": "http://ai.googleblog.com/2010/08/google-north-american-faculty-summit_3.html",
      "author": "Posted by Brian Bershad, Director of Engineering, Site Director, Google Seattle"
    },
    {
      "title": "Google Publications",
      "date": "Friday, July 30, 2010",
      "abstract": "Google Publicationshere",
      "link": "http://ai.googleblog.com/2010/07/google-publications.html",
      "author": "Posted by Corinna Cortes and Alfred Spector, Google Research"
    },
    {
      "title": "Google North American Faculty Summit - Day 1",
      "date": "Friday, July 30, 2010",
      "abstract": "Google North American Faculty Summit - Day 1",
      "link": "http://ai.googleblog.com/2010/07/google-north-american-faculty-summit.html",
      "author": "Posted by \u00dalfar Erlingsson, Manager, Security Research"
    },
    {
      "title": "And the award goes to...",
      "date": "Tuesday, July 27, 2010",
      "abstract": "And the award goes to...Tushar ChandraDijkstra Prize in Distributed ComputingUnreliable Failure Detectors for Reliable Distributed SystemsThe Weakest Failure Detector for Solving Consensushere",
      "link": "http://ai.googleblog.com/2010/07/and-award-goes-to.html",
      "author": "Posted by Fernando Pereira, Research Director"
    },
    {
      "title": "Googlers receive multiple awards at the 2010 International Conference on Machine Learning",
      "date": "Tuesday, July 27, 2010",
      "abstract": "Googlers receive multiple awards at the 2010 International Conference on Machine Learningpaper awardsSajid SiddiqiHilbert Space Embeddings of Hidden Markov ModelsJohn DuchiOn the Consistency of Ranking AlgorithmsYoram SingerReducing Multiclass to Binary: A Unifying Approach for Margin Classifiers852 citations",
      "link": "http://ai.googleblog.com/2010/07/googlers-receive-multiple-awards-at.html",
      "author": "Posted by Fernando Pereira, Research Director"
    },
    {
      "title": "Announcing our Q2 Research Awards",
      "date": "Thursday, July 22, 2010",
      "abstract": "Announcing our Q2 Research Awardsherewebsite",
      "link": "http://ai.googleblog.com/2010/07/announcing-our-q2-research-awards.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations"
    },
    {
      "title": "Google PhD Fellowships go international",
      "date": "Thursday, July 15, 2010",
      "abstract": "Google PhD Fellowships go internationalOfficial Google BlogGoogle Fellowship program",
      "link": "http://ai.googleblog.com/2010/07/google-phd-fellowships-go-international.html",
      "author": "Posted by Alfred Spector, VP of Research and Special Initiatives"
    },
    {
      "title": "Our commitment to the digital humanities",
      "date": "Wednesday, July 14, 2010",
      "abstract": "Our commitment to the digital humanitiesOfficial Google BlogVenetus A manuscriptcollection of ancient Greek and Latin books",
      "link": "http://ai.googleblog.com/2010/07/our-commitment-to-digital-humanities.html",
      "author": "Posted by Jon Orwant, Engineering Manager for Google Books, Magazines and Patents"
    },
    {
      "title": "Google launches Korean Voice Search",
      "date": "Wednesday, June 30, 2010",
      "abstract": "Google launches Korean Voice SearchKorean voice search systemEnglishMandarinJapaneseFrench, Italian, German and Spanishpost",
      "link": "http://ai.googleblog.com/2010/06/google-launches-korean-voice-search.html",
      "author": "Posted by Mike Schuster & Martin Jansche, Google Research"
    },
    {
      "title": "Google Search by Voice now available in France, Italy, Germany and Spain",
      "date": "Monday, June 14, 2010",
      "abstract": "Google Search by Voice now available in France, Italy, Germany and SpainEnglishMandarin ChineseJapanesefour main European languagesBerufskraftfahrerqualifikationsgesetzEierschalensollbruchstellenverursacherVerkehrsinfrastrukturfinanzierungsgesellschaftStichpimpulibockforcelorumsupercalifragilisticespialidosochiripitiflauticoesternocleidomastoideo",
      "link": "http://ai.googleblog.com/2010/06/google-search-by-voice-now-available-in.html",
      "author": "Posted by Thad Hughes, Martin Jansche, and Pedro Moreno, Google Research"
    },
    {
      "title": "Google Fusion Tables celebrates one year of data management",
      "date": "Wednesday, June 9, 2010",
      "abstract": "Google Fusion Tables celebrates one year of data managementlaunched Google Fusion TablesFusion Tables APIlarge numbers of points, lines and polygonscustom HTML in map pop-up balloonstutorialsintegration with the Google Maps APIL.A. TimesKnoxville NewsChicago Tribuneapplication development contestthis oneheart-friendly and people-friendly hospital-findermap real-estate in Monterey countypotholes in Spainwind power dataethanol-selling stationsgeo portal to organize water data for AfricaFusion TablesFeature RequestUser GroupFacebookTwitter",
      "link": "http://ai.googleblog.com/2010/06/google-fusion-tables-celebrates-one.html",
      "author": "Posted by Alon Halevy, Google Research and Rebecca Shapley, User Experience"
    },
    {
      "title": "Recent Accomplishments by Research Award Recipients",
      "date": "Wednesday, May 19, 2010",
      "abstract": "Recent Accomplishments by Research Award RecipientsUniversity RelationsmissionbookWorld Wide Web conferenceSocial Impact AwardACM-W Athena Lecturer AwardKoji Kobayashi Computers and Communication AwardACM International Collegiate Programming Contest World Championswebsite",
      "link": "http://ai.googleblog.com/2010/05/recent-accomplishments-by-research.html",
      "author": "Posted by Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Five more languages on translate.google.com",
      "date": "Thursday, May 13, 2010",
      "abstract": "Five more languages on translate.google.comGoogle Translate BlogOfficial Google Blogauto-captioning on YouTubevirtual keyboards to searchGoogle Translatewe announcedtranslation memoriesTranslator Toolkittranslate.google.comBaietz lehenengoan\u0645\u06cc\u06ba \u062e\u0648\u0634 \u0642\u0633\u0645\u062a \u0645\u062d\u0633\u0648\u0633 \u06a9\u0631 \u0631\u06c1\u0627 \u06c1\u0648\u06ba\u0562\u0561\u056d\u057f\u0561\u0582\u0578\u0580 \u0565\u0574 \u0566\u0563\u0578\u0582\u0574M\u0259n \u015fansl\u0131yam\u10d8\u10e6\u10d1\u10d0\u10da\u10e1 \u10db\u10d8\u10d5\u10d4\u10dc\u10d3\u10dd\u10d1\u10d8",
      "link": "http://ai.googleblog.com/2010/05/five-more-languages-on.html",
      "author": "Posted by Ashish Venugopal, Research Scientist"
    },
    {
      "title": "Lessons learned developing a practical large scale machine learning system",
      "date": "Tuesday, April 6, 2010",
      "abstract": "Lessons learned developing a practical large scale machine learning systeminformation retrievalmachine translationUCI datasetGFS",
      "link": "http://ai.googleblog.com/2010/04/lessons-learned-developing-practical.html",
      "author": "Posted by Simon Tong, Google Research"
    },
    {
      "title": "Hopping on a Face Manifold via People Hopper",
      "date": "Wednesday, March 3, 2010",
      "abstract": "Hopping on a Face Manifold via People HopperannouncedGoogle LabsPeople Hopperpaperspill-treessampling-based matrix decomposition methodsTry People HopperfeedbackPeople Hopper homepage",
      "link": "http://ai.googleblog.com/2010/03/hopping-on-face-manifold-via-people.html",
      "author": "Posted by Sanjiv Kumar and Henry Rowley, Google Research"
    },
    {
      "title": "Announcing Google's Focused Research Awards",
      "date": "Tuesday, February 2, 2010",
      "abstract": "Announcing Google's Focused Research AwardsOfficial Google BlogGoogle Focused Research AwardsUniversity Relations",
      "link": "http://ai.googleblog.com/2010/02/announcing-google-focused-research.html",
      "author": "Posted by Alfred Spector, Vice President of Research and Special Initiatives"
    },
    {
      "title": "Research Areas of Interest:  Building scalable, robust cluster applications",
      "date": "Wednesday, January 27, 2010",
      "abstract": "Research Areas of Interest:  Building scalable, robust cluster applicationsherehereMulitmedia",
      "link": "http://ai.googleblog.com/2010/01/research-areas-of-interest-building.html",
      "author": "Posted by Brad Chen, Technical Lead/Manager"
    },
    {
      "title": "Google Cluster Data",
      "date": "Thursday, January 7, 2010",
      "abstract": "Google Cluster Dataherefeedback",
      "link": "http://ai.googleblog.com/2010/01/google-cluster-data.html",
      "author": "Posted by Joseph L. Hellerstein, Manager of Google Performance Analytics"
    },
    {
      "title": "Announcing our Q4 Research Awards",
      "date": "Tuesday, December 22, 2009",
      "abstract": "Announcing our Q4 Research AwardsmissionGoogle Research Awardsherewebsite",
      "link": "http://ai.googleblog.com/2009/12/announcing-our-q4-research-awards.html",
      "author": "Posted by Maggie Johnson, Director of Education & University Relations and Jeff Walz, Head of University Relations"
    },
    {
      "title": "Teaching a Computer to Understand Japanese",
      "date": "Tuesday, December 15, 2009",
      "abstract": "Teaching a Computer to Understand Japanesebloghow to get startedhere",
      "link": "http://ai.googleblog.com/2009/12/teaching-computer-to-understand-japanese.html",
      "author": "Posted by Mike Schuster, Google Research and Kaisuke Nakajima, Google Japan"
    },
    {
      "title": "Research Areas of Interest - Multimedia",
      "date": "Thursday, December 10, 2009",
      "abstract": "Research Areas of Interest - Multimedia",
      "link": "http://ai.googleblog.com/2009/12/research-areas-of-interest-multimedia.html",
      "author": "Posted by Michele Covell, Vision Research Team"
    },
    {
      "title": "Machine Learning with Quantum Algorithms",
      "date": "Tuesday, December 8, 2009",
      "abstract": "Machine Learning with Quantum Algorithmsmachine learningoptimizationquantum computerGrover\u2019s algorithmquantum adiabatic algorithmsEdward FarhiD-WavequbitsNIPS 2009theory paperhere",
      "link": "http://ai.googleblog.com/2009/12/machine-learning-with-quantum-algorithms.html",
      "author": "Posted by Hartmut Neven, Technical Lead Manager Image Recognition"
    },
    {
      "title": "Celebrating Computer Science Education Week",
      "date": "Monday, December 7, 2009",
      "abstract": "Celebrating Computer Science Education WeekOfficial Google BlogComputer Science Education Weekfree suite of our communication & collaboration applicationscommunity of teacherscontent and activitiesGoogle Code UniversitySummer of Codescholarshipinternprogramstech talks",
      "link": "http://ai.googleblog.com/2009/12/celebrating-computer-science-education.html",
      "author": "Posted by Alfred Spector, VP Research and Special Initiatives and Maggie Johnson, Director of Education and University Relations"
    },
    {
      "title": "Join us for the 2010 Google GRAD CS Forum!",
      "date": "Monday, December 7, 2009",
      "abstract": "Join us for the 2010 Google GRAD CS Forum!Google Student Blog",
      "link": "http://ai.googleblog.com/2009/12/join-us-for-2010-google-grad-cs-forum.html",
      "author": "Posted by Hanah Kim, University Programs"
    },
    {
      "title": "Automatic Captioning in YouTube",
      "date": "Friday, December 4, 2009",
      "abstract": "Automatic Captioning in YouTubeautomatic captioning and automatic alignment featureabout 20 hours of new material uploaded each minute2005 US censusmachine translation51 currently available languageshelp center article",
      "link": "http://ai.googleblog.com/2009/12/automatic-captioning-in-youtube.html",
      "author": "Posted by Christopher Alberti and Michiel Bacchiani, Google Research"
    },
    {
      "title": "Four Googlers elected ACM Fellows",
      "date": "Tuesday, December 1, 2009",
      "abstract": "Four Googlers elected ACM Fellows",
      "link": "http://ai.googleblog.com/2009/12/four-googlers-elected-acm-fellows.html",
      "author": "Posted by Alfred Spector, VP of Research"
    },
    {
      "title": "Explore Images with Google Image Swirl",
      "date": "Monday, November 23, 2009",
      "abstract": "Explore Images with Google Image Swirlannouncedway images are rankedGoogle Similar Imageslandmark recognitionPicasa's face recognitionWordNetprior effortpaper",
      "link": "http://ai.googleblog.com/2009/11/explore-images-with-google-image-swirl.html",
      "author": "Posted by Yushi Jing and Henry Rowley, Google Research"
    },
    {
      "title": "The 50th Symposium on Foundations of Computer Science (FOCS)",
      "date": "Friday, November 13, 2009",
      "abstract": "The 50th Symposium on Foundations of Computer Science (FOCS)Jon FeldmanVahab Mirrokni50th Annual Symposium on Foundations of Computer Science (FOCS)special one-day workshopRichard KarpPageRankMihalis YannakakisNoga AlonManuel Blumpolynomial-time smoothed analysis of the k-means clustering algorithmstronger version of Azuma's concentration inequalitysmoothed analysis frameworkonline stochastic matchingprevious blog post",
      "link": "http://ai.googleblog.com/2009/11/the-50th-symposium-on-foundations-of.html",
      "author": "Posted by Jon Feldman and Vahab Mirrokni, Google Research, NY"
    },
    {
      "title": "A 2x Faster Web",
      "date": "Thursday, November 12, 2009",
      "abstract": "A 2x Faster WebChromium Blogmake the web fasterdocumentationcodeGoogle Group",
      "link": "http://ai.googleblog.com/2009/11/a-2x-faster-web.html",
      "author": "Posted by Mike Belshe, Software Engineer and Roberto Peon, Software Engineer"
    },
    {
      "title": "Google Search by Voice Learns Mandarin Chinese",
      "date": "Monday, November 2, 2009",
      "abstract": "Google Search by Voice Learns Mandarin ChineseGoogle Mobile Appm.google.com",
      "link": "http://ai.googleblog.com/2009/11/google-search-by-voice-learns-mandarin.html",
      "author": "Posted by Pedro J. Moreno, Research Scientist"
    },
    {
      "title": "51 Languages in Google Translate",
      "date": "Monday, August 31, 2009",
      "abstract": "51 Languages in Google TranslateGoogle Translatefind and translateSpanishFrench Editions of Google NewsGoogle Translate chat botsAfrikaansBelarusianIcelandicIrishMacedonianMalaySwahiliWelshYiddishFrenchSpanishtranslate e-mailstranslate web pagestranslate RSS news feedstranslate documentsGoogle Translator Toolkit",
      "link": "http://ai.googleblog.com/2009/08/51-languages-in-google-translate.html",
      "author": "Posted by Franz Och, Principal Scientist"
    },
    {
      "title": "On the predictability of Search Trends",
      "date": "Monday, August 17, 2009",
      "abstract": "On the predictability of Search Trendslaunchingskibasketballyear-over-yearcategoriesFood & DrinkAutomotiveobamatwitterandroidglobal warmingNews & Current EventsFlu TrendsOn the Predictability of Search Trendspredicting the presentforecasting featurenew versionbasketballAutomotive",
      "link": "http://ai.googleblog.com/2009/08/on-predictability-of-search-trends.html",
      "author": "Posted by Yossi Matias, Niv Efron, and Yair Shimshoni, Google Labs, Israel."
    },
    {
      "title": "Under the Hood of App Inventor for Android",
      "date": "Tuesday, August 11, 2009",
      "abstract": "Under the Hood of App Inventor for AndroidApp Inventor for AndroidSchemeScratchSimpleS-expressionKawa",
      "link": "http://ai.googleblog.com/2009/08/under-hood-of-app-inventor-for-android.html",
      "author": "Posted by Bill Magnuson, Hal Abelson, and Mark Friedman"
    },
    {
      "title": "Two Views from the 2009 Google Faculty Summit",
      "date": "Monday, August 3, 2009",
      "abstract": "Two Views from the 2009 Google Faculty SummitOfficial Google BlogComputer Science Faculty SummitFaculty Summit Agendaintroductory presentationProfessor Deborah EstrinProfessor John OusterhoutFlu Trends",
      "link": "http://ai.googleblog.com/2009/08/two-views-from-2009-google-faculty.html",
      "author": "Posted by Alfred Spector, Vice President of Research and Special Initiatives"
    },
    {
      "title": "App Inventor for Android",
      "date": "Friday, July 31, 2009",
      "abstract": "App Inventor for AndroidGoogle University RelationsAndroid applicationsfeedback",
      "link": "http://ai.googleblog.com/2009/07/app-inventor-for-android.html",
      "author": "Posted by Hal Abelson, Visiting Faculty"
    },
    {
      "title": "Predicting Initial Claims for Unemployment Benefits",
      "date": "Wednesday, July 22, 2009",
      "abstract": "Predicting Initial Claims for Unemployment BenefitsRobert GordonJames Hamiltonearlier blog postDepartment of Laborherethis paper",
      "link": "http://ai.googleblog.com/2009/07/predicting-initial-claims-for.html",
      "author": "Posted by Hal Varian, Chief Economist and Hyunyoung Choi, Sr. Economist"
    },
    {
      "title": "ACM EC Conference and Workshop on Ad Auctions",
      "date": "Tuesday, July 21, 2009",
      "abstract": "ACM EC Conference and Workshop on Ad AuctionsJon FeldmanVahab MirrokniEC 20095th Workshop on Ad AuctionstutorialsMuthu MuthukrishnanH. Roeglintutorial at ICMLprevious blog postAd Auctions WorkshopHal VarianGoogle Trendspredicting the presentoptimal pricing mechanisms over social networksoffline optimization in stochastic online ad allocation problemsThe theoretical resultsFOCS 2009",
      "link": "http://ai.googleblog.com/2009/07/acm-ec-conference-and-workshop-on-ad.html",
      "author": "By Jon Feldman and Vahab Mirrokni, Google Research, NY"
    },
    {
      "title": "Google's Research Awards Program Update",
      "date": "Tuesday, July 14, 2009",
      "abstract": "Google's Research Awards Program UpdateResearch Awards Program",
      "link": "http://ai.googleblog.com/2009/07/google-research-awards-program-update.html",
      "author": "Posted by Posted by Juan E. Vargas, University Relations"
    },
    {
      "title": "International Conference on Machine Learning (ICML 2009) in Montreal",
      "date": "Thursday, July 2, 2009",
      "abstract": "International Conference on Machine Learning (ICML 2009) in MontrealEyal Even DarVahab MirrokniICML 2009COLT 2009UAI 2009Corinna Cortesresearch in learning kernelsConvergence of Natural Game Dynamics",
      "link": "http://ai.googleblog.com/2009/07/international-conference-on-machine.html",
      "author": "Posted by Eyal Even Dar and Vahab Mirrokni, Google Research, NY"
    },
    {
      "title": "Speed Matters",
      "date": "Tuesday, June 23, 2009",
      "abstract": "Speed Mattersexperimentscode.google.com/speedPDF",
      "link": "http://ai.googleblog.com/2009/06/speed-matters.html",
      "author": "Posted by Jake Brutlag, Web Search Infrastructure"
    },
    {
      "title": "A new landmark in computer vision",
      "date": "Monday, June 22, 2009",
      "abstract": "A new landmark in computer visionOfficial Google BlogComputer Vision and Pattern Recognition (CVPR) conferenceGolden Gate BridgePicasaPanoramioGoogle Image Searchpaper",
      "link": "http://ai.googleblog.com/2009/06/a-new-landmark-in-computer-vision.html",
      "author": "Posted by Jay Yagnik, Head of Computer Vision Research"
    },
    {
      "title": "Large-scale graph computing at Google",
      "date": "Monday, June 15, 2009",
      "abstract": "Large-scale graph computing at GoogleWeb 2.0major Internet companyBulk Synchronous Parallel ModelPageRankGreg MalewiczACM PODCACM SPAAThe seven bridges of K\u00f6nigsbergLeonhard Euler's",
      "link": "http://ai.googleblog.com/2009/06/large-scale-graph-computing-at-google.html",
      "author": "Posted by Grzegorz Czajkowski, Systems Infrastructure Team"
    },
    {
      "title": "Google Fusion Tables",
      "date": "Tuesday, June 9, 2009",
      "abstract": "Google Fusion TablesGoogle Fusion TablesLabsFusion Tablesfeedback",
      "link": "http://ai.googleblog.com/2009/06/google-fusion-tables.html",
      "author": "Posted by Alon Halevy, Google Research and Rebecca Shapley, User Experience"
    },
    {
      "title": "Remembering Rajeev Motwani",
      "date": "Monday, June 8, 2009",
      "abstract": "Remembering Rajeev Motwanihis blogStanford blog commemorating Rajeev",
      "link": "http://ai.googleblog.com/2009/06/remembering-rajeev-motwani.html",
      "author": "Posted by Alfred Spector, VP of Research"
    },
    {
      "title": "Google Fellowships, the Nuts and Bolts",
      "date": "Friday, May 15, 2009",
      "abstract": "Google Fellowships, the Nuts and BoltsOfficial Google Blog",
      "link": "http://ai.googleblog.com/2009/05/google-fellowships-nuts-and-bolts.html",
      "author": "Posted by Leslie Yeh Johnson, Google University Relations"
    },
    {
      "title": "The best and the brightest",
      "date": "Friday, May 15, 2009",
      "abstract": "The best and the brightestOfficial Google Blog",
      "link": "http://ai.googleblog.com/2009/05/the-best-and-brightest.html",
      "author": "Posted by Leslie Yeh Johnson, Google University Relations"
    },
    {
      "title": "ACM Multimedia 2009 Grand Challenges",
      "date": "Tuesday, May 12, 2009",
      "abstract": "ACM Multimedia 2009 Grand ChallengesResearch AwardsVisiting Faculty ProgramGrand ChallengesACM Multimedia ConferenceMor NaamanTat-Seng Chuahere",
      "link": "http://ai.googleblog.com/2009/05/acm-multimedia-2009-grand-challenges.html",
      "author": "Posted by Jay Yagnik, Head of Computer Vision Research"
    },
    {
      "title": "The bar-bet phenomenon:  increasing diversity in mobile searches",
      "date": "Thursday, May 7, 2009",
      "abstract": "The bar-bet phenomenon:  increasing diversity in mobile searchesresearchbar-betpub-quizfull paper",
      "link": "http://ai.googleblog.com/2009/05/the-bar-bet-phenomenon-increasing.html",
      "author": "Posted by Maryam Kamvar, Melanie Kellar, Rajan Patel and Ya Xu, Google Research"
    },
    {
      "title": "Cloud Computing and the Internet",
      "date": "Tuesday, April 28, 2009",
      "abstract": "Cloud Computing and the Internet",
      "link": "http://ai.googleblog.com/2009/04/cloud-computing-and-internet.html",
      "author": "Posted by Vinton Cerf, Chief Internet Evangelist"
    },
    {
      "title": "The Continuing Metamorphosis of the Web",
      "date": "Monday, April 27, 2009",
      "abstract": "The Continuing Metamorphosis of the Web18th World Wide Web Conference in MadridGoogle Official BlogThe Intelligent CloudFranz OchslidesWWW2009 website",
      "link": "http://ai.googleblog.com/2009/04/the-continuing-metamorphosis-of-web.html",
      "author": "Posted by Alfred Spector, VP Research and Special Initiatives"
    },
    {
      "title": "Congratulations to NSF CLuE Grant awardees",
      "date": "Thursday, April 23, 2009",
      "abstract": "Congratulations to NSF CLuE Grant awardeesAcademic Cluster Computing Initiativeexpanding that programannounced the 2009 CLuE grantsSmall Grant for Exploratory ResearchMapReduceFile System",
      "link": "http://ai.googleblog.com/2009/04/congratulations-to-nsf-clue-grant.html",
      "author": "Posted by Jeff Walz and Andrea Held"
    },
    {
      "title": "Socially Adjusted CAPTCHAs",
      "date": "Thursday, April 16, 2009",
      "abstract": "Socially Adjusted CAPTCHAshere",
      "link": "http://ai.googleblog.com/2009/04/socially-adjusted-captchas.html",
      "author": "Posted by Rich Gossweiler, Maryam Kamvar, Shumeet Baluja"
    },
    {
      "title": "The Grill: Google's Alfred Spector on the hot seat",
      "date": "Wednesday, April 15, 2009",
      "abstract": "The Grill: Google's Alfred Spector on the hot seatAlfred Spectorhere",
      "link": "http://ai.googleblog.com/2009/04/the-grill-google-alfred-spector-on-hot.html",
      "author": "Posted by Ben Bayer, Google Research"
    },
    {
      "title": "Predicting the Present with Google Trends",
      "date": "Thursday, April 2, 2009",
      "abstract": "Predicting the Present with Google TrendsGoogle TrendsGoogle Insights for SearchPredicting the Present with Google Trendsautomobile saleshome salesretail salestravel behavior",
      "link": "http://ai.googleblog.com/2009/04/predicting-present-with-google-trends.html",
      "author": "Posted by Hal Varian, Chief Economist and Hyunyoung Choi, Decision Support Engineering Analyst"
    },
    {
      "title": "The Unreasonable Effectiveness of Data",
      "date": "Wednesday, March 25, 2009",
      "abstract": "The Unreasonable Effectiveness of Datahere",
      "link": "http://ai.googleblog.com/2009/03/the-unreasonable-effectiveness-of-data.html",
      "author": "Posted by Fernando Pereira, Google Research"
    },
    {
      "title": "Google and WPP Marketing Research Awards: Improving industry understanding and practices in online marketing",
      "date": "Thursday, March 19, 2009",
      "abstract": "Google and WPP Marketing Research Awards: Improving industry understanding and practices in online marketingWPP Groupwebsite",
      "link": "http://ai.googleblog.com/2009/03/google-and-wpp-marketing-research.html",
      "author": "Posted by Jeff Walz, University Relations and Anne Bray, Head of Agency, WPP"
    },
    {
      "title": "And the award goes to...",
      "date": "Wednesday, March 18, 2009",
      "abstract": "And the award goes to...soft-margin support vector machine",
      "link": "http://ai.googleblog.com/2009/03/and-award-goes-to.html",
      "author": "Posted by Fernando Pereira, Research Director"
    },
    {
      "title": "Beyond Web-2.0",
      "date": "Wednesday, February 18, 2009",
      "abstract": "Beyond Web-2.0lightning talkToward 2^W --- Beyond Web 2.0Communications Of The ACMhere",
      "link": "http://ai.googleblog.com/2009/02/beyond-web-20.html",
      "author": "Posted by T.V Raman, Research Scientist"
    },
    {
      "title": "Market Algorithms and Optimization Meeting",
      "date": "Wednesday, January 28, 2009",
      "abstract": "Market Algorithms and Optimization MeetingAlfredSpectorTruthfulness is impossibledesirable propertiesrich bidding featureshereWWW08WINE08SODA09publications\u00a0sitehereBobby KleinbergSilvio Micalidesign such mechanisms for combinatorial auctionsKamesh MunagalaAnna Karlinhow to run auctionsRichard ColeAmos FiatUri FeigeMichel GoemansAnupam GuptaNicole ImmorlicaMichael Rabin\u00a0Eva Tardos",
      "link": "http://ai.googleblog.com/2009/01/market-algorithms-and-optimization.html",
      "author": "Posted by"
    },
    {
      "title": "Google University Research Awards",
      "date": "Wednesday, January 28, 2009",
      "abstract": "Google University Research AwardsGoogle Research Awards ProgramAlfredSpectorrecent paperGoogle Research home pageHector Garcia-MolinahereSudhanvaGurumurthiMircea Stan2008 International Symposium on Computer Architecture (ISCA)IEEE MicroMaxineEskenaziweb site",
      "link": "http://ai.googleblog.com/2009/01/google-university-research-awards.html",
      "author": "Posted by Juan Vargas, University Relations"
    },
    {
      "title": "Smart Thumbnails on YouTube",
      "date": "Monday, January 19, 2009",
      "abstract": "Smart Thumbnails on YouTubeYouTube Bloglast year",
      "link": "http://ai.googleblog.com/2009/01/smart-thumbnails-on-youtube.html",
      "author": "Posted by\u00a0Tom\u00e1\u0161 I\u017eo (Software Engineer) and Jay Yagnik (Head of Computer Vision Research)"
    },
    {
      "title": "Maybe your computer just needs a hug",
      "date": "Monday, January 12, 2009",
      "abstract": "Maybe your computer just needs a hug",
      "link": "http://ai.googleblog.com/2009/01/maybe-your-computer-just-needs-hug.html",
      "author": "Posted by Anthony Francis"
    },
    {
      "title": "Translation is Risky Business",
      "date": "Tuesday, December 30, 2008",
      "abstract": "Translation is Risky Businesslanguagetranslationhere",
      "link": "http://ai.googleblog.com/2008/12/translation-is-risky-business.html",
      "author": "Posted by Shankar Kumar and Wolfgang Macherey"
    },
    {
      "title": "plop: Probabilistic Learning of Programs",
      "date": "Monday, November 10, 2008",
      "abstract": "plop: Probabilistic Learning of ProgramsOpen Source at Googleplop projectelsewheremy doctoral dissertationthe plop wiki on Google Code",
      "link": "http://ai.googleblog.com/2008/11/plop-probabilistic-learning-of-programs.html",
      "author": "Posted by Moshe Looks"
    },
    {
      "title": "New Technology Roundtable Series",
      "date": "Friday, October 3, 2008",
      "abstract": "New Technology Roundtable SeriesAlfred SpectorGoogle Technology Roundtable SeriesLarge-Scale Search System Infrastructure and Search QualityJeff DeanAmit SinghalMap ReduceJerry ZhaoMatt AusternThe Intelligent CloudApplications of Human Language TechnologyFranz Josef Och",
      "link": "http://ai.googleblog.com/2008/10/new-technology-roundtable-series.html",
      "author": "Posted by Alfred Spector, VP of Research and Special Initiatives"
    },
    {
      "title": "Doubling Up",
      "date": "Monday, September 29, 2008",
      "abstract": "Doubling Uptranslate.google.comCatalanFilipinoHebrewIndonesianLatvianLithuanianSerbianSlovakSlovenianUkrainianVietnamese",
      "link": "http://ai.googleblog.com/2008/09/doubling-up.html",
      "author": "Posted by Franz Josef Och"
    },
    {
      "title": "Remembering Randy Pausch",
      "date": "Saturday, July 26, 2008",
      "abstract": "Remembering Randy PauschAchieving your Childhood Dreams",
      "link": "http://ai.googleblog.com/2008/07/remembering-randy-pausch.html",
      "author": "Posted by Kevin McCurley, Research Team"
    },
    {
      "title": "Machine Learning Meeting",
      "date": "Tuesday, May 20, 2008",
      "abstract": "Machine Learning MeetingPhil LongMachine LearningArtificial IntelligenceGoogle Translate1-800-GOOG-411New YorkCJ LinPeng Xu",
      "link": "http://ai.googleblog.com/2008/05/machine-learning-meeting.html",
      "author": "Posted by Phil Long"
    },
    {
      "title": "Can You Publish at Google?",
      "date": "Tuesday, May 6, 2008",
      "abstract": "Can You Publish at Google?Rich Gossweilerinterview processresearchIt is easy to publishopen sourcetalksexperimentculture.",
      "link": "http://ai.googleblog.com/2008/05/can-you-publish-at-google.html",
      "author": "Posted by Rich Gossweiler"
    },
    {
      "title": "VisualRank",
      "date": "Thursday, May 1, 2008",
      "abstract": "VisualRankPageRank for Product Image Searchcoverageclick here to help speed up the processjust follow the link",
      "link": "http://ai.googleblog.com/2008/05/visualrank.html",
      "author": "Posted by Shumeet Baluja and Yushi Jing"
    },
    {
      "title": "Research in the Cloud: Providing Cutting Edge Computational Resources to Scientists",
      "date": "Wednesday, April 23, 2008",
      "abstract": "Research in the Cloud: Providing Cutting Edge Computational Resources to ScientistsAcademic Cloud Computing InitiativeCluE initiativesolicitationNSF site",
      "link": "http://ai.googleblog.com/2008/04/research-in-cloud-providing-cutting.html",
      "author": "Posted by Christophe Bisciglia, Senior Software Engineer, and Alfred Spector, Vice President of Research"
    },
    {
      "title": "Deploying Goog411",
      "date": "Friday, March 28, 2008",
      "abstract": "Deploying Goog411our ICASSP paperfeedback",
      "link": "http://ai.googleblog.com/2008/03/deploying-goog411.html",
      "author": "Posted by Francoise Beaufays"
    },
    {
      "title": "This year's scalability conference",
      "date": "Monday, February 11, 2008",
      "abstract": "This year's scalability conference",
      "link": "http://ai.googleblog.com/2008/02/this-year-scalability-conference.html",
      "author": "Posted by Andrew Schwerin, Software Engineer"
    },
    {
      "title": "Google Education Summit",
      "date": "Thursday, October 18, 2007",
      "abstract": "Google Education SummitGoogle Education Summitrecent blog post",
      "link": "http://ai.googleblog.com/2007/10/google-education-summit.html",
      "author": "Posted by Jeff Walz and Kevin McCurley"
    },
    {
      "title": "OpenHTMM Released",
      "date": "Sunday, September 23, 2007",
      "abstract": "OpenHTMM ReleasedProbabilistic latent semantic analysisLatent Dirichlet AllocationAmit GruberEleventh International Conference on Artificial Intelligence and StatisticsHidden Topic Markov Models (HTMM)Michal Rosen-ZviYair WeissOpenHTMM",
      "link": "http://ai.googleblog.com/2007/09/openhtmm-released.html",
      "author": "Posted by Ashok C. Popat, Research Scientist"
    },
    {
      "title": "The Sky is Open",
      "date": "Wednesday, September 19, 2007",
      "abstract": "The Sky is OpenSky in Google Earthwcs2kml",
      "link": "http://ai.googleblog.com/2007/09/the-sky-is-open.html",
      "author": "Posted by Jeremy Brewer"
    },
    {
      "title": "Introducing Sky in Google Earth",
      "date": "Tuesday, August 21, 2007",
      "abstract": "Introducing Sky in Google EarthGoogle Earth 4.2 client Google Earth Gallery Sky section author your own website.",
      "link": "http://ai.googleblog.com/2007/08/introducing-sky-in-google-earth.html",
      "author": "Posted by Andy Connolly and Ryan Scranton"
    },
    {
      "title": "Drink from the firehose with University Research Programs",
      "date": "Thursday, July 26, 2007",
      "abstract": "Drink from the firehose with University Research Programscompany missionGoogle Faculty SummitUniversity Research Program for Google SearchUniversity Research Program for Google Translatesurprise usGoogle Code for Educators",
      "link": "http://ai.googleblog.com/2007/07/drink-from-firehose-with-university.html",
      "author": "Posted by Michael Lancaster and Josh Estelle, Software Engineers"
    },
    {
      "title": "New Conference on Web Search and Data Mining",
      "date": "Monday, June 18, 2007",
      "abstract": "New Conference on Web Search and Data Miningnew conference on Web Search and Data MiningWSDM web site",
      "link": "http://ai.googleblog.com/2007/06/new-conference-on-web-search-and-data.html",
      "author": "Posted by Ziv Bar-Yossef and Kevin McCurley, Research Team"
    },
    {
      "title": "Videos of talks",
      "date": "Monday, June 18, 2007",
      "abstract": "Videos of talksGoogle Researchvideos of talks at GoogleOne of the best features of working at Google is the rich variety of talks that we can attend, both technical and general interest.  Most of these are videotaped for later viewing.  This has multiple benefits:",
      "link": "http://ai.googleblog.com/2007/06/videos-of-talks.html",
      "author": "Posted by Kevin McCurley, Research Team"
    },
    {
      "title": "Seattle conference on scalability",
      "date": "Friday, February 16, 2007",
      "abstract": "Seattle conference on scalabilityscalabilityscalabilityconf@google.comfood",
      "link": "http://ai.googleblog.com/2007/02/seattle-conference-on-scalability.html",
      "author": "Posted by Amanda Camp, Software Engineer"
    },
    {
      "title": "Hear, here.   A Sample of Audio Processing at Google.",
      "date": "Wednesday, February 14, 2007",
      "abstract": "Hear, here.   A Sample of Audio Processing at Google.Waveprint OverviewWaveprint-for-Known-AudioMusic Identification with WFSTMusic Similarity",
      "link": "http://ai.googleblog.com/2007/02/hear-here-sample-of-audio-processing-at.html",
      "author": "Posted by Shumeet Baluja, Michele Covell, Pedro Moreno & Eugene Weinstein"
    },
    {
      "title": "Google Research Picks for Videos of the Year",
      "date": "Monday, December 11, 2006",
      "abstract": "Google Research Picks for Videos of the YearoursWinning the DARPA Grand ChallengeThe Graphing Calculator StoryShould Google Go Nuclear?A New Way to Look at NetworkingPython 3000How to Survive a Robot UprisingThe New \"Bill of Rights of Information Society\"Practical Common LispDebugging Backwards in TimeBuilding Large Systems at GoogleThe Science and Art of User Experience at GoogleUniversally Accessible Demands Accessibility for All of HumanityDNA and the BrainSteve WozniakJane GoodallComputers Versus Common SenseThe Google StoryThe SearchThe Archimedes PalimpsestThe Paradox of Choice - Why More is Less",
      "link": "http://ai.googleblog.com/2006/12/google-research-picks-for-videos-of-year.html",
      "author": "Posted by Peter Norvig"
    },
    {
      "title": "CSCW 2006: Collaborative editing 20 years later",
      "date": "Tuesday, November 28, 2006",
      "abstract": "CSCW 2006: Collaborative editing 20 years laterGoogle Docstalked about this problemACM CSCWhow photos and voice profiles affect people's choice of gaming partnersworkshop exploring how people trust -- and learn to trustGoogle Docs & SpreadsheetsGoogle Notebookuser experience research and design",
      "link": "http://ai.googleblog.com/2006/11/cscw-2006-collaborative-editing-20.html",
      "author": "Posted by Lilly Irani & Jens Riegelsberger, User Experience team"
    },
    {
      "title": "And the Awards Go To ...",
      "date": "Friday, September 22, 2006",
      "abstract": "And the Awards Go To ...SIGKDD Innovation AwardNeural Network based face detectionCVPRprogramming contestDistinguished Alumnicommencement addressReturn of Gonzo GizmosScientific American Book ClubACL/COLINGSemantic taxonomy induction from heterogenous evidenceICMLHow Boosting the Margin Can Also Boost Classifier Complexitymentioned earlierPaul RademacherTechnology Reviewhousingmaps.comVLDB 10 Year Best Paper Award",
      "link": "http://ai.googleblog.com/2006/09/and-awards-go-to.html",
      "author": "Posted by Proud Googlers"
    },
    {
      "title": "All Our N-gram are Belong to You",
      "date": "Thursday, August 3, 2006",
      "abstract": "All Our N-gram are Belong to Youn-gram modelsstatistical machine translationspelling correctioninfrastructureLDClet us hear from youdata available",
      "link": "http://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html",
      "author": "Posted by Alex Franz and Thorsten Brants, Google Machine Translation Team"
    },
    {
      "title": "Call for attendees - Conference on Test Automation",
      "date": "Wednesday, July 12, 2006",
      "abstract": "Call for attendees - Conference on Test Automationnoted earliereasy form",
      "link": "http://ai.googleblog.com/2006/07/call-for-attendees-conference-on-test.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation livevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testinglondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon Strategyhiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPthe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "Interactive TV: Conference and Best Paper",
      "date": "Tuesday, June 6, 2006",
      "abstract": "Interactive TV: Conference and Best PaperRead MoreOur paper [pdf]",
      "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html",
      "author": "Posted by Michele Covell & Shumeet Baluja, Research Scientists"
    },
    {
      "title": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken",
      "date": "Friday, June 2, 2006",
      "abstract": "Extra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are BrokenRead More3.4.3.3Programming Pearls The Sun bug report describing this bug in the JDK A 2003 paper by Salvatore Ruggieri",
      "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html",
      "author": "Posted by Joshua Bloch, Software Engineer"
    },
    {
      "title": "Statistical machine translation live",
      "date": "Friday, April 28, 2006",
      "abstract": "Statistical machine translation liveRead Morevery good resultsArabic-EnglishEnglish-Arabicdiscussion forum",
      "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html",
      "author": "Posted by Franz Och, Research Scientist"
    },
    {
      "title": "Our conference on automated testing",
      "date": "Thursday, April 27, 2006",
      "abstract": "Our conference on automated testingRead Morelondontestconf@google.com",
      "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html",
      "author": "Posted by Allen Hutchison, Engineering Manager"
    },
    {
      "title": "See you at CHI",
      "date": "Sunday, April 23, 2006",
      "abstract": "See you at CHIRead Moreusability testsCHIhereuser experience researchersUI designersmore",
      "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html",
      "author": "Posted by Rick Boardman, User Experience Researcher"
    },
    {
      "title": "First Robots",
      "date": "Wednesday, March 22, 2006",
      "abstract": "First RobotsCheesy PoofsRead MoreCheesy PoofsFIRST",
      "link": "http://ai.googleblog.com/2006/03/first-robots.html",
      "author": "Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman"
    },
    {
      "title": "Hiring: The Lake Wobegon Strategy",
      "date": "Saturday, March 11, 2006",
      "abstract": "Hiring: The Lake Wobegon StrategyRead Morehiring and working philosophy",
      "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    },
    {
      "title": "An experimental study of P2P VoIP",
      "date": "Tuesday, March 7, 2006",
      "abstract": "An experimental study of P2P VoIPRead Morethe paper",
      "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html",
      "author": "Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"
    },
    {
      "title": "Teamwork for problem-solving",
      "date": "Saturday, March 4, 2006",
      "abstract": "Teamwork for problem-solvingRead MoreRobert TarjanJohn LaffertyBrian KernighanJohn HopcroftMichael RabinChristos PapadimitriouVladimir Vapnik latest meteor findings in Antarctica high-end computing and scientific visualization at NASA \"tech talks\" on Google Videowe're always looking.",
      "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html",
      "author": "Posted by Corinna Cortes, Head, Google Research NY"
    },
    {
      "title": "Making a difference",
      "date": "Friday, February 17, 2006",
      "abstract": "Making a differenceRead MoreThe Google File Systemcover story",
      "link": "http://ai.googleblog.com/2006/02/making-difference.html",
      "author": "Posted by Peter Norvig, Director, Google Research"
    }
  ]
}