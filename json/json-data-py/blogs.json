[
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub reduces Marketplace transaction fees, revamps Technology Partner Program\t\t\t", "author": ["\n\t\tRyan J. Salva\t"], "link": "https://github.blog/2021-02-04-github-reduces-marketplace-transaction-fees-revamps-technology-partner-program/", "abstract": "At GitHub, our community is at the heart of everything we do. We want to make it easier to build the things you love, with the tools you prefer to use\u2014which is why we\u2019re committed", "date": "\n\t\t\t\tFebruary 4, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDeployment reliability at GitHub\t\t\t", "author": ["\n\t\tRaffaele Di Fazio\t"], "link": "https://github.blog/2021-02-03-deployment-reliability-at-github/", "abstract": "Last week, we described how we improved the deployment experience for github.com. When we describe deployments at GitHub, the deployment experience is an important part of what it takes to ship applications to production, especially at GitHub\u2019s scale, but there is more to it: the actual deployment mechanics need to be fast and reliable.", "date": "\n\t\t\t\tFebruary 3, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Availability Report: January 2021\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2021-02-02-github-availability-report-january-2021/", "abstract": "Introduction In January, we experienced one incident resulting in significant impact and degraded state of availability for the GitHub Actions service. January 28 04:21 UTC (lasting 3 hours 53 minutes) Our service monitors detected abnormal", "date": "\n\t\t\t\tFebruary 2, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMaking GitHub\u2019s new homepage fast and performant\t\t\t", "author": ["\n\t\tTobias Ahlin\t"], "link": "https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/", "abstract": "This post is the third installment of our five-part series on building GitHub\u2019s new homepage: How our globe is built How we collect and use the data behind the globe How we made the page", "date": "\n\t\t\t\tJanuary 29, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tImproving how we deploy GitHub\t\t\t", "author": ["\n\t\tJulian Nadeau\t"], "link": "https://github.blog/2021-01-25-improving-how-we-deploy-github/", "abstract": "As GitHub doubled it\u2019s developer head count, tooling that worked for us no longer functioned in the same capacity. We aimed to improve the deployment process for all developers at GitHub and mitigate risk associated with deploying one of the largest developer platforms in the world.", "date": "\n\t\t\t\tJanuary 25, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe best of Changelog \u2022 2020 Edition\t\t\t", "author": ["\n\t\tMichelle Mannering\t"], "link": "https://github.blog/2021-01-21-changelog-2020-edition/", "abstract": "If you haven\u2019t seen it, the GitHub Changelog helps you keep up-to-date with all the latest features and updates to GitHub. We shipped a tonne of changes last year, and it\u2019s impossible to blog about", "date": "\n\t\t\t\tJanuary 21, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Availability Report: December 2020\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2021-01-06-github-availability-report-december-2020/", "abstract": "In December, we experienced no incidents resulting in service downtime. This month\u2019s GitHub Availability Report will provide a summary and follow-up details on how we addressed an incident mentioned in November\u2019s report.", "date": "\n\t\t\t\tJanuary 6, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBuilding On-Call Culture at GitHub\t\t\t", "author": ["\n\t\tMary Moore-Simmons\t"], "link": "https://github.blog/2021-01-06-building-on-call-culture-at-github/", "abstract": "GitHub\u2019s engineering group moved from a monolithic, hero-based on-call rotation to a more balanced on-call culture in order to increase our on-call expertise and improve the experience for our customers.", "date": "\n\t\t\t\tJanuary 6, 2021\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit clone: a data-driven study on cloning behaviors\t\t\t", "author": ["\n\t\tSolmaz Abbaspoursani\t"], "link": "https://github.blog/2020-12-22-git-clone-a-data-driven-study-on-cloning-behaviors/", "abstract": "@derrickstolee\u00a0recently\u00a0discussed several different\u00a0git clone\u00a0options, but how do those options actually affect your Git performance? Which option is fastest for your client experience? Which option is fastest for your build machines? How can these options impact", "date": "\n\t\t\t\tDecember 22, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGet up to speed with partial clone and shallow clone\t\t\t", "author": ["\n\t\tDerrick Stolee\t"], "link": "https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/", "abstract": "As your Git repositories grow, it becomes harder and harder for new developers to clone and start working on them. Git is designed as a\u00a0distributed\u00a0version control system. This means that you can work on your", "date": "\n\t\t\t\tDecember 21, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tVisualizing GitHub\u2019s global community\t\t\t", "author": ["\n\t\tTal Safran\t"], "link": "https://github.blog/2020-12-21-visualizing-githubs-global-community/", "abstract": "This is the second post in a series about how we built our new homepage. How our globe is built How we collect and use the data behind the globe How we made the page", "date": "\n\t\t\t\tDecember 21, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we built the GitHub globe\t\t\t", "author": ["\n\t\tTobias Ahlin\t"], "link": "https://github.blog/2020-12-21-how-we-built-the-github-globe/", "abstract": "GitHub is where the world builds software. More than 56 million developers around the world build and work together on GitHub. With our new homepage, we wanted to show how open source development transcends the", "date": "\n\t\t\t\tDecember 21, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCommits are snapshots, not diffs\t\t\t", "author": ["\n\t\tDerrick Stolee\t"], "link": "https://github.blog/2020-12-17-commits-are-snapshots-not-diffs/", "abstract": "Git\u00a0has a reputation\u00a0for\u00a0being confusing. Users stumble over terminology and phrasing that misguides their expectations. This is most apparent in commands that \u201crewrite history\u201d such as\u00a0git cherry-pick\u00a0or\u00a0git rebase. In my experience, the root cause of this", "date": "\n\t\t\t\tDecember 17, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tReducing flaky builds by 18x\t\t\t", "author": ["\n\t\tJordan Raine\t"], "link": "https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/", "abstract": "Part of the\u00a0Building GitHub blog series. It\u2019s four o\u2019clock in the afternoon as you push the last tweak to your branch. Your teammate already reviewed and approved your pull request and now all that\u2019s left", "date": "\n\t\t\t\tDecember 16, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEncapsulating Ruby on Rails views\t\t\t", "author": ["\n\t\tJoel Hawksley\t"], "link": "https://github.blog/2020-12-15-encapsulating-ruby-on-rails-views/", "abstract": "Learn more about how we are bringing encapsulation to our views as we scale to over 4,500 templates in our Ruby on Rails monolith.", "date": "\n\t\t\t\tDecember 15, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe evolving role of operations in DevOps\t\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-12-03-the-evolving-role-of-operations-in-devops/", "abstract": "GitHub\u2019s team delves into answering the question \u201cwhat are operations roles in the development and operations (DevOps) environments\u201d. From automating the role of QA in DevOps and more for smaller, faster delivery cycles.", "date": "\n\t\t\t\tDecember 3, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tImproving the GHES release process: release candidates\t\t\t", "author": ["\n\t\tMaya Ross\t"], "link": "https://github.blog/2020-12-03-improving-the-ghes-release-process-release-candidates/", "abstract": "In our ongoing \u201cBuilding GitHub\u201d series, we talk about some of the projects we\u2019re working on to improve how efficiently we build GitHub, as well as increase GitHub\u2019s availability, stability, and resilience. We know how", "date": "\n\t\t\t\tDecember 3, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Availability Report: November 2020\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-12-02-availability-report-november-2020/", "abstract": "Introduction In November, we experienced two incidents resulting in significant impact and degraded state of availability for issues, pull requests, and GitHub Actions services. November 2 12:00 UTC (lasting 32 minutes) The SSL certificate for", "date": "\n\t\t\t\tDecember 2, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNbdev: A literate programming environment that democratizes software engineering best practices\t\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2020-11-20-nbdev-a-literate-programming-environment-that-democratizes-software-engineering-best-practices/", "abstract": "Learn about nbdev, a new literate programming environment for Python.", "date": "\n\t\t\t\tNovember 20, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGetting started with DevOps automation\t\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-10-29-getting-started-with-devops-automation/", "abstract": "This is the second post in our series on DevOps fundamentals. For a guide to what DevOps is and answers to common DevOps myths check out part one. What role does automation play in DevOps?", "date": "\n\t\t\t\tOctober 29, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMaking GitHub CI workflow 3x faster\t\t\t", "author": ["\n\t\tKeerthana Kumar\t"], "link": "https://github.blog/2020-10-29-making-github-ci-workflow-3x-faster/", "abstract": "Using deferred compliance in GitHub\u2019s CI process to improve developer productivity.", "date": "\n\t\t\t\tOctober 29, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBuilding GitHub: introduction\t\t\t", "author": ["\n\t\tKate Studwell\t"], "link": "https://github.blog/2020-10-29-building-github-introduction/", "abstract": "An introduction to our blog series on GitHub\u2019s investments in technical excellence.", "date": "\n\t\t\t\tOctober 29, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHighlights from Git 2.29\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2020-10-19-git-2-29-released/", "abstract": "The open source Git project\u00a0just released Git 2.29\u00a0with features and bug fixes from over 89 contributors, 24 of them new. Last time\u00a0we caught up\u00a0with you, Git 2.28 had just been released. One version later, let\u2019s", "date": "\n\t\t\t\tOctober 19, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow to get your organization started with containerized deployments\t\t\t", "author": ["\n\t\tSarah Khalife\t"], "link": "https://github.blog/2020-10-15-how-to-get-your-organization-started-with-containerized-deployments/", "abstract": "This is our second post on cloud deployment with containers. Looking for more? Join our upcoming GitHub Actions webcast with Sarah, Solutions Engineer Pavan Ravipati, and Senior Product Manager Kayla Ngan on October 22. In", "date": "\n\t\t\t\tOctober 15, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTesting cloud apps with GitHub Actions and cloud-native open source tools\t\t\t", "author": ["\n\t\tSarah Khalife\t"], "link": "https://github.blog/2020-10-09-devops-cloud-testing/", "abstract": "See this post in action during GitHub Demo Days on October 16. What makes a project successful? For developers building cloud-native applications, successful projects thrive on transparent, consistent, and rigorous collaboration. That collaboration is one", "date": "\n\t\t\t\tOctober 9, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Availability Report: August 2020\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-09-02-github-availability-report-august-2020/", "abstract": "Introduction In August, we experienced no incidents resulting in service downtime. This month\u2019s GitHub Availability Report will dive into updates to the GitHub Status Page and provide follow-up details on how we\u2019ve addressed the incident", "date": "\n\t\t\t\tSeptember 2, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUpgrading GitHub to Ruby 2.7\t\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2020-08-25-upgrading-github-to-ruby-2-7/", "abstract": "GitHub recently upgraded to Ruby 2.7. Learn how the team approached the deprecation warnings, why upgrading is important, and the notable performance improvements.", "date": "\n\t\t\t\tAugust 25, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing the Rally + GitHub integration\t\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-08-18-introducing-the-rally-github-integration/", "abstract": "GitHub\u2019s Professional Services Engineering team has decided to open source another project:\u00a0Rally + GitHub. You may have seen our most recent open source project,\u00a0Super Linter. Well, the team has done it again, this time to", "date": "\n\t\t\t\tAugust 18, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWhy Write ADRs\t\t\t", "author": ["\n\t\tEli Perkins\t"], "link": "https://github.blog/2020-08-13-why-write-adrs/", "abstract": "How architecture decision records can help your team.", "date": "\n\t\t\t\tAugust 13, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Availability Report: July 2020\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-08-05-github-availability-report-july-2020/", "abstract": "Last month we introduced GitHub\u2019s monthly availability report to address service disruptions and share our learnings with the community.", "date": "\n\t\t\t\tAugust 5, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCodeGen: Semantic\u2019s improved language support system\t\t\t", "author": ["\n\t\tAyman Nadeem\t"], "link": "https://github.blog/2020-08-04-codegen-semantics-improved-language-support-system/", "abstract": "The Semantic Code team shipped a massive improvement to the language support system that powers code navigation. Code navigation features only scratch the surface of possibilities that start to open up when we combine Semantic\u2018s program analysis potential with GitHub\u2019s scale.", "date": "\n\t\t\t\tAugust 4, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHighlights from Git 2.28\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2020-07-27-highlights-from-git-2-28/", "abstract": "The open source Git project just released Git 2.28 with features and bug fixes from over 58 contributors, 13 of them new. We last caught up with you on the latest in Git back when", "date": "\n\t\t\t\tJuly 27, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing GitHub\u2019s OpenAPI Description\t\t\t", "author": ["\n\t\tMarc-Andre Giroux\t"], "link": "https://github.blog/2020-07-27-introducing-githubs-openapi-description/", "abstract": "The GitHub REST API has been through three major revisions since it was first released, only a month after the site was launched. We often receive feedback that our REST API is an inspiration to", "date": "\n\t\t\t\tJuly 27, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing the GitHub Availability Report\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-07-08-introducing-the-github-availability-report/", "abstract": "What is the Availability Report? Historically, GitHub has published post-incident reviews for major incidents that impact service availability. Whether we\u2019re sharing new investments to infrastructure or detailing site downtimes, our belief is that we can", "date": "\n\t\t\t\tJuly 8, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we launched docs.github.com\t\t\t", "author": ["\n\t\tSarah Schneider\t"], "link": "https://github.blog/2020-07-02-how-we-launched-docs-github-com/", "abstract": "ICYMI: docs.github.com is the new place to discover all of GitHub\u2019s product documentation! We recently completed a major overhaul of GitHub\u2019s documentation websites. When you visit docs.github.com today, you\u2019ll see content from the former help.github.com", "date": "\n\t\t\t\tJuly 2, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing GitHub Super Linter: one linter to rule them all\t\t\t", "author": ["\n\t\tLucas Gravley\t"], "link": "https://github.blog/2020-06-18-introducing-github-super-linter-one-linter-to-rule-them-all/", "abstract": "Setting up a new repository with all the right linters for the different types of code can be time consuming and tedious. So many tools and configurations to choose from and often more than one", "date": "\n\t\t\t\tJune 18, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUsing GitHub Actions for MLOps & Data Science\t\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2020-06-17-using-github-actions-for-mlops-data-science/", "abstract": "Background Machine Learning Operations (or MLOps) enables Data Scientists to work in a more collaborative fashion, by providing testing, lineage, versioning, and historical information in an automated way.\u00a0 Because the landscape of MLOps is nascent,", "date": "\n\t\t\t\tJune 17, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tApril service disruptions analysis\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-05-22-april-service-disruptions-analysis/", "abstract": "We had multiple service interruptions in April that may have impacted your projects and businesses. We know how important reliability is for our users and have detailed an analysis on the disruptions.", "date": "\n\t\t\t\tMay 22, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThree bugs in the Go MySQL Driver\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2020-05-20-three-bugs-in-the-go-mysql-driver/", "abstract": "Check out what we learned from shipping our busiest Go service in production\u2014we found 3 bugs in the Go MySQL driver.", "date": "\n\t\t\t\tMay 20, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSawfish phishing campaign targets GitHub users\t\t\t", "author": ["\n\t\tGitHub SIRT\t"], "link": "https://github.blog/2020-04-14-sawfish-phishing-campaign-targets-github-users/", "abstract": "A phishing campaign targeting our customers lures GitHub users into providing their credentials (including two-factor authentication codes). Learn more about the threat and what you can do to protect yourself.", "date": "\n\t\t\t\tApril 14, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFrom 48k lines of code to 10\u2014the story of GitHub\u2019s JavaScript SDK\t\t\t", "author": ["\n\t\tGregor Martynus\t"], "link": "https://github.blog/2020-04-09-from-48k-lines-of-code-to-10-the-story-of-githubs-javascript-sdk/", "abstract": "Learn about the legacy, architecture, and methods used to reduce 48k lines of code to 10 as we take a deep dive into GitHub\u2019s Javascript SDK.", "date": "\n\t\t\t\tApril 9, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFebruary service disruptions post-incident analysis\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-03-26-february-service-disruptions-post-incident-analysis/", "abstract": "In-depth analysis of February service disruptions that impacted GitHub services.", "date": "\n\t\t\t\tMarch 26, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSix years of the GitHub Security Bug Bounty program\t\t\t", "author": ["\n\t\tBrian Anglin\t"], "link": "https://github.blog/2020-03-25-six-years-of-the-github-security-bug-bounty-program/", "abstract": "Learn more about the Bug Bounty program, including a recap of 2019\u2019s bugs, our expanded scope, new features, and more.", "date": "\n\t\t\t\tMarch 25, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCERT partners with GitHub Security Lab for automated remediation\t\t\t", "author": ["\n\t\tNico Waisman\t"], "link": "https://github.blog/2020-03-18-cert-partners-with-github-security-lab-for-automated-remediation/", "abstract": "Learn more about how we found ways to scale our vulnerability hunting efforts and empower others to do the same. In this post, we\u2019ll take a deep-dive in the remediation of a security vulnerability with CERT.", "date": "\n\t\t\t\tMarch 18, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFebruary service disruptions\t\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-02-28-february-service-disruptions/", "abstract": "Recently, we\u2019ve had multiple service interruptions on GitHub.com. We know how important reliability of our service is for your projects and teams. We take this responsibility very seriously and apologize for these disruptions. These incidents", "date": "\n\t\t\t\tFebruary 28, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAutomating MySQL schema migrations with GitHub Actions and more\t\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2020-02-14-automating-mysql-schema-migrations-with-github-actions-and-more/", "abstract": "In this deep dive, we cover how our daily schema migrations amounted to a significant toil on the database infrastructure team, and how we searched for a solution to automate the manual parts of the process.", "date": "\n\t\t\t\tFebruary 14, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSupercharge your command line experience: GitHub CLI is now in beta\t\t\t", "author": ["\n\t\tBilly Griffin\t"], "link": "https://github.blog/2020-02-12-supercharge-your-command-line-experience-github-cli-is-now-in-beta/", "abstract": "We want your feedback about GitHub\u2019s new command line tool that makes it easier to work with GitHub and reduce friction for many of your common workflows.", "date": "\n\t\t\t\tFebruary 12, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we built the good first issues feature\t\t\t", "author": ["\n\t\tTiferet Gazit\t"], "link": "https://github.blog/2020-01-22-how-we-built-good-first-issues/", "abstract": "We\u2019ve recently launched good first issues recommendations to help new contributors find easy gateways into open source projects. Read about the machine learning engine behind these recommendations.", "date": "\n\t\t\t\tJanuary 22, 2020\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBehind the scenes: GitHub security alerts\t\t\t", "author": ["\n\t\tJustin Hutchings\t"], "link": "https://github.blog/2019-12-11-behind-the-scenes-github-vulnerability-alerts/", "abstract": "Learn more about what\u2019s behind the scenes with GitHub vulnerability alerts.", "date": "\n\t\t\t\tDecember 11, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDebugging network stalls on Kubernetes\t\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2019-11-21-debugging-network-stalls-on-kubernetes/", "abstract": "In this deep-dive, we identified and worked through sporadic latency issues with services running on Kubernetes in our environment.", "date": "\n\t\t\t\tNovember 21, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGetting started with Git and GitHub is easier than ever with GitHub Desktop 2.2\t\t\t", "author": ["\n\t\tAmanda Pinsker\t"], "link": "https://github.blog/2019-10-02-get-started-easier-with-github-desktop-2-2/", "abstract": "GitHub Desktop 2.2 now features an interactive tutorial to help introduce new users to Git and GitHub.", "date": "\n\t\t\t\tOctober 2, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNew workflow editor for GitHub Actions\t\t\t", "author": ["\n\t\tChris Patterson\t"], "link": "https://github.blog/2019-10-01-new-workflow-editor-for-github-actions/", "abstract": "Edit your GitHub Actions workflow files easier with features to help you minimize errors and more.", "date": "\n\t\t\t\tOctober 1, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing the CodeSearchNet challenge\t\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/", "abstract": "We\u2019re announcing the CodeSearchNet Challenge and releasing a large dataset for natural language processing and machine learning.", "date": "\n\t\t\t\tSeptember 26, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTrack your work easily with the latest changes to project boards\t\t\t", "author": ["\n\t\tLauren Brose\t"], "link": "https://github.blog/2019-09-25-project-board-improvements/", "abstract": "The projects section of the issue sidebar has been updated to better convey project information, and you can change an issue\u2019s project column directly from the sidebar.", "date": "\n\t\t\t\tSeptember 25, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAccelerating the GitHub Sponsors beta with Stripe Connect\t\t\t", "author": ["\n\t\tKatie Delfin\t"], "link": "https://github.blog/2019-09-10-accelerating-the-github-sponsors-beta/", "abstract": "GitHub Sponsors now features a new streamlined onboarding and payment experience with Stripe Connect.", "date": "\n\t\t\t\tSeptember 10, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRunning GitHub on Rails 6.0\t\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2019-09-09-running-github-on-rails-6-0/", "abstract": "On August 26, 2019, the GitHub application was deployed to production with 100% of traffic on the newest Rails version: 6.0. Read more about our process for upgrading, what we learned, and what\u2019s next.", "date": "\n\t\t\t\tSeptember 9, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tC# or Java? TypeScript or JavaScript? Machine learning based classification of programming languages\t\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2019-07-02-c-or-java-typescript-or-javascript-machine-learning-based-classification-of-programming-languages/", "abstract": "To make language detection more robust and maintainable in the long run, we developed a machine learning classifier named OctoLingua based on an Artificial Neural Network (ANN) architecture which can handle language predictions in tricky scenarios.", "date": "\n\t\t\t\tJuly 2, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAtom editor is now faster\t\t\t", "author": ["\n\t\tRafael Oleza\t"], "link": "https://github.blog/2019-06-12-atom-editor-is-now-faster/", "abstract": "The Atom editor has been updated to make common features notably faster.", "date": "\n\t\t\t\tJune 12, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDirect instruction marking in Ruby 2.6\t\t\t", "author": ["\n\t\tAaron Patterson\t"], "link": "https://github.blog/2019-06-04-direct-instruction-marking-in-ruby-2-6/", "abstract": "We recently upgraded GitHub to use the latest version of Ruby 2.6. Ruby 2.6 contains an optimization for reducing memory usage.", "date": "\n\t\t\t\tJune 4, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tVulcanizer: a library for operating Elasticsearch\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2019-03-05-vulcanizer-a-library-for-operating-elasticsearch/", "abstract": "Vulcanizer is a Go library for interacting with an Elasticsearch cluster. Its goal is to provide a high-level API to help with common tasks associated with operating an Elasticsearch cluster such as querying health status of the cluster, migrating data off of nodes, updating cluster settings, and more.", "date": "\n\t\t\t\tMarch 5, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHighlights from Git 2.21\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2019-02-24-highlights-from-git-2-21/", "abstract": "A look at some of the new features in recent Git releases.", "date": "\n\t\t\t\tFebruary 24, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFive years of the GitHub Bug Bounty program\t\t\t", "author": ["\n\t\tPhil Turnbull\t"], "link": "https://github.blog/2019-02-19-five-years-of-the-github-bug-bounty-program/", "abstract": "Read about some big changes for the coming year: full legal protection for researchers, more GitHub properties eligible for rewards, and increased reward amounts.", "date": "\n\t\t\t\tFebruary 19, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAn open source parser for GitHub Actions\t\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2019-02-07-an-open-source-parser-for-github-actions/", "abstract": "Update: This blog post is no longer relevant with the update to GitHub Actions in August 2019. See the GitHub Actions documentation for more information. Since the beta release of GitHub Actions last October, thousands", "date": "\n\t\t\t\tFebruary 7, 2019\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAtom understands your code better than ever before\t\t\t", "author": ["\n\t\tMax Brunsfeld\t"], "link": "https://github.blog/2018-10-31-atoms-new-parsing-system/", "abstract": "Atom 1.32 provides improved syntax highlighting and code folding by parsing your code while you type it.", "date": "\n\t\t\t\tOctober 31, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOctober 21 post-incident analysis\t\t\t", "author": ["\n\t\tJason Warner\t"], "link": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "abstract": "In-depth analysis of the incident that impacted GitHub services on October 21 and 22.", "date": "\n\t\t\t\tOctober 30, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBehind the scenes of GitHub Token Scanning\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-10-17-behind-the-scenes-of-github-token-scanning/", "abstract": "We\u2019ve extended GitHub Token Scanning to include tokens from cloud service providers and additional credentials.", "date": "\n\t\t\t\tOctober 17, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tApplying machine intelligence to GitHub security alerts\t\t\t", "author": ["\n\t\tBen Thompson\t"], "link": "https://github.blog/2018-10-09-applying-machine-intelligence-to-security-alerts/", "abstract": "Learn how we use machine learning to power and build on security alerts and make GitHub more secure.", "date": "\n\t\t\t\tOctober 9, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUpgrading GitHub from Rails 3.2 to 5.2\t\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2018-09-28-upgrading-github-from-rails-3-2-to-5-2/", "abstract": "On August 15th GitHub celebrated a major milestone: our main application is now running on the latest version of Rails: 5.2.1! \ud83c\udf89 In total the project took a year and a half to upgrade from", "date": "\n\t\t\t\tSeptember 28, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTowards Natural Language Semantic Code Search\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2018-09-18-towards-natural-language-semantic-code-search/", "abstract": "Our machine learning scientists have been researching ways to enable the semantic search of code.", "date": "\n\t\t\t\tSeptember 18, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRemoving jQuery from GitHub.com frontend\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2018-09-06-removing-jquery-from-github-frontend/", "abstract": "We have recently completed a milestone where we were able to drop jQuery as a dependency of the frontend code for GitHub.com. This marks the end of a gradual, years-long transition of increasingly decoupling from", "date": "\n\t\t\t\tSeptember 6, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGLB: GitHub\u2019s open source load balancer\t\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2018-08-08-glb-director-open-source-load-balancer/", "abstract": "At GitHub, we serve tens of thousands of requests every second out of our network edge, operating on GitHub\u2019s metal cloud. We\u2019ve previously introduced GLB, our scalable load balancing solution for bare metal datacenters, which", "date": "\n\t\t\t\tAugust 8, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMySQL High Availability at GitHub\t\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2018-06-20-mysql-high-availability-at-github/", "abstract": "GitHub uses MySQL as its main datastore for all things non-git, and its availability is critical to GitHub\u2019s operation. The site itself, GitHub\u2019s API, authentication and more, all require database access. We run multiple MySQL", "date": "\n\t\t\t\tJune 20, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tPerformance Impact of Removing OOBGC\t\t\t", "author": ["\n\t\tAaron Patterson\t"], "link": "https://github.blog/2018-05-18-removing-oobgc/", "abstract": "Until last week, GitHub used an Out of Band Garbage Collector (OOBGC) in production. Since removing it, we decreased CPU time across our production machines by 10%. Let\u2019s talk about what an OOBGC is, when", "date": "\n\t\t\t\tMay 18, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUsing Figma designs to build the Octicons icon library\t\t\t", "author": ["\n\t\tJon Rohan\t"], "link": "https://github.blog/2018-04-12-driving-changes-from-designs/", "abstract": "How we use Figma files to keep the Octicons icon library up to date", "date": "\n\t\t\t\tApril 12, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFour years of the GitHub Security Bug Bounty\t\t\t", "author": ["\n\t\tGreg Ose\t"], "link": "https://github.blog/2018-03-14-four-years-of-bug-bounty/", "abstract": "Last month GitHub celebrated the fourth year of our Security Bug Bounty program. As we\u2019ve done in the past, we\u2019re sharing some details and highlights from 2017 and looking ahead to where we see the", "date": "\n\t\t\t\tMarch 14, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tImproving your OSS dependency workflow with Licensed\t\t\t", "author": ["\n\t\tJon Ruskin\t"], "link": "https://github.blog/2018-03-07-improving-your-oss-dependency-workflow-with-licensed/", "abstract": "GitHub recently open sourced Licensed in the hopes that it is as helpful to the OSS community as it has been to us. <disclaimer> 1 of 1 consulted lawyers agree, Licensed is not a replacement", "date": "\n\t\t\t\tMarch 7, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMeasuring the many sizes of a Git repository\t\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2018-03-05-measuring-the-many-sizes-of-a-git-repository/", "abstract": "Is your Git repository bursting at the seams? git-sizer is a new open source tool that can tell you when your repo is getting too big. git-sizer computes various Git repository size metrics and alerts", "date": "\n\t\t\t\tMarch 5, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFebruary 28th DDoS Incident Report\t\t\t", "author": ["\n\t\tSam Kottler\t"], "link": "https://github.blog/2018-03-01-ddos-incident-report/", "abstract": "On Wednesday, February 28, 2018 GitHub.com was unavailable from 17:21 to 17:26 UTC and intermittently unavailable from 17:26 to 17:30 UTC due to a distributed denial-of-service (DDoS) attack. We understand how much you rely on", "date": "\n\t\t\t\tMarch 1, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWeak cryptographic standards removed\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-02-23-weak-cryptographic-standards-removed/", "abstract": "Earlier today we permanently removed support for the following weak cryptographic standards on github.com and api.github.com: TLSv1/TLSv1.1: This applies to all HTTPS connections, including web, API, and Git connections to https://github.com and https://api.github.com. diffie-hellman-group1-sha1: This", "date": "\n\t\t\t\tFebruary 23, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWeak cryptographic standards removal notice\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-02-01-crypto-removal-notice/", "abstract": "Last year we announced the deprecation of several weak cryptographic standards. Then we provided a status update toward the end of last year outlining some changes we\u2019d made to make the transition easier for clients.", "date": "\n\t\t\t\tFebruary 1, 2018\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDoubling Bug Bounty rewards\t\t\t", "author": ["\n\t\tBrent Johnson\t"], "link": "https://github.blog/2017-10-18-doubling-bug-bounty-rewards/", "abstract": "We\u2019re coming up on four years since the Bug Bounty program was first announced. A lot has changed in that time, and we constantly try to keep our reward structure inline with top security bug", "date": "\n\t\t\t\tOctober 18, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tStretching Spokes\t\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2017-10-13-stretching-spokes/", "abstract": "GitHub\u2019s Spokes system stores multiple distributed copies of Git repositories. This article discusses how we got Spokes replication to span widely separated datacenters. Background: Spokes GitHub developed a system called Spokes to store multiple replicas", "date": "\n\t\t\t\tOctober 13, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMitigating replication lag and reducing read load with freno\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-10-13-mitigating-replication-lag-and-reducing-read-load-with-freno/", "abstract": "At GitHub, we use MySQL as the main database technology backing our services. We run classic MySQL master-replica setups, where writes go to the master, and replicas replay master\u2019s changes asynchronously. To be able to serve our traffic we read data from the MySQL replicas.", "date": "\n\t\t\t\tOctober 13, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEvolution of GitHub\u2019s data centers\t\t\t", "author": ["\n\t\tSam Kottler\t"], "link": "https://github.blog/2017-10-12-evolution-of-our-data-centers/", "abstract": "Over the past 18 months we\u2019ve made a significant investment in GitHub\u2019s physical infrastructure. The goal of this work is to improve the redundancy and global availability of our system. In doing so we\u2019ve solidified", "date": "\n\t\t\t\tOctober 12, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTransit and Peering: How your requests reach GitHub\t\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-10-12-transit-and-peering-how-your-requests-reach-github/", "abstract": "GitHub is at a scale that provides exposure to interesting aspects of running a major site and are working to mature and level-up many parts of our infrastructure as we grow. One of the areas", "date": "\n\t\t\t\tOctober 12, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Debug\t\t\t", "author": ["\n\t\tAlice Goldfuss\t"], "link": "https://github.blog/2017-09-19-github-debug/", "abstract": "GitHub is proud to handle thousands of requests per second from our millions of users. The Internet, however, can be a fickle beast of cables and sparks, and sometimes those requests don\u2019t happen very fast", "date": "\n\t\t\t\tSeptember 19, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWeak cryptographic standards deprecation update\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-09-18-crypto-deprecation-notice-update-1/", "abstract": "Earlier this year, we announced the deprecation of several weak cryptographic standards. As noted during our initial announcement, the vast majority of HTTPS clients connect to GitHub using TLSv1.2 and won\u2019t be affected by our", "date": "\n\t\t\t\tSeptember 18, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit LFS 2.3.0 released\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-09-14-git-lfs-2-3-0-released/", "abstract": "Git LFS v2.3.0 is now available with performance improvements to git lfs migrate and git clone, new features, bug fixes, and more. Download Git LFS v2.3.0 git lfs migrate With our latest release, git lfs", "date": "\n\t\t\t\tSeptember 14, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe data science behind topic suggestions\t\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2017-09-14-the-data-science-behind-topic-suggestions/", "abstract": "Earlier this year, we launched topics, a new feature that lets you tag repositories with descriptive words or phrases. Topics help you create connections between similar GitHub projects and explore them by type, technology, and", "date": "\n\t\t\t\tSeptember 14, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tKeeping an eye on our network\t\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-09-05-keeping-an-eye-on-our-network/", "abstract": "Visibility is essential to effectively operating complex systems. As our network has grown, we\u2019ve had to improve the the way we collect data about it to keep up. Key to these improvements has been the", "date": "\n\t\t\t\tSeptember 5, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tKubernetes at GitHub\t\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2017-08-16-kubernetes-at-github/", "abstract": "Over the last year, GitHub has gradually evolved the infrastructure that runs the Ruby on Rails application responsible for github.com and api.github.com. We reached a big milestone recently: all web and API requests are served", "date": "\n\t\t\t\tAugust 16, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTopic Suggestions for Millions of Repositories\t\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2017-07-31-topics/", "abstract": "We recently launched Topics, a new feature that lets you tag your repositories with descriptive words or phrases, making it easy to discover projects and explore GitHub.com. Topic suggestions on public repositories, provides a quick", "date": "\n\t\t\t\tJuly 31, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing Soft U2F, a software U2F authenticator for macOS\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2017-07-24-introducing-soft-u2f-a-software-u2f-authenticator-for-macos/", "abstract": "In an effort to increase the adoption of FIDO U2F second factor authentication, we\u2019re releasing Soft U2F\u2014a software-based U2F authenticator for macOS. Soft U2F currently works with Google Chrome and Opera\u2019s built-in U2F implementations, as", "date": "\n\t\t\t\tJuly 24, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSoft U2F\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2017-07-20-soft-u2f/", "abstract": "In an effort to increase the adoption of FIDO U2F second factor authentication, we\u2019re releasing Soft U2F: a software-based U2F authenticator for macOS. We\u2019ve long been interested in promoting better user security through two-factor authentication", "date": "\n\t\t\t\tJuly 20, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMySQL infrastructure testing automation at GitHub\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-07-06-mysql-testing-automation-at-github/", "abstract": "Our MySQL infrastructure is a critical component to GitHub. MySQL serves GitHub.com, GitHub\u2019s API, authentication and more. Every git request touches MySQL in some way. We are tasked with keeping the data available, and maintaining", "date": "\n\t\t\t\tJuly 6, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit LFS 2.2.0 released\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-06-27-git-lfs-2-2-0-released/", "abstract": "Git LFS v2.2.0 is now available with the all-new git-lfs-migrate command, making it easier than ever to start using Git LFS in your repository. For example, if you\u2019ve tried to push a large file to", "date": "\n\t\t\t\tJune 27, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDNS Infrastructure at GitHub\t\t\t", "author": ["\n\t\tJoe Williams\t"], "link": "https://github.blog/2017-05-31-dns-infrastructure-at-github/", "abstract": "At GitHub we recently revamped how we do DNS from the ground up. This included both how we interact with external DNS providers and how we serve records internally to our hosts. To do this,", "date": "\n\t\t\t\tMay 31, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntegrating Git in Atom\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-05-16-integrating-git-in-atom/", "abstract": "Perform common Git operations without leaving the editor: stage changes, make commits, create and switch branches, resolve merge conflicts, and more.", "date": "\n\t\t\t\tMay 16, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow Four Native Developers Wrote An Electron App\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-05-16-how-four-native-developers-wrote-an-electron-app/", "abstract": "Today we released the new GitHub Desktop Beta, rewritten on Electron.Electron is a well-known on-ramp for web developers to build desktop apps using familiar web technologies: HTML, CSS, and JavaScript.", "date": "\n\t\t\t\tMay 16, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit 2.13 has been released\t\t\t", "author": ["\n\t\tJeff King\t"], "link": "https://github.blog/2017-05-10-git-2-13-has-been-released/", "abstract": "The open source Git project has just released Git 2.13.0, with features and bugfixes from over 65 contributors. Before we dig into the new features, we have a brief security announcement. For those running their", "date": "\n\t\t\t\tMay 10, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit LFS 2.1.0 released\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-04-28-git-lfs-2-1-0-released/", "abstract": "Today we\u2019re announcing the next major release of Git LFS: v2.1.0, including new features, performance improvements, and more. New features Status subcommand With Git LFS 2.1.0, get a more comprehensive look at which files are", "date": "\n\t\t\t\tApril 28, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEnabling DNS split authority with OctoDNS\t\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-04-27-enabling-split-authority-dns-with-octodns/", "abstract": "Building robust systems involves designing for failure. As Site Reliability Engineers at GitHub, we\u2019re always on the lookout for places where redundancy can help to mitigate problems, and today we\u2019ll be talking about steps we\u2019ve", "date": "\n\t\t\t\tApril 27, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOpen sourcing our Delegated Account Recovery implementation\t\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-04-14-open-sourcing-our-delegated-recovery-implementation/", "abstract": "In February, we shipped the \u201cRecover Accounts Elsewhere\u201d feature to help people regain access to their accounts if they lose access to their two-factor device or token. It is an implementation of the Delegated Account", "date": "\n\t\t\t\tApril 14, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSHA-1 collision detection on GitHub.com\t\t\t", "author": ["\n\t\tJeff King\t"], "link": "https://github.blog/2017-03-20-sha-1-collision-detection-on-github-com/", "abstract": "A few weeks ago, researchers announced SHAttered, the first collision of the SHA-1 hash function. Starting today, all SHA-1 computations on GitHub.com will detect and reject any Git content that shows evidence of being part", "date": "\n\t\t\t\tMarch 20, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBug Bounty third anniversary wrap-up\t\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-03-14-bug-bounty-third-anniversary-wrap-up/", "abstract": "In honor of our Bug Bounty Program\u2019s third birthday, we kicked off a promotional bounty period in January and February. In addition to bonus payouts, the scope of the bug bounty was expanded to include", "date": "\n\t\t\t\tMarch 14, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tA formal spec for GitHub Flavored Markdown\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-03-14-a-formal-spec-for-github-markdown/", "abstract": "We\u2019re releasing a formal specification of the syntax for GitHub Flavored Markdown, and its corresponding reference implementation.", "date": "\n\t\t\t\tMarch 14, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit LFS 2.0.0 released\t\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-03-02-git-lfs-2-0-0-released/", "abstract": "Today we\u2019re announcing the next major release of Git LFS: v2.0.0. The official release notes have the complete list of all the new features, performance improvements, and more. In the meantime, here\u2019s our look at", "date": "\n\t\t\t\tMarch 2, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDiscontinue support for weak cryptographic standards\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-02-27-crypto-deprecation-notice/", "abstract": "Cryptographic standards are ever evolving. It is the canonical game of security cat and mouse, with attacks rendering older standards ill-suited, and driving the community to develop newer and stronger standards to take their place.", "date": "\n\t\t\t\tFebruary 27, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tA glimpse into GitHub\u2019s Bug Bounty workflow\t\t\t", "author": ["\n\t\tGreg Ose\t"], "link": "https://github.blog/2017-02-22-githubs-bug-bounty-workflow/", "abstract": "Last month, we announced the third anniversary of our Bug Bounty Program. While there\u2019s still time to disclose your findings through the program, we wanted to pull back the curtain and give you a glimpse", "date": "\n\t\t\t\tFebruary 22, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAdding Community & Safety checks to new features\t\t\t", "author": ["\n\t\tDanielle Leong\t"], "link": "https://github.blog/2017-01-31-community-and-safety-feature-reviews/", "abstract": "With the continuous shipping nature at GitHub, it\u2019s easy for the most well-intentioned feature to accidentally become the vector of abuse and harassment. The Community & Safety engineering team focuses on building community management tools", "date": "\n\t\t\t\tJanuary 31, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNew and improved two-factor lockout recovery process\t\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-01-30-recover-accounts-elsewhere/", "abstract": "The Recover Accounts Elsewhere feature lets you associate your GitHub account with your Facebook account. This will help us recover your account for certain two-factor authentication lockout scenarios. For example, you may become locked out", "date": "\n\t\t\t\tJanuary 30, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub\u2019s post-CSP journey\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-01-19-githubs-post-csp-journey/", "abstract": "Last year we shared some details on GitHub\u2019s CSP journey. A journey was a good way to describe it, as our usage of Content Security Policy (CSP) significantly changed from our initial release nearly four", "date": "\n\t\t\t\tJanuary 19, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMoving persistent data out of Redis\t\t\t", "author": ["\n\t\tBryana Knight\t"], "link": "https://github.blog/2017-01-10-moving-persistent-data-out-of-redis/", "abstract": "Historically, we have used Redis in two ways at GitHub: We used it as an LRU cache to conveniently store the results of expensive computations over data originally persisted in Git repositories or MySQL. We", "date": "\n\t\t\t\tJanuary 10, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBug Bounty anniversary promotion: bigger bounties in January and February\t\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-01-09-bug-bounty-anniversary-promotion-bigger-bounties-in-january-and-february/", "abstract": "The GitHub Bug Bounty Program is turning three years old. To celebrate, we\u2019re offering bigger bounties for the most severe bugs found in January and February. The bigger the bug, the bigger the prize The", "date": "\n\t\t\t\tJanuary 9, 2017\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOrchestrator at GitHub\t\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-12-08-orchestrator-github/", "abstract": "GitHub uses MySQL to store its metadata: Issues, Pull Requests, comments, organizations, notifications and so forth. While git repository data does not need MySQL to exist and persist, GitHub\u2019s service does. Authentication, API, and the", "date": "\n\t\t\t\tDecember 8, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we made diff pages three times faster\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-12-06-how-we-made-diff-pages-3x-faster/", "abstract": "We serve a lot of diffs here at GitHub. Because it is computationally expensive to generate and display a diff, we\u2019ve traditionally had to apply some very conservative limits on what gets loaded. We knew", "date": "\n\t\t\t\tDecember 6, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGLB part 2: HAProxy zero-downtime, zero-delay reloads with multibinder\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-12-01-glb-part-2-haproxy-zero-downtime-zero-delay-reloads-with-multibinder/", "abstract": "As part of the design of GLB, we set out to solve a few of the common issues found when using HAProxy at scale.", "date": "\n\t\t\t\tDecember 1, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIncident Report: Inadvertent Private Repository Disclosure\t\t\t", "author": ["\n\t\tTodd Berman\t"], "link": "https://github.blog/2016-10-28-incident-report-inadvertent-private-repository-disclosure/", "abstract": "On Thursday, October 20th, a bug in GitHub\u2019s system exposed a small amount of user data via Git pulls and clones. In total, 156 private repositories of GitHub.com users were affected (including one of GitHub\u2019s).", "date": "\n\t\t\t\tOctober 28, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\toctocatalog-diff: GitHub\u2019s Puppet development and testing tool\t\t\t", "author": ["\n\t\tKevin Paulisse\t"], "link": "https://github.blog/2016-10-20-octocatalog-diff-github-s-puppet-development-and-testing-tool/", "abstract": "Today we are announcing the open source release of octocatalog-diff: GitHub\u2019s Puppet development and testing tool. GitHub uses Puppet to configure the infrastructure that powers GitHub.com, comprised of hundreds of roles deployed on thousands of", "date": "\n\t\t\t\tOctober 20, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing the GitHub Load Balancer\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-09-22-introducing-glb/", "abstract": "Over the last year we\u2019ve developed our new load balancer, called GLB (GitHub Load Balancer). Today, and over the next few weeks, we will be sharing the design and releasing its components as open source software.", "date": "\n\t\t\t\tSeptember 22, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe GitHub GraphQL API\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-09-14-the-github-graphql-api/", "abstract": "GitHub announced a public API one month after the site launched. We\u2019ve evolved this platform through three versions, adhering to RFC standards and embracing new design patterns to provide a clear and consistent interface.", "date": "\n\t\t\t\tSeptember 14, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBuilding resilience in Spokes\t\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2016-09-07-building-resilience-in-spokes/", "abstract": "Spokes is the replication system for the file servers where we store over 38 million Git repositories and over 36 million gists.It keeps at least three copies of every repository and every gist so that", "date": "\n\t\t\t\tSeptember 7, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tContext aware MySQL pools via HAProxy\t\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-08-17-context-aware-mysql-pools-via-haproxy/", "abstract": "At GitHub we use MySQL as our main datastore. While repository data lies in git, metadata is stored in MySQL. This includes Issues, Pull Requests, Comments etc. We also auth against MySQL via a custom", "date": "\n\t\t\t\tAugust 17, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tgh-ost: GitHub\u2019s online schema migration tool for MySQL\t\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-08-01-gh-ost-github-s-online-migration-tool-for-mysql/", "abstract": "Today we are announcing the open source release of gh-ost: GitHub\u2019s triggerless online schema migration tool for MySQL. gh-ost has been developed at GitHub in recent months to answer a problem we faced with ongoing,", "date": "\n\t\t\t\tAugust 1, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSYN Flood Mitigation with synsanity\t\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2016-07-12-syn-flood-mitigation-with-synsanity/", "abstract": "GitHub hosts a wide range of user content, and like all large websites this often causes us to become a target of denial of service attacks. Around a year ago, GitHub was on the receiving", "date": "\n\t\t\t\tJuly 12, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub\u2019s CSP journey\t\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2016-04-12-githubs-csp-journey/", "abstract": "We shipped subresource integrity a few months back to reduce the risk of a compromised CDN serving malicious JavaScript. That is a big win, but does not address related content injection issues that may exist", "date": "\n\t\t\t\tApril 12, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing DGit\t\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2016-04-05-introducing-dgit/", "abstract": "Edit: DGit is now called Spokes GitHub hosts over 35 million repositories and over 30 million Gists on hundreds of servers. Over the past year, we\u2019ve built DGit, a new distributed storage system that dramatically", "date": "\n\t\t\t\tApril 5, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRevamping GitHub\u2019s Subversion Bridge\t\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-03-08-revamping-githubs-subversion-bridge/", "abstract": "One of GitHub\u2019s niche features is the ability to access a Git repository on GitHub using Subversion clients. Last year we re-architected a large portion of the Subversion bridge to work with our changing infrastructure.", "date": "\n\t\t\t\tMarch 8, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDelivering Octicons with SVG\t\t\t", "author": ["\n\t\tAaron Shekey\t"], "link": "https://github.blog/2016-02-22-delivering-octicons-with-svg/", "abstract": "GitHub.com no longer delivers its icons via icon font. Instead, we\u2019ve replaced all the Octicons throughout our codebase with SVG alternatives. While the changes are mostly under-the-hood, you\u2019ll immediately feel the benefits of the SVG", "date": "\n\t\t\t\tFebruary 22, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTwo years of bounties\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2016-02-04-two-years-of-bounties/", "abstract": "Despite the best efforts of its writers, software has vulnerabilities, and GitHub is no exception. Finding, fixing, and learning from past bugs is a critical part of keeping our users and their data safe on", "date": "\n\t\t\t\tFebruary 4, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tJanuary 28th Incident Report\t\t\t", "author": ["\n\t\tScott Sanders\t"], "link": "https://github.blog/2016-02-03-january-28th-incident-report/", "abstract": "Last week GitHub was unavailable for two hours and six minutes. We understand how much you rely on GitHub and consider the availability of our service one of the core features we offer. Over the", "date": "\n\t\t\t\tFebruary 3, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScientist: Measure Twice, Cut Once\t\t\t", "author": ["\n\t\tJesse Toth\t"], "link": "https://github.blog/2016-02-03-scientist/", "abstract": "Today we\u2019re releasing Scientist 1.0 to help you rewrite critical code with confidence. As codebases mature and requirements change, it is inevitable that you will need to replace or rewrite a part of your system.", "date": "\n\t\t\t\tFebruary 3, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUpdate on 1/28 service outage\t\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2016-01-29-update-on-1-28-service-outage/", "abstract": "On Thursday, January 28, 2016 at 00:23am UTC, we experienced a severe service outage that impacted GitHub.com. We know that any disruption in our service can impact your development workflow, and are truly sorry for", "date": "\n\t\t\t\tJanuary 29, 2016\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMove Fast and Fix Things\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2015-12-15-move-fast/", "abstract": "Anyone who has worked on a large enough codebase knows that technical debt is an inescapable reality: The more rapidly an application grows in size and complexity, the more technical debt is accrued. With GitHub\u2019s", "date": "\n\t\t\t\tDecember 15, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub\u2019s Metal Cloud\t\t\t", "author": ["\n\t\tLee Reilly\t"], "link": "https://github.blog/2015-12-01-githubs-metal-cloud/", "abstract": "At GitHub we place an emphasis on stability, availability, and performance. A large component of ensuring we excel in these areas is deploying services on bare-metal hardware. This allows us to tailor hardware configurations to", "date": "\n\t\t\t\tDecember 1, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tLIKE injection\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-11-03-like-injection/", "abstract": "Looking through our exception tracker the other day, I ran across a notice from our slow-query logger that caught my eye. I saw a SELECT \u2026 WHERE \u2026 LIKE query with lots of percent signs", "date": "\n\t\t\t\tNovember 3, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit Concurrency in GitHub Desktop\t\t\t", "author": ["\n\t\tAmy Palamountain\t"], "link": "https://github.blog/2015-10-20-git-concurrency-in-github-desktop/", "abstract": "Careful use of concurrency is particularly important when writing responsive desktop applications. Typically, complex operations are executed on background threads. This results in an app that remains responsive to user input, while still performing complex", "date": "\n\t\t\t\tOctober 20, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRunnable Documentation: Code for Humans\t\t\t", "author": ["\n\t\tMike McQuaid\t"], "link": "https://github.blog/2015-10-06-runnable-documentation/", "abstract": "On GitHub Enterprise we\u2019ve moved our release process to using what we like to call \u201cRunnable Documentation\u201d: a step-by-step series of instructions that can be run by any person without requiring special domain knowledge. When", "date": "\n\t\t\t\tOctober 6, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCounting Objects\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2015-09-22-counting-objects/", "abstract": "The Systems Team at GitHub works to solve complex bugs and performance bottlenecks at the lowest levels of our infrastructure. Over the past two years we\u2019ve undertaken a major project to improve the performance of", "date": "\n\t\t\t\tSeptember 22, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSubresource Integrity\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-09-19-subresource-integrity/", "abstract": "Like many sites, GitHub uses a content delivery network (CDN) to serve static assets such as JavaScript, CSS, and images to our users. The CDN makes web browsing faster by delivering assets from data centers", "date": "\n\t\t\t\tSeptember 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub implements Subresource Integrity\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-09-18-github-implements-subresource-integrity/", "abstract": "With Subresource Integrity (SRI), using GitHub is safer than ever. SRI tells your browser to double check that our Content Delivery Network (CDN) is sending the right JavaScript and CSS to your browser. Without SRI,", "date": "\n\t\t\t\tSeptember 18, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCross-platform UI in GitHub Desktop\t\t\t", "author": ["\n\t\tRob Rix\t"], "link": "https://github.blog/2015-08-19-cross-platform-ui-in-github-desktop/", "abstract": "The comparison graph is the centerpiece of GitHub Desktop. It both drives the interaction with your branch and shows you the effects of your changes relative to a base branch. It\u2019s easily the most sophisticated", "date": "\n\t\t\t\tAugust 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBenchmarking GitHub Enterprise\t\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2015-07-22-benchmarking-github-enterprise/", "abstract": "The release of GitHub Enterprise 2.0 brought more than just new features and support for deployment on Amazon Web Services. It also included a rework of our virtual machine architecture to improve performance and reliability.", "date": "\n\t\t\t\tJuly 22, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScripts to Rule Them All\t\t\t", "author": ["\n\t\tJon Maddox\t"], "link": "https://github.blog/2015-06-30-scripts-to-rule-them-all/", "abstract": "At GitHub we have a lot of software running our product and company. We also have a lot of potential contributing members. Being able to get from git clone to an up-and-running project in a", "date": "\n\t\t\t\tJune 30, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRead-only deploy keys\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-06-16-read-only-deploy-keys/", "abstract": "You can now create deploy keys with read-only access. A deploy key is an SSH key that is stored on your server and grants access to a single GitHub repository. They are often used to", "date": "\n\t\t\t\tJune 16, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tException Monitoring and Response\t\t\t", "author": ["\n\t\tJustin Palmer\t"], "link": "https://github.blog/2015-06-16-exception-monitoring-and-response/", "abstract": "Like most software applications, GitHub can generate a few exceptions. Incoming exceptions range from system-level issues including Git timeouts and missing references, to application-level issues including simple code mistakes and JavaScript errors. We take stability", "date": "\n\t\t\t\tJune 16, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBrubeck, a statsd-compatible metrics aggregator\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2015-06-15-brubeck/", "abstract": "One of the key points of GitHub\u2019s engineering culture \u2014and I believe, of any good engineering culture\u2014 is our obsession with aggressively measuring everything. Coda Hale\u2019s seminal talk \u201cMetrics, Metrics Everywhere\u201d has been a cornerstone", "date": "\n\t\t\t\tJune 15, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDeploying branches to GitHub.com\t\t\t", "author": ["\n\t\tAman Gupta\t"], "link": "https://github.blog/2015-06-02-deploying-branches-to-github-com/", "abstract": "At GitHub, we use a variant of the Flow pattern to deploy changes: new code is always deployed from a pull request branch, and merged only once it has been confirmed in production. master is", "date": "\n\t\t\t\tJune 2, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRearchitecting GitHub Pages\t\t\t", "author": ["\n\t\tCharlie Somerville\t"], "link": "https://github.blog/2015-05-27-rearchitecting-github-pages/", "abstract": "GitHub Pages, our static site hosting service, has always had a very simple architecture. From launch up until around the beginning of 2015, the entire service ran on a single pair of machines (in active/standby", "date": "\n\t\t\t\tMay 27, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tBrowser Monitoring for GitHub.com\t\t\t", "author": ["\n\t\tEmily Nakashima\t"], "link": "https://github.blog/2015-05-19-browser-monitoring-for-github-com/", "abstract": "Most large-scale web applications incorporate at least some browser monitoring, collecting metrics about the user experience with JavaScript in the browser, but, as a community, we don\u2019t talk much about what\u2019s working here and what\u2019s", "date": "\n\t\t\t\tMay 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWorkload Analysis with MySQL\u2019s Performance Schema\t\t\t", "author": ["\n\t\tNatalie Siskin\t"], "link": "https://github.blog/2015-05-19-using-mysql-performance-schema-for-workload-analysis/", "abstract": "Earlier this spring, we upgraded our database cluster to MySQL 5.6. Along with many other improvements, 5.6 added some exciting new features to the performance schema. MySQL\u2019s performance schema is a set of tables that", "date": "\n\t\t\t\tMay 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHello World\t\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2015-05-19-hello-world/", "abstract": "What does it take to run and build GitHub? On our new Engineering Blog we\u2019ll show you how it\u2019s done. Through the writings of our engineers you\u2019ll gain insight into the practices we use, the", "date": "\n\t\t\t\tMay 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe GitHub Engineering Blog\t\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2015-05-19-the-github-engineering-blog/", "abstract": "We are happy to introduce GitHub\u2019s Engineering Blog to the world. Starting today, you can read details about our infrastructure, learn about our development practices, and hear about the knowledge we\u2019ve gained while running the", "date": "\n\t\t\t\tMay 19, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEight lessons learned hacking on GitHub Pages for six months\t\t\t", "author": ["\n\t\tBen Balter\t"], "link": "https://github.blog/2015-04-27-eight-lessons-learned-hacking-on-github-pages-for-six-months/", "abstract": "Believe it or not, just over a year ago, GitHub Pages, the documentation hosting service that powers nearly three-quarters of a million sites, was little more than a 100-line shell script. Today, it\u2019s a fully", "date": "\n\t\t\t\tApril 27, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tLarge Scale DDoS Attack on github.com\t\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2015-03-27-large-scale-ddos-attack-on-github-com/", "abstract": "We are currently experiencing the largest DDoS (distributed denial of service) attack in github.com\u2019s history. The attack began around 2AM UTC on Thursday, March 26, and involves a wide combination of attack vectors. These include", "date": "\n\t\t\t\tMarch 27, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe Game Off Returns!\t\t\t", "author": ["\n\t\tLee Reilly\t"], "link": "https://github.blog/2015-03-02-the-game-off-returns/", "abstract": "The GitHub Game Off, our very own game jam is returning next week! We\u2019ve had some great games submitted in previous years and can\u2019t wait to see what you come up with this year. We\u2019ll", "date": "\n\t\t\t\tMarch 2, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit 2.3 has been released\t\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2015-02-06-git-2-3-has-been-released/", "abstract": "The Git developers have just released a major new version of the Git command-line utility, Git 2.3.0. As usual, this release contains many improvements, performance enhancements, and bug fixes. Full details about what\u2019s included can", "date": "\n\t\t\t\tFebruary 6, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tKeeping GitHub OAuth Tokens Safe\t\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2015-02-05-keeping-github-oauth-tokens-safe/", "abstract": "While making your source code available in a public GitHub repository is awesome, it\u2019s important to be sure you don\u2019t accidentally commit your passwords, secrets, or anything else that other people shouldn\u2019t know. Starting today", "date": "\n\t\t\t\tFebruary 5, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Security Bug Bounty program turns one\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-01-28-github-security-bug-bounty-program-turns-one/", "abstract": "It\u2019s already been a year since we launched the GitHub Security Bug Bounty, and, thanks to bug reports from researchers across the globe, 73 previously unknown security vulnerabilities in our applications have been identified and", "date": "\n\t\t\t\tJanuary 28, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow to write the perfect pull request\t\t\t", "author": ["\n\t\tKeavy McMinn\t"], "link": "https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/", "abstract": "As a company grows, people and projects change. To continue to nurture the culture we want at GitHub, we\u2019ve found it useful to remind ourselves what we aim for when we communicate. We recently introduced", "date": "\n\t\t\t\tJanuary 21, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow GitHub uses GitHub to document GitHub\t\t\t", "author": ["\n\t\tGaren Torikian\t"], "link": "https://github.blog/2015-01-06-how-github-uses-github-to-document-github/", "abstract": "Providing well-written documentation helps people understand, make use of, and contribute back to your project, but it\u2019s only half of the documentation equation. The underlying system used to serve documentation can make life easier for", "date": "\n\t\t\t\tJanuary 6, 2015\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tImproving GitHub\u2019s SSL setup\t\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2014-12-23-improving-github-s-ssl-setup/", "abstract": "To keep GitHub as secure as possible for every user, we will remove RC4 support in our SSL configuration on github.com and in the GitHub API on January 5th 2015. RC4 has a number of", "date": "\n\t\t\t\tDecember 23, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tVulnerability announced: update your Git clients\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2014-12-18-vulnerability-announced-update-your-git-clients/", "abstract": "A critical Git security vulnerability has been announced today, affecting all versions of the official Git client and all related software that interacts with Git repositories, including GitHub for Windows and GitHub for Mac. Because", "date": "\n\t\t\t\tDecember 18, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Pages Legacy IP Deprecation\t\t\t", "author": ["\n\t\tBen Balter\t"], "link": "https://github.blog/2014-11-05-github-pages-legacy-ip-deprecation/", "abstract": "Update: We\u2019ve extended the deprecation deadline to February 2, 2015 to give Pages users more time to update their DNS records. If you use a custom domain with GitHub Pages, please verify that your domain\u2019s", "date": "\n\t\t\t\tNovember 5, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSecurity vulnerability in bash addressed\t\t\t", "author": ["\n\t\tShawn Davenport\t"], "link": "https://github.blog/2014-09-25-security-vulnerability-in-bash-addressed/", "abstract": "Update: 2014-09-29 23:10 UTC We have published an update to the Git Shell tools for GitHub for Windows, which resolves the bash vulnerabilities CVE-2014-6271, CVE-2014-7169, CVE-2014-7186 and CVE-2014-7187. If you are running GitHub for Windows,", "date": "\n\t\t\t\tSeptember 25, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMaking MySQL Better at GitHub\t\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2014-09-02-making-mysql-better-at-github/", "abstract": "At GitHub we say, \u201cit\u2019s not fully shipped until it\u2019s fast.\u201d We\u2019ve talked before about some of the ways we keep our frontend experience speedy, but that\u2019s only part of the story. Our MySQL database", "date": "\n\t\t\t\tSeptember 2, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSecurity: Heartbleed vulnerability\t\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2014-04-08-security-heartbleed-vulnerability/", "abstract": "On April 7, 2014 information was released about a new vulnerability (CVE-2014-0160) in OpenSSL, the cryptography library that powers the vast majority of private communication across the Internet. This library is key for maintaining privacy", "date": "\n\t\t\t\tApril 8, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDenial of Service Attacks\t\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2014-03-14-denial-of-service-attacks/", "abstract": "On Tuesday, March 11th, GitHub was largely unreachable for roughly 2 hours as the result of an evolving distributed denial of service (DDoS) attack. I know that you rely on GitHub to be available all", "date": "\n\t\t\t\tMarch 14, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tProxying User Images\t\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2014-01-28-proxying-user-images/", "abstract": "A while back, we started proxying all non-https images to avoid mixed-content warnings using a custom node server called camo. We\u2019re making a small change today and proxying HTTPS images as well. Proxying these images", "date": "\n\t\t\t\tJanuary 28, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDNS Outage Post Mortem\t\t\t", "author": ["\n\t\tJames Fryman\t"], "link": "https://github.blog/2014-01-18-dns-outage-post-mortem/", "abstract": "Last week on Wednesday, January 8th, GitHub experienced an outage of our DNS infrastructure. As a result of this outage, our customers experienced 42 minutes of downtime of services along with an additional 1 hour", "date": "\n\t\t\t\tJanuary 18, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOptimizing large selector sets\t\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2014-01-16-optimizing-large-selector-sets/", "abstract": "CSS selectors are to frontend development as SQL statements are to the backend. Aside from their origin in CSS, we use them all over our JavaScript. Importantly, selectors are declarative, which makes them prime candidates", "date": "\n\t\t\t\tJanuary 16, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tVideo from Passion Projects Talk #7 with Jen Myers\t\t\t", "author": ["\n\t\tManisha Sharma\t"], "link": "https://github.blog/2014-01-15-video-from-passion-projects-talk-7-with-jen-myers/", "abstract": "Jen Myers joined us in December of 2013 for the 7th installment of our Passion Projects talk series. Jen taught us the importance of not being an expert and how to be responsible for our", "date": "\n\t\t\t\tJanuary 15, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tImproving our SSL setup\t\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2014-01-07-improving-our-ssl-setup/", "abstract": "As we announced previously we\u2019ve improved our SSL setup by deploying forward secrecy and improving the list of supported ciphers. Deploying forward secrecy and up to date cipher lists comes with a number of considerations", "date": "\n\t\t\t\tJanuary 7, 2014\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing Forward Secrecy and Authenticated Encryption Ciphers\t\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2013-12-31-introducing-forward-secrecy-and-authenticated-encryption-ciphers/", "abstract": "As of yesterday we\u2019ve updated our SSL setup on the systems that serve traffic for GitHub. The changes introduce support for Forward Secrecy and Authenticated Encryption Ciphers. So what is Forward Secrecy? The EFF provides", "date": "\n\t\t\t\tDecember 31, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe Ghost of Issues Past\t\t\t", "author": ["\n\t\tTim Pease\t"], "link": "https://github.blog/2013-12-18-the-ghost-of-issues-past/", "abstract": "The end of the year is fast approaching, and this is a good time to review open issues from long ago. A great way to find older issues and pull requests is our wonderful search", "date": "\n\t\t\t\tDecember 18, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tJoin our Octostudy!\t\t\t", "author": ["\n\t\tChrissie Brodigan\t"], "link": "https://github.blog/2013-11-20-join-our-octostudy/", "abstract": "There are a lot of interesting people on GitHub today. Since we can\u2019t meet everyone at a conference, drinkup, or charity dodgeball game, we are hoping you can tell us a little more about yourself.", "date": "\n\t\t\t\tNovember 20, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tWeak passwords brute forced\t\t\t", "author": ["\n\t\tShawn Davenport\t"], "link": "https://github.blog/2013-11-20-weak-passwords-brute-forced/", "abstract": "Some GitHub user accounts with weak passwords were recently compromised due to a brute force password-guessing attack. I want to take this opportunity to talk about our response to this specific incident and account security", "date": "\n\t\t\t\tNovember 20, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAn African hack trip\t\t\t", "author": ["\n\t\tLuke Hefson\t"], "link": "https://github.blog/2013-11-13-an-african-hack-trip/", "abstract": "GitHub has a long tradition of supporting developer communities throughout the world. We throw drinkups, speak at and sponsor conferences, and host training events in most corners of the globe. However, with the exception of", "date": "\n\t\t\t\tNovember 13, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDisabling old IP addresses\t\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2013-10-30-disabling-old-ip-addresses/", "abstract": "We\u2019ve made some significant upgrades to the network infrastructure powering GitHub, and it\u2019s time to turn off some of the old gear. We\u2019ve updated DNS records to point at our new IP space, but continue", "date": "\n\t\t\t\tOctober 30, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tModeling your App\u2019s User Session\t\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2013-10-18-modeling-your-app-s-user-session/", "abstract": "If you\u2019ve been keeping an eye on your cookies, you may have noticed some recent changes GitHub has made to how we track your session. You shouldn\u2019t notice any difference in session behavior (beyond the", "date": "\n\t\t\t\tOctober 18, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit Internals PDF Open Sourced\t\t\t", "author": ["\n\t\tScott Chacon\t"], "link": "https://github.blog/2013-09-20-git-internals-pdf-open-sourced/", "abstract": "Over 5 years ago, shortly after GitHub initially launched, Chris pointed out on one of our earliest blog posts this Peepcode PDF on Git internals that I had just written: Well, today Pluralsight has agreed", "date": "\n\t\t\t\tSeptember 20, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSite Maintenance August 31st 2013\t\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2013-08-26-site-maintenance-august-31st-2013/", "abstract": "This Saturday, August 31st, 2013 at 5AM PDT we will be upgrading a large portion of our database infrastructure in order to provide a faster and more reliable GitHub experience. We estimate that the upgrades", "date": "\n\t\t\t\tAugust 26, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIP Address Changes\t\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2013-08-25-ip-address-changes/", "abstract": "As we continue to expand the infrastructure that powers GitHub, we want to make everyone aware of some changes to the IP addresses that we use. Most customers won\u2019t have to do anything as a", "date": "\n\t\t\t\tAugust 25, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub Flow in the Browser\t\t\t", "author": ["\n\t\tCoby Chapple\t"], "link": "https://github.blog/2013-07-11-github-flow-in-the-browser/", "abstract": "Now that you can delete files directly on GitHub we\u2019ve reached a very exciting milestone\u2014the entire GitHub Flow\u2122 is now possible using nothing but a web browser. What is GitHub Flow? A little while ago", "date": "\n\t\t\t\tJuly 11, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit Merge Berlin 2013\t\t\t", "author": ["\n\t\tScott Chacon\t"], "link": "https://github.blog/2013-06-10-git-merge-berlin-2013/", "abstract": "Last month GitHub was proud to host the first Git Merge conference, a place for Git core developers and Git users to meet, talk about Git and share what they\u2019ve been working on or interested", "date": "\n\t\t\t\tJune 10, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHey Judy, don\u2019t make it bad\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2013-05-01-hey-judy-don-t-make-it-bad/", "abstract": "Last week we explained how we greatly reduced the rendering time of our web views by switching our escaping routines from Ruby to C. This speed-up was two-fold: the C code for escaping HTML was", "date": "\n\t\t\t\tMay 1, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHeads up: nosniff header support coming to Chrome and Firefox\t\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2013-04-24-heads-up-nosniff-header-support-coming-to-chrome-and-firefox/", "abstract": "Both GitHub and Gist offer ways to view \u201craw\u201d versions of user content. Instead of viewing files in the visual context of the website, the user can see the actual text content as it was", "date": "\n\t\t\t\tApril 24, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tContent Security Policy\t\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2013-04-19-content-security-policy/", "abstract": "We\u2019ve started rolling out a new security feature called \u201cContent Security Policy\u201d or CSP. As a user, it will better protect your account against XSS attacks. But, be aware, it may cause issues with some", "date": "\n\t\t\t\tApril 19, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEscape Velocity\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2013-04-17-escape-velocity/", "abstract": "We work very hard to keep GitHub fast. Ruby is not the fastest programming language, so we go to great lengths benchmarking and optimizing our large codebase: our goal is to keep bringing down response", "date": "\n\t\t\t\tApril 17, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tYummy cookies across domains\t\t\t", "author": ["\n\t\tVicent Mart\u00ed\t"], "link": "https://github.blog/2013-04-09-yummy-cookies-across-domains/", "abstract": "Last Friday we announced and performed a migration of all GitHub Pages to their own github.io domain. This was a long-planned migration, with the specific goal of mitigating phishing attacks and cross-domain cookie vulnerabilities arising", "date": "\n\t\t\t\tApril 9, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNew GitHub Pages domain: github.io\t\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2013-04-05-new-github-pages-domain-github-io/", "abstract": "Beginning today, all GitHub Pages sites are moving to a new, dedicated domain: github.io. This is a security measure aimed at removing potential vectors for cross domain attacks targeting the main github.com session as well", "date": "\n\t\t\t\tApril 5, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUpcoming IP address changes\t\t\t", "author": ["\n\t\tScott J. Goldman\t"], "link": "https://github.blog/2013-04-02-upcoming-ip-address-changes/", "abstract": "As we expand our infrastructure, we are making changes to the IP addresses currently in use. If you are explicitly whitelisting GitHub in your firewall rules, please make sure that you include all of the", "date": "\n\t\t\t\tApril 2, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub for Windows Recent Improvements\t\t\t", "author": ["\n\t\tPhil Haack\t"], "link": "https://github.blog/2013-02-21-github-for-windows-recent-improvements/", "abstract": "It\u2019s been almost a year since we first released GitHub for Windows. Today we just shipped version 1.0.38. That\u2019s 38 updates since 1.0! As we\u2019ve said before, we ship early and often. Since we\u2019ve been", "date": "\n\t\t\t\tFebruary 21, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTCMalloc and MySQL\t\t\t", "author": ["\n\t\tTed Nyman\t"], "link": "https://github.blog/2013-02-21-tcmalloc-and-mysql/", "abstract": "Over the last month or so, we noticed steady week-over-week rises in overall MySQL query time. A poorly-performing database makes for a slow site and a slow site makes for a terrible experience, so we", "date": "\n\t\t\t\tFebruary 21, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tIntroducing Boxen\t\t\t", "author": ["\n\t\tWill Farrington\t"], "link": "https://github.blog/2013-02-15-introducing-boxen/", "abstract": "Today we\u2019re proud to open source Boxen, our tool for automating and managing Macs at GitHub. Boxen started nearly a year ago as a project called \u201cThe Setup\u201d \u2014 a pipe dream to let anyone", "date": "\n\t\t\t\tFebruary 15, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRecent Code Search Outages\t\t\t", "author": ["\n\t\tWill Farrington\t"], "link": "https://github.blog/2013-02-04-recent-code-search-outages/", "abstract": "Last week, between Thursday, January 24 and Friday, January 25 we experienced a critical outage surrounding our newly-launched Code Search service. As always, we strive to provide detailed, transparent post-mortems about these incidents. We\u2019ll do", "date": "\n\t\t\t\tFebruary 4, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSecrets in the code\t\t\t", "author": ["\n\t\tBrian Doll\t"], "link": "https://github.blog/2013-01-25-secrets-in-the-code/", "abstract": "Programming often involves keeping a bunch of secrets around. You\u2019ve got account passwords, OAuth tokens, SSL and SSH private keys. The best way to keep a secret is, well, to keep it secret. Sometimes in", "date": "\n\t\t\t\tJanuary 25, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tReleasing Make Me\t\t\t", "author": ["\n\t\tMike Skalnik\t"], "link": "https://github.blog/2013-01-03-releasing-make-me/", "abstract": "A few months ago, GitHub HQ 2.0 got a MakerBot Replicator 2. GitHubbers started printing almost immediately due to the easy setup but having to leave a laptop connected was painful. We quickly learned how", "date": "\n\t\t\t\tJanuary 3, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tA More Transparent Clipboard Button\t\t\t", "author": ["\n\t\tJon Rohan\t"], "link": "https://github.blog/2013-01-02-a-more-transparent-clipboard-button/", "abstract": "Copying long lines of text and shas to your clipboard has been just a click away for a few years now. Today we\u2019re putting a new face on that click-to-copy feature, making it easier to", "date": "\n\t\t\t\tJanuary 2, 2013\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDowntime last Saturday\t\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-26-downtime-last-saturday/", "abstract": "On Saturday, December 22nd we had a significant outage and we want to take the time to explain what happened. This was one of the worst outages in the history of GitHub, and it\u2019s not", "date": "\n\t\t\t\tDecember 26, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScheduled Maintenance Windows\t\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-20-scheduled-maintenance-windows/", "abstract": "As our infrastructure continues to grow and evolve, it\u2019s sometimes necessary to perform system maintenance that may cause downtime. We have a number of projects queued up over the coming months to take our infrastructure", "date": "\n\t\t\t\tDecember 20, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNetwork problems last Friday\t\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-05-network-problems-last-friday/", "abstract": "On Friday, November 30th, GitHub had a rough day. We experienced 18 minutes of complete unavailability along with sporadic bursts of slow responses and intermittent errors for the entire day. I\u2019m very sorry this happened", "date": "\n\t\t\t\tDecember 5, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\thtml-pipeline: Chainable Content Filters\t\t\t", "author": ["\n\t\tJerry Cheung\t"], "link": "https://github.blog/2012-11-27-html-pipeline-chainable-content-filters/", "abstract": "Ever wondered how to get emoji, syntax highlighting, custom linking, and markdown to play nice together? HTML::Pipeline is the answer. We\u2019ve extracted several HTML utilities that we use internally in GitHub and packaged them into", "date": "\n\t\t\t\tNovember 27, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRebel: a Framework for Improving AppKit\t\t\t", "author": ["\n\t\tJustin Spahr-Summers\t"], "link": "https://github.blog/2012-10-23-rebel-a-framework-for-improving-appkit/", "abstract": "In our last blog post, we revealed Mantle, our Cocoa model framework. Today, we\u2019re announcing Rebel, a framework for improving AppKit. Since you may recall our original TwUI announcement, the decision to start using AppKit", "date": "\n\t\t\t\tOctober 23, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMantle: a Model Framework for Objective-C\t\t\t", "author": ["\n\t\tJustin Spahr-Summers\t"], "link": "https://github.blog/2012-10-22-mantle-a-model-framework-for-objective-c/", "abstract": "Lately, we\u2019ve been shipping more in GitHub for Mac than ever before. Now that username autocompletion and Notification Center support are out the door, we\u2019re releasing the two frameworks that helped make it happen. This", "date": "\n\t\t\t\tOctober 22, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe GitHub:Training Web Site is a Thing\t\t\t", "author": ["\n\t\tTim Berglund\t"], "link": "https://github.blog/2012-10-05-the-github-training-web-site-is-a-thing/", "abstract": "The past six months have seen tremendous growth in GitHub\u2019s training organization. We\u2019ve added people, we\u2019ve added materials, and we\u2019ve been to more places on the planet to help make it easy for humans to", "date": "\n\t\t\t\tOctober 5, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe GitHub hiring experience\t\t\t", "author": ["\n\t\tCoby Chapple\t"], "link": "https://github.blog/2012-09-24-the-github-hiring-experience/", "abstract": "Crafting experiences is central to what we do here at GitHub, and our interviewing, hiring, and on-boarding experiences are no exception. Having recently been through this process first-hand, I\u2019d like to share a little bit", "date": "\n\t\t\t\tSeptember 24, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we ship GitHub for Windows\t\t\t", "author": ["\n\t\tAdam Roben\t"], "link": "https://github.blog/2012-09-24-how-we-ship-github-for-windows/", "abstract": "We\u2019ve shipped 25 updates to GitHub for Windows in the 4 months since we first launched. That\u2019s more than one release per week, for 17 weeks straight! Here\u2019s how we do it. One release per", "date": "\n\t\t\t\tSeptember 24, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGitHub availability this week\t\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2012-09-14-github-availability-this-week/", "abstract": "GitHub.com suffered two outages early this week that resulted in one hour and 46 minutes of downtime and another hour of significantly degraded performance. This is far below our standard of quality, and for that", "date": "\n\t\t\t\tSeptember 14, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we keep GitHub fast\t\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2012-09-05-how-we-keep-github-fast/", "abstract": "The most important factor in web application design is responsiveness. And the first step toward responsiveness is speed. But speed within a web application is complicated. Our strategy for keeping GitHub fast begins with powerful", "date": "\n\t\t\t\tSeptember 5, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDeploying at GitHub\t\t\t", "author": ["\n\t\tJake Douglas\t"], "link": "https://github.blog/2012-08-29-deploying-at-github/", "abstract": "Deploying is a big part of the lives of most GitHub employees. We don\u2019t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves", "date": "\n\t\t\t\tAugust 29, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOptimizing Sales for Happiness\t\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2012-08-28-optimizing-sales-for-happiness/", "abstract": "Looking for GitHub\u2019s sales team? Please Contact Us. 10/17/14 Update from @pjhyett: Building products that people love paired with a customer-focused sales team hasn\u2019t changed, but the specifics of our approach have evolved over time", "date": "\n\t\t\t\tAugust 28, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSurviving the SSHpocolypse\t\t\t", "author": ["\n\t\tScott J. Goldman\t"], "link": "https://github.blog/2012-07-26-surviving-the-sshpocolypse/", "abstract": "Over the past few days, we have had some issues with our SSH infrastructure affecting a small number of Git SSH operations. We apologize for the inconvenience, and are happy to report that we\u2019ve completed", "date": "\n\t\t\t\tJuly 26, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDesigning GitHub for Windows\t\t\t", "author": ["\n\t\tTimothy Clem\t"], "link": "https://github.blog/2012-06-07-designing-github-for-windows/", "abstract": "This article hasn\u2019t been updated in a while. For the most current information, please refer to the official Help documentation and the Desktop website. Today, I thought it would be fun to give some insight", "date": "\n\t\t\t\tJune 7, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe Making of Octicons\t\t\t", "author": ["\n\t\tCameron McEfee\t"], "link": "https://github.blog/2012-05-09-the-making-of-octicons/", "abstract": "In our last post we announced Octicons, our new icon font. We put a lot of work into the font and gained a lot of knowledge in the process. With five different designers working to", "date": "\n\t\t\t\tMay 9, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tReactiveCocoa for a better world\t\t\t", "author": ["\n\t\tJosh Abernathy\t"], "link": "https://github.blog/2012-05-04-reactivecocoa-for-a-better-world/", "abstract": "Native apps spend a lot of time waiting and then reacting. We wait for the user to do something in the UI. Wait for a network call to respond. Wait for an asynchronous operation to", "date": "\n\t\t\t\tMay 4, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow we use Pull Requests to build GitHub\t\t\t", "author": ["\n\t\tBen Bleikamp\t"], "link": "https://github.blog/2012-05-02-how-we-use-pull-requests-to-build-github/", "abstract": "We recently shipped a new About section. It has all sorts of stuff like high resolution logos, pictures of the GitHub team, a little bit about our story, recent press mentions and maybe most importantly", "date": "\n\t\t\t\tMay 2, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAkavache is now open source\t\t\t", "author": ["\n\t\tPaul Betts\t"], "link": "https://github.blog/2012-04-28-akavache-is-now-open-source/", "abstract": "Today, we\u2019re open-sourcing a library that we have been using at GitHub: Akavache. Akavache is an asynchronous, persistent key-value cache created for writing native desktop and mobile applications in C#. Think of it like memcached", "date": "\n\t\t\t\tApril 28, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tFileserver Maintenance Wednesday Night\t\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2012-04-24-fileserver-maintenance-wednesday-night/", "abstract": "We\u2019ll be performing some scheduled maintenance on a fileserver pair this Wednesday (April 25th, 2012) at 9PM PDT. A small percentage of repositories will be placed in maintenance mode for around 30 minutes during this", "date": "\n\t\t\t\tApril 24, 2012\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tAll of the Hooks\t\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2011-10-19-all-of-the-hooks/", "abstract": "Over three years ago, @pjhyett launched GitHub Services with just four services: Campfire, IRC, Lighthouse, and Twitter. Since then, 124 other people contributed to a total of 68 third-party services. We and many others depend", "date": "\n\t\t\t\tOctober 19, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGist creation in 5 lines with Java v3 API\t\t\t", "author": ["\n\t\tKevin Sawicki\t"], "link": "https://github.blog/2011-05-18-gist-creation-in-5-lines-with-java-v3-api/", "abstract": "As part of the Mylyn connector announcement last week a new stand-alone Java library was released for accessing GitHub API v3. Below is a 5 line example showing how to create a new private Gist", "date": "\n\t\t\t\tMay 18, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThose are some big numbers\t\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2011-04-20-those-are-some-big-numbers/", "abstract": "Every night, our friendly Hubot pops into one of our Campfire rooms and posts some numbers. Turns out we passed some pretty significant numbers in the past couple days. And numbers are fun, so we", "date": "\n\t\t\t\tApril 20, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tReply to Comments from Email\t\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2011-03-10-reply-to-comments-from-email/", "abstract": "You should notice a small change to the From address on your email notifications now: they\u2019re no longer from no-reply@github.com. We\u2019re now accepting replies from most email notifications that you\u2019ll receive: Issue comments Commit comments", "date": "\n\t\t\t\tMarch 10, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScheduled Maintenance Tonight at 22:00 PST\t\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2011-01-24-scheduled-maintenance-tonight-at-22-00-pst/", "abstract": "We will be performing scheduled maintenance tonight from 22:00 to 22:20 PST. During this window GitHub and all Git access will be entirely unavailable for a short period while we perform MySQL and Redis maintenance.", "date": "\n\t\t\t\tJanuary 24, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRecent Services Interruptions\t\t\t", "author": ["\n\t\tCorey Donohoe\t"], "link": "https://github.blog/2011-01-06-recent-services-interruptions/", "abstract": "Here\u2019s a summary of the outages we encountered this week and what we\u2019re doing to prevent this from happening again. Monday January 3rd Monday marked the first \u201creal\u201d workday for most people in 2011. Our", "date": "\n\t\t\t\tJanuary 6, 2011\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe Tree Slider\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2010-12-07-the-tree-slider/", "abstract": "Those of you running recent versions of Safari, Chrome, or Firefox 4 may have noticed some changes to tree browsing on GitHub. The new HTML5 History API (which really has nothing to do with HTML", "date": "\n\t\t\t\tDecember 7, 2010\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScala Projects Classified Properly\t\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2010-04-20-scala-projects-classified-properly/", "abstract": "We just fixed an issue where some Scala projects were being misclassified as Java projects. Now, recent projects like Twitter\u2019s FlockDB and Gizzard show up in the Scala language dashboard as they should. package examples", "date": "\n\t\t\t\tApril 20, 2010\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tTracking Deploys with Compare View\t\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2010-03-09-tracking-deploys-with-compare-view/", "abstract": "We log a message to Campfire anytime someone deploys code to staging or production. It looks like this: Recently, we added the link pointing to a Compare View where you can review the commits that", "date": "\n\t\t\t\tMarch 9, 2010\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tNew Languages Highlighted\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2010-02-24-new-languages-highlighted/", "abstract": "CoffeeScript (.coffee) LotteryDraw: { play: -> result: LotteryTicket.new_random() winners: {} this.tickets.each (buyer, ticket_list) -> ticket_list.each (ticket) -> score: ticket.score(result) return if score is 0 winners[buyer] ||= [] winners[buyer].push([ticket, score]) this.tickets: {} winners } Objective-J (.j)", "date": "\n\t\t\t\tFebruary 24, 2010\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tOptimizing asset bundling and serving with Rails\t\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2009-11-19-optimizing-asset-bundling-and-serving-with-rails/", "abstract": "We spend a lot of time optimizing the front end experience at GitHub. With that said, our asset (css, javascript, images) packaging and serving has evolved to be the best setup I\u2019ve seen out of", "date": "\n\t\t\t\tNovember 19, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tMultiple file gist improvements\t\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2009-11-19-multiple-file-gist-improvements/", "abstract": "We\u2019ve always had the ability to embed multiple file gists: Your browser does not support IFrames But today we added the ability to embed specific files in a multi-file gist! Your browser does not support", "date": "\n\t\t\t\tNovember 19, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tHow We Made GitHub Fast\t\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2009-10-20-how-we-made-github-fast/", "abstract": "Now that things have settled down from the move to Rackspace, I wanted to take some time to go over the architectural changes that we\u2019ve made in order to bring you a speedier, more scalable", "date": "\n\t\t\t\tOctober 20, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tunicorn.god\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-10-12-unicorn-god/", "abstract": "Some people have been asking for our Unicorn god config. Here it is: # http://unicorn.bogomips.org/SIGNALS.html rails_env = ENV[\u2018RAILS_ENV\u2019] || \u2018production\u2019 rails_root = ENV[\u2018RAILS_ROOT\u2019] || \u201c/data/github/current\u201d God.watch do |w| w.name = \u201cunicorn\u201d w.interval = 30.seconds #", "date": "\n\t\t\t\tOctober 12, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tUnicorn!\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-10-09-unicorn/", "abstract": "We\u2019ve been running Unicorn for more than a month. Time to talk about it. What is it? Unicorn is an HTTP server for Ruby, similar to Mongrel or Thin. It uses Mongrel\u2019s Ragel HTTP parser", "date": "\n\t\t\t\tOctober 9, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tRackspace Move Scheduled for Sunday, September 27th at 5PM Pacific Time\t\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2009-09-22-rackspace-move-scheduled-for-sunday-september-27th-at-5pm-pacific-time/", "abstract": "On Sunday, September 27th at 5PM Pacific time (see it in your timezone) we will begin the final move to Rackspace. We are aiming to have less than an hour of website and push unavailability", "date": "\n\t\t\t\tSeptember 22, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDeployment Script Spring Cleaning\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-08-05-deployment-script-spring-cleaning/", "abstract": "Better late than never, right? As we get ready to upgrade our servers I thought it\u2019d be a good time to upgrade our deployment process. Currently pushing out a new version of GitHub takes upwards", "date": "\n\t\t\t\tAugust 5, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSmart JS Polling\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-07-30-smart-js-polling/", "abstract": "While Comet may be all the rage, some of us are still stuck in web 2.0. And those of us that are use Ajax polling to see if there\u2019s anything new on the server. Here", "date": "\n\t\t\t\tJuly 30, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGit as a Data Store in Python (and Ruby)\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-03-10-git-as-a-data-store-in-python-and-ruby/", "abstract": "I recently stumbled across an older article titled Using Git as a versioned data store in Python by @jwiegley. Your browser does not support IFrames Basically, it\u2019s a module similar to Python\u2019s shelve which stores", "date": "\n\t\t\t\tMarch 10, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tKeeping GoogleBot Happy\t\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2009-03-04-keeping-googlebot-happy/", "abstract": "One of the interesting side effects I hadn\u2019t considered when we rolled out some fairly significant caching updates on GitHub in the beginning of January was how much Google\u2019s crawler would take full advantage of", "date": "\n\t\t\t\tMarch 4, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tPHP in Erlang\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-02-16-php-in-erlang/", "abstract": "You heard me right. php_app manages a pool of persistent PHP processes and provides a simple API to evaluate PHP code from Erlang. The blog post gives a quick overview and some examples. Your browser", "date": "\n\t\t\t\tFebruary 16, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tEasy Git!\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-02-03-easy-git/", "abstract": "eg is a nifty piece of work. Are you meeting resistance trying to move your coworkers or friends to Git? (\u201cSVN is good enough.\u201d) Know someone who would love to use GitHub but can\u2019t seem", "date": "\n\t\t\t\tFebruary 3, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScripting Bioclipse\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-23-scripting-bioclipse/", "abstract": "Bioclipse (a Java-based, open source, visual platform for chemo- and bioinformatics) has scripting support and the community has developed a great method for sharing those scripts: Gist! They create Gists then tag them on delicious", "date": "\n\t\t\t\tJanuary 23, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tCompojure: Clojure Web Framework\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-20-compojure-clojure-web-framework/", "abstract": "Compojure is a Clojure web framework similar to web.py or Sinatra. Your browser does not support IFrames The project also has a Wikibook and mailing list. Looks cool.", "date": "\n\t\t\t\tJanuary 20, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tGist for Greasemonkey\t\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2009-01-19-gist-for-greasemonkey/", "abstract": "We\u2019re now appending the gist name at the end of its raw url. That means it\u2019s dead-simple to serve greasemonkey (or greasekit) scripts directly from gist.github.com. I was able to write my first script and", "date": "\n\t\t\t\tJanuary 19, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDirty Git State in Your Prompt\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-15-dirty-git-state-in-your-prompt/", "abstract": "Henrik has a great article explaining why and how to display Git\u2019s dirty state status (along with the branch, of course) in your bash prompt. topfunky prefers a skull and bones for his dirty state", "date": "\n\t\t\t\tJanuary 15, 2009\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tdj.god\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-11-18-dj-god/", "abstract": "People have asked for our delayed_job god config. Welp, here it is:", "date": "\n\t\t\t\tNovember 18, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSpeedy Queries\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-10-31-speedy-queries/", "abstract": "First our new queue, and now this: ![](https://cloud.githubusercontent.com/assets/391331/6140975/de5ad0f6-b153-11e4-8000-eaf6de1f3d72.png) ![](https://cloud.githubusercontent.com/assets/391331/6140976/e1b220c4-b153-11e4-9b07-b8e763ef49e7.jpg) The site should be much faster \u2013 but it\u2019s still not fast enough. We\u2019re hard at work making things like git clones, tree browsing, and commit", "date": "\n\t\t\t\tOctober 31, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tThe New Queue\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-10-30-the-new-queue/", "abstract": "Yesterday we moved to a new queue, Shopify\u2019s delayed_job (or dj). After trying a few different solutions in the early days, we settled on Ara Howard\u2019s Bj. It was fine for quite a while, but", "date": "\n\t\t\t\tOctober 30, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tScaling Lesson #23742\t\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2008-07-17-scaling-lesson-23742/", "abstract": "GitHub was created as a side project, but it seems to have struck a nerve and gained traction quickly. As such, a lot of the infrastructure decisions were made not figuring on this sort of", "date": "\n\t\t\t\tJuly 17, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSupercharged git-daemon\t\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2008-07-14-supercharged-git-daemon/", "abstract": "Over the past several weeks I\u2019ve been working on a secret Erlang project that will allow us to grow GitHub in new and novel ways. The project is called egitd and is a replacement for", "date": "\n\t\t\t\tJuly 14, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tSupercharged Ruby-Git\t\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2008-07-09-supercharged-ruby-git/", "abstract": "One of the slowest things you can do in Ruby is shell out to the operating system. As a contrived example, let\u2019s open an empty file 1,000 times: >> require \u2018benchmark\u2019 >> `touch foo` >>", "date": "\n\t\t\t\tJuly 9, 2008\t\t\t"},
{"website": "Github-Engineering", "title": "\n\t\t\t\tDowntime Tonight\t\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-03-07-downtime-tonight/", "abstract": "We\u2019ll have an hour or two of downtime tonight around midnight PST while the awesome dudes at Engine Yard upgrade our disk capacity. Thanks, see you on the flip side.", "date": "\n\t\t\t\tMarch 7, 2008\t\t\t"}
][
{"website": "Google-AI", "title": "\nHDR+ with Bracketing on Pixel Phones\n", "author": ["Posted by Manfred Ernst and Bartlomiej Wronski, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2021/04/hdr-with-bracketing-on-pixel-phones.html", "abstract": "We're continuously working to improve the Pixel \u2014 making it more helpful, more capable, and more fun \u2014 with regular updates, such as the recent V8.2 update to the Camera app. One such improvement (launched on Pixel 5 and Pixel 4a 5G in October) is a feature that operates \u201cunder the hood\u201d, HDR+ with Bracketing. This feature works by merging images taken with different exposure times to improve image quality (especially in shadows), resulting in more natural colors, improved details and texture, and reduced noise.\n\n\nThe original HDR+  system is the engine behind high-quality mobile photography, which captures a rapid series of deliberately underexposed images, then combines and renders them in a way that preserves detail across the range of tones. But this system had one limitation: scenes with  (HDR) like the one below were noisy in the shadows because all images captured are underexposed.\nCapturing HDR scenes is difficult because of the physical constraints of image sensors combined with limited signal in the shadows. We can correctly expose either the shadows or the highlights, but not both at the same time.\n\nPhotographers sometimes work around these limitations by taking two different exposures and combining them. This approach, known as  can deliver the best of both worlds, but it is time-consuming to do by hand. It is also challenging in computational photography because it requires:\n\nTo avoid these challenges, the original HDR+ system used a different approach to handle high dynamic range scenes.\n\n\nThe capture strategy used by  is based on underexposure, which avoids loss of detail in the highlights. While this strategy comes at the expense of noise in the shadows, HDR+ offsets the increased noise through the use of burst photography.\n\nThis approach works well for scenes with moderate dynamic range, but breaks down for HDR scenes. To understand why, we need to take a closer look at how two types of noise get into an image.\n\n\nOne important type of noise is called , which depends only on the total amount of light captured \u2014 the sum of N frames, each with E seconds of exposure time has the same amount of shot noise as a single frame exposed for N \u00d7 E seconds. If this were the only type of noise present in captured images, burst photography would be as efficient as taking longer exposures. Unfortunately, a second type of noise, , is introduced by the sensor every time a frame is captured. Read noise doesn\u2019t depend on the amount of light captured but instead depends on the number of frames taken \u2014 that is, with each frame taken, an additional fixed amount of read noise is added. \n\nThis is why using burst photography to reduce total noise isn\u2019t as efficient as simply taking longer exposures: taking multiple frames can reduce the effect of shot noise, but will also increase read noise. Even though read noise increases with the number of frames, it is still possible to reduce the  noisiness with burst photography, but it becomes less efficient. If one were to break a long exposure into N shorter exposures, the ratio of signal to noise in the final image would be lower because of the additional read noise. In this case, to get back to the signal-to-noise ratio in the single long exposure, one would need to merge N short-exposure frames. In the example below, if a long exposure were divided into 12 short exposures, we'd have to capture 144 (12 \u00d7 12) short frames to match the signal-to-noise ratio in the shadows! Capturing and processing this many frames would be much more time consuming \u2014 burst capture and processing could take over a minute and result in a poor user experience. Instead, with bracketing one can capture both short and long exposures \u2014 combining highlight protection and noise reduction.\n\n\nWhile the challenges of bracketing prevented the original HDR+ system from using it, incremental improvements since then, plus a recent concentrated effort, have made it possible in the Camera app. To start, adding bracketing to HDR+ required redesigning the capture strategy. Capturing is complicated by  (ZSL), which underpins the fast capture experience on Pixel. With ZSL, the frames displayed in the viewfinder  the shutter press are the frames we use for HDR+ burst merging. For bracketing, we capture an additional long exposure frame after the shutter press, which is not shown in the viewfinder. Note that holding the camera still for half a second after the shutter press to accommodate the long exposure can help improve image quality, even with a typical amount of handshake. \n\nFor Night Sight, the capture strategy isn't constrained by the viewfinder \u2014 because all frames are captured after the shutter press while the viewfinder is stopped, this mode easily accommodates capturing longer exposure frames. In this case, we capture three long exposures to further reduce noise.\n\n\nWhen merging bracketed shots, we choose one of the short frames as the reference frame to avoid potentially clipped highlights and motion blur. All other frames are aligned to this frame before they are merged. This introduces a challenge \u2014 for complex scene motion or occluded regions, it is impossible to find exactly matching regions and a na\u00efve merge algorithm would produce  in these cases.\n\nTo address this, we designed a new spatial merge algorithm, similar to the one used for , that decides per pixel whether image content should be merged or not. This  is more complicated for frames with different exposures. Long exposure frames have different noise characteristics, clipped highlights, and different amounts of motion blur, which makes comparisons with the short exposure reference frame more difficult. In addition, ghosting artifacts are more visible in bracketed shots, because noise that would otherwise mask these errors is reduced. Despite those challenges, our algorithm is as robust to these issues as the original HDR+ and Super Res Zoom and doesn\u2019t produce ghosting artifacts. At the same time, it merges images 40% faster than its predecessors. Because it merges RAW images early in the photographic pipeline, we were able to achieve all of those benefits while keeping the rest of processing and the signature HDR+ look unchanged. Furthermore, users who prefer to use computational RAW images can take advantage of those image quality and performance improvements.\n\n\nHDR+ with Bracketing is available to users of Pixel 4a (5G) and 5 in the default camera, as well as in Night Sight and Portrait modes. For users of Pixel 4 and 4a, the Google Camera app supports bracketing in Night Sight mode. No user interaction is needed to activate HDR+ with Bracketing \u2014 depending on the dynamic range of the scene, and the presence of motion, HDR+ with bracketing chooses the best exposures to maximize image quality ().", "date": "\nFriday, April 23, 2021\n"},
{"website": "Google-AI", "title": "\nEvolving Reinforcement Learning Algorithms\n", "author": ["Posted by John D. Co-Reyes, Research Intern and Yingjie Miao, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2021/04/evolving-reinforcement-learning.html", "abstract": "A long-term, overarching goal of research into reinforcement learning (RL) is to design a single general purpose learning algorithm that can solve a wide array of problems. However, because the RL algorithm  is quite large, and designing new RL algorithms requires extensive tuning and validation, this goal is a daunting one.  A possible solution would be to devise a  that could design new RL algorithms that generalize to a wide variety of tasks automatically.\n\nIn recent years,  has shown great success in automating the design of machine learning components, such as neural networks architectures and model update rules. One example is  (NAS), which has been used to develop better  for image classification and efficient architectures for running on  and . In addition to NAS,  shows that it\u2019s even possible to learn the  algorithm from scratch using basic mathematical operations. One common theme in these approaches is that the neural network architecture or the entire algorithm is represented by a graph, and a separate algorithm is used to optimize the graph for certain objectives.\n\nThese earlier approaches were designed for , in which the overall algorithm is more straightforward. But in RL, there are more components of the algorithm that could be potential targets for design automation (e.g., neural network architectures for , \u00a0from the replay buffer, overall formulation of the ), and it is not always clear what the best model update procedure would be to integrate these components. Prior efforts for the automation RL algorithm discovery have focused primarily on model update rules. These  learn the optimizer or RL  procedure itself and commonly represent the update rule with a neural network such as an  or , which can be efficiently optimized with gradient-based methods. However, these learned rules are not interpretable or generalizable, because the learned weights are opaque and domain specific.\nIn our paper \u201c\u201d, accepted at , we show that it\u2019s possible to learn new, and generalizable RL algorithms by using a graph representation and applying optimization techniques from the AutoML community. In particular, we represent the loss function, which is used to optimize an agent\u2019s parameters over its experience, as a , and use   to evolve a population of the computational graphs over a set of simple training environments. This results in increasingly better RL algorithms, and the discovered algorithms generalize to more complex environments, even those with visual observations like Atari games.\n\n\nInspired by ideas from NAS, which searches over the space of graphs representing neural network architectures, we meta-learn RL algorithms by representing the loss function of an RL algorithm as a computational graph. In this case, we use a  for the loss function, with nodes representing inputs, operators, parameters and outputs. For example, in the computational graph for , input nodes include data from the replay buffer, operator nodes include neural network operators and basic math operators, and the output node represents the loss, which will be minimized with gradient descent. \n\nThere are a few benefits of such a representation. This representation is expressive enough to define existing algorithms but also new, undiscovered algorithms. It is also interpretable. This graph representation can be analyzed in the same way as human designed RL algorithms, making it more interpretable than approaches that use black box function approximators for the entire RL update procedure. If researchers can understand why a learned algorithm is better, then they can both modify the internal components of the algorithm to improve it and transfer the beneficial components to other problems. Finally, the representation supports general algorithms that can solve a wide variety of problems. \n\nWe implemented this representation using the  library, which conveniently turns the graph into a search space that can be optimized with regularized evolution. \n\n \nWe use an evolutionary based approach to optimize the RL algorithms of interest. First, we initialize a population of training agents with randomized graphs. This population of agents is trained in parallel over a set of training environments. The agents first train on a hurdle environment \u2014 an easy environment, such as , intended to quickly weed out poorly performing programs. \n\nIf an agent cannot solve the hurdle environment, the training is stopped early with a score of zero. Otherwise the training proceeds to more difficult environments (e.g., , simple  environments, etc.). The algorithm performance is evaluated and used to update the population, where more promising algorithms are further mutated. To reduce the search space, we use a functional equivalence checker which will skip over newly proposed algorithms if they are functionally the same as previously examined algorithms. This loop continues as new mutated candidate algorithms are trained and evaluated. At the end of training, we select the best algorithm and evaluate its performance over a set of unseen test environments.\n\nThe population size in the experiments was around 300 agents, and we observed the evolution of good candidate loss functions after 20-50 thousand mutations, requiring about three days of training. We were able to train on CPUs because the training environments were simple, controlling for the computational and energy cost of training. To further control the cost of training, we seeded the initial population with human-designed RL algorithms such as DQN. \n\n\nWe highlight two discovered  that exhibit good generalization performance. The first is , which builds on DQN by adding a weighted penalty on the Q-values to the normal squared Bellman error. The second learned loss function, , is more complex, although its dominating term has a simple form \u2014 the max of the Q-value and the squared Bellman error (modulo a constant). Both algorithms can be viewed as a way to regularize the Q-values. While DQNReg adds a soft constraint, DQNClipped can be interpreted as a kind of constrained optimization that will minimize the Q-values if they become too large. We show that this learned constraint kicks in during the early stage of training when overestimating the Q-values is a potential issue. Once this constraint is satisfied, the loss will instead minimize the original squared Bellman error.\n\nA closer analysis shows that while baselines like DQN commonly overestimate Q-values, our learned algorithms address this issue in different ways. DQNReg underestimates the Q-values, while DQNClipped has similar behavior to  in that it slowly approaches the ground truth without overestimating it.\n\nIt\u2019s worth pointing out that these two algorithms  emerge when the evolution is seeded with DQN. Learning from scratch, the method rediscovers the . For completeness, we  of top 1000 performing algorithms discovered during evolution. Curious readers could further investigate the properties of these learned loss functions.\n\n\nNormally in RL, generalization refers to a trained policy generalizing across tasks. However, in this work we\u2019re interested in algorithmic generalization performance, which means how well an algorithm works over a set of environments. On a set of , the learned algorithms can match baselines on the dense reward tasks (CartPole, , LunarLander) and outperform DQN on the sparser reward task, . \n\nOn a set of sparse reward MiniGrid environments, which test a variety of different tasks, we see that DQNReg greatly outperforms baselines on both the training and test environments, in terms of sample efficiency and final performance. In fact, the effect is even more pronounced on the test environments, which vary in size, configuration, and existence of new obstacles, such as lava.\n\nWe visualize the performance of normal DDQN vs. the learned algorithm DQNReg on a few  MiniGrid environments. The starting location, wall configuration, and object configuration of these environments are randomized at each reset, which requires the agent to generalize instead of simply memorizing the environment. While DDQN often struggles to learn any meaningful behavior, DQNReg can learn the optimal behavior efficiently.\n\nEven on image-based Atari environments we observe improved performance, even though training was on non-image-based environments. This suggests that meta-training on a set of cheap but diverse training environments with a generalizable algorithm representation could enable radical algorithmic generalization.\n\n\nIn this post, we\u2019ve discussed learning new interpretable RL algorithms by representing their loss functions as computational graphs and evolving a population of agents over this representation. The computational graph formulation allows researchers to both build upon human-designed algorithms and study the learned algorithms using the same mathematical toolset as the existing algorithms. We analyzed a few of the learned algorithms and can interpret them as a form of entropy regularization to prevent value overestimation. These learned algorithms can outperform baselines and generalize to unseen environments. The top performing algorithms are available for further analytical study. \n\nWe hope that future work will extend to more varied RL settings such as actor critic algorithms or offline RL. Furthermore we hope that this work can lead to machine assisted algorithm development where computational meta-learning can help researchers find new directions to pursue and incorporate learned algorithms into their own work.", "date": "\nThursday, April 22, 2021\n"},
{"website": "Google-AI", "title": "\nMaX-DeepLab: Dual-Path Transformers for End-to-End Panoptic Segmentation\n", "author": ["Posted by Huiyu Wang, Student Researcher and Liang-Chieh Chen, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/04/max-deeplab-dual-path-transformers-for.html", "abstract": "is a computer vision task that unifies  (assigning a class label to each pixel) and  (detecting and segmenting each object instance). A core task for , panoptic segmentation predicts a set of non-overlapping masks along with their corresponding class labels (i.e., category of object, like \"car\", \"traffic light\", \"road\", etc.) and is generally accomplished using multiple surrogate sub-tasks that approximate (e.g., by using box detection methods) the goals of panoptic segmentation.\n\nEach surrogate sub-task in this proxy tree introduces extra manually-designed modules, such as anchor design rules, box assignment rules,  (NMS), , etc. Although there are good solutions to individual surrogate sub-tasks and modules, undesired artifacts are introduced when these sub-tasks come together in a pipeline for panoptic segmentation, especially in challenging conditions (e.g., two people with similar bounding boxes will trigger NMS, resulting in a missing mask).\n\nPrevious efforts, such as , attempted to solve some of these issues by simplifying the box detection sub-task into an  operation, which is more computationally efficient and results in fewer undesired artifacts. However, the training process still relies heavily on box detection, which does not align with the mask-based definition of panoptic segmentation. Another line of work completely removes boxes from the pipeline, which has the benefit of removing an entire surrogate sub-task along with its associated modules and artifacts. For example,  predicts pixel-wise offsets to predefined instance centers, but the surrogate sub-task it uses encounters challenges with highly deformable objects, which have a large variety of shapes (e.g., a cat), or nearby objects with close centers in the image plane, e.g. the image below of a dog seated in a chair.\n\nIn \u201c\u201d, to be presented at , we propose the first  end-to-end approach for the panoptic segmentation pipeline, directly predicting class-labeled masks by extending the  to this computer vision task. Dubbed -DeepLab for extending Axial-DeepLab with a sk former, our method employs a dual-path architecture that introduces a global memory path, allowing for direct communication with any convolution layers. As a result, MaX-DeepLab shows a significant 7.1%  (PQ) gain in the box-free regime on the challenging  dataset, closing the gap between box-based and box-free methods for the first time. MaX-DeepLab achieves the state-of-the-art 51.3% PQ on COCO test-dev set, without test time augmentation.\n\n\nInspired by , our model directly predicts a set of non-overlapping masks and their corresponding semantic labels, with output masks and classes that are optimized with a PQ-style objective. Specifically, inspired by the evaluation metric, PQ, which is defined as the recognition quality (whether or not the predicted class is correct) times the segmentation quality (whether the predicted mask is correct), we define a similarity metric between two class-labeled masks in the exact same way. The model is directly trained by maximizing this similarity between ground truth masks and predicted masks via one-to-one matching. This direct modeling of panoptic segmentation enables end-to-end training and inference, removing the hand-coded priors that are necessary in existing box-based and box-free methods.\n\n\nInstead of stacking a traditional transformer on top of a  (CNN), we propose a dual-path framework for combining CNNs with transformers. Specifically, we enable any CNN layer to read and write to global memory by using a dual-path transformer block. This proposed block adopts all four types of attention between the CNN-path and the memory-path, and can be inserted anywhere in a CNN, enabling communication with the global memory at any layer. MaX-DeepLab also employs a stacked-hourglass-style decoder that aggregates multi-scale features into a high resolution output. The output is then multiplied with the global memory feature, to form the mask set prediction. The classes for the masks are predicted with another branch of the mask transformer.\n\n\nWe evaluate MaX-DeepLab on one of the most challenging panoptic segmentation datasets, , against both of the state-of-the-art box-free (Axial-DeepLab) and box-based (DetectoRS) methods. MaX-DeepLab, without test time augmentation, achieves the state-of-the-art result of 51.3% PQ on the test-dev set.\n\nThis result surpasses Axial-DeepLab by 7.1% PQ in the box-free regime and DetectoRS by 1.7% PQ, bridging the gap between box-based and box-free methods for the first time. For a consistent comparison with , we also evaluated a lightweight version of MaX-DeepLab that matches the number of parameters and computations of DETR. The lightweight MaX-DeepLab outperforms DETR by 3.3% PQ on the val set and 3.0% PQ on the test-dev set. In addition, we performed extensive ablation studies and analyses on our end-to-end formulation, model scaling, dual-path architectures, and loss functions. Also the extra-long training schedule of DETR is not necessary for MaX-DeepLab.\n\nAs an example in the figure below, MaX-DeepLab correctly segments a dog sitting on a chair.  relies on a surrogate sub-task of regressing object center offsets. It fails because the centers of the dog and the chair are close to each other. DetectoRS classifies object bounding boxes, instead of masks, as a surrogate sub-task. It filters out the chair mask because the chair bounding box has a low confidence.\n\nAnother example shows how MaX-DeepLab correctly segments images with challenging conditions.\n\n\nWe have shown for the first time that panoptic segmentation can be trained end-to-end. MaX-DeepLab directly predicts masks and classes with a mask transformer, removing the need for many hand-designed priors such as object bounding boxes, thing-stuff merging, etc. Equipped with a PQ-style loss and a dual-path transformer, MaX-DeepLab achieves the state-of-the-art result on the challenging  dataset, closing the gap between box-based and box-free methods.", "date": "\nWednesday, April 21, 2021\n"},
{"website": "Google-AI", "title": "\nMulti-Task Robotic Reinforcement Learning at Scale\n", "author": ["Posted by Karol Hausman, Senior Research Scientist and Yevgen Chebotar, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html", "abstract": "For general-purpose robots to be most useful, they would need to be able to perform a range of tasks, such as cleaning, maintenance and delivery. But training even a single task (e.g., grasping) using  (RL), a trial and error learning method where the agent uses training previously collected data, can take , in addition to the significant engineering needed to enable autonomous operation of a large-scale robotic system. Thus, the computational costs of building general-purpose  using current robot learning methods becomes prohibitive as the number of tasks grows. \n\nIn other large-scale machine learning domains, such as  and , a number of strategies have been applied to amortize the effort of learning over multiple skills. For example,  on large natural language datasets can enable few- or zero-shot learning of multiple tasks, such as  and . However, because robots collect their own data, robotic skill learning presents a unique set of opportunities and challenges. Automating this process is a large engineering endeavour, and effectively reusing  remains an open problem. \n\nToday we present two new advances for robotic RL at scale, , a new multi-task RL system for automated data collection and multi-task RL training, and , which leverages the acquired data for  RL. MT-Opt introduces a scalable data-collection mechanism that is used to collect over 800,000 episodes of various tasks on real robots and demonstrates a successful application of multi-task RL that yields ~3x average improvement over baseline. Additionally, it enables robots to master new tasks quickly through use of its extensive multi-task dataset (new task fine-tuning in <1 day of data collection). Actionable Models enables learning in the absence of specific tasks and rewards by training an implicit model of the world that is also an actionable robotic policy. This drastically increases the number of tasks the robot can perform (via visual goal specification) and enables more efficient learning of downstream tasks.\n\n\nThe cornerstone for both MT-Opt and Actionable Models is the volume and quality of training data. To collect diverse, multi-task data at scale, users need a way to specify tasks, decide for which tasks to collect the data, and finally, manage and balance the resulting dataset. To that end, we create a scalable and intuitive multi-task success detector using data from all of the chosen tasks. The multi-task success is trained using supervised learning to detect the outcome of a given task and it allows users to quickly define new tasks and their rewards. When this success detector is being applied to collect data, it is periodically updated to accommodate distribution shifts caused by various real-world factors, such as varying lighting conditions, changing background surroundings, and novel states that the robots discover. \n\nSecond, we simultaneously collect data for multiple distinct tasks across multiple robots by using solutions to easier tasks to effectively bootstrap learning of more complex tasks. This allows training of a policy for the harder tasks and improves the data collected for them. As such, the amount of per-task data and the number of successful episodes for each task grows over time. To further improve the performance, we focus data collection on underperforming tasks, rather than collecting data uniformly across tasks. \n\nThis system collected 9600 robot hours of data (from 57 continuous data collection days on seven robots). However, while this data collection strategy was effective at collecting data for a large number of tasks, the success rate and data volume was imbalanced between tasks. \n\n\nWe address the data collection imbalance by transferring data across tasks and re-balancing the per-task data. The robots generate episodes that are labelled as success or failure for each task and are then copied and shared across other tasks. The balanced batch of episodes is then sent to our multi-task RL training pipeline to train the MT-Opt policy.\n\nMT-Opt uses , a popular RL method that learns a function that estimates the future sum of rewards, called the Q-function. The learned policy then picks the action that maximizes this learned Q-function. For multi-task policy training, we specify the task as an extra input to a large Q-learning network (inspired by our previous work on large-scale single-task learning with ) and then train all of the tasks simultaneously with  using the entire multi-task dataset. In this way, MT-Opt is able to train on a wide variety of skills that include picking specific objects, placing them into various fixtures, aligning items on a rack, rearranging and covering objects with towels, etc. \n\nCompared to single-task baselines, MT-Opt performs similarly on the tasks that have the most data and significantly improves performance on underrepresented tasks. So, for a generic lifting task, which has the most supporting data, MT-Opt achieved an 89% success rate (compared to 88% for QT-Opt) and achieved a 50% average success rate across rare tasks, compared to 1% with a single-task QT-Opt baseline and 18% using a na\u00efve, multi-task QT-Opt baseline. Using MT-Opt not only enables zero-shot generalization to new but similar tasks, but also can quickly (in about 1 day of data collection on seven robots) be fine-tuned to new, previously unseen tasks. For example, when applied to an unseen towel-covering task, the system achieved a zero-shot success rate of 92% for towel-picking and 79% for object-covering, which wasn\u2019t present in the original dataset.\n\n\nWhile supplying a rigid definition of tasks facilitates autonomous data collection for MT-Opt, it limits the number of learnable behaviors to a fixed set. To enable learning a wider range of tasks from the same data, we use  learning, i.e., learning to reach given goal configurations of a scene in front of the robot, which we specify with goal images. In contrast to explicit  that learn predictive models of future world observations, or approaches that employ , this approach learns goal-conditioned policies via offline model-free RL. \n\nTo learn to reach any goal state, we perform  of all trajectories and sub-sequences in our collected dataset and train a goal-conditioned Q-function in a  manner (in contrast to learning online using a fixed set of success examples as in ). One challenge in this setting is the distributional shift caused by learning only from \u201cpositive\u201d hindsight relabeled examples. This we address by employing a  to minimize Q-values of unseen actions using artificial negative actions. Furthermore, to enable reaching temporary-extended goals, we introduce a technique for chaining goals across multiple episodes. \n\nTraining with Actionable Models allows the system to learn a large repertoire of visually indicated skills, such as object grasping, container placing and object rearrangement. The model is also able to generalize to novel objects and visual objectives not seen in the training data, which demonstrates its ability to learn general functional knowledge about the world. We also show that downstream reinforcement learning tasks can be learned more efficiently by either fine-tuning a pre-trained goal-conditioned model or through a goal-reaching auxiliary objective during training.\n\n\nThe results of both MT-Opt and Actionable Models indicate that it is possible to collect and then learn many distinct tasks from large diverse real-robot datasets within a single model, effectively amortizing the cost of learning across many skills. We see this an important step towards general robot learning systems that can be further scaled up to perform many useful services and serve as a starting point for learning downstream tasks. \nThis post is based on two papers, \"\" and \",\" with additional information and videos on the project websites for  and .", "date": "\nMonday, April 19, 2021\n"},
{"website": "Google-AI", "title": "\nPresenting the iGibson Challenge on Interactive and Social Navigation\n", "author": ["Posted by Anthony Francis, Software Engineer and Alexander Toshev, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/04/presenting-igibson-challenge-on.html", "abstract": "Computer vision has significantly advanced over the past decade thanks to large-scale benchmarks, such as  for  or  for , which provide vast datasets and criteria for evaluating models. However, these traditional benchmarks evaluate  tasks in which the emphasis is on perception alone, whereas more recent computer vision research has tackled  tasks, which require both perception and action (often called \u201c\u201d). \n\nThe , co-organized by Google at , hosted several benchmark challenges for active tasks, including the Stanford and Google organized , which provided a real-world setup to test navigation policies trained in photo-realistic simulation environments. An open-source setup in the challenge enabled the community to train policies in simulation, which could then be run in repeatable real world navigation experiments, enabling the evaluation of the \u201csim-to-real gap\u201d \u2014 the difference between simulation and the real world. Many research teams submitted solutions during the pandemic, which were run safely by challenge organizers on real robots, with  virtually at the workshop. \n\nThis year, Stanford and Google are proud to announce a new version of the , one of the 10 active visual challenges affiliated with the   at . This year\u2019s Embodied AI Workshop is co-organized by Google and nine other research organizations, and explores issues such as simulation, sim-to-real transfer, , ,  and , , and following instructions for  and  tasks. In addition, this year\u2019s interactive and social iGibson challenge explores interactive navigation and social navigation \u2014 how robots can learn to interact with people and objects in their environments \u2014 by combining  the , the , and .\n\n\nActive perception tasks are challenging, as they require both perception and actions in response. For example,  involves navigating through mapped space, such as driving , while recognizing and avoiding obstacles. Similarly  involves looking for objects in buildings, requiring  and . Additionally,  involves . These problems become even harder in a real-world environment, where robots must be able to handle a variety of physical and social interactions that are much more dynamic and challenging to solve. In this year\u2019s iGibson Challenge, we focus on two of those settings:\n\n\nTo facilitate research into techniques that address these problems, the iGibson Challenge 2021 dataset provides simulated interactive scenes for training. The dataset includes eight fully interactive scenes derived from real-world apartments, and another seven scenes held back for testing and evaluation.\n\nTo enable interactive navigation, these scenes are populated with small objects drawn from the , a dataset of common household objects scanned in 3D for use in robot simulation and computer vision research, licensed under a Creative Commons license to give researchers the freedom to use them in their research.\n\nThe challenge is implemented in Stanford\u2019s open-source , a fast, interactive, photorealistic robotic simulator with physics based on . For this year\u2019s challenge, iGibson has been expanded with  and  based on the .\n\n\nThe iGibson Challenge has launched and its  is open in the Dev phase, in which participants are encouraged to submit robotic control to the development leaderboard, where they will be tested on the Interactive and Social Navigation challenges on our holdout dataset. The Test phase opens for teams to submit final solutions on May 16th and closes on May 31st, with the winner demo scheduled for June 20th, 2021. For more details on participating, please check out the .", "date": "\nWednesday, April 14, 2021\n"},
{"website": "Google-AI", "title": "\nMonster Mash: A Sketch-Based Tool for Casual 3D Modeling and Animation\n", "author": ["Posted by Cassidy Curtis, Visual Designer and David Salesin, Principal Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/04/monster-mash-sketch-based-tool-for.html", "abstract": "3D computer animation is a time-consuming and highly technical medium \u2014 to complete even a single animated scene requires numerous steps, like modeling,  and animating, each of which is itself a sub-discipline that can take years to master. Because of its complexity, 3D animation is generally practiced by teams of skilled specialists and is inaccessible to almost everyone else, despite decades of advances in technology and tools. With the recent development of tools that facilitate  and , a natural question arises: is it possible to democratize the 3D animation process so it\u2019s accessible to everyone?  \n\nTo explore this concept, we start with the observation that most forms of artistic expression have a : a classical guitarist might jam without any written music, a trained actor could ad-lib a line or two while rehearsing, and an oil painter can jot down a quick gesture drawing. What these casual modes have in common is that they allow an artist to express a complete thought quickly and intuitively without fear of making a mistake. This turns out to be essential to the  \u2014 when each sketch is nearly effortless, it is possible to iteratively explore the space of possibilities far more effectively.\n\nIn this post, we describe , an open source tool  at  that allows experts and amateurs alike to create rich, expressive, deformable 3D models from scratch \u2014 and to animate them \u2014 all in a casual mode, without ever having to leave the 2D plane. With Monster Mash, the user sketches out a character, and the software automatically converts it to a soft, deformable 3D model that the user can immediately animate by grabbing parts of it and moving them around in real time. There is also an , where you can try it out for yourself.\n\nThe insight that makes this casual sketching approach possible is that many , particularly those of organic forms, can be described by an ordered set of overlapping 2D regions. This abstraction makes the complex task of 3D modeling much easier: the user creates 2D regions by drawing their outlines, then the algorithm creates a 3D model by stitching the regions together and inflating them. The result is a simple and intuitive user interface for sketching 3D figures.\n\nFor example, suppose the user wants to create a 3D model of an elephant. The first step is to draw the body as a closed stroke (a). Then the user adds strokes to depict other body parts such as legs (b). Drawing those additional strokes as open curves provides a hint to the system that they are meant to be smoothly connected with the regions they overlap. The user can also specify that some new parts should go behind the existing ones by drawing them with the right mouse button (c), and mark other parts as symmetrical by double-clicking on them (d). The result is an ordered list of 2D regions.\n\n\nTo understand how a 3D model is created from these 2D regions, let\u2019s look more closely at one part of the elephant. First, the system identifies where the leg must be connected to the body (a) by finding the segment (red) that completes the open curve. The system cuts the body\u2019s front surface along that segment, and then stitches the front of the leg together with the body (b). It then inflates the model into 3D by solving a modified form of  to produce a surface with a rounded cross-section (c). The resulting model (d) is smooth and well-shaped, but because all of the 3D parts are rooted in the drawing plane, they may intersect each other, resulting in a somewhat odd-looking \u201celephant\u201d. These intersections will be resolved by the deformation system.\n\nAt this point we just have a static model \u2014 we need to give the user an easy way to pose the model, and also separate the intersecting parts somehow. Monster Mash\u2019s layered deformation system, based on the well-known smooth deformation method  (ARAP), solves both of these problems at once.What\u2019s novel about our  \u201cARAP-L\u201d approach is that it combines deformation and other constraints into a single optimization framework, allowing these processes to run in parallel at interactive speed, so that the user can manipulate the model in real time.\n\nThe framework incorporates a set of which move body parts along the axis to prevent them from visibly intersecting each other. These constraints are applied only at the silhouettes of overlapping parts, and are dynamically updated each frame.\nMeanwhile, in a separate thread of the framework, we satisfy  to make the model follow user-defined control points (described in the section below) in the -plane. This ARAP-L method allows us to combine modeling, rigging, deformation, and animation all into a single process that is much more approachable to the non-specialist user.\n\nTo pose the model, the user can create control points anywhere on the model\u2019s surface and move them. The deformation system converges over multiple frames, which gives the model\u2019s movement a soft and floppy quality, allowing the user to intuitively grasp its dynamic properties \u2014 an essential prerequisite for .\nTo create animation, the system records the user\u2019s movements in real time. The user can animate one control point, then play back that movement while recording additional control points. In this way, the user can build up a complex action like a walk by layering animation, one body part at a time. At every stage of the animation process, the only task required of the user is to move points around in 2D, a low-risk workflow meant to encourage experimentation and play. \n\n\nWe believe this new way of creating animation is intuitive and can thus help democratize the field of computer animation, encouraging novices who would normally be unable to try it on their own as well as experts who . Here you can see a few of the animated characters that have been created using Monster Mash. Most of these were created in a matter of minutes.\n\nAll of the code for Monster Mash is available as , and you can watch our  and read  from SIGGRAPH Asia 2020 to learn more. We hope this software will make creating 3D animations more broadly accessible. Try out the  and see for yourself!", "date": "\nFriday, April 9, 2021\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2021 Research Scholar Program Recipients\n", "author": ["Posted by Negar Saei, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2021/04/announcing-2021-research-scholar.html", "abstract": "In March 2020  the , an effort focused on developing collaborations with new professors and encouraging the formation of long-term relationships with the academic community. In November we opened the inaugural call for proposals for this program, which was received with enthusiastic interest from faculty who are working on cutting edge research across many research areas in computer science, including machine learning, human-computer interaction, health research, systems and more.\n\nToday we are pleased to announce that in this first year of the program we have granted 77 awards, which included 86 principal investigators representing 15+ countries and over 50 universities. Of the 86 award recipients, 43% identify as an . Please  see the full list of 2021 recipients on our , as well as in the list below.\n\nWe offer our congratulations to this year\u2019s recipients, and look forward to seeing what they achieve!", "date": "\nWednesday, April 7, 2021\n"},
{"website": "Google-AI", "title": "\nConstructing Transformers For Longer Sequences with Sparse Attention Methods\n", "author": ["Posted by Avinava Dubey, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html", "abstract": "(NLP) models based on , such as , , , or , are successful for a wide variety of tasks and a mainstay of modern NLP research. The versatility and robustness of Transformers are the primary drivers behind their wide-scale adoption, leading them to be easily adapted for a diverse range of sequence-based tasks \u2014 as a  model for , , , and others, or as a standalone encoder for , , , etc.  The key innovation in Transformers is the introduction of a self-attention mechanism, which computes similarity scores for all pairs of positions in an input sequence, and can be evaluated in parallel for each token of the input sequence, avoiding the sequential dependency of recurrent neural networks, and enabling Transformers to vastly outperform previous sequence models like .\n\nA limitation of existing Transformer models and their derivatives, however, is that the full  has computational and memory requirements that are quadratic with the input sequence length. With commonly available current hardware and model sizes, this typically limits the input sequence to roughly 512 tokens, and prevents Transformers from being directly applicable to tasks that require larger context, like ,  or . Two natural questions arise: 1) Can we achieve the empirical benefits of quadratic full Transformers using sparse models with computational and memory requirements that scale linearly with the input sequence length? 2) Is it possible to show theoretically that these linear Transformers preserve the expressivity and flexibility of the quadratic full Transformers?\n\nWe address both of these questions in a recent pair of papers. In \u201c\u201d, presented at , we present the Extended Transformer Construction (ETC), which is a novel method for sparse attention, in which one uses structural information to limit the number of computed pairs of similarity scores. This reduces the quadratic dependency on input length to linear and yields strong empirical results in the NLP domain. Then, in \u201c\u201d, presented at , we introduce another sparse attention method, called  BigBird that extends ETC to more generic scenarios where prerequisite domain knowledge about structure present in the source data may be unavailable. Moreover, we also show that theoretically our proposed sparse attention mechanism preserves the expressivity and flexibility of the quadratic full Transformers. Our proposed methods achieve a new state of the art on challenging long-sequence tasks, including ,  and .\n\n\nThe attention module used in Transformer models computes similarity scores for all pairs of positions in an input sequence. It is useful to think of the attention mechanism as a directed , with tokens represented by nodes and the similarity score computed between a pair of tokens represented by an edge. In this view, the full attention model is a . The core idea behind our approach is to carefully design sparse graphs, such that one only computes a linear number of similarity scores. \n\n\nOn NLP tasks that require long and structured inputs, we propose a structured sparse attention mechanism, which we call  (ETC). To achieve structured sparsification of self attention, we developed the mechanism. Here the input to the Transformer is split into two parts: a  where tokens have unrestricted attention, and a  where tokens can only attend to either the global input or to a local neighborhood. This achieves linear scaling of attention, which allows ETC to significantly scale input length.\n\nIn order to further exploit the structure of long documents, ETC combines additional ideas: representing the positional information of the tokens in a , rather than using their absolute position in the sequence; using an additional training objective beyond the usual ; and flexible masking of tokens to control which tokens can attend to which other tokens. For example, given a long selection of text, a global token is applied to each sentence, which connects to all tokens within the sentence, and a global token is also applied to each paragraph, which connects to all tokens within the same paragraph.\n\nWith this approach, we report state-of-the-art results in five challenging NLP datasets requiring long or structured inputs: , , , , and .\n\n\nExtending the work of ETC,  we propose  \u2014 a sparse attention mechanism that is also linear in the number of tokens and is a generic replacement for the attention mechanism used in Transformers. In contrast to ETC, BigBird doesn\u2019t require any prerequisite knowledge about structure present in the source data. Sparse attention in the BigBird model consists of three main parts:\n\nIn the , we explain why sparse attention is sufficient to approximate quadratic attention, partially explaining why ETC was successful. A crucial observation is that there is an inherent tension between how few similarity scores one computes and the flow of information between different nodes (i.e., the ability of one token to influence each other). Global tokens serve as a conduit for information flow and we prove that sparse attention mechanisms with global tokens can be as powerful as the full attention model. In particular, we show that BigBird is as expressive as the original Transformer, is  (following the work of  and ), and is a  of continuous functions. Furthermore, our proof suggests that the use of random graphs can further help ease the flow of information \u2014 motivating the use of the random attention component.\n\nThis design scales to much longer sequence lengths for both structured and unstructured tasks. Further scaling can be achieved by using  by trading off training time for sequence length. This lets us extend our efficient sparse transformers to include generative tasks that require an encoder and a decoder, such as long document summarization, on which we achieve a new state of the art.Moreover, the fact that BigBird is a generic replacement also allows it to be extended to new domains without pre-existing domain knowledge. In particular, we introduce a novel application of Transformer-based models where long contexts are beneficial \u2014 extracting contextual representations of genomic sequences (DNA). With longer masked language model pre-training, BigBird achieves state-of-the-art performance on downstream tasks, such as  and .\n\nOne of the main impediments to the large scale adoption of sparse attention is the fact that sparse operations are quite inefficient in modern hardware. Behind both ETC and BigBird, one of our key innovations is to make an efficient implementation of the sparse attention mechanism. As modern hardware accelerators like GPUs and TPUs excel using coalesced memory operations, which load blocks of contiguous bytes at once, it is not efficient to have small sporadic look-ups caused by a sliding window (for local attention) or random element queries (random attention). Instead we transform the sparse local and random attention into dense tensor operations to take full advantage of modern  (SIMD) hardware. \n\nTo do this, we first \u201cblockify\u201d the attention mechanism to better leverage GPUs/TPUs, which are designed to operate on blocks. Then we convert the sparse attention mechanism computation into a dense tensor product through a series of simple matrix operations such as reshape, roll, and gather, as illustrated in the animation below. \n\nRecently, \u201c\u201c provided a benchmark of six tasks that require longer context, and performed experiments to benchmark all existing long range transformers.  show that the BigBird model, unlike its counterparts, clearly reduces memory consumption without sacrificing performance.\n\n\nWe show that carefully designed sparse attention can be as expressive and flexible as the original full attention model. Along with theoretical guarantees, we provide a very efficient implementation which allows us to scale to much longer inputs. As a consequence, we achieve state-of-the-art results for question answering, document summarization and genome fragment classification. Given the generic nature of our sparse attention, the approach should be applicable to many other tasks like  and . We have open sourced the code for both  and , both of which run efficiently for long sequences on both GPUs and TPUs.", "date": "\nThursday, March 25, 2021\n"},
{"website": "Google-AI", "title": "\nRecursive Classification: Replacing Rewards with Examples in RL\n", "author": ["Posted by Benjamin Eysenbach, Student Researcher, Google Research"], "link": "http://ai.googleblog.com/2021/03/recursive-classification-replacing.html", "abstract": "A general goal of robotics research is to design systems that can assist in a variety of tasks that can potentially improve daily life. Most  algorithms for teaching agents to perform new tasks require a which provides positive feedback to the agent for taking actions that lead to good outcomes. However, actually specifying these reward functions can be quite tedious and can be very difficult to define for situations without a clear objective, such as whether a room is clean or if a door is sufficiently shut. Even for tasks that are easy to describe, actually measuring whether the task has been solved can be difficult and may require  to a robot's environment. \n\nAlternatively, training a model using examples, called , has the potential to overcome the limitations of approaches that rely on traditional reward functions. This new problem statement is most similar to   based on \"success detectors\", and efficient algorithms for example-based control could enable non-expert users to teach robots to perform new tasks, without the need for coding expertise, knowledge of reward function design, or the installation of environmental sensors.\n\nIn \",\" we propose a machine learning algorithm for teaching agents how to solve new tasks by providing examples of success (e.g., if \u201csuccess\u201d examples show a nail embedded into a wall, the agent  will learn to pick up a hammer and knock nails into the wall). This algorithm,  (RCE), does not rely on hand-crafted reward functions, distance functions, or features, but rather learns to solve tasks directly from data, requiring the agent to learn how to solve the entire task by itself, without requiring examples of any intermediate states. Using a version of  \u2014 similar to , but replacing the typical reward function term using only examples of success \u2014 RCE outperforms prior approaches based on imitation learning on simulated robotics tasks.  Coupled with theoretical guarantees similar to those for reward-based learning, the proposed method offers a user-friendly alternative for teaching robots new tasks.\n\n\nWhile the example-based control method is similar to , there is an important distinction \u2014 it does not require  . In fact, the user can actually be quite bad at performing the task themselves, as long as they can look back and pick out the small fraction of states where they did happen to solve the task. \n\nAdditionally, whereas   used a stage-wise approach in which the model first uses success examples to learn a reward function and then applies that reward function with an off-the-shelf reinforcement learning algorithm, RCE learns directly from the examples and skips the intermediate step of defining the reward function. Doing so avoids potential bugs and bypasses the process of defining the  (such as ) and, when debugging, removes the need to examine code related to learning the reward function.\n\n\nThe intuition behind the RCE approach is simple: the model should predict whether the agent will solve the task in the future, given the current state of the world and the action that the agent is taking. If there were data that specified which state-action pairs lead to future success and which state-action pairs lead to future failure, then one could solve this problem using standard . However, when the only data available consists of success examples, the system doesn\u2019t know which states and actions led to success, and while the system also has experience interacting with the environment, this experience isn't labeled as leading to success or not.\n\nNonetheless, one can piece together what these data would look like, if it were available. First, by definition, a successful example must be one that solves the given task. Second, even though it is unknown whether an arbitrary state-action pair will lead to success in solving a task, it is possible to estimate how likely it is that the task will be solved if the agent started at the next state. If the next state is likely to lead to future success, it can be assumed that the current state is also likely to lead to future success. In effect, this is , where the labels are inferred based on predictions at the next time step. \n\nThe underlying algorithmic idea of using a model's predictions at a future time step as a label for the current time step closely resembles existing temporal-difference methods, such as  and . The key difference is that the approach described here does not require a reward function. Nonetheless, we show that this method inherits many of the same theoretical   as temporal difference methods. In practice, implementing RCE requires changing only a few lines of code in an existing Q-learning implementation.\n\n\nWe evaluated the RCE method on a range of challenging robotic manipulation tasks. For example, in one task we required a robotic hand to pick up a hammer and hit a nail into a board. Previous research into this task [, ] have used a complex reward function (with terms corresponding to the distance between the hand and the hammer, the distance between the hammer and the nail, and whether the nail has been knocked into the board). In contrast, the RCE method requires only a few observations of what the world would look like if the nail were hammered into the board. \n\nWe compared the performance of RCE to a number of prior methods, including those that learn an explicit reward function and those based on imitation learning , all of which struggle to solve this task. This experiment highlights how example-based control makes it easy for users to specify even complex tasks, and demonstrates that recursive classification can successfully solve these sorts of tasks.\n\n\nWe have presented a method to teach autonomous agents to perform tasks by providing them with examples of success, rather than meticulously  or collecting . An important aspect of example-based control, which we discuss in the paper, is what assumptions the system makes about the capabilities of different users. Designing variants of RCE that are robust to differences in users' capabilities may be important for applications in real-world robotics. The , and the  contains additional videos of the learned behaviors.", "date": "\nWednesday, March 24, 2021\n"},
{"website": "Google-AI", "title": "\nProgress and Challenges in Long-Form Open-Domain Question Answering\n", "author": ["Posted by Aurko Roy, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/03/progress-and-challenges-in-long-form.html", "abstract": "(LFQA) is a fundamental challenge in natural language processing (NLP) that involves retrieving documents relevant to a given question and using them to generate an elaborate paragraph-length answer. While there has been remarkable recent progress in factoid open-domain question answering (QA), where a short phrase or entity is enough to answer a question, much less work has been done in the area of long-form question answering. LFQA is nevertheless an important task, especially because it provides a testbed to measure the factuality of generative text models. But, are current benchmarks and evaluation metrics really suitable for making progress on LFQA?\n\nIn \u201c\u201d (to appear at ), we present a new system for open-domain long-form question answering that leverages two recent advances in NLP: 1) state-of-the-art sparse attention models, such as  (RT), which allow attention-based models to scale to long sequences, and 2) retrieval-based models, such as , which facilitate retrievals of  articles related to a given query. To encourage more factual grounding, our system combines information from several retrieved Wikipedia articles related to the given question before generating an answer. It achieves a new state of the art on , the only large-scale publicly available dataset for long-form question answering.\n\nHowever, while our system tops the public leaderboard, we discover several troubling trends with the ELI5 dataset and its associated evaluation metrics. In particular, we find 1) little evidence that models actually use the retrievals on which they condition; 2) that trivial baselines (e.g., ) beat modern systems, like  / +; and 3) that there is a significant train/validation overlap in the dataset. Our paper suggests mitigation strategies for each of these issues.\n\n\nThe main workhorse of NLP models is the  architecture, in which each token in a sequence  to every other token in a sequence, resulting in a model that scales quadratically with sequence length. The RT model introduces a dynamic, content-based sparse attention mechanism that reduces the complexity of attention in the Transformer model from to where is the sequence length, which enables it to scale to long sequences. This allows each word to attend to other relevant words  in the entire piece of text, unlike methods such as  where a word can only attend to words in its immediate vicinity.\n\nThe key insight of the RT work is that each token attending to every other token is often redundant, and may be approximated by a combination of local and global attention. Local attention allows each token to build up a local representation over several layers of the model, where each token attends to a local neighborhood, facilitating local consistency and fluency. Complementing local attention, the RT model also uses mini-batch  to enable each token to attend only to a set of most relevanttokens.\nWe pre-train an RT model on the  data-set with a language modeling objective, i.e, the model learns to predict the next word given all the previous words, so as to be able to generate fluent paragraph long text.\n\n\nTo demonstrate the effectiveness of the RT model on the task of LFQA, we combine it with retrievals from REALM. The  model () is a retrieval-based model that uses the  to retrieve Wikipedia articles relevant to a particular query or question. The model was fine-tuned for factoid-based question answering on the  dataset. REALM utilizes the  to learn good representations for a question and uses  to retrieve Wikipedia articles that have a high topical similarity with the question representation. This is then trained end-to-end to maximize the log-likelihood on the QA task.  \n\nWe further improve the quality of REALM retrievals by using a . The idea behind this is to encourage the representation of a question to get close to its ground truth answer and diverge from the other answers in its mini-batch. This ensures that when the system retrieves relevant items using this question representation, it returns articles that are \"similar\" to ground truth answers. We call this retriever contrastive-REALM or c-REALM. \n\n\nWe test the model on long-form question answering using the , which is a part of the , and is the publicly available large-scaleLFQA dataset. The KILT benchmark measures text retrievals using  (R-Prec) and text generation using . The two scores are combined to give a KILT R-L score, which determines a model\u2019s ranking on the leaderboard. We fine-tune the pre-trained RT model together with retrievals from c-REALM on the ELI5 dataset from KILT.\n\nOur submission tops the KILT  for long-form question answering on ELI5 with a combined KILT R-L score of 2.36. It improves on the previous leaderboard entry of BART + DPR (KILT R-L score of 1.9), while having a similar number of parameters as the other models on the leaderboard. In terms of text generation quality, we see an improvement of +4.11, +5.78 and +9.14 Rouge-L over , BART + DPR and RAG, respectively. \n\n\n\n\n  \n\nHowever, while the RT system described here tops the public leaderboard, a detailed analysis of the model and the ELI5 dataset reveal some concerning trends.\n\n\n\nWe find little to no evidence that the model is actually grounding its text generation in the retrieved documents \u2014 fine-tuning an RT model with random retrievals from Wikipedia (i.e., random retrieval + RT) performs nearly as well as the c-REALM + RT model (24.2 vs 24.4 ROUGE-L). We also find significant overlap in the training, validation and test sets of ELI5 (with several questions being paraphrases of each other), which may eliminate the need for retrievals. The KILT benchmark measures the quality of retrievals and generations separately, without making sure that the text generation actually use the retrievals. \n\nMoreover, we find issues with the Rouge-L metric used to evaluate the quality of text generation, with trivial nonsensical baselines, such as a  and , achieving relatively high Rouge-L scores (even beating BART + DPR and RAG). \n\n\nWe proposed a system for long form-question answering based on Routing Transformers and REALM, which tops the KILT leaderboard on ELI5. However, a detailed analysis reveals several issues with the benchmark that preclude using it to inform meaningful modelling advances. We hope that the community works together to solve these issues so that researchers can climb the right hills and make meaningful progress in this challenging but important task.", "date": "\nTuesday, March 23, 2021\n"},
{"website": "Google-AI", "title": "\nLeveraging Machine Learning for Game Development\n", "author": ["Posted by Ji Hun Kim and Richard Wu, Software Engineers, Stadia"], "link": "http://ai.googleblog.com/2021/03/leveraging-machine-learning-for-game.html", "abstract": "Over the years, online multiplayer games have exploded in popularity, captivating millions of players across the world. This popularity has also exponentially increased demands on game designers, as players expect games to be well-crafted and balanced \u2014 after all, it's no fun to play a game where a single strategy beats all the rest.\n\nIn order to create a positive gameplay experience, game designers typically tune the balance of a game iteratively:\n\nThis process is not only time-consuming but also imperfect \u2014 the more complex the game, the easier it is for subtle flaws to slip through the cracks. When games often have many different roles that can be played, with dozens of interconnecting skills, it makes it all the more difficult to hit the right balance. \n\nToday, we present an approach that leverages machine learning (ML) to adjust  by training models to serve as play-testers, and demonstrate this approach on the digital card game prototype , which we\u2019ve previously shown as a testbed for . By running millions of simulations using trained agents to collect data, this ML-based game testing approach enables game designers to more efficiently make a game more fun, balanced, and aligned with their original vision.\n\n\nWe developed  as a game prototype that would heavily lean on machine learning during its development process. For the game itself, we purposefully designed the rules to expand the possibility space, making it difficult to build a traditional hand-crafted AI to play the game.\n\nThe gameplay of Chimera revolves around the titular , creature mash-ups that players aim to strengthen and evolve. The objective of the game is to defeat the opponent's chimera. These are the key points in the game design:\n\n\nAs an  card game with a large state space, we expected Chimera to be a difficult game for an ML model to learn, especially as we were aiming for a relatively simple model. We used an approach inspired by those used by earlier game-playing agents like , in which a  (CNN) is trained to predict the probability of a win when given an arbitrary game state. After training an initial model on games where random moves were chosen, we set the agent to play against itself, iteratively collecting game data, that was then used to train a new agent. With each iteration, the quality of the training data improved, as did the agent\u2019s ability to play the game.\n\nFor the actual game state representation that the model would receive as input, we found that passing an \"image\" encoding to the CNN resulted in the best performance, beating all benchmark procedural agents and other types of networks (e.g. fully connected). The chosen model architecture is small enough to run on a CPU in reasonable time, which allowed us to download the model weights and run the agent live in a Chimera game client using .\n\n\nThis approach enabled us to simulate millions more games than real players would be capable of playing in the same time span. After collecting data from the games played by the best-performing agents, we analyzed the results to find imbalances between the two of the player decks we had designed.\n\nFirst, the deck was composed of spells and creatures with abilities that generated extra link energy used to evolve a player\u2019s chimera. It also contained spells that enabled creatures to evade attacks. In contrast, the deck contained creatures of variable strength with spells that focused on healing and inflicting minor damage. Although we had designed these decks to be of equal strength, the  deck was winning 60% of the time when played against the  deck.\n\nWhen we collected various stats related to biomes, creatures, spells, and chimera evolutions, two things immediately jumped out at us:\n\nFrom these insights, we made some adjustments to the game. To emphasize chimera evolution as a core mechanism in the game, we decreased the amount of link energy required to evolve a chimera from 3 to 1. We also added a \u201ccool-off\u201d period to the T-Rex creature, doubling the time it took to recover from any of its actions. \n\nRepeating our \u2018self-play\u2019 training procedure with the updated rules, we observed that these changes pushed the game in the desired direction \u2014 the average number of evolves per game increased, and the T-Rex's dominance faded.\n\nBy weakening the T-Rex, we successfully reduced the  deck's reliance on an overpowered creature. Even so, the win ratio between the decks remained at 60/40 rather than 50/50. A closer look at the individual game logs revealed that the gameplay was often less strategic than we would have liked. Searching through our gathered data again, we found several more areas to introduce changes in.\n\nTo start, we increased the starting health of both players as well as the amount of health that healing spells could replenish. This was to encourage longer games that would allow a more diverse set of strategies to flourish. In particular, this enabled the  deck to survive long enough to take advantage of its healing strategy. To encourage proper summoning and strategic biome placement, we increased the existing penalties on playing creatures into incorrect or overcrowded biomes. And finally, we decreased the gap between the strongest and weakest creatures through minor attribute adjustments.\n\nNew adjustments in place, we arrived at the final game balance stats for these two decks:\n\n\nNormally, identifying imbalances in a newly prototyped game can take months of playtesting. With this approach, we were able to not only discover potential imbalances but also introduce tweaks to mitigate them in a span of days. We found that a relatively simple neural network was sufficient to reach high level performance against humans and traditional game AI. These agents could be leveraged in further ways, such as for coaching new players or discovering unexpected strategies. We hope this work will inspire more exploration in the possibilities of machine learning for game development.", "date": "\nFriday, March 19, 2021\n"},
{"website": "Google-AI", "title": "\nMassively Parallel Graph Computation: From Theory to Practice\n", "author": ["Posted by Jakub \u0141\u0105cki and Vahab Mirrokni, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2021/03/massively-parallel-graph-computation.html", "abstract": "are useful theoretical representations of the connections between groups of entities, and have been used for a variety of purposes in data science, from  by popularity and , to . In many cases, such applications require the processing of graphs containing , which is far too large to be processed on a single consumer-grade machine. A typical approach to scaling graph algorithms is to run in a  setting, i.e., to partition the data (and the algorithm) among multiple computers to perform the computation in parallel. While this approach allows one to process graphs with trillions of edges, it also introduces new challenges. Namely, because each computer only sees a small piece of the input graph at a time, one needs to handle inter-machine communication and design algorithms that can be split across multiple computers.\n\nA framework for implementing distributed algorithms, , was introduced in 2008. It transparently handled communication between machines while offering good fault-tolerance capabilities and inspired the development of a number of distributed computation frameworks, including , , and many others. Still, the challenge of developing algorithms for distributed computation on very large graphs remained, and designing efficient algorithms in this context even for basic problems, such as ,  or , has been an active area of research. While recent work has demonstrated new algorithms for many problems, including our algorithms for connected components (both in  and ) and , there was still a need for methods that could solve a range of problems more quickly. \n\nToday we present a pair of recent papers that address this problem by first constructing a theoretical model for distributed graph algorithms and then demonstrating how the model can be applied. The proposed model,  (AMPC), augments the theoretical capabilities of MapReduce, providing a pathway to solve many graph problems in fewer computation rounds. We also show how the AMPC model can be . The suite of algorithms we describe, which includes algorithms for , ,  and  work up to 7x faster than current state-of-the-art approaches. \n\n\nIn order to understand the limitations of MapReduce for developing graph algorithms, consider a simplified variant of the . The input is a collection of rooted trees, and the goal is to compute, for each node, the root of its tree. Even this seemingly simple problem is not easy to solve in MapReduce. In fact, in the  (MPC) model \u2014 the theoretical model behind MapReduce, Pregel,  and many other distributed computation frameworks \u2014 this problem is  to require at least a number of rounds of computation proportional to , where  is the total number of nodes in the graph. While  may not seem to be a large number, algorithms processing trillion-edge graphs often write hundreds of terabytes of data to disk in each round, and thus even a small reduction in the number of rounds may bring significant resource savings.\n\nA similar subproblem showed up in our algorithms for finding  and computing a . We observed that one can bypass the limitations of MapReduce by implementing these algorithms through the use of a  (DHT), a service that is initialized with a collection of key-value pairs and then returns a value associated with a provided key in real-time. In our implementation, for each node, the DHT stores its parent node. Then, a machine that processes a graph node can use the DHT and \u201cwalk up\u201d the tree until it reaches the root. While the use of a DHT worked well for this particular problem (although it relied on the input trees being not too deep), it was unclear if the idea could be applied more broadly.\n\n\nTo extend this approach to other problems, we started by developing a model to theoretically analyze algorithms that utilize a DHT. The resulting AMPC model builds upon the well-established MPC model and formally describes the capabilities brought by the use of a distributed hash table. \n\nIn the MPC model there is a collection of machines, which communicate via message passing in synchronous rounds. Messages sent in one round are delivered in the beginning of the following round and constitute that round\u2019s entire input (i.e., the machines do not retain information from one round to the next). In the first round, one can assume that the input is randomly distributed across the machines. The goal is to minimize the number of computation rounds, while assuring load-balancing between machines in each round.\n\nWe then formalized the AMPC model by introducing a new approach, in which machines write to a write-only distributed hash table each round, instead of communicating via messages. Once a new round starts, the hash table from the previous round becomes read-only and a new write-only output hash table becomes available. What is important is that only the  changes \u2014 the amount of communication and available space per machine is constrained exactly in the same way as in the MPC model. Hence, at a high level the added capability of the AMPC model is that each machine can  what data to read, instead of being provided a piece of data. \n\n\nThis seemingly small difference in the way machines communicate allowed us to design much faster algorithms to a number of basic graph problems. In particular, we show that it is possible to find  and  in a constant number of rounds, regardless of the size of the graph. \n\nTo investigate the practical applicability of the AMPC algorithms, we have instantiated the model by combining Flume C++ (a C++ counterpart of ) with a DHT communication layer. We have  for minimum spanning tree, maximal independent set and maximum matching and observed that we can achieve up to 7x speedups over implementations that did not use a DHT. At the same time, the AMPC implementations used 10x fewer rounds on averageto complete, and also wrote less data to disk.\n\nOur implementation of the AMPC model took advantage of hardware-accelerated  (RDMA), a technology that allows reading from the memory of a remote machine with a latency of a few , which is just an order of magnitude slower than reading from local memory.While some of the AMPC algorithms communicated more data than their MPC counterparts, they were overall faster, as they performed mostly fast reads using RDMA, instead of costly writes to disk.\n\n\nWith the AMPC model, we built a theoretical framework inspired by practically efficient implementations, and then developed new theoretical algorithms that delivered good empirical performance and maintained good fault-tolerance properties. We've been happy to see that the AMPC model has already been  and are excited to learn what other problems can be solved more efficiently using the AMPC model or its practical implementations.", "date": "\nThursday, March 18, 2021\n"},
{"website": "Google-AI", "title": "\nContactless Sleep Sensing in Nest Hub\n", "author": ["Posted by Michael Dixon, Software Engineer and Reena Singhal Lee, Product Manager, Google Health"], "link": "http://ai.googleblog.com/2021/03/contactless-sleep-sensing-in-nest-hub.html", "abstract": "People often turn to technology to manage their health and wellbeing, whether it is to record their daily exercise,  or increasingly, to understand their sleep patterns. Sleep is foundational to a person\u2019s everyday wellbeing and can be impacted by (and in turn, have an impact on) other aspects of one\u2019s life \u2014 mood, energy, diet, productivity, and more. \n\nAs part of our ongoing efforts to support people\u2019s health and happiness, today we announced  in the new Nest Hub, which uses radar-based sleep tracking in addition to an algorithm for cough and snore detection. While not intended for medical purposes, Sleep Sensing is an opt-in feature that can help users better understand their nighttime wellness using a contactless bedside setup. Here we describe the technologies behind Sleep Sensing and discuss how we leverage on-device signal processing to enable sleep monitoring (comparable to other clinical- and consumer-grade devices) in a way that protects user privacy.\n\n\nSleep Sensing in Nest Hub demonstrates the first wellness application of , a miniature radar sensor that can be used for gesture sensing at various scales, from a finger tap to movements of a person\u2019s body. In Pixel 4, Soli powers ,  with the phone to skip songs, snooze alarms, and silence phone calls. We extended this technology and developed an embedded Soli-based algorithm that could be implemented in Nest Hub for sleep tracking.   \n\n consists of a millimeter-wave  (FMCW)  transceiver that emits an ultra-low power radio wave and measures the reflected signal from the scene of interest. The frequency spectrum of the reflected signal contains an aggregate representation of the distance and velocity of objects within the scene. This signal can be processed to isolate a specified range of interest, such as a user\u2019s sleeping area, and to detect and characterize a wide range of motions within this region, ranging from large body movements to sub-centimeter respiration.  \n\nIn order to make use of this signal for Sleep Sensing, it was necessary to design an algorithm that could determine whether a person is present in the specified sleeping area and, if so, whether the person is asleep or awake. We designed a custom machine-learning (ML) model to efficiently process a continuous stream of 3D radar tensors (summarizing activity over a range of distances, frequencies, and time) and automatically classify each feature into one of three possible states: absent, awake, and asleep.\nTo train and evaluate the model, we recorded more than a million hours of radar data from thousands of individuals, along with thousands of sleep diaries, reference sensor recordings, and external annotations. We then leveraged the  framework to construct a training pipeline to process this data and produce an efficient  embedded model. In addition, we created an automatic calibration algorithm that runs during setup to configure the part of the scene on which the classifier will focus. This ensures that the algorithm ignores motion from a person on the other side of the bed or from other areas of the room, such as ceiling fans and swaying curtains. \n\n  To validate the accuracy of the algorithm, we compared it to the gold-standard of sleep-wake determination, the , in a cohort of 33 \u201chealthy sleepers\u201d (those without significant sleep issues, like  or ) across a broad age range (19-78 years of age). Sleep studies are typically conducted in clinical and research laboratories in order to collect various body signals (brain waves, muscle activity, respiratory and heart rate measurements, body movement and position, and snoring), which can then be interpreted by trained sleep experts to determine stages of sleep and identify relevant events. To account for variability in how different scorers apply the  staging and scoring rules, our study used two board-certified sleep technologists to independently annotate each night of sleep and establish a definitive groundtruth.\nWe compared our Sleep Sensing algorithm\u2019s outputs to the corresponding groundtruth sleep and wake labels for every 30-second epoch of time to compute standard performance metrics (e.g., ). While not a true head-to-head comparison, this study\u2019s results can be compared against previously published studies in similar cohorts with comparable methodologies in order to get a rough estimate of performance. In \u201c\u201d, we share the full details of these validation results, demonstrating sleep-wake estimation equivalent to or, in some cases, better than current clinical and consumer sleep tracking devices. \n\n  \nThe Soli-based sleep tracking algorithm described above gives users a convenient and reliable way to see how much sleep they are getting and  sleep disruptions occur. However, to understand and improve their sleep, users also need to understand  their sleep is disrupted. To assist with this, Nest Hub uses its array of sensors to track common sleep disturbances, such as light level changes or uncomfortable room temperature. In addition to these, respiratory events like coughing and snoring are also frequent sources of disturbance, but people are often unaware of these events. \n\nAs with other audio-processing applications like speech or music recognition, coughing and snoring exhibit distinctive temporal patterns in the audio frequency spectrum, and with sufficient data an ML model can be trained to reliably recognize these patterns while simultaneously ignoring a wide variety of background noises, from a humming fan to passing cars. The model uses entirely on-device audio processing with privacy-preserving analysis, with no raw audio data sent to Google\u2019s servers. A user can then opt to save the outputs of the processing (sound occurrences, such as the number of coughs and snore minutes) in Google Fit, in order to view personal insights and summaries of their night time wellness over time.\n\n  To train the model, we assembled a large, hand-labeled dataset, drawing examples from the publicly available  research dataset as well as hundreds of thousands of additional real-world audio clips contributed by thousands of individuals.\n\n  When a user opts in to cough and snore tracking on their bedside Nest Hub, the device first uses its Soli-based sleep algorithms to detect when a user goes to bed. Once it detects that a user has fallen asleep, it then activates its on-device sound sensing model and begins processing audio. The model works by continuously extracting spectrogram-like features from the audio input and feeding them through a  classifier in order to estimate the probability that coughing or snoring is happening at a given instant in time. These estimates are analyzed over the course of the night to produce a report of the overall cough count and snoring duration and highlight exactly when these events occurred.\n\nThe new Nest Hub, with its underlying Sleep Sensing features, is a first step in empowering users to understand their nighttime wellness using privacy-preserving radar and audio signals. We continue to research additional ways that ambient sensing and the predictive ability of consumer devices could help people better understand their daily health and wellness in a privacy-preserving way.", "date": "\nTuesday, March 16, 2021\n"},
{"website": "Google-AI", "title": "\nLEAF: A Learnable Frontend for Audio Classification\n", "author": ["Posted by Neil Zeghidour, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/03/leaf-learnable-frontend-for-audio.html", "abstract": "Developing machine learning (ML) models for audio understanding has seen tremendous progress over the past several years. Leveraging the ability to learn parameters from data, the field has progressively shifted from composite, handcrafted systems to today\u2019s deep neural classifiers that are used to , , or . However, unlike  models, which can learn from raw pixels, deep neural networks for audio classification are rarely trained from raw audio waveforms. Instead, they rely on pre-processed data in the form of mel filterbanks \u2014 handcrafted   that have been designed to replicate some aspects of the human auditory response.\n\nAlthough modeling mel filterbanks for ML tasks has been historically successful, it is limited by the inherent biases of : even though using a fixed mel-scale and a logarithmic compression works well in general, we have no guarantee that they provide the best representations for the task at hand. In particular, even though matching human perception provides good  for some application domains, e.g., speech recognition or music understanding, these biases may be detrimental to domains for which imitating the human ear is not important, such as . So, in order to achieve optimal performance, the mel filterbanks should be tailored to the task of interest, a tedious process that requires an iterative effort informed by expert domain knowledge. As a consequence, standard mel filterbanks are used for most audio classification tasks in practice, even though they are suboptimal. In addition, while researchers have proposed ML systems to address these problems, such as ,  and , they have yet to match the performance of traditional mel filterbanks.\n\nIn \u201c\u201d, accepted at , we present an alternative method for crafting learnable spectrograms for audio understanding tasks. LEarnable Audio Frontend (LEAF) is a neural network that can be initialized to approximate mel filterbanks, and then be trained jointly with any audio classifier to adapt to the task at hand, while only adding a handful of parameters to the full model. We show that over a wide range of audio signals and classification tasks, including speech, music and bird songs, LEAF spectrograms improve classification performance over fixed mel filterbanks and over previously proposed learnable systems. We have implemented the code in  and released it to the community through . \n\n\nThe first step in the traditional approach to creating a mel filterbank is to capture the sound\u2019s time-variability by windowing, i.e., cutting the signal into short segments with fixed duration. Then, one performs filtering, by passing the windowed segments through a bank of fixed frequency filters, that replicate the human logarithmic sensitivity to pitch. Because we are more sensitive to variations in low frequencies than high frequencies, mel filterbanks give more importance to the low-frequency range of sounds. Finally, the audio signal is compressed to mimic the ear\u2019s logarithmic sensitivity to loudness \u2014 a sound needs to double its power for a person to perceive an increase of 3 .\n\nLEAF loosely follows this traditional approach to mel filterbank generation, but replaces each of the fixed operations (i.e., the filtering layer, windowing layer, and  compression function) by a learned counterpart. The output of LEAF is a time-frequency representation (a spectrogram) similar to mel filterbanks, but fully learnable. So, for example, while a mel filterbank uses a fixed scale for pitch, LEAF learns the scale that is best suited to the task of interest. Any model that can be trained using mel filterbanks as input features, can also be trained on LEAF spectrograms.\n\nWhile LEAF can be initialized randomly, it can also be initialized in a way that approximates mel filterbanks, which have been shown to be a better starting point. Then, LEAF can be trained with any classifier to adapt to the task of interest.\n\n\nA potential downside of replacing fixed features that involve no learnable parameter with a trainable system is that it can significantly increase the number of parameters to optimize. To avoid this issue, LEAF uses  convolution layers that have only two parameters per filter, instead of the ~400 parameters typical of a standard convolution layer. This way, even when paired with a small classifier, such as , the LEAF model only accounts for 0.01% of the total parameters.\n\n\nWe apply LEAF to diverse audio classification tasks, including recognizing , , , , and . On average, LEAF outperforms both mel filterbanks and previous learnable frontends, such as ,  and . In particular, LEAF achieves a 76.9% average accuracy across the different tasks, compared to 73.9% for mel filterbanks. Moreover we show that LEAF can be trained in a multi-task setting, such that a single LEAF parametrization can work well across all these tasks. Finally, when combined with a large audio classifier, LEAF reaches state-of-the-art performance on the challenging , with a 2.74  score.\n\n\nThe scope of audio understanding tasks keeps growing, from  to . Adapting mel filterbanks to every new task can require a significant amount of hand-tuning and experimentation. In this context, LEAF provides a drop-in replacement for these fixed features, that can be trained to adapt to the task of interest, with minimal task-specific adjustments. Thus, we believe that LEAF can accelerate development of models for new audio understanding tasks.", "date": "\nFriday, March 12, 2021\n"},
{"website": "Google-AI", "title": "\nA New Lens on Understanding Generalization in Deep Learning\n", "author": ["Hanie Sedghi, Google Research and Preetum Nakkiran, Harvard University"], "link": "http://ai.googleblog.com/2021/03/a-new-lens-on-understanding.html", "abstract": "Understanding generalization is one of the fundamental unsolved problems in deep learning. Why does optimizing a model on a  lead to good performance on a ? This problem has been  in machine learning, with a  going back more than 50 years. There are now many   that help researchers understand generalization in certain models. Unfortunately, most of these existing theories fail when applied to modern deep networks \u2014 they are both  and  in realistic settings.  This gap between theory and practice is largest for  models, which in theory have the capacity to overfit their train sets, but often do not in practice.\n\nIn \u201c\u201d, accepted at , we present a new framework for approaching this problem by connecting generalization to the field of. In a typical setting, a model trains on a  set of samples, which are reused for multiple epochs. But in online optimization, the model has access to an  stream of samples, and can be iteratively updated while processing this stream. In this work, we find that models that train quickly on infinite data are the same models that generalize well if they are instead trained on finite data. This connection brings new perspectives on design choices in practice, and lays a roadmap for understanding generalization from a theoretical perspective.\n\n\nThe main idea of the Deep Bootstrap framework is to compare the real world, where there is finite training data, to an \"ideal world\", where there is infinite data. We define these as:\n\n, one might expect the real and ideal worlds may have nothing to do with each other, since in the real world the model sees a finite number of examples from the distribution while in the ideal world the model sees the whole distribution. But in practice, we found that the real and ideal models actually have similar test error.\n\nIn order to quantify this observation, we simulated an ideal world setting by creating a new dataset, which we call . We trained a  on , which we then used to generate ~6 million images. The scale of the dataset was chosen to ensure that it is \u201cvirtually infinite\u201d from the model\u2019s perspective, so that the model never resamples the same data. That is, in the ideal world, the model sees an entirely fresh set of samples. \n\nThe figure below presents the test error of several models, comparing their performance when trained on CIFAR-5m data in the real world setting (i.e., re-used data) and the ideal world (\u201cfresh\u201d data). The solid blue line shows a  model in the real world, trained on 50K samples for 100 epochs with standard CIFAR-10 hyperparameters. The dashed blue line shows the corresponding model in the ideal world, trained on 5 million samples in a single pass. Surprisingly, these worlds have very similar test error \u2014 the model in some sense \"doesn't care\" whether it sees re-used samples or fresh ones.\n\nThis also holds for other architectures, e.g., a  (red), a  (green), and  of architecture, optimizer, data distribution, and sample size. These experiments suggest a new perspective on generalization: models that optimize quickly (on infinite data), generalize well (on finite data). For example, the ResNet model generalizes better than the MLP model on finite data, but this is \"because\" it optimizes faster even on infinite data. \n\n\nThe key observation is that real world and ideal world models remain close, in test error, for all timesteps, until the real world converges (< 1% train error). Thus, one can study models in the real world by studying their corresponding behavior in the ideal world. \n\nThis means that the generalization of the model can be understood in terms of its optimization performance under two frameworks:\n\nThus, to study generalization, we can equivalently study the two terms above, which can be conceptually simpler, since they only involve optimization concerns. Based on this observation, good models and training procedures are those that (1) optimize quickly in the ideal world and (2) do not optimize too quickly in the real world.\n\nAll design choices in deep learning can be viewed through their effect on these two terms. For example, some advances like , , and - help primarily by  ideal world optimization, while other advances like  and  help primarily by  real world optimization. \n\n\nResearchers can use the Deep Bootstrap framework to study and guide design choices in deep learning. The principle is: whenever one makes a change that affects generalization in the real world (the architecture, learning-rate, etc.), one should consider its effect on (1) the ideal world optimization of test error (faster is better) and (2) the real world optimization of train error (slower is better).\n\nFor example,  is often used in practice to help generalization of models in small-data regimes. However, the reason that pre-training helps remains poorly understood. One can study this using the Deep Bootstrap framework by looking at the effect of pre-training on terms (1) and (2) above. We find that the primary effect of pre-training is to improve the ideal world optimization (1) \u2014 pre-training turns the network into a \"fast learner\" for online optimization. The  of pretrained models is thus almost exactly captured by their  in the ideal world. The figure below shows this for  (ViT) trained on , comparing training from scratch vs. pre-training on .\n\nOne can also study  using this framework. Data-augmentation in the ideal world corresponds to augmenting each fresh sample once, as opposed to augmenting the same sample multiple times. This framework implies that good data-augmentations are those that (1) do not significantly harm ideal world optimization (i.e., augmented samples don't look too \"out of distribution\") or (2) inhibit real world optimization speed (so the real world takes longer to fit its train set).\n\nThe main benefit of data-augmentation is through the second term, prolonging the real world optimization time. As for the first term, some aggressive data augmentations (/) can actually harm the ideal world, but this effect is dwarfed by the second term.\n\n\nThe Deep Bootstrap framework provides a new lens on generalization and empirical phenomena in deep learning. We are excited to see it applied to understand other aspects of deep learning in the future. It is especially interesting that generalization can be characterized via purely  considerations, which is in contrast to many  in theory. Crucially, we consider both online and offline optimization, which are individually insufficient, but that together determine generalization. \n\nThe Deep Bootstrap framework can also shed light on why deep learning is fairly robust to many design choices: many kinds of , , , , and   can generalize well. This framework suggests a unifying principle: that essentially any choice that works well in the online optimization setting will also generalize well in the offline setting. \n\nFinally, modern neural networks can be either overparameterized (e.g., large networks trained on   ) or underparmeterized (e.g., , , or ). The Deep Bootstrap framework implies that online optimization is a crucial factor to success in both regimes.", "date": "\nWednesday, March 10, 2021\n"},
{"website": "Google-AI", "title": "\nAccelerating Neural Networks on Mobile and Web with Sparse Inference\n", "author": ["Posted by Artsiom Ablavatski and Marat Dukhan, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html", "abstract": "On-device inference of neural networks enables a variety of real-time applications, like  and , in a low-latency and privacy-conscious way. Using ML inference frameworks like  with  ML acceleration library, engineers optimize their models to run on a variety of devices by finding a sweet spot between model size, inference speed and the quality of the predictions. \n\nOne way to optimize a model is through use of sparse neural networks [, , ], which have a significant fraction of their weights set to zero. In general, this is a desirable quality as it not only reduces the model size via compression, but also makes it possible to skip a significant fraction of multiply-add operations, thereby speeding up inference. Further, it is possible to increase the number of parameters in a model and then sparsify it to match the quality of the original model, while still benefiting from the accelerated inference. However, the use of this technique remains limited in production largely due to a lack of tools to sparsify popular convolutional architectures as well as insufficient support for running these operations on-device. \n\nToday we announce the release of a set of new features for the  and TensorFlow Lite that enable efficient inference of sparse networks, along with guidelines on how to sparsify neural networks, with the goal of helping researchers develop their own sparse on-device models. Developed in collaboration with , these tools power a new generation of live perception experiences, including  in  and  in Google Meet, accelerating inference speed from 1.2 to 2.4 times, while reducing the model size by half. In this post, we provide a technical overview of sparse neural networks \u2014 from inducing sparsity during training to on-device deployment \u2014 and offer some ideas on how researchers might create their own sparse models.\n\nMany modern deep learning architectures, like  and , are primarily composed of  with a small spatial kernel and  that linearly combine features from the input image. While such architectures have a number of potential targets for sparsification, including the full  that frequently occur at the beginning of many networks or the , it is the 1x1 convolutions that are the most expensive operators as measured by inference time. Because they account for over 65% of the total compute, they are an optimal target for sparsification. \n\nIn modern on-device inference engines, like XNNPACK, the implementation of 1x1 convolutions as well as other operations in the deep learning models , in which the tensor dimensions correspond to the height, width, and channel (e.g., red, green or blue) of the input image. This tensor configuration allows the inference engine to process the channels corresponding to each spatial location (i.e., each pixel of an image) in parallel. However, this ordering of the tensor is not a good fit for sparse inference because it sets the channel as the innermost dimension of the tensor and makes it more computationally expensive to access. \n\nOur updates to XNNPACK enable it to detect if a model is sparse. If so, it switches from its standard dense inference mode to  mode, in which it employs a CHW (channel, height, width) tensor layout. This reordering of the tensor allows for an accelerated implementation of the sparse 1x1 convolution kernel for two reasons: 1) entire spatial slices of the tensor can be skipped when the corresponding channel weight is zero following a single condition check, instead of a per-pixel test; and 2) when the channel weight is non-zero, the computation can be made more efficient by loading neighbouring pixels into the same memory unit. This enables us to process multiple pixels simultaneously, while also performing each operation in parallel across several threads. Together these changes result in a speed-up of 1.8x to 2.3x when at least 80% of the weights are zero.\n\nIn order to avoid converting back and forth between the CHW tensor layout that is optimal for sparse inference and the standard HWC tensor layout after each operation, XNNPACK provides efficient implementations of  in CHW layout.\n\n\nTo create a sparse neural network, the guidelines included in this release suggest one start with a dense version and then gradually set a fraction of its weights to zero during training. This process is called pruning. Of the many available techniques for pruning, we recommend using  (available in the ) or the recently introduced  method. With a modest increase in training time, both of these can successfully sparsify deep learning models without degrading their quality. The resulting sparse models can be  that reduces the size by a factor of two compared to their dense equivalent. \n\nThe quality of sparse networks is influenced by several hyperparameters, including training time, learning rate and schedules for pruning. The  provides an excellent example of how to select these, as well as some tips for training such models. We recommend running hyperparameter searches to find the sweet spot for your application.\n\n\nWe demonstrate that it is possible to sparsify classification tasks,  dense segmentation (e.g., ) and regression problems (), which provides tangible benefits to users. For example, in the case of Google Meet, sparsification lowered the inference time of the model by 30%, which provided access to higher quality models for more users.\n\nThe approach to sparsity described here works best with architectures based on , such as ,  and . The degree of sparsity in a network influences both inference speed and quality. Starting from a dense network of a fixed capacity, we found modest performance gains even at 30% sparsity. With increased sparsity, the quality of the model remains relatively close to the dense baseline until reaching 70% sparsity, beyond which there is a more pronounced drop in accuracy. However, one can compensate for the reduced accuracy at 70% sparsity by increasing the size of the base network by 20%, which results in faster inference times without degrading the quality of the model. No further changes are required to run the sparsified models, because XNNPACK can recognize and automatically enable sparse inference.\n\n\nBackground blur in Google Meet uses a segmentation model based on a modified MobileNetV3 backbone with . We were able to speed up the model by 30% by applying a 70% sparsification, while preserving the quality of the foreground mask.  We examined the predictions of the sparse and dense models on images from 17 geographic subregions, finding no significant difference, and released the details in the associated . \n\nSimilarly,  predicts hand landmarks in real-time on mobile and the web using a model based on the EfficientNetLite backbone. This backbone model was  from the large dense model, which is a computationally expensive, iterative process. Using the sparse version of the dense model instead of distilled one, we were able to maintain the same inference speed but without the labor intensive process of distilling from a dense model. Compared with the dense model the sparse model improved the inference by a factor of two, achieving the identical landmark quality as the distilled model. In a sense, sparsification can be thought of as an automatic approach to unstructured model distillation, which can improve model performance without extensive manual effort. We evaluated the sparse model on the geodiverse dataset and made the model card publicly .\n\n\nWe find sparsification to be a simple yet powerful technique for improving CPU inference of neural networks. Sparse inference allows engineers to run larger models without incurring a significant performance or size overhead and offers a promising new direction for research. We are continuing to extend XNNPACK with wider support for operations in CHW layout and are exploring how it might be combined with other optimization techniques like quantization. We are excited to see what you might build with this technology!", "date": "\nTuesday, March 9, 2021\n"},
{"website": "Google-AI", "title": "\nPAIRED: A New Multi-agent Approach for Adversarial Environment Generation\n", "author": ["Posted by Natasha Jaques, Google Research and Michael Dennis, UC Berkeley"], "link": "http://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html", "abstract": "The effectiveness of any machine learning method is critically dependent on its training data. In the case of  (RL), one can rely either on limited data collected by an agent , or a simulated training environment that can be used to collect as much data as needed. This latter method of training in simulation is increasingly popular, but it has a problem \u2014 the RL agent can learn what is built into the simulator, but tends to be    to tasks that are even slightly different than the ones simulated. And obviously building a simulator that covers all the complexity of the real-world is extremely challenging.\n\nAn approach to address this is to automatically create more diverse training environments by  all the parameters of the simulator, a process called  (DR). However, DR can fail even in very simple environments. For example, in the animation below, the blue agent is trying to navigate to the green goal. The left panel shows an environment created with DR where the positions of the obstacles and goal have been randomized. Many of these DR environments were used to train the agent, which was then transferred to the simple Four Rooms environment in the middle panel. Notice that the agent can\u2019t find the goal. This is because it has not learned to walk around walls. Even though the wall configuration from the Four Rooms example  have been generated randomly in the DR training phase, it\u2019s unlikely. As a result, the agent has not spent enough time training on walls similar to the Four Rooms structure, and is unable to reach the goal.\n\nInstead of just randomizing the environment parameters, one could train a second RL agent to  how to set the environment parameters. This   can be trained to minimize the performance of the first RL agent by finding and exploiting weaknesses in its policy, e.g. building wall configurations it has not encountered before. But again there is a problem. The right panel shows an environment built by a minimax adversary in which it is actually impossible for the agent to reach the goal. While the minimax adversary has succeeded in its task \u2014 it has minimized the performance of the original agent \u2014 it provides no opportunity for the agent to learn. Using a purely adversarial objective is not well suited to generating training environments, either. \n\nIn collaboration with , we propose a new multi-agent approach for training the adversary in  \u201c\u201d, a publication recently presented at . In this work we present an algorithm, Protagonist Antagonist Induced Regret Environment Design(PAIRED), that is based on  and prevents the adversary from creating impossible environments, while still enabling it to correct weaknesses in the agent\u2019s policy. PAIRED incentivizes the adversary to tune the difficulty of the generated environments to be just outside the agent\u2019s current abilities, leading to an of increasingly challenging training tasks. We show that agents trained with PAIRED learn more complex behavior and generalize better to unknown test tasks. We have released open-source code for PAIRED on our .\n\n\nTo flexibly constrain the adversary, PAIRED introduces a  RL agent, which we call the  agent, because it is allied with the adversarial agent, i.e., the one designing the environment. We rename our initial agent, the one navigating the environment, the . Once the adversary generates an environment, both the protagonist and antagonist play through that environment.\n\n  The adversary\u2019s job is to maximize the antagonist\u2019s reward while minimizing the protagonist's reward. This means it must create environments that are  (because the antagonist can solve them and get a high score), but  to the protagonist (exploit weaknesses in its current policy). The gap between the two rewards is the \u00a0\u2014 the adversary tries to maximize the regret, while the protagonist competes to minimize it. \nThe methods discussed above (domain randomization, minimax regret and PAIRED) can be analyzed using the same theoretical framework, unsupervised environment design (UED), which we describe in detail in the paper. UED draws a connection between environment design and , enabling us to show that domain randomization is equivalent to the , the minimax adversary follows the , and PAIRED is optimizing . This formalism enables us to use tools from decision theory to understand the benefits and drawbacks of each method. Below, we show how each of these ideas works for environment design:\n\n\nWhat\u2019s interesting about minimax regret is that it incentivizes the adversary to generate a curriculum of initially easy, then increasingly challenging environments. In most RL environments, the reward function will give a higher score for completing the task more efficiently, or in fewer timesteps. When this is true, we can show that regret incentivizes the adversary to create . To see this, let\u2019s assume the antagonist is perfect, and always gets the highest score that it possibly can. Meanwhile, the protagonist is terrible, and gets a score of zero on everything. In that case, the regret just depends on the difficulty of the environment. Since easier environments can be completed in fewer timesteps, they allow the antagonist to get a higher score. Therefore, the regret of failing at an easy environment is greater than the regret of failing on a hard environment:\n\n  So, by maximizing regret the adversary is searching for easy environments that the protagonist fails to do. Once the protagonist learns to solve each environment, the adversary must move on to finding a slightly harder environment that the protagonist can\u2019t solve. Thus, the adversary generates a curriculum of increasingly difficult tasks. \n\n\nWe can see the curriculum emerging in the learning curves below, which plot the shortest path length of a maze the agents have successfully solved. Unlike minimax or domain randomization, the PAIRED adversary creates a curriculum of increasingly longer, but possible, mazes, enabling PAIRED agents to learn more complex behavior. \n\n  But can these different training schemes help an agent generalize better to unknown test tasks? Below, we see the zero-shot transfer performance of each algorithm on a series of challenging test tasks. As the complexity of the transfer environment increases, the performance gap between PAIRED and the baselines widens. For extremely difficult tasks like Labyrinth and Maze, PAIRED is the only method that can occasionally solve the task. These results provide promising evidence that PAIRED can be used to improve generalization for deep RL.\n\n  Admittedly, these simple gridworlds do not reflect the complexities of the real world tasks that many RL methods are attempting to solve. We address this in \u201c\u201d, which examines the performance of PAIRED when applied to more complex problems, such as teaching RL agents to navigate web pages. We propose an improved version of PAIRED, and show how it can be used to train an adversary to generate a curriculum of increasingly challenging websites:\n\n  Above, you can see websites built by the adversary in the early, middle, and late training stages, which progress from using very few elements per page to many simultaneous elements, making the tasks progressively harder. We test whether agents trained on this curriculum can generalize to standardized web navigation tasks, and achieve a 75% success rate, with a 4x improvement over the strongest curriculum learning baseline:\n\n  \nDeep RL is very good at fitting a simulated training environment, but how can we build simulations that cover the complexity of the real world? One solution is to automate this process. We propose Unsupervised Environment Design (UED) as a framework that describes different methods for automatically creating a distribution of training environments, and show that UED subsumes prior work like domain randomization and minimax adversarial training. We think PAIRED is a good approach for UED, because regret maximization leads to a curriculum of increasingly challenging tasks, and prepares agents to transfer successfully to unknown test tasks. \n\n\n\u201c\u201d", "date": "\nFriday, March 5, 2021\n"},
{"website": "Google-AI", "title": "\nLyra: A New Very Low-Bitrate Codec for Speech Compression\n", "author": ["Posted by Alejandro Luebs, Software Engineer and Jamieson Brettle, Product Manager, Chrome"], "link": "http://ai.googleblog.com/2021/02/lyra-new-very-low-bitrate-codec-for.html", "abstract": "Connecting to others online via voice and video calls is something that is increasingly a part of everyday life. The real-time communication frameworks, like ,  that make this possible depend on efficient compression techniques, , to encode (or decode) signals for transmission or storage. A vital part of media applications for decades, codecs allow bandwidth-hungry applications  to efficiently transmit data, and have led to an expectation of high-quality communication anywhere at any time.  \n\nAs such, a continuing challenge in developing codecs, both for video and audio, is to provide increasing quality, using less data, and to minimize latency for real-time communication. Even though video might seem much more bandwidth hungry than audio, modern video codecs can reach lower bitrates than some high-quality speech codecs used today. Combining low-bitrate video and speech codecs can deliver a high-quality video call experience even in low-bandwidth networks. Yet historically, the lower the bitrate for an audio codec, the less intelligible and more robotic the voice signal becomes. Furthermore, while some people have access to a consistent  high-quality, high-speed network, this  level of  connectivity isn\u2019t universal, and even those in well connected areas at times experience poor quality, low bandwidth, and congested network connections.  \n\nTo solve this problem, we have created , a high-quality, very low-bitrate speech codec that makes voice communication available even on the slowest networks.  To do this, we\u2019ve applied traditional codec techniques while leveraging advances in machine learning (ML) with models trained on thousands of hours of data to create a novel method for compressing and transmitting voice signals.\n\n\nThe basic architecture of the Lyra codec is quite simple. Features, or distinctive speech attributes, are extracted from speech every 40ms and are then compressed for transmission. The features themselves are , a list of numbers representing the speech energy in different frequency bands, which have traditionally been used for their perceptual relevance because they are modeled after human auditory response. On the other end, a  uses those features to recreate the speech signal. In this sense, Lyra is very similar to other traditional parametric codecs, such as . \n\n  However traditional  codecs, which simply extract from speech critical parameters that can then be used to recreate the signal at the receiving end, achieve low bitrates, but often sound robotic and unnatural. These shortcomings have led to the development of a new generation of high-quality audio  that have revolutionized the field by being able to not only differentiate between signals, but also generate completely new ones. DeepMind\u2019s  was the first of these generative models that paved the way for many to come.  Additionally, , the generative model-based packet-loss-concealment system currently used in Duo, has demonstrated how this technology can be used in real-world scenarios.  \n\n\nUsing these models as a baseline, we\u2019ve developed a new model capable of reconstructing speech using minimal amounts of data.  Lyra harnesses the power of these new natural-sounding generative models to maintain the low bitrate of parametric codecs while achieving high quality, on par with state-of-the-art waveform codecs used in most streaming and communication platforms today. The drawback of waveform codecs is that they achieve this high quality by compressing and sending over the signal sample-by-sample, which requires a higher bitrate and, in most cases, isn\u2019t necessary to achieve natural sounding speech. \n\nOne concern with generative models is their computational complexity. Lyra avoids this issue by using a cheaper recurrent generative model, a  variation, that works at a lower rate, but generates in parallel multiple signals in different frequency ranges that it later combines into a single output signal at the desired sample rate. This trick enables Lyra to not only run on cloud servers, but also on-device on mid-range phones in real time (with a processing latency of 90ms, which is in line with other traditional speech codecs).\u00a0This generative model is then trained on thousands of hours of speech data and optimized, similarly to WaveNet, to accurately recreate the input audio.\n\nSince the inception of Lyra, our mission has been to provide the best quality audio using a fraction of the bitrate data of alternatives.  Currently, the royalty-free open-source codec , is the most widely used codec for WebRTC-based  applications and, with audio at 32kbps, typically obtains transparent speech quality, i.e., indistinguishable from the original.   However, while Opus can be used in more bandwidth constrained environments down to 6kbps, it starts to demonstrate degraded audio quality.  Other codecs are capable of operating at comparable bitrates to Lyra (, MELP, ), but each suffer from increased artifacts and result in a robotic sounding voice.   \n\nLyra is currently designed to operate at 3kbps and listening tests show that Lyra outperforms any other codec at that bitrate and  is compared favorably to Opus at 8kbps, thus achieving more than a 60% reduction in bandwidth.  Lyra can be used wherever the bandwidth conditions are insufficient for higher-bitrates and existing low-bitrate codecs do not provide adequate quality.\n\n  \nAs with any ML based system, the model must be trained to make sure that it works for everyone.  We\u2019ve trained Lyra with thousands of hours of audio with speakers in over 70 languages using open-source audio libraries and then verifying the audio quality with expert and crowdsourced listeners.  One of the design goals of Lyra is to ensure universally accessible high-quality audio experiences.  Lyra trains on a wide dataset, including speakers in a myriad of languages, to make sure the codec is robust to any situation it might encounter.\n\n\nThe implications of technologies like Lyra are far reaching, both in the short and long term.  With Lyra, billions of users in emerging markets can have access to an efficient low-bitrate codec that allows them to have higher quality audio than ever before. Additionally, Lyra can be used in cloud environments enabling users with various network and device capabilities to chat seamlessly with each other.  Pairing Lyra with new video compression technologies, like , will allow video chats to take place, even for users connecting to the internet via a 56kbps dial-in modem. \n\n already uses ML to reduce , and is currently rolling out Lyra to improve audio call quality and reliability on very low bandwidth connections. We will continue to optimize Lyra\u2019s performance and quality to ensure maximum availability of the technology, with investigations into acceleration via GPUs and TPUs. We are also beginning to research how these technologies can lead to a low-bitrate general-purpose audio codec (i.e., music and other non-speech use cases).", "date": "\nThursday, February 25, 2021\n"},
{"website": "Google-AI", "title": "\nThe Technology Behind Cinematic Photos\n", "author": ["Posted by Per Karlsson and Lucy Yu, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2021/02/the-technology-behind-cinematic-photos.html", "abstract": "Looking at photos from the past can help people relive some of their most treasured moments. Last December , a new feature in Google Photos that aims to recapture the sense of immersion felt the moment a photo was taken, simulating camera motion and by inferring 3D representations in an image. In this post, we take a look at the technology behind this process, and demonstrate how Cinematic photos can turn a single 2D photo from the past into a more immersive 3D animation.\n\nTo enable Cinematic photos on existing pictures that were not taken in multi-view stereo, we trained a  with encoder-decoder architecture to predict a depth map from just a single RGB image. Using only one view, the model learned to estimate depth using  cues, such as the relative sizes of objects, linear perspective, defocus blur, etc.\n\nBecause monocular depth estimation datasets are typically designed for domains such as AR, robotics, and self-driving, they tend to emphasize street scenes or indoor room scenes instead of features more common in casual photography, like people, pets, and objects, which have different composition and framing. So, we created our own dataset for training the monocular depth model using photos captured on a custom 5-camera rig as well as another dataset of Portrait photos captured on Pixel 4. Both datasets included ground-truth depth from multi-view stereo that is critical for training a model. \n\nMixing several datasets in this way exposes the model to a larger variety of scenes and camera hardware, improving its predictions on photos in the wild. However, it also introduces new challenges, because the ground-truth depth from different datasets may differ from each other by an unknown scaling factor and shift. Fortunately, the Cinematic photo effect only needs the  depths of objects in the scene, not the  depths. Thus we can combine datasets by using a scale-and-shift-invariant loss during training and then normalize the output of the model at inference.\n\nThe Cinematic photo effect is particularly sensitive to the depth map\u2019s accuracy at person boundaries. An error in the depth map can result in jarring artifacts in the final rendered effect. To mitigate this, we apply median filtering to improve the edges, and also infer segmentation masks of any people in the photo using a  segmentation model trained on the . The masks are used to pull forward pixels of the depth map that were incorrectly predicted to be in the background.\n\n  \nThere can be many degrees of freedom when animating a camera in a 3D scene, and our virtual camera setup is inspired by professional video camera rigs to create cinematic motion. Part of this is identifying the optimal pivot point for the virtual camera\u2019s rotation in order to yield the best results by drawing one\u2019s eye to the subject.\n\nThe first step in 3D scene reconstruction is to create a mesh by extruding the RGB image onto the depth map. By doing so, neighboring points in the mesh can have large depth differences. While this is not noticeable from the \u201cface-on\u201d view, the more the virtual camera is moved, the more likely it is to see polygons spanning large changes in depth. In the rendered output video, this will look like the input texture is stretched. The biggest challenge when animating the virtual camera is to find a trajectory that introduces parallax while minimizing these \u201cstretchy\u201d artifacts.\n\nBecause of the wide spectrum in user photos and their corresponding 3D reconstructions, it is not possible to share one trajectory across all animations. Instead, we define a loss function that captures how much of the stretchiness can be seen in the final animation, which allows us to optimize the camera parameters for each unique photo. Rather than counting the total number of pixels identified as artifacts, the loss function triggers more heavily in areas with a greater number of connected artifact pixels, which reflects a viewer\u2019s tendency to more easily notice artifacts in these connected areas. \n\nWe utilize padded segmentation masks from a human pose network to divide the image into three different regions: head, body and background. The loss function is normalized inside each region before computing the final loss as a weighted sum of the normalized losses. Ideally the generated output video is free from artifacts but in practice, this is rare. Weighting the regions differently biases the optimization process to pick trajectories that prefer artifacts in the background regions, rather than those artifacts near the image subject.\n\nGenerally, the reprojected 3D scene does not neatly fit into a rectangle with portrait orientation, so it was also necessary to frame the output with the correct right aspect ratio while still retaining the key parts of the input image. To accomplish this, we use a deep neural network that predicts per-pixel saliency of the full image. When framing the virtual camera in 3D, the model identifies and captures as many salient regions as possible while ensuring that the rendered mesh fully occupies every output video frame. This sometimes requires the model to shrink the camera's field of view.\n\n\nThrough Cinematic photos, we implemented a system of algorithms \u2013 with each ML model evaluated for fairness \u2013 that work together to allow users to relive their memories in a new way, and we are excited about future research and feature improvements. Now that you know how they are created, keep an eye open for automatically created Cinematic photos that may appear in your recent memories within the Google Photos app!", "date": "\nTuesday, February 23, 2021\n"},
{"website": "Google-AI", "title": "\nIntroducing Model Search: An Open Source Platform for Finding Optimal ML Models\n", "author": ["Posted by Hanna Mazzawi, Research Engineer and Xavi Gonzalvo, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/02/introducing-model-search-open-source.html", "abstract": "The success of a  (NN) often depends on how well it can generalize to various tasks. However, designing NNs that can generalize well is challenging because the research community's understanding of how a neural network generalizes is currently somewhat limited: What does the appropriate neural network look like for a given problem? How deep should it be? Which types of layers should be used? Would  be enough or would  layers be better? Or maybe a combination of the two? Would  or  boost performance? These tricky questions are made even more challenging when considering machine learning (ML) domains where there may exist better intuition and deeper understanding than others. \n\nIn recent years,  algorithms have emerged [e.g., , , ] to help researchers find the right neural network automatically without the need for manual experimentation. Techniques like  (NAS), use algorithms, like  (RL), , and , to build a neural network out of a given search space. With the proper setup, these techniques have demonstrated they are capable of delivering results that are better than the manually designed counterparts. But more often than not, these algorithms are compute heavy, and need thousands of models to train before converging. Moreover, they explore search spaces that are domain specific and incorporate substantial prior human knowledge that does not transfer well across domains. As an example, in image classification, the traditional NAS searches for two good building blocks (), that it arranges following traditional conventions to create the full network.\n\nTo overcome these shortcomings and to extend access to AutoML solutions to the broader research community, we are excited to announce the open source release of , a platform that helps researchers develop the best ML models, efficiently and automatically. Instead of focusing on a specific domain, Model Search is domain agnostic, flexible and is capable of finding the appropriate architecture that best fits a given dataset and problem, while minimizing coding time, effort and compute resources. It is built on Tensorflow, and can run either on a single machine or in a distributed setting. \n\n\nThe Model Search system consists of multiple trainers, a search algorithm, a transfer learning algorithm and a database to store the various evaluated models. The system runs both training and evaluation experiments for various ML models (different architectures and training techniques) in an adaptive, yet asynchronous, fashion. While each trainer conducts experiments independently, all trainers share the knowledge gained from their experiments. At the beginning of every cycle, the search algorithm looks up all the completed trials and uses  to decide what to try next. It then invokes mutation over one of the best architectures found thus far and assigns the resulting model back to a trainer.\n\n  The system builds a neural network model from a set of predefined blocks known micro-architecture, like LSTM,  or Transformer layers. By using blocks of pre-existing architectural components, Model Search is able to leverage existing best knowledge from NAS research across domains. This approach is also more efficient, because it explores structures, not their more fundamental and detailed components, therefore reducing the scale of the search space.\n\n  Because the Model Search framework is built on , blocks can implement any function  that takes a tensor as an input. For example, imagine that one wants to introduce a new search space built with a selection of micro architectures. The framework will take the newly defined blocks and incorporate them into the search process so that algorithms can build the best possible neural network from the components provided. The blocks provided can even be fully defined neural networks that are already known to work for the problem of interest. In that case, Model Search can be configured to simply act as a powerful ensembling machine.\nThe search algorithms implemented in Model Search are adaptive,  and incremental, which makes them converge faster than RL algorithms. They do however imitate the \u201c\u201d nature of RL algorithms by separating the search for a good candidate (explore step), and boosting accuracy by ensembling good candidates that were discovered (exploit step). The main search algorithm adaptively modifies one of the top  performing experiments (where  can be specified by the user) after applying random changes to the architecture or the training technique (e.g., making the architecture deeper).\n\n  To further improve efficiency and accuracy, transfer learning is enabled between various internal experiments. Model Search does this in two ways \u2014 via  or . Knowledge distillation allows one to improve candidates' accuracies by adding a loss term that matches the high performing models\u2019 predictions in addition to the ground truth. Weight sharing, on the other hand, bootstraps some of the parameters (after applying mutation) in the network from previously trained candidates by copying suitable weights from previously trained models and randomly initializing the remaining ones. This enables faster training, which allows opportunities to discover more (and better) architectures.\n\nModel Search improves upon production models with minimal iterations. In a , we demonstrated the capabilities of Model Search in the speech domain by discovering a model for  and . Over fewer than 200 iterations, the resulting model slightly improved upon internal state-of-the-art production models designed by experts in accuracy using ~130K fewer trainable parameters (184K compared to 315K parameters). \n\n  We also applied Model Search to find an architecture suitable for image classification on the heavily explored  imaging dataset. Using a set known convolution blocks, including convolutions, resnet blocks (i.e., two convolutions and a skip connection), NAS-A cells, fully connected layers, etc., we observed that we were able to quickly reach a benchmark accuracy of 91.83 in 209 trials (i.e., exploring only 209 models). In comparison, previous top performers reached the same threshold accuracy in 5807 trials for the , and 1160 for .\n\nWe hope the  will provide researchers with a flexible, domain-agnostic framework for ML model discovery. By building upon previous knowledge for a given domain, we believe that this framework is powerful enough to build models with the state-of-the-art performance on well studied problems when provided with a search space composed of standard building blocks.", "date": "\nFriday, February 19, 2021\n"},
{"website": "Google-AI", "title": "\nMastering Atari with Discrete World Models\n", "author": ["Posted by Danijar Hafner, Student Researcher, Google Research"], "link": "http://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html", "abstract": "Deep reinforcement learning (RL) enables  to improve their decisions over time. Traditional  learn which of the actions are successful in different situations by interacting with the environment through a large amount of trial and error. In contrast, recent advances in deep RL have enabled  to learn accurate  from image inputs and use them for planning. World models can , facilitate , enable , and allow reusing knowledge .\n\nDespite their intriguing benefits, existing world models (such as ) have not been accurate enough to compete with the top model-free approaches on the most competitive reinforcement learning benchmarks \u2014  to date, the well-established  requires model-free algorithms, such as , , and , to reach human-level performance. As a result, many researchers have focused instead on developing  planning methods, such as  and , which learn by predicting sums of expected task rewards. However, these methods are specific to individual tasks and it is unclear how well they would generalize to new tasks or learn from unsupervised datasets. Similar to the recent breakthrough of unsupervised representation learning in computer vision [, ], world models aim to learn patterns in the environment that are more general than any particular task to later solve tasks more efficiently.\n\nToday, in collaboration with \u00a0and the , we introduce , the first RL agent based on a world model to achieve human-level performance on the Atari benchmark. It constitutes the second generation of the  that learns behaviors purely within the latent space of a world model trained from pixels. DreamerV2 relies exclusively on general information from the images and accurately predicts future task rewards even when its representations were not influenced by those rewards. Using a single GPU, DreamerV2 outperforms top model-free algorithms with the same compute and sample budget.\n\n  \nJust like its predecessor, DreamerV2 learns a world model and uses it to  purely from predicted trajectories. The world model automatically learns to compute compact representations of its images that discover useful concepts, such as object positions, and learns how these concepts change in response to different actions. This lets the agent generate abstractions of its images that ignore irrelevant details and enables massively parallel predictions on a single GPU. During 200 million environment steps, DreamerV2 predicts 468 billion compact states for learning its behavior.\n\nDreamerV2 builds upon the Recurrent State-Space Model (RSSM) that we introduced for  and was also used for . During training, an encoder turns each image into a stochastic representation that is incorporated into the  of the world model. Because the representations are stochastic, they do not have access to perfect information about the images and instead extract only what is necessary to make predictions, making the agent robust to unseen images. From each state, a decoder reconstructs the corresponding image to learn . Moreover, a small reward network is trained to rank outcomes during planning. To enable planning without generating images, a  learns to guess the stochastic representations without access to the images from which they were computed.\n\nImportantly, DreamerV2 introduces two new techniques to RSSM that lead to a substantially more accurate world model for learning successful policies. The first technique is to represent each image with multiple  instead of the Gaussian variables used by PlaNet, DreamerV1, and many more world models in the literature [, , , , ]. This leads the world model to reason about the world in terms of  concepts and enables more accurate predictions of future representations.\n\nThe encoder turns each image into 32 distributions over 32 classes each, the meanings of which are determined automatically as the world model learns. The  sampled from these distributions are concatenated to a  that is passed on to the recurrent state. To  through the samples, we use  that are easy to implement using automatic differentiation. Representing images with categorical variables allows the predictor to accurately learn the distribution over the one-hot vectors of the possible next images. In contrast, earlier world models that use Gaussian predictors cannot accurately match the distribution over multiple Gaussian representations for the possible next images.\n\nThe second new technique of DreamerV2 is . Many previous world models use the  that encourages accurate reconstructions while keeping the stochastic representations (posteriors) close to their predictions (priors) to regularize the amount of information extracted from each image and facilitate generalization. Because the objective is optimized end-to-end, the stochastic representations and their predictions can be made more similar by bringing either of the two towards the other. However, bringing the representations towards their predictions can be problematic when the predictor is not yet accurate. KL balancing lets the predictions move faster toward the representations than vice versa. This results in more accurate predictions, a key to successful planning.\n\n \nDreamerV2 is the first world model that enables learning successful behaviors with human-level performance on the well-established and competitive Atari benchmark. We select the 55 games that many previous studies have in common and recommend this set of games for future work. Following the , the agents are allowed 200M environment interactions using an action repeat of 4 and sticky actions (25% chance that an action is ignored and the previous action is repeated instead). We compare to the top model-free agents IQN and Rainbow, as well as to the well-known  and  agents implemented in the .\n\nDifferent standards exist for aggregating the scores across the 55 games. Ideally, a new algorithm would perform better under all conditions. For all four aggregation methods, DreamerV2 indeed outperforms all compared model-free algorithms while using the same computational budget.\n\nThe first three aggregation methods were previously proposed in the literature. We identify important drawbacks in each and recommend a new aggregation method, the  to overcome their drawbacks.\n\nWhile many current algorithms exceed the human gamer baseline, they are still quite far behind the human world record. As shown in the right-most plot above, DreamerV2 leads by achieving 25% of the human record on average across games. Clipping the scores at the record line lets us focus our efforts on developing methods that come closer to the human world record on all of the games rather than exceeding it on just a few games.\n\n\nTo gain insights into the important components of DreamerV2, we conduct an extensive ablation study. Importantly, we find that categorical representations offer a clear advantage over Gaussian representations despite the fact that Gaussians have been used extensively in prior works. KL balancing provides an even more substantial advantage over the KL regularizer used by most generative models.\n\nBy preventing the image reconstruction or reward prediction gradients from shaping the model states, we study their importance for learning successful representations. We find that DreamerV2 relies completely on universal information from the high-dimensional input images and . This mirrors the success of unsupervised representation learning in the computer vision community.\n\n\nWe show how to learn a powerful world model to achieve human-level performance on the competitive Atari benchmark and outperform the top model-free agents. This result demonstrates that world models are a powerful approach for achieving high performance on reinforcement learning problems and are ready to use for practitioners and researchers. We see this as an indication that the success of unsupervised representation learning in computer vision\u00a0[, ] is now starting to be realized in reinforcement learning in the form of world models. An unofficial implementation of DreamerV2 is  and provides a productive starting point for future research projects. We see world models that leverage large offline datasets, long-term memory, hierarchical planning, and directed exploration as exciting avenues for future research.", "date": "\nThursday, February 18, 2021\n"},
{"website": "Google-AI", "title": "\nRearranging the Visual World\n", "author": ["Posted by Andy Zeng and Pete Florence, Research Scientists, Robotics at Google"], "link": "http://ai.googleblog.com/2021/02/rearranging-visual-world.html", "abstract": "Rearranging objects (such as organizing books on a bookshelf, moving utensils on a dinner table, or pushing piles of coffee beans) is a fundamental skill that can enable robots to physically interact with our diverse and unstructured world. While easy for people, accomplishing such tasks remains an open  for embodied machine learning (ML) systems, as it requires both high-level and low-level perceptual reasoning. For example, when stacking a pile of books, one might consider where the books should be stacked, and in which order, while ensuring that the edges of the books align with each other to form a neat pile.\n\nAcross many application areas in ML, simple differences in model architecture can exhibit vastly different generalization properties. Therefore, one might ask whether there are certain deep network architectures that favor simple underlying elements of the rearrangement problem. , for example, are common in computer vision as they encode translational invariance, yielding the same response even if an image is shifted, while  are common in language processing because they exploit  to capture long-range contextual dependencies. In robotics applications, one common architectural element is to use object-centric representations such as , , or  inside learned models, but these representations require additional training data (often manually annotated) and struggle to describe difficult scenarios such as deformables (e.g., playdough), fluids (honey), or piles of stuff (chopped onions).\n\nToday, we present the , a simple model architecture for learning vision-based rearrangement tasks, which appeared as a publication and plenary talk during . Transporter Nets use a novel approach to 3D spatial understanding that avoids reliance on object-centric representations, making them general for vision-based manipulation but far more sample efficient than benchmarked end-to-end alternatives. As a consequence, they are fast and practical to train on real robots. We are also releasing an accompanying open-source implementation of Transporter Nets together with , our new simulated benchmark suite of ten vision-based manipulation tasks.\n\n\nThe key idea behind the Transporter Network architecture is that one can formulate the rearrangement problem as  Rather than relying on an explicit  (which is bound to struggle at capturing all edge cases), 3D space is a much broader definition for what could serve as the atomic units being rearranged, and can broadly encompass an object, part of an object, or multiple objects, etc. Transporter Nets leverage this structure by capturing a deep representation of the 3D visual world, then overlaying parts of it on itself to imagine various possible rearrangements of 3D space. It then chooses the rearrangements that best match those it has seen during training (e.g., from expert demonstrations), and uses them to parameterize robot actions. This formulation allows Transporter Nets to generalize to unseen objects and enables them to better exploit geometric symmetries in the data, so that they can extrapolate to new scene configurations. Transporter Nets are applicable to a wide variety of rearrangement tasks for robotic manipulation, expanding beyond our earlier models, such as  and , that focus only on grasping and tossing. \n\n\nTo evaluate the performance of Transporter Nets in a consistent environment for fair comparisons to baselines and ablations, we developed , a benchmark suite of ten simulated vision-based rearrangement tasks. Ravens features a  API with a built-in  to evaluate the sample efficiency of imitation learning methods. Ravens avoids assumptions that cannot transfer to a real setup: observation data contains only RGB-D images and camera parameters; actions are end effector poses (transposed into joint positions with inverse kinematics). \n\nExperiments on these ten tasks show that Transporter Nets are orders of magnitude more sample efficient than other end-to-end methods, and are capable of achieving over 90% success on many tasks with just 100 demonstrations, while the baselines struggle to generalize with the same amount of data. In practice, this makes collecting enough demonstrations a more viable option for training these models on real robots (which we show examples of below). \n\n  Our new  benchmark includes ten simulated vision-based manipulation tasks, including pushing and pick-and-place, for which experiments show that Transporter Nets are orders of magnitude more sample efficient than other end-to-end methods. Ravens features a  API with a built-in stochastic oracle to evaluate the sample efficiency of imitation learning methods.\n\n\nGiven 10 example demonstrations, Transporter Nets can learn pick and place tasks such as stacking plates (surprisingly easy to misplace!), multimodal tasks like aligning any corner of a box to a marker on the tabletop, or building a pyramid of blocks.\n\n  By leveraging closed-loop visual feedback, Transporter Nets have the capacity to learn various multi-step sequential tasks with a modest number of demonstrations: such as moving disks for , palletizing boxes, or assembling kits of new objects not seen during training. These tasks have considerably \u201clong horizons\u201d, meaning that to solve the task the model must correctly sequence many individual choices. Policies also tend to learn emergent recovery behaviors.\n\n\n  One surprising thing about these results was that beyond just perception, the models were starting to learn behaviors that resemble high-level planning. For example, to solve Towers of Hanoi, the models have to pick which disk to move next, which requires recognizing the state of the board based on the current visible disks and their positions. With a box-palletizing task, the models must locate the empty spaces of the pallet, and identify how new boxes can fit into those voids. Such behaviors are exciting because they suggest that with all the baked-in invariances, the model can focus its capacity on learning the more high-level patterns in manipulation.\n\nTransporter Nets can also learn tasks that use any motion primitive defined by two end effector poses, such as pushing piles of small objects into a target set, or reconfiguring a deformable rope to connect the two end-points of a 3-sided square. This suggests that rigid spatial displacements can serve as useful priors for nonrigid ones.\n\n  \nTransporter Nets bring a promising approach to learning vision-based manipulation, but are not without limitations. For example, they can be susceptible to noisy 3D data, we have only demonstrated them for sparse waypoint-based control with motion primitives, and it remains unclear how to extend them beyond spatial action spaces to force or torque-based actions. But overall, we are excited about this direction of work, and we hope that it provides inspiration for extensions beyond the applications we\u2019ve discussed. For more details, please check out our .", "date": "\nTuesday, February 16, 2021\n"},
{"website": "Google-AI", "title": "\n3D Scene Understanding with TensorFlow 3D\n", "author": ["Posted by Alireza Fathi, Research Scientist and Rui Huang, AI Resident, Google Research"], "link": "http://ai.googleblog.com/2021/02/3d-scene-understanding-with-tensorflow.html", "abstract": "The growing ubiquity of 3D sensors (e.g., ,  and ) over the last few years has created a need for scene understanding technology that can process the data these devices capture. Such technology can enable machine learning (ML) systems that use these sensors, like autonomous cars and robots, to navigate and operate in the real world, and can create an improved augmented reality experience on mobile devices. The field of computer vision has recently begun making good progress in 3D scene understanding, including models for , , and more, but entry to the field can be challenging due to the limited availability tools and resources that can be applied to 3D data.\n\nIn order to further improve 3D scene understanding and reduce barriers to entry for interested researchers, we are releasing  (TF 3D), a highly modular and efficient library that is designed to bring 3D deep learning capabilities into TensorFlow. TF 3D provides a set of popular operations, loss functions, data processing tools, models and metrics that enables the broader research community to develop, train and deploy state-of-the-art 3D scene understanding models. \n\nTF 3D contains training and evaluation pipelines for state-of-the-art 3D , 3D  and 3D , with support for . It also enables other potential applications like 3D object shape prediction,  and . In addition, it offers a unified dataset specification and configuration for training and evaluation of the standard 3D scene understanding datasets. It currently supports the , , and  datasets. However, users can freely convert other popular datasets, such as  and , into a similar format and use them in the pre-existing or custom created pipelines, and can leverage TF 3D for a wide variety of 3D deep learning research and applications, from quickly prototyping and trying new ideas to deploying a real-time inference system. \n\n  Here, we will present the efficient and configurable sparse convolutional backbone that is provided in TF 3D, which is the key to achieving state-of-the-art results on various 3D scene understanding tasks. Furthermore, we will go over each of the three pipelines that TF 3D currently supports: 3D semantic segmentation, 3D object detection and 3D instance segmentation.\n\nThe 3D data captured by sensors often consists of a scene that contains a set of objects of interest (e.g. cars, pedestrians, etc.) surrounded mostly by open space, which is of limited (or no) interest. As such, 3D data is inherently sparse. In such an environment, standard implementation of convolutions would be computationally intensive and consume a large amount of memory. So, in TF 3D we use  and  operations, which are designed to process 3D sparse data more efficiently. Sparse convolutional models are core to the state-of-the-art methods applied in most outdoor self-driving (e.g. Waymo, NuScenes) and indoor benchmarks (e.g. ScanNet). \n\nWe also use various  techniques to speed up the computation (e.g., , partitioning / caching the filter in shared memory, and using bit operations). Experiments on the Waymo Open dataset shows that this implementation is around 20x faster than a well-designed implementation with pre-existing TensorFlow operations. \n\nTF 3D then uses the 3D submanifold sparse  to extract a feature for each voxel. The U-Net architecture has proven to be effective by letting the network extract both coarse and fine features and combining them to make the predictions. The U-Net network consists of three modules, an encoder, a bottleneck, and a decoder, each of which consists of a number of sparse convolution blocks with possible pooling or un-pooling operations.\n\n  The sparse convolutional network described above is the backbone for the 3D scene understanding pipelines that are offered in TF 3D. Each of the models described below uses this backbone network to extract features for the sparse voxels, and then adds one or multiple additional prediction heads to infer the task of interest. The user can configure the U-Net network by changing the number of  layers and the number of convolutions in each layer, and by modifying the convolution filter sizes, which enables a wide range of speed / accuracy tradeoffs to be explored through the different backbone configurations\n\nThe  has only one output head for predicting the per-voxel semantic scores, which are mapped back to points to predict a semantic label per point.\n\n  \nIn 3D instance segmentation, in addition to predicting semantics, the goal is to group the voxels that belong to the same object together. The 3D instance segmentation algorithm used in TF 3D is based on our previous work on . The model predicts a per-voxel instance embedding vector as well as a semantic score for each voxel. The instance embedding vectors map the voxels to an embedding space where voxels that correspond to the same object instance are close together, while those that correspond to different objects are far apart. In this case, the input is a point cloud instead of an image, and it uses a 3D sparse network instead of a 2D image network. At inference time, a  picks one instance seed at a time, and uses the distance between the voxel embeddings to group them into segments.\n\n\nThe 3D object detection model predicts per-voxel size, center, and rotation matrices and the object semantic scores. At inference time, a box proposal mechanism is used to reduce the hundreds of thousands of per-voxel box predictions into a few accurate box proposals, and then at training time, box prediction and classification losses are applied to per-voxel predictions. We apply a  on the distance between predicted and the ground-truth box corners. Since the function that estimates the box corners from its size, center and rotation matrix is differentiable, the loss will automatically propagate back to those predicted object properties.  We use a dynamic box classification loss that classifies a box that strongly overlaps with the ground-truth as positive and classifies the non-overlapping boxes as negative. \n\n  In our recent paper, \u201c\u201d, we describe in detail the single-stage weakly supervised learning algorithm used for object detection in TF 3D. In addition, in a follow up work, we extended the 3D object detection model to leverage temporal information by proposing a . We go on to show that this temporal model outperforms the frame-by-frame approach by 7.5% in the .\n  \nWe\u2019ve certainly found this codebase to be useful for our 3D computer vision projects, and we hope that you will as well. Contributions to the codebase are welcome and please stay tuned for our own further updates to the framework. To get started please visit .", "date": "\nThursday, February 11, 2021\n"},
{"website": "Google-AI", "title": "\nUncovering Unknown Unknowns in Machine Learning\n", "author": ["Posted by Lora Aroyo and Praveen Paritosh, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2021/02/uncovering-unknown-unknowns-in-machine.html", "abstract": "The performance of machine learning (ML) models depends both on the learning algorithms, as well as the data used for training and evaluation. The role of the algorithms is well studied and the focus of a multitude of challenges, such as , , , and many others. In addition, there have been efforts to also improve the data, including a  addressing issues for ML evaluation.  In contrast, research and challenges that focus on thedata used for evaluation of ML models are not commonplace. Furthermore, many evaluation datasets contain items that are easy to evaluate, e.g., photos with a subject that is easy to identify, and thus they miss the natural ambiguity of real world context. The absence of ambiguous real-world examples in evaluation , which makes ML models prone to develop \u201cweak spots\u201d, i.e., classes of examples that are difficult or impossible for a model to accurately evaluate, because that class of examples is missing from the evaluation set. \n\nTo address the problem of identifying these weaknesses in ML models, we recently launched the  (CATS4ML) Data Challenge at  (open until 30 April, 2021 to researchers and developers worldwide). The goal of the challenge is to raise the bar in ML evaluation sets and to find as many examples as possible that are confusing or otherwise problematic for algorithms to process. CATS4ML relies on people\u2019s abilities and intuition to spot new data examples about which machine learning is confident, but actually misclassifies. \n\n\nThere are two categories of weak spots: and Known unknowns are examples for which a model is unsure about the correct classification. The research community continues to study this in a field known as , and has found the solution to be, in very general terms, to interactively solicit new labels from people on uncertain examples.  For example, if a model is not certain whether or not the subject of a photo is a cat, a person is asked to verify; but if the system is certain, a person is not asked. While there is room for improvement in this area, what is comforting is that the confidence of the model is correlated with its performance, i.e., one can see what the model doesn\u2019t know. \n\nUnknown unknowns, on the other hand, are examples where a model is confident about its answer, but is actually wrong. Efforts to proactively discover unknown unknowns (e.g.,  and ) have helped uncover a multitude of unintended machine behaviours. In contrast to such approaches for the discovery of unknown unknowns,  (GANs)  for image recognition models in the form of optical illusions for computers that cause deep learning models to make mistakes beyond human perception. While GANs uncover model exploits in the event of an intentional manipulation, real-world examples can better highlight a model\u2019s failures in its day-to-day performance. These real-world examples are the unknown unknowns of interest to CATS4ML \u2014 the challenge aims to gather unmanipulated examples that humans can reliably interpret but on which many ML models would confidently disagree.\n\nThe  focuses on visual recognition, using images and labels from the . The target images for the challenge are selected from the Open Images Dataset along with a set of 24 target labels from the same dataset. The challenge participants are invited to invent new and creative ways to explore this existing publicly available dataset and, focussed on a list of pre-selected target labels, discover examples of unknown unknowns for ML models.  \n is a complementary effort to \u2019s recently introduced  research platform for dynamic data collection. Where DynaBench tackles issues with static benchmarks using ML models with humans in the loop, CATS4ML focuses on improving evaluation datasets for ML by encouraging the exploration of existing ML benchmarks for adverse examples that can be unknown unknowns. The results will help detect and avoid future errors, and also will give insights to model explainability.\nIn this way, CATS4ML aims to raise greater awareness of the problem by providing dataset resources that developers can use to uncover the weak spots of their algorithms. This will also inform researchers on how to create benchmark datasets for machine learning that are more balanced, diverse and socially aware. \n\n\nWe invite the global community of ML researchers and practitioners to join us in the effort of discovering interesting, difficult examples from the Open Images Dataset. Register on the , download the target images and labeled data, contribute the images you discover and join the competition for the winning participant! \n\nTo score points in this competition, participants should submit a set of image-label pairs that will be confirmed by human-in-the-loop raters, whose votes should be in disagreement with the average machine score for the label over a number of machine learning models. \n\nThe challenge is open until 30 April, 2021 to researchers and developers worldwide. To learn more about CATS4ML and how to join, please\u00a0visit the .", "date": "\nThursday, February 11, 2021\n"},
{"website": "Google-AI", "title": "\nTracIn \u2014 A Simple Method to Estimate Training Data Influence\n", "author": ["Posted by Frederick Liu and Garima Pruthi, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2021/02/tracin-simple-method-to-estimate.html", "abstract": "The quality of a machine learning (ML) model\u2019s training data can have a significant impact on its performance. One measure of data quality is the notion of , i.e., the degree to which a given training example affects the model and its predictive performance. And while  is a well-known concept to ML researchers, the complexity behind deep learning models, coupled with their growing size, features and datasets, have made the quantification of influence difficult. \n\nA few methods have been proposed recently to quantify influence. Some rely on changes in accuracy when retraining with one or several data points dropped, and some use established statistical methods, e.g.,  that estimate the impact of perturbing input points or  that decompose a prediction into an importance weighted combination of training examples. Still other approaches require use of additional estimators, such as . Though these approaches are theoretically sound, their use in products has been limited by the resources needed to run them at scale or the additional burdens they place on training. \n\nIn \u201c\u201d, published as a spotlight paper at , we proposed , a simple scalable approach to tackle this challenge. The idea behind TracIn is straightforward \u2014  the training process to capture changes in prediction as individual training examples are visited. TracIn is effective in finding mislabeled examples and outliers from a variety of datasets, and is useful in explaining predictions in terms of training examples (as opposed to features) by assigning an influence score to each training example.\n\n\nDeep learning algorithms are typically trained using an algorithm called  (SGD), or a variant of it. SGD operates by making multiple passes over the data and making modifications to the model parameters that locally reduce the loss (i.e., the model\u2019s objective) with each pass. An example of this is demonstrated for an image classification task in the figure below, where the model\u2019s task is to predict the subject of the test image on the left (\u201czucchini\u201d). As the model progresses through training, it is exposed to various training examples that affect the  on the test image, where the  is a function both of the prediction score and the actual label \u2014 the higher the prediction score for zucchini, the lower the loss. \n\nSuppose that the test example is known at training time and that the training process visited each training example one at a time. During the training, visiting a specific training example would change the model\u2019s parameters, and that change would then modify the prediction/loss on the test example. If one could trace the training example through the process, then the change in loss or prediction on the test example could be attributed to the training example in question, where the influence of a training example would be the cumulative attribution across visits to the training example.\n\nThere are two types of relevant training examples. Those that reduce loss, like the images of zucchinis above, are called , while those that increase loss, like the images of seatbelts, are called . In the example above, the image labeled \u201csunglasses\u201d is also a proponent, because it has a seatbelt in the image, but is labeled as \u201csunglasses,\u201d driving the model to better distinguish between zucchini and seatbelts.\n\nIn practice, the test example is unknown at training time, a limitation that can be overcome by using the checkpoints output by the learning algorithm as a sketch of the training process. Another challenge is that the learning algorithm typically visits several points at once, not individually, which requires a method to disentangle the relative contributions of each training example. This can be done by applying pointwise loss gradients. Together, these two strategies capture the TracIn method, which can be reduced to the simple form of the dot product of loss gradients of the test and training examples, weighted by the learning rate, and summed across checkpoints.  \n\nAlternatively, one could instead examine the influence on the prediction score, which would be useful if the test example has no label. This form simply requires the substitution of the loss gradient at the test example with the prediction gradient. \n\n\nWe illustrate the utility of TracIn by first calculating the loss gradient vector for some training data and a test example for a specific classification \u2014 an image of a chameleon \u2014 and then leveraging a standard  library to retrieve the top proponents and opponents. The top opponents indicate the chameleon\u2019s ability to blend in! For comparison, we also show the  nearest neighbors with embeddings from the penultimate layer. Proponents are images that are not only similar, but also belong to the same class, and opponents are similar images but in a different class. Note that there isn\u2019t an explicit enforcement on whether proponents or opponents belong to the same class.\n\n\nThe simplistic breakdown of the loss of the test example into training example influences given by TracIn also suggests that the loss (or prediction) from any gradient descent based neural model can be expressed as a sum of similarities in the space of gradients.  has demonstrated that this functional form is similar to that of a , implying that this gradient similarity described here can be applied to other similarity tasks, like clustering. \n\nIn this case, TracIn can be used as a  within a . To bound the similarity metric so that it can be converted to a distance measure (1 - similarity), we normalize the gradient vectors to have unit norm. Below, we apply TracIn clustering on images of zucchini to obtain finer clusters.\n\n\nFinally, we can also use TracIn to identify outliers that exhibit a high , i.e., the influence of a training point on its own prediction. This happens either when the example is mislabeled or rare, both of which make it difficult for the model to generalize over the example. Below are some examples with high self-influence.\n\nHaving no requirement other than being trained using SGD (or related variants), TracIn is task-independent and applicable to a variety of models. For example, we have used TracIn to study training data for a deep learning model used to parse queries to the Google Assistant, queries of the kind \u201cset my alarm for 7AM\u201d. We were intrigued to see that the top opponent for the query \u201cdisable my alarm\u201d with an alarm active on the device, was \u201cdisable my timer\u201d, also with an alarm active on the device. This suggests that Assistant users often interchange the words \u201ctimer\u201d and \u201calarm\u201d. TracIn helped us interpret the Assistant data. \n\nMore examples can be found in the , including a regression task on structured data and a number of text classification tasks.\n\n\nTracIn is a simple, easy-to-implement, scalable way to compute the influence of training data examples on individual predictions or to find rare and mislabeled training examples. For implementation references of the method, you can find a link to code examples for images from the github linked in the .", "date": "\nFriday, February 5, 2021\n"},
{"website": "Google-AI", "title": "\nMachine Learning for Computer Architecture\n", "author": ["Posted by Amir Yazdanbakhsh, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/02/machine-learning-for-computer.html", "abstract": "One of the key contributors to recent machine learning (ML) advancements is the development of custom accelerators, such as  and , which significantly increase available compute power unlocking various capabilities such as , , , and . This increase can lead to improved performance in neural network training and inference, enabling new possibilities in a broad range of applications, such as , , , and . \n\nTo sustain these advances, the hardware accelerator ecosystem must continue to innovate in architecture design and acclimate to rapidly evolving ML models and applications. This requires the evaluation of many different accelerator design points, each of which may not only improve the compute power, but also unravel a new capability. These design points are generally parameterized by a variety of hardware and software factors (e.g., memory capacity, number of compute units at different levels, parallelism, interconnection networks, pipelining, software mapping, etc.). This is a daunting optimization task, due to the fact that the search space is exponentially large while the objective function (e.g., lower latency and/or higher energy efficiency) is computationally expensive to evaluate through simulations or synthesis, making identification of feasible accelerator configurations challenging . \n\nIn \u201c\u201d, we present the progress of our research on ML-driven design of custom accelerators. While   has demonstrated promising results in leveraging ML to improve the  floorplanning process (in which the hardware components are spatially laid out and connected in silicon), in this work we focus on blending ML into the  system specification and architectural design stage, a pivotal contributing factor to the overall performance of the chip in which the design elements that control the high-level functionality are established. Our research shows how ML algorithms can facilitate architecture exploration and suggest high-performing architectures across a range of deep neural networks, with domains spanning , ,  and . \n\n\nThe objective in architecture exploration is to discover a set of feasible accelerator parameters for a set of workloads, such that a desired objective function (e.g., the weighted average of runtime) is minimized under an optional set of user-defined constraints. However, the manifold of architecture search generally contains many points for which there is no feasible mapping from software to hardware. Some of these design points are known  and can be bypassed by formulating them as optimization constraints by the user (e.g., in the case of an area budget constraint, the total memory size must not pass over a predefined limit). However, due to the interplay of the architecture and compiler and the complexity of the search space, some of the constraints may not be properly formulated into the optimization, and so the compiler may not find a feasible software mapping for the target hardware. These infeasible points are not easy to formulate in the optimization problem, and are generally unknown until the whole compiler pass is performed. As such, one of main challenges for architecture exploration is to effectively sidestep the infeasible points for efficient exploration of the search space with a minimum number of cycle-accurate architecture simulations.\n\nThe following figure shows the overall architecture search space of a target ML accelerator. The accelerator contains a 2D array of processing elements (PE), each of which performs a set of arithmetic computations in a  (SIMD) manner. The main architectural components of each PE are processing cores that include multiple compute lanes for SIMD operations. Each PE has shared memory () across all their compute cores, which is mainly used to store model activations, partial results, and outputs, while individual cores feature memory that is mainly used for storing model parameters. Each core has multiple compute lanes with multi-way  (MAC) units. The results of model computations at each cycle are either stored back in the PE memory for further computation or are offloaded back into the DRAM.\n\n  \nIn this study, we explored four optimization strategies in the context of architecture exploration:\n\n\nTo better visualize the effectiveness of each optimization strategy in navigating the accelerator search space, we use \u00a0(t-SNE) to map the explored configurations into a two-dimensional space across the optimization horizon. The objective (reward) for all the experiments is defined as the throughput (inference/second) per accelerator area. In the figures below, the  and  axes indicate the t-SNE components (embedding 1 and embedding 2) of the embedding space. The star and circular markers show the infeasible (zero reward) and feasible design points, respectively, with the size of the feasible points corresponding to their reward. \n\nAs expected, the random strategy searches the space in a uniformly distributed way and eventually finds very few feasible points in the design space.\n\nCompared to the random sampling approach, the  default optimization strategy strikes a good balance between exploring the search space and finding the design points with higher rewards (1.14 vs. 0.96). However, this approach tends to get stuck in infeasible regions and, while it does find a few points with the maximum reward (indicated by the red cross markers), it finds few feasible points during the last iterations of exploration.\n\nThe evolutionary optimization strategy, on the other hand, finds feasible solutions very early in the optimization and assemble clusters of feasible points around them. As such, this approach mostly navigates the feasible regions (the green circles) and efficiently sidesteps the infeasible points. In addition, the evolutionary search is able to find more design options with maximum reward (the red crosses). This diversity in the solutions with high reward provides flexibility to the designer in exploring various architectures with different design trade-offs.\n\nFinally, the population-based optimization method (P3BO) explores the design space in a more targeted way (regions with high reward points) in order to find optimal solutions. The P3BO strategy finds design points with the highest reward in search spaces with tighter constraints (e.g., a larger number of infeasible points), showing its effectiveness in navigating search spaces with large numbers of infeasible points.\n\n\nWe also studied the benefits of each optimization strategy under different area budget constraints, 6.8 mm, 5.8 mm and 4.8 mm. The following violin plots  show the full distribution of the maximum achievable reward at the end of optimization (after ten runs each with 4K trials) across the studied optimization strategies. The wider sections represent a higher probability of observing feasible architecture configurations at a particular given reward. This implies that we favor the optimization algorithm that yields increased width at the points with higher reward (higher performance). \n\nThe two top-performing optimization strategies for architecture exploration are evolutionary and P3BO, both in terms of delivering solutions with high reward and robustness across multiple runs. Looking into different design constraints, we observe that as one tightens the area budget constraint, the P3BO optimization strategy yields more high performing solutions. For example, when the area budget constraint is set to 5.8 mm, P3BO finds design points with a reward (throughput / accelerator area)  of 1.25 outperforming all the other optimization strategies. The same trend is observed when the area budget constraint is set to 4.8 mm, a slightly better reward is found with more robustness (less variability) across multiple runs.\n\n\nWhile  presents the first step towards better understanding of accelerator design space and building more efficient hardware, inventing accelerators with new capabilities is still an uncharted territory and a new frontier. We believe that this research is an exciting path forward to further explore ML-driven techniques for architecture design  and co-optimization (e.g., compiler, mapping, and scheduling) across the computing stack to  invent  efficient accelerators with new capabilities for the next generation of applications.", "date": "\nThursday, February 4, 2021\n"},
{"website": "Google-AI", "title": "\nEvaluating Design Trade-offs in Visual Model-Based Reinforcement Learning\n", "author": ["Posted by Mohammad Babaeizadeh, Research Engineer and Dumitru Erhan, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/02/evaluating-design-trade-offs-in-visual.html", "abstract": "has been successfully demonstrated across a range of domains, including , ,  and . These systems learn by simple trial and error and thus require a vast number of attempts at a given task before solving it. In contrast,  (MBRL) learns a  of the environment (often referred to as a ) that enables the agent to predict the outcomes of potential actions, which reduces the amount of environment interaction needed to solve a task.\n\nIn principle, all that is strictly necessary for planning is to predict future , which could then be used to select near-optimal future actions. Nevertheless, many recent methods, such as , , and , additionally leverage the training signal of predicting future images. But is predicting future images actually necessary, or helpful? What benefit do visual MBRL algorithms actually derive from  predicting future images? The computational and representational cost of predicting entire images is considerable, so understanding whether this is actually useful is of profound importance for MBRL research. \n\nIn \u201c\u201d, we demonstrate that predicting future images provides a substantial benefit, and is in fact a key ingredient in training successful visual MBRL agents. We developed a new open-source library, called the , which enabled us to rigorously evaluate various world model designs to determine the relative impact of image prediction on returned rewards for each. \n\n\nThe World Models Library, designed specifically for visual MBRL training and evaluation, enables the empirical study of the effects of each design decision on the final performance of an agent across multiple tasks on a large scale. The library introduces a platform-agnostic visual MBRL simulation loop and the APIs to seamlessly define new world-models, planners and tasks or to pick and choose from the existing catalog, which includes agents (e.g., ), video models (e.g., ), and a variety of  tasks and planners, such as  and . \n\nUsing the library,  developers can study the effect of a varying factor in MBRL, such as the model design or representation space, on the performance of the agent on a suite of tasks. The library supports the training of the agents from scratch, or on a pre-collected set of trajectories, as well as evaluation of a pre-trained agent on a given task. The models, planning algorithms and the tasks can be easily mixed and matched to any desired combination. \n\nTo provide the greatest flexibility for users, the library is built using the  interface, which enables different components to be implemented in either ,  or . Please look at  for a quick introduction.\n\n\nUsing the World Models Library, we trained multiple world models with different levels of image prediction. All of these models use the same input (previously observed images) to predict an image and a reward, but they differ on what percentage of the image they predict. As the number of image pixels predicted by the agent increases, the agent performance as measured by the true reward generally improves.  \n\nInterestingly, the correlation between reward prediction accuracy and agent performance is not as strong, and in some cases a more accurate reward prediction can even result in lower agent performance. At the same time, there is a strong correlation between image reconstruction error and the performance of the agent.\n\nThis phenomenon is directly related to ,i.e., when the agent attempts more risky and potentially less rewarding actions in order to collect more information about the unknown options in the environment. This can be shown by testing and comparing models in an  (i.e., learning policies from pre-collected datasets, as opposed to  RL, which learns policies by interacting with an environment). An offline setup ensures that there is no exploration and all of the models are trained on the same data. We observed that models that fit the data better usually perform better in the offline setup, and surprisingly, these may not be the same models that perform the best when learning and exploring from scratch.\n\n\nWe have empirically demonstrated that predicting images can substantially improve task performance over models that only predict the expected reward. We have also shown that the accuracy of image prediction strongly correlates with the final task performance of these models. These findings can be used for better model design and can be particularly useful for any future setting where the input space is high-dimensional and collecting data is expensive. \n\nIf you'd like to develop your own models and experiments, head to our  and  where you'll find instructions on how to reproduce this work and use or extend the World Models Library.", "date": "\nWednesday, February 3, 2021\n"},
{"website": "Google-AI", "title": "\nLearning to Reason Over Tables from Less Data\n", "author": ["Posted by Julian Eisenschlos, AI Resident, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2021/01/learning-to-reason-over-tables-from.html", "abstract": "The task of recognizing, also known as natural language inference, consists of determining whether a piece of text (a , can be implied or contradicted (or neither) by another piece of text (the . While this problem is often considered an important test for the reasoning skills of machine learning (ML) systems and has been studied in depth for plain text inputs, much less effort has been put into applying such models to structured data, such as websites, tables, databases, etc. Yet, recognizing textual entailment is especially relevant whenever the contents of a table need to be accurately summarized and presented to a user, and is essential for high fidelity  systems and .\n\nIn \"\", published in , we introduce the first pre-training tasks customized for table parsing, enabling models to learn better, faster and from less data. We build upon our earlier  model, which was an extension of the  bi-directional  model with special embeddings to find answers in tables. Applying our new pre-training objectives to TAPAS yields a new state of the art on multiple datasets involving tables. On , for example, it reduces the gap between model and human performance by ~50%. We also systematically benchmark methods of selecting relevant input for higher efficiency, achieving 4x gains in speed and memory, while retaining 92% of the results. All the models for different tasks and sizes are released on , where you can try them out yourself in a  Notebook.\n\n\nThe task of textual entailment is more challenging when applied to tabular data than plain text. Consider, for example, a table from Wikipedia with some sentences derived from its associated table content. Assessing if the content of the table entails or contradicts the sentence may require looking over multiple columns and rows, and possibly performing simple numeric computations, like averaging, summing, differencing, etc.\n\nFollowing the methods used by TAPAS, we encode the content of a statement and a table together, pass them through a Transformer model, and obtain a single number with the probability that the statement is entailed or refuted by the table.\n\nBecause the only information in the training examples is a binary value (i.e., \"correct\" or \"incorrect\"), training a model to understand whether a statement is entailed or not is challenging and highlights the difficulty in achieving generalization in deep learning, especially when the provided training signal is scarce. Seeing isolated entailed or refuted examples, a model can easily pick-up on spurious patterns in the data to make a prediction, for example the presence of the word \"tie\" in \"Greg Norman and Billy Mayfair tie in rank\", instead of truly comparing their ranks, which is what is needed to successfully apply the model beyond the original training data. \n\n\nPre-training tasks can be used to \u201cwarm-up\u201d models by providing them with large amounts of readily available unlabeled data. However, pre-training typically includes primarily plain text and not tabular data. In fact, TAPAS was originally pre-trained using a simple masked language modelling objective that was not designed for tabular data applications. In order to improve the model performance on tabular data, we introduce two novel pretraining binary-classification tasks called  and , which can be applied as a second stage of pre-training (often called ). \n\nIn the counterfactual task, we source sentences from Wikipedia that mention an entity (person, place or thing) that also appears in a given table. Then, 50% of the time, we modify the statement by swapping the entity for another alternative. To make sure the statement is realistic, we choose a replacement among the entities in the same column in the table. The model is trained to recognize whether the statement was modified or not. This pre-training task includes millions of such examples, and although the reasoning about them is not complex, they typically will still sound natural.\n\nFor the synthetic task, we follow a method similar to  in which we generate statements using a simple set of grammar rules that require the model to understand basic mathematical operations, such as sums and averages (e.g., \"the sum of earnings\"), or to understand how to filter the elements in the table using some condition (e.g.,\"the country is Australia\"). Although these statements are artificial, they help improve the numerical and logical reasoning skills of the model.\n\n\nWe evaluate the success of the counterfactual and synthetic pre-training objectives on the TabFact dataset by comparing to the baseline TAPAS model and to  two prior models that have exhibited success in the textual entailment domain,  (LFC) and  (SAT). The baseline TAPAS model exhibits improved performance relative to LFC and SAT, but the pre-trained model (TAPAS+CS) performs significantly better, achieving a new state of the art.\n\nWe also apply TAPAS+CS to question answering tasks on the , which requires that the model find answers from the content of tables in a dialog setting. The inclusion of CS objectives improves the previous best performance by more than 4 points, demonstrating that this approach also generalizes performance beyond just textual entailment.\n\n\nAnother aspect of the counterfactual and synthetic pre-training tasks is that since the models are already tuned for binary classification, they can be applied without any fine-tuning to TabFact. We explore what happens to each of the models when trained only on a subset (or even none) of the data. Without looking at a single example, the TAPAS+CS model is competitive with a strong baseline Table-Bert, and when only 10% of the data are included, the results are comparable to the previous state-of-the-art.\n\nA general concern when trying to use large models such as this to operate on tables, is that  their high computational requirements makes it difficult for them to parse very large tables. To address this, we investigate whether one can heuristically select subsets of the input to pass through the model in order to optimize its computational efficiency. \n\nWe conducted a systematic study of different approaches to filter the input and discovered that simple methods that select for word overlap between a full column and the subject statement give the best results. By dynamically selecting which tokens of the input to include, we can use fewer resources or work on larger inputs at the same cost. The challenge is doing so without losing important information and hurting accuracy.\u00a0For instance, the models discussed above all use sequences of 512 tokens, which is around the normal limit for a transformer model (although recent efficiency methods like the  or  are proving effective in scaling the input size). The column selection methods we propose here can allow for faster training while still achieving high accuracy on TabFact. For 256 input tokens we get a very small drop in accuracy, but the model can now be pre-trained, fine-tuned and make predictions up to two times faster. With 128 tokens the model still outperforms the previous state-of-the-art model, with an even more significant speed-up \u2014 4x faster across the board.\nUsing both the column selection method we proposed and the novel pre-training tasks, we can create table parsing models that need fewer data and less compute power to obtain better results. \n\nWe have made available the new models and pre-training techniques at , where you can try it out yourself in . In order to make this approach more accessible, we also shared models of varying sizes all the way down to \u201c\u201d. It is our hope that these results will help spur development of table reasoning among the broader research community.", "date": "\nFriday, January 29, 2021\n"},
{"website": "Google-AI", "title": "\nImproving Mobile App Accessibility with Icon Detection\n", "author": ["Posted by Gilles Baechler and Srinivas Sunkara, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2021/01/improving-mobile-app-accessibility-with.html", "abstract": "enables users to control their Android device hands free, using only verbal commands. In order to function properly, it needs on-screen user interface (UI) elements to have reliable , which are provided to the operating system\u2019s accessibility services via the . Unfortunately, in many apps, adequate labels aren\u2019t always available for UI elements, e.g. images and icons, reducing the usability of Voice Access.\n\nAddressing this challenge requires a system that can automatically detect icons using only the pixel values displayed on the screen, regardless of whether icons have been given suitable accessibility labels. What little research exists on this topic typically uses , sometimes combined with  to infer classes and attributes from UI elements. However, these classifiers still rely on the accessibility tree to obtain bounding boxes for UI elements, and fail when appropriate labels do not exist.\n\nHere, we describe IconNet, a vision-based object detection model that can automatically detect icons on the screen in a manner that is agnostic to the underlying structure of the app being used, launched as part of the latest version of Voice Access. IconNet can detect 31 different icon types (to be extended to more than 70 types soon) based on UI screenshots alone. IconNet is optimized to run on-device for mobile environments, with a compact size and fast inference time to enable a seamless user experience. The current IconNet model achieves a  (mAP) of 94.2% running at 9 FPS on a Pixel 3A.\n\n\nFrom a technical perspective, the problem of detecting icons on app screens is similar to classical object detection, in that individual elements are labelled by the model with their locations and sizes. But, in other ways, it\u2019s quite different. Icons are typically small objects, with relatively basic geometric shapes and a limited range of colors, and app screens widely differ from natural images in that they are more structured and geometrical.\n\nA significant challenge in the development of an on-device UI element detector for Voice Access is that it must be able to run on a wide variety of phones with a range of performance performance capabilities, while preserving the user\u2019s privacy. For a fast user experience, a lightweight model with low inference latency is needed. Because Voice Access needs to use the labels in response to an utterance from a user (e.g., \u201ctap camera\u201d, or \u201cshow labels\u201d) inference time needs to be short (<150 ms on a Pixel 3A) with a model size less than 10 MB.\n\n\nIconNet is based on the novel  architecture, which extracts features from input images and then predicts appropriate bounding box centers and sizes (in the form of heatmaps). CenterNet is particularly suited here because UI elements consist of simple, symmetric geometric shapes, making it easier to identify their centers than for natural images. The total loss used is a combination of a standard  for the icon sizes and a modified  for the center predictions, the latter of which addresses icon class imbalances between commonly occurring icons (e.g., arrow backward, menu, more, and star) and underrepresented icons (end call, delete, launch apps, etc.)..\n\nAfter experimenting with several backbones (MobileNet, ResNet, UNet, etc), we selected the most promising server-side architecture \u2014  \u2014 as a starting point for designing a backbone tailored for icon and UI element detection. While this architecture is perfectly suitable for server side models, vanilla Hourglass backbones are not an option for a model that will run on a mobile device, due to their large size and slow inference time. We restricted our on-device network design to a single stack, and drastically reduced the width of the backbone. Furthermore, as the detection of icons relies on more local features (compared to real objects), we could further reduce the depth of the backbone without adversely affecting the performance. Ablation studies convinced us of the importance of skip connections and high resolution features. For example, trimming skip connections in the final layer reduced the mAP by 1.5%, and removing such connections from both the final and penultimate layers resulted in a  decline of 3.5% mAP.\n\n\nOnce the backbone architecture was selected, we used  (NAS) to explore variations on the network architecture and uncover an optimal set of training and model parameters that would balance model performance (mAP) with latency (FLOPs).  Additionally, we used  (FiGS) to further refine the backbone design. FiGS is a differentiable architecture search technique that uncovers sparse structures by pruning a candidate architecture and discarding unnecessary connections. This technique allowed us to reduce the model size by 20% without any loss in performance, and by 50% with only a minor drop of 0.3% in mAP.\n\nImproving the quality of the training dataset also played an important role in boosting the model performance. We collected and labeled more than 700K screenshots, and in the process, we streamlined data collection by using heuristics and auxiliary models to identify rarer icons. We also took advantage of data augmentation techniques by enriching existing screenshots with infrequent icons.\n\nTo improve the inference time, we modified our model to run using  (NNAPI) on a variety of  available on many mobile phones. For this we converted the model to use 8-bit integer quantization which gives the additional benefit of model size reduction. After some experimentation, we used  to quantize the model, while matching the performance of a server-side floating point model. The quantized model results in a 6x speed-up (700ms vs 110ms) and 50% size reduction while losing only ~0.5% mAP compared to the unquantized model.\n\n\nWe use traditional object detection metrics (e.g., mAP) to measure model performance. In addition, to better capture the use case of voice controlled user actions, we define a modified version of a false positive (FP) detection, where we penalize more incorrect detections for icon classes that are present on the screen. For comparing detections with ground truth, we use the  (CIROI), another metric we developed for this work, which returns in a positive match when the center of the detected bounding box lies inside the ground truth bounding box. This better captures the Voice Access mode of operation, where actions are performed by tapping anywhere in the region of the UI element of interest.\n\nWe compared the IconNet model with various other mobile compatible object detectors, including  and . Experiments showed that for a fixed latency, IconNet outperformed the other models in terms of mAP@CIROI on our internal evaluation set.\n\nThe performance advantage of IconNet persists when considering quantized models and models for a fixed latency budget.\n\n\nWe are constantly working on improving IconNet. Among other things, we are interested in increasing the range of elements supported by IconNet to include any generic UI element, such as images, text, or buttons. We also plan to extend IconNet to differentiate between similar looking icons by identifying their functionality. On the application side, we are hoping to increase the number of apps with valid content descriptions by augmenting developer tools to suggest content descriptions for different UI elements when building applications.", "date": "\nThursday, January 28, 2021\n"},
{"website": "Google-AI", "title": "\nAddressing Range Anxiety with Smart Electric Vehicle Routing\n", "author": ["Posted by Kostas Kollias and Sreenivas Gollapudi, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2021/01/addressing-range-anxiety-with-smart.html", "abstract": "Mapping algorithms used for navigation often rely on , a fundamental textbook solution for finding shortest paths in graphs. Dijkstra\u2019s algorithm is simple and elegant -- rather than considering all possible routes (an exponential number) it iteratively improves an initial solution, and works in . The original algorithm and practical extensions of it (such as the ) are used millions of times per day for routing vehicles on the global road network. However, due to the fact that most vehicles are gas-powered, these algorithms ignore refueling considerations because a) gas stations are usually available everywhere at the cost of a small detour, and b) the time needed to refuel is typically only a few minutes and is negligible compared to the total  travel time. \n\nThis situation is different for electric vehicles (EVs). First, EV charging stations are not as commonly available as gas stations, which can cause , the fear that the car will run out of power before reaching a charging station. This concern is common enough that it is considered one of the barriers to the widespread adoption of EVs. Second, charging an EV\u2019s battery is a more decision-demanding task, because the charging time can be a significant fraction of the total travel time and can vary widely by station, vehicle model, and battery level. In addition, the charging time is non-linear \u2014 e.g., it takes longer to charge a battery from 90% to 100% than from 20% to 30%.\n\nToday,  integrated into the latest release of  for participating EVs that reduces range anxiety by integrating recharging stations into the navigational route. Based on the battery level and the destination, Maps will recommend the charging stops and the corresponding charging levels that will minimize the total duration of the trip. To accomplish this we engineered a highly scalable solution for recommending efficient routes through charging stations, which optimizes the sum of the driving time and the charging time together. \n\n\nA fundamental constraint on route selection is that the distance between recharging stops cannot be higher than what the vehicle can reach on a full charge.  Consequently, the route selection model emphasizes the  of charging stations, as opposed to the graph of road segments of the road network, where each charging station is a node and each trip between charging stations is an edge. Taking into consideration the various characteristics of each EV (such as the weight, maximum battery level, plug type, etc.) the algorithm identifies which of the edges are feasible for the EV under consideration and which are not. Once the routing request comes in, Maps EV routing augments the feasible graph with two new nodes, the origin and the destination, and with multiple new (feasible) edges that outline the potential trips from the origin to its nearby charging stations and to the destination from each of its nearby charging stations.\n\nRouting using Dijkstra\u2019s algorithm or A* on this graph is sufficient to give a feasible solution that optimizes for the travel time for drivers that do not care at all about the charging time, (i.e., drivers who always fully charge their batteries at each charging station). However, such algorithms are not sufficient to account for charging times. In this case, the algorithm constructs a new graph by replicating each charging station node multiple times. Half of the copies correspond to entering the station with a partially charged battery, with a charge, , ranging from 0%-100%. The other half correspond to exiting the station with a fractional charge,  (again from 0%-100%). We add an edge from the entry node at the charge  to the exit node at charge  (constrained by y > x), with a corresponding charging time to get from x to y. When the trip from  to  spends some fraction () of the battery charge, we introduce an edge between every exit node of  to the corresponding entry node of  (at charge -). After performing this transformation, using Dijkstra or A* recovers the solution.\n\n\nTo perform the above operations while addressing range anxiety with confidence, the algorithm must compute the battery consumption of each trip between stations with good precision. For this reason, Maps maintains detailed information about the road characteristics along the trip between any two stations (e.g., the length, elevation, and slope, for each segment of the trip), taking into consideration the properties of each type of EV.  \n\nDue to the volume of information required for each segment, maintaining a large number of edges can become a memory intensive task. While this is not a problem for areas where EV charging stations are sparse, there exist locations in the world (such as Northern Europe) where the density of stations is very high. In such locations, adding an edge for every pair of stations between which an EV can travel quickly grows to billions of possible edges.\n\nHowever, this high density implies that a trip between two stations that are relatively far apart will undoubtedly pass through multiple other stations. In this case, maintaining information about the long edge is redundant, making it possible to simply add the smaller edges () in the graph, resulting in sparser, more computationally feasible, graphs. \n\nThe spanner construction algorithm is a direct generalization of the . The trips between charging stations are sorted from fastest to slowest and are processed in that order. For each trip between points  and , the algorithm examines whether smaller subtrips already included in the spanner subsume the direct trip. To do so it compares the trip time and battery consumption that can be achieved using subtrips already in the spanner, against the same quantities for the direct - route. If they are found to be within a tiny error threshold, the direct trip from  to  is not added to the spanner, otherwise it is. Applying this sparsification algorithm has a notable impact and allows the graph to be served efficiently in responding to users\u2019 routing requests.\n\n\nIn this work we engineer a scalable solution for routing EVs on long trips to include access to charging stations through the use of graph sparsification and novel framing of standard routing algorithms. We are excited to put algorithmic ideas and techniques in the hands of Maps users and look forward to serving stress-free routes for EV drivers across the globe!", "date": "\nWednesday, January 27, 2021\n"},
{"website": "Google-AI", "title": "\nStabilizing Live Speech Translation in Google Translate\n", "author": ["Posted by Naveen Arivazhagan, Senior Software Engineer and Colin Cherry, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/01/stabilizing-live-speech-translation-in.html", "abstract": "The may be used to create a live, translated transcription for events like meetings and speeches, or simply for a story at the dinner table in a language you don\u2019t understand. In such settings, it is useful for the translated text to be displayed promptly to help keep the reader engaged and in the moment.\n\nHowever, with early versions of this feature the translated text suffered from multiple real-time revisions, which can be distracting. This was because of the non-monotonic relationship between the source and the translated text, in which words at the end of the source sentence can influence words at the beginning of the translation.\n\nToday, we are excited to describe some of the technology behind a recently released update to the transcribe feature in the Google Translate app that significantly reduces translation revisions and improves the user experience. The research enabling this is presented in two papers. The  formulates an evaluation framework tailored to live translation and develops methods to reduce instability.  The  demonstrates that these methods do very well compared to alternatives, while still retaining the simplicity of the original approach. The resulting model is much more stable and provides a noticeably improved reading experience within Google Translate.\n\n\nBefore attempting to make any improvements, it was important to first understand and quantifiably measure the different aspects of the user experience, with the goal of maximizing quality while minimizing latency and instability. In \u201c\u201d, we developed an evaluation framework for live-translation that has since guided our research and engineering efforts. This work presents a performance measure using the following metrics:\n\nIt is important to recognize the inherent trade-offs between these different aspects of quality. enables live-translation by stacking machine translation on top of real-time automatic speech recognition. For each update to the recognized transcript, a fresh translation is generated in real time; several updates can occur each second. This approach placed at one extreme of the 3 dimensional quality framework: it exhibited minimal  and the best , but also had high . Understanding this allowed us to work towards finding a better balance.\n\n\nOne straightforward solution to reduce erasure is to decrease the frequency with which translations are updated. Along this line, \u201cstreaming translation\u201d models (for example,  and ) intelligently learn to recognize when sufficient source information has been received to extend the translation safely, so the translation never needs to be changed. In doing so, streaming translation models are able to achieve zero erasure.\n\nThe downside with such streaming translation models is that they once again take an extreme position: zero erasure necessitates sacrificing BLEU and lag. Rather than eliminating erasure altogether, a small budget for occasional instability may allow better BLEU and lag. More importantly, streaming translation would require retraining and maintenance of specialized models specifically for live-translation. This precludes the use of streaming translation in some cases, because keeping a lean pipeline is an important consideration for a product like Google Translate that supports 100+ languages.\n\nIn our second paper, \u201c\u201d, we show that our original \u201cre-translation\u201d approach to live-translation can be fine-tuned to reduce erasure and achieve a more favorable erasure/lag/BLEU trade-off. Without training any specialized models, we applied a pair of inference-time heuristics to the original machine translation models \u2014  and .\n\nThe end of an on-going translation tends to flicker because it is more likely to have dependencies on source words that have yet to arrive. We reduce this by truncating some number of words from the translation until the end of the source sentence has been observed. This  process thus trades latency for stability, without affecting quality. This is very similar to delay-based strategies used in streaming methods such as , but applied only during inference and not during training.\n\nNeural machine translation often \u201csee-saws\u201d between equally good translations, causing unnecessary erasure. We improve stability by  the output towards what we have already shown the user. On top of reducing erasure, biasing also tends to reduce lag by stabilizing translations earlier. Biasing interacts nicely with masking, as masking words that are likely to be unstable also prevents the model from biasing toward them. However, this process does need to be tuned carefully, as a high bias, along with insufficient masking, may have a negative impact on quality.\n\nThe combination of masking and biasing, produces a re-translation system with high quality and low latency, while virtually eliminating erasure. The table below shows how the metrics react to the heuristics we introduced and how they compare to the other systems discussed above. The graph demonstrates that even with a very small erasure budget, re-translation surpasses zero-flicker streaming translation systems (MILk and Wait-k) trained specifically for live-translation.\n\n\nThe solution outlined above returns a decent translation very quickly, while allowing it to be revised as more of the source sentence is spoken. The simple structure of re-translation enables the application of our best speech and translation models with minimal effort. However, reducing erasure is just one part of the story \u2014 we are also looking forward to improving the overall speech translation experience through new technology that can reduce lag when the translation is spoken, or that can enable better transcriptions when multiple people are speaking.", "date": "\nTuesday, January 26, 2021\n"},
{"website": "Google-AI", "title": "\nImproving Indian Language Transliterations in Google Maps\n", "author": ["Posted by Cibu Johny, Software Engineer, Google Research and Saumya Dalal, Product Manager, Google Geo"], "link": "http://ai.googleblog.com/2021/01/improving-indian-language.html", "abstract": "Nearly 75% of India\u2019s population \u2014 which possesses the second highest number of internet users in the world \u2014  primarily using Indian languages, rather than English. Over the next five years, that number is . In order to make Google Maps as accessible as possible to the next billion users, it must allow people to use it in their preferred language, enabling them to explore anywhere in the world. \n\nHowever, the names of most Indian places of interest (POIs) in Google Maps are not generally available in the native scripts of the languages of India. These names are often in English and may be combined with acronyms based on the Latin script, as well as Indian language words and names. Addressing such mixed-language representations requires a  system that maps characters from one script to another, based on the source and target languages, while accounting for the phonetic properties of the words as well.\n\nFor example, consider a user in Ahmedabad, Gujarat, who is looking for a nearby hospital, KD Hospital. They issue the search query, \u0a95\u0ac7\u0aa1\u0ac0 \u0ab9\u0acb\u0ab8\u0acd\u0aaa\u0abf\u0a9f\u0ab2, in the native script of Gujarati, the 6th most widely spoken language in India. Here, \u0a95\u0ac7\u0aa1\u0ac0 (\u201c\u201d) is the sounding out of the acronym KD, and \u0ab9\u0acb\u0ab8\u0acd\u0aaa\u0abf\u0a9f\u0ab2 is \u201chospital\u201d. In this search, Google Maps knows to look for hospitals, but it doesn't understand that \u0a95\u0ac7\u0aa1\u0ac0 is KD, hence it finds another hospital, CIMS. As a consequence of the relative sparsity of names available in the Gujarati script for places of interest (POIs) in India, instead of their desired result, the user is shown a result  that is further away. \n\n\nOur goal was to design a system that will transliterate from a reference Latin script name into the scripts and orthographies native to the above-mentioned languages. For example, the Devanagari script is the native script for both Hindi and Marathi (the language native to Nagpur, Maharashtra). Transliterating the Latin script names for  and , both POIs in Nagpur, results in \u090f\u0928\u0906\u0908\u091f\u0940 \u0917\u093e\u0930\u094d\u0921\u0928 and \u091a\u0902\u0926\u094d\u0930\u092e\u0923\u0940 \u0917\u093e\u0930\u094d\u0921\u0928, respectively, depending on the specific language\u2019s orthography in that script. \n\nIt is important to note that the transliterated POI names are  translations. Transliteration is only concerned with writing the same words in a different script, much like an English language newspaper might choose to write the name \u0413\u043e\u0440\u0431\u0430\u0447\u0451\u0432 from the Cyrillic script as \u201cGorbachev\u201d\u00a0for their readers who do not read the Cyrillic script. For example, the second word in both of the transliterated POI names above is still pronounced \u201cgarden\u201d, and the second word of the Gujarati example earlier is still \u201chospital\u201d \u2014 they remain the English words \u201cgarden\u201d\u00a0and \u201chospital\u201d, just written in the other script. Indeed, common English words are frequently used in POI names in India, even when written in the native script. How the name is written in these scripts is largely driven by its pronunciation; so \u090f\u0928\u0906\u0908\u091f\u0940 from the acronym  is pronounced \u201c\u201d, not as the English word \u201c\u201d. Knowing that NIT is a common acronym from the region is one piece of evidence that can be used when deriving the correct transliteration.\n\nNote also that, while we use the term , following convention in the NLP community for mapping directly between writing systems, romanization in South Asian languages regardless of the script is generally pronunciation-driven, and hence one could call these methods  rather than transliteration.  The task remains, however, mapping between scripts, since pronunciation is only relatively coarsely captured in the Latin script for these languages, and there remain many script-specific correspondences that must be accounted for.  This, coupled with the lack of standard spelling in the Latin script and the resulting variability, is what makes the task challenging.\n\n\nWe use an ensemble of models to automatically transliterate from the reference Latin script name (such as  or ) into the scripts and orthographies native to the above-mentioned languages. Candidate transliterations are derived from a pair of (seq2seq)models. One is a  model for general text transliteration, trained in a manner similar to models  for . The other is a neural   (LSTM) model trained, in part, on the publicly released  This dataset contains Latin and native script data drawn from Wikipedia in 12 South Asian languages, including all but one of the languages mentioned above, and permits training and evaluation of various transliteration methods. Because the two models have such different characteristics, together they produce a greater variety of transliteration candidates.\n\nTo deal with the tricky phenomena of acronyms (such as the \u201cNIT\u201d\u00a0and \u201cKD\u201d\u00a0examples above), we developed a specialized transliteration module that generates additional candidate transliterations for these cases.\n\nFor each native language script, the ensemble makes use of specialized romanization dictionaries of varying provenance that are tailored for place names, proper names, or common words.  Examples of such romanization dictionaries are found in the Dakshina dataset.\n\n\nThe ensemble combines scores for the possible transliterations in a weighted mixture, the parameters of which are tuned specifically for POI name accuracy using small targeted development sets for such names. \n\nFor each native script token in candidate transliterations, the ensemble also weights the result according to its  in a very large sample of on-line text. Additional candidate scoring is based on a deterministic romanization approach derived from the  romanization standard, which maps each native script token to a unique Latin script string. This string allows the ensemble to track certain key correspondences when compared to the original Latin script token being transliterated, even though the ISO-derived mapping itself does not always perfectly correspond to how the given native script word is typically written in the Latin script.\n\n  In aggregate, these many moving parts provide substantially higher quality transliterations than possible for any of the individual methods alone.\n\n\nThe following table provides the per-language quality and coverage improvements due to the ensemble over existing automatic transliterations of POI names. The  measures the increase in items for which an automatic transliteration has been made available.   measures the ratio of updated transliterations that were judged to be improvements versus those that were judged to be inferior to existing automatic transliterations.\n\n\nAs with any machine learned system, the resulting automatic transliterations may contain a few errors or infelicities, but the large increase in coverage in these widely spoken languages marks a substantial expansion of the accessibility of information within Google Maps in India.  Future work will include using the ensemble for transliteration of other classes of entities within Maps and its extension to other languages and scripts, including Perso-Arabic scripts, which are also commonly used in the region.", "date": "\nFriday, January 22, 2021\n"},
{"website": "Google-AI", "title": "\nRxR: A Multilingual Benchmark for Navigation Instruction Following\n", "author": ["Posted by Alexander Ku, Software Engineer and Peter Anderson, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2021/01/rxr-multilingual-benchmark-for.html", "abstract": "A core challenge in machine learning (ML) is to build agents that can navigate complex human environments in response to spoken or written commands. While today\u2019s agents, including robots, can often navigate complicated environments, they cannot yet understand navigation goals expressed in natural language, such as, \u201cG and \nThis challenge, referred to as  (VLN), demands a sophisticated understanding of spatial language. For example, the ability to identify the position \u201c\u201drequires finding the table, identifying which part of the table is considered to be the \u201chead\u201d, finding the chair closest to the head, identifying the area behind this chair and so on. While people can follow these instructions easily, these challenges cannot be easily solved with current ML-based methods, requiring systems that can better connect language to the physical world it describes. \n\nTo help spur progress in this area, we are excited to introduce  (RxR), a new dataset for VLN. Described in \u201c\u201d, RxR is the first multilingual dataset for VLN, containing 126,069 human-annotated navigation instructions in three typologically diverse languages \u2014 English, Hindi and Telugu. Each instruction describes a path through a photorealistic simulator populated with indoor environments from the , which includes 3D captures of homes, offices and public buildings. To track progress on VLN, we are also announcing the , a competition that encourages the machine learning community to train and evaluate their own instruction following agents on RxR instructions.\n\nExamples of English, Hindi and Telugu navigation instructions from the RxR dataset. Each navigation instruction describes the same path.\n\n\nIn addition to navigation instructions and paths, RxR also includes a new, more detailed multimodal annotation called a . Inspired by the mouse traces captured in the  dataset, pose traces provide dense groundings between language, vision and movement in a rich 3D setting. To generate navigation instructions, we ask  annotators to move along a path in the simulator while narrating the path based on the surroundings. The pose trace is a record of everything the guide sees along the path, time-aligned with the words in the navigation instructions. These traces are then paired with pose traces from  annotators, who are tasked with following the intended path by listening to the guide\u2019s audio, thereby validating the quality of the navigation instructions. Pose traces implicitly capture notions of landmark selection and , and represent a play-by-play account of how to solve the navigation instruction generation task (for guides) and the navigation instruction following task (for followers).\n\n\nIn total, RxR contains almost 10 million words, making it around 10 times larger than existing datasets, such as  and /. This is important because, in comparison to tasks based on static image and text data, language tasks that require learning through movement or interaction with an environment typically suffer from a lack of large-scale training data. RxR also addresses known biases in the construction of the paths that have arisen in other datasets, such as R2R in which all paths have similar lengths and take the shortest route to the goal. In contrast, the paths in RxR are on average longer and less predictable, making them more challenging to follow and encouraging models trained on the dataset to place greater emphasis on the role of language in the task. The size, scope and detail of RxR will expand the frontier for research on grounded language learning while reducing the dominance of high resource languages such as English. \n\n\nTo better characterize and understand the RxR dataset, we trained a variety of agents on RxR using our open source framework , and language representations from the  model. We found that results were improved by including follower annotations as well as guide annotations during training, and that independently trained monolingual agents outperformed a single multilingual agent. \n\nConceptually, evaluation of these agents is straightforward \u2014 did the agent follow the intended path? Empirically, we measure the similarity between the path taken by the VLN agent and the reference path using , a normalized measure of path fidelity that ranges between 100 (perfect correspondence) and 0 (completely wrong). The average score for the follower annotators across all three languages is 79.5, due to natural variation between similar paths. In contrast, the best model (a composite of three independently trained monolingual agents, one for each language) achieved an NDTW score on the RxR test set of 41.5. While this is much better than random (15.4), it remains far below human performance. Although advances in language modeling continue to rapidly erode the headroom for improvement in text-only language understanding benchmarks such as  and , benchmarks like RxR that connect language to the physical world offer substantial room for improvement. \n\n\nTo encourage further research in this area, we are launching the , an ongoing competition for the machine learning community to develop computational agents that can follow natural language navigation instructions. To take part, participants upload the navigation paths taken by their agent in response to the provided RxR test instructions. In the most difficult setting (reported here and in the ), all the test environments are previously unseen. However, we also allow for settings in which the agent is either trained in or explores the test environments in advance. For more details and the latest results please visit the .\n\n\nWe are also releasing the custom web-based annotation tool that we developed to collect the RxR dataset. The Panoramic Graph Environment Annotation toolkit (PanGEA), is a lightweight and customizable codebase for collecting speech and text annotations in panoramic graph environments, such as  and . It includes speech recording and virtual pose tracking, as well as tooling to align the resulting pose trace with a manual transcript. For more details please visit the .", "date": "\nThursday, January 21, 2021\n"},
{"website": "Google-AI", "title": "\nToTTo: A Controlled Table-to-Text Generation Dataset\n", "author": ["Posted by Ankur Parikh and Xuezhi Wang, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2021/01/totto-controlled-table-to-text.html", "abstract": "In the last few years, research in , used for tasks like text summarization, has made tremendous progress. Yet, despite achieving high levels of fluency, neural systems can still be prone to  (i.e.generating text that is understandable, but not faithful to the source), which can prohibit these systems from being used in many applications that require high degrees of accuracy. Consider an example from the , where the neural  tasked with summarizing a Wikipedia infobox entry for Belgian football player  summarizes incorrectly that he is an American figure skater.\n\nWhile the process of assessing the faithfulness of generated text to the source content can be challenging, it is often easier when the source content is structured (e.g., in tabular format). Moreover, structured data can also test a model\u2019s ability for reasoning and numerical inference. However, existing large scale structured datasets are often noisy (i.e., the reference sentence cannot be fully inferred from the tabular data), making them unreliable for the measurement of hallucination in model development.\nIn \u201c\u201d,  we present an open domain table-to-text generation dataset created using a novel annotation process (via sentence revision) along with a controlled text generation task that can be used to assess model hallucination. ToTTo (shorthand for \u201cTable-To-Text\u201d) consists of 121,000 training examples, along with 7,500 examples each for development and test. Due to the accuracy of annotations, this dataset is suitable as a challenging benchmark for research in high precision text generation. The dataset and code are open-sourced on . \n\n\nToTTo introduces a generation task in which a given Wikipedia table with a set of selected cells is used as the source material for the task of producing a single sentence description that summarizes the cell contents in the context of the table. The example below demonstrates some of the many challenges posed by the task, such as numerical reasoning, a large open-domain vocabulary, and varied table structure.\n\n\nDesigning an annotation process to obtain natural but also clean target sentences from tabular data  is a significant challenge. Many datasets like  and  pair naturally occurring text heuristically with tables, a noisy process that makes it difficult to disentangle whether hallucination is primarily caused by data noise or model shortcomings. On the other hand, one can elicit annotators to write sentence targets , which are faithful to the table, but the resulting targets often  in terms of structure and style.\n\nIn contrast, ToTTo is constructed using a novel data annotation strategy in which annotators revise existing Wikipedia sentences in stages. This results in target sentences that are clean, as well as natural, containing interesting and varied linguistic properties. The data collection and annotation process begins by collecting tables from Wikipedia, where a given table is paired with a summary sentence collected from the supporting page context according to heuristics, such as word overlap between the page text and the table and hyperlinks referencing tabular data. This summary sentence may contain information not supported by the table and may contain pronouns with antecedents found in the table only, not the sentence itself. \n\nThe annotator then highlights the cells in the table that support the sentence and deletes phrases in the sentence that are not supported by the table. They also decontextualize the sentence so that it is standalone (e.g., with correct pronoun resolution) and correct grammar, where necessary.\n\nWe show that annotators obtain high agreement on the above task: 0.856  for cell highlighting, and 67.0  for the final target sentence.\n\nWe conducted a topic analysis on the ToTTo dataset over 44 categories and found that the Sports and Countries topics, each of which consists of a range of fine-grained topics, e.g., football/olympics for sports and population/buildings for countries, together comprise 56.4% of the dataset. The other 44% is composed of a much more broad set of topics, including Performing Arts, Transportation, and Entertainment. \n\nFurthermore, we conducted a manual analysis of the different types of linguistic phenomena in the dataset over 100 randomly chosen examples. The table below summarizes the fraction of examples that require reference to the page and section titles, as well as some of the linguistic phenomena in the dataset that potentially pose new challenges to current systems.\n\n\nWe present some baseline results of three state-of-the-art models from the literature (, , and the ) on two evaluation metrics,  and . In addition to reporting the score on the  test set, we also evaluate each model on a more challenging subset consisting of out-of-domain examples. As the table below shows, the BERT-to-BERT model performs best in terms of both BLEU and PARENT. Moreover, all models achieve considerably lower performance on the challenge set indicating the challenge of out-of-domain generalization.\n\nWhile automatic metrics can give some indication of performance, they are not currently sufficient for evaluating hallucination in text generation systems. To better understand hallucination, we manually evaluate the top performing baseline, to determine how faithful it is to the content in the source table, under the assumption that discrepancies indicate hallucination. To compute the \u201cExpert\u201d performance, for each example in our multi-reference test set, we held out one reference and asked annotators to compare it with the other references for faithfulness. As the results show, the top performing baseline appears to hallucinate information ~20% of the time. \n\n\nIn the table below, we present a selection of the observed model errors to highlight some of the more challenging aspects of the ToTTo dataset. We find that state-of-the-art models struggle with hallucination, numerical reasoning, and rare topics, even when using cleaned references (errors in ). The last example shows that even when the model output is correct it is sometimes not as informative as the original reference which contains more reasoning about the table (shown in ).\n\n\nIn this work, we presented ToTTo, a large, English table-to-text dataset that presents both a controlled generation task and a data annotation process based on iterative sentence revision. We also provided several state-of-the-art baselines, and demonstrated ToTTo could be a useful dataset for modeling research as well as for developing evaluation metrics that can better detect model improvements. \n\nIn addition to the proposed task, we hope our dataset can also be helpful for other tasks such as  and sentence revision. ToTTo is available at our .", "date": "\nFriday, January 15, 2021\n"},
{"website": "Google-AI", "title": "\nRecognizing Pose Similarity in Images and Videos\n", "author": ["Posted by Jennifer J. Sun, Student Researcher and Ting Liu, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2021/01/recognizing-pose-similarity-in-images.html", "abstract": "Everyday actions, such as jogging, reading a book, pouring water, or playing sports, can be viewed as a sequence of , consisting of the position and orientation of a person\u2019s body. An understanding of poses from images and videos is a crucial step for enabling a range of applications, including  display, , and . However, a 3-dimensional pose captured in two dimensions in images and videos appears different depending on  the viewpoint of the camera. The ability to recognize similarity in 3D pose using only 2D information will help vision systems better understand the world.\n\nIn \u201c\u201d (Pr-VIPE), a spotlight paper at , we present a new algorithm for human pose perception that recognizes similarity in human body poses across different camera views by mapping  to a view-invariant embedding space. This ability enables tasks, such as pose retrieval, action recognition, action video synchronization, and more. Compared to  that directly map 2D pose keypoints to 3D pose keypoints, the Pr-VIPE embedding space is (1) view-invariant, (2)  in order to capture 2D input ambiguity, and (3) does not require  during training or inference. Trained with in-lab setting data, the model works on in-the-wild images out of the box, given a reasonably good 2D pose estimator (e.g., , , among others). The model is simple, results in compact embeddings, and can be trained (in ~1 day) using 15 CPUs. We have released the code on . \n\n\nThe input to Pr-VIPE is a set of 2D keypoints, from any 2D pose estimator that produces a minimum of , and the output is the  of the pose embedding. The distances between embeddings of 2D poses correlate to their similarities in absolute 3D pose space. Our approach is based on two observations:\n\nThe first observation motivates the need for view-invariance. To accomplish this, we define the , i.e., the likelihood that different 2D poses were projected from the same, or similar 3D poses. The matching probability predicted by Pr-VIPE for matching pose pairs should be higher than for non-matching pairs.\n\nTo address the second observation, Pr-VIPE utilizes a probabilistic embedding formulation. Because many 3D poses can project to the same or similar 2D poses, the model input exhibits an inherent ambiguity that is difficult to capture through  mapping point-to-point in embedding space. Therefore, we map a 2D pose through a probabilistic mapping to an embedding distribution, of which we use the variance to represent the uncertainty of the input 2D pose. As an example, in the figure below the third 2D view of the 3D pose on the left is similar to the first 2D view of a different 3D pose on the right, so we map them into a similar location in the embedding space with large variances.\n\n\nWe propose a new  benchmark to evaluate the view-invariance property of the embedding. Given a monocular pose image, cross-view retrieval aims to retrieve the same pose from different views without using camera parameters. The results demonstrate that Pr-VIPE retrieves poses more accurately across views compared to baseline methods in both evaluated datasets (, ).\n\nCommon 3D pose estimation methods (such as the  used for comparison above, , and , amongst many others), predict 3D poses in camera coordinates, which are not directly view-invariant. Thus,  between every query-index pair is required for retrieval using estimated 3D poses, which is computationally expensive due to the need for  (SVD). In contrast, Pr-VIPE embeddings can be directly used for distance computation in Euclidean space, without any post-processing.\n\n\nView-invariant pose embedding can be applied to many image and video related tasks. Below, we show Pr-VIPE applied to cross-view retrieval on in-the-wild images without using camera parameters.\nThe same Pr-VIPE model can also be used for video alignment. To do so, we stack Pr-VIPE embeddings within a small time window, and use the  (DTW) algorithm to align video pairs. \n\nThe video alignment distance calculated via DTW can then be used for action recognition by classifying videos using . We evaluate the Pr-VIPE embedding using the  dataset and demonstrate that using the Pr-VIPE embedding without fine-tuning on the target dataset, yields highly competitive recognition accuracy. In addition, we show that Pr-VIPE even achieves relatively accurate results using only videos from a single view in the index set.\n\n\nWe introduce the Pr-VIPE model for mapping 2D human poses to a view-invariant probabilistic embedding space, and show that the learned  embeddings can be directly used for pose retrieval, action recognition, and video alignment. Our cross-view retrieval benchmark can be used to test the view-invariant property of other embeddings. We look forward to hearing about what you can do with pose embeddings!", "date": "\nThursday, January 14, 2021\n"},
{"website": "Google-AI", "title": "\nGoogle Research: Looking Back at 2020, and Forward to 2021\n", "author": ["Posted by Jeff Dean, Senior Fellow and SVP of Google Research and Health, on behalf of the entire Google Research community"], "link": "http://ai.googleblog.com/2021/01/google-research-looking-back-at-2020.html", "abstract": "When I joined Google over 20 years ago, we were just figuring out how to really start on the journey of making a high quality and comprehensive search service for information on the web, using . Fast forward to today, and while we\u2019re taking on a much broader array of technical challenges, it\u2019s still with the same overarching goal of organizing the world's information and making it universally accessible and useful. In 2020, as the world has been reshaped by COVID-19, we saw the ways research-developed technologies could help billions of people better communicate, understand the world, and get things done. I\u2019m proud of what we\u2019ve accomplished, and excited about new possibilities on the horizon.\n\nThe goal of  is to work on long-term, ambitious problems across a wide range of important topics \u2014  from predicting the spread of COVID-19, to designing algorithms, to learning to translate more and more languages automatically, to mitigating bias in ML models. In the spirit of our annual reviews for , , and more narrowly focused reviews of some work in  and , this post covers key Google Research highlights from this unusual year. For a more comprehensive look, please see our . This is a long post, but is grouped into many different sections, which you can jump to directly using the table below. Hopefully, there\u2019s something interesting in here for everyone! \n\n\nAs the impact of COVID-19 took a tremendous toll on people\u2019s lives, researchers and developers around the world rallied together to develop tools and technologies to help public health officials and policymakers understand and respond to the pandemic.  to develop the Exposure Notifications System (ENS), a Bluetooth-enabled privacy-preserving technology that allows people to be notified if they have been exposed to others who have tested positive for COVID-19. ENS supplements traditional contact tracing efforts and has been deployed by public health authorities in more than 50 countries, states and regions to help curb the spread of infection.\n\nIn the early days of the pandemic, public health officials signalled their need for more comprehensive data to combat the virus\u2019 rapid spread. Our , which provide anonymized insights into movement trends, are helping researchers not only understand the  like stay-at-home directives and social distancing, and also conduct . \n\nOur own researchers have also explored using this anonymized data tousing graph neural networks instead of traditional time series-based models.\n\nAlthough the research community knew little about this disease and secondary effects initially, we\u2019re learning more every day. Our  allows researchers to explore temporal or symptomatic associations, such as anosmia \u2014 the loss of smell that is sometimes a symptom of the virus. To further support the broader research community, we launched  to provide the public ways to participate in research studies.\n\nTeams across Google are contributing tools and resources to the , which is working to address the health and economic impacts of the virus.\n\nAccurate information is critical in dealing with public health threats. We collaborated with many product teams at Google in order to improve information quality about COVID-19 in Google News and Search through supporting , as well as similar efforts in .\n\nWe helped multilingual communities get equal access to critical COVID-19 information by  and .\n\nModelling a complex global event is particularly challenging and requires more comprehensive , the development of  and  to inform the public health response. Machine learning techniques have also helped in other ways from deploying  to helping researchers quickly navigate the mountains of COVID-19 scientific literature, applying  to protect privacy while making useful datasets available, and exploring whether public health can conduct faster screening with fewer tests via .\n\nThese are only a sample of the many pieces of work that happened across Google to help users and public health authorities respond to COVID-19. For more, see .\n\n\nWe continue to make headway helping clinicians harness the power of ML to deliver better care for more patients. This year we have described notable advances in applying computer vision to aid doctors in the diagnosis and management of cancer, including helping to make sure that doctors , and showing that an ML system  tissue, enabling radiologists .\n\nWe\u2019ve also been working on systems to help identify , help detect  (the leading cause of blindness in the U.S. and U.K., and the third-largest cause of blindness worldwide), and on potential novel non-invasive diagnostics (e.g., being able to detect signs of ).\n\nThis year has also brought exciting demonstrations of how these same technologies can peer into the human genome. Google\u2019s open-source tool, DeepVariant, identifies genomic variants in sequencing data using a convolutional neural network, and this year won the . Using this same tool, a  improved diagnostic yield by 14% for genetic variants that lead to prostate cancer and melanoma in a cohort of 2,367 cancer patients. \n\nResearch doesn\u2019t end at measurement of experimental accuracy. Ultimately, truly helping patients receive better care requires understanding how ML tools will affect people in the real world. This year we began work with Mayo Clinic to develop a machine learning system to assist in  and to better understand how this technology could be deployed into clinical practice. With our partners in Thailand, we\u2019ve used diabetic eye disease screening as a test case in how we can build systems with , and recognize in building tools for a healthier world.\n\n\nMachine learning can help us better understand the environment and make useful predictions to help people in both their everyday life as well as in disaster situations. For weather and precipitation forecasting, computationally intensive physics-based models like NOAA\u2019s  have long reigned supreme. We have been able to show, though, that ML-based forecasting systems  (\u201c and not just \u201c) and  that are considerably more accurate than HRRR, and can compute the forecast more quickly, yet with higher temporal and spatial resolution.\n\nWe\u2019ve also , which uses a network of neural networks to model the actual river systems in the world to more accurately understand the interactions of upstream water levels to downstream inundation, resulting in more accurate water-level predictions and flood forecasting. Using these techniques, we've expanded , helping to better protect more than 200 million people in 250,000 square kilometers. \n\nBetter analysis of satellite imagery data can also give Google users a  (which caused devastating effects in California and Australia this year). We showed that automated analysis of satellite imagery can help with  even with . It can also aid  by helping cities assess their current tree canopy coverage and where they should focus on planting new trees. We\u2019ve also shown how  can help improve ecological and wildlife monitoring.\n\nBased on this work, we\u2019re excited to  on using AI and ML to amplify NOAA\u2019s environmental monitoring, weather forecasting and climate research using Google Cloud\u2019s infrastructure.\n\n\nMachine learning continues to provide amazing opportunities for improving accessibility, because it can learn to transfer one kind of sensory input into others. As one example, we released , an Android application that can help visually impaired users by identifying packaged foods, both in a grocery store and also in their kitchen cupboard at home. The  demonstrates that a powerful-but-compact machine learning model can accomplish this in real-time on a phone for nearly 2 million products.\n\nSimilarly, people who communicate with sign language find it difficult to use video conferencing systems because even if they are signing, they are not detected as actively speaking by audio-based speaker detection systems.  presents a real-time sign language detection model and demonstrates how it can be used to provide video conferencing systems with a mechanism to identify the person signing as the active speaker.\n\n\nWe also enabled useful Android accessibility capabilities such as  and  for important household sounds.\n\n was expanded to support calls on the Pixel phone with the ability to caption phone calls and video calls. This came out of the  research project, which enables .\n\n\nMachine learning continues to prove vital in helping us make progress across many fields of science. In 2020, in collaboration with the  team at HHMI , we , the large synapse-resolution map of brain connectivity, reconstructed using large-scale machine learning models applied to high-resolution electron microscope imaging of brain tissue. This connectome information will aid neuroscientists in a wide variety of inquiries, helping us all better understand how brains function. Be sure to check out the !\n\nThe application of ML to problems in systems biology is also on the rise. Our  team, in collaboration with our colleagues at Calico, have been , to get a better understanding of how genes work together as a whole system. We\u2019ve also been exploring how to use  like DNA or proteins that have desirable properties for medical or industrial uses. Model-based RL is used to improve sample efficiency. At each round of experimentation the policy is trained offline using a simulator fit on functional measurements from prior rounds. On various tasks like designing DNA transcription factor binding sites, designing antimicrobial proteins, and optimizing the energy of  based on protein structures, we find that model-based RL is an attractive alternative to existing methods.\n\nIn partnership with  and , we have also been . Previous work in this area has tended to focus on relatively small sets of related compounds, but in this work, we are trying to use DNA-encoded small molecule libraries in order to be able to generalize to find \u201chits\u201d across a wide swath of chemical space, reducing the need for slower, physical-based lab work in order to progress from idea to working pharmaceutical.\n\nWe\u2019ve also seen success applying machine learning to core computer science and computer systems problems, a growing trend that is spawning entire new conferences like . In , a neural network-based language model predicts context-sensitive per-allocation site object lifetime information, and then uses this to organize the heap so as to reduce fragmentation. It is able to reduce fragmentation by up to 78% while only using huge pages (which are better for  behavior).  described an end-to-end transferable deep reinforcement learning method for computational graph optimization that shows 33%-60% speedup on three graph optimization tasks compared to  default optimization, with 15x faster convergence over prior computation graph optimization methods.\n\nAs described in , we have also been applying reinforcement learning to the problem of place-and-route in computer chip design. This is normally a very time-consuming, labor-intensive process, and is a major reason that going from an idea for a chip to actually having a fully designed and fabricated chip takes so long. Unlike prior methods, our approach has the ability to learn from past experience and improve over time. In particular, as we train over a greater number of chip blocks, our method becomes better at rapidly generating optimized placements for previously unseen chip blocks. The system is able to generate placements that usually outperform those of human chip design experts, and we have been using this system (running on TPUs) to do placement and layout for major portions of future generations of TPUs.  is a recent infrastructure we\u2019ve built for large-scale distributed reinforcement learning that is yielding promising performance for difficult RL tasks such as chip design.\n\n\nThe  guide our development of advanced technologies. We continue to invest in  and , update our  in this area, and  \u2014 including a 2020  and  \u2014 on our progress in implementation. \n\nTo help better understand the behavior of language models, we developed the \u00a0(LIT), a toolkit for better interpretability of language models, enabling interactive exploration and analysis of their decisions. We developed techniques for  and We used the kernel trick . To help non-specialists interpret machine learning results, we extended the  introduced in 2019 to now provide a . With the original TCAV work, we were able to say that \u2018fur\u2019 and \u2018long ears\u2019 are important concepts for \u2018rabbit\u2019 prediction. With this work, we can also say that these two concepts are enough to fully explain the prediction; you don\u2019t need any other concepts.  are a technique to make models more interpretable by training them so that one of the layers is aligned with pre-defined expert concepts (e.g., \u201cbone spurs present\u201d, or \u201cwing color\u201d, as shown below) before making a final prediction for a task, so that we can not only interpret but also turn on/off these concepts on the fly.\n\nIn collaboration with many other institutions, we also looked into , showing that training data extraction attacks are realistic threats on state-of-the-art large language models. This finding along with a result that  can have significant privacy implications (especially for models trained on private data). In , we demonstrated that attackers with only API access to a language model could create models whose outputs had very high correlation with the original model, even with relatively few API queries to the original model. Subsequent work demonstrated that attackers can . On the AI Principle of safety we demonstrated that thirteen published  despite attempting to perform evaluations using adaptive attacks. Our work focuses on laying out the methodology and the approach necessary to perform an adaptive attack, and thus will allow the community to make further progress in building more robust models.\n\nExamining the way in which machine learning systems themselves are examined is also an important area of exploration. In collaboration with the , we defined a , drawing on lessons from the aerospace, medical devices, and finance industries and their best practices. In joint work with University of Toronto and MIT, we identified . In joint work with the University of Washington, we identified some for evaluating algorithmic fairness. As an initial step in making responsible AI work for the next billion users and to help understand if notions of fairness were consistent in different parts of the world, we analyzed and created a , accounting for datasets, fairness optimizations, infrastructures, and ecosystems\n\nThe  work that was introduced in collaboration with the University of Toronto in 2019 has been growing in influence.  Indeed, many well-known models like OpenAI\u2019s  and , many of Google\u2019s  and various  have all adopted Model Cards as a way of giving users of a machine learning model more information about the model\u2019s development and the observed behavior of the model under different conditions. To make this easier for others to adopt for their own machine learning models, we also introduced the  for easier model transparency reporting. In order to increase transparency in ML development practices, we demonstrate the applicability of a range of , including data requirements specification and data acceptance testing. \n\nIn collaboration with the U.S. National Science Foundation (NSF), we . We also released the , a new regularization technique available in the  for effectively and efficiently mitigating unfair biases when training ML models, along with  for building simple simulations that explore potential long-run impacts of deploying machine learning-based decision systems in social environments. \n\nIn addition to developing frameworks for fairness, we developed approaches for identifying and improving the health and quality of experiences with Recommender Systems, including using . We also continue to work on improving the reliability of our machine learning systems, where we\u2019ve seen that approaches such as  can improve robustness and that .\n\n is a way to formally quantify privacy protections and requires a rethinking of the most basic algorithms to operate in a way that they do not leak information about any particular individual. In particular, differential privacy can help in addressing memorization effects and information leakage of the kinds mentioned above. In 2020 there were a number of exciting developments, from more efficient ways of computing private  to  with tight approximation guarantees and . We also  that lie at the core of our internal tools, taking  to protect against leakage caused by the floating point representation of real numbers. These are the exact same tools that we use to produce differentially private  that have been a valuable source of anonymous data for researchers and policymakers. \n\nTo help developers assess the privacy properties of their classification models we released an . We hope this library will be the starting point of a robust privacy testing suite that can be used by any machine learning developer around the world.\n\nIn addition to pushing the state of the art in developing private algorithms, I am excited about the advances we made in weaving privacy into the fabric of our products. One of the best examples is Chrome\u2019s , which changes the underpinnings of the advertising ecosystem and helps systematically protect individuals\u2019 privacy. As part of the project, we proposed and  a number of different APIs, including federated learning of cohorts () for interest based targeting, and aggregate APIs for differentially . \n\nLaunched in 2017,  is now a complete research field unto itself, with over  on federated learning appearing in 2020 alone. Our cross-institutional  survey paper published in 2019 has been cited  in the past year, and an updated version will soon  in the Foundations & Trends in Machine Learning series. In July, we hosted a Workshop on Federated Learning and Analytics, and made  and a  publicly available.\n\nWe continue to push the state of the art in federated learning, including the development of new federated optimization algorithms including , , and , substantial improvements in complimentary , and more. We announced and deployed , enabling data science over raw data that is stored locally on users\u2019 devices. New uses of federated learning in Google products include  in Gboard, and pioneering privacy-preserving medical research with . Furthermore, in  we presented the first privacy accounting mechanism for Federated Learning.\n\nSecurity for our users is also an area of considerable interest for us. In 2020, we continued to improve protections for Gmail users, by deploying a  that provides protection against malicious documents, which increased malicious office document detection by 10% on a daily basis. Thanks to its ability to generalize, this tool has been very effective at blocking some adversarial malware campaigns that elude other detection mechanisms and increased our detection rate by 150% in some cases. \n\nOn the account protection side, we released to help advance state of art in the two factor authentication space, staying focused on security keys as the best way to protect accounts against phishing. \n\n\nBetter understanding of language is an area where we saw considerable progress this year. Much of the work in this space from Google and elsewhere now relies on , a particular style of neural network model  (but with a growing body of evidence that they are also useful for , , , , and a wide variety of other domains). \n\nOne area of excitement is in dialog systems that can chat with a user about something of interest, often encompassing multiple turns of interaction. While successful work in this area to date has involved creating systems that are specialized around particular topics (e.g., ) these systems cannot carry on general conversations. In pursuit of the general research goal of creating systems capable of much more open-ended dialog, in 2020 we described , a learned conversational agent that aspirationally can chat about anything. Meena achieves high scores on a dialog system metric called SSA, which measures both sensibility and specificity of responses. We\u2019ve seen that as we scale up the model size of Meena, it is able to achieve lower perplexity and, as shown in the , lower perplexity correlates extremely closely with improved SSA.\n\nOne well-known issue with generative language models and dialog systems is that when discussing factual data, the model\u2019s capacity may not be large enough to remember every specific detail about a topic, so they generate language that is plausible but incorrect. (This is not unique to machines \u2014 people can commit these errors too.) To address this in dialog systems, we are exploring ways to augment a conversational agent by giving it access to external information sources (e.g., a large corpus of documents or a search engine API), and developing learning techniques to use this as an additional resource in order to generate language that is consistent with the retrieved text. Work in this area includes  (and a key underlying technology for this to work well is something like , to efficiently match the desired information to information in the corpus of text). Once appropriate content is found, it can be better understood with approaches like \u00a0and . Our work on  can also help to create automatic summaries from any piece of text, a general technique useful in conversations, retrieval systems, and many other places.\n\nEfficiency of NLP models has also been a significant focus for our work in 2020. Techniques like transfer learning and multi-task learning can dramatically help with making general NLP models usable for new tasks with modest amounts of computation. Work in this vein includes , sparse activation of models (as in our  mentioned below), and . Several threads of work also look to improve on the basic Transformer architecture, including , which uses locality-sensitive hashing and reversible computation to more efficiently support much larger attention windows, , which use an approach for attention that scales linearly rather than quadratically (and discusses its use in the context of protein modeling), and  and , which utilize global and sparse random connections, to enable linear scaling for larger and structured sequences. We also explored techniques for  that are 100x smaller than a larger BERT model, but perform nearly as well for some tasks, making them very suitable for on-device NLP. In , we also explored new approaches for generative text models that use edit operations rather than fully general text generation, which can have advantages in computation requirements for generation, more control over the generated text, and require less training data.\n\n\nEffective language translation helps bring the world closer together by enabling us to all communicate, despite speaking different languages. To date, over a billion people around the world use Google Translate, and last year we added support for  (Kinyarwanda, Odia, Tatar, Turkmen and Uyghur, collectively spoken by 75 million people). , showing an average +5  point gain across more than 100 languages from May 2019 to May 2020, through a wide variety of techniques like improved model architectures and training, better handling of noise in datasets, multilingual transfer and multi-task learning, and better use of monolingual data to improve low-resource languages (those without much written public content on the web), directly in line with our goals of improving ML fairness of machine learning systems to provide benefits to the greatest number of people possible.\n\nWe strongly believe that continued scaling of multilingual translation models will bring further quality improvements, especially to the billions of speakers of low-resource languages around the world. In , Google researchers showed that training sparsely-activated multilingual translation models of up to 600 billion parameters leads to major improvements in translation quality for 100 languages as measured by BLEU score improvement over a baseline of a separate 400M parameter monolingual baseline model for each language. Three trends stood out in this work, illustrated by Figure 6 in the paper, reproduced below (see the paper for complete discussion): \n\nWe\u2019re actively working on bringing the benefits demonstrated in this GShard research work to Google Translate, as well as training single models that cover 1000 languages, including languages like Dhivehi and Sudanese Arabic (while sharing  along the way).\n\nWe also developed techniques to create , which can help with developing better translation models. To more effectively evaluate translation quality, we introduced  for tasks like translation that considers the semantics of the generated text, rather than just the amount of word overlap with ground-truth data, illustrated in the table below.\n\n\nWe continue to develop new machine learning algorithms and approaches for training that enable systems to learn more quickly and from less supervised data. By , we find that we can fill idle time on ML accelerators and therefore can train neural networks faster. By , we can find better solutions compared with statically-connected neural networks. We also developed , a new self-supervised and semi-supervised learning technique that simultaneously maximizes agreement between differently transformed views of the same image and minimizes agreement between transformed views of different images. This approach significantly improves on the best self-supervised learning techniques.\n\nWe also extended the idea of , resulting in a loss function that significantly improves over cross-entropy for supervised classification problems.\n\n\nReinforcement learning (RL), which learns to make good long-term decisions from limited experience, has been an important focus area for us. An important challenge in RL is to learn to make decisions from few data points, and we\u2019ve improved RL algorithm efficiency through learning from fixed datasets, learning from other agents, and improving exploration. \n\nA major focus area this year has been around RL, which ,  (for example, from previous experiments or human demonstrations), extending RL to the applications that can\u2019t collect training data on-the-fly. We\u2019ve , developed improved algorithms for , estimating  , and . In addition, we\u2019re collaborating with the broader community to tackle these problems by releasing , and . \n\nAnother line of research improved sample efficiency by learning from other agents through apprenticeship learning. We developed methods to learn from , , or learning from . To improve the exploration in RL, we explored  methods including imitation techniques able to  arising in agents having prior knowledge about their environment. \n\nWe\u2019ve also made significant advances in the mathematical theory of reinforcement learning. One of our main areas of research was studying reinforcement learning as an optimization process. We found connections to the ,  methods, , , and ; some of these insights led to an  in challenging RL benchmarks and  avoid convergence problems associated with softmax, both in RL and supervised learning. We\u2019ve made some exciting progress on the topic of safe reinforcement learning, where one seeks to discover optimal control rules while respecting important experimental constraints. This includes a . We studied  for solving a class of problems known as mean field games, which model systems with a large number of decision-makers, from mobile networks to electric grids.\n\nWe\u2019ve made breakthroughs toward generalization to new tasks and environments, an important challenge for scaling up RL to complex real-world problems. A 2020 focus area was population-based learning-to-learn methods, where another RL or evolutionary agent trained a population of RL agents to create a curriculum of , and discover . Learning to  in the training set and  with selective attention resulted in significantly more skillful RL agents. \n\nFurther, we made progress in model-based RL by showing that learning predictive behavior models , and enables  multi-agent tasks in diverse teams, and learning . Observing that skills bring predictable changes in the environment, we discover. Better representations , while  and  yield better performance. \n\nWe shared open source tools for scaling up and productionizing RL. To expand the scope and problems tackled by users, we\u2019ve introduced , a massively parallel RL agent, released a library for , and a new version of  that includes distributed RL, TPU support, and a full set of  algorithms. In addition, we performed a  of RL algorithms to improve hyperparameter selection and algorithm design.\n\nFinally, in collaboration with Loon, we , improving both power usage and their ability to navigate. \n\n\nUsing learning algorithms to develop new machine learning techniques and solutions, or meta-learning, is a very active and exciting area of research. In much of our previous work in this area, we\u2019ve created search spaces that look at how to find ways to combine sophisticated hand-designed components together in interesting ways. In , we took a different approach, by giving an evolutionary algorithm a search space consisting of very primitive operations (like addition, subtraction, variable assignment, and matrix multiplication) in order to see if it was possible to evolve modern ML algorithms from scratch. The presence of useful learning algorithms in this space is incredibly sparse, so it is remarkable that the system was able to progressively evolve more and more sophisticated ML algorithms. As shown in the figure below, the system reinvents many of the most important ML discoveries over the past 30 years, such as linear models, gradient descent, rectified linear units, effective learning rate settings and weight initializations, and gradient normalization.\n\nWe also used meta-learning to discover a variety of new efficient architectures for object detection in both still images and videos.  Last year\u2019s work on  for efficient image classification architectures showed significant accuracy improvements and computational cost reductions for image classification. In follow-on work this year,  builds on top of the EfficientNet work to derive new efficient architectures for object detection and localization, showing remarkable improvements in both highest absolute accuracy, as well as computational cost reductions of 13-42x over previous approaches to achieve a given level of accuracy.\n\nOur work on  describes a meta-learned architecture that can retain spatial information more effectively, allowing detection to be done at finer resolution. We also focused on learning effective architectures for a variety of video classification problems. , , and  demonstrate how to use evolutionary algorithms to create novel state-of-the-art video processing machine learning architectures.\n\nThis approach can also be used to develop effective model architectures for time series forecasting.  describes the system that discovers new forecasting models via an automated search over a search space involving many interesting kinds of low-level building blocks, and its effectiveness was demonstrated in the Kaggle , by generating an algorithm and system that placed 138th out of 5558 participants (top 2.5%). While many of the competitive forecasting models required months of manual effort to create, our AutoML solution found the model in a short time with only a moderate compute cost (500 CPUs for 2 hours) and no human intervention.\n\n\nDeeper understanding of machine learning algorithms and models is crucial for designing and training more effective models, as well as understanding when models may fail. Last year, we focused on fundamental questions around representation power, optimization, model generalization, and label noise, among others. As mentioned earlier in this post,  have had a huge impact on modeling language, speech and vision problems, but what is the class of functions represented by these models? Recently we showed that . Furthermore,  even when they use just a linear number of interactions among the tokens. We have been developing new optimization techniques based on layerwise adaptive learning rates to improve the convergence speed of transformers, e.g., . \n\nAs neural networks are made wider and deeper, they often train faster and generalize better. This is a core mystery in deep learning since classical learning theory suggests that large networks should overfit more. We are working to understand neural networks in this overparameterized regime. In the limit of  width, neural networks take on a surprisingly simple form, and are described by a Neural Network Gaussian Process (NNGP) or Neural Tangent Kernel (NTK). We studied this phenomenon  and , and released , an  written in  that allows researchers to build and train infinite-width neural networks. \n\nAs finite width networks are made larger, they also demonstrate peculiar phenomena \u2014 where they generalize better, then worse, then better again with increasing width. We have shown that this phenomenon can be explained by a novel .\n\nLastly, in real-world problems, one often needs to deal with significant label noise. For instance, in large scale learning scenarios, weakly labeled data is available in abundance with large label noise. We have developed new techniques for  leading to state-of-the-art results. We have further analyzed the effects of , and shown that it leads to alignment between network parameters and input data, enabling faster downstream training than initializing from scratch. We have also explored questions such as whether  or  can mitigate label noise, leading to new insights for developing robust training techniques with noisy labels.\n\n\n2020 was a productive year for our work in algorithmic foundations and theory, with several impactful research publications and notable results. On the optimization front, our paper on  develops a new technique for online  and solves a for the edge-weighted variant with applications in efficient online ad allocation. Along with this work in online allocation, we developed  that generalize to a variety of models with additional diversity and  constraints, and published a sequence of papers on the topic of online optimization with ML advice in,  and .  gave the first improvement in 50 years on the classic bipartite matching in dense graphs. Finally, another solves a long-standing open problem about chasing convex bodies online \u2014 using , no less. \n\nWe also continued our work in scalable graph mining and graph-based learning and hosted the  at NeurIPS\u201920, which covered work on scalable graph algorithms including graph clustering, graph embedding, causal inference, and graph neural networks. As part of the workshop, we showed how to , by augmenting standard synchronous computation frameworks like  with a distributed hash-table similar to a . Our extensive empirical study validates the practical relevance of the  inspired by our use of distributed hash tables in massive parallel algorithms for hierarchical clustering and connected components, and our theoretical results show how to solve many of these problems in constant distributed rounds, greatly improving upon our . We also achieved exponential speedup for . On the graph-based learning side, we presented , our framework for designing graphs for use in machine learning. Furthermore, we presented our work on more , where we show that  can be used to greatly accelerate inference in GNNs. \n\nIn market algorithms, an area at the intersection of computer science and economics, we continued our research in designing improved online marketplaces, such as , , and . In the area of repeated auctions, we developed frameworks to make dynamic mechanisms robust against  or estimation errors of  and/or , leading to provably tight low-regret dynamic mechanisms. Later, we characterized when it is possible to  through geometry-based criteria. We also  used in practice, showed their impact on the tradeoff between revenue and buyers' utility and . Additionally, we continued our research in learning optimal auction parameters, and settled the . We designed the  and  for contextual auction pricing, and developed a  and . Finally, motivated by the importance of incentives in ad auctions, and in the hope to help advertisers study the impact of incentives in auctions, we introduce a .\n\n\nPerceiving the world around us \u2014 understanding, modeling and acting on visual, auditory and multimodal input \u2014 continues to be a research area with tremendous potential to be beneficial in our everyday lives.\n\nIn 2020, deep learning powered new approaches that bring 3D computer vision and computer graphics closer together. , ,  and  are a few examples of this direction. Furthermore, our research on representing scenes as neural radiance fields (aka , see also this ) is a good example of how Google Research's academic collaborations stimulate rapid progress in the area of neural volume rendering.\n\nIn , a collaboration with UC Berkeley, we proposed a learning-based framework for disentangling outdoor scenes into temporally-varying illumination and permanent scene factors. This gives the ability to change lighting effects and scene geometry for any Street View panorama, or even turn it into a full-day timelapse video.\n\nOur work on  introduces a statistical, articulated 3D human shape modeling pipeline, within a fully trainable, modular, deep learning framework. Such models enable 3D human pose and shape reconstruction of people from a single photo to better understand the scene.\n\nThe growing area of media compression using neural networks continued to make strong progress in 2020, not only on , but also in deep approaches to ,  and nice results in deep .\n\nAdditional important themes in perceptual research included:\n\nEngaging with the broader research community through open sourcing of solutions and datasets is another important aspect of furthering perceptual research. In 2020, we open sourced multiple new perceptual inference capabilities and solutions in , such as , , , and .\n\nWe continued to make strides to improve experiences and promote helpfulness on mobile devices through ML-based solutions. Our ability to run sophisticated natural language processing on-device, enabling more natural conversational features, continues to improve. In 2020, we expanded  and launched  to allow users to save time when performing mundane tasks, and we also launched  and  to aid productivity. \n\nWe have used Google's  technology to make calls to businesses and confirm things like temporary closures. This has  us to make 3 million updates to business information globally, that have been seen over 20 billion times on Maps and Search. We also used text to speech technology for easier access to web pages, by enabling Google Assistant to, supporting 42 languages. \n\nWe also continued to make meaningful improvements to imaging applications. We made it easier to  precious moments on Pixel with  and new ways to , ,  and  them again in Google Photos. For the Pixel camera, beginning with Pixel 4 and 4a, we added , which uses machine learning to approximate the vibrance and balanced exposure and appearance of  in real time in the viewfinder. We also created dual exposure controls, which allow the brightness of shadows and highlights in a scene to be adjusted independently \u2014 live in the viewfinder. \n\nMore recently, we introduced , a new post-capture feature for the Pixel Camera and Google Photos apps that adds a simulated directional light source to portraits.  This feature is again one that is powered by machine learning, having been trained on 70 different people, photographed one light at a time, in our pretty cool 331-LED  computational illumination system.\n\nIn the past year, Google researchers were excited to contribute to many new (and timely) ways of using Google products. Here are a few examples\n\n\nIn the area of , we\u2019ve made tremendous progress in our ability to learn more and more complex, safe and robust robot behaviors with less and less data, using many of the RL techniques described earlier in the post.\n\n are a novel approach to learning how to represent robotic tasks as spatial displacements. Representing relations between objects and the robot end-effectors, as opposed to absolute positions in the environment, makes learning robust transformations of the workspace very efficient.\n\nIn , we demonstrated how a robot can be taught to follow natural language instructions (in many languages!). This required a scalable approach to collecting paired data of natural language instructions and robot behaviors. One key insight is that this can be accomplished by asking robot operators to simply , and label after-the-fact what instructions would have led to the robot accomplishing the same task.\n\nWe also explored  (by having humans use a camera-equipped grasping stick) for even more scalable data collection, and how to efficiently  across robotic tasks.\n\nWe investigated how to learn very agile strategies for robot locomotion, by , using  strategies, , and various approaches to training  using deep reinforcement learning.\n\nOne increased emphasis this year has been on safety: how do we deploy  in the real world? How do we  in a way that always allows the robot to recover from its mistakes? How do we  of learned behaviors? This is a critical area of research on which we expect to see increased focus in the future.\n\n\nOur  team continued its work to establish practical uses of quantum computing. We ran experimental algorithms on our  to simulate systems relevant to  and . These simulations are approaching a scale at which they can not be performed on classical computers anymore, making good on Feynman\u2019s original idea of using quantum computers as an efficient means to simulate systems in which quantum effects are important. We published new quantum algorithms, for instance to perform precise processor calibration, to show an advantage for  or to test . We also worked on programming models to . We released , an efficient simulation tool to develop and test quantum algorithms with up to 40 qubits on Google Cloud. \n\nWe continued to follow our . Our next milestone is the demonstration that quantum error correction can work in practice. To achieve this, we will show that a larger grid of qubits can hold logical information exponentially longer than a smaller grid, even though individual components such as qubits, couplers or I/O devices have imperfections. We are also particularly excited that we now have our own cleanroom which should significantly increase the speed and quality of our processor fabrication.\n\n\nThis year marked TensorFlow\u2019s , passing 160M downloads. The TensorFlow community continued its impressive growth with , , , , and inspiring demos . We significantly improved TF 2.x with seamless TPU support, out of the box performance (), , .\n\nWe also added many more capabilities to the TensorFlow Ecosystem to help developers and researchers in their workflows:  demonstrated going from research to production in under 90 days, using  for training and  for deployment in the browser. With , we pushed the boundaries of model parallelism to provide. We open-sourced the new ,  for model performance debugging, and , such as the for model transparency and a . With  we made it possible to easily host, track, and share your ML experiments for free.\n\nIn addition, we redoubled our investment in , an open-source, research-focused ML system that has been actively developed over the past two years. Researchers at Google and beyond are now using JAX in a wide range of fields, including , , , , , , , and . JAX , powering a growing . We also used JAX and the  to build , which we demonstrated live at NeurIPS on a large TPU Pod slice with a next-generation Cloud TPU user experience (, , ). Finally, we\u2019re ensuring that JAX works seamlessly with TF ecosystem tooling, from  for data preprocessing and  for experiment visualization to the  for performance debugging, with more to come in 2021.\n\nMany recent research breakthroughs have been enabled by increased computing power, and we make more than 500 petaflops of Cloud TPU computing power available for free to researchers around the world via the  to help broaden access to the machine learning research frontier. More than 120 TFRC-supported papers have been published to date, many of which would not have been possible without the computing resources that the program provides. For example, TFRC researchers have recently developed ,  and  on social media networks, and advanced our collective understanding of  and . Members of the TFRC community have also published , , and shared  and  as starting points for others. In 2021, we will change the name of the TFRC program to the TPU Research Cloud program to be more inclusive now that Cloud TPUs support  and  in addition to TensorFlow.\n\nFinally, this was a huge year for . Usage doubled, and we launched productivity features to help people do their work more efficiently, including  and access to the Colab VM . And we launched  to enable users to access faster GPUs, longer runtimes and more memory.\n\n\nOpen datasets with clear and measurable goals are often very helpful in driving forward the field of machine learning. To help the research community find interesting datasets, we continue to index a wide variety of open datasets sourced from many different organizations with . We also think it's important to create new datasets for the community to explore and to develop new techniques, while ensuring that we . This year, in addition to , we released a number of open datasets across many different areas:\n\n\nWe are proud to enthusiastically support and participate in the broader research community. In 2020, Google researchers presented over 500 papers at leading research conferences, additionally serving on program committees, organizing workshops, tutorials and numerous other activities aimed at collectively progressing the state of the art in the field. To learn more about our contributions to some of the larger research conferences this year, please see our blog posts for , , , ,  and .\n\nIn 2020 we supported external research with $37M in funding, including $8.5M in COVID research, $8M in research inclusion and equity, and $2M in responsible AI research. In February, we announced the , funding research proposals from 150 faculty members throughout the world. Among this group, 27% self-identified as members of historically underrepresented groups within technology. We also announced a new  to support early-career professors who are pursuing research in fields relevant to Google via unrestricted gifts. As we have for more than a decade, we selected a group of incredibly talented PhD student researchers to receive , which provides funding for graduate studies, as well as mentorship as they pursue their research, and opportunities to interact with other Google PhD Fellows.\n\nWe are also expanding the ways that we support inclusion and bring new voices into the field of computer science. In 2020, we created a new  program that supports academic research in computing and technology addressing the needs of underrepresented populations. In the inaugural set of awards, we selected 16 proposals for funding with 25 principal investigators, focused on topics around diversity and inclusion, algorithmic bias, education innovation, health tools, accessibility, gender bias, AI for social good, security, and social justice. We additionally partnered with the  (CAHSI) and the (FLIP) to create an  to support the last year of the completion of the dissertation requirements. \n\nIn 2019,  (CSRMP) helped provide mentoring to 37 undergraduate students to introduce them to conducting computer science research. Based on the success of the program in 2019/2020, we\u2019re excited to greatly expand this  in 2020/2021 and will have hundreds of Google researchers mentoring hundreds of undergraduate students in order to encourage more people from underrepresented backgrounds to pursue computer science research careers. Finally, in October we provided  to 50 institutions around the world for the 2020 academic year. These awards fund faculty to host workshops for undergraduates from underrepresented groups in order to encourage them to pursue CS research.\n\n\nI\u2019m excited about what\u2019s to come, from our technical work on next-generation AI models, to the very human work of growing our community of researchers. \n\nWe\u2019ll keep ensuring our research is done responsibly and has a positive impact, using our  as a guiding framework and applying particular scrutiny to topics that can have broad societal impact. This post covers just a few of  that Google published in the past year. While pursuing our research, we\u2019ll focus on:\n\nFinally, looking ahead to the year, I\u2019m particularly enthusiastic about the possibilities of building more general-purpose machine learning models that can handle a variety of modalities and that can automatically learn to accomplish new tasks with very few training examples. Advances in this area will empower people with dramatically more capable products, bringing better translation, speech recognition, language understanding and creative tools to billions of people all around the world.This kind of exploration and impact is what keeps us excited about our work!", "date": "\nTuesday, January 12, 2021\n"},
{"website": "Google-AI", "title": "\nEnd-to-End, Transferable Deep RL for Graph Optimization\n", "author": ["Posted by Yanqi Zhou and Sudip Roy, Research Scientists,  Google Research"], "link": "http://ai.googleblog.com/2020/12/end-to-end-transferable-deep-rl-for.html", "abstract": "An increasing number of applications are driven by large and complex neural networks trained on diverse sets of accelerators. This process is facilitated by ML compilers that map high-level computational  to low-level, device-specific executables. In doing so, ML compilers need to solve many optimization problems, including graph rewriting, assignment of operations on devices, operation fusion, layout and tiling of tensors, and scheduling. For example, in a , the compiler needs to determine the mapping between operations in the computational graph to the target physical devices so that an objective function, such as training step time, can be minimized. The placement performance is determined by a mixture of intricate factors, including inter-device network bandwidth, peak device memory, co-location constraints, etc., making it challenging for heuristics or search-based algorithms, which typically settle for fast, but sub-optimal, solutions. Furthermore, heuristics are hard to develop and maintain, especially as newer model architectures emerge. \n\nRecent attempts at using learning-based approaches have demonstrated promising results, but they have a number of limitations that make them infeasible to be deployed in practice. Firstly, these approaches do not easily generalize to unseen graphs, especially those arising from newer model architectures, and second, they have poor sample efficiency, leading to high resource consumption during training. Finally, they are only able to solve a single optimization task, and consequently, do not capture the dependencies across the tightly coupled optimization problems in the compilation stack.\n\nIn \u201d, recently published as an oral paper at , we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO) that overcomes all of the above limitations. We demonstrate 33%-60% speedup on three graph optimization tasks compared to  default optimization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including , , and , GO achieves an average 21% improvement over expert optimization and an 18% improvement over the prior state of the art with 15x faster convergence.\n\n\nThere are three coupled optimization tasks that frequently arise in ML compilers, which we formulate as decision problems that can be solved using a learned policy. The decision problems for each of the tasks can be reframed as making a decision for each node in the computational graph. \n\nThe first optimization task is , where the goal is to determine how best to assign the nodes of the graph to the physical devices on which it runs such that the end-to-end run time is minimized.\n\nThe second optimization task is An operation in a computational graph is to run when its incoming tensors are present in the device memory.  A frequently used scheduling strategy is to maintain a ready queue of operations for each device and schedule operations in first-in-first-out order. However, this scheduling strategy does not take into account the downstream operations placed on other devices that might be blocked by an operation, and often leads to schedules with underutilized devices. To find schedules that can keep track of such cross-device dependencies, our approach uses a priority-based scheduling algorithm that schedules operations in the ready queue based on the priority of each. Similar to device placement, operation scheduling can then be formulated as the problem of learning a policy that assigns a priority for each node in the graph to maximize a reward based on run time.\n\nThe third optimization task is . For brevity we omit a detailed discussion of this problem here, and instead just note that similar to priority-based scheduling, operation fusion can also use a priority-based algorithm to decide which nodes to fuse. The goal of the policy network in this case is again to assign a priority for each node in the graph.\n\nFinally, it is important to recognize that the decisions taken in each of the three optimization problems can affect the optimal decision for the other problems. For example, placing two nodes on two different devices effectively disables fusion and introduces a communication delay that can influence scheduling.\n\n\nOur research presents GO, a deep RL framework that can be adapted to solve each of the aforementioned optimization problems \u2014 both individually as well as jointly. There are three key aspects of the proposed architecture: \n\nFirst, we use graph neural networks (specifically ) to capture the topological information encoded in the computational graph. The inductive network of GraphSAGE leverages node attribute information to generalize to previously unseen graphs, which enables decision making for unseen data without incurring significant cost on training.  \n\nSecond, computational graphs for many models often contain more than 10k nodes. Solving the optimization problems effectively over such large scales requires that the network is able to capture long-range dependencies between nodes. GO\u2019s architecture includes a scalable attention network that uses segment-level recurrence to capture such long-range node dependencies.\n\nThird, ML compilers need to solve optimization problems over a wide variety of graphs from different application domains. A naive strategy of training a shared policy network with heterogeneous graphs is unlikely to capture the idiosyncrasies of a particular class of graphs.  To overcome this, GO uses a feature modulation mechanism that allows the network to specialize for specific graph types without increasing the number of parameters.\n\nTo jointly solve multiple dependent optimization tasks, GO has the ability to add additional recurrent attention layers for each task with parameters shared across different tasks. The recurrent attention layers with residual connections of actions enables tracking inter-task dependencies.\n\n\nNext, we present evaluation results on a single-task speedup on a device placement task based on real-hardware measurements, generalization to unseen graphs with different GO variants, and multi-task performance jointly optimizing operations fusion, device placement, and scheduling.\n\n\nTo evaluate the performance of this architecture, we apply GO to a device placement problem based on real-hardware evaluation, where we first train the model separately on each of our . This approach, called , consistently outperforms expert manual placement (HP), TensorFlow  placement, and  (HDP) \u2014 the current state-of-the-art reinforcement learning-based device placement. Importantly, with the efficient end-to-end single-shot placement, GO-one has a 15x speedup in convergence time of the placement network over HDP.\n\nOur empirical results show that  consistently outperforms expert placement, TensorFlow METIS placement, and  (HDP). Because GO is designed in a way to scale up to extremely large graphs consisting of over 80,000 nodes like an 8-layer  (GNMT) model, it outperforms previous approaches, including HDP, , and . GO achieves optimized graph runtimes for large graphs like GNMT that are 21.7% and 36.5% faster than HP and HDP, respectively. Overall, GO-one achieves on average  across a diverse set of 14 graphs, compared to HP and HDP respectively. Importantly, with the efficient end-to-end single-shot placement, GO-one has a  in convergence time of the placement network over HDP.\n\n\nGO generalizes to unseen graphs using offline pre-training followed by fine-tuning on the unseen graphs. During pre-training, we train GO on heterogeneous subsets of graphs from the training set. We train GO for 1000 steps on each such batch of graphs before switching to the next. This pretrained model is then fine-tuned ( on hold-out graphs for fewer than 50 steps, which typically takes less than one minute. for hold-out graphs outperforms both expert placement and HDP consistently on all datasets, and on average matches . \n\nWe also run inference directly on just the pre-trained model without any fine-tuning for the target hold-out graphs, and name this . The performance of this untuned model is only marginally worse than , while being slightly better than expert placement and HDP. This indicates that both graph embedding and the learned policies transfer efficiently, allowing the model to generalize to the unseen data.\n\n\nOptimizing simultaneously for placement, scheduling and fusion provides 30%-73% speedup compared to the single-gpu unoptimized case and 33%-60% speedup compared to TensorFlow default placement, scheduling, and fusion. Comparing to optimizing each tasks individually, multi-task GO () outperforms single-task GO () \u2014 optimizing all tasks, one at a time \u2014 by an average of 7.8%. Furthermore, for all workloads, co-optimizing all three tasks offers faster run time than optimizing any two of them and using the default policy for the third.\n\n\nThe increasing complexity and diversity of hardware accelerators has made the development of robust and adaptable ML frameworks onerous and time-consuming, often requiring multiple years of effort from hundreds of engineers. In this article, we demonstrated that many of the optimization problems in such frameworks can be solved efficiently and optimally using a carefully designed learned approach.", "date": "\nThursday, December 17, 2020\n"},
{"website": "Google-AI", "title": "\nPrivacy Considerations in Large Language Models\n", "author": ["Posted by Nicholas Carlini, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/12/privacy-considerations-in-large.html", "abstract": "Machine learning-based language models trained to predict the next word in a sentence have become increasingly capable, common, and useful, leading to groundbreaking improvements in applications like , , and more. But as language models continue to advance, new and unexpected risks can be exposed, requiring the research community to proactively work to develop new ways to mitigate potential problems.\n\nOne such risk is the potential for models to leak details from the data on which they\u2019re trained. While this may be a concern for all large language models, additional issues may arise if a model trained on private data were to be made publicly available. Because these datasets can be large (hundreds of gigabytes) and pull from a range of sources, they can sometimes contain sensitive data, including personally identifiable information (PII) \u2014 names, phone numbers, addresses, etc., even if trained on public data. This raises the possibility that a model trained using such data could reflect some of these private details in its output. It is therefore important to identify and minimize the risks of such leaks, and to develop strategies to address the issue for future models. \n\nIn \u201c\u201d, a collaboration with , , , , and , we demonstrate that, given only the ability to query a pre-trained language model, it is possible to extract specific pieces of training data that the model has memorized. As such,  are realistic threats on state-of-the-art large language models. This research represents an early, critical step intended to inform researchers about this class of vulnerabilities, so that they may take steps to mitigate these weaknesses.\n\n\nA training data extraction attack has the greatest potential for harm when applied to a model that is available to the public, but for which the dataset used in training is not. However, since conducting this research on such a dataset could have harmful consequences, we instead mount a proof of concept training data extraction attack on , a large, publicly available language model developed by OpenAI, that was trained using only public data. While this work focuses on GPT-2 specifically, the results apply to understanding what privacy threats are possible on large language models generally. \n\nAs with other privacy- and security-related research, it is important to consider the ethics of such attacks before actually performing them. To minimize the potential risk of this work, the training data extraction attack in this work was developed using publicly available data. Furthermore, the GPT-2 model itself was made public by OpenAI in 2019, and the training data used to train GPT-2 was collected from the public internet, and is available for download by anyone who follows the data collection process documented in the .\n\nAdditionally, in accordance with responsible computer security disclosure norms, we followed up with individuals whose PII was extracted, and secured their permission before including references to this data in publication. Further, in all publications of this work, we have redacted any personally identifying information that may identify individuals. We have also worked closely with OpenAI in the analysis of GPT-2.\n\n\nBy design, language models make it very easy to generate a large amount of output data. By seeding the model with random short phrases, the model can generate millions of , i.e., probable phrases that complete the sentence. Most of the time, these continuations will be benign strings of sensible text. For example, when asked to predict the continuation of the string \u201c\u201d, a language model will have high confidence that the next token is the word \u201c\u201d. However, if one particular training document happened to repeat the string \u201c\u201d many times, the model might predict that phrase instead.\n\nThe goal of a training data extraction attack is then to sift through the millions of output sequences from the language model and predict which text is memorized. To accomplish this, our approach leverages the fact that models tend to be more confident on results captured directly from their training data. These  enable us to predict if a result was used in the training data by checking the confidence of the model on a particular sequence.\n\nThe main technical contribution of this work is the development of a method for inferring membership with high accuracy along with techniques for sampling from models in a way that encourages the output of memorized content. We tested a number of different sampling strategies, the most successful of which generates text conditioned on a wide variety of input phrases. We then compare the output of two different language models. When one model has high confidence in a sequence, but the other (equally accurate) model has low confidence in a sequence, it's likely that the first model has memorized the data.\n\n\nOut of 1800 candidate sequences from the GPT-2 language model, we extracted over 600 that were memorized from the public training data, with the total number limited by the need for manual verification. The memorized examples cover a wide range of content, including news headlines, log messages, JavaScript code, PII, and more.  Many of these examples are memorized even though they appear infrequently in the training dataset.  For example, for many samples of PII we extract are found in only a single document in the dataset. However, in most of these cases, the originating document contains multiple instances of the PII, and as a result, the model still learns it as high likelihood text.\n\nFinally, we also find that the larger the language model, the more easily it memorizes training data. For example, in one experiment we find that the 1.5 billion parameter GPT-2 XL model memorizes 10 times more information than the 124 million parameter GPT-2 Small model. Given that the research community has already trained models 10 to 100 times larger, this means that as time goes by, more work will be required to monitor and mitigate this problem in increasingly large language models.\n\n\nWhile we demonstrate these attacks on GPT-2 specifically, they show potential flaws in all large generative language models. The fact that these attacks are possible has important consequences for the future of machine learning research using these types of models.\n\nFortunately, there are several ways to mitigate this issue. The most straightforward solution is to ensure that models do not train on any potentially problematic data. But this can be difficult to do in practice.\n\nThe use of , which allows training on a dataset without revealing any details of individual training examples, is one of the most principled techniques to train machine learning models with privacy. In TensorFlow, this can be achieved with the use of the  (or similar for PyTorch or JAX) that is a drop-in replacement for existing optimizers. Even this can have limitations and won\u2019t prevent memorization of content that is repeated often enough. If this is not possible, we recommend at least  how much memorization occurs so appropriate action can be taken.\n\nLanguage models continue to demonstrate great utility and flexibility\u2014yet, like all innovations, they can also pose risks. Developing them responsibly means proactively identifying those risks and developing ways to mitigate them. We hope that this effort to highlight current weaknesses in large language modeling will raise awareness of this challenge in the broader machine learning community and motivate researchers to continue to develop effective techniques to train models with reduced memorization.", "date": "\nTuesday, December 15, 2020\n"},
{"website": "Google-AI", "title": "\nPortrait Light: Enhancing Portrait Lighting with Machine Learning\n", "author": ["Posted by Yun-Ta Tsai", " and Rohit Pandey, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html", "abstract": "Professional portrait photographers are able to create compelling photographs by using specialized equipment, such as off-camera flashes and reflectors, and expert knowledge to capture just the right illumination of their subjects. In order to allow users to better emulate professional-looking portraits, we recently released , a new post-capture feature for the Pixel Camera and Google Photos apps that adds a simulated directional light source to portraits, with the directionality and intensity set to complement the lighting from the original photograph. \n\nIn the Pixel Camera on Pixel 4, Pixel 4a, Pixel 4a (5G), and Pixel 5, Portrait Light is automatically applied post-capture to images in the default mode and to  photos that include people \u2014 just one person or even a small group. In  photographs, Portrait Light provides more dramatic lighting to accompany the shallow depth-of-field effect already applied, resulting in a studio-quality look. But because lighting can be a personal choice, Pixel users who shoot in Portrait Mode can manually re-position and adjust the brightness of the applied lighting within Google Photos to match their preference. For those running Google Photos on Pixel, this relighting capability is also available for many pre-existing portrait photographs.\n\nToday we present the technology behind Portrait Light. Inspired by the off-camera lights used by portrait photographers, Portrait Light models a repositionable light source that can be added into the scene, with the initial lighting direction and intensity automatically selected to complement the existing lighting in the photo. We accomplish this by leveraging novel machine learning models, each trained using a diverse dataset of photographs captured in the  computational illumination system. These models enabled two new algorithmic capabilities: \n\nThese innovations enable Portrait Light to help create attractive lighting at any moment for every portrait \u2014 all on your mobile device.\n\n\nPhotographers usually rely on perceptual cues when deciding how to augment environmental illumination with off-camera light sources. They assess the intensity and directionality of the light falling on the face, and also adjust their subject\u2019s head pose to complement it. To inform Portrait Light\u2019s automatic light placement, we developed computational equivalents to these two perceptual signals. \n\nFirst, we trained a novel machine learning model to estimate a , omnidirectional illumination profile for a scene based on an input portrait. This new  infers the direction, relative intensity, and color of all light sources in the scene coming from all directions, considering the face as a . We also estimate the head pose of the portrait\u2019s subject using . \n\nUsing these clues, we determine the direction from which the synthetic lighting should originate. In studio portrait photography, the main off-camera light source, or , is placed about 30\u00b0 above the eyeline and between 30\u00b0 and 60\u00b0 off the camera axis, when looking overhead at the scene. We follow this guideline for a classic portrait look, enhancing any pre-existing lighting directionality in the scene while targeting a balanced, subtle  lighting ratio of about 2:1.\n\n \nGiven a desired lighting direction and portrait, we next trained a new machine learning model to add the illumination from a directional light source to the original photograph. Training the model required millions of pairs of portraits both with and without extra light. Photographing such a dataset in normal settings would have been impossible because it requires near-perfect registration of portraits captured across different lighting conditions. \n\nInstead, we generated training data by photographing seventy different people using the  computational illumination system. This spherical lighting rig includes 64 cameras with different viewpoints and 331 individually-programmable LED light sources. We photographed each individual illuminated one-light-at-a-time (OLAT) by each light, which generates their  \u2014 or their appearance as illuminated by the discrete sections of the spherical environment. The reflectance field encodes the unique color and light-reflecting properties of the subject\u2019s skin, hair, and clothing \u2014 how shiny or dull each material appears. Due to the  for light, these OLAT images can then be linearly added together to render realistic images of the subject as they would appear in any  environment, with complex light transport phenomena like  correctly represented. \n\nUsing the Light Stage, we photographed many individuals with different face shapes, genders, skin tones, hairstyles, and clothing/accessories. For each person, we generated synthetic portraits in many different lighting environments, both with and without the added directional light, rendering millions of pairs of images. This dataset encouraged model performance across diverse lighting environments and individuals. \n\n\nRather than trying to directly predict the output relit image, we trained the relighting model to output a low-resolution , i.e., a per-pixel multiplier that when upsampled can be applied to the original input image to produce the desired output image with the contribution of the extra light source added. This technique is computationally efficient and encourages only low-frequency lighting changes, without impacting high-frequency image details, which are directly transferred from the input to maintain image quality. \n\n\nWhen photographers add an extra light source into a scene, its orientation relative to the subject\u2019s facial geometry determines how much brighter each part of the face appears. To model the optical behavior of light sources reflecting off relatively matte surfaces, we first trained a machine learning model to estimate surface normals given the input photograph, and then applied  to compute a \u201clight visibility map\u201d for the desired lighting direction. We provided this light visibility map as input to the quotient image predictor, ensuring that the model is trained using physics-based insights.\n\nWe optimized the full pipeline to run at interactive frame-rates on mobile devices, with total model size under 10 MB. Here are a few examples of Portrait Light in action.\n\n\nYou can try Portrait Light in the Pixel Camera and change the light position and brightness to your liking in Google Photos. For those who use , Portrait Light can be applied post-capture for additional creative flexibility to find just the right balance between light and shadow.\u00a0On existing images from your Google Photos library, try it where faces are slightly underexposed, where Portrait Light can illuminate and highlight your subject. It will especially benefit images with a single individual posed directly at the camera.\n\nWe see Portrait Light as the first step on the journey towards creative post-capture lighting controls for mobile cameras, powered by machine learning.", "date": "\nFriday, December 11, 2020\n"},
{"website": "Google-AI", "title": "\nMediaPipe Holistic \u2014 Simultaneous Face, Hand and Pose Prediction, on Device\n", "author": ["Posted by Ivan Grishchenko and Valentin Bazarevsky, Research Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html", "abstract": "Real-time, simultaneous  perception of human pose, face landmarks and hand tracking on mobile devices can enable a variety of impactful applications, such as fitness and sport analysis, gesture control and sign language recognition, augmented reality effects and more. , an open-source framework designed specifically for complex perception pipelines leveraging accelerated inference (e.g., GPU or CPU), already offers fast and accurate, yet separate, solutions for these tasks. Combining them all in real-time into a semantically consistent end-to-end solution is a uniquely difficult problem requiring simultaneous inference of multiple, dependent neural networks.\n\nToday, we are excited to announce , a solution to this challenge that provides a novel state-of-the-art human pose topology that unlocks novel use cases. MediaPipe Holistic consists of a new pipeline with optimized ,  and  components that each run in real-time, with minimum memory transfer between their inference backends, and added support for interchangeability of the three components, depending on the quality/speed tradeoffs. When including all three components, MediaPipe Holistic provides a unified topology for a groundbreaking 540+ keypoints (33 pose, 21 per-hand and 468 facial landmarks) and achieves near real-time performance on mobile devices.  is being released as part of  and is available on-device for mobile (Android, iOS) and desktop. We are also introducing MediaPipe\u2019s new ready-to-use APIs for research () and web () to ease access to the technology.\n\n\nThe MediaPipe Holistic pipeline integrates separate models for pose, face and hand components, each of which are optimized for their particular domain. However, because of their different specializations, the input to one component is not well-suited for the others. The pose estimation model, for example, takes a lower, fixed resolution video frame (256x256) as input. But if one were to crop the hand and face regions from that image to pass to their respective models, the image resolution would be too low for accurate articulation. Therefore, we designed MediaPipe Holistic as a multi-stage pipeline, which treats the different regions using a region appropriate image resolution. \n\nFirst, MediaPipe Holistic estimates the human pose with  pose detector and subsequent keypoint model. Then, using the inferred pose key points, it derives three regions of interest (ROI) crops for each hand (2x) and the face, and employs a re-crop model to improve the ROI (details below). The pipeline then crops the full-resolution input frame to these ROIs and applies task-specific  and  models to estimate their corresponding keypoints. Finally, all key points are merged with those of the pose model to yield the full 540+ keypoints.\n\nTo streamline the identification of ROIs, a tracking approach similar to the one used for the standalone  and  pipelines is utilized. This approach assumes that the object doesn't move significantly between frames, using an estimation from the previous frame as a guide to the object region in the current one. However, during fast movements, the tracker can lose the target, which requires the detector to re-localize it in the image. MediaPipe Holistic uses pose prediction (on every frame) as an additional ROI prior to reduce the response time of the pipeline when reacting to fast movements. This also enables the model to retain semantic consistency across the body and its parts by preventing a mixup between left and right hands or body parts of one person in the frame with another.\n\nIn addition, the resolution of the input frame to the pose model is low enough that the resulting ROIs for face and hands are still too inaccurate to guide the re-cropping of those regions, which require a precise input crop to remain lightweight. To close this accuracy gap we use lightweight face and hand re-crop models that play the role of  and cost only ~10% of the corresponding model's inference time. \n\n\nMediaPipe Holistic requires coordination between up to 8 models per frame \u2014 1 pose detector, 1 pose landmark model, 3 re-crop models and 3 keypoint models for hands and face. While building this solution, we optimized not only machine learning models, but also pre- and post-processing algorithms (e.g., ), which take significant time on most devices due to pipeline complexity. In this case, moving all the pre-processing computations to GPU resulted in ~1.5 times overall pipeline speedup depending on the device. As a result, MediaPipe Holistic runs in near real-time performance even on mid-tier devices and in the browser.\n\nThe multi-stage nature of the pipeline provides two more performance benefits. As models are mostly independent, they can be replaced with lighter or heavier versions (or turned off completely) depending on the performance and accuracy requirements. Also, once pose is inferred, one knows precisely whether hands and face are within the frame bounds, allowing the pipeline to skip inference on those body parts.\n\n\nMediaPipe Holistic, with its 540+ key points, aims to enable a holistic, simultaneous perception of body language, gesture and facial expressions. Its blended approach enables remote gesture interfaces, as well as full-body AR, sports analytics, and sign language recognition. To demonstrate the quality and performance of the MediaPipe Holistic, we built a simple  that runs and enables a compelling user interaction, no mouse or keyboard required. The user can manipulate objects on the screen, type on a virtual keyboard while sitting on the sofa, and point to or touch specific face regions (e.g., mute or turn off the camera). Underneath it relies on accurate hand detection with subsequent gesture recognition mapped to a \"trackpad\" space anchored to the user\u2019s shoulder, enabling remote control from up to 4 meters. \n\nThis technique for gesture control can unlock various novel use-cases when other human-computer interaction modalities are not convenient.  and prototype your own ideas with it. \n\n\nTo accelerate ML research as well as its adoption in the web developer community, MediaPipe now offers ready-to-use, yet customizable ML solutions in  and in . We are starting with those in our previous publications: ,  and , including , with many more to come. Try them directly in the web browser: for Python using the notebooks in , and for JavaScript with your own webcam input in !\n\n\nWe hope the release of MediaPipe Holistic will inspire the research and development community members to build new unique applications. We anticipate that these pipelines will open up avenues for future research into challenging domains, such as sign-language recognition, touchless control interfaces, or other complex use cases. We are looking forward to seeing what you can build with it!", "date": "\nThursday, December 10, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at NeurIPS 2020\n", "author": ["Posted by Jaqui Herman and Cat Armato, Program Managers"], "link": "http://ai.googleblog.com/2020/12/google-at-neurips-2020.html", "abstract": "This week marks the beginning of the 34 annual  (NeurIPS 2020), the biggest machine learning conference of the year. Held virtually for the first time, this conference includes invited talks, demonstrations and presentations of some of the latest in machine learning research. \nAs a  of NeurIPS 2020, Google will have a strong presence with more than 180 accepted papers, additionally contributing to and learning from the broader academic research community via talks, posters, workshops and tutorials.\n\nIf you are registered for NeurIPS 2020, we hope you\u2019ll visit our  and chat with our researchers about the projects and opportunities at Google that go into solving the world's most challenging research problems, and to see demonstrations of some of the exciting research we pursue, such as , , ,  and much more. You can also learn more about our work being presented in the list below (Google affiliations highlighted in).\n  \n\n\n\nOrganizers: \n\n\nOrganizers: \n\n\nOrganizers: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ()Mentorship Roundtables:  ()Organizers include: Invited Speaker: Mentorship Roundtables: \n\n\u00a0()\nOrganizers include: ()Organizers include: , Invited Speaker: Sponsor Talk: Mentorship Roundtables: , , , , , , , , , , , , , , , , , , , , , , \n\n\nOrganizers include:\n\n\n\nOrganizers include:\n\n\n\nOrganizers include:\n\n\n\nOrganizers include:\n\n\nOrganizers include: \n\n  \nOrganizers include: Invited Speaker:\n\n\nOrganizers include:Invited Speaker: \nOrganizers include:Invited Speaker: \n\n  \n\nInvited Speaker: \n\n\nInvited Speakers: \n\n\nInvited Speakers: \n\n\nPanel Moderator: \n\nOrganizers include:Invited Speaker: \n\n\nOrganizers include: \n\n\nSteering Committee Member: \n\n  \n\nOrganizers include:\n\n\n\nOrganizers include: \nInvited Speaker: \n\n\nInvited Speaker: \n\n\nInvited Speaker: \n\n  \n\nOrganizers include:Invited Speaker: \n\n\nPanelists include: \n\n\nOrganizers include: \n\n\nInvited Speaker: \n\n\nInvited Speaker: \n\n\nOrganizers include: \n\n\nInvited Speaker:\n\n  \n\nOrganizers include:Invited Speaker: \n\n\n\nOrganizers include:\n\n\nOrganizers include: \n\n\nOrganizers include: \n\n\nOrganizers include: \nInvited Speaker: \n\n\nOrganizers\u00a0include:\n\n\n\n\n\nOrganizers include:\n\n\n\nOrganizers include:\n\n\n\nOrganizers include:", "date": "\nMonday, December 7, 2020\n"},
{"website": "Google-AI", "title": "\nUsing AutoML for Time Series Forecasting\n", "author": ["Posted by Chen Liang and Yifeng Lu, Software Engineers, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/12/using-automl-for-time-series-forecasting.html", "abstract": "is an important research area for machine learning (ML), particularly where accurate forecasting is critical, including several industries such as retail, supply chain, energy, finance, etc. For example, in the consumer goods domain, . Current ML-based forecasting solutions are usually built by experts and require significant manual effort, including model construction,  and . However, such expertise may not be broadly available, which can limit the benefits of applying ML towards time series forecasting challenges. \n\nTo address this,  (AutoML) is an approach that makes ML more widely accessible by automating the process of creating ML models, and has recently accelerated both ML research and the application of ML to real-world problems. For example, the initial work on  enabled breakthroughs in computer vision, such as , , and , and in natural language processing, such as . More recently, AutoML has also been applied to . \n\nToday we introduce a scalable end-to-end AutoML solution for time series forecasting, which meets three key criteria: \n\nWe demonstrate the success of this approach through participation in the , where this AutoML solution achieved competitive performance against hand-crafted models with moderate compute cost. \n\n\nTime series forecasting presents several challenges to machine learning models. First, the uncertainty is often high since the goal is to predict the future based on historical data. Unlike other machine learning problems, the test set, for example, future product sales, might have a different distribution from the training and validation set, which are extracted from the historical data. Second, the time series data from the real world often suffers from missing data and high  (i.e., when a high fraction of the time series has the value of zero). Some time series tasks may not have historical data available and suffer from the cold start problem, for example, when predicting the sales of a new product. Third, since we aim to build a fully automated generic solution, the same solution needs to apply to a variety of datasets, which can vary significantly in the domain (product sales, web traffic, etc), the granularity (daily, hourly, etc), the history length, the types of features (categorical, numerical, date time, etc), and so on. \n\n\nTo tackle these challenges, we designed an end-to-end TensorFlow pipeline with a specialized search space for time series forecasting. It is based on an encoder-decoder architecture, in which an encoder transforms the historical information in a time series into a set of vectors, and a decoder generates the future predictions based on these vectors. Inspired by the state-of-the-art sequence models, such as  and , and best practices in time series forecasting, our search space included components such as , , , , and different feature transformations. The resulting AutoML solution searches for the best combination of these components as well as core hyperparameters.\n\nTo combat the uncertainty in predicting the future of a time series, an ensemble of the top models discovered in the search is used to make final predictions. The diversity in the top models made the predictions more robust to uncertainty and less prone to overfitting the historical data. To handle time series with missing data, we fill in the gaps with a trainable vector and let the model learn to adapt to the missing time steps. To address intermittency, we predict, for each future time step, not only the value, but also the probability that the value at this time step is non-zero, and combine the two predictions. Finally, we found that the automated search is able to adjust the architecture and hyperparameter choices for different datasets, which makes the AutoML solution generic and automates the modeling efforts. \n\n\nTo benchmark our AutoML solution, we participated in the , the latest in the , which is one of the most important competitions in the forecasting community, with a long history spanning nearly 40 years. This most recent competition was hosted  and used a dataset from Walmart product sales, the real-world nature of which makes the problem quite challenging. \n\nWe participated in the competition with our fully automated solution and achieved a rank of 138 out of 5558 participants (top 2.5%) on the , which is in the silver medal zone. Participants in the competition had almost four months to produce their models. While many of the competitive forecasting models required months of manual effort to create,  our AutoML solution found the model in a short time with only a moderate compute cost (500 CPUs for 2 hours) and no human intervention. \n\nWe also benchmarked our AutoML forecasting solution on several other Kaggle datasets and found that on average it outperforms 92% of hand-crafted models, despite its limited resource use. \n\nThis work demonstrates the strength of an end-to-end AutoML solution for time series forecasting, and we are excited about its potential impact on real-world applications.", "date": "\nFriday, December 4, 2020\n"},
{"website": "Google-AI", "title": "\nTransformers for Image Recognition at Scale\n", "author": ["Posted by Neil Houlsby and Dirk Weissenborn, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html", "abstract": "While  (CNNs) have been used in computer vision , they were not at the forefront until 2012 when  surpassed the performance of contemporary state-of-the-art image recognition methods by a large margin. Two factors helped enable this breakthrough: (i) the availability of training sets like , and (ii) the use of commoditized GPU hardware, which provided significantly more compute for training. As such, since 2012, CNNs have become the go-to model for vision tasks. \n\nThe benefit of using CNNs was that they avoided the need for hand-designed visual features, instead learning to perform tasks directly from data \u201cend to end\u201d. However, while CNNs avoid hand-crafted feature-extraction, the architecture itself is designed specifically for images and can be computationally demanding. Looking forward to the next generation of scalable vision models, one might ask whether this domain-specific design is necessary, or if one could successfully leverage more domain agnostic and computationally efficient architectures to achieve state-of-the-art results.\n\nAs a first step in this direction, we present the  (ViT), a vision model based as closely as possible on the  architecture originally designed for text-based tasks. ViT represents an input image as a sequence of image patches, similar to the sequence of word embeddings used when applying Transformers to text, and directly predicts class labels for the image. ViT demonstrates excellent performance when trained on sufficient data, outperforming a comparable state-of-the-art CNN with four times fewer computational resources. To foster additional research in this area, we have open-sourced both the . \n\n\nThe original text Transformer takes as input a sequence of words, which it then uses for , , or other NLP tasks. For ViT, we make the fewest possible modifications to the Transformer design to make it operate directly on images instead of words, and observe how much about image structure the model can learn on its own. \n\nViT divides an image into a grid of square patches. Each patch is flattened into a single vector by concatenating the channels of all pixels in a patch and then linearly projecting it to the desired input dimension. Because Transformers are agnostic to the structure of the input elements we add learnable position embeddings to each patch, which allow the model to learn about the structure of the images. , ViT does not know about the relative location of patches in the image, or even that the image has a 2D structure \u2014 it must learn such relevant information from the training data and encode structural information in the position embeddings.\n\n\n\nWe first train ViT on ImageNet, where it achieves a best score of 77.9% top-1 accuracy. While this is decent for a first attempt, it falls far short of the state of the art \u2014 the  CNN trained on ImageNet with no extra data reaches 85.8%. Despite mitigation strategies (e.g., ), ViT overfits the ImageNet task due to its lack of inbuilt knowledge about images. \n\nTo investigate the impact of dataset size on model performance, we train ViT on  (14M images, 21k classes) and  (300M images, 18k classes), and compare the results to a state-of-the-art CNN,  (BiT), trained on the same datasets. As previously observed, ViT performs significantly worse than the CNN equivalent (BiT) when trained on ImageNet (1M images). However, on ImageNet-21k (14M images) performance is comparable, and on JFT (300M images), ViT now outperforms BiT.\n\nFinally, we investigate the impact of the amount of computation involved in training the models. For this, we train several different ViT models and CNNs on JFT. These models span a range of model sizes and training durations. As a result, they require varying amounts of compute for training. We observe that, for a given amount of compute, ViT yields better performance than the equivalent CNNs. \n\n\nOur data suggest that (1) with sufficient training ViT can perform very well, and (2) ViT yields an excellent performance/compute trade-off at both smaller and larger compute scales. Therefore, to see if performance improvements carried over to even larger scales, we trained a 600M-parameter ViT model. \n\nThis large ViT model attains state-of-the-art  performance on multiple popular benchmarks, including 88.55% top-1 accuracy on ImageNet and 99.50% on CIFAR-10.  ViT also performs well on the  of the ImageNet evaluations set \u201cImageNet-Real\u201d, attaining 90.72% top-1 accuracy. Finally, ViT works well on diverse tasks, even with few training data points. For example, on the  (19 tasks with 1,000 data points each), ViT attains 77.63%, significantly ahead of the single-model state of the art (SOTA) (76.3%), and even matching SOTA attained by an  (77.6%). Most importantly, these results are obtained using fewer compute resources compared to previous SOTA CNNs, e.g., 4x fewer than the pre-trained BiT models.\n\n\nTo gain some intuition into what the model learns, we visualize some of its internal workings. First, we look at the position embeddings \u2014 parameters that the model learns to encode the relative location of patches \u2014 and find that ViT is able to reproduce an intuitive image structure. Each position embedding is most similar to others in the same row and column, indicating that the model has recovered the grid structure of the original images. Second, we examine the average spatial distance between one element attending to another for each transformer block. At higher layers (depths of 10-20) only global features are used (i.e., large attention distances), but the lower layers (depths 0-5) capture both global and local features, as indicated by a large range in the mean attention distance. By contrast, only local features are present in the lower layers of a CNN. These experiments indicate that ViT can learn features hard-coded into CNNs (such as awareness of grid structure), but is also free to learn more generic patterns, such as a mix of local and global features at lower layers, that can aid generalization.\n\n\nWhile CNNs have revolutionized computer vision, our results indicate that models tailor-made for imaging tasks may be unnecessary, or even sub-optimal. With ever-increasing dataset sizes, and the continued development of unsupervised and semi-supervised methods, the development of new vision architectures that train more efficiently on these datasets becomes increasingly important. We believe ViT is a preliminary step towards generic, scalable architectures that can solve many vision tasks, or even tasks from many domains, and are excited for future developments.\n\nA  of our work as well as  are publically available.", "date": "\nThursday, December 3, 2020\n"},
{"website": "Google-AI", "title": "\nNavigating Recorder Transcripts Easily, with Smart Scrolling\n", "author": ["Posted by Itay Inbar, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/11/navigating-recorder-transcripts-easily.html", "abstract": "Last year we launched , a new kind of recording app that made audio recording smarter and more useful by leveraging on-device machine learning (ML) to transcribe the recording, highlight audio events, and suggest appropriate tags for titles. Recorder makes editing, sharing and searching through transcripts easier. Yet because Recorder can transcribe very long recordings (up to 18 hours!), it can still be difficult for users to find specific sections, necessitating a new solution to quickly navigate such long transcripts.  \n\nTo increase the navigability of content, we introduce Smart Scrolling, a new ML-based feature in Recorder that automatically marks important sections in the transcript, chooses the most representative keywords from each section, and then surfaces those keywords on the vertical scrollbar, like chapter headings. The user can then scroll through the keywords or tap on them to quickly navigate to the sections of interest. The models used are lightweight enough to be executed on-device without the need to upload the transcript, thus preserving user privacy.\n\nThe Smart Scrolling feature is composed of two distinct tasks. The first extracts representative keywords from each section and the second picks which sections in the text are the most informative and unique.\n\nFor each task, we utilize two different natural language processing (NLP) approaches: a distilled  (BERT) model pre-trained on data sourced from a , alongside a modified extractive  (TF-IDF) model. By using the bidirectional transformer and the TF-IDF-based models in parallel for both the keyword extraction and important section identification tasks, alongside aggregation heuristics, we were able to harness the advantages of each approach and mitigate their respective drawbacks (more on this in the next section). \n\nThe bidirectional transformer is a neural network architecture that employs a  mechanism to achieve context-aware processing of the input text in a non-sequential fashion. This enables parallel processing of the input text to identify contextual clues both before and after a given position in the transcript. \n\nThe extractive  approach rates terms based on their frequency in the text compared to their inverse frequency in the trained dataset, and enables the finding of unique representative terms in the text.\n\nBoth models were trained on publicly available conversational datasets that were labeled and evaluated by independent raters. The conversational datasets were from the same domains as the expected product use cases, focusing on meetings, lectures, and interviews, thus ensuring the same word frequency distribution ().\n\n\nThe TF-IDF-based model detects informative keywords by giving each word a score, which corresponds to how representative this keyword is within the text. The model does so, much like a standard TF-IDF model, by utilizing the ratio of the number of occurrences of a given word in the text compared to the whole of the conversational data set, but it also takes into account the  of the term, i.e., how broad or specific it is. Furthermore, the model then aggregates these features into a score using a pre-trained function curve. In parallel, the bidirectional transformer model, which was fine tuned on the task of extracting keywords, provides a deep semantic understanding of the text, enabling it to extract precise context-aware keywords. \n\nThe TF-IDF approach is conservative in the sense that it is prone to finding uncommon keywords in the text (high bias), while the drawback for the bidirectional transformer model is the high variance of the possible keywords that can be extracted. But when used together, these two models complement each other, forming a balanced bias-variance tradeoff.\n\nOnce the keyword scores are retrieved from both models, we normalize and combine them by utilizing NLP heuristics (e.g., the weighted average), removing duplicates across sections, and eliminating stop words and verbs. The output of this process is an ordered list of suggested keywords for each of the sections.\n\n\nThe next task is to determine which sections should be highlighted as informative and unique. To solve this task, we again combine the two models mentioned above, which yield two distinct importance scores for each of the sections. We compute the first score by taking the TF-IDF scores of all the keywords in the section and weighting them by their respective number of appearances in the section, followed by a summation of these individual keyword scores. We compute the second score by running the section text through the bidirectional transformer model, which was also trained on the sections rating task. The scores from both models are normalized and then combined to yield the section score.\n\n\nA significant challenge in the development of Smart Scrolling was how to identify whether a section or keyword is important - what is of great importance to one person can be of less importance to another. The key was to highlight sections only when it is possible to extract helpful keywords from them.\n\nTo do this, we configured the solution to select the top scored sections that also have highly rated keywords, with the number of sections highlighted proportional to the length of the recording.  In the context of the Smart Scrolling features, a keyword was more highly rated if it better represented the unique information of the section.\n\nTo train the model to understand this criteria, we needed to prepare a labeled training dataset tailored to this task. In collaboration with a team of skilled raters, we applied this labeling objective to a small batch of examples to establish an initial dataset in order to evaluate the quality of the labels and instruct the raters in cases where there were deviations from what was intended. Once the labeling process was complete we reviewed the labeled data manually and made corrections to the labels as necessary to align them with our definition of importance.\n\nUsing this limited labeled dataset, we ran automated model evaluations to establish initial metrics on model quality, which were used as a less-accurate proxy to the model quality, enabling us to quickly assess the model performance and apply changes in the architecture and heuristics. Once the solution metrics were satisfactory, we utilized a more accurate manual evaluation process over a closed set of carefully chosen examples that represented expected Recorder use cases. Using these examples, we tweaked the model heuristics parameters to reach the desired level of performance using a reliable model quality evaluation.\n\n\nAfter the initial release of Recorder, we conducted a series of user studies to learn how to improve the usability and performance of the Smart Scrolling feature. We found that many users expect the navigational keywords and highlighted sections to be available as soon as the recording is finished. Because the computation pipeline described above can take a considerable amount of time to compute on long recordings, we devised a partial processing solution that amortizes this computation over the whole duration of the recording. During recording, each section is processed as soon as it is captured, and then the intermediate results are stored in memory. When the recording is done, Recorder aggregates the intermediate results.\n\nWhen running on a Pixel 5, this approach reduced the average processing time of an hour long recording (~9K words) from 1 minute 40 seconds to only 9 seconds, while outputting the same results.\n\n\nThe goal of Recorder is to improve users\u2019 ability to access their recorded content and navigate it with ease. We have already made substantial progress in this direction with the existing ML features that automatically suggest title words for recordings and enable users to search recordings for sounds and text.  Smart Scrolling provides additional text navigation abilities that will further improve the utility of Recorder, enabling users to rapidly surface sections of interest, even for long recordings.", "date": "\nTuesday, November 24, 2020\n"},
{"website": "Google-AI", "title": "\nThe Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models\n", "author": ["Posted by James Wexler, Software Developer and Ian Tenney, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html", "abstract": "As natural language processing (NLP) models become more powerful and are deployed in more real-world contexts, understanding their behavior is becoming increasingly critical. While advances in modeling have brought unprecedented performance on many NLP tasks, many research questions remain about not only the behavior of these models under domain shift and adversarial settings, but also their tendencies to behave according to social biases or shallow heuristics. \n\nFor any new model, one might want to know in which cases a model performs poorly, why a model makes a particular prediction, or whether a model will behave consistently under varying inputs, such as changes to textual style or . But, despite the recent explosion of work on model understanding and evaluation, there is no \u201csilver bullet\u201d for analysis. Practitioners must often experiment with many techniques, looking at local explanations, aggregate metrics, and counterfactual variations of the input to build a better understanding of model behavior, with each of these techniques often requiring its own software package or bespoke tool. Our previously released  was built to address this challenge by enabling black-box probing of classification and regression models, thus enabling researchers to more easily debug performance and analyze the fairness of machine learning models through interaction and visualization. But there was still a need for a toolkit that would address challenges specific to NLP models.\n\nWith these challenges in mind, we built and  the  (LIT), an interactive platform for NLP model understanding. LIT builds upon the lessons learned from the  with greatly expanded capabilities, which cover a wide range of NLP tasks including sequence generation, span labeling, classification and regression, along with customizable and extensible visualizations and model analysis.\n\nLIT supports local explanations, including , , and rich visualizations of model predictions, as well as aggregate analysis including metrics, embedding spaces, and flexible slicing. It allows users to easily hop between visualizations to test local hypotheses and validate them over a dataset. LIT provides support for counterfactual generation, in which new data points can be added on the fly, and their effect on the model visualized immediately. Side-by-side comparison allows for two models, or two individual data points, to be visualized simultaneously. More details about LIT can be found in our system demonstration , which was presented at .\n\n\nIn order to better address the broad range of users with different interests and priorities that we hope will use LIT, we\u2019ve built the tool to be easily customizable and extensible from the start. Using LIT on a particular NLP model and dataset only requires writing a small bit of Python code. Custom components, such as task-specific metrics calculations or counterfactual generators, can be written in Python and added to a LIT instance through our provided APIs. Also, the front end itself can be customized, with new modules that integrate directly into the UI.\u00a0For more on extending the tool, check out our documentation on .\n\nTo illustrate some of the capabilities of LIT, we have created a few demos using pre-trained models. The full list is available on the LIT , and we describe two of them here:\nAlthough LIT is a new tool, we have already seen the value that it can provide for model understanding. Its visualizations can be used to find patterns in model behavior, such as outlying clusters in embedding space, or words with outsized importance to the predictions. Exploration in LIT can test for potential biases in models, as demonstrated in our . This type of analysis can inform next steps in improving model performance, such as . It can also be used as an easy and fast way to create an interactive demo for any NLP model.\n\nCheck out the tool either through our provided demos, or by bringing up a LIT server for your own models and datasets. The work on LIT has just started, and there are a number of new capabilities and refinements planned, including the addition of new interpretability techniques from cutting edge ML and NLP research. If there are other techniques that you\u2019d like to see added to the tool, please let us know! Join our  to stay up to date as LIT evolves. And as the code is open-source, we welcome  on and contributions to the tool.", "date": "\nFriday, November 20, 2020\n"},
{"website": "Google-AI", "title": "\nHaptics with Input: Using Linear Resonant Actuators for Sensing\n", "author": ["Posted by Artem Dementyev, Hardware Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/11/haptics-with-input-using-linear.html", "abstract": "As wearables and handheld devices decrease in size,  become an increasingly vital channel for feedback, be it through silent alerts or a subtle \"click\" sensation when pressing buttons on a touch screen. Haptic feedback, ubiquitous in nearly all wearable devices and mobile phones, is commonly enabled by a  (LRA), a small linear motor that leverages resonance to provide a strong haptic signal in a small package. However, the touch and pressure sensing needed to activate the haptic feedback tend to depend on additional, separate hardware which increases the price, size and complexity of the system. \n\nIn \u201c\u201d, presented at , we demonstrate that widely available LRAs can sense a wide range of external information, such as touch, tap and pressure, in addition to being able to relay information about contact with the skin, objects and surfaces. We achieve this with off-the-shelf LRAs by multiplexing the actuation with short pulses of custom waveforms that are designed to enable sensing using the .  We demonstrate the potential of this approach to enable expressive discrete buttons and vibrotactile interfaces and show how the approach could bring rich sensing opportunities to integrated haptics modules in mobile devices, increasing sensing capabilities with fewer components. Our technique is potentially compatible with many existing LRA drivers, as they already employ back-EMF sensing for autotuning of the vibration frequency. \n\n\nInside the LRA enclosure is a magnet attached to a small mass, both moving freely on a spring. The magnet moves in response to the excitation voltage introduced by the . The motion of the oscillating mass produces a , or , which is a voltage proportional to the rate of change of magnetic flux. A greater oscillation speed creates a larger back-EMF voltage, while a stationary mass generates zero back-EMF voltage.\n\nTouching or making contact with the LRA during vibration changes the velocity of the interior mass, as energy is dissipated into the contact object. This works well with soft materials that deform under pressure, such as the human body. A finger, for example, absorbs different amounts of energy depending on the contact force as it flattens against the LRA. By driving the LRA with small amounts of energy, we can measure this phenomenon using the back-EMF voltage. Because leveraging the back-EMF behavior for sensing is an active process, the key insight that enabled this work was the design of a custom, off-resonance driver waveform that allows continuous sensing while minimizing vibrations, sound and power consumption.\n\nWe measure back-EMF from the floating voltage between the two LRA leads, which requires disconnecting the motor driver briefly to avoid disturbances. While the driver is disconnected, the mass is still oscillating inside the LRA, producing an oscillating back-EMF voltage. Because commercial back-EMF sensing LRA drivers do not provide the raw data, we designed a custom circuit that is able to pick up and amplify small back-EMF voltage. We also generated custom drive pulses that minimize vibrations and energy consumption. \n\n\nThe behavior of the LRAs used in mobile phones is the same, whether they are on a table, on a soft surface, or hand held. This may cause problems, as a vibrating phone could slide off a glass table or emit loud and unnecessary vibrating sounds. Ideally, the LRA on a phone would automatically adjust based on its environment. We demonstrate our approach for sensing using the LRA back-EMF technique by wiring directly to a Pixel 4\u2019s LRA, and then classifying whether the phone is held in hand, placed on a soft surface (foam), or placed on a table. \n\nWe also present a prototype that demonstrates how LRAs could be used as combined input/output devices in portable electronics. We attached two LRAs, one on the left and one on the right side of a phone. The buttons provide tap, touch, and pressure sensing. They are also programmed to provide haptic feedback, once the touch is detected. \n\nThere are a number of wearable tactile aid devices, such as sleeves, vests, and bracelets. To transmit tactile feedback to the skin with consistent force, the  has to apply the right pressure; it can not be too loose or too tight. Currently, the typical way to do so is through manual adjustment, which can be inconsistent and lacks measurable feedback. We show how the LRA back-EMF technique can be used to continuously monitor the fit bracelet device and prompt the user if it's too tight, too loose, or just right. \n\n\nThe LRA works well as a pressure sensor, because it has a quadratic response to the force magnitude during touch. Our method works for all five off-the-shelf LRA types that we evaluated. Because the typical power consumption is only 4.27 mA, all-day sensing would only reduce the battery life of a Pixel 4 phone from 25 to 24 hours. The power consumption can be greatly reduced by using low-power amplifiers and employing active sensing only when needed, such as when the phone is active and interacting with the user. \n\nThe challenge with active sensing is to minimize vibrations, so they are not perceptible when touching and do not produce audible sound. We optimize the active sensing to produce only 2 dB of sound and 0.45 m/s peak-to-peak acceleration, which is just barely perceptible by finger and is quiet, in contrast to the regular 8.49 m/s .\n\n\nTo see the work presented here in action, please see the video below.\nIn the future, we plan to explore other sensing techniques, perhaps measuring the current could be an alternative approach. Also, using machine learning could potentially improve the sensing and provide more accurate classification of the complex back-EMF patterns. Our method could be developed further to enable closed-loop feedback with the actuator and sensor, which would allow the actuator to provide the same force, regardless of external conditions.\nWe believe that this work opens up new opportunities for leveraging existing ubiquitous hardware to provide rich interactions and closed-loop feedback haptic actuators.", "date": "\nWednesday, November 18, 2020\n"},
{"website": "Google-AI", "title": "\nUsing GANs to Create Fantastical Creatures\n", "author": ["Posted by Andeep Singh Toor, Stadia Software Engineer and Fred Bertsch, Software Engineer, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html", "abstract": "Creating art for digital video games takes a high degree of artistic creativity and technical knowledge, while also requiring game artists to quickly iterate on ideas and produce a high volume of assets, often in the face of tight deadlines. What if artists had a paintbrush that acted less like a tool and more like an assistant? A machine learning model acting as such a paintbrush could reduce the amount of time necessary to create high-quality art without sacrificing artistic choices, perhaps even enhancing creativity.\n\nToday, we present , a trained machine learning (ML) model that automatically creates a fully fleshed out rendering from a user-supplied creature outline. Employed as a demo application, Chimera Painter adds features and textures to a creature outline segmented with body part labels, such as \u201cwings\u201d or \u201cclaws\u201d, when the user clicks the \u201ctransform\u201d button. Below is an example using the demo with one of the preset creature outlines. \n\nIn this post, we describe some of the challenges in creating the ML model behind Chimera Painter and demonstrate how one might use the tool for the creation of video game-ready assets. \n\n\nIn developing an ML model to produce video-game ready creature images, we created a digital card game prototype around the concept of combining creatures into new hybrids that can then battle each other. In this game, a player would begin with cards of real-world animals (e.g., an  or a whale) and could make them more powerful by combining them (making the dreaded Axolotl-Whale ). This provided a creative environment for demonstrating an image-generating model, as the number of possible chimeras necessitated a method for quickly designing large volumes of artistic assets that could be combined naturally, while still retaining identifiable visual characteristics of the original creatures.\n\nSince our goal was to create high-quality creature card images guided by artist input, we experimented with  (GANs), informed by artist feedback, to create creature images that would be appropriate for our fantasy card game prototype. GANs pair two convolutional neural networks against each other: a generator network to create new images and a discriminator network to determine if these images are samples from the training dataset (in this case, artist-created images) or not. We used a variant called a conditional GAN, where the generator takes a separate input to guide the image generation process. Interestingly, our approach was a strict departure from other GAN efforts, which typically focus on .\nTo train the GANs, we created a dataset of full color images with single-species creature outlines adapted from 3D creature models. The creature outlines characterized the shape and size of each creature, and provided a segmentation map that identified individual body parts. After model training, the model was tasked with generating multi-species chimeras, based on outlines provided by artists. The best performing model was then incorporated into Chimera Painter. Below we show some sample assets generated using the model, including single-species creatures, as well as the more complex multi-species chimeras.\n\nAn issue with using GANs for generating creatures was the potential for loss of anatomical and spatial coherence when rendering subtle or low-contrast parts of images, despite these being of  to humans. Examples of this can include eyes, fingers, or even distinguishing between overlapping body parts with similar textures (see the affectionately named BoggleDog below).\n\nGenerating chimeras required a new non-photographic fantasy-styled dataset with unique characteristics, such as dramatic perspective, composition, and lighting. Existing repositories of illustrations were not appropriate to use as datasets for training an ML model, because they may be subject to licensing restrictions, have conflicting styles, or simply lack the variety needed for this task. \n\nTo solve this, we developed a new artist-led, semi-automated approach for creating an ML training dataset from 3D creature models, which allowed us to work at scale and rapidly iterate as needed. In this process, artists would create or obtain a set of 3D creature models, one for each creature type needed (such as hyenas or lions). Artists then produced two sets of textures that were overlaid on the 3D model using the  \u2014 one with the full color texture (left image, below) and the other with flat colors for each body part (e.g., head, ears, neck, etc), called a \u201csegmentation map\u201d (right image, below). This second set of body part segments was given to the model at training to ensure that the GAN learned about body part-specific structure, shapes, textures, and proportions for a variety of creatures. \n\nThe 3D creature models were all placed in a simple 3D scene, again using the Unreal Engine. A set of automated scripts would then take this 3D scene and interpolate between different poses, viewpoints, and zoom levels for each of the 3D creature models, creating the full color images and segmentation maps that formed the training dataset for the GAN. Using this approach, we generated 10,000+ image + segmentation map pairs per 3D creature model, saving the artists millions of hours of time compared to creating such data manually (at approximately 20 minutes per image). \n\n\nThe GAN had many different hyper-parameters that could be adjusted, leading to different qualities in the output images. In order to better understand which versions of the model were better than others, artists were provided samples for different creature types generated by these models and asked to cull them down to a few best examples. We gathered feedback about desired characteristics present in these examples, such as a feeling of depth, style with regard to creature textures, and realism of faces and eyes. This information was used both to train new versions of the model and, after the model had generated hundreds of thousands of creature images, to select the best image from each creature category (e.g., gazelle, lynx, gorilla, etc).\n\nWe tuned the GAN for this task by focusing on the perceptual loss. This loss function component (also used in ) computes a difference between two images using extracted features from a separate convolutional neural network (CNN) that was previously trained on millions of photographs from the  dataset. The features are extracted from different layers of the CNN and a weight is applied to each, which affects their contribution to the final loss value. We discovered that these weights were critically important in determining what a final generated image would look like. Below are some examples from the GAN trained with different perceptual loss weights.\n\nSome of the variation in the images above is due to the fact that the dataset includes multiple textures for each creature (for example, a reddish or grayish version of the bat). However, ignoring the coloration, many differences are directly tied to changes in perceptual loss values. In particular, we found that certain values brought out sharper facial features (e.g., bottom right vs. top right) or \u201csmooth\u201d versus \u201cpatterned\u201d (top right vs. bottom left) that made generated creatures feel more real.\n\nHere are some creatures generated from the GAN trained with different perceptual loss weights, showing off a small sample of the outputs and poses that the model can handle.\n\n\nThe trained GAN is now available in the , allowing artists to work iteratively with the model, rather than drawing dozens of similar creatures from scratch. An artist can select a starting point and then adjust the shape, type, or placement of creature parts, enabling rapid exploration and for the creation of a large volume of images. The demo also allows for uploading a creature outline created in an external program, like Photoshop. Simply download one of the preset creature outlines to get the colors needed for each creature part and use this as a template for drawing one outside of Chimera Painter, and then use the \u201cLoad\u2019 button on the demo to use this outline to flesh out your creation.\nIt is our hope that these GAN models and the Chimera Painter demonstration tool might inspire others to think differently about their art pipeline. What can one create when using machine learning as a paintbrush?", "date": "\nTuesday, November 17, 2020\n"},
{"website": "Google-AI", "title": "\nMitigating Unfair Bias in ML Models with the MinDiff Framework\n", "author": ["Posted by Flavien Prost, Senior Software Engineer and Alex Beutel, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html", "abstract": "The responsible research and development of machine learning (ML) can play a pivotal role in helping to solve a wide variety of societal challenges. At Google, our research reflects our , from  and improving , to presenting methods that tackle unfair bias in products, such as , and  for other researchers to do the same. \n\nOne broad category for applying ML  responsibly is the task of  \u2014 systems that sort data into labeled categories.  At Google, such models are used throughout our products to enforce policies, ranging from the detection of hate speech to age-appropriate content filtering.  While these classifiers serve vital functions, it is also essential that they are built in ways that minimize unfair biases for users.\n\nToday, we are announcing the release of , a new regularization technique available in the  for effectively and efficiently mitigating unfair biases when training ML models.  In this post, we discuss the research behind this technique and explain how it addresses thepractical constraints and requirements we\u2019ve observed when incorporating it in Google\u2019s products. \n\n\nTo illustrate how MinDiff can be used, consider an  of a product policy classifier that is tasked with identifying and removing text comments that could be considered toxic. One challenge is to make sure that  biased against submissions from a particular group of users, which could result in incorrect removal of content from these groups. \n\nThe academic community has laid a solid theoretical foundation for ML fairness, offering a breadth of perspectives on what  and on the tensions between different frameworks for . One of the most common metrics is , which, in our example, means measuring and seeking to minimize the difference in  (FPR) across groups. In the example above, this means that the classifier should not be more likely to  remove safe comments from one group than another. Similarly, the classifier\u2019s false negative rate should be equal between groups. That is, the classifier should not miss toxic comments against one group more than it does for another.\n\nWhen the end goal is to improve products, it\u2019s important to be able to scale unfair bias mitigation to many models. However, this poses a number of challenges:\n\n\nWe iteratively developed the MinDiff framework over the previous few years to meet these design requirements. Because demographic information is so rarely known, we utilize in-process approaches in which the model\u2019s training objective is augmented with an objective specifically focused on removing biases. This new objective is then optimized over the small sample of data with known demographic information. To improve ease of use, we switched from adversarial training to a , which penalizes statistical dependency between its predictions and demographic information for non-harmful examples. This encourages the model to equalize error rates across groups, e.g., classifying non-harmful examples as toxic. \n\nThere are several ways to encode this dependency between predictions and demographic information. Our initial MinDiff implementation minimized the correlation between the predictions and the demographic group, which essentially optimized for the average and variance of predictions to be equal across groups, even if the distributions still differ afterward.  We have since  MinDiff further by considering the  (MMD) loss, which is closer to optimizing for the distribution of predictions to be independent of demographics. We have found that this approach is better able to both remove biases and maintain model accuracy.\n\n\nTo date we have launched modeling improvements across several classifiers at Google that moderate content quality.  We went through multiple iterations to develop a robust, responsible, and scalable approach, solving research challenges and enabling broad adoption.  \n\nGaps in error rates of classifiers is an important set of unfair biases to address, but not the only one that arises in ML applications.  For ML researchers and practitioners, we hope this work can further advance research toward addressing even broader classes of unfair biases and the development of approaches that can be used in practical applications. In addition, we hope that the release of the MinDifflibrary and the associated  and , along with the tools and experience shared here, can help practitioners improve their models and products.", "date": "\nMonday, November 16, 2020\n"},
{"website": "Google-AI", "title": "\nThe Machine Learning Behind Hum to Search\n", "author": ["Posted by Christian Frank, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2020/11/the-machine-learning-behind-hum-to.html", "abstract": "Melodies stuck in your head, often referred to as \u201c,\u201d are a well-known and sometimes irritating phenomenon \u2014 once that earworm is there, it can be tough to get rid of it. Research  that , whether that\u2019s listening to or singing it, will drive the earworm away. But what if you can\u2019t quite recall the name of the song, and can only hum the melody? \n\nExisting methods to match a hummed melody to its original polyphonic studio recording face several challenges. With lyrics, background vocals and instruments, the audio of a musical or studio recording can be quite different from a hummed tune. By mistake or design, when someone hums their interpretation of a song, often the pitch, key, tempo or rhythm may vary slightly or even significantly.  That\u2019s why so many  to  match the hummed tune against a database of pre-existing melody-only or hummed versions of a song, instead of identifying the song directly. However, this type of approach often relies on a limited database that requires manual updates. \n\nLaunched in October,  is a new fully machine-learned system within Google Search that allows a person to find a song using only a hummed rendition of it. In contrast to existing methods, this approach produces an embedding of a melody from a spectrogram of a song without generating an intermediate representation. This enables the model to match a hummed melody directly to the original (polyphonic) recordings without the need for a hummed or MIDI version of each track or for other complex hand-engineered logic to extract the melody. This approach greatly simplifies the database for Hum to Search, allowing it to constantly be refreshed with embeddings of original recordings from across the world \u2014 even the latest releases. \n\n\nMany existing music recognition systems convert an audio sample into a spectrogram before processing it, in order to find a good match. However, one challenge in recognizing a hummed melody is that a hummed tune often contains relatively little information, as illustrated by  of . The difference between the hummed version and the same segment from the corresponding studio recording can be visualized using , seen below: \n\nGiven the image on the left, a model needs to locate the audio corresponding to the right-hand image from a collection of over 50M similar-looking images (corresponding to segments of studio recordings of other songs). To achieve this, the model has to learn to focus on the dominant melody, and ignore background vocals, instruments, and voice timbre, as well as differences stemming from background noise or room reverberations. To find by eye the dominant melody that might be used to match these two spectrograms, a person might look for similarities in the lines near the bottom of the above images. \n\nPrior efforts to enable discovery of music, in particular in the context of recognizing  music being played in an environment such as a cafe or a club, demonstrated how machine learning might be applied to this problem. , released to Pixel phones in 2017, uses an on-device deep neural network to recognize songs without the need for a server connection, and  further developed this technology to provide a server-based recognition service for faster and more accurate searching of over 100 million songs. The next challenge then was to leverage what was learned from these releases to recognize hummed or sung music from a similarly large library of songs. \n\n\nThe first step in developing Hum to Search was to modify the music-recognition models used in Now Playing and Sound Search to work with hummed recordings. In principle, many such retrieval systems (e.g., image recognition) work in a similar way. A neural network is trained with pairs of input (here pairs of hummed or sung audio with recorded audio) to produce embeddings for each input, which will later be used for matching to a hummed melody.  \n\nTo enable humming recognition, the network should produce embeddings for which pairs of audio containing the same melody are close to each other, even if they have different instrumental accompaniment and singing voices. Pairs of audio containing different melodies should be far apart. In training, the network is provided such pairs of audio until it learns to produce embeddings with this property.\n\nThe trained model can then generate an embedding for a tune that is similar to the embedding of the song\u2019s reference recording.  Finding the correct song is then only a matter of searching for similar embeddings from a database of reference recordings computed from audio of popular music. \n\n\nBecause training of the model required song pairs (recorded and sung), the first challenge was to obtain enough training data. Our initial dataset consisted of mostly sung music segments (very few of these contained humming). To make the model more robust, we augmented the audio during training, for example by varying the pitch or tempo of the sung input randomly. The resulting model worked well enough for people singing, but not for people humming or whistling.\n\nTo improve the model\u2019s performance on hummed melodies we generated additional training data of simulated \u201chummed\u201d melodies from the existing audio dataset using , a pitch extraction model developed by our wider team as part of the  project. SPICE extracts the pitch values from given audio, which we then use to generate a melody consisting of discrete audio tones. The very first version of this system transformed  into . \n\nWe later refined this approach by replacing the simple tone generator with a neural network that generates audio resembling an actual hummed or whistled tune. For example, the network generates this  or  from the above .\n\nAs a final step, we compared training data by mixing and matching the audio samples. For example, if we had a similar clip from two different singers, we\u2019d align those two clips with our preliminary models, and are therefore able to show the model an additional pair of audio clips that represent the same melody.\n\n\nWhen training the Hum to Search model, we started with a  function. This loss  to perform well across a variety of classification tasks like images and . Given a pair of audio corresponding to the same melody (points R and P in embedding space, shown below), triplet loss would ignore certain parts of the training data derived from a different melody. This helps the machine improve learning behavior, either when it finds a different melody that is too \u2018easy\u2019 in that it is already far away from R and P (see point E) or because it is too hard in that, given the model's current state of learning, the audio ends up being too close to R \u2014 even though according to our data it represents a different melody (see point H). \n\nWe\u2019ve found that we could improve the accuracy of the model by taking these additional training data (points H and E) into account, namely by formulating a general notion of model confidence across a batch of examples: How sure is the model that  the data it has seen can be classified correctly, or has it seen examples that do not fit its current understanding? Based on this notion of confidence, we added a loss that drives model confidence towards 100% across all areas of the embedding space, which led to improvements in our model\u2019s . \n\nThe above changes, but in particular our variations, augmentations and superpositions of the training data, enabled the neural network model deployed in Google Search to recognize sung or hummed melodies. The current system reaches a high level of accuracy on a song database that contains over half a million  songs that we are continually updating. This song corpus still has room to grow to include more of the world\u2019s many melodies. \n\nTo try the feature, you can open the latest version of the Google app, tap the mic icon and say \u201cwhat's this song?\u201d or click the \u201cSearch a song\u201d button, after which you can hum, sing, or whistle away! We hope that Hum to Search can help with that earworm of yours, or maybe just help you in case you want to find and playback a song without having to type its name.", "date": "\nThursday, November 12, 2020\n"},
{"website": "Google-AI", "title": "\nImproving On-Device Speech Recognition with VoiceFilter-Lite\n", "author": ["Posted by Quan Wang, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/11/improving-on-device-speech-recognition.html", "abstract": "Voice assistive technologies, which enable users to employ voice commands to interact with their devices, rely on accurate  to ensure responsiveness to a specific user. But in many real-world use cases, the input to such technologies often consists of overlapping speech, which poses great challenges to many speech recognition algorithms. In 2018, we published a , which leverages Google\u2019s  to personalize interaction with assistive technology by allowing people to . \n\nWhile the VoiceFilter approach is highly successful, achieving a better  (SDR) than conventional approaches, efficient  streaming speech recognition requires addressing restrictions such as model size, CPU and memory limitations, as well as battery usage considerations and latency minimization. \n\nIn \u201c\u201d, we present an update to  for on-device use that can significantly improve speech recognition in overlapping speech by leveraging the  of a selected speaker. Importantly, this model can be easily integrated with existing on-device speech recognition applications, allowing the user to access voice assistive features under extremely noisy conditions even if an internet connection is unavailable. Our experiments show that a 2.2MB VoiceFilter-Lite model provides a 25.1% improvement to the  (WER) on overlapping speech.\n\n\nWhile the original VoiceFilter system was very successful at separating a target speaker's speech signal from other overlapping sources, its model size, computational cost and latency are not feasible for .\n\nThe new  system has been carefully designed to fit on-device applications. Instead of processing audio waveforms, VoiceFilter-Lite takes exactly the same input features as the speech recognition model (stacked log ), and directly enhances these features by filtering out components not belonging to the target speaker in real time. Together with several optimizations on network topologies, the number of runtime operations is drastically reduced. After  with the  library, the model size is only 2.2 MB, which fits most on-device applications.\n\nTo train the VoiceFilter-Lite model, the filterbanks of the noisy speech are fed as input to the network together with an embedding vector that represents the identity of the target speaker (i.e., a ). The network predicts a mask that is element-wise multiplied to the input to produce enhanced filterbanks. A loss function is defined to minimize the difference between the enhanced filterbanks and the filterbanks from the clean speech during training. \n\nVoiceFilter-Lite is a plug-and-play model, which allows the application in which it\u2019s implemented to easily bypass it if the speaker did not enroll their voice. This also means that the speech recognition model and the VoiceFilter-Lite model can be separately trained and updated, which largely reduces engineering complexity in the deployment process.\n\n\nWhen speech separation models are used for improving speech recognition, two types of error could occur: under-suppression, when the model fails to filter out noisy components from the signal; and over-suppression, when the model fails to preserve useful signal, resulting in some words being dropped from the recognized text. Over-suppression is especially problematic since modern speech recognition models are usually already trained with extensively augmented data (such as  and ), and thus are more robust to under-suppression. \n\nVoiceFilter-Lite addresses the over-suppression issue with two novel approaches. First, it uses an asymmetric loss during the training process, such that the model is less tolerant to over-suppression than under-suppression. Second, it predicts the type of noise at runtime, and adaptively adjusts the suppression strength according to this prediction.\n\nWith these two solutions, the VoiceFilter-Lite model retains great performance on streaming speech recognition for other scenarios, such as single-speaker speech under quiet or various noise conditions, while still providing significant improvement on overlapping speech. From our experiments, we observed a 25.1% improvement of word error rate after the 2.2MB VoiceFilter-Lite model is applied on additive overlapping speech. For reverberant overlapping speech, which is a more challenging task to simulate far-field devices such as smart home speakers, we also observed a 14.7% improvement of word error rate with VoiceFilter-Lite.\n\n\nWhile VoiceFilter-Lite has shown great promise for various on-device speech applications, we are also exploring several other directions to make VoiceFilter-Lite more useful. First, our current model is trained and evaluated with English speech only. We are excited about adopting the same technology to improve speech recognition for more languages. Second, we would like to directly optimize the speech recognition loss during the training of VoiceFilter-Lite, which can potentially further improve speech recognition beyond overlapping speech.", "date": "\nWednesday, November 11, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Objectron Dataset\n", "author": ["Posted by Adel Ahmadyan and Liangkai Zhang, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/11/announcing-objectron-dataset.html", "abstract": "The state of the art in machine learning (ML) has achieved exceptional accuracy on many computer vision tasks solely by training models on photos. Building upon these successes and advancing 3D object understanding has great potential to power a wider range of applications, such as augmented reality, robotics, autonomy, and image retrieval. For example, earlier this year we released , a set of real-time 3D object detection models designed for mobile devices, which were trained on a fully annotated, real-world 3D dataset, that can predict objects\u2019 3D bounding boxes. \n\nYet, understanding objects in 3D remains a challenging task due to the lack of large real-world datasets compared to 2D tasks (e.g., , , and ).  To empower the research community for continued advancement in 3D object understanding, there is a strong need for the release of object-centric video datasets, which capture more of the 3D structure of an object, while matching the data format used for many vision tasks (i.e., video or camera streams), to aid in the training and benchmarking of machine learning models. \n\nToday, we are excited to release the , a collection of short, object-centric video clips capturing a larger set of common objects from different angles. Each video clip is accompanied by AR session metadata that includes camera poses and sparse point-clouds. The data also contain manually annotated 3D bounding boxes for each object, which describe the object\u2019s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images collected from a geo-diverse sample (covering 10 countries across five continents).\n\n\nAlong with the dataset, we are also sharing a  for four categories of objects \u2014 shoes, chairs, mugs, and cameras. These models are released in , Google's open source framework for cross-platform customizable ML solutions for live and streaming media, which also powers  like on-device real-time ,  and .\n\nIn contrast to the previously released , these newest versions utilize a two-stage architecture. The first stage employs the  model to find the 2D crop of the object. The second stage then uses the image crop to estimate the 3D bounding box while simultaneously computing the 2D crop of the object for the next frame, so that the object detector does not need to run every frame. The second stage 3D bounding box predictor runs at 83 FPS on Adreno 650 mobile GPU.\n\n\nWith ground truth annotations, we evaluate the performance of 3D object detection models using 3D  (IoU) similarity statistics, a commonly used metric for computer vision tasks, which measures how close the bounding boxes are to the ground truth. \n\nWe propose an algorithm for computing accurate 3D IoU values for general 3D-oriented boxes. First, we compute the intersection points between faces of the two boxes using . This is similar to , a technique used in computer graphics. The volume of the intersection is computed by the  of all the clipped polygons. Finally, the IoU is computed from the volume of the intersection and volume of the union of two boxes. We are releasing the  along with the dataset.\n\n\nThe technical details of the Objectron dataset, including usage and tutorials, are available on the . The dataset includes bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes, and is stored in the  on  with the following assets:\n\nWith the dataset, we are also open-sourcing a data-pipeline to parse the dataset in popular Tensorflow, PyTorch and Jax frameworks. Example  are also provided.\n\nBy releasing this Objectron dataset, we hope to enable the research community to push the limits of 3D object geometry understanding. We also hope to foster new research and applications, such as , , and unsupervised learning. Stay tuned for future activities and developments by  and visiting .", "date": "\nMonday, November 9, 2020\n"},
{"website": "Google-AI", "title": "\nBackground Features in Google Meet, Powered by Web ML\n", "author": ["Posted by Tingbo Hou and Tyler Mullen, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/10/background-features-in-google-meet.html", "abstract": "Video conferencing is becoming ever more critical in people's work and personal lives. Improving that experience with privacy enhancements or fun visual touches can help center our focus on the meeting itself. As part of this goal, we recently announced ways to  and  your background in , which use machine learning (ML) to better highlight participants regardless of their surroundings. Whereas other solutions require installing additional software, Meet\u2019s features are powered by cutting-edge web ML technologies built with  that work directly in your browser \u2014 no extra steps necessary. One key goal in developing these features was to provide real-time, in-browser performance on almost all modern devices, which we accomplished by combining efficient on-device ML models, -based rendering, and web-based ML inference via  and . \n\n\nThe new features in Meet are developed with , Google's open source framework for cross-platform customizable ML solutions for live and streaming media, which also powers ML solutions like on-device real-time ,  and . \n\nA core need for any on-device solution is to achieve high performance. To accomplish this, MediaPipe\u2019s  leverages , a low-level binary code format designed specifically for web browsers that improves speed for compute-heavy tasks. At runtime, the browser converts WebAssembly instructions into native machine code that executes much faster than traditional  code. In addition,  recently introduced support for WebAssembly , which processes multiple data points with each instruction, resulting in a performance boost of more than 2x.\n\nOur solution first processes each video frame by segmenting a user from their background (more about our segmentation model later in the post) utilizing ML inference to compute a low resolution mask. Optionally, we further refine the mask to align it with the image boundaries. The mask is then used to render the video output via , with the background blurred or replaced. \n\nIn the current version, model inference is executed on the client\u2019s CPU for low power consumption and widest device coverage. To achieve real-time performance, we designed efficient ML models with inference accelerated by the  library, the first inference engine specifically designed for the novel WebAssembly SIMD specification. Accelerated by  and , the segmentation model can run in real-time on the web.\n\nEnabled by MediaPipe's flexible configuration, the background blur/replace solution adapts its processing based on device capability. On high-end devices it runs the full pipeline to deliver the highest visual quality, whereas on low-end devices it continues to perform at speed by switching to  compute-light ML models and bypassing the mask refinement.\n\n\nOn-device ML models need to be ultra lightweight for fast inference, low power consumption, and small download size. For models running in the browser, the input resolution greatly affects the number of floating-point operations (FLOPs) necessary to process each frame, and therefore needs to be small as well. We downsample the image to a smaller size before feeding it to the model. Recovering a segmentation mask as fine as possible from a low-resolution image adds to the challenges of model design. \n\nThe overall segmentation network has a symmetric structure with respect to encoding and decoding, while the decoder blocks (light green) also share a symmetric layer structure with the encoder blocks (light blue). Specifically,  with global average pooling is applied in both encoder and decoder blocks, which is friendly to efficient CPU inference.\n\nWe modified  as the encoder, which has been tuned by  for the best performance with low resource requirements. To reduce the model size by 50%, we exported our model to TFLite using float16 quantization, resulting in a slight loss in weight precision but with no noticeable effect on quality. The resulting model has 193K parameters and is only 400KB in size.\n\n\nOnce segmentation is complete, we use OpenGL shaders for video processing and effect rendering, where the challenge is to render efficiently without introducing artifacts. In the refinement stage, we apply a joint  to smooth the low resolution mask.\n\nThe blur shader simulates a  effect by adjusting the blur strength at each pixel proportionally to the segmentation mask values, similar to the  (CoC) in optics. Pixels are weighted by their CoC radii, so that foreground pixels will not bleed into the background. We implemented  for the weighted blur, instead of the popular , as it removes halo artifacts surrounding the person. The blur is performed at a low resolution for efficiency, and blended with the input frame at the original resolution.\n\nFor background replacement, we adopt a compositing technique, known as , for blending segmented persons and customized background images. Light wrapping helps soften segmentation edges by allowing background light to spill over onto foreground elements, making the compositing more immersive. It also helps minimize halo artifacts when there is a large contrast between the foreground and the replaced background.\n\n\nTo optimize the experience for different devices, we provide model variants at multiple input sizes (i.e., 256x144 and 160x96 in the current release), automatically selecting the best according to available hardware resources. \n\nWe evaluated the speed of model inference and the end-to-end pipeline on two common devices: MacBook Pro 2018 with 2.2 GHz 6-Core Intel Core i7, and Acer Chromebook 11 with Intel Celeron N3060. For 720p input, the MacBook Pro can run the higher-quality model at 120 FPS and the end-to-end pipeline at 70 FPS, while the Chromebook runs inference at 62 FPS with the lower-quality model and 33 FPS end-to-end.\n\nFor quantitative evaluation of model accuracy, we adopt the popular metrics of  (IOU) and . Both models achieve high quality, especially for having such a lightweight network:\n\nWe also release the accompanying  for our segmentation models, which details our fairness evaluations. Our evaluation data contains images from 17 geographical subregions of the globe, with annotations for skin tone and gender. Our analysis shows that the model is consistent in its performance across the various regions, skin-tones, and genders, with only small deviations in IOU metrics. \n\n\nWe introduced a new in-browser ML solution for blurring and replacing your background in Google Meet. With this, ML models and OpenGL shaders can run efficiently on the web. The developed features achieve real-time performance with low power consumption, even on low-power devices.", "date": "\nFriday, October 30, 2020\n"},
{"website": "Google-AI", "title": "\nExperimenting with Automatic Video Creation from a Web Page\n", "author": ["Posted by Peggy Chi, Senior Research Scientist and Irfan Essa, Senior Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/10/experimenting-with-automatic-video.html", "abstract": "At Google, we're actively exploring how people can use creativity tools powered by machine learning and computational methods when producing multimedia content, from  and , to  and more. One creative process in particular, video production, can especially benefit from such tools, as it requires a series of decisions about what content is best suited to a target audience, how to position the available assets within the field of view, and what temporal arrangement will yield the most compelling narrative. But what if one could leverage existing assets, such as a website, to get a jump-start on video creation? Businesses commonly host websites that contain rich visual representations about their services or products, all of which could be repurposed for other multimedia formats, such as videos, potentially enabling those without extensive resources the ability to reach a broader audience.\n\nIn \u201c\u201d, published at , we introduce URL2Video, a research prototype pipeline to automatically convert a web page into a short video, given temporal and visual constraints provided by the content owner. URL2Video extracts assets (text, images, or videos) and their design styles (including fonts, colors, graphical layouts, and hierarchy) from HTML sources and organizes the visual assets into a sequence of shots, while maintaining a look-and-feel similar to the source page. Given a user-specified aspect ratio and duration, it then renders the repurposed materials into a video that is ideal for product and service advertising.\n\n\nAssume a user provides an URL to a web page that illustrates their business. The URL2Video pipeline automatically selects key content from the page and decides the temporal and visual presentation of each asset, based on a set of heuristics derived from an interview study with designers who were familiar with web design and video ad creation. These designer-informed heuristics capture common video editing styles, including content hierarchy, constraining the amount of information in a shot and its time duration, providing consistent color and style for branding, and more. Using this information, the URL2Video pipeline parses a web page, analyzing the content and selecting visually salient text or images while preserving their design styles, which it organizes according to the video specifications provided by the user. \n\n\nGiven a webpage URL, URL2Video extracts  (DOM) information and multimedia materials. For the purposes of our research prototype, we limited the domain to static web pages that contain salient assets and headings preserved in an HTML hierarchy that follows recent , which encourage the use of prominent elements, distinct sections, and an order of visual focus that guides readers in perceiving information. URL2Video identifies such visually-distinguishable elements as a candidate list of asset groups, each of which may contain a heading, a product image, detailed descriptions, and call-to-action buttons, and  captures both the raw assets (text and multimedia files) and detailed design specifications (HTML tags, CSS styles, and rendered locations) for each element. It then ranks the asset groups by assigning each a priority score based on their visual appearance and annotations, including their HTML tags, rendered sizes, and ordering shown on the page. In this way, an asset group that occupies a larger area at the top of the page receives a higher score.\n\n\nWe consider two goals when composing a video: (1) each video shot should provide concise information, and (2) the visual design should be consistent with the source page. Based on these goals and the video constraints provided by the user, including the intended video duration (in seconds) and aspect ratio (commonly 16:9, 4:3, 1:1, etc.), URL2Video automatically selects and orders the asset groups to optimize the total priority score. To make the content concise, it presents only dominant elements from a page, such as a headline and a few multimedia assets. It constrains the duration of each visual element for viewers to perceive the content. In this way, a short video highlights the most salient information from the top of the page, and a longer video contains more campaigns or products. \n\n\nGiven an ordered list of assets based on the DOM hierarchy, URL2Video follows the design heuristics obtained from interview studies to make decisions about both the  and  arrangement to present the assets in individual shots. It transfers the graphical layout of elements into the video\u2019s aspect ratio, and applies the style choices including fonts and colors. To make a video more dynamic and engaging, it adjusts the presentation timing of assets. Finally, it renders the content into a video in the MPEG-4 container format.\n\n\nThe interface to the research prototype allows the user to review the design attributes in each video shot extracted from the source page, reorder the materials, change the detailed design, such as colors and fonts, and adjust the constraints to generate a new video.\n\n\nWe demonstrate the performance of the end-to-end URL2Video pipeline on a variety of existing web pages. Below we highlight an example result where URL2Video converts a page that embeds multiple short video clips into a 12-second output video. Note how the pipeline makes automatic editing decisions on font and color choices, timing, and content ordering in a video captured from the source page.\n\nThe video below provides further demonstration:\n\nTo evaluate the automatically-generated videos, we conducted a user study with designers at Google. Our results show that URL2Video effectively extracted design elements from a web page and supported designers by bootstrapping the video creation process. \n\n\nWhile this current research focuses on the visual presentation, we are developing new techniques that support the audio track and a voiceover in video editing. All in all, we envision a future where creators focus on making high-level decisions and an ML model interactively suggests detailed temporal and graphical edits for a final video creation on multiple platforms.", "date": "\nThursday, October 29, 2020\n"},
{"website": "Google-AI", "title": "\nEstimating the Impact of Training Data with Reinforcement Learning\n", "author": ["Posted by Jinsung Yoon and Sercan O. Arik, Research Scientists, Cloud AI Team, Google Research"], "link": "http://ai.googleblog.com/2020/10/estimating-impact-of-training-data-with.html", "abstract": "suggests that not all data samples are equally useful for training, particularly for  (DNNs). Indeed, if a dataset contains low-quality or incorrectly labeled data, one can often  by removing a significant portion of training samples.  Moreover, in cases where there is a mismatch between the train and test datasets (e.g., due to difference in train and test location or time), one can also achieve higher performance by carefully  in the training set to those most relevant for the test scenario.  Because of the ubiquity of these scenarios, accurately quantifying the values of training samples has great potential for improving model performance on real-world datasets.\n  In addition to improving model performance, assigning a quality value to individual data can also enable new use cases. It can be used to suggest better practices for data collection, e.g., what kinds of additional data would benefit the most, and can be used to construct large-scale training datasets more efficiently, e.g., by web searching using the labels as keywords and filtering out less valuable data.\nIn \u201c\u201d, accepted at , we address the challenge of quantifying the value of training data using a novel approach based on meta-learning. Our method integrates data valuation into the training procedure of a predictor model that learns to recognize samples that are more valuable for the given task, improving both predictor and data valuation performance. We have also launched four  Notebooks that exemplify the use cases of DVRL and are designed to be conveniently adapted to other tasks and datasets, such as\u00a0,\u00a0,\u00a0\u00a0and\u00a0.\n\nNot all data are equal for a given ML model \u2014 some have greater relevance for the task at hand or are more rich in informative content than others. So how does one evaluate the value of a single datum? At the granularity of a full dataset, it is straightforward; one can simply train a model on the entire dataset and use its performance on a test set as its value. However, estimating the value of  is far more difficult, especially for complex models that rely on large-scale datasets, because it is computationally infeasible to re-train and re-evaluate a model on all possible subsets. \n\nTo tackle this, researchers have explored permutation-based methods (e.g., ), and game theory-based methods (e.g., ). However, even the best current methods are far from being computationally feasible for large datasets and complex models, and their data valuation performance is limited. Concurrently,  have been developed to estimate the weight values using a meta-objective. But rather than prioritizing learning from high value data samples, their data value mapping is typically based on gradient descent learning or other heuristic approaches that alter the conventional predictor model training dynamics, which can result in performance changes that are unrelated to the value of individual data points.\n\n\nTo infer the data values, we propose a  (DVE) that estimates data values and selects the most valuable samples to train the predictor model. This selection operation is fundamentally non-differentiable and thus conventional -based methods cannot be used. Instead, we propose to use  (RL) such that the supervision of the DVE is based on a reward that quantifies the predictor performance on a small (but clean) validation set. The reward guides the optimization of the policy towards the action of optimal data valuation, given the state and input samples. Here, we treat the predictor model learning and evaluation framework as the environment, a novel application scenario of RL-assisted machine learning.\n\nWe evaluate the data value estimation quality of DVRL on multiple types of datasets and use cases. \n\n\nWe propose a novel meta learning framework for data valuation which determines how likely each training sample will be used in training of the predictor model. Unlike previous works, our method integrates data valuation into the training procedure of the predictor model, allowing the predictor and DVE to improve each other's performance. We model this data value estimation task using a DNN trained through RL with a reward obtained from a small validation set that represents the target task performance.  In a computationally-efficient way, DVRL can provide high quality ranking of training data that is useful for domain adaptation, corrupted sample discovery and robust learning.  We show that DVRL significantly outperforms alternative methods on diverse types of tasks and datasets.", "date": "\nWednesday, October 28, 2020\n"},
{"website": "Google-AI", "title": "\nRethinking Attention with Performers\n", "author": ["Posted by Krzysztof Choromanski and Lucy Colwell, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html", "abstract": "have achieved state-of-the-art results across a diverse range of domains, including , , , and even . The core block of every Transformer architecture is the  which computes similarity scores for all pairs of positions in an input sequence. This however, scales poorly with the length of the input sequence, requiring  computation time to produce all similarity scores, as well as quadratic memory size to construct a matrix to store these scores.\n\nFor applications where long-range attention is needed, several fast and more space-efficient proxies have been proposed such as , but a far more common way is to rely on . Sparse attention reduces computation time and the memory requirements of the attention mechanism by computing a limited selection of similarity scores from a sequence rather than all possible pairs, resulting in a sparse matrix rather than a full matrix. These sparse entries may be manually proposed, found via optimization methods, learned, or even randomized, as demonstrated by such methods as  and. Since sparse matrices can also be represented by , sparsification methods are also motivated by the  literature, with specific relationships to attention outlined in . Such sparsity-based architectures usually require additional layers to implicitly produce a full attention mechanism. \t\t\n\nUnfortunately, sparse attention methods can still suffer from a number of limitations. (1) They require efficient sparse-matrix multiplication operations, which are not available on all accelerators; (2) they usually do not provide rigorous theoretical guarantees for their representation power; (3) they are optimized primarily for Transformer models and generative pre-training; and (4) they usually stack more attention layers to compensate for sparse representations, making them difficult to use with other pre-trained models, thus requiring . In addition to these shortcomings, sparse attention mechanisms are often still not sufficient to address the full range of problems to which regular attention methods are applied, such as . There are also some operations that cannot be sparsified, such as the commonly used , which normalizes similarity scores in the attention mechanism and is used heavily in .\n\nTo resolve these issues, we introduce the , a Transformer architecture with attention mechanisms that scale linearly, thus enabling faster training while allowing the model to process longer lengths, as required for certain image datasets such as  and text datasets such as . The Performer uses an efficient (linear) generalized attention framework, which allows a broad class of attention mechanisms based on different similarity measures (kernels). The framework is implemented by our novel , which provides scalable  and  estimation of attention mechanisms that can be expressed by random feature map decompositions (in particular, regular softmax-attention). We obtain strong accuracy guarantees for this method while preserving linear space and time complexity, which can also be applied to standalone softmax operations. \n\n\nIn the original attention mechanism, the  and  inputs, corresponding respectively to rows and columns of a matrix, are multiplied together and passed through a softmax operation to form an attention matrix, which stores the similarity scores. Note that in this method, one  cannot decompose the query-key product back into its original query and key components after passing it into the nonlinear softmax operation. However, it is possible to decompose the attention matrix backto a product of random nonlinear  of the original queries and keys, otherwise known as , which allows one to encode the similarity information in a more efficient manner.\n\nRegular softmax-attention can be seen as a special case with these nonlinear functions defined by exponential functions and Gaussian projections. Note that we can also reason inversely, by implementing more general nonlinear functions , implicitly defining other types of similarity measures, or , on the query-key product. We frame this as based on earlier work in . Although for most kernels, closed-form formulae do not exist, our mechanism can still be applied since it does not rely on them.\n\nTo the best of our knowledge, we are the first to show that  attention matrix can be  approximated in downstream Transformer-applications using random features. The novel mechanism enabling this is the use of , i.e., nonlinear functions of the original queries and keys, which prove to be crucial for avoiding instabilities during training and provide more accurate approximation of the regular softmax attention mechanism.\n\n\nThe decomposition described above allows one to store the implicit attention matrix with linear, rather than quadratic, memory complexity. One can also obtain a  attention mechanism using this decomposition. While the original attention mechanism multiplies the stored attention matrix with the  input to obtain the final result, after decomposing the attention matrix, one can rearrange matrix multiplications to approximate the result of the regular attention mechanism, without explicitly constructing the quadratic-sized attention matrix. This ultimately leads to .\n\nThe above analysis is relevant for so-called  attention, i.e., non-causal attention  where there is no notion of past and future. For  (causal) attention, where tokens do not attend to other tokens appearing later in the input sequence, we slightly modify the approach to use , which only store running totals of matrix computations rather than storing an explicit lower-triangular regular attention matrix.\n\n\nWe first benchmark the space- and time-complexity of the Performer and show that the attention speedups and memory reductions are empirically nearly optimal, i.e., very close to simply not using an attention mechanism at all in the model.\n\nWe further show that the Performer, using our unbiased softmax approximation, is backwards compatible with pretrained Transformer models after a bit of fine-tuning, which could potentially lower energy costs by improving inference speed, without having to fully retrain pre-existing models.  \n\n\nProteins are large molecules with complex 3D structures and specific functions that are essential to life. Like words, proteins are specified as linear sequences where each character is one of 20 amino acid building blocks. Applying Transformers to large unlabeled corpora of protein sequences (e.g. ) yields  that can be used to make accurate predictions about the folded, functional macromolecule. Performer-ReLU (which uses -based attention, an instance of generalized attention that is different from softmax) performs strongly at modeling protein sequence data, while Performer-Softmax matches the performance of the Transformer, as predicted by our theoretical results. \n\nBelow we visualize a protein Performer model, trained using the ReLU-based approximate attention mechanism. Using the Performer to estimate similarity between amino acids recovers similar  to well-known  obtained by analyzing  across carefully curated sequence alignments. More generally, we find local and global attention patterns consistent with . The dense attention approximation of the Performer has the potential to capture  across multiple protein sequences. As a proof of concept, we train models on long concatenated protein sequences, which overloads the memory of a regular Transformer model, but not the Performer due to its space efficiency.\n\n\nOur work contributes to the recent efforts on  and . Our method is interoperable with other techniques like reversible layers and we have even integrated . We provide the links for the , , and the . We believe that our research opens up a brand new way of thinking about attention, Transformer architectures, and even kernel methods.", "date": "\nFriday, October 23, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Recipients of the 2020 Award for Inclusion Research\n", "author": ["Posted by Negar Saei, Program Manager, Google Research"], "link": "http://ai.googleblog.com/2020/10/announcing-2020-award-for-inclusion.html", "abstract": "At Google, it is our ongoing goal to support faculty who are conducting innovative research that will have positive societal impact. As part of that goal, earlier this year we launched the  program, a global program that supports academic research in computing and technology addressing the needs of underrepresented populations. The Award for Inclusion Research program allows faculty and Google researchers an opportunity to partner on their research initiatives and build new and constructive long-term relationships.\n\nWe received 100+ applications from over 100 universities, globally, and today we are excited to announce the 16 proposals chosen for funding, focused on an array of topics around diversity and inclusion, algorithmic bias, education innovation, health tools, accessibility, gender bias, AI for social good, security, and social justice. The proposals include 25 principal investigators who focus on making the community stronger through their research efforts. \n\nCongratulations to announce this year\u2019s recipients:\n\n  \"\"\nAnicia Peters (University of Namibia) and Shaimaa Lazem (City for Scientific Research and Technological Applications, Egypt)\n\n  \"\"\nAntonios Anastasopoulos (George Mason University)\n\n  \"\n  Aqueasha Martin-Hammond (Indiana University - Purdue University Indianapolis) and Tanjala S. Purnell (Johns Hopkins University)\n  \"\"\n  Destenie Nock and Constantine Samaras (Carnegie Mellon University)\n  \"\"\n  Erin Walker (University of Pittsburgh) and Leshell Hatley (Coppin State University)\n  \"\"\n  Hinrich Schuetze (LMU Munich)\n  \"\"\n  Jacob O. Wobbrock (University of Washington)\n  \"\"\n  Jasmine McNealy (University of Florida)\n  \"\n  Karen Elizabeth Fisher (University of Washington) and Yacine Ghamri-Doudane (University of La Rochelle)\n  \"\"\n  Latifa Jackson (Howard University) and Hasan Jackson (Howard University)\n  \"\"\n  Legand Burge (Howard University) and Marlon Mejias (University of North Carolina at Charlotte)\n  \"\"\n  Maria De-Arteaga (University of Texas at Austin)\n  \"\"\n  Meenakshi Balakrishnan (Indian Institute of Technology Delhi, India) and Volker Sorge (University of Birmingham)\n  \"\"\n  Nicki Washington (Duke University)\n  \"\"\n  Steve Oney (University of Michigan)\n  \"\"\n  Timothy Sherwood and Sharon Tettegah (University of California, Santa Barbara)", "date": "\nWednesday, October 21, 2020\n"},
{"website": "Google-AI", "title": "\nRecreating Historical Streetscapes Using Deep Learning and Crowdsourcing\n", "author": ["Posted by Raimondas Kiveris, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/10/recreating-historical-streetscapes.html", "abstract": "For many, gazing at an old photo of a city can evoke feelings of both nostalgia and wonder \u2014 what was it like to walk through Manhattan in the 1940s? How much has the street one grew up on changed? While Google Street View allows people to see what an area looks like in the present day, what if you want to explore how places looked in the past? \n\nTo create a rewarding \u201ctime travel\u201d experience for both research and entertainment purposes, we are launching a  (pronounced as re\u201d\"), an open source, scalable system running on  and  that can reconstruct cities from historical maps and photos, which represents an implementation of our suite of open source tools earlier this year. Referencing the common prefix meaning  or ,  is meant to represent the themes of , ,  and  behind this crowdsourced research effort, and consists of three components:\n\nOur goal is for  to become a compendium that allows history enthusiasts to virtually experience historical cities around the world, aids researchers, policy makers and educators, and provides a dose of nostalgia to everyday users.        \n\n\nReconstructing how cities used to look at scale is a challenge \u2014 historical image data is more difficult to work with than modern data, as there are far fewer images available and much less metadata captured from the images. To help with this difficulty, the  maps module is a suite of open source tools that work together to create a map server with a , allowing users to jump back and forth between time periods using a slider. These tools allow users to upload scans of historical print maps, georectify them to match real world coordinates, and then convert them to vector format by tracing their geographic features. These vectorized maps are then served on a tile server and rendered as , which lets the user zoom in and pan around. \n\nThe entry point of the  maps module is , a web app that allows users to upload historical images of maps and georectify them by finding control points on the historical map and corresponding points on a base map. The next app, , allows users to load the georectified historical maps as the background and then trace their geographic features (e.g., building footprints, roads, etc.). This traced data is stored in an  (OSM) vector format. They are then converted to vector tiles and served from the  app, a vector tile server. Finally, our map renderer, , visualizes the spatiotemporal vector tiles allowing the users to navigate space and time on historical maps. These tools were built on top of numerous open source resources including OpenStreetMap, and we intend for our tools and data to be completely open source as well.\n\n\nThe  module aims to reconstruct the detailed full 3D structures of historical buildings using the associated images and maps data, organize these 3D models properly in one repository, and render them on the historical maps with a time dimension. \n\nIn many cases, there is only one historical image available for a building, which makes the 3D reconstruction an extremely challenging problem. To tackle this challenge, we developed a coarse-to-fine reconstruction-by-recognition algorithm.\n\nStarting with footprints on maps and fa\u00e7ade regions in historical images (both are annotated by crowdsourcing or detected by automatic algorithms), the footprint of one input building is extruded upwards to generate its coarse 3D structure. The height of this extrusion is set to the number of floors from the corresponding metadata in the maps database. \n\nIn parallel, instead of directly inferring the detailed 3D structures of each fa\u00e7ade as one entity, the 3D reconstruction pipeline recognizes all individual constituent components (e.g., windows, entries, stairs, etc.) and reconstructs their 3D structures separately based on their categories. Then these detailed 3D structures are merged with the coarse one for the final 3D mesh. The results are stored in a 3D repository and ready for 3D rendering. \n\nThe key technology powering this feature is a number of state-of-art deep learning models:\n\n\n\n\n  With , we have developed tools that facilitate crowdsourcing to tackle the main challenge of insufficient historical data when recreating virtual cities. The 3D experience is still a work-in-progress and we aim to improve it with future updates. We hope  acts as a nexus for an active community of enthusiasts and casual users that not only utilizes our historical datasets and open source code, but actively contributes to both.", "date": "\nThursday, October 15, 2020\n"},
{"website": "Google-AI", "title": "\nMeasuring Gendered Correlations in Pre-trained NLP Models\n", "author": ["Posted by Kellie Webster, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/10/measuring-gendered-correlations-in-pre.html", "abstract": "(NLP) has seen significant progress over the past several years, with pre-trained models like , , , and  achieving remarkable accuracy across a variety of tasks. In , representations are learned from a large text corpus, e.g., , by repeatedly masking out words and trying to predict them (this is called ). The resulting representations encode rich information about language and correlations between concepts, such as  and . There is then a second training stage, , in which the model uses task-specific training data to learn how to use the general pre-trained representations to do a concrete task, like . Given the broad adoption of these representations in many NLP tasks, it is crucial to understand the information encoded in them and how any learned correlations affect performance downstream, to ensure the application of these models aligns with our . \n\nIn \u201c\u201d we perform a case study on BERT and its low-memory counterpart ALBERT, looking at correlations related to gender, and formulate a series of best practices for using pre-trained language models. We present experimental results over public model checkpoints and an academic task dataset to illustrate how the best practices apply, providing a foundation for exploring settings beyond the scope of this case study. We will soon release a series of checkpoints, , which reduce gendered correlations while maintaining state-of-the-art accuracy on standard NLP task metrics.\n\n\nTo understand how correlations in pre-trained representations can affect downstream task performance, we apply a diverse set of evaluation metrics for studying the representation of gender. Here, we\u2019ll discuss results from one of these tests, based on , which is the capability that allows models to understand the correct antecedent to a given pronoun in a sentence. For example, in the sentence that follows, the model should recognize  refers to the, and not to the. \n\nThe standard academic formulation of the task is the  test (), and we measure how accurate a model is at coreference resolution in a general setting using an  over this data (as in ). Since OntoNotes represents only one data distribution, we also consider the  benchmark that provides additional, balanced data designed to identify when model associations between gender and profession incorrectly influence coreference resolution. High values of the WinoGender metric (close to one) indicate a model is basing decisions on  (e.g., associating  with the female gender and not male). When model decisions have no consistent association between gender and profession, the score is zero, which suggests that decisions are based on some other information, such as sentence structure or semantics.\n\nIn this study, we see that neither the (Large)  or  public model achieves zero score on the WinoGender examples, despite achieving impressive accuracy on OntoNotes (close to 100%). At least some of this is due to models preferentially using gendered correlations in reasoning. This isn\u2019t completely surprising: there are a range of cues available to understand text and it is possible for a general model to pick up on any or all of these. However, there is reason for caution, as it is undesirable for a model to make predictions primarily based on gendered correlations learned as priors rather than the evidence available in the input.\n\n\nGiven that it is possible for unintended correlations in pre-trained model representations to affect downstream task reasoning, we now ask: what can one do to mitigate any risk this poses when developing new NLP models?\n\n\nWe believe these best practices provide a starting point for developing robust NLP systems that perform well across the broadest possible range of linguistic settings and applications.  Of course these techniques on their own are not sufficient to capture and remove all potential issues. Any model deployed in a real-world setting should undergo rigorous testing that considers the many ways it will be used, and implement safeguards to ensure alignment with ethical norms, such as Google's AI Principles. We look forward to developments in evaluation frameworks and data that are more expansive and inclusive to cover the many uses of language models and the breadth of people they aim to serve.", "date": "\nWednesday, October 14, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2020 Google PhD Fellows\n", "author": ["Posted by Susie Kim, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2020/10/announcing-2020-google-phd-fellows.html", "abstract": "Google created the  in 2009 to recognize and support outstanding graduate students who seek to influence the future of technology by pursuing exceptional research in computer science and related fields. Now in its twelfth year, these Fellowships have helped support approximately 500 graduate students globally in North America and Europe, Africa, Australia, East Asia, and India.\n\nIt is our ongoing goal to continue to support the academic community as a whole, and these Fellows as they make their mark on the world. We congratulate all of this year\u2019s awardees! \n\n\nJan van den Brand, \nMahsa Derakhshan, \nSidhanth Mohanty, \n\n\nConnor Brennan, \n\n\nAbdelkareem Bedri, \nBrendan David-John, \nHiromu Yakura, \nManaswi Saha, \nMuratcan Cicek, \nPrashan Madumal, \n\n\nAlon Brutzkus, \nChin-Wei Huang, \nEli Sherman, \nEsther Rolf, \nImke Mayer, \nJean Michel Sarr, \nLei Bai, \nNontawat Charoenphakdee, \nPreetum Nakkiran, \nSravanti Addepalli, \nTaesik Gong, \nVihari Piratla, \nVishakha Patil, \nWilson Tsakane Mongwe, \nXinshi Chen, \nYadan Luo, \n\n\nBenjamin van Niekerk, \nEric Heiden, \nGyeongsik Moon, \nHou-Ning Hu, \nNan Wu,\nShaoshuai Shi, \nYaman Kumar, \nYifan Liu, \nYu Wu, \nZhengqi Li, \n\n\nXiaofan Zhang, \n\n\nAnjalie Field, \nMingda Chen, \nShang-Yu Su, \nYanai Elazar, \n\n\nJulien Gamba, \nShuwen Deng, \nYunusa Simpa Abdulsalm, \n\n\nAdriana Sejfia, \nJohn Cyphert, \n\n\nAmira Abbas, \nMozafari Ghoraba Fereshte, \n\n\nYanqing Peng, \n\n\nHuynh Nguyen Van, \nMichael Sammler, \nSihang Liu, \nYun-Zhan Cai,", "date": "\nThursday, October 8, 2020\n"},
{"website": "Google-AI", "title": "\nMassively Large-Scale Distributed Reinforcement Learning with Menger\n", "author": ["Posted by Amir Yazdanbakhsh, Research Scientist and Junchao Chen, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/10/massively-large-scale-distributed.html", "abstract": "In the last decade,  (RL) has become one of the most promising research areas in machine learning and has demonstrated great potential for solving sophisticated real-world problems, such as  and , and solving challenging games (e.g., , , and ). In simplest terms, an RL infrastructure is a loop of data collection and training, where  explore the environment and collect samples, which are then sent to the  to train and update the model.  Most current RL techniques require many iterations over batches of millions of samples from the environment to learn a target task (e.g.,   learns from batches of 2 million frames every 2 seconds). As such, an RL infrastructure should not only scale efficiently (e.g., increase the number of actors) and collect an immense number of samples, but also be able to swiftly iterate over these extensive amounts of samples during training.\n\n  Today we introduce Menger, a massive large-scale distributed RL infrastructure with localized inference that scales up to several thousand actors across multiple processing clusters (e.g., ), reducing the overall training time in the task of chip placement. In this post we describe how we implement Menger using  for fast training iterations, and present its performance and scalability on the challenging task of . Menger reduces the training time by up to 8.6compared to a baseline .\n\n\nThere are various distributed RL systems, such as  and , each of which focus on optimizing a single particular design point in the space of distributed reinforcement learning systems. For example, while Acme uses local inference on each actor with frequent model retrieval from the learner, SEED RL benefits from a centralized inference design by allocating a portion of TPU cores for performing batched calls. The tradeoffs between these design points are (1) paying the communication cost of sending/receiving observations and actions to/from a centralized inference server or paying the communication cost of model retrieval from a learner and (2) the cost of inference on actors (e.g., CPUs) compared to accelerators (e.g., TPUs/GPUs). Because of the requirements of our target application (e.g., size of observations, actions, and model size), Menger uses local inference in a manner similar to Acme, but pushes the scalability of actors to virtually an unbounded limit. The main challenges to achieving massive scalability and fast training on accelerators include:\n\n\nTo address the first challenge, we introduce transparent and distributed caching components between the learner and the actors optimized in TensorFlow and backed by  (similar approach used in ). The main responsibility of the caching components is to strike a balance between the large number of requests from actors and the learner job. Adding these caching components not only significantly reduces the pressure on the learner to service the read requests, but also further distributes the actors across multiple  with a marginal communication overhead. In our study, we show that for a 16 MB model with 512 actors, the introduced caching components reduce the average read latency by a factor of ~4.0 leading to faster training iterations, especially for on-policy algorithms such as .\n\n\nTo deliver a high throughput input data pipeline, Menger uses , a recently open-sourced data storage system designed for machine learning applications that provides an efficient and flexible platform to implement  in a variety of on-policy/off-policy algorithms. However, using a single Reverb replay buffer service does not currently scale well in a distributed RL setting with thousands of actors, and simply becomes inefficient in terms of write throughput from actors.\nTo better understand the efficiency of the replay buffer in a distributed setting, we evaluate the average write latency for various payload sizes from 16 MB to 512 MB and a number of actors ranging from 16 to 2048. We repeat the experiment when the replay buffer and actors are placed on the same  cell. As the number of actors grows the average write latency also increases significantly. Expanding the number of actors from 16 to 2048, the average write latency increases by a factor of ~6.2 and ~18.9 for payload size 16 MB and 512 MB, respectively. This increase in the write latency negatively impacts the data collection time and leads to inefficiency in the overall training time.\n\nTo mitigate this, we use the  capability provided by  to increase the throughput between actors, learner, and replay buffer services. Sharding balances the write load from the large number of actors across multiple replay buffer servers, instead of throttling a single replay buffer server, and also minimizes the average write latency for each replay buffer server (as fewer actors share the same server). This enables Menger to scale efficiently to thousands of actors across multiple Borg cells.\n\n\nWe studied the benefits of Menger in the complex task of  for a large . Using 512 TPU cores, Menger achieves significant improvements in the training time (up to ~8.6, reducing the training time from ~8.6 hours down to merely one hour in the fastest configuration) compared to a strong . While Menger was optimized for TPUs, that the key factor for this performance gain is the architecture, and we would expect to see similar gains when tailored to use on GPUs. \n\nWe believe that Menger infrastructure and its promising results in the intricate task of chip placement demonstrate an innovative path forward to further shorten the chip design cycle and has the potential to not only enable further innovations in the chip design process, but other challenging real-world tasks as well.", "date": "\nFriday, October 2, 2020\n"},
{"website": "Google-AI", "title": "\nDeveloping Real-Time, Automatic Sign Language Detection for Video Conferencing\n", "author": ["Posted by Amit Moryossef, Research Intern, Google Research"], "link": "http://ai.googleblog.com/2020/10/developing-real-time-automatic-sign.html", "abstract": "Video conferencing should be accessible to everyone, including users who communicate using sign language. However, since most video conference applications transition window focus to those who speak aloud, it makes it difficult for signers to \u201cget the floor\u201d so they can communicate easily and effectively. Enabling real-time sign language detection in video conferencing is challenging, since applications need to perform classification using the high-volume video feed as the input, which makes the task computationally heavy. In part, due to these challenges, there is only limited research on sign language detection. \n\nIn \u201c\u201d, presented at  and demoed at , we present a real-time sign language detection model and demonstrate how it can be used to provide video conferencing systems a mechanism to identify the person signing as the active speaker. \n\n\nTo enable a real-time working solution for a variety of video conferencing applications, we needed to design a light weight model that would be simple to \u201cplug and play.\u201d Previous attempts to integrate models for video conferencing applications on the client side demonstrated the importance of a light-weight model that consumes fewer CPU cycles in order to minimize the effect on call quality. To reduce the input dimensionality, we isolated the information the model needs from the video in order to perform the classification of every frame. \n\nBecause sign language involves the user\u2019s body and hands, we start by running a pose estimation model, . This reduces the input considerably from an entire HD image to a small set of landmarks on the user\u2019s body, including the eyes, nose, shoulders, hands, etc. We use these landmarks to calculate the frame-to-frame , which quantifies user motion for use by the model without retaining user-specific information. Each pose is normalized by the width of the person\u2019s shoulders in order to ensure that the model attends to the person signing over a range of distances from the camera. The optical flow is then normalized by the video\u2019s frame rate before being passed to the model.\n\nTo test this approach, we used the  (DGS), which contains long videos of people signing, and includes span annotations that indicate in which frames signing is taking place. As a na\u00efve baseline, we trained a linear regression model to predict when a person is signing using optical flow data. This baseline reached around 80% accuracy, using only ~3\u03bcs (0.000003 seconds) of processing time per frame. By including the 50 previous frames\u2019 optical flow as context to the linear model, it is able to reach 83.4%.\n\nTo generalize the use of context, we used a  (LSTM) architecture, which contains memory over the previous timesteps, but no lookback. Using a single layer LSTM, followed by a linear layer, the model achieves up to 91.5% accuracy, with 3.5ms (0.0035 seconds) of processing time per frame.\n\nOnce we had a functioning sign language detection model, we needed to devise a way to use it for triggering the active speaker function in video conferencing applications. We developed a lightweight, real-time, sign language detection web demo that connects to various video conferencing applications and can set the user as the \u201cspeaker\u201d when they sign. This demo leverages  fast human pose estimation and sign language detection models running in the browser using , which enables it to work reliably in real-time.\n\nWhen the sign language detection model determines that a user is signing, it passes an ultrasonic audio tone through a , which can be detected by any video conferencing application as if the signing user is \u201cspeaking.\u201d The audio is transmitted at 20kHz, which is normally outside the hearing range for humans. Because video conferencing applications usually detect the audio \u201cvolume\u201d as talking rather than only detecting speech, this fools the application into thinking the user is speaking.\n\nYou can try  right now! By default, the demo acts as a sign language detector. The  as well as the \u00a0source code is available on GitHub.\n\n\nIn the following video, we demonstrate how the model might be used. Notice the yellow chart at the top left corner, which reflects the model\u2019s confidence in detecting that activity is indeed sign language. When the user signs, the chart values rise to nearly 100, and when she stops signing, it falls to zero. This process happens in real-time, at 30 frames per second, the maximum frame rate of the camera used.\n\n\nTo better understand how well the demo works in practice, we conducted a user experience study in which participants were asked to use our experimental demo during a video conference and to communicate via sign language as usual. They were also asked to sign over each other, and over speaking participants to test the speaker switching behavior. Participants responded positively that sign language was being detected and treated as audible speech, and that the demo successfully identified the signing attendee and triggered the conferencing system\u2019s audio meter icon to draw focus to the signing attendee.\n\n\nWe believe video conferencing applications should be accessible to everyone and hope this work is a meaningful step in this direction. We have demonstrated how our model could be leveraged to empower signers to use video conferencing more conveniently.", "date": "\nThursday, October 1, 2020\n"},
{"website": "Google-AI", "title": "\nAudiovisual Speech Enhancement in YouTube Stories\n", "author": ["Posted by Inbar Mosseri, Software Engineer and Michael Rubinstein, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/10/audiovisual-speech-enhancement-in.html", "abstract": "While tremendous efforts are invested in improving the quality of videos taken with smartphone cameras, the quality of in videos is often overlooked. For example, the speech of a subject in a video where there are multiple people speaking or where there is high background noise might be muddled, distorted, or difficult to understand. In an effort to address this, two years ago we introduced , a machine learning (ML) technology that uses both visual and audio cues to isolate the speech of a video\u2019s subject. By training the model on a , we are able to capture correlations between speech and visual signals such as mouth movements and facial expressions, which can then be used to separate the speech of one person in a video from another, or to separate speech from background sounds.  that this technology not only achieves state-of-the-art results in speech separation and enhancement (a noticeable 1.5dB improvement over audio-only models), but in particular can improve the results over audio-only processing when there are multiple people speaking, as the visual cues in the video help determine who is saying what.\n\nWe are now happy to make the Looking to Listen technology available to users through a new audiovisual Speech Enhancement feature in  (on iOS), allowing creators to take better selfie videos by automatically enhancing their voices and reducing background noise. Getting this technology into users\u2019 hands was no easy feat. Over the past year, we worked closely with users to learn how they would like to use such a feature, in what scenarios, and what balance of speech and background sounds they would like to have in their videos. We heavily optimized the Looking to Listen model to make it run efficiently on mobile devices, overall reducing the running time from 10 real-time on a desktop when our  came out, to 0.5 real-time performance on the phone. We also put the technology through extensive testing to verify that it performs consistently across different recording conditions and for people with different appearances and voices.\n\nOptimizing Looking to Listen to allow fast and robust operation on mobile devices required us to overcome a number of challenges. First, all processing needed to be done on-device within the client app in order to minimize processing time and to preserve the user\u2019s privacy; no audio or video information would be sent to servers for processing. Further, the model needed to co-exist alongside  used in the YouTube app in addition to the resource-consuming video recording itself. Finally, the algorithm needed to run quickly and efficiently on-device while minimizing battery consumption.\nThe first step in the Looking to Listen pipeline is to isolate thumbnail images that contain the faces of the speakers from the video stream. By leveraging  with GPU accelerated inference, this step is now able to be executed in just a few milliseconds. We then switched the model part that processes each thumbnail separately to a lighter weight  () architecture, which outputs visual features learned for the purpose of speech enhancement, extracted from the face thumbnails in 10 ms per frame. Because the compute time to embed the visual features is short, it can be done while the video is still being recorded. This avoids the need to keep the frames in memory for further processing, thereby reducing the overall memory footprint. Then, after the video finishes recording, the audio and the computed visual features are streamed to the audio-visual speech separation model which produces the isolated and enhanced speech.\n\nWe reduced the total number of parameters in the audio-visual model by replacing \u201cregular\u201d 2D convolutions with separable ones (1D in the frequency dimension, followed by 1D in the time dimension) with fewer filters. We then optimized the model further using  \u2014 a set of tools that enable running TensorFlow models on mobile devices with low latency and a small binary size. Finally, we reimplemented the model within the  framework in order to take advantage of built-in quantized training and  support.\n\nThese optimizations and improvements reduced the running time from 10 real-time on a desktop using the original formulation of , to 0.5 real-time performance using only an iPhone CPU;  and brought the model size down from 120MB to 6MB now, which makes it easier to deploy. Since YouTube Stories videos are short \u2014 limited to 15 seconds \u2014 the result of the video processing is available within a couple of seconds after the recording is finished.\n\nFinally, to avoid processing videos with clean speech (so as to avoid unnecessary computation), we first run our model only on the first two seconds of the video, then compare the speech-enhanced output to the original input audio. If there is sufficient difference (meaning the model cleaned up the speech), then we enhance the speech throughout the rest of the video.\n\nEarly versions of Looking to Listen were designed to entirely isolate speech from the background noise. In a user study conducted together with YouTube, we found that users prefer to leave in some of the background sounds to give context and to retain some the general ambiance of the scene. Based on this user study, we take a linear combination of the original audio and our produced clean speech channel: output_audio = 0.1 x original_audio + 0.9 x speech. The following video presents clean speech combined with different levels of the background sounds in the scene (10% background is the balance we use in practice).\n\nBelow are additional examples of the enhanced speech results from the new Speech Enhancement feature in YouTube Stories. We recommend watching the videos with good speakers or headphones.\n\nAnother important requirement is that the model be . It must be able to handle different types of voices, languages and accents, as well as different visual appearances. To this end, we conducted a series of tests exploring the performance of the model with respect to various visual and speech/auditory attributes: the speaker\u2019s age, skin tone, spoken language, voice pitch, visibility of the speaker\u2019s face (% of video in which the speaker is in frame), head pose throughout the video, facial hair, presence of glasses, and the level of background noise in the (input) video.\nFor each of the above visual/auditory attributes, we ran our model on segments from our evaluation set (separate from the training set) and measured the speech enhancement accuracy, broken down according to the different attribute values. Results for some of the attributes are summarized in the following plots. Each data point in the plots represents hundreds (in most cases thousands) of videos fitting the criteria.\n\nYouTube creators who are eligible for YouTube Stories creation may record a video on iOS, and select \u201cEnhance speech\u201d from the volume controls editing tool. This will immediately apply speech enhancement to the audio track and will play back the enhanced speech in a loop. It is then possible to toggle the feature on and off multiple times to compare the enhanced speech with the original audio.\nIn parallel to this new feature in YouTube, we are also exploring additional venues for this technology. More to come later this year \u2014 stay tuned!", "date": "\nThursday, October 1, 2020\n"},
{"website": "Google-AI", "title": "\nAdvancing Instance-Level Recognition Research\n", "author": ["Posted by Cam Askew and Andr\u00e9 Araujo, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/09/advancing-instance-level-recognition.html", "abstract": "Instance-level recognition (ILR) is the computer vision task of recognizing a specific  of an object, rather than simply the category to which it belongs. For example, instead of labeling an image as \u201cpost-impressionist painting\u201d, we\u2019re interested in instance-level labels like \u201c by Vincent van Gogh\u201d, or \u201c, Paris, France\u201d, instead of simply \u201carch\u201d. Instance-level recognition problems exist in many domains, like landmarks, artwork, products, or logos, and have applications in visual search apps, personal photo organization, shopping and more. Over the past several years, Google has been contributing to research on ILR with the  and  (GLDv2), and novel models such as  and .\n\nToday, we highlight some results from the  at . The workshop brought together experts and enthusiasts in this area, with many fruitful discussions, some of which included our ECCV\u201920 paper \u201c\u201d (DELG), a state-of-the-art image feature model for instance-level recognition, and a supporting  for DELG and other related ILR techniques. Also presented were two new landmark challenges (on  and  tasks) based on GLDv2, and future ILR challenges that extend to other domains:  and . The long-term goal of the workshop and challenges is to foster advancements in the field of ILR and push forward the state of the art by unifying research workstreams from different domains, which so far have mostly been tackled as separate problems.\n\n\nEffective image representations are the key components required to solve instance-level recognition problems. Often, two types of representations are necessary: global and local image features. A global feature summarizes the entire contents of an image, leading to a compact representation but discarding information about spatial arrangement of visual elements that may be characteristic of unique examples. Local features, on the other hand, comprise descriptors and geometry information about specific image regions; they are especially useful to match images depicting the same objects.\n\nCurrently, most systems that rely on both of these types of features need to separately adopt each of them using different models, which leads to redundant computations and lowers overall efficiency. To address this, we proposed DELG, a unified model for local and global image features.\n\nThe DELG model leverages a fully- with two different heads: one for global features and the other for local features. Global features are obtained using pooled feature maps of deep network layers, which in effect summarize the salient features of the input images making the model more robust to subtle changes in input. The local feature branch leverages intermediate feature maps to detect salient image regions, , and to produce descriptors that represent associated localized contents in a discriminative manner.\n\nThis novel design allows for efficient inference since it enables extraction of global and local features within a single model. For the first time, we demonstrated that such a unified model can be trained end-to-end and deliver state-of-the-art results for instance-level recognition tasks. When compared to previous global features, this method outperforms other approaches by up to 7.5% mean average precision; and for the local feature re-ranking stage, DELG-based results are up to 7% better than previous work. Overall, DELG achieves 61.2% average precision on the recognition task of GLDv2, which outperforms all except two methods of the . Note that all top methods from that challenge used complex model ensembles, while our results use only a single model.\n\n\nTo foster research reproducibility, we are also releasing a revamped  that includes DELG and other techniques relevant to instance-level recognition, such as  and . Our code adopts the latest  releases, and makes available reference implementations for model training & inference, besides image retrieval and matching functionalities. We invite the community to use and  to this codebase in order to develop strong foundations for research in the ILR field.\n\n\nFocused on the landmarks domain, the  (GLDv2) is the largest available dataset for instance-level recognition, with 5 million images spanning 200 thousand categories. By training landmark retrieval models on this dataset,  improvements of up to 6% mean average precision, compared to models trained on earlier datasets. We have also recently launched a  for visually exploring the GLDv2 dataset.\n\nThis year, we also launched two new challenges within the landmark domain, one focusing on  and the other on . These competitions feature newly-collected test sets, and a new evaluation methodology: instead of uploading a CSV file with pre-computed predictions, participants have to submit models and code that are run on Kaggle servers, to compute predictions that are then scored and ranked. The compute restrictions of this environment put an emphasis on efficient and practical solutions. \n\nThe challenges attracted over 1,200 teams, a 3 increase over last year, and participants achieved significant improvements over our strong DELG baselines. On the recognition task, the highest scoring submission achieved a relative increase of 43% average precision score and on the retrieval task, the winning team achieved a 59% relative improvement of the mean average precision score. This latter result was achieved via a combination of more effective neural networks, pooling methods and training protocols (see more details ).\n\nIn addition to the landmark recognition and retrieval challenges, our academic and industrial collaborators discussed their progress on developing benchmarks and competitions in other domains. A large-scale research benchmark for  is under construction, leveraging \u2019s , and with a new test set consisting of guest photos exhibiting various photometric and geometric variations. Similarly, a new large-scale  competition will capture various challenging aspects, including a very large number of products, a long-tailed class distribution and variations in object appearance and context. More information on the ILR workshop, including slides and video recordings, is available on its .\n\nWith this research, open source code, data and challenges, we hope to spur progress in instance-level recognition and enable researchers and machine learning enthusiasts from different communities to develop approaches that generalize across different domains.", "date": "\nFriday, September 25, 2020\n"},
{"website": "Google-AI", "title": "\nAdvancing NLP with Efficient Projection-Based Model Architectures\n", "author": ["Posted by Prabhu Kaliamoorthi, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/09/advancing-nlp-with-efficient-projection.html", "abstract": "Deep neural networks have radically transformed natural language processing (NLP) in the last decade, primarily through their application in data centers using specialized hardware. However, issues such as preserving user privacy, eliminating network latency, enabling offline functionality, and reducing operation costs have rapidly spurred the development of NLP models that can be run on-device rather than in data centers. Yet mobile devices have limited memory and processing power, which requires models running on them to be small and efficient \u2014 without compromising quality.\n\nLast year, we published a neural architecture called , which at the time achieved state-of-the-art performance on many text classification problems, using a model with less than 200K parameters. While most models use a fixed number of parameters per token, the PRADO model used a network structure that required extremely few parameters to learn the most relevant or useful tokens for the task. \n\nToday we describe a new extension to the model, called pQRNN, which advances the state of the art for NLP performance with a minimal model size. The novelty of pQRNN is in how it combines a simple projection operation with a  encoder for fast, parallel processing. We show that the pQRNN model is able to achieve -level performance on a text classification task with orders of magnitude fewer number of parameters.\n\n\nWhen developed a year ago, PRADO exploited NLP domain-specific knowledge on text segmentation to reduce the model size and improve the performance. Normally, the text input to NLP models is first processed into a form that is suitable for the neural network, by segmenting text into pieces (tokens) that correspond to values in a predefined  (a list of all possible tokens). The neural network then uniquely identifies each segment using a trainable parameter vector, which comprises the embedding table. However, the way in which text is segmented has a significant impact on the model performance, size, and latency. The figure below shows the spectrum of approaches used by the NLP community and their pros and cons.\n\nSince the number of text segments is such an important parameter for model performance and compression, it raises the question of whether or not an NLP model needs to be able to distinctly identify every possible text segment. To answer this question we look at the inherent complexity of NLP tasks. \n\nOnly a few NLP tasks (e.g., language models and machine translation) need to know subtle differences between text segments and thus need to be capable of uniquely identifying all possible text segments. In contrast, the majority of other tasks can be solved by knowing a small subset of these segments. Furthermore, this subset of task-relevant segments will likely not be the most frequent, as a significant fraction of segments will undoubtedly be dedicated to articles, such as , , , etc., which for many tasks are not necessarily critical. Hence, allowing the network to determine the most relevant segments for a given task results in better performance. In addition, the network does not need to be able to uniquely identify these segments, but only needs to recognize clusters of text segments. For example, a  just needs to know segment clusters that are strongly correlated to the sentiment in the text.\n\nLeveraging these insights, PRADO was designed to learn clusters of text segments from words rather than word pieces or characters, which enabled it to achieve good performance on low-complexity NLP tasks. Since word units are more meaningful, and yet the most relevant words for most tasks are reasonably small, many fewer model parameters are needed to learn such a reduced subset of relevant word clusters.\n\nBuilding on the success of PRADO, we developed an improved NLP model, called pQRNN. This model is composed of three building blocks, a projection operator that converts tokens in text to a sequence of ternary vectors, a dense bottleneck layer and a stack of QRNN encoders. \n\nThe implementation of the projection layer in pQRNN is identical to that used in PRADO and helps the model learn the most relevant tokens without a fixed set of parameters to define them. It first fingerprints the tokens in the text and converts it to a ternary feature vector using a simple mapping function. This results in a ternary vector sequence with a balanced symmetric distribution that uniquely represents the text. This representation is not directly useful since it does not have any information needed to solve the task of interest and the network has no control over this representation. We combine it with a dense bottleneck layer to allow the network to learn a per word representation that is relevant for the task at hand. The representation resulting from the bottleneck layer still does not take the context of the word into account. We learn a contextual representation by using a stack of bidirectional QRNN encoders. The result is a network that is capable of learning a contextual representation from just text input without employing any kind of preprocessing. \n\n\nWe evaluated pQRNN on the  dataset and compared it with the  model on the same task. Simply because the model size is proportional to the number of parameters, pQRNN is much smaller than BERT. But in addition, pQRNN is , further reducing the model size by a factor of 4x. The public pretrained version of BERT performed poorly on the task hence the comparison is done to a BERT version that is pretrained on several different relevant multilingual data sources to achieve the best possible performance.\n\n\nUsing our previous generation model PRADO, we have demonstrated how it can be used as the foundation for the next generation of state-of-the-art light-weight text classification models. We present one such model, pQRNN, and show that this new architecture can nearly achieve BERT-level performance, despite being 300x smaller and being trained on only supervised data. To stimulate further research in this area, we have  and encourage the community to use it as a jumping off point for new model architectures.", "date": "\nMonday, September 21, 2020\n"},
{"website": "Google-AI", "title": "\nImproving the Accuracy of Genomic Analysis with DeepVariant 1.0\n", "author": ["Posted by Andrew Carroll, Product Lead and Pi-Chuan Chang, Technical Lead, Google Health"], "link": "http://ai.googleblog.com/2020/09/improving-accuracy-of-genomic-analysis.html", "abstract": "Sequencing genomes involves sampling short pieces of the DNA from the ~6 billion pairs of nucleobases \u2014 i.e., adenine (A), thymine (T), guanine (G), and cytosine (C) \u2014 we inherit from our parents. Genome sequencing is enabled by two key technologies: DNA sequencers (hardware) that \"read\" relatively small fragments of DNA, and variant callers (software) that combine the reads to identify where and how an individual's genome differs from a reference genome, like the one assembled in the . Such variants may be indicators of genetic disorders, such as an , , or .\n\nIn 2017, , an open-source tool which identifies genome variants in sequencing data using a  (CNN). The sequencing process begins with a physical sample being sequenced by any of a handful of instruments, depending on the end goal of the sequencing. The raw data, which consists of numerous reads of overlapping fragments of the genome, are then mapped to a reference genome. DeepVariant analyzes these mappings to identify variant locations and distinguish them from sequencing errors.\n\nSoon after it was first  in 2018, DeepVariant underwent a number of , including significant changes to improve accuracy for whole  and  (PCR) sequencing.\nWe are now releasing , which incorporates a large number of improvements for all sequencing types. DeepVariant v1.0 is an improved version of our submission to the , which achieved Best Overall accuracy for 3 of 4 instrument categories. Compared to previous state-of-the-art models, DeepVariant v1.0 significantly reduces the errors for widely-used sequencing data types, including  and . In addition, through a collaboration with the  we have also released a model that combines DeepVariant with the UCSC\u2019s  method, called , which extends coverage to  data for the first time.\n\n\nFor the last decade, the majority of sequence data were generated using  instruments, which produce short (75-250 bases) and accurate sequences. In recent years, new technologies have become available that can sequence much longer pieces, including , which can produce  up to ~15,000 bases in length, and Oxford Nanopore, which can produce reads up to . The particular type of sequencing data a researcher might use depends on the ultimate use-case.\n\nBecause DeepVariant is a deep learning method, we can quickly re-train it for these new instrument types, ensuring highly accurate sequence identification. Accuracy is important because a missed variant call could mean missing the causal variant for a disorder, while a false positive variant call could lead to identifying an incorrect one. Earlier\u00a0state-of-the-art methods could reach ~99.1% accuracy (~73,000 errors) on a 35-fold coverage Illumina whole genome, whereas an early version of DeepVariant (v0.10) had ~99.4% accuracy (46,000 errors), corresponding to a 38% error reduction. DeepVariant v1.0 reduces Illumina errors by another ~22% and PacBio errors by another ~52% relative to the last DeepVariant release (v0.10).\n\n\nDeepVariant is a convolutional neural network (CNN) that treats the task of identifying genetic variants as an image classification problem. DeepVariant constructs tensors, essentially multi-channel images, where each channel represents an aspect of the sequence, such as the bases in the sequence (called ), the quality of alignment between different reads (), whether a given read supports an alternate  (),  etc. It then analyzes these data and outputs three genotype likelihoods, corresponding to how many copies (0, 1, or 2) of a given alternate allele are present. \n\n\nBecause DeepVariant uses the same codebase for each data type, improvements apply to each of Illumina, PacBio, and Oxford Nanopore. Below, we show the numbers for Illumina and PacBio for two types of small variants:  (single nucleotide polymorphisms, which change a single base without changing sequence length) and  (insertions and deletions). \n\nThe  consortium from the  (NIST) creates gold-standard samples with known variants covering the regions of the genome. These are used as labels to train DeepVariant. Using long-read technologies the Genome in a Bottle , increasing the regions described by the standard set from 85% of the genome to 92% of it. These more difficult regions were already used in training the PacBio models, and including them in the Illumina models reduced errors by 11%. By relaxing the filter for reads of lower mapping quality, we further reduced errors by 4% for Illumina and 13% for PacBio.\n\nWe inherit one copy of DNA from our mother and another from our father. PacBio and Oxford Nanopore sequences are long enough to separate sequences by parental origin, which is called a . By providing this information to the neural network, DeepVariant improves its identification of random sequence errors and can better determine whether a variant has a copy from one or both parents. \n  \nDeepVariant uses input sequence fragments that have been aligned to a reference genome. The optimal alignment for variants that include insertions or deletions could be different if the aligner knew they were present. To capture this information, we implemented an additional alignment step relative to the candidate variant.  The figure below shows an additional second row where the reads are aligned to the candidate variant, which is a large insertion. You can see sequences that abruptly stop in the first row can now be fully aligned, providing additional information. \n\nVariants can have multiple alleles, with a different base inherited from each parent. DeepVariant\u2019s classifier only generates a probability for one potential variant at a time. In previous versions, simple hand-written rules converted the probabilities into a composite call, but these rules failed in some edge cases. In addition, it also separated the way a final call was made from the backpropagation to train the network. By adding a small, fully-connected neural network to the post-processing step, we are able to better handle these tricky multi-allelic cases.\n\nThe timeframe for the competition was compressed, so we trained only with data similar to the challenge data (PCR-Free NovaSeq) to speed model training. In our production releases, we seek high accuracy for multiple instruments as well as PCR+ preparations. Training with data from these diverse classes helps the model generalize, so our DeepVariant v1.0 release model outperforms the one submitted.\n\nThe charts below show the error reduction achieved by each improvement.\n\n\nDeepVariant v1.0 also includes a hybrid model for PacBio and Illumina reads. In this case, the model leverages the strengths of both input types, without needing new logic.\n\nWe observed no change in SNP errors, suggesting that PacBio reads are strictly superior for SNP calling. We observed a further 49% reduction in Indel errors relative to the PacBio model, suggesting that the Indel error modes of Illumina and PacBio HiFi can be used in a complementary manner.\n\nUntil the PrecisionFDA competition, a DeepVariant model was not available for Oxford Nanopore data, because the higher base error rate created too many candidates for DeepVariant to classify. We partnered with the , which has extensive expertise with Nanopore data. They had previously trained a deep learning method called , which could narrow down the candidates to a more tractable number. The larger neural network of DeepVariant can then accurately characterize the remaining candidates with a reasonable runtime. \n\nThe combined PEPPER-DeepVariant pipeline with the Oxford Nanopore model is open-source and . This pipeline was able to achieve a superior SNP calling accuracy to DeepVariant Illumina on the PrecisionFDA challenge, which is the first time anyone has shown Nanopore outperforming Illumina in this way.\n\n\nDeepVariant v1.0 isn\u2019t the end of development. We look forward to working with the genomics community to further maximize the value of genomic data to patients and researchers.", "date": "\nFriday, September 18, 2020\n"},
{"website": "Google-AI", "title": "\nImproving Sparse Training with RigL\n", "author": ["Posted by Utku Evci and Pablo Samuel Castro, Research Engineers, Google Research, Montreal"], "link": "http://ai.googleblog.com/2020/09/improving-sparse-training-with-rigl.html", "abstract": "Modern deep neural network architectures are often highly redundant [, , ], making it possible to remove a significant fraction of connections without harming performance. The sparse neural networks that result have been shown to be more  and  efficient compared to dense networks, and, in many cases, can significantly decrease wall clock inference times. \n\nBy far the most popular method for training sparse neural networks is , () which usually requires first training a dense model, and then \u201csparsifying\u201d it by cutting out the connections with negligible weights. However, this process has two limitations. \n\nIn \u201c\u201d, presented at , we introduce , an algorithm for training sparse neural networks that uses a fixed parameter count and computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. The algorithm identifies which neurons should be active during training, which helps the optimization process to utilize the most relevant connections and results in better sparse solutions. An example of this is shown below, where, during the training of a  (MLP) network on , our sparse network trained with RigL learns to focus on the center of the images, discarding the uninformative pixels from the edges. A  implementation of our method along with three other baselines (, , ) can be found at . \n\n\nThe RigL method starts with a network initialized with a random sparse topology. At regularly spaced intervals we remove a fraction of the connections with the smallest weight magnitudes. Such a strategy  to have very little effect on the loss. RigL then activates new connections using instantaneous gradient information, i.e., without using past gradient information. After updating the connectivity, training continues with the updated network until the next scheduled update. Next, the system activates connections with large gradients, since these connections are expected to decrease the loss most quickly. \n\n\nBy changing the connectivity of the neurons dynamically during training, RigL helps optimize to find better solutions. To demonstrate this, we restart training from a bad solution that exhibits poor accuracy and show that RigL's mask updates help the optimization achieve better loss compared to static training, in which connectivity of the sparse network remains the same.\n\nThe figure below summarizes the performance of various methods on training an 80% sparse  architecture. We compare RigL with two recent sparse training methods,  and  and three baseline training methods: ,  and . Two of these methods (SNFS and Pruning) require dense resources as they need to either train a large network or store the gradients of it. Overall, we observe that the performance of all methods improves with additional training time; thus, for each method we run extended training with up to 5 the training steps of the original 100 epochs. \n\nAs noted in a number of studies [, , , ], training a network with fixed sparsity from scratch ( leads to inferior performance compared to solutions found by pruning. Training a small, dense network () with the same number of parameters gets better results than , but fails to match the performance of dynamic sparse models. Similarly,  improves the performance over , but saturates at around 75% accuracy, revealing the limits of growing new connections randomly. Methods that use gradient information to grow new connections ( and ) obtain higher accuracy in general, but  achieves the highest accuracy, while also consistently requiring fewer FLOPs (and memory footprint) than the other methods.\n\nObserving the trend between extended training and performance, we compare the results using longer training runs. Within the interval considered (i.e., 1-100) RigL's performance constantly improves with additional training. RigL achieves state of art performance of 68.07% Top-1 accuracy at training with a 99% sparse ResNet-50 architecture. Similarly extended training of a 90% sparse  architecture with RigL achieves 70.55% Top-1 accuracy. Obtaining the same results with fewer training iterations is an exciting future research direction. \n\nOther experiments include image classification on  datasets and character-based language modelling using RNNs with the  and can be found in the . \n\n\nRigL is useful in three different scenarios:", "date": "\nWednesday, September 16, 2020\n"},
{"website": "Google-AI", "title": "\nImitation Learning in the Low-Data Regime\n", "author": ["Posted by Robert Dadashi, Research Software Engineer and L\u00e9onard Hussenot, Student Researcher, Google Research"], "link": "http://ai.googleblog.com/2020/09/imitation-learning-in-low-data-regime.html", "abstract": "Reinforcement Learning (RL) is a paradigm for using trial-and-error to train agents to sequentially make decisions in complex environments, which has had great success in a number of domains, including ,  and . Agents typically aim at maximizing the sum of the reward they collect in an environment, which can be based on a variety of parameters, including speed, , aesthetics and more. However, designing a specific RL reward function is a challenge since it can be hard to specify or too sparse. In such cases,  (IL) methods offer an alternative as they learn how to solve a task from expert demonstrations, rather than a carefully designed reward function. However, state-of-the-art IL methods rely on , which uses min/max optimization procedures, making them algorithmically unstable and difficult to deploy.\n\nIn \u201c\u201d (PWIL), we introduce a new IL method, based on the  form of the , also known as the earth mover\u2019s distance, which does not rely on adversarial training. Using the , we demonstrate the efficacy of the PWIL method by imitating a simulated expert with a limited number of demonstrations (even a single example) and limited interactions with the environment.\n\n\nState-of-the-art adversarial IL methods operate similarly to  (GANs) in which a generator (the) is trained to maximize the confusion of a discriminator (the) that itself is trained to differentiate between the agent\u2019s state-action pairs and the expert\u2019s. Adversarial IL methods boil down to a , i.e., the problem of minimizing a distance between probability distributions in a\u00a0. However, just as GANs, adversarial IL methods rely on a min/max optimization problem and hence come with a .\n\nThe PWIL method is based on the formulation of IL as a distribution matching problem, in this case, the Wasserstein distance. The first step consists of inferring from the demonstrations a state-action distribution of the expert, the collection of relationships between the actions taken by the expert and the corresponding state of the environment. The goal is then to minimize the distance between the agent\u2019s and the expert\u2019s state-action distributions, through interactions with the environment. In contrast, PWIL is a non-adversarial method, enabling it to bypass the min/max optimization problem and directly minimize the Wasserstein distance between the agent\u2019s and the expert\u2019s state-action pair distributions. \n\n\nComputing the exact Wasserstein distance can be restrictive since one must wait until the end of a trajectory of the agent to calculate it, meaning that the rewards can be computed only when the agent is done interacting with the environment. To avoid this restriction, we use an upper bound on the distance instead, from which we can define a reward that we optimize using RL. We show that by doing so, we indeed recover expert behaviour and minimize the Wasserstein distance between the agent and the expert on a number of locomotion tasks of the MuJoCo simulator. While adversarial IL methods use a reward function from a neural network that must be optimized and re-estimated continuously as the agent interacts with the environment, PWIL defines a reward function offline from demonstrations, which does not change and is based on substantially fewer hyperparameters than adversarial IL approaches.\n\n\nAs in numerous challenges in ML, a number of IL methods are evaluated on synthetic tasks, where one usually has access to the underlying reward function of the task and can measure similarity between the expert\u2019s and the agent\u2019s behaviour in terms of performance, which is the expected sum of rewards. A byproduct of PWIL is the creation of a metric that can compare expert behavior to an agent\u2019s behavior for any IL method, without access to the true reward of the task. In this sense, we can use the Wasserstein distance in the true IL setting, not only on synthetic tasks.\n\n\nIn environments where interacting is costly (e.g., a real robot or a complex simulator), PWIL is a prime candidate not only because it can recover expert behaviour, but also because the reward function it defines is easy to tune and is defined without interactions with the environment. This opens multiple opportunities for future exploration, including deployment to real systems, extending PWIL to the setup where we have only access to demonstration states (rather than states and actions), and finally applying PWIL to visual based observations.", "date": "\nTuesday, September 15, 2020\n"},
{"website": "Google-AI", "title": "\nThe Technology Behind our Recent Improvements in Flood Forecasting\n", "author": ["Posted by Sella Nevo, Senior Software Engineer, Google Research, Tel Aviv"], "link": "http://ai.googleblog.com/2020/09/the-technology-behind-our-recent.html", "abstract": "Flooding is the most common natural disaster on the planet, affecting the lives of hundreds of millions of people around the globe and causing around $10 billion in damages each year. Building on , earlier this week we announced some of our recent efforts to , expanding coverage to  more than 250 million people, and providing unprecedented lead time, accuracy and clarity. \nTo enable these breakthroughs, we have devised a new approach for inundation modeling, called a , which combines physics-based modeling with machine learning (ML) to create more accurate and scalable inundation models in real-world settings. Additionally, our new  allows identifying areas at risk of flooding at unprecedented scale using end-to-end machine learning models and data that is publicly available globally. In this post, we also describe developments for the next generation of flood forecasting systems, called\u00a0 (presented at  and  this year), which is a new architecture specially built for hydrologic modeling across multiple basins, while still optimizing for accuracy at each location. \n\n\nThe first step in a flood forecasting system is to identify whether a river is expected to flood.  (or gauge-to-gauge models) have long been used by governments and disaster management agencies to improve the accuracy and extend the lead time of their forecasts. These models receive inputs like precipitation or upstream  of water level (i.e., the absolute elevation of the water above sea level) and output a forecast for the water level (or discharge) in the river at some time in the future.\n\nThe hydrologic model component of the\u00a0flood forecasting system described in this week\u2019s  doubled the lead time of flood alerts for areas covering more than 75 million people. These models not only increase lead time, but also provide unprecedented accuracy, achieving an  of more than 99% across all basins we cover, and predicting the water level within a 15 cm error bound more than 90% of the time.\u00a0Once a river is predicted to reach flood level, the next step in generating actionable warnings is to convert the river level forecast into a prediction for how the floodplain will be affected.\n, we developed high quality elevation maps based on satellite imagery, and ran physics-based models to simulate water flow across these digital terrains, which allowed  warnings with . In collaboration with our satellite partners, ,  and , we have now expanded the elevation maps to cover hundreds of millions of square kilometers. However, in order to scale up the coverage to such a large area while still retaining high accuracy, we had to re-invent how we develop inundation models.\n\nInundation modeling at scale suffers from three significant challenges. Due to the large areas involved and the resolution required for such models, they necessarily have high computational complexity. In addition, most global elevation maps don\u2019t include riverbed bathymetry, which is important for accurate modeling. Finally, the errors in existing data, which may include gauge measurement errors, missing features in the elevation maps, and the like, need to be understood and corrected. Correcting such problems may require collecting additional high-quality data or fixing erroneous data manually, neither of which scale well. \n\nOur new approach to inundation modeling, which we call a , addresses these issues by using several innovative tricks. Instead of modeling the complex behaviors of water flow in real time, we compute modifications to the morphology of the elevation map that allow one to simulate the inundation using simple physical principles, such as those describing .  \n\nFirst, we train a pure-ML model (devoid of physics-based information) to estimate the one-dimensional river profile from gauge measurements. The model takes as input the water level at a specific point on the river (the stream gauge) and outputs the river profile, which is the water level at all points in the river. We assume that if the gauge increases, the water level increases monotonically, i.e., the water level at other points in the river increases as well. We also assume that the absolute elevation of the river profile decreases downstream (i.e., the river flows downhill). \n\nWe then use this learned model and some heuristics to edit the elevation map to approximately \u201ccancel out\u201d the pressure gradient that would exist if that region were flooded. This new synthetic elevation map provides the foundation on which we model the flood behavior using a simple . Finally, we match the resulting flooded map to the satellite-based flood extent with the original stream gauge measurement.\n\nThis approach abandons some of the realistic constraints of classical , but in data scarce regions where existing methods currently struggle, its flexibility allows the model to automatically learn the correct bathymetry and fix various errors to which physics-based models are sensitive. This morphological model improves accuracy by 3%, which can significantly improve forecasts for large areas, while also allowing for much more rapid model development by reducing the need for manual modeling and correction.\n\n\nMany people reside in areas that are not covered by the morphological inundation models, yet access to accurate predictions are still urgently needed. To reach this population and to increase the impact of our flood forecasting models, we designed an end-to-end ML-based approach, using almost exclusively data that is globally publicly available, such as stream gauge measurements, public satellite imagery, and low resolution elevation maps. We train the model to use the data it is receiving to directly infer the inundation map in real time. \n\nThis approach works well \u201cout of the box\u201d when the model only needs to forecast an event that is within the range of events previously observed. Extrapolating to more extreme conditions is much more challenging. Nevertheless, proper use of existing elevation maps and real-time measurements can enable alerts that are more accurate than presently available for those in areas not covered by the more detailed morphological inundation models. Because this model is highly scalable, we were able to launch it across India after only a few months of work, and we hope to roll it out to many more countries soon. \n\n\nIn an effort to continue improving flood forecasting, we have developed  \u2014 a specialized deep neural network architecture built specifically for water levels forecasting \u2014 which allows us utilize some exciting  in ML-based hydrology in a real-world operational setting. Two prominent features distinguish it from standard hydrologic models. First, it is able to differentiate between model components that generalize well between sites, such as the modeling of rainfall-runoff processes, and those that are specific to a given site, like the , which converts a predicted discharge volume into an expected water level. This enables the model to generalize well to different sites, while still fine-tuning its performance to each location. Second, HydroNets takes into account the structure of the river network being modeled, by training a large architecture that is actually a web of smaller neural networks, each representing a different location along the river. This allows neural networks that are modeling upstream sites to pass information encoded in embeddings to models of downstream sites, so that every model can know everything it needs without a drastic increase in parameters. \n\nThe animation below illustrates the structure and flow of information in HydroNets. The output from the modeling of upstream sub-basins is combined into a single representation of a given basin state. It is then processed by the shared model component, which is informed by all basins in the network, and passed on to the label prediction model, which calculates the water level (and the loss function). The output from this iteration of the network is then passed on to inform downstream models, and so on.\n\nWe\u2019re incredibly excited about this progress, and are working hard on improving our systems further.", "date": "\nThursday, September 3, 2020\n"},
{"website": "Google-AI", "title": "\nKeyPose: Estimating the 3D Pose of Transparent Objects from Stereo\n", "author": ["Posted by Kurt Konolige, Software Engineer, Robotics at Google"], "link": "http://ai.googleblog.com/2020/09/keypose-estimating-3d-pose-of.html", "abstract": "Estimating the position and orientation of 3D objects is one of the core problems in computer vision applications that involve object-level perception, such as  and . In these applications, it is important to know the 3D position of objects in the world, either to directly affect them, or to place simulated objects correctly around them. While there has been much research on this topic using machine learning (ML) techniques, especially , most have relied on the use of depth sensing devices, such as the , which give direct measurements of the distance to an object. For objects that are shiny or transparent, direct depth sensing does not work well. For example, the figure below includes a number of objects (), two of which are transparent stars. A depth device does not find good depth values for the stars, and gives a very poor reconstruction of the actual 3D points ().\nOne solution to this problem, such as that proposed by , is to use a deep neural network to  the corrupted depth map of the transparent objects. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries, which it uses to refine the initial depth estimates for all transparent surfaces in the scene (\u00a0in the figure above). This approach is very promising, and allows scenes with transparent objects to be processed by pose-estimation methods that rely on depth.\u00a0 But inpainting can be tricky, especially when trained completely with synthetic images, and can still result in errors in depth.\nIn \u201c\u201d, presented at  in collaboration with the , we describe an ML system that estimates the depth of transparent objects by directly predicting 3D keypoints. To train the system we gather a large real-world dataset of images of transparent objects in a semi-automated way, and efficiently label their pose using 3D keypoints selected by hand. We then train deep models (called KeyPose) to estimate the 3D keypoints end-to-end from monocular or stereo images, without explicitly computing depth. The models work on objects both seen and unseen during training, for both individual objects and categories of objects. While KeyPose can work with monocular images, the extra information available from stereo images allows it to improve its results by a factor of two over monocular image input, with typical errors from 5 mm to 10 mm, depending on the objects. It substantially improves over state-of-the-art in pose estimation for these objects, even when competing methods are provided with ground truth depth. We are releasing the  for use by the research community.\n\nTo facilitate gathering large quantities of real-world images, we set up a robotic data-gathering system in which a robot arm moves through a trajectory while taking video with two devices, a stereo camera and the  depth camera.\nThe  on the target enable accurate tracing of the pose of the cameras. By hand-labelling only a few images in each video with 2D keypoints, we can extract 3D keypoints for all frames of the video using multi-view geometry, thus increasing the labelling efficiency by a factor of 100.\nWe captured imagery for 15 different transparent objects in five categories, using 10 different background textures and four different poses for each object, yielding a total of 600 video sequences comprising 48k stereo and depth images. We also captured the same images with an opaque version of the object, to provide accurate ground truth depth images. All the images are labelled with 3D keypoints. We are releasing this  publicly, complementing the synthetic ClearGrasp dataset with which it shares similar objects.\n\nThe idea of using stereo images directly for keypoint estimation was developed independently for this project; it has also  in the context of hand-tracking. The diagram below shows the basic idea: the two images from a stereo camera are cropped around the object and fed to the KeyPose network, which predicts a sparse set of 3D keypoints that represent the 3D pose of the object. The network is trained using supervision from the labelled 3D keypoints.\nOne of the key aspects of stereo KeyPose is the use of early fusion to intermix the stereo images, and allow the network to implicitly compute disparity, in contrast to late fusion, in which keypoints are predicted for each image separately, and then combined. As shown in the diagram below, the output of KeyPose is a 2D keypoint heatmap in the image plane along with a disparity (i.e., inverse depth) heatmap for each keypoint. The combination of these two heatmaps yields the 3D coordinate of the keypoint, for each keypoint.\nWhen compared to late fusion or to monocular input, early fusion stereo typically is twice as accurate.\n\nThe images below show qualitative results of KeyPose on individual objects. On the left is one of the original stereo images; in the middle are the predicted 3D keypoints projected onto the image. On the right, we visualize points from a 3D model of the bottle, placed at the pose determined by the predicted 3D keypoints. The network is efficient and accurate, predicting keypoints with an MAE of 5.2 mm for the bottle and 10.1 mm for the mug using just 5 ms on a standard GPU.\nThe following table shows results for KeyPose on category-level estimation. The test set used a background texture not seen by the training set. Note that the MAE varies from 5.8 mm to 9.9 mm, showing the accuracy of the method.For a complete accounting of quantitative results, as well as, ablation studies, please see the  and the .\n\nThis work shows that it is possible to accurately estimate the 3D pose of transparent objects from RGB images without reliance on depth images. It validates the use of stereo images as input to an early fusion deep net, where the network is trained to extract sparse 3D keypoints directly from the stereo pair. We hope the availability of an extensive, labelled dataset of transparent objects will help to advance the field. Finally, while we used semi-automatic methods to efficiently label the dataset, we hope to employ  in future work to do away with manual labelling.", "date": "\nWednesday, September 2, 2020\n"},
{"website": "Google-AI", "title": "\nUsing Machine Learning to Detect Deficient Coverage in Colonoscopy Screenings\n", "author": ["Posted by Daniel Freedman and Ehud Rivlin, Research Scientists, Google Health"], "link": "http://ai.googleblog.com/2020/08/using-machine-learning-to-detect.html", "abstract": "(CRC) is a global health problem and the  in the United States, resulting in an . While deadly, CRC can be prevented by removing small precancerous lesions in the colon, called polyps, before they become cancerous. In fact, it is estimated that a 1% increase in the  (ADR, defined as the fraction of procedures in which a physician discovers at least one polyp) can lead to a  in the rate of  (a CRC that is diagnosed within 60 months of a negative colonoscopy).\n\nColonoscopy is considered the gold standard procedure for the detection and removal of polyps. Unfortunately, the literature indicates that endoscopists ; furthermore, 20% to 24% of polyps that have the potential to become cancerous () are missed. Two major factors that may cause an endoscopist to miss a polyp are (1) the polyp appears in the field of view, but the endoscopist misses it, perhaps due to its small size or flat shape; and (2) the polyp does not appear in the field of view, as the endoscopist has not fully covered the relevant area during the procedure. \n\nIn \u201c\u201d, we introduce the Colonoscopy Coverage Deficiency via Depth algorithm, or C2D2, a machine learning-based approach to improving colonoscopy coverage. The C2D2 algorithm performs a local 3D reconstruction of the colon as images are captured during the procedure, and on that basis, identifies which areas of the colon were covered and which remained outside of the field of view. C2D2 can then indicate in real time whether a particular area of the colon has suffered from deficient coverage so the endoscopist can return to that area. Our work proposes a novel approach to compute coverage in real time, for which 3D reconstruction is done using a calibration-free,  method, and evaluate it in a large scale way.\n\n\nWhen considering colon coverage, it is important to estimate the coverage fraction \u2014 what percentage of the relevant regions were covered by a complete procedure. While a retrospective analysis is useful for the physician and could provide general guidance for future procedures, it is more useful to have real-time estimation of coverage fraction, on a segment by segment basis, i.e. knowledge of what fraction of the  segment has been covered while traversing the colon. The helpfulness of such functionality is clear: during the procedure itself, a physician may be alerted to segments with deficient coverage, and can immediately return to review these areas. Higher coverage will result in a higher proportion of polyps being seen. \n\nThe C2D2 algorithm is designed to compute such a segment-by-segment coverage in two phases: computing depth maps for each frame of the colonoscopy video, followed by computation of coverage based on these depth maps.\nDepth map creation consists of both depth estimation as well as pose estimation \u2014 the localization of where the endoscope is in space, as well as the direction it is pointing. In addition to the detection of deficient coverage, depth and pose estimation are useful for a variety of other interesting tasks. For example, depth can be used for improved detection of flat polyps, while pose estimation can be used for relocalizing areas of the colon (including polyps) that the endoscopist wishes to revisit, and both together can be used for visualization and navigation.\nIn order to compute coverage fractions from these depth maps, we trained C2D2 on two sources of data: synthetic sequences and real sequences. We generated the synthetic videos using a graphical model of a colon. For each synthetic video, ground truth coverage is available in the form of a number between 0 (completely uncovered) and 1 (completely covered). For real sequences, we analyzed de-identified colonoscopy videos, for which ground truth coverage is unavailable.\n\n\nWhen using synthetic videos, the availability of ground truth coverage enables the direct measurement of C2D2\u2019s performance. We quantify this using the  (MAE), which indicates how much the algorithm\u2019s prediction differs, on average, from the ground truth. We find that C2D2\u2019s MAE = 0.075; meaning that, on average, the prediction of C2D2 is within 7.5% of the ground truth. By contrast, a group of physicians given the same task achieved MAE = 0.177, i.e., within 17.7% of the ground truth. Thus, the C2D2 attained an accuracy rate 2.4 times higher on synthetic sequences.\n\n\nOf course, what matters most is performance on videos of real colonoscopies. The challenge in this case is the absence of ground truth labelling: we don\u2019t know what the actual coverage is. Additionally, one cannot use labels provided by experts directly as they are not always accurate, due to the challenges described earlier. However, C2D2 can still perform inference on real colonoscopy videos. Indeed, the learning pipeline is designed to perform equally well on synthetic and real colonoscopy videos. \n\nTo verify performance on real sequences, we used a variant of a technique common in the generative modelling literature, which involves providing video sequences to human experts along with C2D2\u2019s coverage scores for those sequences. We then ask the experts to assess whether C2D2\u2019s score is correct. The idea is that while it is difficult for experts to assign a score directly, the task of verifying a given score is considerably easier. (This is similar to the fact that verifying a proposed solution to an algorithmic problem is generally much easier than computing that solution.) Using this methodology, experts verified C2D2\u2019s score 93% of the time. And in a more qualitative sense, C2D2\u2019s output seems to pass the \u201ceyeball test\u201d, see the figure below.\n\nBy alerting physicians to missed regions of the colon wall, C2D2 promises to lead to the discovery of more adenomas, thereby increasing the ADR and concomitantly decreasing the rate of . This would be of tremendous benefit to patients.\n\nIn addition to this work that addresses colonoscopy coverage, we are concurrently conducting research to improve polyp  by combining C2D2 with an automatic, real-time polyp detection algorithm. This study adds to the mounting evidence that physicians may use machine learning methods to augment their efforts, especially during procedures, to improve the quality of care for patients.", "date": "\nFriday, August 28, 2020\n"},
{"website": "Google-AI", "title": "\nScaling Up Fundamental Quantum Chemistry Simulations on Quantum Hardware\n", "author": ["Posted by Nicholas Rubin and Charles Neill, Research Scientists, Google AI Quantum"], "link": "http://ai.googleblog.com/2020/08/scaling-up-fundamental-quantum.html", "abstract": "Accurate computational prediction of chemical processes from the quantum mechanical laws that govern them is a tool that can unlock new frontiers in chemistry, improving a wide variety of industries. Unfortunately, the exact solution of quantum chemical equations for all but the smallest systems remains out of reach for modern classical computers, due to the exponential scaling in the number and statistics of quantum variables. However, by using a quantum computer, which by its very nature takes advantage of unique quantum mechanical properties to handle calculations intractable to its classical counterpart, simulations of complex chemical processes can be achieved. While today\u2019s quantum computers are powerful enough for a clearit is an open question whether such devices can be used to accelerate our current quantum chemistry simulation techniques.\n\nIn \u201c\u201d, appearing today in  the  explores this complex question by performing the largest chemical simulation performed on a quantum computer to date. In our experiment, we used a noise-robust  (VQE) to directly simulate a chemical mechanism via a quantum algorithm. Though the calculation focused on the  approximation of a real chemical system, it was twice as large as previous chemistry calculations on a quantum computer, and contained ten times as many quantum gate operations. Importantly, we validate that algorithms being developed for currently available quantum computers can achieve the precision required for experimental predictions, revealing pathways towards realistic simulations of quantum chemical systems. Furthermore, we have  for the experiment, which uses , our open source repository for quantum computations of chemistry.\n\nThere are a number of ways to use a quantum computer to simulate the ground state energy of a molecular system.  In this work we focused on a quantum algorithm \u201cbuilding block\u201d, or circuit primitive, and perfect its performance through a VQE (more on that later).  In the classical setting this circuit primitive is equivalent to the Hartree-Fock model and is an important circuit component of an algorithm we  for optimal chemistry simulations.  This allows us to focus on scaling up without incurring exponential simulation costs to validate our device.  Therefore, robust error mitigation on this component is crucial for accurate simulations when scaling to the \u201cbeyond classical\u201d regime.\n\nErrors in quantum computation emerge from interactions of the quantum circuitry with the environment, causing erroneous logic operations \u2014 even minor temperature fluctuations can cause  errors. Algorithms for simulating chemistry on near-term quantum devices must account for these errors with low overhead, both in terms of the number of qubits or additional quantum resources, such as implementing a . The most popular method to account for errors (and why we used it for our experiment) is to use a VQE. For our experiment, we selected the , which treats the quantum processor like an neural network and attempts to optimize a quantum circuit\u2019s parameters to account for noisy quantum logic by minimizing a cost function. Just like how classical neural networks can tolerate imperfections in data by optimization, a VQE dynamically adjusts quantum circuit parameters to account for errors that occur during the quantum computation. \n\n\nThe experiment was run on the Sycamore processor that was recently used to . Though our experiment required fewer qubits, even higher quantum gate fidelity was needed to resolve chemical bonding. This led to the development of new, targeted calibration techniques that optimally amplify errors so they can be diagnosed and corrected.\nErrors in the quantum computation can originate from a variety of sources in the quantum hardware stack. Sycamore has 54-qubits and consists of over 140 individually tunable elements, each controlled with high-speed, analog electrical pulses. Achieving precise control over the whole device requires fine tuning more than 2,000 control parameters, and even small errors in these parameters can quickly add up to large errors in the total computation. \n\nTo accurately control the device, we use an automated framework that maps the control problem onto a graph with thousands of nodes, each of which represent a physics experiment to determine a single unknown parameter. Traversing this graph takes us from basic priors about the device to a high fidelity quantum processor, and can be done in less than a day.  Ultimately, these techniques along with the algorithmic error mitigation enabled orders of magnitude reduction in the errors.\n\nWe hope that this experiment serves as a blueprint for how to run chemistry calculations on quantum processors, and as a jumping off point on the path to physical simulation advantage. One exciting prospect is that it is known how to modify the quantum circuits used in this experiment in a simple way such that they are no longer efficiently simulable, which would determine new directions for improved quantum algorithms and applications. We hope that the results from this experiment can be used to explore this regime by the broader research community. To run these experiments, you can find the code .", "date": "\nThursday, August 27, 2020\n"},
{"website": "Google-AI", "title": "\nAxial-DeepLab: Long-Range Modeling in All Layers for Panoptic Segmentation\n", "author": ["Posted by Huiyu Wang, Student Researcher and Yukun Zhu, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/08/axial-deeplab-long-range-modeling-in.html", "abstract": "The success of  (CNNs) mainly comes from two properties of :  and . Translation equivariance, although , ensures that the model functions well for objects at different positions in an image or for images of different sizes. Locality ensures efficient computation, but at the cost of making the modeling of long-range spatial relations challenging for  of large images. For example, segmenting a large object requires modeling the shape of it, which could potentially cover a very large pixel area, and context that could be helpful for segmenting the object may come from farther away. In such cases, the inability to inform the model from context far from the convolution kernel could negatively impact the performance.\n\nA rich set of literature has discussed approaches to solving the limitation of locality and enabling long-range interactions in CNNs. Some employ , or , which expand the receptive field somewhat, but it is still limited to a small local region. Another line of work adopts  mechanisms, e.g., , which allow the receptive field to cover the entire input image, as opposed to local convolutions. Unfortunately, such approaches are computationally expensive, especially for large inputs. Recent works enable building , but at a cost of applying local constraints to non-local neural networks. These restrictions limit the model receptive field, which is harmful to tasks such as , especially on high-resolution inputs.\n\nIn our recent  paper, \u201c\u201d, we propose to adopt  (or ), which recovers large receptive field in fully attentional models. The core idea is to separate 2D attention into two steps that apply 1D attention in the height and width axes sequentially. The efficiency of this approach enables attention over large regions, allowing models that learn long-range, or even global, interactions. Additionally, we propose a novel formulation for self-attention modules, which is more sensitive to the position of relevant context in a large receptive field with marginal costs. We evaluate our position-sensitive axial-attention method on panoptic segmentation by applying it to , a simple and efficient method for panoptic segmentation. The effectiveness of our model is demonstrated on , , and . Axial-DeepLab achieves state-of-the-art results on  and , outperforming  by a large margin.\n\n\nAxial-DeepLab consists of an Axial-ResNet backbone and  output heads, which produce panoptic segmentation results. Our Axial-ResNet is built on a , in which  the 3\u00d73  convolutions in the ResNet bottleneck blocks are replaced by our proposed  position-sensitive axial-attention, thus enabling both a large receptive field and precise positional information.\nThe Axial-DeepLab  axial attention layer provides 1-dimensional self-attention globally, propagating information within individual columns \u2014 it does not transfer information between columns. The second 1D attention layer operating in the horizontal direction allows one to capture both column-wise and row-wise information. This separation reduces the complexity of self-attention from quadratic (2D) to linear (1D), which enables using a much larger (65\u00d765 vs. previously 3\u00d73) or even global context in all layers for long-range modeling in panoptic segmentation.\nNote that a message or feature vector at (1, 1) can always be passed globally on a 2D lattice to any position (x2, y2), with one hop on the height-axis (1, 1 \u21921, 2), followed by another hop on the width axis (1, 2 \u2192 2, 2). In this way, we are able to model 2D long-range relations in a single residual block. This axial-attention design also reduces the complexity from quadratic to linear and enables global receptive fields in all layers of a model.\n\n\nAdditionally, we propose a position-sensitive formulation for self-attention. Previous self-attention formulations enabled a given pixel  to aggregate long-range context , but provided no information about where in the receptive field the context originated. For example, perhaps the feature at pixel  represents the eye of a cat, and the context  might be the nose and another eye. In this case, the  feature at pixel  would be a nose and two eyes, regardless of the geometric structure of a face. This could cause a false indication of the presence of a face when the two eyes are on the bottom-left of an image and the nose is on the top-right. A  is to impose a positional bias on where in the receptive field the context can originate. This bias depends on the feature at  only, (an eye), but not the feature at B, which contains important contextual information.\n\nIn this work, we let this bias also depend on the context feature at  (i.e., the nose and another eye). This change enables a more accurate positional bias when a pixel and the context informing it are far away from one another and thus contains different information about the bias. In addition, when pixel  aggregates the context feature , we also include a feature that indicates the relative position from  to . This change enables  to know precisely where  originated. These two changes make self-attention , especially in the situation of long-range modeling.\n\n\nWe have tested Axial-DeepLab on , and  for . Improvements over the state-of-the-art  for each dataset can be seen in the table below. In particular, our Axial-DeepLab outperforms Panoptic-DeepLab by 2.8% Panoptic Quality (PQ) on the COCO test-dev set. Our single-scale small model performs better than multi-scale Panoptic-DeepLab while improving computational efficiency by 27x and using only 1/4 the number of parameters. We also show state-of-the-art results on Cityscapes. Moreover, we find that the performance increases as the block receptive field increases from 5 \u00d7 5 to 65 \u00d7 65. Our model is also more robust to out-of-distribution scales, on which the model was not trained.\nBesides our main results on panoptic segmentation, our full axial-attention model, Axial-ResNet, also performs better than the previous best  on ImageNet.\n\nWe have proposed and demonstrated the effectiveness of position-sensitive axial-attention on image classification and panoptic segmentation. On ImageNet, our Axial-ResNet, formed by stacking axial-attention blocks, achieves state-of-the-art results among stand-alone self-attention models. We further convert Axial-ResNet to Axial-DeepLab for bottom-up panoptic segmentation, and also show state-of-the-art performance on several benchmarks, including COCO, and Cityscapes. We hope our promising results could establish that axial-attention is an effective building block for modern computer vision models.", "date": "\nWednesday, August 26, 2020\n"},
{"website": "Google-AI", "title": "\nAn Analysis of Online Datasets Using Dataset Search (Published, in Part, as a Dataset)\n", "author": ["Posted by Natasha Noy, Research Scientist and Omar Benjelloun, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/08/an-analysis-of-online-datasets-using.html", "abstract": "There are tens of millions of datasets on the web, with content ranging from sensor data and government records, to results of scientific experiments and business reports. Indeed, there are datasets for almost anything one can imagine, be it  or . More than two years ago, we undertook an effort to design a search engine that would provide a single entry point to these millions of datasets and thousands of repositories.  The result is , which we  in 2018 and  in January 2020. In addition to facilitating access to data, Dataset Search reconciles and indexes datasets using the metadata descriptions that come directly from the dataset web pages using  structure.\n\nAs of today, the complete Dataset Search corpus contains more than 31 million datasets from more than 4,600 internet domains. About half of these datasets come from .com domains, but .org and governmental domains are also well represented. The graph below shows the growth of the corpus over the last two years, and while we still don\u2019t know what fraction of datasets on the web are currently in Dataset Search, the number continues to grow steadily.\nTo better understand the breadth and utility of the datasets made available through Dataset Search, we published \u201c\u201d, accepted at the . Here we provide an overview of the available datasets, present metrics and insights originating from their analysis, and suggest best practices for publishing future scientific datasets. In order to enable other researchers to build analysis and tools using the metadata, we are also making a  publicly available.\n\n\nIn order to determine the distribution of topics covered by the datasets, we infer the research category based on dataset titles and descriptions, as well as other text on the dataset Web pages. The two most common topics are geosciences and social sciences, which account for roughly 45% of the datasets. Biology is a close third at ~15%, followed by a roughly even distribution for other topics, including computer science, agriculture, and chemistry, among others.\nIn our  to launch Dataset Search, we reached out to specific communities, which was key to bootstrapping widespread use of the corpus. Initially, we focused on geosciences and social sciences, but since then, we have allowed the corpus to grow organically. We were surprised to see that the fields associated with the communities we reached out to early on are still dominating the corpus. While their early involvement certainly contributes to their prevalence, there may be other factors involved, such as differences in culture across communities. For instance, geosciences have been  in making their data findable, accessible, interoperable, and reusable (), a core component to reducing barriers for access.\n\n\nThere is a growing  among researchers across scientific disciplines that it is important to make datasets available, to publish details relevant to their use, and to cite them when they are used. Many funding agencies and academic publishers require proper publication and citation of data.\n\nPeer-reviewed journals such as  are dedicated to publishing valuable datasets, and efforts such as  provide  (DOIs) for them. Resolution services (e.g., ) also provide persistent, de-referenceable identifiers, allowing for easy citation, which is key to making datasets widely available in scientific discourse. Unfortunately, we found that only about 11% of the datasets in the corpus (or ~3M) have DOIs. We chose this subset from the dataset corpus to be included in our . From this collection, about 2.3M datasets come from two sites,  and :\n\nPublishers can specify access requirements for a dataset via schema.org metadata properties, including details of the  and information indicating whether or not the dataset . Only 34% of datasets specify license information, but when no license is specified, users cannot make any assumptions on whether or not they are allowed to reuse the data. Thus, adding licensing information, and, ideally, adding as open a license as possible, will greatly improve the reusability of the data. \n\nAmong the datasets that did specify a license, we were able to recognize a known license in 72% of cases. Those licenses include Open Government licenses for the  and , , and several Public Domain licenses (e.g., ). We found 89.5% of these datasets to either be accessible for free or use a license that allows redistribution, or both. And of these open datasets, 5.6M (91%) allow commercial reuse.\n\nAnother critical component of data reusability is providing downloadable data, yet only 44% of datasets specify download information in their metadata. A possible explanation for this surprisingly low value is that webmasters (or dataset-hosting platforms) fear that exposing the data download link through schema.org metadata may lead search engines or other applications to give their users direct access to download the data, thus \u201cstealing\u201d traffic from their website. Another concern may be that data needs the proper context to be used appropriately (e.g., methodology, footnotes, and license information), and providers feel that only their web pages can give the complete picture. In Dataset Search, we do not show download links as part of dataset metadata so that users must go to the publisher\u2019s website to download the data, where they will see the full context for the dataset.\n\n\nFinally, we examine how Dataset Search is being used. Overall, 2.1M unique datasets from 2.6K domains appeared in the top 100 Dataset Search results over 14 days in May 2020. We find that the distribution of topics being queried is different from that of the corpus as a whole. For instance, geoscience takes up a much smaller fraction, and conversely, biology and medicine represent a larger fraction relative to their share of the corpus. This result is likely explained by the timing of our analysis, as it was performed during the first weeks of the COVID-19 pandemic.\n\nBased on our analysis, we have identified a set of best practices that can improve how datasets are discovered, reused and cited. \n\nDataset metadata should be on pages that are accessible to web crawlers and that provide metadata in machine-readable formats in order to improve discoverability.\nPublishing metadata on sites that are likely to be more persistent than personal web pages will facilitate data reuse and citation. Indeed, during our analysis of Dataset Search, we noted a very high rate of turnover \u2014 many URLs that hosted a dataset one day did not have it a few weeks or months later. Data repositories, such as , , ,  and many others, are a good way to ensure dataset persistence. Many of these repositories have  to preserve data in perpetuity.\nWith datasets often published in multiple repositories, it would be useful for repositories to describe the provenance information more explicitly in the metadata. The provenance information helps users understand who collected the data, where the primary source of the dataset is, or how it might have changed.  \nDatasets should include licensing information, ideally in a machine-readable format. Our analysis indicates that when dataset providers select a license, they tend to choose a fairly open one. So, encouraging and enabling scientists to choose licenses for their data will result in many more datasets being openly available.\nDOIs are critical for long-term tracking and useability. Not only do these identifiers allow for much easier citation of datasets and version tracking, they are also dereferenceable: if a dataset moves, the identifier can point to a different location.\n\nAs part of the announcement today, we are also  for others to use. It contains the metadata for more than three million datasets that have DOIs and other types of persistent identifiers \u2013- these are the datasets that are the most easily citable. Researchers can use this metadata to perform deeper analysis or to build their own applications using this data. For example, much of the growth of DOI usage appears to have been within the last decade. How does this timeframe relate to the datasets covered in the corpus? Is the DOI usage distribution uniform across datasets, or are there significant differences between research communities?\n\nWe will update the dataset on a regular basis. Finally, we hope that focusing this data release on datasets with persistent citable identifiers will encourage more data providers to describe their datasets in more detail and to make them more easily citable.\n\nIn conclusion, we hope that having data more discoverable through tools such as Google's Dataset Search will encourage scientists to share their data more broadly and do it in a way that makes data truly .", "date": "\nTuesday, August 25, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at ECCV 2020\n", "author": ["Posted by Melody Pound and Lauren Jones"], "link": "http://ai.googleblog.com/2020/08/google-at-eccv-2020.html", "abstract": "This week, the  (ECCV2020) begins, a premier forum for the dissemination of research in computer vision and related fields. Being held virtually for the first time this year, Google is proud to be an  and is excited to share our research with the community with nearly 50 accepted publications, alongside several tutorials and workshops.\nIf you are registered for ECCV this year, please visit our virtual booth in the Platinum Exhibition Hall to learn more about the research we\u2019re presenting at ECCV 2020, including some demos and opportunities to connect with our researchers. You can also learn more about our contributions below (Google affiliations in bold).\n\nGeneral Chairs:\u00a0\nAcademic Demonstrations Chair:\u00a0\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganizers:\n\t\nOrganizers:\n\n\nOrganizers:\n\n\n  \nOrganizers:\n\nOrganizers:\n\nOrganizers:\n\nOrganizers:\n\nOrganizers:\n \n  ()\nPanel Participation: \n  *Work performed while at Google", "date": "\nMonday, August 24, 2020\n"},
{"website": "Google-AI", "title": "\nUnderstanding View Selection for Contrastive Learning\n", "author": ["Posted by Yonglong Tian, Student Researcher and Chen Sun, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/08/understanding-view-selection-for.html", "abstract": "Most people take for granted the ability to view an object from several different angles, but still recognize that it's the same object\u2014 a dog viewed from the front is still a dog when viewed from the side. While people do this naturally, computer scientists need to explicitly enable machines to  that are , with the goal of seeking robust data representations that retain information that is useful to downstream tasks. \n\nOf course, in order to learn these representations, manually annotated training data can be used. However, as in many cases such annotations aren\u2019t available, which gives rise to a series of  and  supervised approaches that do not require manually annotated training data. Currently, a popular paradigm for training with such data is , where two views of the same scene (for example, , , and ) will tend to converge in representation space while two views of different scenes diverge. Despite their success, one important question remains: \u201cIf one doesn\u2019t have annotated labels readily available, how does one select the views to which the representations should be invariant?\u201d In other words, how does one identify an object using information that resides in the pixels of the image itself, while still remaining accurate when that image is viewed from disparate viewpoints?\n\nIn \u201c\u201d, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that one should reduce the  between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their mutual information. We also consider data augmentation as a way to reduce mutual information, and show that increasing data augmentation indeed leads to decreasing mutual information while improving downstream classification accuracy. To encourage further research in this space, we have open-sourced the .\n\n\nThe goal of contrastive multiview learning is to learn a parametric encoder, whose output representations can be used to discriminate between pairs of views with the same identities, and pairs with different identities. The amount and type of information shared between the views determines how well the resulting model performs on downstream tasks. We hypothesize that the views that yield the best results should discard as much information in the input as possible except for the task relevant information (e.g., object labels), which we call the . \n\nConsider the example below in which two patches of the same image represent the different \u201cviews\u201d. The training objective is to identify that the two views belong to the same image. It is undesirable to have views that share too much information, for example, where low-level color and texture cues can be exploited as \u201cshortcuts\u201d (left), or to have views that share too little information to identify that they belong to the same image (right). Rather, views at the \u201csweet spot\u201d share the information related to downstream tasks, such as patches corresponding to different parts of the panda for an object classification task (center).\n\nWe design several sets of experiments to verify the InfoMin hypothesis, motivated by the fact that there are simple ways to control the mutual information shared between views without any supervision. For example, we can sample different patches from the same images, and reduce their mutual information simply by increasing the distance between the patches. Here, we estimate the mutual information using  (I), which is a quantitative measure of the mutual information lower bound. Indeed, we observe a reverse U-shape curve: as mutual information is reduced, the downstream task accuracy first increases and then begins to decrease.\nFurthermore, we demonstrate that several state-of-the-art contrastive learning methods (, , , ,  and ) can be unified through the perspective of view selection: despite the differences in architecture, objective and engineering details, all recent contrastive learning methods create two views that implicitly follow the InfoMin hypothesis, where the information shared between views are controlled by the strength of data augmentation. Motivated by this, we propose a new set of data augmentations, which outperforms the prior state of the art, , by nearly 4% on the ImageNet . We also found that transferring our unsupervised pre-trained models to  and  consistently outperforms ImageNet pre-training.\n\n\nIn our work, we design unsupervised and semi-supervised methods that synthesize novel views following the InfoMin hypothesis. We learn  that transfer natural color spaces into novel color spaces, from which we split the channels to get views. For the unsupervised setup, the view generators are optimized to minimize the InfoNCE bound between views. As shown in the results below, we observe a similar reverse U-shape trend while minimizing the InfoNCE bound.\nTo reach the sweet spot without overly minimizing mutual information, we can use the semi-supervised setup and guide the view generator to retain label information. As expected, all learned views are now centered around the sweet spot, no matter what the input color space is.\n\n\nTo accelerate research in self-supervised contastive learning, we are excited to share the code and pretrained models of InfoMin with the academic community. They can be found .", "date": "\nFriday, August 21, 2020\n"},
{"website": "Google-AI", "title": "\nTackling Open Challenges in Offline Reinforcement Learning\n", "author": ["Posted by George Tucker, Research Scientist and Sergey Levine, Faculty Advisor, Google Research"], "link": "http://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html", "abstract": "Over the past several years, there has been a surge of interest in  (RL) driven by its high-profile successes in  and . However, unlike  methods, which learn from massive datasets that are collected once and then reused, RL algorithms use a trial-and-error feedback loop that requires active interaction during learning, collecting data every time a new policy is learned. This approach is prohibitive in many real-world settings, such as healthcare, autonomous driving, and dialogue systems, where trial-and-error data collection can be costly, time consuming, or even irresponsible. Even for problems where some active data collection  be used, the requirement for interactive collection limits dataset size and diversity.\nOffline RL (also called  RL or  RL) relies solely on a previously collected dataset without further interaction. It provides a way to utilize previously collected datasets \u2014 from previous RL experiments, from human demonstrations, and from hand-engineered exploration strategies \u2014 in order to automatically learn decision-making strategies. In principle, while  (), they are generally only successful when used with active environment interaction \u2014 without receiving this direct feedback, they often exhibit . Consequently, while offline RL has enormous potential, that potential cannot be reached without resolving significant algorithmic challenges. \n\nIn \u201c\u201d, we provide a comprehensive tutorial on approaches for tackling the challenges of offline RL and discuss the many issues that remain. To address these issues, we have designed and released an open-source benchmarking framework,  (D4RL), as well as a new, simple, and highly effective offline RL algorithm, called  (CQL).\n\n\nIn order to understand the capabilities of current approaches and to guide future progress, it is first necessary to have effective benchmarks. A common choice in prior work was to simply use . However, while simple, this data collection approach is artificial because it involves training an online RL agent which is prohibitive in many real-world settings as we discussed previously. One wishes to learn a policy that is  than the current best from diverse data sources that provides good  of the task. For example, one might have data collected from a hand-designed controller of a robot arm, and use offline RL to train an improved controller. To enable progress in this field under realistic settings, one needs a benchmark suite that accurately reflects these settings, while being simple and accessible enough to enable rapid experimentation. \n\n provides standardized environments, datasets and evaluation protocols, as well as reference scores for recent algorithms to help accomplish this. This is a \u201cbatteries-included\u201d resource, making it ideal for anyone to jump in and get started with minimal fuss.The key design goal for D4RL was to develop tasks that reflect both real-world dataset challenges as well as real-world applications. Previous datasets used data collected either from random agents or agents . Instead, by thinking through potential applications in autonomous driving, robotics, and other domains, we considered how real-world applications of offline RL might require handling of data generated from human demonstrations or hard-coded controllers, data collected from heterogeneous sources, and data collected by agents with a variety of different goals.\nAside from the widely used  locomotion tasks, D4RL includes datasets for more complex tasks. The , which requires manipulating a realistic robotic hand to use a hammer, for example, illustrates the challenges of working with limited human demonstrations, without which these tasks are extremely challenging.  found that existing datasets could not distinguish between competing methods, whereas the Adroit domain reveals clear deficiencies between them.\n\nAnother common scenario for real-world tasks is one in which the dataset used for training is collected from agents performing a wide range of other activities that are related to, but not specifically targeted towards, the task of interest. For example, data from human drivers may illustrate how to drive a car well, but do not necessarily show how to reach a specific desired destination. In this case, one might like offline RL methods to \u201cstitch\u201d together parts of routes in the driving dataset to accomplish a task that was not actually seen in the data (i.e., navigation). As an illustrative example, given paths labeled \u201cA\u201d and \u201cB\u201d in the picture below, offline RL should be able to \u201cremix\u201d them to produce path C.\n\nWe constructed a series of increasingly difficult tasks to exercise this \u201cstitching\u201d ability. The maze environments, shown below, require two robots (a simple ball or an \u201cAnt\u201d robot) to navigate to locations in a series of mazes.\nA more complex \u201cstitching\u201d scenario is provided by the Franka kitchen domain (based on the ), where demonstrations from humans using a VR interface comprise a multi-task dataset, and offline RL methods must again \u201cremix\u201d this data.\nFinally, D4RL includes two tasks that are meant to more accurately reflect potential realistic applications of offline RL, both based on existing driving simulators. One is a first-person driving dataset that utilizes the widely used  simulator developed at Intel, which provides photo-realistic images in realistic driving domains, and the other is a dataset from the  traffic control simulator (from UC Berkeley), which requires controlling autonomous vehicles to facilitate effective traffic flow.\nWe have packaged these tasks and standardized datasets into an easy-to-use  to accelerate research. Furthermore, we provide  for all tasks using relevant prior methods (, , , , , ), in order to  baseline new approaches. We are not the first to propose a benchmark for offline RL: a number of   have proposed simple datasets based on running RL algorithms, and several  works have proposed datasets with image observations and other features. However, we believe that the more realistic dataset composition in D4RL makes it an effective way to drive progress in the field.\n\n\nAs we developed the benchmark tasks, we found that existing methods could not solve the more challenging tasks. The central challenge arises from a : in order to improve over the historical data, offline RL algorithms must learn to make decisions that differ from the decisions taken in the dataset. However, this can lead to problems when the consequences of a seemingly good decision cannot be deduced from the data \u2014 if no agent has taken this particular turn in the maze, how does one know if it leads to the goal or not? Without handling this distributional shift problem, offline RL methods can extrapolate erroneously, making over-optimistic conclusions about the outcomes of rarely seen actions. Contrast this with the online setting, where reward bonuses modeled after  optimistically bias the agent to explore all potentially rewarding paths. Because the agent receives interactive feedback, if the action turns out to be unrewarding, then it can simply avoid the path in the future.\n\nTo address this, we developed  (CQL), an offline RL algorithm designed to guard against overestimation while avoiding explicit construction of a separate behavior model and without using importance weights. While standard  (and actor-critic) methods , CQL is unique in that it is fundamentally a pessimistic algorithm: it assumes that if a good outcome was not seen for a given action, that action is likely to not be a good one. The central idea of CQL is to learn a lower bound on the policy\u2019s expected return (called the ), instead of learning to approximate the expected return. If we then optimize our policy under this conservative Q-function, we can be confident that its value is no lower than this estimate, preventing errors from overestimation.\n\nWe found that CQL attains state-of-the-art results on many of the harder D4RL tasks: CQL outperformed other approaches on the AntMaze, Kitchen tasks, and 6 out of 8 Adroit tasks. In particular, on the AntMaze tasks, which require navigating through a maze with an \u201cAnt\u201d robot, CQL is often the only algorithm that is able to learn non-trivial policies. CQL also performs well on other tasks, including Atari games. On the Atari tasks from , CQL outperforms prior methods when data is limited (\u201c1%\u201d dataset). Moreover, CQL is simple to implement on top of existing algorithms (e.g.,  and ), without training additional neural networks.\n\n\nWe are excited about the fast-moving field of offline RL. While we took a first step towards a standard benchmark, there is clearly still room for improvement. We expect that as algorithms improve, we will need to reevaluate the tasks in the benchmark and develop more challenging tasks. We look forward to working with the community to evolve the benchmark and evaluation protocols. Together, we can bring the rich promises of offline RL to real-world applications.", "date": "\nThursday, August 20, 2020\n"},
{"website": "Google-AI", "title": "\nUnderstanding Deep Learning on Controlled Noisy Labels\n", "author": ["Posted by Lu Jiang, Senior Research Scientist and Weilong Yang, Senior Staff Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/08/understanding-deep-learning-on.html", "abstract": "The success of deep neural networks depends on access to high-quality labeled training data, as the presence of label errors (label noise) in training data  of models on clean test data. Unfortunately, large training datasets almost always contain examples with inaccurate or incorrect labels. This leads to a paradox: on one hand, large datasets are necessary to train better deep networks, while on the other hand, deep networks tend to memorize training label noise, resulting in poorer model performance in practice.\n\nThe research community has recognized the importance of this problem, introducing works attempting to understand noisy training labels, e.g., by , as well as mitigation strategies, such as  or , to overcome them.  play a crucial role in understanding noisy labels by studying the impact of the the percentage of examples with incorrect labels in the dataset \u2014 on model performance. However, current experiments have only been performed on  labels, in which noisy examples have randomly assigned labels, not  label noise, which follows a different noise distribution. Such studies may then result in very different or even contradictory findings about noisy labels compared to practical experience. In addition, methods that perform well on synthetic noise may not work as well on real-world noisy labels.\n\nIn \u201c\u201d, published at , we make three contributions towards better understanding deep learning on non-synthetic noisy labels. First, we establish the first controlled dataset and benchmark of realistic, real-world label noise sourced from the web (i.e., ). Second, we propose a simple but highly effective method to overcome both synthetic and real-world noisy labels. Finally, we conduct the largest study to date that compares synthetic and web label noise across a wide variety of settings.\n\n\nThere are a number of differences between the distribution of images with synthetic versus real-world (web) label noise. First, images with web label noise tend to be more consistent, visually or semantically, with the true positive images. Second, synthetic label noise is at class-level (all examples in the same class are equally noisy), whereas real-world label noise is at instance-level (certain images are more likely to be mislabelled than others, regardless of the associated class). For example, images of \u201cHonda Civic\u201d and \u201cHonda Accord\u201d are more often confused when the images are taken from the side than when the vehicles are imaged from the front. Third, images with real-world label noise come from an open class vocabulary that may not overlap with the class vocabulary of a specific dataset. For example, the web noisy images of \u201cladybug\u201d include classes such as \u201cfly\u201d and other bugs that are not included in the class list of the dataset being used. The benchmark for controlled label noise will help provide better quantitative understanding of the differences between synthetic and real-world web label noise.\n\n\nThe benchmark in this work is built on two public datasets: , for coarse-grained image classification, and , for fine-grained image classification. We gradually replace clean images in these datasets with incorrectly labeled images gathered from the web, following  for the construction of  datasets.\n\nTo do this, we collect images from the web using the class name (e.g., \u201cladybug\u201d) as a keyword \u2014 an automatic approach to collect noisy labeled images from the web without manual annotations. Each retrieved image is then examined by 3-5 annotators using  who identify whether or not the web label given is correct, yielding nearly 213k annotated images. We use these web images with incorrect labels to replace a percentage of the clean training images in the original Mini-ImageNet and Stanford Cars datasets. We create 10 different datasets with progressively higher levels of label noise (from 0% clean data to 80% data with erroneous labels). The datasets have been open-sourced at our .\n\nGiven a dataset of some unknown noise level, our goal is to train a robust model that can generalize well on the clean test data. We introduce a simple yet effective method for dealing with both synthetic and real-world noisy labels, called MentorMix, which we developed on the Controlled Noisy Web Labels dataset.\nMentorMix is an iterative approach built on two existing techniques,  and , that comprises four steps: weight, sample, mixup, and weight again. In the first step, a weight is computed for every example in a mini-batch by a MentorNet network, which can be tailored to the task at hand, and the weights are normalized into a distribution. In practice, the goal is to assign high weights for correctly labeled examples and zero weights for incorrectly labeled examples. In reality, we don't know which are correct and which are incorrect, so MentorNet weights are based on approximations. In the example here, MentorNet uses the StudentNet training loss to determine the weights in the distribution.\u00a0Next, for each example, we use  to select another example in the same mini-batch according to the distribution. As examples with higher weights tend to have the correct label, they are favored in the sampling procedure. We then use Mixup to mix the original and sampled examples so that the model interpolates between the two and avoids over-fitting the noisy training examples. Finally, we may compute another weight for the mixed example to scale the final loss. The impact of this second weighting strategy becomes more pronounced for high noise levels.\u00a0Conceptually, the above steps implement a new robust loss, which turns out to be more resilient to noisy training labels. More discussion on this topic can be found in . The animation below illustrates the four key steps in MentorMix, where StudentNet is the model to be trained on noisy labeled data. We employ a very simple version of MentorNet, as described by , to compute the weight for each example.\n\nWe evaluate MentorMix on five datasets including  with synthetic label noise, and , a large dataset of 2.2 million images with real-world noisy labels. MentorMix consistently yields improved results on the CIFAR 10/100 datasets and achieves the best published result on the WebVision dataset, improving the previous best method by a significant ~3% in terms of the top-1 classification accuracy on the  validation set.\n\nThis work represents the largest study to date into understanding deep neural networks trained on noisy labels. We propose three new findings on web label noise:\n\n    While it is well known that deep neural networks , our results suggest that deep neural networks generalize much better on web label noise. For example, the classification accuracy of a network trained on the Stanford Cars dataset using the 60% web label noise level is 0.66, much higher than that for the same network trained at the same 60% level of synthetic noise, which achieves only 0.09. This pattern is consistent across our two datasets using both fine-tuning and training from scratch.\n\n    Our common understanding is that  \u2014 an interesting property in which DNNs are able to automatically capture generalizable \u201cpatterns\u201d in the early training stage before memorizing noisy training labels. Because of this,  is commonly used for training on noisy data. However, our results suggest deep neural networks may not learn patterns first when trained using datasets with web label noise, at least for the fine-grained classification task, suggesting that early stopping may not be effective on real-world label noise from the web.\n\n     found that fine-tuning more advanced architectures trained on ImageNet tend to perform better on downstream tasks that have clean training labels. Our results extend this finding to noisy training data, showing that a better pre-trained architecture that exhibits better performance when pre-trained on ImageNet is likely to perform better even when it is fine-tuned on noisy training labels.\n\n\nBased on , we have the following practical recommendations for training deep neural networks on noisy data.\n\nThe code of MentorMix is available on , the datasets are on our .", "date": "\nWednesday, August 19, 2020\n"},
{"website": "Google-AI", "title": "\nLanguage-Agnostic BERT Sentence Embedding\n", "author": ["Posted by Yinfei Yang and Fangxiaoyu Feng, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html", "abstract": "A multilingual embedding model is a powerful tool that encodes text from different languages into a shared embedding space, enabling it to be applied to a range of downstream tasks, like , , and others, while also leveraging semantic information for language understanding. Existing approaches for generating such embeddings, like  or ,  rely on parallel data, mapping a sentence from one language directly to another language in order to encourage consistency between the sentence embeddings. While these existing multilingual approaches yield good overall performance across a number of languages, they often underperform on high-resource languages compared to dedicated bilingual models, which can leverage approaches like  with translation pairs as training data to obtain more closely aligned representations. Further, due to limited model capacity and the often poor quality of training data for low-resource languages, it can be difficult to extend multilingual models to support a larger number of languages while maintaining good performance.\nRecent efforts to improve language models include the development of  (MLM) pre-training, such as that used by ,  and  This approach has led to exceptional gains across a wide range of languages and a variety of natural language processing tasks since it only requires monolingual text. In addition, MLM pre-training has been extended to the multilingual setting by modifying MLM training to include concatenated translation pairs, known as  (TLM), or by simply introducing pre-training data from multiple languages. However, while the internal model representations learned during MLM and TLM training are helpful when fine-tuning on downstream tasks, without a sentence level objective, they do not directly produce sentence embeddings, which are critical for translation tasks. \n\nIn \u201c\u201d, we present a multilingual  embedding model, called LaBSE, that produces language-agnostic cross-lingual sentence embeddings for 109 languages. The model is trained on 17 billion monolingual sentences and 6 billion bilingual sentence pairs using MLM and TLM pre-training, resulting in a model that is effective even on low-resource languages for which there is no data available during training. Further, the model establishes a new state of the art on multiple  (a.k.a. ) retrieval tasks. We have released the pre-trained model to the community through , which includes modules that can be used as-is or can be fine-tuned using domain-specific data.\n\nIn , we proposed the use of a to learn a multilingual sentence embedding space. This approach tasks the model with ranking the true translation over a collection of sentences in the target language, given a sentence in the source language. The translation ranking task is trained using a dual encoder architecture with a shared  encoder. The resulting bilingual models achieved state-of-the-art performance on multiple parallel text retrieval tasks (including  and ). However, the model suffered when the bi-lingual models were extended to support multiple languages (16 languages, in our test case) due to limitations in model capacity, vocabulary coverage, training data quality and more.\nFor LaBSE, we leverage recent advances on language model pre-training, including MLM and TLM, on a -like architecture and follow this with fine-tuning on a translation ranking task. A 12-layer  with a 500k token vocabulary pre-trained using MLM and TLM on 109 languages is used to increase the model and vocabulary coverage. The resulting LaBSE model offers extended \n\nWe evaluate the proposed model using the , a dataset consisting of up to 1,000 English-aligned sentence pairs for 112 languages. For more than 30 of the languages in the dataset, the model has no training data. The model is tasked with finding the nearest neighbor translation for a given sentence, which it calculates using the . \n\nTo understand the performance of the model for languages at the head or tail of the training data distribution, we divide the set of languages into several groups and compute the average accuracy for each set. The first 14-language group is selected from the languages supported by m~USE, which cover the languages from the head of the distribution (). We also evaluate a second language group composed of 36 languages from the . The third 82-language group, selected from the languages covered by the LASER training data, includes many languages from the tail of the distribution (). Finally, we compute the average accuracy for all languages.\n\nThe table below presents the average accuracy achieved by LaBSE, compared to the m~USE and LASER models, for each language group. As expected, all models perform strongly on the 14-language group that covers most head languages. With more languages included, the averaged accuracy for both LASER and LaBSE declines. However, the reduction in accuracy from the LaBSE model with increasing numbers of languages is much less significant, outperforming LASER significantly, particularly when the full distribution of 112 languages is included (83.7% accuracy vs. 65.5%).\n\n\nThe average performance of all languages included in Tatoeba is very promising. Interestingly, LaBSE even performs relatively well for many of the 30+ Tatoeba languages for which it has no training data (see below). For one third of these languages the LaBSE accuracy is higher than 75% and only 8 have accuracy lower than 25%, indicating very strong transfer performance to languages without training data. Such positive language transfer is only possible due to the massively multilingual nature of LaBSE.\n\nLaBSE can be used for mining parallel text (bi-text) from web-scale data. For example, we applied LaBSE to , a large-scale monolingual corpus, to process 560 million Chinese and 330 million German sentences for the extraction of parallel text. Each Chinese and German sentence pair is encoded using the LaBSE model and then the encoded embedding is used to find a potential translation from a pool of 7.7 billion English sentences pre-processed and encoded by the model. An  is employed to quickly search through the high-dimensional sentence embeddings. After a simple filtering, the model returns 261M and 104M potential parallel pairs for English-Chinese and English-German, respectively. The trained NMT model using the mined data reaches BLEU scores of 35.7 and 27.2 on the  (wmt17 for English-to-Chinese and wmt14 for English-to-German). The performance is only a few points away from current state-of-art-models trained on high quality parallel data.\n\n\nWe're excited to share this research, and the model, with the community.  The pre-trained model is released at  to support further research on this direction and possible downstream applications. We also believe that what we're showing here is just the beginning, and there are more important research problems to be addressed, such as building better models to support all languages.", "date": "\nTuesday, August 18, 2020\n"},
{"website": "Google-AI", "title": "\nOn-device, Real-time Body Pose Tracking with MediaPipe BlazePose\n", "author": ["Posted by Valentin Bazarevsky and Ivan Grishchenko, Research Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html", "abstract": "Pose estimation from video plays a critical role enabling the overlay of digital content and information on top of the physical world in , recognition, , and even , where it can form the basis for yoga, dance, and fitness applications. Pose estimation for fitness applications is particularly challenging due to the wide variety of possible poses (e.g., hundreds of yoga ), numerous degrees of freedom, occlusions (e.g. the body or other objects occlude limbs as seen from the camera), and a variety of appearances or outfits.\n\n\n\nToday we are announcing the release of a new approach to human body pose perception, , which we  at the  at . Our approach provides human pose tracking by employing machine learning (ML) to infer 33, 2D landmarks of a body from a single frame. In contrast to current pose models based on the standard , BlazePose accurately localizes more keypoints, making it uniquely suited for fitness applications. In addition, current state-of-the-art approaches rely primarily on powerful desktop environments for inference, whereas our method achieves real-time performance on mobile phones with CPU inference. If one leverages GPU inference, BlazePose achieves super-real-time performance, enabling it to run subsequent ML models, like face or hand tracking.\n\n\n\nThe current standard for human body pose is the , which consists of 17 landmarks across the torso, arms, legs, and face. However, the COCO keypoints only localize to the ankle and wrist points, lacking scale and orientation information for hands and feet, which is vital for practical applications like fitness and dance. The inclusion of more keypoints is crucial for the subsequent application of domain-specific pose estimation models, like those for hands, face, or feet.\n\nWith BlazePose, we present a new topology of 33 human body keypoints, which is a superset of COCO,  and  topologies. This allows us to determine body semantics from pose prediction alone that is consistent with face and hand models.\n\n\n\nFor pose estimation, we utilize our  two-step . Using a detector, this pipeline first locates the pose region-of-interest (ROI) within the frame. The tracker subsequently predicts all 33 pose keypoints from this ROI. Note that for video use cases, the detector is run only on the first frame. For subsequent frames we derive the ROI from the previous frame\u2019s pose keypoints as discussed below.\n\n\n\nFor real-time performance of the full ML pipeline consisting of pose detection and tracking models, each component must be very fast, using only a few milliseconds per frame. To accomplish this, we observe that the strongest signal to the neural network about the position of the torso is the person's face (due to its high-contrast features and comparably small variations in appearance). Therefore, we achieve a fast and lightweight pose detector by making the strong (yet for many mobile and web applications valid) assumption that the head should be visible for our single-person use case.\n\nConsequently, we trained a face detector, inspired by our sub-millisecond  model, as a proxy for a pose detector. Note, this model only detects the location of a person within the frame and can not be used to identify individuals. In contrast to the  and  tracking pipelines, where we derive the ROI from predicted keypoints, for the human pose tracking we explicitly predict two additional keypoints that firmly describe the human body center, rotation and scale as a circle. Inspired by , we predict the midpoint of a person's hips, the radius of a circle circumscribing the whole person, and the incline angle of the line connecting the shoulder and hip midpoints. This results in consistent tracking even for very complicated cases, like specific yoga asanas. The figure below illustrates the approach.\n\n\n\nThe pose estimation component of the pipeline predicts the location of all 33 person keypoints with three degrees of freedom each ( location and visibility) plus the two virtual alignment keypoints described above. Unlike current approaches that employ compute-intensive  prediction, our model uses a regression approach that is by a combined heat map/offset prediction of all keypoints, as shown below. \n\n\nSpecifically, during training we first employ a  to train the center and left tower of the network. We then remove the heatmap output and train the regression encoder (right tower), thus, effectively using the heatmap to supervise a lightweight embedding.\n\nThe table below shows an ablation study of the model quality resulting from different training strategies. As an evaluation metric, we use the Percent of Correct Points with 20% tolerance (PCK@0.2) (where we assume the point to be detected correctly if the 2D Euclidean error is smaller than 20% of the  corresponding person\u2019s torso size).  To obtain a human baseline, we asked annotators to annotate several samples redundantly and obtained an average PCK@0.2 of 97.2. The training and validation have been done on a geo-diverse dataset of various poses, sampled uniformly.\n\nTo cover a wide range of customer hardware, we present two pose tracking models: lite and full, which are differentiated in the balance of speed versus quality. For performance evaluation on CPU, we use ; for mobile GPUs, we use the  backend.\n\n\nBased on human pose, we can build a variety of applications, like fitness or yoga trackers. As an example, we present squats and push up counters, which can automatically count user statistics, or verify the quality of performed exercises. Such use cases can be implemented either using an additional classifier network or even with a simple joint pairwise distance lookup algorithm, which matches the closest pose in normalized pose space.\n\n\nWe have released a version of  targeting upper body use cases in  running on Android, iOS and Python. BlazePose will also be made available to the broader mobile developer community via the  in the upcoming release of . Apart from the mobile domain, we preview our  as well. We hope that providing this human pose perception functionality to the broader research and development community will result in an emergence of creative use cases, stimulating new applications, and new research avenues.\n\nWe plan to extend this technology with more robust and stable tracking to an even larger variety of human poses and activities. In the accompanying , we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with  We believe that publishing this technology can provide an impulse to new creative ideas and applications by the members of the research and developer community at large. We are excited to see what you can build with it!", "date": "\nThursday, August 13, 2020\n"},
{"website": "Google-AI", "title": "\nREALM: Integrating Retrieval into Language Representation Models\n", "author": ["Posted by Ming-Wei Chang and Kelvin Guu, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html", "abstract": "Recent advances in natural language processing have largely built upon the power of , which trains general purpose language representation models using a large amount of text, without human annotations or labels. These pre-trained models, such as  and , have been shown to , such as \u201cthe birthplace of \u201d, \u201cthe developer of \u201d and \u201cthe owner of \u201d. While the ability to encode knowledge is especially important for certain natural language processing tasks such as question answering, information retrieval and text generation, these models memorize knowledge  \u2014 i.e., world knowledge is captured in an abstract way in the model weights \u2014 making it difficult to determine what knowledge has been stored and where it is kept in the model. Furthermore, the storage space, and hence the accuracy of the model, is limited by the size of the network. To capture more world knowledge, the standard practice is to train ever-larger networks, which can be prohibitively slow or expensive. \nInstead, what if there was a method for pre-training that could access knowledge , e.g., by referencing an additional large external text corpus, in order to achieve accurate results without increasing the model size or complexity?\u00a0 For example, a sentence found in an external document collection, \"Francesco Bartolomeo Conti was born in Florence,\" could be referenced by the model to determine the birthplace of the musician, rather than relying on the model's opaque ability to access the knowledge stored in its own parameters. The ability to retrieve text containing explicit knowledge such as this would improve the efficiency of pre-training while enabling the model to perform well on knowledge-intensive tasks without using billions of parameters.\nIn \u201c\u201d, accepted at the , we share a novel paradigm for language model pre-training, which augments a language representation model with a , allowing REALM models to retrieve textual world knowledge  from raw text documents, instead of memorizing all the knowledge in the model parameters. We have also open sourced the  to demonstrate how one can train the retriever and the language representation jointly.\n\n\nTo understand how standard language representation models memorize world knowledge, one should first review how these models are pre-trained. Since the invention of , the fill-in-the-blank task, called , has been widely used for pre-training language representation models.  Given any text with certain words masked out, the task is to fill back the missing words. An example of this task looks like:\n\nDuring pre-training, a model will go over a large number of examples and adjust the parameters in order to predict the missing words (, in the above example). Interestingly, the fill-in-the-blank task makes the model memorize certain facts about the world. For example, the knowledge of Einstein's birthplace is required to fill the missing word in the following example:\n\n\n()\n\n\nHowever, because the world knowledge captured by the model is stored in the model weights, it is abstract, making it difficult to understand what information is stored. \n\n\nIn contrast to standard language representation models, REALM augments the language representation model with a  that first retrieves another piece of text from an external document collection as the supporting knowledge \u2014 in our experiments, we use the  \u2014 and then feeds this supporting text as well as the original text into a language representation model.\n\nThe key intuition of REALM is that a retrieval system should improve the model's ability to fill in missing words. Therefore, a retrieval that provides more context for filling the missing words should be rewarded. If the retrieved information does not help the model make its predictions, it should be discouraged, making room for better retrievals.\n\nHow does one train a knowledge retriever, given that only unlabeled text is available during pre-training? It turns out that one can use the task of filling words to train the knowledge retriever indirectly, without any human annotations. Assume the input of the query is:\n\nFilling the missing word () in this sentence without retrieval can be tricky, as the model would need to have implicitly stored knowledge of the country in which the Buckingham Palace is located and the associated currency, as well as make the connection between the two. It would be easier for the model to fill in the missing word if it was presented with a passage that explicitly connects some of the necessary knowledge, retrieved from an external corpus. \n\n\n In this example, the retriever would be rewarded for retrieving the following sentence.\n\nSince the retrieval step needs to add more context, there may be multiple retrieval targets that could be helpful in filling the missing word, for example, \u201c\u201d  The whole process is demonstrated in the next figure:\n\n\n\n\n\n\nScaling REALM pre-training such that models can retrieve knowledge from millions of documents is challenging. In REALM, the selection of the best document is formulated as  (MIPS). To perform retrieval, MIPS models need to first encode all of the documents in the collection, such that each document has a corresponding document vector. When an input arrives, it is encoded as a query vector. In MIPS, given a query, the document in the collection that has the maximum inner product value between its document vector and the query vector is retrieved, as shown in the following figure:\n\n\n\n\n\n\nWe evaluate the effectiveness of REALM by applying it to  (Open-QA), one of the most knowledge-intensive tasks in natural language processing. The goal of the task is to answer questions, such as \u201cWhat is the angle of the equilateral triangle?\u201d \n\nIn standard question answering tasks (e.g.,   or ), the supporting document is provided as part of input, so a model only needs to look up the answer in the given document. In Open-QA, there are no given documents, so that Open-QA models need to look up the knowledge by themselves \u2014 this makes Open-QA an excellent task to examine the effectiveness of REALM.\n\nThe following figure shows the results on the OpenQA version of . We mainly compared our results with , another approach that trains models without annotated supporting documents. From the figure, one can clearly see that REALM pre-training generates very powerful Open-QA models, and even outperforms the much larger T5 (11B) model by almost 4 points, using only a fraction of the parameters (300M).\n\n\n\n\n\n\nThe release of REALM has helped drive interest in developing end-to-end retrieval-augmented models, including a recent . We look forward to the possibility of extending this line of work in several ways, including 1) applying REALM-like methods to new applications that require knowledge-intensive reasoning and interpretable provenance (beyond Open-QA), and 2) exploring the benefits of retrieving other forms of knowledge, such as images, knowledge graph structures, or even text in other languages. We are also excited to see what the research community does with the open source !", "date": "\nWednesday, August 12, 2020\n"},
{"website": "Google-AI", "title": "\nA Simulation Suite for Tackling Applied Reinforcement Learning Challenges\n", "author": ["Posted by Daniel J. Mankowitz, Research Scientist, DeepMind and Gabriel Dulac-Arnold, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/08/a-simulation-suite-for-tackling-applied.html", "abstract": "(RL) has proven to be effective in solving numerous complex problems ranging from   and  to  and . In each of these cases, a simulator is available or the real environment is quick and inexpensive to access. Yet, there are still considerable challenges to deploying RL to real-world products and systems. For example, in physical control systems, such as robotics and autonomous driving, RL controllers are trained to solve tasks like grasping objects or driving on a highway. These controllers are susceptible to effects such as sensor noise, system delays, or normal wear-and-tear that can reduce the quality of input to the controller, leading to incorrect decision-making and potentially catastrophic failures. \n\n\n\n\n\nIn \u201c\u201d, we identify and discuss nine different challenges that hinder the application of current RL algorithms to applied systems. We then follow up this work with an  in which we simulated versions of these challenges on state-of-the-art RL algorithms, and benchmark the effects of each. We have open-sourced these simulated challenges in the  (RWRL) task suite to help draw attention to these important issues, as well as accelerate research toward solving them. \n\n\nThe RWRL suite is a set of simulated tasks inspired by applied reinforcement learning challenges, the goal of which is to enable fast algorithmic iterations for both researchers and practitioners, without having to run slow, expensive experiments on real-systems. While there will be additional challenges transitioning from RL algorithms that were trained in simulation to real-world applications, this suite intends to close some of the more fundamental, algorithmic gaps. At present, RWRL supports a subset of the  domains, but the goal is to broaden the suite to support an even more diverse domain set.\n\n\nWe designed the suite with two main goals in mind. (1) It should be easy to use \u2014 a user should be able to start running experiments within minutes of downloading the suite, simply by changing a few lines of code. (2) It should be flexible \u2014 a user should be able to incorporate any combination of challenges into the environment with very little effort.\n\n\nTo illustrate the ease of use of the RWRL suite, imagine a researcher or practitioner wants to implement action delays (i.e., temporal delays on actions being sent to the environment). To use the RWRL suite, simply import the  module. Next, load an environment (e.g., cartpole) with the  argument. This optional argument is specified as a dictionary configuring delay applied to actions, observations, or rewards and the number of timesteps the corresponding element is delayed (e.g., 20 timesteps). Once the environment is loaded, the effects of actions are automatically delayed without any other changes to the experiment. This makes it easy to test an RL algorithm with action delays in a range of different environments supported by the RWRL suite.\nA user can combine different challenges or choose from a set of predefined benchmark challenges by simply adding additional arguments to the load function, all of which are specified in the open-source RWRL suite . \n\n\nThe RWRL suite provides functionality to support experiments related to eight of the nine different challenges that make applying current RL algorithms on applied systems difficult: sample efficiency; system delays; high-dimensional state and action spaces; constraints; partial observability, stochasticity and non-stationarity; multiple objectives; real-time inference; and training from offline logs. RWRL excludes the explainability challenge, which is abstract and non-trivial to define. The supported experiments are non-exhaustive and provide researchers and practitioners with the ability to analyze the capabilities of their agent with respect to each challenge dimension. Examples of the supported challenges include:\n\nAs can be seen in the graphs, a researcher or practitioner can quickly gain insights as to which type of delay affects their agent\u2019s performance. These delays can also be combined together to observe their combined effect.\n\n  \nMost systems rarely manifest only a single challenge, and we are excited to see how algorithms can deal with an environment in which there are multiple challenges combined with increasing levels of difficulty (\u2018Easy\u2019, \u2018Medium\u2019 and \u2018Hard\u2019). We highly encourage the research community to try and solve these challenges, as we believe that solving them will facilitate more widespread applications of RL to products and real-world systems. \n\nWhile the initial set of RWRL suite features and experiments provide a starting point for closing the gap between the current state of RL and the challenges of applied systems, there is still much work to do. The supported experiments are not exhaustive and we welcome new ideas from the wider community to better evaluate the capabilities of our RL agents. Our main goal with this suite is to highlight and encourage research on the core problems that limit the effectiveness of RL algorithms in applied products and systems and to accelerate progress towards enabling future RL applications.", "date": "\nWednesday, August 12, 2020\n"},
{"website": "Google-AI", "title": "\nOn-device Supermarket Product Recognition\n", "author": ["Posted by Chao Chen, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/07/on-device-supermarket-product.html", "abstract": "", "date": "\nTuesday, August 11, 2020\n"},
{"website": "Google-AI", "title": "\nMediaPipe Iris: Real-time Iris Tracking & Depth Estimation\n", "author": ["Posted by Andrey Vakunov and Dmitry Lagun, Research Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html", "abstract": "A wide range of real-world applications, including computational photography (e.g.,  and glint reflections) and  (e.g., virtual avatars) rely on estimating eye position by tracking the iris. Once accurate iris tracking is available, we show that it is possible to determine the metric distance from the camera to the user \u2014 without the use of a dedicated depth sensor. This, in-turn, can improve a variety of use cases, ranging from computational photography, over virtual try-on of properly sized glasses and hats to usability enhancements that adopt the font size depending on the viewer\u2019s distance. \n\nIris tracking is a challenging task to solve on mobile devices, due to limited computing resources, variable light conditions and the presence of occlusions, such as hair or people squinting. Often, sophisticated specialized hardware is employed, limiting the range of devices on which the solution could be applied.\n\nToday, we announce the release of , a new machine learning model for accurate iris estimation. Building on our work on , this model is able to track landmarks involving the iris, pupil and the eye contours using a single RGB camera, in real-time, without the need for specialized hardware. Through use of iris landmarks, the model is also able to determine the metric distance between the subject and the camera with relative error less than 10% without the use of depth sensor. Note that iris tracking does not infer the location at which people are looking, nor does it provide any form of identity recognition. Thanks to the fact that this system is implemented in  \u2014 an open source cross-platform framework for researchers and developers to build world-class ML solutions and applications \u2014 it can run on most modern mobile phones, desktops, laptops and even . \n\n\nThe first step in the pipeline leverages our previous work on , which uses high-fidelity facial landmarks to generate a mesh of the approximate face geometry. From this mesh, we isolate the eye region in the original image for use in the iris tracking model. The problem is then divided into two parts: eye contour estimation and iris location. We designed a multi-task model consisting of a unified encoder with a separate component for each task, which allowed us to use task-specific training data.\nTo train the model from the cropped eye region, we manually annotated ~50k images, representing a variety of illumination conditions and head poses from geographically diverse regions, as shown below. \n\nOur iris-tracking model is able to determine the metric distance of a subject to the camera with less than 10% error, without requiring any specialized hardware. This is done by relying on the fact that the horizontal iris diameter of the human eye remains roughly constant at 11.7\u00b10.5 mm across a wide population [, , , ], along with some simple geometric arguments. For illustration, consider a pinhole camera model projecting onto a sensor of square pixels. The distance to a subject can be estimated from facial landmarks by using the  of the camera, which can be obtained using camera capture APIs or directly from the  of a captured image, along with other camera intrinsic parameters. Given the focal length, the distance from the subject to the camera is directly proportional to the physical size of the subject\u2019s eye, as visualized below. \n\nIn order to quantify the accuracy of the method, we compared it to the depth sensor on an iPhone 11 by collecting front-facing, synchronized video and depth images on over 200 participants. We experimentally verified the error of the iPhone 11 depth sensor to be < 2% for distances up to 2 meters, using a laser ranging device. Our evaluation shows that our approach for depth estimation using iris size has a mean relative error of 4.3% and standard deviation of 2.4%. We tested our approach on participants with and without eyeglasses (not accounting for contact lenses on participants) and found that eyeglasses increase the mean relative error slightly to 4.8% (standard deviation 3.1%). We did not test this approach on participants with any eye diseases (like  or ). Considering MediaPipe Iris requires no specialized hardware, these results suggest it may be possible to obtain metric depth from a single image on devices with a wide range of cost-points.\n\n\nWe are releasing the iris and depth estimation models as a cross-platform MediaPipe pipeline that can run on desktop, mobile and the web. As described in our recent , we leverage  and  to run our Iris ML pipeline locally in the browser, without any data being sent to the cloud. \n\n\nWe plan to extend our MediaPipe Iris model with even more stable tracking for lower error and deploy it for accessibility use cases. We strongly believe in sharing code that enables reproducible research, rapid experimentation, and development of new ideas in different areas. In our  and the accompanying , we detail the intended uses, limitations and model fairness to ensure that use of these models aligns with . Note, that any form of surveillance or identification is explicitly out of scope and not enabled by this technology. We hope that  to the wider research and development community will result in an emergence of creative use cases, stimulating responsible new applications and new research avenues.\n\nFor more ML solutions from MediaPipe, please see our  and  for the latest updates.", "date": "\nThursday, August 6, 2020\n"},
{"website": "Google-AI", "title": "\nLive HDR+ and Dual Exposure Controls on Pixel 4 and 4a\n", "author": ["Posted by Jiawen Chen and Sam Hasinoff, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/08/live-hdr-and-dual-exposure-controls-on.html", "abstract": "", "date": "\nMonday, August 3, 2020\n"},
{"website": "Google-AI", "title": "\nIntroducing the Model Card Toolkit for Easier Model Transparency Reporting\n", "author": ["Posted by Huanming Fang and Hui Miao, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/07/introducing-model-card-toolkit-for.html", "abstract": "", "date": "\nWednesday, July 29, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing ScaNN: Efficient Vector Similarity Search\n", "author": ["Posted by Philip Sun, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html", "abstract": "", "date": "\nTuesday, July 28, 2020\n"},
{"website": "Google-AI", "title": "\nImproving Holistic Scene Understanding with Panoptic-DeepLab\n", "author": ["Posted by Bowen Cheng, Student Researcher and Liang-Chieh Chen, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/07/improving-holistic-scene-understanding.html", "abstract": "", "date": "\nTuesday, July 21, 2020\n"},
{"website": "Google-AI", "title": "\nExploring Faster Screening with Fewer Tests via Bayesian Group Testing\n", "author": ["Posted by Marco Cuturi and Jean-Philippe Vert, Research Scientists, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/07/exploring-faster-screening-with-fewer.html", "abstract": "", "date": "\nTuesday, July 14, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at ICML 2020\n", "author": ["Posted by Jaqui Herman and Cat Armato, Program Managers"], "link": "http://ai.googleblog.com/2020/07/google-at-icml-2020.html", "abstract": "", "date": "\nMonday, July 13, 2020\n"},
{"website": "Google-AI", "title": "\nGrounding Natural Language Instructions to Mobile UI Actions\n", "author": ["Posted by Yang Li, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/07/grounding-natural-language-instructions.html", "abstract": "", "date": "\nFriday, July 10, 2020\n"},
{"website": "Google-AI", "title": "\nAutoML-Zero: Evolving Code that Learns\n", "author": ["Posted by Esteban Real, Staff Software Engineer and Chen Liang, Software Engineer, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/07/automl-zero-evolving-code-that-learns.html", "abstract": "", "date": "\nThursday, July 9, 2020\n"},
{"website": "Google-AI", "title": "\nDuality \u2014 A New Approach to Reinforcement Learning\n", "author": ["Posted by Ofir Nachum and Bo Dai, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2020/07/duality-new-approach-to-reinforcement.html", "abstract": "", "date": "\nWednesday, July 8, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at ACL 2020\n", "author": ["Posted by Cat Armato and Emily Knapp, Program Managers"], "link": "http://ai.googleblog.com/2020/07/google-at-acl-2020.html", "abstract": "", "date": "\nMonday, July 6, 2020\n"},
{"website": "Google-AI", "title": "\nSmartReply for YouTube Creators\n", "author": ["Posted by Rami Al-Rfou, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/07/smartreply-for-youtube-creators.html", "abstract": "", "date": "\nWednesday, July 1, 2020\n"},
{"website": "Google-AI", "title": "\nSpineNet: A Novel Architecture for Object Detection Discovered with Neural Architecture Search\n", "author": ["Posted by Xianzhi Du, Software Engineer and Jaeyoun Kim, Technical Program Manager, Google Research"], "link": "http://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html", "abstract": "", "date": "\nTuesday, June 30, 2020\n"},
{"website": "Google-AI", "title": "\nLeveraging Temporal Context for Object Detection\n", "author": ["Posted by Sara Beery, Student Researcher and Jonathan Huang, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/06/leveraging-temporal-context-for-object.html", "abstract": "", "date": "\nFriday, June 26, 2020\n"},
{"website": "Google-AI", "title": "\nSensing Force-Based Gestures on the Pixel 4\n", "author": ["Posted by Philip Quinn and Wenxin Feng, Research Scientists, Android UX"], "link": "http://ai.googleblog.com/2020/06/sensing-force-based-gestures-on-pixel-4.html", "abstract": "", "date": "\nWednesday, June 24, 2020\n"},
{"website": "Google-AI", "title": "\nPresenting a Challenge and Workshop in Efficient Open-Domain Question Answering\n", "author": ["Posted by Eunsol Choi, Visiting Faculty Researcher and Tom Kwiatkowski, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/06/presenting-challenge-and-workshop-in.html", "abstract": "", "date": "\nTuesday, June 23, 2020\n"},
{"website": "Google-AI", "title": "\nRepNet: Counting Repetitions in Videos\n", "author": ["Posted by Debidatta Dwibedi, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html", "abstract": "", "date": "\nMonday, June 22, 2020\n"},
{"website": "Google-AI", "title": "\nImproving Speech Representations and Personalized Models Using Self-Supervision\n", "author": ["Posted by Joel Shor, Software Engineer and Oran Lang, Software Engineer, Google Research, Israel"], "link": "http://ai.googleblog.com/2020/06/improving-speech-representations-and.html", "abstract": "", "date": "\nThursday, June 18, 2020\n"},
{"website": "Google-AI", "title": "\nUsing Selective Attention in Reinforcement Learning Agents\n", "author": ["Posted by Yujin Tang, Research Software Engineer and David Ha, Staff Research Scientist, Google Research, Tokyo"], "link": "http://ai.googleblog.com/2020/06/using-selective-attention-in.html", "abstract": "", "date": "\nThursday, June 18, 2020\n"},
{"website": "Google-AI", "title": "\nMachine Learning-based Damage Assessment for Disaster Relief\n", "author": ["Posted by Joseph Xu, Senior Software Engineer and Pranav Khaitan, Engineering Lead, Google Research"], "link": "http://ai.googleblog.com/2020/06/machine-learning-based-damage.html", "abstract": "", "date": "\nTuesday, June 16, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at CVPR 2020\n", "author": ["Posted by Emily Knapp, Program Manager and Benjamin H\u00fctteroth, Program Specialist"], "link": "http://ai.googleblog.com/2020/06/google-at-cvpr-2020.html", "abstract": "", "date": "\nMonday, June 15, 2020\n"},
{"website": "Google-AI", "title": "\nExtracting Structured Data from Templatic Documents\n", "author": ["Posted by Sandeep Tata, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/06/extracting-structured-data-from.html", "abstract": "", "date": "\nFriday, June 12, 2020\n"},
{"website": "Google-AI", "title": "\nUnlocking the \"Chemome\" with DNA-Encoded Chemistry and Machine Learning\n", "author": ["Posted by Patrick Riley, Principal Engineer, Accelerated Science Team, Google Research"], "link": "http://ai.googleblog.com/2020/06/unlocking-chemome-with-dna-encoded.html", "abstract": "", "date": "\nThursday, June 11, 2020\n"},
{"website": "Google-AI", "title": "\nPEGASUS: A State-of-the-Art Model for Abstractive Text Summarization\n", "author": ["Posted by Peter J. Liu and Yao Zhao, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "abstract": "", "date": "\nTuesday, June 9, 2020\n"},
{"website": "Google-AI", "title": "\nRecent Advances in Google Translate\n", "author": ["Posted by Isaac Caswell and Bowen Liang, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html", "abstract": "", "date": "\nMonday, June 8, 2020\n"},
{"website": "Google-AI", "title": "\nDADS: Unsupervised Reinforcement Learning for Skill Discovery\n", "author": ["Posted by Archit Sharma, AI Resident, Google Research"], "link": "http://ai.googleblog.com/2020/05/dads-unsupervised-reinforcement.html", "abstract": "", "date": "\nFriday, May 29, 2020\n"},
{"website": "Google-AI", "title": "\nFederated Analytics: Collaborative Data Science without Data Collection\n", "author": ["Posted by Daniel Ramage, Research Scientist and Stefano Mazzocchi, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html", "abstract": "", "date": "\nWednesday, May 27, 2020\n"},
{"website": "Google-AI", "title": "\nEvaluating Natural Language Generation with BLEURT\n", "author": ["Posted by Thibault Sellam, Software Engineer and Ankur P. Parikh, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html", "abstract": "", "date": "\nTuesday, May 26, 2020\n"},
{"website": "Google-AI", "title": "\nOpen-Sourcing BiT: Exploring Large-Scale Pre-training for Computer Vision\n", "author": ["Posted by Lucas Beyer and Alexander Kolesnikov, Research Engineers, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2020/05/open-sourcing-bit-exploring-large-scale.html", "abstract": "", "date": "\nThursday, May 21, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 7th Fine-Grained Visual Categorization Workshop\n", "author": ["Posted by Christine Kaeser-Chen, Software Engineer and Serge Belongie, Visiting Faculty, Google Research"], "link": "http://ai.googleblog.com/2020/05/announcing-7th-fine-grained-visual.html", "abstract": "", "date": "\nWednesday, May 20, 2020\n"},
{"website": "Google-AI", "title": "\nEnabling E-Textile Microinteractions: Gestures and Light through Helical Structures\n", "author": ["Posted by Alex Olwal, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/05/enabling-e-textile-microinteractions.html", "abstract": "", "date": "\nFriday, May 15, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing Meta-Dataset: A Dataset of Datasets for Few-Shot Learning\n", "author": ["Posted by Eleni Triantafillou, Student Researcher and Vincent Dumoulin, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/05/announcing-meta-dataset-dataset-of.html", "abstract": "", "date": "\nWednesday, May 13, 2020\n"},
{"website": "Google-AI", "title": "\nSpeeding Up Neural Network Training with Data Echoing\n", "author": ["Posted by Dami Choi, Student Researcher and George Dahl, Senior Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/05/speeding-up-neural-network-training.html", "abstract": "", "date": "\nTuesday, May 12, 2020\n"},
{"website": "Google-AI", "title": "\nAgile and Intelligent Locomotion via Deep Reinforcement Learning\n", "author": ["Posted by Yuxiang Yang and Deepali Jain, AI Residents, Robotics at Google"], "link": "http://ai.googleblog.com/2020/05/agile-and-intelligent-locomotion-via.html", "abstract": "", "date": "\nWednesday, May 6, 2020\n"},
{"website": "Google-AI", "title": "\nUnderstanding the Shape of Large-Scale Data\n", "author": ["Posted by Anton Tsitsulin, Research Intern and Bryan Perozzi, Senior Research Scientist, Graph Mining Team, Google Research"], "link": "http://ai.googleblog.com/2020/05/understanding-shape-of-large-scale-data.html", "abstract": "", "date": "\nTuesday, May 5, 2020\n"},
{"website": "Google-AI", "title": "\nAn NLU-Powered Tool to Explore COVID-19 Scientific Literature\n", "author": ["Posted by Keith Hall, Research Scientist, Natural Language Understanding, Google Research"], "link": "http://ai.googleblog.com/2020/05/an-nlu-powered-tool-to-explore-covid-19.html", "abstract": "", "date": "\nMonday, May 4, 2020\n"},
{"website": "Google-AI", "title": "\nUsing Neural Networks to Find Answers in Tables\n", "author": ["Posted by Thomas M\u00fcller, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html", "abstract": "", "date": "\nThursday, April 30, 2020\n"},
{"website": "Google-AI", "title": "\nApplying Machine Learning to\u2026..Yeast?\n", "author": ["Posted by Ted Baltz, Senior Staff Software Engineer, Google Research, Accelerated Science Team"], "link": "http://ai.googleblog.com/2020/04/applying-machine-learning-toyeast.html", "abstract": "", "date": "\nWednesday, April 29, 2020\n"},
{"website": "Google-AI", "title": "\nYet More Google Compute Cluster Trace Data\n", "author": ["Posted by John Wilkes, Principal Software Engineer, Google Cloud"], "link": "http://ai.googleblog.com/2020/04/yet-more-google-compute-cluster-trace.html", "abstract": "", "date": "\nTuesday, April 28, 2020\n"},
{"website": "Google-AI", "title": "\nOptimizing Multiple Loss Functions with Loss-Conditional Training\n", "author": ["Posted by Alexey Dosovitskiy, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/04/optimizing-multiple-loss-functions-with.html", "abstract": "", "date": "\nMonday, April 27, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle at ICLR 2020\n", "author": ["Posted by Christian Howard, Google Research"], "link": "http://ai.googleblog.com/2020/04/google-at-iclr-2020.html", "abstract": "", "date": "\nSunday, April 26, 2020\n"},
{"website": "Google-AI", "title": "\nChip Design with Deep Reinforcement Learning\n", "author": ["Posted by Anna Goldie, Senior Software Engineer and Azalia Mirhoseini, Senior Research Scientist, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html", "abstract": "", "date": "\nThursday, April 23, 2020\n"},
{"website": "Google-AI", "title": "\nA Scalable Approach to Reducing Gender Bias in Google Translate\n", "author": ["Posted by Melvin Johnson, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html", "abstract": "", "date": "\nWednesday, April 22, 2020\n"},
{"website": "Google-AI", "title": "\nExploring Evolutionary Meta-Learning in Robotics\n", "author": ["Posted by Xingyou (Richard) Song, Software Engineer and Yuxiang Yang, AI Resident, Robotics at Google"], "link": "http://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html", "abstract": "", "date": "\nTuesday, April 21, 2020\n"},
{"website": "Google-AI", "title": "\nOff-Policy Estimation for Infinite-Horizon Reinforcement Learning\n", "author": ["Posted by Ali Mousavi, AI Resident and Lihong Li, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/04/off-policy-estimation-for-infinite.html", "abstract": "", "date": "\nFriday, April 17, 2020\n"},
{"website": "Google-AI", "title": "\nEfficientDet: Towards Scalable and Efficient Object Detection\n", "author": ["Posted by Mingxing Tan, Software Engineer and Adams Yu, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/04/efficientdet-towards-scalable-and.html", "abstract": "", "date": "\nWednesday, April 15, 2020\n"},
{"website": "Google-AI", "title": "\nAn Optimistic Perspective on Offline Reinforcement Learning\n", "author": ["Posted by Rishabh Agarwal, AI Resident and Mohammad Norouzi, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/04/an-optimistic-perspective-on-offline.html", "abstract": "", "date": "\nTuesday, April 14, 2020\n"},
{"website": "Google-AI", "title": "\nXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\n", "author": ["Posted by Melvin Johnson, Senior Software Engineer, Google Research and Sebastian Ruder, Research Scientist, DeepMind"], "link": "http://ai.googleblog.com/2020/04/xtreme-massively-multilingual-multi.html", "abstract": "", "date": "\nMonday, April 13, 2020\n"},
{"website": "Google-AI", "title": "\nuDepth: Real-time 3D Depth Sensing on the Pixel 4\n", "author": ["Posted by Michael Schoenberg, uDepth Software Lead and Adarsh Kowdle, uDepth Hardware/Systems Lead, Google Research"], "link": "http://ai.googleblog.com/2020/04/udepth-real-time-3d-depth-sensing-on.html", "abstract": "", "date": "\nFriday, April 10, 2020\n"},
{"website": "Google-AI", "title": "\nAdvancing Self-Supervised and Semi-Supervised Learning with SimCLR\n", "author": ["Posted by Ting Chen, Research Scientist and Geoffrey Hinton, VP & Engineering Fellow, Google Research"], "link": "http://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html", "abstract": "", "date": "\nWednesday, April 8, 2020\n"},
{"website": "Google-AI", "title": "\nExploring Nature-Inspired Robot Agility\n", "author": ["Posted by Xue Bin (Jason) Peng, Student Researcher and Sehoon Ha, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2020/04/exploring-nature-inspired-robot-agility.html", "abstract": "", "date": "\nFriday, April 3, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2020 Image Matching Benchmark and Challenge\n", "author": ["Posted by Eduard Trulls, Research Scientist, Google Maps"], "link": "http://ai.googleblog.com/2020/04/announcing-2020-image-matching.html", "abstract": "", "date": "\nThursday, April 2, 2020\n"},
{"website": "Google-AI", "title": "\nA Step Towards Protecting Patients from Medication Errors\n", "author": ["Posted by Kathryn Rough, Research Scientist and Alvin Rajkomar, MD, Google Health"], "link": "http://ai.googleblog.com/2020/04/a-step-towards-protecting-patients-from.html", "abstract": "", "date": "\nThursday, April 2, 2020\n"},
{"website": "Google-AI", "title": "\nImproving Audio Quality in Duo with WaveNetEQ\n", "author": ["Posted by Pablo Barrera, Software Engineer, Google Research and Florian Stimberg, Research Engineer, DeepMind"], "link": "http://ai.googleblog.com/2020/04/improving-audio-quality-in-duo-with.html", "abstract": "", "date": "\nWednesday, April 1, 2020\n"},
{"website": "Google-AI", "title": "\nExploring New Ways to Support Faculty Research\n", "author": ["Posted by Maggie Johnson, VP, Google Research"], "link": "http://ai.googleblog.com/2020/03/exploring-new-ways-to-support-faculty.html", "abstract": "", "date": "\nThursday, March 26, 2020\n"},
{"website": "Google-AI", "title": "\nA Neural Weather Model for Eight-Hour Precipitation Forecasting\n", "author": ["Posted by Nal Kalchbrenner and Casper S\u00f8nderby, Research Scientists, Google Research, Amsterdam"], "link": "http://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html", "abstract": "", "date": "\nWednesday, March 25, 2020\n"},
{"website": "Google-AI", "title": "\nMassively Scaling Reinforcement Learning with SEED RL\n", "author": ["Posted by Lasse Espeholt, Research Engineer, Google Research, Amsterdam"], "link": "http://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html", "abstract": "", "date": "\nMonday, March 23, 2020\n"},
{"website": "Google-AI", "title": "\nVisual Transfer Learning for Robotic Manipulation\n", "author": ["Posted by Yen-Chen Lin, Research Intern and  Andy Zeng, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2020/03/visual-transfer-learning-for-robotic.html", "abstract": "", "date": "\nFriday, March 20, 2020\n"},
{"website": "Google-AI", "title": "\nIntroducing Dreamer: Scalable Reinforcement Learning Using World Models\n", "author": ["Posted by Danijar Hafner, Student Researcher, Google Research"], "link": "http://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html", "abstract": "", "date": "\nWednesday, March 18, 2020\n"},
{"website": "Google-AI", "title": "\nFast and Easy Infinitely Wide Networks with Neural Tangents\n", "author": ["Posted by Samuel S. Schoenholz, Senior Research Scientist and Roman Novak, Research Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/03/fast-and-easy-infinitely-wide-networks.html", "abstract": "", "date": "\nFriday, March 13, 2020\n"},
{"website": "Google-AI", "title": "\nSoli Radar-Based Perception and Interaction in Pixel 4\n", "author": ["Posted by Jaime Lien, Research Engineer and Nicholas Gillian, Software Engineer, Google Advanced Technology and Projects"], "link": "http://ai.googleblog.com/2020/03/soli-radar-based-perception-and.html", "abstract": "", "date": "\nThursday, March 12, 2020\n"},
{"website": "Google-AI", "title": "\nReal-Time 3D Object Detection on Mobile Devices with MediaPipe\n", "author": ["Posted by Adel Ahmadyan and Tingbo Hou, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html", "abstract": "", "date": "\nWednesday, March 11, 2020\n"},
{"website": "Google-AI", "title": "\nMore Efficient NLP Model Pre-training with ELECTRA\n", "author": ["Posted by Kevin Clark, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html", "abstract": "", "date": "\nTuesday, March 10, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing TensorFlow Quantum: An Open Source Library for Quantum Machine Learning\n", "author": ["Posted by Alan Ho, Product Lead and Masoud Mohseni, Technical Lead, Google Research"], "link": "http://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html", "abstract": "", "date": "\nMonday, March 9, 2020\n"},
{"website": "Google-AI", "title": "\nMeasuring Compositional Generalization\n", "author": ["Posted by Marc van Zee, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/03/measuring-compositional-generalization.html", "abstract": "", "date": "\nFriday, March 6, 2020\n"},
{"website": "Google-AI", "title": "\nToward Human-Centered Design for ML Frameworks\n", "author": ["Posted by Carrie J. Cai, Senior Research Scientist, Google Research and Philip J. Guo, Assistant Professor, UC San Diego"], "link": "http://ai.googleblog.com/2020/03/toward-human-centered-design-for-ml.html", "abstract": "", "date": "\nTuesday, March 3, 2020\n"},
{"website": "Google-AI", "title": "\nUltra-High Resolution Image Analysis with Mesh-TensorFlow\n", "author": ["Posted by Le Hou and Youlong Cheng, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/02/ultra-high-resolution-image-analysis.html", "abstract": "", "date": "\nFriday, February 28, 2020\n"},
{"website": "Google-AI", "title": "\nOpen Images V6 \u2014 Now Featuring Localized Narratives\n", "author": ["Posted by Jordi Pont-Tuset, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html", "abstract": "", "date": "\nWednesday, February 26, 2020\n"},
{"website": "Google-AI", "title": "\nEnhancing the Research Community\u2019s Access to Street View Panoramas for Language Grounding Tasks\n", "author": ["Posted by Harsh Mehta, Software Engineer and Jason Baldridge, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/02/enhancing-research-communitys-access-to.html", "abstract": "", "date": "\nTuesday, February 25, 2020\n"},
{"website": "Google-AI", "title": "\nExploring Transfer Learning with T5: the Text-To-Text Transfer Transformer\n", "author": ["Posted by Adam Roberts, Staff Software Engineer and Colin Raffel, Senior Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "abstract": "", "date": "\nMonday, February 24, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2019 Google Faculty Research Award Recipients\n", "author": ["Posted by Maggie Johnson, VP and Negar Saei, Program Manager, Google Research"], "link": "http://ai.googleblog.com/2020/02/announcing-2019-google-faculty-research.html", "abstract": "", "date": "\nMonday, February 24, 2020\n"},
{"website": "Google-AI", "title": "\nSetting Fairness Goals with the TensorFlow Constrained Optimization Library\n", "author": ["Posted by Andrew Zaldivar, Responsible AI Advocate, Google Research, on behalf of the TFCO Team"], "link": "http://ai.googleblog.com/2020/02/setting-fairness-goals-with-tensorflow.html", "abstract": "", "date": "\nFriday, February 21, 2020\n"},
{"website": "Google-AI", "title": "\nGenerating Diverse Synthetic Medical Image Data for Training Machine Learning Models\n", "author": ["Posted by Timo Kohlberger and Yuan Liu, Software Engineers, Google Health"], "link": "http://ai.googleblog.com/2020/02/generating-diverse-synthetic-medical.html", "abstract": "", "date": "\nWednesday, February 19, 2020\n"},
{"website": "Google-AI", "title": "\nAutoFlip: An Open Source Framework for Intelligent Video Reframing\n", "author": ["Posted by Nathan Frey, Senior Software Engineer, Google Research, Los Angeles and Zheng Sun, Senior Software Engineer, Google Research, Mountain View"], "link": "http://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html", "abstract": "", "date": "\nThursday, February 13, 2020\n"},
{"website": "Google-AI", "title": "\nLearning to See Transparent Objects\n", "author": ["Posted by Shreeyak Sajjan, Research Engineer, Synthesis AI and Andy Zeng, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html", "abstract": "", "date": "\nWednesday, February 12, 2020\n"},
{"website": "Google-AI", "title": "\nTyDi QA: A Multilingual Question Answering Benchmark\n", "author": ["Posted by Jonathan Clark, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html", "abstract": "", "date": "\nThursday, February 6, 2020\n"},
{"website": "Google-AI", "title": "\nML-fairness-gym: A Tool for Exploring Long-Term Impacts of Machine Learning Systems\n", "author": ["Posted by Hansa Srinivasan, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/02/ml-fairness-gym-tool-for-exploring-long.html", "abstract": "", "date": "\nWednesday, February 5, 2020\n"},
{"website": "Google-AI", "title": "\nEncode, Tag and Realize: A Controllable and Efficient Approach for Text Generation\n", "author": ["Posted by Eric Malmi and Sebastian Krause, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html", "abstract": "", "date": "\nFriday, January 31, 2020\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Third Workshop and Challenge on Learned Image Compression\n", "author": ["Posted by Nick Johnston, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/01/announcing-third-workshop-and-challenge.html", "abstract": "", "date": "\nThursday, January 30, 2020\n"},
{"website": "Google-AI", "title": "\nTowards a Conversational Agent that Can Chat About\u2026Anything\n", "author": ["Posted by Daniel Adiwardana, Senior Research Engineer, and Thang Luong, Senior Research Scientist, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html", "abstract": "", "date": "\nTuesday, January 28, 2020\n"},
{"website": "Google-AI", "title": "\nReleasing the Drosophila Hemibrain Connectome \u2014 The Largest Synapse-Resolution Map of Brain Connectivity\n", "author": ["Posted by Michal Januszewski, Software Engineer and Viren Jain, Research Scientist and Technical Lead, Connectomics at Google"], "link": "http://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html", "abstract": "", "date": "\nWednesday, January 22, 2020\n"},
{"website": "Google-AI", "title": "\nReformer: The Efficient Transformer\n", "author": ["Posted by Nikita Kitaev, Student Researcher, UC Berkeley and \u0141ukasz Kaiser, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2020/01/reformer-efficient-transformer.html", "abstract": "", "date": "\nThursday, January 16, 2020\n"},
{"website": "Google-AI", "title": "\nCan You Trust Your Model\u2019s Uncertainty?\n", "author": ["Posted by Jasper Snoek, Research Scientist and Zachary Nado, Research Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/01/can-you-trust-your-models-uncertainty.html", "abstract": "", "date": "\nWednesday, January 15, 2020\n"},
{"website": "Google-AI", "title": "\nUsing Machine Learning to \u201cNowcast\u201d Precipitation in High Resolution\n", "author": ["Posted by Jason Hickey, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2020/01/using-machine-learning-to-nowcast.html", "abstract": "", "date": "\nMonday, January 13, 2020\n"},
{"website": "Google-AI", "title": "\nGoogle Research: Looking Back at 2019, and Forward to 2020 and Beyond\n", "author": ["Posted by Jeff Dean, Senior Fellow and SVP of Google Research and Health, on behalf of the entire Google Research community"], "link": "http://ai.googleblog.com/2020/01/google-research-looking-back-at-2019.html", "abstract": "", "date": "\nThursday, January 9, 2020\n"},
{"website": "Google-AI", "title": "\nALBERT: A Lite BERT for Self-Supervised Learning of Language Representations\n", "author": ["Posted by Radu Soricut and Zhenzhong Lan, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html", "abstract": "", "date": "\nFriday, December 20, 2019\n"},
{"website": "Google-AI", "title": "\nThe On-Device Machine Learning Behind Recorder\n", "author": ["Posted by Itay Inbar and Nir Shemy, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/12/the-on-device-machine-learning-behind.html", "abstract": "", "date": "\nWednesday, December 18, 2019\n"},
{"website": "Google-AI", "title": "\nImproving Out-of-Distribution Detection in Machine Learning Models\n", "author": ["Posted by Jie Ren, Research Scientist, Google Research and Balaji Lakshminarayanan, Research Scientist, DeepMind"], "link": "http://ai.googleblog.com/2019/12/improving-out-of-distribution-detection.html", "abstract": "", "date": "\nTuesday, December 17, 2019\n"},
{"website": "Google-AI", "title": "\nImprovements to Portrait Mode on the Google Pixel 4 and Pixel 4 XL\n", "author": ["Posted by Neal Wadhwa, Software Engineer and Yinda Zhang, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/12/improvements-to-portrait-mode-on-google.html", "abstract": "", "date": "\nMonday, December 16, 2019\n"},
{"website": "Google-AI", "title": "\nFairness Indicators: Scalable Infrastructure for Fair ML Systems\n", "author": ["Posted by Catherina Xu and Tulsee Doshi, Product Managers, Google Research"], "link": "http://ai.googleblog.com/2019/12/fairness-indicators-scalable.html", "abstract": "", "date": "\nWednesday, December 11, 2019\n"},
{"website": "Google-AI", "title": "\nLessons Learned from Developing ML for Healthcare\n", "author": ["Posted by Yun Liu, Research Scientist and Po-Hsuan Cameron Chen, Research Engineer, Google Health"], "link": "http://ai.googleblog.com/2019/12/lessons-learned-from-developing-ml-for.html", "abstract": "", "date": "\nTuesday, December 10, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at NeurIPS 2019\n", "author": ["Posted by Andrew Helton, Editor, Google Research Communications"], "link": "http://ai.googleblog.com/2019/12/google-at-neurips-2019.html", "abstract": "", "date": "\nMonday, December 9, 2019\n"},
{"website": "Google-AI", "title": "\nUnderstanding Transfer Learning for Medical Imaging\n", "author": ["Posted by Maithra Raghu and Chiyuan Zhang, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2019/12/understanding-transfer-learning-for.html", "abstract": "", "date": "\nFriday, December 6, 2019\n"},
{"website": "Google-AI", "title": "\nDeveloping Deep Learning Models for Chest X-rays with Adjudicated Image Labels\n", "author": ["Posted by Dave Steiner, MD, Research Scientist and Shravya Shetty, Technical Lead, Google Health"], "link": "http://ai.googleblog.com/2019/12/developing-deep-learning-models-for.html", "abstract": "", "date": "\nTuesday, December 3, 2019\n"},
{"website": "Google-AI", "title": "\nAstrophotography with Night Sight on Pixel Phones\n", "author": ["Posted by Florian Kainz and  Kiran Murthy, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/11/astrophotography-with-night-sight-on.html", "abstract": "", "date": "\nTuesday, November 26, 2019\n"},
{"website": "Google-AI", "title": "\nRecSim: A Configurable Simulation Platform for Recommender Systems\n", "author": ["Posted by Martin Mladenov, Research Scientist and Chih-wei Hsu, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/11/recsim-configurable-simulation-platform.html", "abstract": "", "date": "\nTuesday, November 19, 2019\n"},
{"website": "Google-AI", "title": "\nNew Solutions for Quantum Gravity with TensorFlow\n", "author": ["Posted by Thomas Fischbacher, Software Engineer, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/11/new-solutions-for-quantum-gravity-with.html", "abstract": "", "date": "\nFriday, November 15, 2019\n"},
{"website": "Google-AI", "title": "\nSPICE: Self-Supervised Pitch Estimation\n", "author": ["Posted by Marco Tagliasacchi, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/11/spice-self-supervised-pitch-estimation.html", "abstract": "", "date": "\nThursday, November 14, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing the Next Generation of On-Device Vision Models: MobileNetV3 and MobileNetEdgeTPU\n", "author": ["Posted by Andrew Howard, Software Engineer and Suyog Gupta, Silicon Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html", "abstract": "", "date": "\nWednesday, November 13, 2019\n"},
{"website": "Google-AI", "title": "\nNew Insights into Human Mobility with Privacy Preserving Aggregation\n", "author": ["Posted by Adam Sadilek, Software Engineer and Xerxes Dotiwalla, Product Manager, Google Research"], "link": "http://ai.googleblog.com/2019/11/new-insights-into-human-mobility-with.html", "abstract": "", "date": "\nTuesday, November 12, 2019\n"},
{"website": "Google-AI", "title": "\nHighlights from the 3rd Cohort of the Google AI Residency Program\n", "author": ["Posted by Katie Meckley, Program Manager, Google AI Residency"], "link": "http://ai.googleblog.com/2019/11/highlights-from-2019-google-ai.html", "abstract": "", "date": "\nThursday, November 7, 2019\n"},
{"website": "Google-AI", "title": "\nThe Visual Task Adaptation Benchmark\n", "author": ["Posted by Neil Houlsby, Research Scientist and Xiaohua Zhai, Research Engineer, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html", "abstract": "", "date": "\nWednesday, November 6, 2019\n"},
{"website": "Google-AI", "title": "\nLearning to Assemble and to Generalize from Self-Supervised Disassembly\n", "author": ["Posted by Kevin Zakka, Research Intern and Andy Zeng, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2019/10/learning-to-assemble-and-to-generalize.html", "abstract": "", "date": "\nThursday, October 31, 2019\n"},
{"website": "Google-AI", "title": "\nOn-Device Captioning with Live Caption\n", "author": ["Posted by Michelle Tadmor-Ramanovich and Nadav Bar, Senior Software Engineers, Google Research, Tel-Aviv"], "link": "http://ai.googleblog.com/2019/10/on-device-captioning-with-live-caption.html", "abstract": "Recently we introduced , a new Android feature that automatically captions media playing on your phone. The captioning happens in real time, completely on-device, without using network resources, thus preserving privacy and lowering latency. The feature is currently available on Pixel 4 and Pixel 4 XL, will roll out to Pixel 3 models later this year, and will be more widely available on other Android devices soon.", "date": "\nTuesday, October 29, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing the Schema-Guided Dialogue Dataset for Conversational Assistants\n", "author": ["Posted by Abhinav Rastogi, Software Engineer and Pranav Khaitan, Engineering Lead, Google Research"], "link": "http://ai.googleblog.com/2019/10/introducing-schema-guided-dialogue.html", "abstract": "", "date": "\nMonday, October 28, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at ICCV 2019\n", "author": ["Posted by Andrew Helton, Editor, Google Research Communications"], "link": "http://ai.googleblog.com/2019/10/google-at-iccv-2019.html", "abstract": "", "date": "\nMonday, October 28, 2019\n"},
{"website": "Google-AI", "title": "\nA New Workflow for Collaborative Machine Learning Research in Biodiversity\n", "author": ["Posted by Serge Belongie, Visiting Faculty and Hartwig Adam, Engineering Director, Google Research"], "link": "http://ai.googleblog.com/2019/10/a-new-workflow-for-collaborative.html", "abstract": "", "date": "\nFriday, October 25, 2019\n"},
{"website": "Google-AI", "title": "\nLearning to Smell: Using Deep Learning to Predict the Olfactory Properties of Molecules\n", "author": ["Posted by Alexander B Wiltschko, Senior Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/10/learning-to-smell-using-deep-learning.html", "abstract": "", "date": "\nThursday, October 24, 2019\n"},
{"website": "Google-AI", "title": "\nQuantum Supremacy Using a Programmable Superconducting Processor\n", "author": ["Posted by John Martinis, Chief Scientist Quantum Hardware and Sergio Boixo, Chief Scientist Quantum Computing Theory, Google AI Quantum"], "link": "http://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html", "abstract": "", "date": "\nWednesday, October 23, 2019\n"},
{"website": "Google-AI", "title": "\nAudio and Visual Quality Measurement Using Fr\u00e9chet Distance\n", "author": ["Posted by Kevin Kilgour, Software Engineer and Thomas Unterthiner, Research Software Engineer, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/10/audio-and-visual-quality-measurement.html", "abstract": "", "date": "\nTuesday, October 22, 2019\n"},
{"website": "Google-AI", "title": "\nVideo Architecture Search\n", "author": ["Posted by Michael S. Ryoo, Research Scientist and AJ Piergiovanni, Student Researcher, Robotics at Google"], "link": "http://ai.googleblog.com/2019/10/video-architecture-search.html", "abstract": "", "date": "\nThursday, October 17, 2019\n"},
{"website": "Google-AI", "title": "\nExploring Massively Multilingual, Massive Neural Machine Translation\n", "author": ["Posted by Ankur Bapna, Software Engineer and Orhan Firat, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/10/exploring-massively-multilingual.html", "abstract": "", "date": "\nFriday, October 11, 2019\n"},
{"website": "Google-AI", "title": "\nROBEL: Robotics Benchmarks for Learning with Low-Cost Robots\n", "author": ["Posted by Michael Ahn, Software Engineer and Vikash Kumar, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2019/10/robel-robotics-benchmarks-for-learning.html", "abstract": "", "date": "\nWednesday, October 9, 2019\n"},
{"website": "Google-AI", "title": "\nImproving Quantum Computation with Classical Machine Learning\n", "author": ["Posted by Murphy Yuezhen Niu and Sergio Boixo, Research Scientists\u00a0"], "link": "http://ai.googleblog.com/2019/10/improving-quantum-computation-with.html", "abstract": "", "date": "\nThursday, October 3, 2019\n"},
{"website": "Google-AI", "title": "\nReleasing PAWS and PAWS-X: Two New Datasets to Improve Natural Language Understanding Models\n", "author": ["Posted by Yuan Zhang, Research Scientist and Yinfei Yang, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/10/releasing-paws-and-paws-x-two-new.html", "abstract": "", "date": "\nWednesday, October 2, 2019\n"},
{"website": "Google-AI", "title": "\nLarge-Scale Multilingual Speech Recognition with a Streaming End-to-End Model\n", "author": ["Posted by Arindrima Datta and Anjuli Kannan, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/09/large-scale-multilingual-speech.html", "abstract": "", "date": "\nMonday, September 30, 2019\n"},
{"website": "Google-AI", "title": "\nContributing Data to Deepfake Detection Research\n", "author": ["Posted by Nick Dufour, Google Research and Andrew Gully, Jigsaw"], "link": "http://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html", "abstract": "", "date": "\nTuesday, September 24, 2019\n"},
{"website": "Google-AI", "title": "\nAn Inside Look at Flood Forecasting\n", "author": ["Sella Nevo, Senior Software Engineer, Google Research, Tel Aviv"], "link": "http://ai.googleblog.com/2019/09/an-inside-look-at-flood-forecasting.html", "abstract": "", "date": "\nWednesday, September 18, 2019\n"},
{"website": "Google-AI", "title": "\nProject Ihmehimmeli: Temporal Coding in Spiking Neural Networks\n", "author": ["Posted by Iulia-Maria Com\u0219a and Krzysztof Potempa, Research Engineers, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/09/project-ihmehimmeli-temporal-coding-in.html", "abstract": "", "date": "\nWednesday, September 18, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at Interspeech 2019\n", "author": ["Andrew Helton, Editor, Google Research Communications"], "link": "http://ai.googleblog.com/2019/09/google-at-interspeech-2019.html", "abstract": "", "date": "\nSunday, September 15, 2019\n"},
{"website": "Google-AI", "title": "\nUsing Deep Learning to Inform Differential Diagnoses of Skin Diseases\n", "author": ["Posted by Yuan Liu, PhD, Software Engineer and Peggy Bui, MD, Technical Program Manager, Google Health"], "link": "http://ai.googleblog.com/2019/09/using-deep-learning-to-inform.html", "abstract": "", "date": "\nThursday, September 12, 2019\n"},
{"website": "Google-AI", "title": "\nLearning Cross-Modal Temporal Representations from Unlabeled Videos\n", "author": ["Posted by Chen Sun and Cordelia Schmid, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html", "abstract": "", "date": "\nWednesday, September 11, 2019\n"},
{"website": "Google-AI", "title": "\nRecursive Sketches for Modular Deep Learning\n", "author": ["Posted by Badih Ghazi and  Joshua R. Wang, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2019/09/recursive-sketches-for-modular-deep.html", "abstract": "", "date": "\nTuesday, September 10, 2019\n"},
{"website": "Google-AI", "title": "\nAssessing the Quality of Long-Form Synthesized Speech\n", "author": ["Posted by Tom Kenter, Google Research, London"], "link": "http://ai.googleblog.com/2019/09/assessing-quality-of-long-form.html", "abstract": "", "date": "\nMonday, September 9, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing Two New Natural Language Dialog Datasets\n", "author": ["Posted by Bill Byrne and Filip Radlinski, Research Scientists, Google Research"], "link": "http://ai.googleblog.com/2019/09/announcing-two-new-natural-language.html", "abstract": "", "date": "\nFriday, September 6, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncement of the 2019 Fellowship Awardees and Highlights from the Google PhD Fellowship Summit\n", "author": ["Posted by Susie Kim, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2019/09/announcement-of-2019-fellowship.html", "abstract": "", "date": "\nThursday, September 5, 2019\n"},
{"website": "Google-AI", "title": "\nGiving Lens New Reading Capabilities in Google Go\n", "author": ["Posted by Rajan Patel, Director, Augmented Reality"], "link": "http://ai.googleblog.com/2019/09/giving-lens-new-reading-capabilities-in.html", "abstract": "", "date": "\nWednesday, September 4, 2019\n"},
{"website": "Google-AI", "title": "\nExploring Weight Agnostic Neural Networks\n", "author": ["Posted by Adam Gaier, Student Researcher and David Ha, Staff Research Scientist, Google Research, Tokyo"], "link": "http://ai.googleblog.com/2019/08/exploring-weight-agnostic-neural.html", "abstract": "", "date": "\nTuesday, August 27, 2019\n"},
{"website": "Google-AI", "title": "\nBi-Tempered Logistic Loss for Training Neural Nets with Noisy Data\n", "author": ["Posted by Ehsan Amid, Student Researcher and Rohan Anil, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/08/bi-tempered-logistic-loss-for-training.html", "abstract": "", "date": "\nMonday, August 26, 2019\n"},
{"website": "Google-AI", "title": "\nTurbo, An Improved Rainbow Colormap for Visualization\n", "author": ["Posted by Anton Mikhailov, Senior Software Engineer, Daydream"], "link": "http://ai.googleblog.com/2019/08/turbo-improved-rainbow-colormap-for.html", "abstract": "", "date": "\nTuesday, August 20, 2019\n"},
{"website": "Google-AI", "title": "\nOn-Device, Real-Time Hand Tracking with MediaPipe\n", "author": ["Posted by Valentin Bazarevsky and Fan Zhang, Research Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html", "abstract": "", "date": "\nMonday, August 19, 2019\n"},
{"website": "Google-AI", "title": "\nJoint Speech Recognition and Speaker Diarization via Sequence Transduction\n", "author": ["Posted by Laurent El Shafey, Software Engineer and Izhak Shafran, Research Scientist, Google Health"], "link": "http://ai.googleblog.com/2019/08/joint-speech-recognition-and-speaker.html", "abstract": "", "date": "\nFriday, August 16, 2019\n"},
{"website": "Google-AI", "title": "\nProject Euphonia\u2019s Personalized Speech Recognition for Non-Standard Speech\n", "author": ["Posted by Joel Shor and Dotan Emanuel, Research Engineers, Google Research, Tel Aviv"], "link": "http://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html", "abstract": "", "date": "\nTuesday, August 13, 2019\n"},
{"website": "Google-AI", "title": "\nVideo Understanding Using Temporal Cycle-Consistency Learning\n", "author": ["Posted by Debidatta Dwibedi, Research Associate, Google Research"], "link": "http://ai.googleblog.com/2019/08/video-understanding-using-temporal.html", "abstract": "", "date": "\nThursday, August 8, 2019\n"},
{"website": "Google-AI", "title": "\nEfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML\n", "author": ["Posted by Suyog Gupta, Machine Learning Accelerator Architect and Mingxing Tan, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html", "abstract": "", "date": "\nTuesday, August 6, 2019\n"},
{"website": "Google-AI", "title": "\nAn Interactive, Automated 3D Reconstruction of a Fly Brain\n", "author": ["Posted by Peter H. Li, Research Scientist and Jeremy Maitin-Shepard, Software Engineer, Connectomics at Google"], "link": "http://ai.googleblog.com/2019/08/an-interactive-automated-3d.html", "abstract": "", "date": "\nMonday, August 5, 2019\n"},
{"website": "Google-AI", "title": "\nRobust Neural Machine Translation\n", "author": ["Posted by Yong Cheng, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/07/robust-neural-machine-translation.html", "abstract": "", "date": "\nMonday, July 29, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at ACL 2019\n", "author": ["Andrew Helton, Editor, Google Research Communications"], "link": "http://ai.googleblog.com/2019/07/google-at-acl-2019.html", "abstract": "", "date": "\nMonday, July 29, 2019\n"},
{"website": "Google-AI", "title": "\nLearning Better Simulation Methods for Partial Differential Equations\n", "author": ["Posted by Stephan Hoyer, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/07/learning-better-simulation-methods-for.html", "abstract": "", "date": "\nTuesday, July 23, 2019\n"},
{"website": "Google-AI", "title": "\nBuilding SMILY, a Human-Centric, Similar-Image Search Tool for Pathology\n", "author": ["Posted by Narayan Hegde, Software Engineer, Google Health and Carrie J. Cai, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/07/building-smily-human-centric-similar.html", "abstract": "", "date": "\nFriday, July 19, 2019\n"},
{"website": "Google-AI", "title": "\nParrotron: New Research into Improving Verbal Communication for People with Speech Impairments\n", "author": ["Posted by Fadi Biadsy, Research Scientist and Ron Weiss, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html", "abstract": "", "date": "\nWednesday, July 17, 2019\n"},
{"website": "Google-AI", "title": "\nMultilingual Universal Sentence Encoder for Semantic Retrieval\n", "author": ["Posted by Yinfei Yang and Amin Ahmad, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "abstract": "", "date": "\nFriday, July 12, 2019\n"},
{"website": "Google-AI", "title": "\nAdvancing Semi-supervised Learning with Unsupervised Data Augmentation\n", "author": ["Posted by Qizhe Xie, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team"], "link": "http://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html", "abstract": "", "date": "\nWednesday, July 10, 2019\n"},
{"website": "Google-AI", "title": "\nPredicting the Generalization Gap in Deep Neural Networks\n", "author": ["Posted by Yiding Jiang, Google AI Resident"], "link": "http://ai.googleblog.com/2019/07/predicting-generalization-gap-in-deep.html", "abstract": "", "date": "\nTuesday, July 9, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing the YouTube-8M Segments Dataset\n", "author": ["Posted by Joonseok Lee and Joe Yue-Hei Ng, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2019/06/announcing-youtube-8m-segments-dataset.html", "abstract": "", "date": "\nFriday, June 28, 2019\n"},
{"website": "Google-AI", "title": "\nPredicting Bus Delays with Machine Learning\n", "author": ["Posted by Alex Fabrikant, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2019/06/predicting-bus-delays-with-machine.html", "abstract": "", "date": "\nThursday, June 27, 2019\n"},
{"website": "Google-AI", "title": "\nInnovations in Graph Representation Learning\n", "author": ["Posted by Alessandro Epasto, Senior Research Scientist and Bryan Perozzi, Senior Research Scientist, Graph Mining Team"], "link": "http://ai.googleblog.com/2019/06/innovations-in-graph-representation.html", "abstract": "", "date": "\nTuesday, June 25, 2019\n"},
{"website": "Google-AI", "title": "\nOff-Policy Classification - A New Reinforcement Learning Model Selection Method\n", "author": [], "link": "http://ai.googleblog.com/2019/06/off-policy-classification-new.html", "abstract": "", "date": "\nWednesday, June 19, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at CVPR 2019\n", "author": ["Posted by\u00a0"], "link": "http://ai.googleblog.com/2019/06/google-at-cvpr-2019.html", "abstract": "", "date": "\nMonday, June 17, 2019\n"},
{"website": "Google-AI", "title": "\nApplying AutoML to Transformer Architectures\n", "author": ["Posted by David So, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2019/06/applying-automl-to-transformer.html", "abstract": "", "date": "\nFriday, June 14, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at ICML 2019\n", "author": ["Posted by Andrew Helton, Editor, Google AI Communications"], "link": "http://ai.googleblog.com/2019/06/google-at-icml-2019.html", "abstract": "", "date": "\nMonday, June 10, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing Google Research Football: A Novel Reinforcement Learning Environment\n", "author": ["Posted by Karol Kurach, Research Lead and Olivier Bachem, Research Scientist, Google Research, Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/06/introducing-google-research-football.html", "abstract": "", "date": "\nFriday, June 7, 2019\n"},
{"website": "Google-AI", "title": "\nAn Inside Look at Google Earth Timelapse\n", "author": ["Posted by Paul Dille, Senior Software Developer, Carnegie Mellon University CREATE Lab, and Chris Herwig, Geo Data Engineer, Google Earth Outreach"], "link": "http://ai.googleblog.com/2019/06/an-inside-look-at-google-earth-timelapse.html", "abstract": "", "date": "\nWednesday, June 5, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing TensorNetwork, an Open Source Library for Efficient Tensor Calculations\n", "author": ["Posted by Chase Roberts, Research Engineer, Google AI and Stefan Leichenauer, Research Scientist, X"], "link": "http://ai.googleblog.com/2019/06/introducing-tensornetwork-open-source.html", "abstract": "", "date": "\nTuesday, June 4, 2019\n"},
{"website": "Google-AI", "title": "\nEfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling\n", "author": ["Posted by Mingxing Tan, Staff Software Engineer and Quoc V. Le, Principal Scientist, Google AI"], "link": "http://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html", "abstract": "", "date": "\nWednesday, May 29, 2019\n"},
{"website": "Google-AI", "title": "\nMoving Camera, Moving People: A Deep Learning Approach to Depth Prediction\n", "author": ["Posted by Tali Dekel, Research Scientist and Forrester Cole, Software Engineer, Machine Perception"], "link": "http://ai.googleblog.com/2019/05/moving-camera-moving-people-deep.html", "abstract": "", "date": "\nThursday, May 23, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing Translatotron: An End-to-End Speech-to-Speech Translation Model\n", "author": ["Posted by Ye Jia and Ron Weiss, Software Engineers, Google AI"], "link": "http://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html", "abstract": "", "date": "\nWednesday, May 15, 2019\n"},
{"website": "Google-AI", "title": "\nAn End-to-End AutoML Solution for Tabular Data at KaggleDays\n", "author": ["Posted by Yifeng Lu, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2019/05/an-end-to-end-automl-solution-for.html", "abstract": "", "date": "\nThursday, May 9, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing Open Images V5 and the ICCV 2019 Open Images Challenge\n", "author": ["Posted by Vittorio Ferrari, Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html", "abstract": "", "date": "\nWednesday, May 8, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle at ICLR 2019\n", "author": ["Posted by Andrew Helton, Editor, Google AI Communications"], "link": "http://ai.googleblog.com/2019/05/google-at-iclr-2019.html", "abstract": "", "date": "\nMonday, May 6, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing Google-Landmarks-v2: An Improved Dataset for Landmark Recognition & Retrieval\n", "author": ["Posted by Bingyi Cao and Tobias Weyand, Software Engineers, Google AI"], "link": "http://ai.googleblog.com/2019/05/announcing-google-landmarks-v2-improved.html", "abstract": "", "date": "\nFriday, May 3, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 6th Fine-Grained Visual Categorization Workshop\n", "author": ["Posted by Christine Kaeser-Chen, Software Engineer and Serge Belongie, Visiting Faculty, Google AI"], "link": "http://ai.googleblog.com/2019/04/announcing-6th-fine-grained-visual.html", "abstract": "", "date": "\nMonday, April 29, 2019\n"},
{"website": "Google-AI", "title": "\nEvaluating the Unsupervised Learning of Disentangled Representations\n", "author": ["Posted by Olivier Bachem, Research Scientist, Google AI Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html", "abstract": "", "date": "\nWednesday, April 24, 2019\n"},
{"website": "Google-AI", "title": "\nSpecAugment: A New Data Augmentation Method for Automatic Speech Recognition\n", "author": ["Posted by Daniel S. Park, AI Resident and William Chan, Research Scientist"], "link": "http://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html", "abstract": "", "date": "\nMonday, April 22, 2019\n"},
{"website": "Google-AI", "title": "\nMorphNet: Towards Faster and Smaller Neural Networks\n", "author": ["Posted by Andrew Poon, Senior Software Engineer and Dhyanesh Narayanan, Product Manager, Google AI Perception"], "link": "http://ai.googleblog.com/2019/04/morphnet-towards-faster-and-smaller.html", "abstract": "", "date": "\nWednesday, April 17, 2019\n"},
{"website": "Google-AI", "title": "\nTake Your Best Selfie Automatically, with Photobooth on Pixel 3\n", "author": ["Posted by Navid Shiee, Senior Software Engineer and Aseem Agarwala, Staff Research Scientist, Google AI"], "link": "http://ai.googleblog.com/2019/04/take-your-best-selfie-automatically.html", "abstract": "", "date": "\nTuesday, April 16, 2019\n"},
{"website": "Google-AI", "title": "\nCapturing Special Video Moments with Google Photos\n", "author": ["Posted by Sudheendra Vijayanarasimhan and David Ross, Software Engineers"], "link": "http://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html", "abstract": "", "date": "\nWednesday, April 3, 2019\n"},
{"website": "Google-AI", "title": "\nUsing Deep Learning to Improve Usability on Mobile Devices\n", "author": ["Posted by Yang Li, Research Scientist, Google AI"], "link": "http://ai.googleblog.com/2019/04/using-deep-learning-to-improve.html", "abstract": "", "date": "\nTuesday, April 2, 2019\n"},
{"website": "Google-AI", "title": "\nUnifying Physics and Deep Learning with TossingBot\n", "author": ["Posted by Andy Zeng, Student Researcher, Robotics at Google"], "link": "http://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html", "abstract": "", "date": "\nTuesday, March 26, 2019\n"},
{"website": "Google-AI", "title": "\nSimulated Policy Learning in Video Models\n", "author": ["Posted by \u0141ukasz Kaiser and Dumitru Erhan, Research Scientists, Google AI"], "link": "http://ai.googleblog.com/2019/03/simulated-policy-learning-in-video.html", "abstract": "", "date": "\nMonday, March 25, 2019\n"},
{"website": "Google-AI", "title": "\nReducing the Need for Labeled Data in Generative Adversarial Networks\n", "author": ["Posted by Mario Lu\u010di\u0107, Research Scientist and Marvin Ritter, Software Engineer, Google AI Z\u00fcrich"], "link": "http://ai.googleblog.com/2019/03/reducing-need-for-labeled-data-in.html", "abstract": "", "date": "\nWednesday, March 20, 2019\n"},
{"website": "Google-AI", "title": "\nMeasuring the Limits of Data Parallel Training for Neural Networks\n", "author": ["Posted by Chris Shallue, Senior Software Engineer and George Dahl, Senior Research Scientist, Google AI"], "link": "http://ai.googleblog.com/2019/03/measuring-limits-of-data-parallel.html", "abstract": "", "date": "\nTuesday, March 19, 2019\n"},
{"website": "Google-AI", "title": "\nA Summary of the Google Flood Forecasting Meets Machine Learning Workshop\n", "author": ["Posted by Sella Nevo, Senior Software Engineer and Rainier Aliment, Program Manager"], "link": "http://ai.googleblog.com/2019/03/a-summary-of-google-flood-forecasting.html", "abstract": "", "date": "\nMonday, March 18, 2019\n"},
{"website": "Google-AI", "title": "\nGoogle Faculty Research Awards 2018\n", "author": ["Posted by Maggie Johnson, VP, Education and Negar Saei, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2019/03/google-faculty-research-awards-2018.html", "abstract": "", "date": "\nFriday, March 15, 2019\n"},
{"website": "Google-AI", "title": "\nHarnessing Organizational Knowledge for Machine Learning\n", "author": ["Posted by Alex Ratner, Stanford University and Cassandra Xia, Google AI"], "link": "http://ai.googleblog.com/2019/03/harnessing-organizational-knowledge-for.html", "abstract": "", "date": "\nThursday, March 14, 2019\n"},
{"website": "Google-AI", "title": "\nAn All-Neural On-Device Speech Recognizer\n", "author": ["Posted by Johan Schalkwyk, Google Fellow, Speech Team"], "link": "http://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html", "abstract": "", "date": "\nTuesday, March 12, 2019\n"},
{"website": "Google-AI", "title": "\nReal-Time AR Self-Expression with Machine Learning\n", "author": ["Posted by Artsiom Ablavatski and Ivan Grishchenko, Research Engineers, Google AI"], "link": "http://ai.googleblog.com/2019/03/real-time-ar-self-expression-with.html", "abstract": "", "date": "\nFriday, March 8, 2019\n"},
{"website": "Google-AI", "title": "\nRNN-Based Handwriting Recognition in Gboard\n", "author": ["Posted by Sandro Feuz and Pedro Gonnet, Senior Software Engineers, Handwriting Team"], "link": "http://ai.googleblog.com/2019/03/rnn-based-handwriting-recognition-in.html", "abstract": "", "date": "\nThursday, March 7, 2019\n"},
{"website": "Google-AI", "title": "\nExploring Neural Networks with Activation Atlases\n", "author": ["Posted by Shan Carter, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2019/03/exploring-neural-networks.html", "abstract": "", "date": "\nWednesday, March 6, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models\n", "author": ["Posted by Yanping Huang, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html", "abstract": "", "date": "\nMonday, March 4, 2019\n"},
{"website": "Google-AI", "title": "\nLong-Range Robotic Navigation via Automated Reinforcement Learning\n", "author": ["Aleksandra Faust, Senior Research Scientist and Anthony Francis, Senior Software Engineer, Robotics at Google"], "link": "http://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html", "abstract": "", "date": "\nThursday, February 28, 2019\n"},
{"website": "Google-AI", "title": "\nLearning to Generalize from Sparse and Underspecified Rewards\n", "author": ["Posted by Rishabh Agarwal, Google AI Resident and Mohammad Norouzi, Research Scientist"], "link": "http://ai.googleblog.com/2019/02/learning-to-generalize-from-sparse-and.html", "abstract": "", "date": "\nFriday, February 22, 2019\n"},
{"website": "Google-AI", "title": "\nOn the Path to Cryogenic Control of Quantum Processors\n", "author": ["Posted by Joseph Bardin, Visiting Faculty Researcher and Erik Lucero, Staff Research Scientist and Hardware Lead, Google AI Quantum Team"], "link": "http://ai.googleblog.com/2019/02/on-path-to-cryogenic-control-of-quantum.html", "abstract": "", "date": "\nThursday, February 21, 2019\n"},
{"website": "Google-AI", "title": "\nIntroducing PlaNet: A Deep Planning Network for Reinforcement Learning\n", "author": ["Posted by Danijar Hafner, Student Researcher, Google AI"], "link": "http://ai.googleblog.com/2019/02/introducing-planet-deep-planning.html", "abstract": "", "date": "\nFriday, February 15, 2019\n"},
{"website": "Google-AI", "title": "\nUsing Global Localization to Improve Navigation\n", "author": ["Posted by Tilman Reinhardt\u200e, Software Engineer, Google Maps"], "link": "http://ai.googleblog.com/2019/02/using-global-localization-to-improve.html", "abstract": "", "date": "\nMonday, February 11, 2019\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Second Workshop and Challenge on Learned Image Compression\n", "author": ["Posted by Nick Johnston, Software Engineer, Machine Perception"], "link": "http://ai.googleblog.com/2019/02/announcing-second-workshop-and.html", "abstract": "", "date": "\nWednesday, February 6, 2019\n"},
{"website": "Google-AI", "title": "\nReal-time Continuous Transcription with Live Transcribe\n", "author": ["Posted by Sagar Savla, Product Manager, Machine Perception"], "link": "http://ai.googleblog.com/2019/02/real-time-continuous-transcription-with.html", "abstract": "", "date": "\nMonday, February 4, 2019\n"},
{"website": "Google-AI", "title": "\nTransformer-XL: Unleashing the Potential of Attention Models\n", "author": ["Posted by Zhilin Yang and Quoc Le, Google AI"], "link": "http://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html", "abstract": "", "date": "\nTuesday, January 29, 2019\n"},
{"website": "Google-AI", "title": "\nNatural Questions: a New Corpus and Challenge for Question Answering Research\n", "author": ["Posted by Tom Kwiatkowski and Michael Collins, Research Scientists, Google AI Language"], "link": "http://ai.googleblog.com/2019/01/natural-questions-new-corpus-and.html", "abstract": "", "date": "\nWednesday, January 23, 2019\n"},
{"website": "Google-AI", "title": "\nExpanding the Application of Deep Learning to Electronic Health Records\n", "author": ["Posted by Alvin Rajkomar, MD and Eyal Oren, PhD, Google AI, Healthcare"], "link": "http://ai.googleblog.com/2019/01/expanding-application-of-deep-learning.html", "abstract": "", "date": "\nTuesday, January 22, 2019\n"},
{"website": "Google-AI", "title": "\nSoft Actor-Critic: Deep Reinforcement Learning for Robotics\n", "author": ["Posted by Tuomas Haarnoja, Student Researcher and Sergey Levine, Faculty Advisor, Robotics at Google"], "link": "http://ai.googleblog.com/2019/01/soft-actor-critic-deep-reinforcement.html", "abstract": "", "date": "\nFriday, January 18, 2019\n"},
{"website": "Google-AI", "title": "\nLooking Back at Google\u2019s Research Efforts in 2018\n", "author": ["Posted by Jeff Dean, Senior Fellow and Google AI Lead, on behalf of the entire Google Research Community", "\n"], "link": "http://ai.googleblog.com/2019/01/looking-back-at-googles-research.html", "abstract": "", "date": "\nTuesday, January 15, 2019\n"},
{"website": "Google-AI", "title": "\nTop Shot on Pixel 3\n", "author": ["Posted by Li Zhang and Wei (Alex) Hong, Software Engineers"], "link": "http://ai.googleblog.com/2018/12/top-shot-on-pixel-3.html", "abstract": "", "date": "\nThursday, December 20, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle AI Princeton: Current and Future Research\n", "author": ["Posted by Elad Hazan and Yoram Singer, Research Scientists, Google AI and Princeton University"], "link": "http://ai.googleblog.com/2018/12/google-ai-princeton-current-and-future.html", "abstract": "", "date": "\nTuesday, December 18, 2018\n"},
{"website": "Google-AI", "title": "\nExploring Quantum Neural Networks\n", "author": ["Posted by Jarrod McClean, Senior Research Scientist and Hartmut Neven, Director of Engineering, Google AI Quantum Team"], "link": "http://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html", "abstract": "", "date": "\nMonday, December 17, 2018\n"},
{"website": "Google-AI", "title": "\n Improving the Effectiveness of Diabetic Retinopathy Models\n", "author": ["Posted by Rory Sayres PhD and Jonathan Krause PhD, Google AI, Healthcare"], "link": "http://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html", "abstract": "", "date": "\nThursday, December 13, 2018\n"},
{"website": "Google-AI", "title": "\nGrasp2Vec: Learning Object Representations from Self-Supervised Grasping\n", "author": ["Posted by Eric Jang, Software Engineer, Robotics at Google and Coline Devin, Berkeley PhD Student and former Research Intern"], "link": "http://ai.googleblog.com/2018/12/grasp2vec-learning-object.html", "abstract": "", "date": "\nTuesday, December 11, 2018\n"},
{"website": "Google-AI", "title": "\nProviding Gender-Specific Translations in Google Translate\n", "author": ["Posted by  Melvin Johnson, Senior Software Engineer, Google Translate"], "link": "http://ai.googleblog.com/2018/12/providing-gender-specific-translations.html", "abstract": "", "date": "\nMonday, December 10, 2018\n"},
{"website": "Google-AI", "title": "\nAdding Diversity to Images with Open Images Extended\n", "author": ["Posted by Anurag Batra and Parker Barnes, Product Managers, Google AI"], "link": "http://ai.googleblog.com/2018/12/adding-diversity-to-images-with-open.html", "abstract": "", "date": "\nFriday, December 7, 2018\n"},
{"website": "Google-AI", "title": "\nTF-Ranking: A Scalable TensorFlow Library for Learning-to-Rank\n", "author": ["Posted by Xuanhui Wang and Michael Bendersky, Software Engineers, Google AI"], "link": "http://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html", "abstract": "", "date": "\nWednesday, December 5, 2018\n"},
{"website": "Google-AI", "title": "\nThe NeurIPS 2018 Test of Time Award: The Trade-Offs of Large Scale Learning\n", "author": ["Posted by Anna Ukhanova, Program Manager, Google AI Z\u00fcrich"], "link": "http://ai.googleblog.com/2018/12/the-neurips-2018-test-of-time-award.html", "abstract": "", "date": "\nTuesday, December 4, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at NeurIPS 2018\n", "author": ["Posted by Slav Petrov, Principal Scientist, Google"], "link": "http://ai.googleblog.com/2018/12/google-at-neurips-2018.html", "abstract": "", "date": "\nMonday, December 3, 2018\n"},
{"website": "Google-AI", "title": "\nHighlights from the 2018 Google PhD Fellowship Summit\n", "author": ["Posted by Susie Kim, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2018/11/highlights-from-2018-google-phd.html", "abstract": "", "date": "\nFriday, November 30, 2018\n"},
{"website": "Google-AI", "title": "\nLearning to Predict Depth on the Pixel 3 Phones\n", "author": ["Posted by Rahul Garg, Research Scientist and Neal Wadhwa, Software Engineer"], "link": "http://ai.googleblog.com/2018/11/learning-to-predict-depth-on-pixel-3.html", "abstract": "", "date": "\nThursday, November 29, 2018\n"},
{"website": "Google-AI", "title": "\nA Structured Approach to Unsupervised Depth Learning from Monocular Videos\n", "author": ["Posted by Anelia Angelova, Research Scientist, Robotics at Google"], "link": "http://ai.googleblog.com/2018/11/a-structured-approach-to-unsupervised.html", "abstract": "", "date": "\nTuesday, November 27, 2018\n"},
{"website": "Google-AI", "title": "\nImproved Grading of Prostate Cancer Using Deep Learning\n", "author": ["Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Healthcare, Google AI"], "link": "http://ai.googleblog.com/2018/11/improved-grading-of-prostate-cancer.html", "abstract": "", "date": "\nFriday, November 16, 2018\n"},
{"website": "Google-AI", "title": "\nNight Sight: Seeing in the Dark on Pixel Phones\n", "author": ["Posted by Marc Levoy, Distinguished Engineer and Yael Pritch, Staff Software Engineer"], "link": "http://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html", "abstract": "", "date": "\nWednesday, November 14, 2018\n"},
{"website": "Google-AI", "title": "\nAccurate Online Speaker Diarization with Supervised Learning\n", "author": ["Posted by Chong Wang, Research Scientist, Google AI"], "link": "http://ai.googleblog.com/2018/11/accurate-online-speaker-diarization.html", "abstract": "", "date": "\nMonday, November 12, 2018\n"},
{"website": "Google-AI", "title": "\nOpen Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing\n", "author": ["Posted by Jacob Devlin and Ming-Wei Chang, Research Scientists, Google AI Language"], "link": "http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "abstract": "", "date": "\nFriday, November 2, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at EMNLP 2018\n", "author": ["Posted by Manaal Faruqui, Senior Research Scientist and Emily Pitler, Staff Research Scientist, Google AI Language"], "link": "http://ai.googleblog.com/2018/10/google-at-emnlp-2018.html", "abstract": "", "date": "\nWednesday, October 31, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing AdaNet: Fast and Flexible AutoML with Learning Guarantees\n", "author": ["Posted by Charles Weill, Software Engineer, Google AI, NYC"], "link": "http://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html", "abstract": "", "date": "\nTuesday, October 30, 2018\n"},
{"website": "Google-AI", "title": "\nAcoustic Detection of Humpback Whales Using a Convolutional Neural Network\n", "author": ["Posted by Matt Harvey, Software Engineer, Google AI Perception"], "link": "http://ai.googleblog.com/2018/10/acoustic-detection-of-humpback-whales.html", "abstract": "", "date": "\nMonday, October 29, 2018\n"},
{"website": "Google-AI", "title": "\nCuriosity and Procrastination in Reinforcement Learning\n", "author": ["Posted by Nikolay Savinov, Research Intern, Google Brain Team and Timothy Lillicrap, Research Scientist, DeepMind"], "link": "http://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html", "abstract": "", "date": "\nWednesday, October 24, 2018\n"},
{"website": "Google-AI", "title": "\nFluid Annotation: An Exploratory Machine Learning\u2013Powered Interface for Faster Image Annotation\n", "author": ["Posted by Jasper Uijlings and Vittorio Ferrari, Research Scientists, Machine Perception"], "link": "http://ai.googleblog.com/2018/10/fluid-annotation-exploratory-machine.html", "abstract": "", "date": "\nMonday, October 22, 2018\n"},
{"website": "Google-AI", "title": "\nSee Better and Further with Super Res Zoom on the Pixel 3\n", "author": ["Posted by Bartlomiej Wronski, Software Engineer and Peyman Milanfar, Lead Scientist, Computational Imaging"], "link": "http://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html", "abstract": "", "date": "\nMonday, October 15, 2018\n"},
{"website": "Google-AI", "title": "\nApplying Deep Learning to Metastatic Breast Cancer Detection\n", "author": ["Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Healthcare, Google AI"], "link": "http://ai.googleblog.com/2018/10/applying-deep-learning-to-metastatic.html", "abstract": "", "date": "\nFriday, October 12, 2018\n"},
{"website": "Google-AI", "title": "\nOpen Sourcing Active Question Reformulation with Reinforcement Learning\n", "author": ["Posted by Michelle Chen Huebscher, Software Engineer and Rodrigo Nogueira, New York University PhD Student and Software Engineering Intern, Google AI Language"], "link": "http://ai.googleblog.com/2018/10/open-sourcing-active-question.html", "abstract": "", "date": "\nWednesday, October 10, 2018\n"},
{"website": "Google-AI", "title": "\nHighlights from the Google AI Residency Program\n", "author": ["Posted by Phing Lee, Program Manager, Google AI Residency", "\n"], "link": "http://ai.googleblog.com/2018/10/highlights-from-google-ai-residency.html", "abstract": "", "date": "\nTuesday, October 9, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the Kaggle \u201cQuick, Draw!\u201d Doodle Recognition Challenge\n", "author": ["Posted by Thomas Deselaers, Senior Staff Software Engineer and Jake Walker, Product Manager, Machine Perception"], "link": "http://ai.googleblog.com/2018/09/introducing-kaggle-quick-draw-doodle.html", "abstract": "", "date": "\nFriday, September 28, 2018\n"},
{"website": "Google-AI", "title": "\nBuilding Google Dataset Search and Fostering an Open Data Ecosystem\n", "author": ["Posted by Matthew Burgess and Natasha Noy, Google AI"], "link": "http://ai.googleblog.com/2018/09/building-google-dataset-search-and.html", "abstract": "", "date": "\nWednesday, September 26, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle\u2019s Next Generation Music Recognition\n", "author": ["Posted by James Lyon, Google AI, Z\u00fcrich"], "link": "http://ai.googleblog.com/2018/09/googles-next-generation-music.html", "abstract": "", "date": "\nFriday, September 14, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the Unrestricted Adversarial Examples Challenge\n", "author": ["Posted by Tom B. Brown and Catherine Olsson, Research Engineers, Google Brain Team"], "link": "http://ai.googleblog.com/2018/09/introducing-unrestricted-adversarial.html", "abstract": "", "date": "\nThursday, September 13, 2018\n"},
{"website": "Google-AI", "title": "\nThe What-If Tool: Code-Free Probing of Machine Learning Models\n", "author": ["Posted by James Wexler, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2018/09/the-what-if-tool-code-free-probing-of.html", "abstract": "", "date": "\nTuesday, September 11, 2018\n"},
{"website": "Google-AI", "title": "\nText-to-Speech for Low-Resource Languages (Episode 4): One Down, 299 to Go\n", "author": ["Posted by Alexander Gutkin, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2018/09/text-to-speech-for-low-resource.html", "abstract": "", "date": "\nFriday, September 7, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the Inclusive Images Competition\n", "author": ["Posted by Tulsee Doshi, Product Manager, Google AI"], "link": "http://ai.googleblog.com/2018/09/introducing-inclusive-images-competition.html", "abstract": "", "date": "\nThursday, September 6, 2018\n"},
{"website": "Google-AI", "title": "\nConceptual Captions: A New Dataset and Challenge for Image Captioning\n", "author": ["Posted by Piyush Sharma, Software Engineer and Radu Soricut, Research Scientist, Google AI"], "link": "http://ai.googleblog.com/2018/09/conceptual-captions-new-dataset-and.html", "abstract": "", "date": "\nWednesday, September 5, 2018\n"},
{"website": "Google-AI", "title": "\nUnderstanding Performance Fluctuations in Quantum Processors\n", "author": ["Posted by Paul V. Klimov, Research Scientist, Google AI Quantum Team"], "link": "http://ai.googleblog.com/2018/08/understanding-performance-fluctuations.html", "abstract": "", "date": "\nFriday, August 31, 2018\n"},
{"website": "Google-AI", "title": "\nTeaching the Google Assistant to be Multilingual\n", "author": ["Posted by Johan Schalkwyk, VP and Ignacio Lopez Moreno, Engineer, Google Speech"], "link": "http://ai.googleblog.com/2018/08/Multilingual-Google-Assistant.html", "abstract": "", "date": "\nThursday, August 30, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing a New Framework for Flexible and Reproducible Reinforcement Learning Research\n", "author": ["Posted by Pablo Samuel Castro, Research Software Developer and Marc G. Bellemare, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html", "abstract": "", "date": "\nMonday, August 27, 2018\n"},
{"website": "Google-AI", "title": "\nMoving Beyond Translation with the Universal Transformer\n", "author": ["Posted by Stephan Gouws, Research Scientist, Google Brain Team and Mostafa Dehghani, University of Amsterdam PhD student and Google Research Intern"], "link": "http://ai.googleblog.com/2018/08/moving-beyond-translation-with.html", "abstract": "", "date": "\nWednesday, August 15, 2018\n"},
{"website": "Google-AI", "title": "\nThe Machine Learning Behind Android Smart Linkify\n", "author": ["Posted by Lukas Zilka, Software Engineer, Google AI, Z\u00fcrich"], "link": "http://ai.googleblog.com/2018/08/the-machine-learning-behind-android.html", "abstract": "", "date": "\nThursday, August 9, 2018\n"},
{"website": "Google-AI", "title": "\n MnasNet: Towards Automating the Design of Mobile Machine Learning Models\n", "author": ["Posted by Mingxing Tan, Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2018/08/mnasnet-towards-automating-design-of.html", "abstract": "", "date": "\nTuesday, August 7, 2018\n"},
{"website": "Google-AI", "title": "\nMachine Learning in Google BigQuery\n", "author": ["Posted by Umar Syed and Sergei Vassilvitskii, Research Scientists, Google AI, NYC"], "link": "http://ai.googleblog.com/2018/07/machine-learning-in-google-bigquery.html", "abstract": "", "date": "\nWednesday, July 25, 2018\n"},
{"website": "Google-AI", "title": "\nAnnouncing Cirq: An Open Source Framework for NISQ Algorithms\n", "author": ["Posted by Alan Ho, Product Lead and Dave Bacon, Software Lead, Google AI Quantum Team"], "link": "http://ai.googleblog.com/2018/07/announcing-cirq-open-source-framework.html", "abstract": "", "date": "\nWednesday, July 18, 2018\n"},
{"website": "Google-AI", "title": "\nImproving Connectomics by an Order of Magnitude\n", "author": ["Posted by Viren Jain, Research Scientist and Technical Lead and Michal Januszewski, Software Engineer, Connectomics at Google"], "link": "http://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html", "abstract": "", "date": "\nMonday, July 16, 2018\n"},
{"website": "Google-AI", "title": "\nAccelerated Training and Inference with the Tensorflow Object Detection API\n", "author": ["Posted by Jonathan Huang, Research Scientist and Vivek Rathod, Software Engineer, Google AI Perception"], "link": "http://ai.googleblog.com/2018/07/accelerated-training-and-inference-with.html", "abstract": "", "date": "\nFriday, July 13, 2018\n"},
{"website": "Google-AI", "title": "\nAutomating Drug Discoveries Using Computer Vision\n", "author": ["Vincent Vanhoucke, Principal Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2018/07/automating-drug-discoveries-using.html", "abstract": "", "date": "\nThursday, July 12, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at ICML 2018\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Google AI Communications"], "link": "http://ai.googleblog.com/2018/07/google-at-icml-2018.html", "abstract": "", "date": "\nMonday, July 9, 2018\n"},
{"website": "Google-AI", "title": "\nScalable Deep Reinforcement Learning for Robotic Manipulation\n", "author": ["Posted Alex Irpan, Software Engineer, Google Brain Team and Peter Pastor, Senior Roboticist, X"], "link": "http://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html", "abstract": "", "date": "\nThursday, June 28, 2018\n"},
{"website": "Google-AI", "title": "\nSelf-Supervised Tracking via Video Colorization\n", "author": ["Posted by Carl Vondrick, Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html", "abstract": "", "date": "\nWednesday, June 27, 2018\n"},
{"website": "Google-AI", "title": "\nTeaching Uncalibrated Robots to Visually Self-Adapt\n", "author": ["Posted by Fereshteh Sadeghi, Student Researcher, Google Brain Team"], "link": "http://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html", "abstract": "", "date": "\nFriday, June 22, 2018\n"},
{"website": "Google-AI", "title": "\nHow Can Neural Network Similarity Help Us Understand Training and Generalization?\n", "author": ["Posted by Maithra Raghu, Google Brain Team and Ari S. Morcos, DeepMind", "\n"], "link": "http://ai.googleblog.com/2018/06/how-can-neural-network-similarity-help.html", "abstract": "", "date": "\nThursday, June 21, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at CVPR 2018\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Google AI Communications", "\n"], "link": "http://ai.googleblog.com/2018/06/google-at-cvpr-2018.html", "abstract": "", "date": "\nMonday, June 18, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at NAACL\n", "author": ["Posted by Kenton Lee, Research Scientist and Slav Petrov, Principal Scientist, Language Team, Google AI"], "link": "http://ai.googleblog.com/2018/06/google-at-naacl.html", "abstract": "", "date": "\nFriday, June 8, 2018\n"},
{"website": "Google-AI", "title": "\nRealtime tSNE Visualizations with TensorFlow.js\n", "author": ["Posted by Nicola Pezzotti, Software Engineering Intern, Google Z\u00fcrich"], "link": "http://ai.googleblog.com/2018/06/realtime-tsne-visualizations-with.html", "abstract": "", "date": "\nThursday, June 7, 2018\n"},
{"website": "Google-AI", "title": "\nAnnouncing an updated YouTube-8M, and the 2nd YouTube-8M Large-Scale Video Understanding Challenge and Workshop\n", "author": ["Posted by Joonseok Lee, Software Engineer, Google AI"], "link": "http://ai.googleblog.com/2018/06/announcing-updated-youtube-8m-and-2nd.html", "abstract": "", "date": "\nTuesday, June 5, 2018\n"},
{"website": "Google-AI", "title": "\nImproving Deep Learning Performance with AutoAugment\n", "author": ["Posted by Ekin Dogus Cubuk, Google AI Resident and Barret Zoph, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2018/06/improving-deep-learning-performance.html", "abstract": "", "date": "\nMonday, June 4, 2018\n"},
{"website": "Google-AI", "title": "\nAdvances in Semantic Textual Similarity\n", "author": ["Posted by Yinfei Yang, Software Engineer and Chris Tar,  Engineering Manager, Google AI"], "link": "http://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html", "abstract": "", "date": "\nThursday, May 17, 2018\n"},
{"website": "Google-AI", "title": "\nSmart Compose: Using Neural Networks to Help Write Emails\n", "author": ["Posted by Yonghui Wu, Principal Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html", "abstract": "", "date": "\nWednesday, May 16, 2018\n"},
{"website": "Google-AI", "title": "\nAutomatic Photography with Google Clips\n", "author": ["Posted by Aseem Agarwala, Research Scientist, Clips Content Team Lead"], "link": "http://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html", "abstract": "", "date": "\nFriday, May 11, 2018\n"},
{"website": "Google-AI", "title": "\nCustom On-Device ML Models with Learn2Compress\n", "author": ["Posted by Sujith Ravi, Senior Staff Research Scientist, Google Expander Team"], "link": "http://ai.googleblog.com/2018/05/custom-on-device-ml-models.html", "abstract": "", "date": "\nWednesday, May 9, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle Duplex: An AI System for Accomplishing Real-World Tasks Over the Phone\n", "author": ["Posted by Yaniv Leviathan, Principal Engineer and Yossi Matias, Vice President, Engineering, Google"], "link": "http://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html", "abstract": "", "date": "\nTuesday, May 8, 2018\n"},
{"website": "Google-AI", "title": "\nDeep Learning for Electronic Health Records\n", "author": ["Posted by Alvin Rajkomar MD, Research Scientist and Eyal Oren PhD, Product Manager, Google AI"], "link": "http://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html", "abstract": "", "date": "\nTuesday, May 8, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing Google AI\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Google AI Communications"], "link": "http://ai.googleblog.com/2018/05/introducing-google-ai.html", "abstract": "", "date": "\nMonday, May 7, 2018\n"},
{"website": "Google-AI", "title": "\nThe Question of Quantum Supremacy\n", "author": ["Posted by Sergio Boixo, Research Scientist and Theory Team Lead, and Charles Neill, Quantum Electronics Engineer, Quantum A.I. Lab"], "link": "http://ai.googleblog.com/2018/05/the-question-of-quantum-supremacy.html", "abstract": "", "date": "\nFriday, May 4, 2018\n"},
{"website": "Google-AI", "title": "\nAnnouncing Open Images V4 and the ECCV 2018 Open Images Challenge\n", "author": ["Posted by Vittorio Ferrari, Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2018/04/announcing-open-images-v4-and-eccv-2018.html", "abstract": "", "date": "\nMonday, April 30, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle at ICLR 2018\n", "author": ["Posted by Jeff Dean, Google Senior Fellow, Head of Google Research and Machine Intelligence"], "link": "http://ai.googleblog.com/2018/04/google-at-iclr-2018.html", "abstract": "", "date": "\nSunday, April 29, 2018\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Google Cloud Platform Research Credits Program\n", "author": ["Posted by Steven Butschi, Head of Higher Education, Google"], "link": "http://ai.googleblog.com/2018/04/announcing-google-cloud-platform.html", "abstract": "", "date": "\nThursday, April 26, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle\u2019s Workshop on AI/ML Research and Practice in India\n", "author": ["Posted by Pankaj Gupta and Anand Rangarajan, Engineering Directors, Google India"], "link": "http://ai.googleblog.com/2018/04/googles-workshop-on-aiml-research-and.html", "abstract": "", "date": "\nTuesday, April 24, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the CVPR 2018 On-Device Visual Intelligence Challenge\n", "author": ["Posted by Bo Chen, Software Engineer and Jeffrey M. Gilbert, Member of Technical Staff, Google Research"], "link": "http://ai.googleblog.com/2018/04/introducing-cvpr-2018-on-device-visual.html", "abstract": "", "date": "\nFriday, April 20, 2018\n"},
{"website": "Google-AI", "title": "\nDeepVariant Accuracy Improvements for Genetic Datatypes\n", "author": ["Posted by Pi-Chuan Chang, Software Engineer and Lizzie Dorfman, Technical Program Manager, Google Brain Team"], "link": "http://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html", "abstract": "", "date": "\nThursday, April 19, 2018\n"},
{"website": "Google-AI", "title": "\nAn Augmented Reality Microscope for Cancer Detection\n", "author": ["Posted by Martin Stumpe, Technical Lead and Craig Mermel, Product Manager, Google Brain Team"], "link": "http://ai.googleblog.com/2018/04/an-augmented-reality-microscope.html", "abstract": "", "date": "\nMonday, April 16, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing Semantic Experiences with Talk to Books and Semantris\n", "author": ["Posted by Ray Kurzweil, Director of Engineering and Rachel Bernstein, Product Manager, Google Research"], "link": "http://ai.googleblog.com/2018/04/introducing-semantic-experiences-with.html", "abstract": "", "date": "\nFriday, April 13, 2018\n"},
{"website": "Google-AI", "title": "\nSeeing More with In Silico Labeling of Microscopy Images\n", "author": ["Eric Christiansen, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2018/04/seeing-more-with-in-silico-labeling-of.html", "abstract": "", "date": "\nThursday, April 12, 2018\n"},
{"website": "Google-AI", "title": "\nLooking to Listen: Audio-Visual Speech Separation\n", "author": ["Posted by Inbar Mosseri and Oran Lang, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2018/04/looking-to-listen-audio-visual-speech.html", "abstract": "", "date": "\nWednesday, April 11, 2018\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2018 Google PhD Fellows for North America, Europe and the Middle East\n", "author": ["Posted by Susie Kim, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2018/04/announcing-2018-google-phd-fellows-for.html", "abstract": "", "date": "\nThursday, April 5, 2018\n"},
{"website": "Google-AI", "title": "\nMobileNetV2: The Next Generation of On-Device Computer Vision Networks\n", "author": ["Posted by Mark Sandler and Andrew Howard, Google Research"], "link": "http://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html", "abstract": "", "date": "\nTuesday, April 3, 2018\n"},
{"website": "Google-AI", "title": "\nInvesting in France\u2019s AI Ecosystem\n", "author": ["Posted by Olivier Bousquet, Principal Engineer, Google Z\u00fcrich"], "link": "http://ai.googleblog.com/2018/03/investing-in-frances-ai-ecosystem.html", "abstract": "", "date": "\nWednesday, March 28, 2018\n"},
{"website": "Google-AI", "title": "\nUsing Machine Learning to Discover Neural Network Optimizers\n", "author": ["Posted by Irwan Bello, Research Associate, Google Brain Team"], "link": "http://ai.googleblog.com/2018/03/using-machine-learning-to-discover.html", "abstract": "", "date": "\nWednesday, March 28, 2018\n"},
{"website": "Google-AI", "title": "\nExpressive Speech Synthesis with Tacotron\n", "author": ["Posted by Yuxuan Wang, Research Scientist and RJ Skerry-Ryan, Software Engineer, on behalf of the Machine Perception, Google Brain and TTS Research teams"], "link": "http://ai.googleblog.com/2018/03/expressive-speech-synthesis-with.html", "abstract": "", "date": "\nTuesday, March 27, 2018\n"},
{"website": "Google-AI", "title": "\nReformulating Chemistry for More Efficient Quantum Computation\n", "author": ["Posted by Ryan Babbush, Senior Research Scientist, Quantum AI Team"], "link": "http://ai.googleblog.com/2018/03/reformulating-chemistry-for-more.html", "abstract": "", "date": "\nThursday, March 22, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle Faculty Research Awards 2017\n", "author": ["Posted by Maggie Johnson, Vice President of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2018/03/google-faculty-research-awards-2017.html", "abstract": "", "date": "\nTuesday, March 20, 2018\n"},
{"website": "Google-AI", "title": "\nUsing Deep Learning to Facilitate Scientific Image Analysis\n", "author": ["Posted by Samuel Yang, Research Scientist, Google Accelerated Science Team"], "link": "http://ai.googleblog.com/2018/03/using-deep-learning-to-facilitate.html", "abstract": "", "date": "\nFriday, March 16, 2018\n"},
{"website": "Google-AI", "title": "\nUsing Evolutionary AutoML to Discover Neural Network Architectures\n", "author": ["Posted by Esteban Real, Senior Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html", "abstract": "", "date": "\nThursday, March 15, 2018\n"},
{"website": "Google-AI", "title": "\nBalanced Partitioning and Hierarchical Clustering at Scale\n", "author": ["Posted by Hossein Bateni, Research Scientist and Kevin Aydin, Software Engineer, NYC Algorithms and Optimization Research Team"], "link": "http://ai.googleblog.com/2018/03/balanced-partitioning-and-hierarchical.html", "abstract": "", "date": "\nWednesday, March 14, 2018\n"},
{"website": "Google-AI", "title": "\nBehind the Motion Photos Technology in Pixel 2\n", "author": ["Posted by Matthias Grundmann, Research Scientist and Jianing Wei, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2018/03/behind-motion-photos-technology-in.html", "abstract": "", "date": "\nTuesday, March 13, 2018\n"},
{"website": "Google-AI", "title": "\nSemantic Image Segmentation with DeepLab in TensorFlow\n", "author": ["Posted by Liang-Chieh Chen and Yukun Zhu, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html", "abstract": "", "date": "\nMonday, March 12, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the iNaturalist 2018 Challenge\n", "author": ["Posted by Yang Song, Staff Software Engineer and Serge Belongie, Visiting Faculty, Google Research"], "link": "http://ai.googleblog.com/2018/03/introducing-inaturalist-2018-challenge.html", "abstract": "", "date": "\nFriday, March 9, 2018\n"},
{"website": "Google-AI", "title": "\nOpen Sourcing the Hunt for Exoplanets\n", "author": ["Posted by Chris Shallue, Senior Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2018/03/open-sourcing-hunt-for-exoplanets.html", "abstract": "", "date": "\nThursday, March 8, 2018\n"},
{"website": "Google-AI", "title": "\nThe Building Blocks of Interpretability\n", "author": ["Posted by Chris Olah, Research Scientist and Arvind Satyanarayan, Visiting Researcher, Google Brain Team"], "link": "http://ai.googleblog.com/2018/03/the-building-blocks-of-interpretability.html", "abstract": "", "date": "\nTuesday, March 6, 2018\n"},
{"website": "Google-AI", "title": "\nA Preview of Bristlecone, Google\u2019s New Quantum Processor\n", "author": ["Posted by Julian Kelly, Research Scientist, Quantum AI Lab"], "link": "http://ai.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html", "abstract": "", "date": "\nMonday, March 5, 2018\n"},
{"website": "Google-AI", "title": "\nMaking Healthcare Data Work Better with Machine Learning\n", "author": ["Posted by Patrik Sundberg, Software Engineer and Eyal Oren, Product Manager, Google Brain Team"], "link": "http://ai.googleblog.com/2018/03/making-healthcare-data-work-better-with.html", "abstract": "", "date": "\nFriday, March 2, 2018\n"},
{"website": "Google-AI", "title": "\nMobile Real-time Video Segmentation\n", "author": [" Valentin Bazarevsky and Andrei Tkachenka, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2018/03/mobile-real-time-video-segmentation.html", "abstract": "", "date": "\nThursday, March 1, 2018\n"},
{"website": "Google-AI", "title": "\nGoogle-Landmarks: A New Dataset and Challenge for Landmark Recognition\n", "author": ["Posted by Andr\u00e9 Araujo and Tobias Weyand, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2018/03/google-landmarks-new-dataset-and.html", "abstract": "", "date": "\nThursday, March 1, 2018\n"},
{"website": "Google-AI", "title": "\nA Summary of the Google Z\u00fcrich Algorithms & Optimization Workshop\n", "author": ["Posted by Silvio Lattanzi, Research Scientist, Google Z\u00fcrich and Vahab Mirrokni, Research Scientist, Google New York"], "link": "http://ai.googleblog.com/2018/02/a-summary-of-google-zurich-algorithms.html", "abstract": "", "date": "\nFriday, February 23, 2018\n"},
{"website": "Google-AI", "title": "\nAssessing Cardiovascular Risk Factors with Computer Vision\n", "author": ["Posted by Lily Peng MD PhD, Product Manager, Google Brain Team"], "link": "http://ai.googleblog.com/2018/02/assessing-cardiovascular-risk-factors.html", "abstract": "", "date": "\nMonday, February 19, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the HDR+ Burst Photography Dataset\n", "author": ["Posted by Sam Hasinoff, Software Engineer, Machine Perception"], "link": "http://ai.googleblog.com/2018/02/introducing-hdr-burst-photography.html", "abstract": "", "date": "\nMonday, February 12, 2018\n"},
{"website": "Google-AI", "title": "\nThe Instant Motion Tracking Behind Motion Stills AR\n", "author": ["Posted by Jianing Wei and Tyler Mullen, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2018/02/the-instant-motion-tracking-behind.html", "abstract": "", "date": "\nTuesday, February 6, 2018\n"},
{"website": "Google-AI", "title": "\nThe Google Brain Team \u2014 Looking Back on 2017 (Part 2 of 2)\n", "author": ["Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"], "link": "http://ai.googleblog.com/2018/01/the-google-brain-team-looking-back-on_12.html", "abstract": "", "date": "\nFriday, January 12, 2018\n"},
{"website": "Google-AI", "title": "\nThe Google Brain Team \u2014 Looking Back on 2017  (Part 1 of 2)\n", "author": ["Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"], "link": "http://ai.googleblog.com/2018/01/the-google-brain-team-looking-back-on.html", "abstract": "", "date": "\nThursday, January 11, 2018\n"},
{"website": "Google-AI", "title": "\nIntroducing the CVPR 2018 Learned Image Compression Challenge\n", "author": ["Posted by Michele Covell, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2018/01/introducing-cvpr-2018-learned-image.html", "abstract": "", "date": "\nWednesday, January 10, 2018\n"},
{"website": "Google-AI", "title": "\nEvaluation of Speech for the Google Assistant\n", "author": ["Posted by Enrique Alfonseca, Staff Research Scientist, Google Assistant"], "link": "http://ai.googleblog.com/2017/12/evaluation-of-speech-for-google.html", "abstract": "", "date": "\nThursday, December 21, 2017\n"},
{"website": "Google-AI", "title": "\nTacotron 2: Generating Human-like Speech from Text\n", "author": ["Posted by Jonathan Shen and Ruoming Pang, Software Engineers, on behalf of the Google Brain and Machine Perception Teams"], "link": "http://ai.googleblog.com/2017/12/tacotron-2-generating-human-like-speech.html", "abstract": "", "date": "\nTuesday, December 19, 2017\n"},
{"website": "Google-AI", "title": "\nIntroducing NIMA: Neural Image Assessment\n", "author": ["Posted by Hossein Talebi, Software Engineer and Peyman Milanfar Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2017/12/introducing-nima-neural-image-assessment.html", "abstract": "", "date": "\nMonday, December 18, 2017\n"},
{"website": "Google-AI", "title": "\nImproving End-to-End Models For Speech Recognition\n", "author": ["Posted by Tara N. Sainath, Research Scientist, Speech Team and Yonghui Wu, Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2017/12/improving-end-to-end-models-for-speech.html", "abstract": "", "date": "\nThursday, December 14, 2017\n"},
{"website": "Google-AI", "title": "\nA Summary of the First Conference on Robot Learning\n", "author": ["Posted by Vincent Vanhoucke, Principal Scientist, Google Brain Team and Melanie Salda\u00f1a, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2017/12/a-summary-of-first-conference-on-robot.html", "abstract": "", "date": "\nWednesday, December 13, 2017\n"},
{"website": "Google-AI", "title": "\nTFGAN: A Lightweight Library for Generative Adversarial Networks\n", "author": ["Posted by Joel Shor, Senior Software Engineer, Machine Perception"], "link": "http://ai.googleblog.com/2017/12/tfgan-lightweight-library-for.html", "abstract": "", "date": "\nTuesday, December 12, 2017\n"},
{"website": "Google-AI", "title": "\nIntroducing Appsperiments: Exploring the Potentials of Mobile Photography\n", "author": ["Posted by Alex Kauffmann, Interaction Researcher, Google Research"], "link": "http://ai.googleblog.com/2017/12/introducing-appsperiments-exploring.html", "abstract": "", "date": "\nMonday, December 11, 2017\n"},
{"website": "Google-AI", "title": "\nIntroducing a New Foveation Pipeline for Virtual/Mixed Reality\n", "author": ["Posted by Behnam Bastani, Software Engineer Manager and Eric Turner, Software Engineer, Daydream"], "link": "http://ai.googleblog.com/2017/12/introducing-new-foveation-pipeline-for.html", "abstract": "", "date": "\nTuesday, December 5, 2017\n"},
{"website": "Google-AI", "title": "\nDeepVariant: Highly Accurate Genomes With Deep Neural Networks\n", "author": ["Posted by Mark DePristo and Ryan Poplin, Google Brain Team"], "link": "http://ai.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html", "abstract": "", "date": "\nMonday, December 4, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle at NIPS 2017\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Research Communications"], "link": "http://ai.googleblog.com/2017/12/google-at-nips-2017.html", "abstract": "", "date": "\nSunday, December 3, 2017\n"},
{"website": "Google-AI", "title": "\nUnderstanding Bias in Peer Review\n", "author": ["Posted by Andrew Tomkins, Director of Engineering and William D. Heavlin, Statistician, Google Research"], "link": "http://ai.googleblog.com/2017/11/understanding-bias-in-peer-review.html", "abstract": "", "date": "\nThursday, November 30, 2017\n"},
{"website": "Google-AI", "title": "\nInterpreting Deep Neural Networks with SVCCA\n", "author": ["Posted by Maithra Raghu, Google Brain Team"], "link": "http://ai.googleblog.com/2017/11/interpreting-deep-neural-networks-with.html", "abstract": "", "date": "\nTuesday, November 28, 2017\n"},
{"website": "Google-AI", "title": "\nUnderstanding Medical Conversations\n", "author": ["Posted by Katherine Chou, Product Manager and Chung-Cheng Chiu, Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2017/11/understanding-medical-conversations.html", "abstract": "", "date": "\nTuesday, November 21, 2017\n"},
{"website": "Google-AI", "title": "\nSLING: A Natural Language Frame Semantic Parser\n", "author": ["Posted by Michael Ringgaard, Software Engineer and Rahul Gupta, Research Scientist"], "link": "http://ai.googleblog.com/2017/11/sling-natural-language-frame-semantic.html", "abstract": "", "date": "\nWednesday, November 15, 2017\n"},
{"website": "Google-AI", "title": "\nOn-Device Conversational Modeling with TensorFlow Lite\n", "author": ["Posted by Sujith Ravi, Research Scientist, Google Expander Team"], "link": "http://ai.googleblog.com/2017/11/on-device-conversational-modeling-with.html", "abstract": "", "date": "\nTuesday, November 14, 2017\n"},
{"website": "Google-AI", "title": "\nFused Video Stabilization on the Pixel 2 and Pixel 2 XL\n", "author": ["Posted by Chia-Kai Liang, Senior Staff Software Engineer and Fuhao Shi, Android Camera Team"], "link": "http://ai.googleblog.com/2017/11/fused-video-stabilization-on-pixel-2.html", "abstract": "", "date": "\nFriday, November 10, 2017\n"},
{"website": "Google-AI", "title": "\nSeamless Google Street View Panoramas\n", "author": ["Posted by Mike Krainin, Software Engineer and Ce Liu, Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2017/11/seamless-google-street-view-panoramas.html", "abstract": "", "date": "\nThursday, November 9, 2017\n"},
{"website": "Google-AI", "title": "\nFeature Visualization\n", "author": ["Posted by Christopher Olah, Research Scientist, Google Brain Team and Alex Mordvintsev, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2017/11/feature-visualization.html", "abstract": "", "date": "\nTuesday, November 7, 2017\n"},
{"website": "Google-AI", "title": "\nTangent: Source-to-Source Debuggable Derivatives\n", "author": ["Posted by Alex Wiltschko, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/11/tangent-source-to-source-debuggable.html", "abstract": "", "date": "\nMonday, November 6, 2017\n"},
{"website": "Google-AI", "title": "\nAutoML for large scale image classification and object detection\n", "author": ["Posted by Barret Zoph, Vijay Vasudevan, Jonathon Shlens and Quoc Le, Research Scientists, Google Brain Team"], "link": "http://ai.googleblog.com/2017/11/automl-for-large-scale-image.html", "abstract": "", "date": "\nThursday, November 2, 2017\n"},
{"website": "Google-AI", "title": "\nLatest Innovations in TensorFlow Serving\n", "author": ["Posted by Chris Olston, Research Scientist, and Noah Fiedel, Software Engineer, TensorFlow Serving"], "link": "http://ai.googleblog.com/2017/11/latest-innovations-in-tensorflow-serving.html", "abstract": "", "date": "\nThursday, November 2, 2017\n"},
{"website": "Google-AI", "title": "\nEager Execution: An imperative, define-by-run interface to TensorFlow \n", "author": ["Posted by Asim Shankar and Wolff Dobson, Google Brain Team"], "link": "http://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html", "abstract": "", "date": "\nTuesday, October 31, 2017\n"},
{"website": "Google-AI", "title": "\nClosing the Simulation-to-Reality Gap for Deep Robotic Learning\n", "author": ["Posted by Konstantinos Bousmalis,\u00a0Senior Research Scientist, and Sergey Levine, Faculty Advisor, Google Brain Team"], "link": "http://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html", "abstract": "", "date": "\nMonday, October 30, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing OpenFermion: The Open Source Chemistry Package for Quantum Computers\n", "author": ["Posted by Ryan Babbush and Jarrod McClean, Quantum Software Engineers, Quantum AI Team", "\n"], "link": "http://ai.googleblog.com/2017/10/announcing-openfermion-open-source.html", "abstract": "", "date": "\nMonday, October 23, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing AVA: A Finely Labeled Video Dataset for Human Action Understanding \n", "author": ["Posted by Chunhui Gu & David Ross, Software Engineers"], "link": "http://ai.googleblog.com/2017/10/announcing-ava-finely-labeled-video.html", "abstract": "", "date": "\nThursday, October 19, 2017\n"},
{"website": "Google-AI", "title": "\nPortrait mode on the Pixel 2 and Pixel 2 XL smartphones\n", "author": ["Posted by Marc Levoy, Principal Engineer and Yael Pritch, Software Engineer"], "link": "http://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html", "abstract": "", "date": "\nTuesday, October 17, 2017\n"},
{"website": "Google-AI", "title": "\nTensorFlow Lattice: Flexibility Empowered by Prior Knowledge\n", "author": ["Posted by Maya Gupta, Research Scientist, Jan Pfeifer, Software Engineer and Seungil You, Software Engineer"], "link": "http://ai.googleblog.com/2017/10/tensorflow-lattice-flexibility.html", "abstract": "", "date": "\nWednesday, October 11, 2017\n"},
{"website": "Google-AI", "title": "\nThe Google Brain Team\u2019s Approach to Research\n", "author": ["Posted by Jeff Dean, Google Senior Fellow"], "link": "http://ai.googleblog.com/2017/09/the-google-brain-teams-approach-to.html", "abstract": "", "date": "\nWednesday, September 13, 2017\n"},
{"website": "Google-AI", "title": "\nHighlights from the Annual Google PhD Fellowship Summit, and Announcing the 2017 Google PhD Fellows\n", "author": ["Posted by Susie Kim, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2017/09/highlights-from-annual-google-phd.html", "abstract": "", "date": "\nTuesday, September 12, 2017\n"},
{"website": "Google-AI", "title": "\nBuild your own Machine Learning Visualizations with the new TensorBoard API\n", "author": ["Posted by Chi Zeng and Justine Tunney, Software Engineers, Google Brain Team"], "link": "http://ai.googleblog.com/2017/09/build-your-own-machine-learning.html", "abstract": "", "date": "\nMonday, September 11, 2017\n"},
{"website": "Google-AI", "title": "\nSeminal Ideas from 2007\n", "author": ["Posted by Anna Ukhanova, Technical Program Manager, Google Research Europe"], "link": "http://ai.googleblog.com/2017/09/seminal-ideas-from-2007.html", "abstract": "", "date": "\nWednesday, September 6, 2017\n"},
{"website": "Google-AI", "title": "\nTransformer: A Novel Neural Network Architecture for Language Understanding\n", "author": ["Posted by Jakob Uszkoreit, Software Engineer, Natural Language Understanding"], "link": "http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html", "abstract": "", "date": "\nThursday, August 31, 2017\n"},
{"website": "Google-AI", "title": "\nExploring and Visualizing an Open Global Dataset\n", "author": ["Posted by Reena Jana, Creative Lead, Business Inclusion, and Josh Lovejoy, UX Designer, Google Research"], "link": "http://ai.googleblog.com/2017/08/exploring-and-visualizing-open-global.html", "abstract": "", "date": "\nFriday, August 25, 2017\n"},
{"website": "Google-AI", "title": "\nLaunching the Speech Commands Dataset\n", "author": ["Posted by Pete Warden, Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html", "abstract": "", "date": "\nThursday, August 24, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle at KDD\u201917: Graph Mining and Beyond\n", "author": ["Posted by Bryan Perozzi, Research Scientist,  NYC Algorithms and Optimization Team"], "link": "http://ai.googleblog.com/2017/08/google-at-kdd17-graph-mining-and-beyond.html", "abstract": "", "date": "\nWednesday, August 23, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing the NYC Algorithms and Optimization Site\n", "author": ["Posted by Vahab Mirrokni, Principal Research Scientist and Xerxes Dotiwalla, Product Manager, NYC Algorithms and Optimization Team"], "link": "http://ai.googleblog.com/2017/08/announcing-nyc-algorithms-and.html", "abstract": "", "date": "\nMonday, August 21, 2017\n"},
{"website": "Google-AI", "title": "\nMaking Visible Watermarks More Effective\n", "author": ["Posted by Tali Dekel and Michael Rubinstein, Research Scientists"], "link": "http://ai.googleblog.com/2017/08/making-visible-watermarks-more-effective.html", "abstract": "", "date": "\nThursday, August 17, 2017\n"},
{"website": "Google-AI", "title": "\nHarness the Power of Machine Learning in Your Browser with Deeplearn.js\n", "author": ["Posted by Nikhil Thorat and Daniel Smilkov, Software Engineers, Google Big Picture Team"], "link": "http://ai.googleblog.com/2017/08/harness-power-of-machine-learning-in.html", "abstract": "", "date": "\nFriday, August 11, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle at ICML 2017\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Research Communications", "\n"], "link": "http://ai.googleblog.com/2017/08/google-at-icml-2017.html", "abstract": "", "date": "\nSunday, August 6, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle at ACL 2017\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Research Communications"], "link": "http://ai.googleblog.com/2017/07/google-at-acl-2017.html", "abstract": "", "date": "\nSunday, July 30, 2017\n"},
{"website": "Google-AI", "title": "\nExpressions in Virtual Reality\n", "author": ["Posted by Steven Hickson, Software Engineering Intern, and Nick Dufour, Avneesh Sud, Software Engineers, Machine Perception"], "link": "http://ai.googleblog.com/2017/07/expressions-in-virtual-reality.html", "abstract": "", "date": "\nFriday, July 28, 2017\n"},
{"website": "Google-AI", "title": "\nSo there I was, firing a megawatt plasma collider at work...\n", "author": ["Posted by Ted Baltz, Senior Staff Software Engineer, Google Accelerated Science Team"], "link": "http://ai.googleblog.com/2017/07/so-there-i-was-firing-megawatt-plasma.html", "abstract": "", "date": "\nTuesday, July 25, 2017\n"},
{"website": "Google-AI", "title": "\nTeaching Robots to Understand Semantic Concepts\n", "author": ["Posted by Sergey Levine, Faculty Advisor and Pierre Sermanet, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/07/teaching-robots-to-understand-semantic.html", "abstract": "", "date": "\nFriday, July 21, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle at CVPR 2017\n", "author": ["Posted by Christian Howard, Editor-in-Chief, Research Communications"], "link": "http://ai.googleblog.com/2017/07/google-at-cvpr-2017.html", "abstract": "", "date": "\nFriday, July 21, 2017\n"},
{"website": "Google-AI", "title": "\nAn Update to Open Images - Now with Bounding-Boxes\n", "author": ["Posted by Vittorio Ferrari, Research Scientist, Machine Perception"], "link": "http://ai.googleblog.com/2017/07/an-update-to-open-images-now-with.html", "abstract": "", "date": "\nThursday, July 20, 2017\n"},
{"website": "Google-AI", "title": "\nMotion Stills \u2014 Now on Android\n", "author": ["Posted by Karthik Raveendran and Suril Shah, Software Engineers, Google Research"], "link": "http://ai.googleblog.com/2017/07/motion-stills-now-on-android.html", "abstract": "", "date": "\nThursday, July 20, 2017\n"},
{"website": "Google-AI", "title": "\nFacets: An Open Source Visualization Tool for Machine Learning Training Data\n", "author": ["Posted by James Wexler, Senior Software Engineer, Google Big Picture Team"], "link": "http://ai.googleblog.com/2017/07/facets-open-source-visualization-tool.html", "abstract": "", "date": "\nMonday, July 17, 2017\n"},
{"website": "Google-AI", "title": "\nUsing Deep Learning to Create Professional-Level Photographs\n", "author": ["Posted by Hui Fang, Software Engineer, Machine Perception"], "link": "http://ai.googleblog.com/2017/07/using-deep-learning-to-create.html", "abstract": "", "date": "\nThursday, July 13, 2017\n"},
{"website": "Google-AI", "title": "\nBuilding Your Own Neural Machine Translation System in TensorFlow\n", "author": ["Posted by Thang Luong, Research Scientist, and Eugene Brevdo, Staff Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2017/07/building-your-own-neural-machine.html", "abstract": "", "date": "\nWednesday, July 12, 2017\n"},
{"website": "Google-AI", "title": "\nRevisiting the Unreasonable Effectiveness of Data\n", "author": ["Posted by Abhinav Gupta, Faculty Advisor, Machine Perception"], "link": "http://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html", "abstract": "", "date": "\nTuesday, July 11, 2017\n"},
{"website": "Google-AI", "title": "\nThe Google Brain Residency Program \u2014 One Year Later\n", "author": ["Posted by Luke Metz, Research Associate and Yun Liu, Software Engineer, 2016 Google Brain Resident Alumni"], "link": "http://ai.googleblog.com/2017/07/the-google-brain-residency-program-one.html", "abstract": "", "date": "\nMonday, July 10, 2017\n"},
{"website": "Google-AI", "title": "\nMultiModel: Multi-Task Machine Learning Across Domains\n", "author": ["Posted by \u0141ukasz Kaiser, Senior Research Scientist, Google Brain Team and Aidan N. Gomez, Researcher, Department of Computer Science Machine Learning Group, University of Toronto"], "link": "http://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learning.html", "abstract": "", "date": "\nWednesday, June 21, 2017\n"},
{"website": "Google-AI", "title": "\nAccelerating Deep Learning Research with the Tensor2Tensor Library\n", "author": ["Posted by \u0141ukasz Kaiser, Senior Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html", "abstract": "", "date": "\nMonday, June 19, 2017\n"},
{"website": "Google-AI", "title": "\nSupercharge your Computer Vision models with the TensorFlow Object Detection API\n", "author": ["Posted by Jonathan Huang, Research Scientist and Vivek Rathod, Software Engineer"], "link": "http://ai.googleblog.com/2017/06/supercharge-your-computer-vision-models.html", "abstract": "", "date": "\nThursday, June 15, 2017\n"},
{"website": "Google-AI", "title": "\nMobileNets: Open-Source Models for Efficient On-Device Vision\n", "author": ["Posted by Andrew G. Howard, Senior Software Engineer and Menglong Zhu, Software Engineer"], "link": "http://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html", "abstract": "", "date": "\nWednesday, June 14, 2017\n"},
{"website": "Google-AI", "title": "\nThe Machine Intelligence Behind Gboard\n", "author": ["Posted by Fran\u00e7oise Beaufays, Principal Scientist, Speech and Keyboard Team and Michael Riley, Principal Scientist, Speech and Languages Algorithms Team"], "link": "http://ai.googleblog.com/2017/05/the-machine-intelligence-behind-gboard.html", "abstract": "", "date": "\nWednesday, May 24, 2017\n"},
{"website": "Google-AI", "title": "\nIntroducing the TensorFlow Research Cloud\n", "author": ["Posted by Zak Stone, Product Manager for TensorFlow"], "link": "http://ai.googleblog.com/2017/05/introducing-tensorflow-research-cloud.html", "abstract": "", "date": "\nWednesday, May 17, 2017\n"},
{"website": "Google-AI", "title": "\nEfficient Smart Reply, now for Gmail\n", "author": ["Posted by Brian Strope, Research Scientist, and Ray Kurzweil, Engineering Director, Google Research"], "link": "http://ai.googleblog.com/2017/05/efficient-smart-reply-now-for-gmail.html", "abstract": "", "date": "\nWednesday, May 17, 2017\n"},
{"website": "Google-AI", "title": "\nUsing Machine Learning to Explore Neural Network Architecture\n", "author": ["Posted by Quoc Le & Barret Zoph, Research Scientists, Google Brain team"], "link": "http://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html", "abstract": "", "date": "\nWednesday, May 17, 2017\n"},
{"website": "Google-AI", "title": "\nCoarse Discourse: A Dataset for Understanding Online Discussions\n", "author": ["Posted by Praveen Paritosh, Senior Research Scientist, Ka Wong, Senior Data Scientist"], "link": "http://ai.googleblog.com/2017/05/coarse-discourse-dataset-for.html", "abstract": "", "date": "\nTuesday, May 16, 2017\n"},
{"website": "Google-AI", "title": "\nNeural Network-Generated Illustrations in Allo\n", "author": ["Posted by Jennifer Daniel, Expressions Creative Director, Allo "], "link": "http://ai.googleblog.com/2017/05/neural-network-generated-illustrations.html", "abstract": "", "date": "\nThursday, May 11, 2017\n"},
{"website": "Google-AI", "title": "\nUpdating Google Maps with Deep Learning and Street View\n", "author": ["Posted by Julian Ibarz, Staff Software Engineer, Google Brain Team and Sujoy Banerjee, Product Manager, Ground Truth Team"], "link": "http://ai.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html", "abstract": "", "date": "\nWednesday, May 3, 2017\n"},
{"website": "Google-AI", "title": "\nExperimental Nighttime Photography with Nexus and Pixel\n", "author": ["Posted by Florian Kainz, Software Engineer, Google Daydream"], "link": "http://ai.googleblog.com/2017/04/experimental-nighttime-photography-with.html", "abstract": "", "date": "\nTuesday, April 25, 2017\n"},
{"website": "Google-AI", "title": "\nResearch at Google and ICLR 2017\n", "author": ["Posted by Ian Goodfellow, Staff Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/04/research-at-google-and-iclr-2017.html", "abstract": "", "date": "\nMonday, April 24, 2017\n"},
{"website": "Google-AI", "title": "\nPhotoScan: Taking Glare-Free Pictures of Pictures\n", "author": ["Posted by Ce Liu, Michael Rubinstein, Mike Krainin and Bill Freeman, Research Scientists"], "link": "http://ai.googleblog.com/2017/04/photoscan-taking-glare-free-pictures-of.html", "abstract": "", "date": "\nThursday, April 20, 2017\n"},
{"website": "Google-AI", "title": "\nTeaching Machines to Draw\n", "author": ["Posted by David Ha, Google Brain Resident"], "link": "http://ai.googleblog.com/2017/04/teaching-machines-to-draw.html", "abstract": "", "date": "\nThursday, April 13, 2017\n"},
{"website": "Google-AI", "title": "\nIntroducing tf-seq2seq: An Open Source Sequence-to-Sequence Framework in TensorFlow\n", "author": ["Posted by Anna Goldie and Denny Britz, Research Software Engineer and Google Brain Resident, Google Brain Team"], "link": "http://ai.googleblog.com/2017/04/introducing-tf-seq2seq-open-source.html", "abstract": "", "date": "\nTuesday, April 11, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2017 Google PhD Fellows for North America, Europe and the Middle East\n", "author": ["Posted by Michael Rennaker, Program Manager"], "link": "http://ai.googleblog.com/2017/04/announcing-2017-google-phd-fellows-for.html", "abstract": "", "date": "\nMonday, April 10, 2017\n"},
{"website": "Google-AI", "title": "\nPredicting Properties of Molecules with Machine Learning\n", "author": ["Posted by George Dahl, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/04/predicting-properties-of-molecules-with.html", "abstract": "", "date": "\nFriday, April 7, 2017\n"},
{"website": "Google-AI", "title": "\nFederated Learning: Collaborative Machine Learning without Centralized Training Data \n", "author": ["Posted by Brendan McMahan and Daniel Ramage, Research Scientists"], "link": "http://ai.googleblog.com/2017/04/federated-learning-collaborative.html", "abstract": "", "date": "\nThursday, April 6, 2017\n"},
{"website": "Google-AI", "title": "\nKeeping fake listings off Google Maps\n", "author": ["Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security & Anti-Abuse Research"], "link": "http://ai.googleblog.com/2017/04/keeping-fake-listings-off-google-maps.html", "abstract": "", "date": "\nThursday, April 6, 2017\n"},
{"website": "Google-AI", "title": "\nAnd the award goes to...\n", "author": ["Posted by Evgeniy Gabrilovich, Senior Staff Research Scientist, Google Research, and WWW 2017 Technical Program Co-Chair"], "link": "http://ai.googleblog.com/2017/04/and-award-goes-to.html", "abstract": "", "date": "\nWednesday, April 5, 2017\n"},
{"website": "Google-AI", "title": "\nConsistent Hashing with Bounded Loads\n", "author": ["Posted by Vahab Mirrokni, Principal Scientist, Morteza Zadimoghaddam, Research Scientist, NYC Algorithms Team"], "link": "http://ai.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html", "abstract": "", "date": "\nMonday, April 3, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing AudioSet: A Dataset for Audio Event Research\n", "author": ["Posted by Dan Ellis, Research Scientist, Sound Understanding Team"], "link": "http://ai.googleblog.com/2017/03/announcing-audioset-dataset-for-audio.html", "abstract": "", "date": "\nThursday, March 30, 2017\n"},
{"website": "Google-AI", "title": "\nAdding Sound Effect Information to YouTube Captions\n", "author": ["Posted by Sourish Chaudhuri, Software Engineer, Sound Understanding"], "link": "http://ai.googleblog.com/2017/03/adding-sound-effect-information-to.html", "abstract": "", "date": "\nThursday, March 23, 2017\n"},
{"website": "Google-AI", "title": "\nDistill: Supporting Clarity in Machine Learning\n", "author": ["Posted by Shan Carter, Software Engineer and Chris Olah, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2017/03/distill-supporting-clarity-in-machine.html", "abstract": "", "date": "\nMonday, March 20, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing Guetzli: A New Open Source JPEG Encoder\n", "author": ["Posted by Robert Obryk and Jyrki Alakuijala, Software Engineers, Google Research Europe"], "link": "http://ai.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html", "abstract": "", "date": "\nThursday, March 16, 2017\n"},
{"website": "Google-AI", "title": "\nAn Upgrade to SyntaxNet, New Models and a Parsing Competition\n", "author": ["Posted by David Weiss and Slav Petrov, Research Scientists"], "link": "http://ai.googleblog.com/2017/03/an-upgrade-to-syntaxnet-new-models-and.html", "abstract": "", "date": "\nWednesday, March 15, 2017\n"},
{"website": "Google-AI", "title": "\nQuick Access in Drive: Using Machine Learning to Save You Time\n", "author": ["Posted by Sandeep Tata, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2017/03/quick-access-in-drive-using-machine.html", "abstract": "", "date": "\nFriday, March 10, 2017\n"},
{"website": "Google-AI", "title": "\nAssisting Pathologists in Detecting Cancer with Deep Learning\n", "author": ["Posted by Martin Stumpe, Technical Lead, and Lily Peng, Product Manager"], "link": "http://ai.googleblog.com/2017/03/assisting-pathologists-in-detecting.html", "abstract": "", "date": "\nFriday, March 3, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards 2016\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2017/02/google-research-awards-2016.html", "abstract": "", "date": "\nThursday, February 23, 2017\n"},
{"website": "Google-AI", "title": "\nPreprocessing for Machine Learning with tf.Transform\n", "author": ["Posted by Kester Tong, David Soergel, and Gus Katsiapis, Software Engineers"], "link": "http://ai.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html", "abstract": "", "date": "\nWednesday, February 22, 2017\n"},
{"website": "Google-AI", "title": "\nHeadset \u201cRemoval\u201d for Virtual and Mixed Reality\n", "author": ["Posted by Vivek Kwatra, Research Scientist and Christian Frueh, Avneesh Sud, Software Engineers"], "link": "http://ai.googleblog.com/2017/02/headset-removal-for-virtual-and-mixed.html", "abstract": "", "date": "\nTuesday, February 21, 2017\n"},
{"website": "Google-AI", "title": "\nThe CS Capacity Program - New Tools and SIGCSE 2017\n", "author": ["Posted by Chris Stephenson, Head of Computer Science Education Strategy"], "link": "http://ai.googleblog.com/2017/02/the-cs-capacity-program-new-tools-and.html", "abstract": "", "date": "\nThursday, February 16, 2017\n"},
{"website": "Google-AI", "title": "\nAn updated YouTube-8M, a video understanding challenge, and a CVPR workshop. Oh my!\n", "author": ["Posted by Paul Natsev, Software Engineer"], "link": "http://ai.googleblog.com/2017/02/an-updated-youtube-8m-video.html", "abstract": "", "date": "\nWednesday, February 15, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing TensorFlow 1.0\n", "author": ["Posted by Amy McDonald Sandjideh, Technical Program Manager, TensorFlow"], "link": "http://ai.googleblog.com/2017/02/announcing-tensorflow-10.html", "abstract": "", "date": "\nWednesday, February 15, 2017\n"},
{"website": "Google-AI", "title": "\nOn-Device Machine Intelligence\n", "author": ["Posted by Sujith Ravi, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2017/02/on-device-machine-intelligence.html", "abstract": "", "date": "\nThursday, February 9, 2017\n"},
{"website": "Google-AI", "title": "\nAnnouncing TensorFlow Fold: Deep Learning With Dynamic Computation Graphs\n", "author": ["Posted by Moshe Looks, Marcello Herreshoff and DeLesley Hutchins, Software Engineers"], "link": "http://ai.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html", "abstract": "", "date": "\nTuesday, February 7, 2017\n"},
{"website": "Google-AI", "title": "\nAdvancing Research on Video Understanding with the YouTube-BoundingBoxes Dataset\n", "author": ["Posted by Esteban Real, Vincent Vanhoucke, Jonathon Shlens, Google Brain Team and", "\nStefano Mazzocchi, Google Research"], "link": "http://ai.googleblog.com/2017/02/advancing-research-on-video.html", "abstract": "", "date": "\nMonday, February 6, 2017\n"},
{"website": "Google-AI", "title": "\nUsing Machine Learning to Predict Parking Difficulty\n", "author": ["Posted by James Cook, Yechen Li, Software Engineers and Ravi Kumar, Research Scientist"], "link": "http://ai.googleblog.com/2017/02/using-machine-learning-to-predict.html", "abstract": "", "date": "\nFriday, February 3, 2017\n"},
{"website": "Google-AI", "title": "\nApp Discovery with Google Play, Part 3: Machine Learning to Fight Spam and Abuse at Scale\n", "author": ["Posted by Hsu-Chieh Lee, Xing Chen, Software Engineers, and Qian An, Analyst"], "link": "http://ai.googleblog.com/2017/01/app-discovery-with-google-play-part-3.html", "abstract": "", "date": "\nMonday, January 30, 2017\n"},
{"website": "Google-AI", "title": "\nFacilitating the discovery of public datasets\n", "author": ["Posted by Natasha Noy, Google Research and Dan Brickley, Open Source Programs Office", "\n"], "link": "http://ai.googleblog.com/2017/01/facilitating-discovery-of-public.html", "abstract": "", "date": "\nTuesday, January 24, 2017\n"},
{"website": "Google-AI", "title": "\nA Large Corpus for Supervised Word-Sense Disambiguation\n", "author": ["Posted by Colin Evans and Dayu Yuan, Software Engineers"], "link": "http://ai.googleblog.com/2017/01/a-large-corpus-for-supervised-word.html", "abstract": "", "date": "\nWednesday, January 18, 2017\n"},
{"website": "Google-AI", "title": "\nThe Google Brain Team \u2014 Looking Back on 2016\n", "author": ["Posted by Jeff Dean, Google Senior Fellow, on behalf of the entire Google Brain Team"], "link": "http://ai.googleblog.com/2017/01/the-google-brain-team-looking-back-on.html", "abstract": "", "date": "\nThursday, January 12, 2017\n"},
{"website": "Google-AI", "title": "\nGoogle Brain Residency Program - 7 months in and looking ahead\n", "author": ["Posted by Jeff Dean, Google Senior Fellow and Leslie Phillips, Google Brain Residency Program Manager"], "link": "http://ai.googleblog.com/2017/01/google-brain-residency-program-7-months.html", "abstract": "", "date": "\nThursday, January 5, 2017\n"},
{"website": "Google-AI", "title": "\nGet moving with the new Motion Stills\n", "author": ["Posted by Matthias Grundmann and Ken Conley, Machine Perception"], "link": "http://ai.googleblog.com/2016/12/get-moving-with-new-motion-stills.html", "abstract": "", "date": "\nThursday, December 15, 2016\n"},
{"website": "Google-AI", "title": "\nApp Discovery with Google Play, Part 2: Personalized Recommendations with Related Apps\n", "author": ["Posted by Ananth Balashankar & Levent Koc, Software Engineers, and Norberto Guimaraes, Product Manager"], "link": "http://ai.googleblog.com/2016/12/app-discovery-with-google-play-part-2.html", "abstract": "", "date": "\nWednesday, December 14, 2016\n"},
{"website": "Google-AI", "title": "\nOpen sourcing the Embedding Projector: a tool for visualizing high dimensional data\n", "author": ["Posted by Daniel Smilkov and the Big Picture group"], "link": "http://ai.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html", "abstract": "", "date": "\nWednesday, December 7, 2016\n"},
{"website": "Google-AI", "title": "\nNIPS 2016 & Research at Google\n", "author": ["Posted by Doug Eck, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2016/12/nips-2016-research-at-google.html", "abstract": "", "date": "\nSunday, December 4, 2016\n"},
{"website": "Google-AI", "title": "\nDeep Learning for Detection of Diabetic Eye Disease\n", "author": ["Posted by Lily Peng MD PhD, Product Manager and Varun Gulshan PhD, Research Engineer"], "link": "http://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html", "abstract": "", "date": "\nTuesday, November 29, 2016\n"},
{"website": "Google-AI", "title": "\nZero-Shot Translation with Google\u2019s Multilingual Neural Machine Translation System\n", "author": ["Posted by Mike Schuster (Google Brain Team), Melvin Johnson (Google Translate) and Nikhil Thorat (Google Brain Team)"], "link": "http://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html", "abstract": "", "date": "\nTuesday, November 22, 2016\n"},
{"website": "Google-AI", "title": "\nEnhance! RAISR Sharp Images with Machine Learning\n", "author": ["Posted by Peyman Milanfar, Research Scientist"], "link": "http://ai.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html", "abstract": "", "date": "\nMonday, November 14, 2016\n"},
{"website": "Google-AI", "title": "\nOpen Source Visualization of GPS Displacements for Earthquake Cycle Physics\n", "author": ["Posted by Jimbo Wilson, Software Engineer, Google Big Picture Team and Brendan Meade, Professor, Harvard Department of Earth and Planetary Sciences"], "link": "http://ai.googleblog.com/2016/11/open-source-visualization-of-gps.html", "abstract": "", "date": "\nThursday, November 10, 2016\n"},
{"website": "Google-AI", "title": "\nCelebrating TensorFlow\u2019s First Year\n", "author": ["Posted by Zak Stone, Product Manager for TensorFlow, on behalf of the TensorFlow team"], "link": "http://ai.googleblog.com/2016/11/celebrating-tensorflows-first-year.html", "abstract": "", "date": "\nWednesday, November 9, 2016\n"},
{"website": "Google-AI", "title": "\nApp Discovery with Google Play, Part 1: Understanding Topics\n", "author": ["Posted by Malay Haldar, Matt MacMahon, Neha Jha and Raj Arasu, Software Engineers"], "link": "http://ai.googleblog.com/2016/11/app-discovery-with-google-play-part-1.html", "abstract": "", "date": "\nTuesday, November 8, 2016\n"},
{"website": "Google-AI", "title": "\nResearch suggestions at your fingertips with Explore in Docs\n", "author": ["Posted by Kishore Papineni, Research Scientist, Google Research NY"], "link": "http://ai.googleblog.com/2016/11/research-suggestions-at-your-fingertips.html", "abstract": "", "date": "\nTuesday, November 1, 2016\n"},
{"website": "Google-AI", "title": "\nSupercharging Style Transfer\n", "author": ["Posted by Vincent Dumoulin", ", Jonathon Shlens and Manjunath Kudlur, Google Brain Team"], "link": "http://ai.googleblog.com/2016/10/supercharging-style-transfer.html", "abstract": "", "date": "\nWednesday, October 26, 2016\n"},
{"website": "Google-AI", "title": "\nCourse Builder now supports scheduling, easier customization and more\n", "author": ["Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"], "link": "http://ai.googleblog.com/2016/10/course-builder-now-supports-scheduling.html", "abstract": "", "date": "\nMonday, October 17, 2016\n"},
{"website": "Google-AI", "title": "\nEquality of Opportunity in Machine Learning\n", "author": ["Posted by Moritz Hardt, Research Scientist, Google Brain Team"], "link": "http://ai.googleblog.com/2016/10/equality-of-opportunity-in-machine.html", "abstract": "", "date": "\nFriday, October 7, 2016\n"},
{"website": "Google-AI", "title": "\nGraph-powered Machine Learning at Google\n", "author": ["Posted by Sujith Ravi, Staff Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2016/10/graph-powered-machine-learning-at-google.html", "abstract": "", "date": "\nThursday, October 6, 2016\n"},
{"website": "Google-AI", "title": "\nHow Robots Can Acquire New Skills from Their Shared Experience\n", "author": ["Posted by Sergey Levine (Google Brain Team), Timothy Lillicrap (DeepMind), Mrinal Kalakrishnan (X)"], "link": "http://ai.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html", "abstract": "", "date": "\nMonday, October 3, 2016\n"},
{"website": "Google-AI", "title": "\nIntroducing the Open Images Dataset\n", "author": ["Posted by Ivan Krasin and Tom Duerig, Software Engineers"], "link": "http://ai.googleblog.com/2016/09/introducing-open-images-dataset.html", "abstract": "", "date": "\nFriday, September 30, 2016\n"},
{"website": "Google-AI", "title": "\nImage Compression with Neural Networks\n", "author": ["Posted by Nick Johnston and David Minnen, Software Engineers"], "link": "http://ai.googleblog.com/2016/09/image-compression-with-neural-networks.html", "abstract": "", "date": "\nThursday, September 29, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing YouTube-8M: A Large and Diverse Labeled Video Dataset for Video Understanding Research\n", "author": ["Posted by Sudheendra Vijayanarasimhan and Paul Natsev, Software Engineers"], "link": "http://ai.googleblog.com/2016/09/announcing-youtube-8m-large-and-diverse.html", "abstract": "", "date": "\nWednesday, September 28, 2016\n"},
{"website": "Google-AI", "title": "\nA Neural Network for Machine Translation, at Production Scale\n", "author": ["Posted by Quoc V. Le & Mike Schuster, Research Scientists, Google Brain Team"], "link": "http://ai.googleblog.com/2016/09/a-neural-network-for-machine.html", "abstract": "", "date": "\nTuesday, September 27, 2016\n"},
{"website": "Google-AI", "title": "\nShow and Tell: image captioning open sourced in TensorFlow\n", "author": ["Posted by Chris Shallue, Software Engineer, Google Brain Team"], "link": "http://ai.googleblog.com/2016/09/show-and-tell-image-captioning-open.html", "abstract": "", "date": "\nThursday, September 22, 2016\n"},
{"website": "Google-AI", "title": "\nThe 280-Year-Old Algorithm Inside Google Trips\n", "author": ["Posted by Bogdan Arsintescu, Software Engineer & Sreenivas Gollapudi, Kostas Kollias, Tamas Sarlos and Andrew Tomkins, Research Scientists"], "link": "http://ai.googleblog.com/2016/09/the-280-year-old-algorithm-inside.html", "abstract": "", "date": "\nTuesday, September 20, 2016\n"},
{"website": "Google-AI", "title": "\nThe 2016 Google Earth Engine User Summit: Turning pixels into insights\n", "author": ["Posted by Chris Herwig, Program Manager, Google Earth Engine"], "link": "http://ai.googleblog.com/2016/09/the-2016-google-earth-engine-user.html", "abstract": "", "date": "\nMonday, September 19, 2016\n"},
{"website": "Google-AI", "title": "\nResearch from VLDB 2016: Improved Friend Suggestion using Ego-Net Analysis\n", "author": ["Posted by Alessandro Epasto, Research Scientist, Google Research NY"], "link": "http://ai.googleblog.com/2016/09/research-from-vldb-2016-improved-friend.html", "abstract": "", "date": "\nThursday, September 15, 2016\n"},
{"website": "Google-AI", "title": "\nComputational Thinking from a Dispositions Perspective\n", "author": ["Posted by Chris Stephenson, Head of Computer Science Education Programs at Google, and Joyce Malyn-Smith, Managing Project Director at Education Development Center (EDC)"], "link": "http://ai.googleblog.com/2016/09/computational-thinking-from.html", "abstract": "", "date": "\nTuesday, September 13, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing the First Annual Global PhD Fellowship Summit and the 2016 Google PhD Fellows\n", "author": ["Posted by Michael Rennaker, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2016/09/announcing-first-annual-global-phd.html", "abstract": "", "date": "\nWednesday, September 7, 2016\n"},
{"website": "Google-AI", "title": "\nReproducible Science: Cancer Researchers Embrace Containers in the Cloud\n", "author": ["Posted by Dr. Kyle Ellrott, Oregon Health and Sciences University, Dr. Josh Stuart, University of California Santa Cruz, and Dr. Paul Boutros, Ontario Institute for Cancer Research"], "link": "http://ai.googleblog.com/2016/09/reproducible-science-cancer-researchers.html", "abstract": "", "date": "\nTuesday, September 6, 2016\n"},
{"website": "Google-AI", "title": "\nImproving Inception and Image Classification in TensorFlow\n", "author": ["Posted by Alex Alemi, Software Engineer"], "link": "http://ai.googleblog.com/2016/08/improving-inception-and-image.html", "abstract": "", "date": "\nWednesday, August 31, 2016\n"},
{"website": "Google-AI", "title": "\nTF-Slim: A high level library to define complex models in TensorFlow\n", "author": ["Posted by Nathan Silberman and Sergio Guadarrama, Google Research"], "link": "http://ai.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html", "abstract": "", "date": "\nTuesday, August 30, 2016\n"},
{"website": "Google-AI", "title": "\nText summarization with TensorFlow\n", "author": ["Posted by Peter Liu and Xin Pan, Software Engineers, Google Brain Team"], "link": "http://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html", "abstract": "", "date": "\nWednesday, August 24, 2016\n"},
{"website": "Google-AI", "title": "\nMeet Parsey\u2019s Cousins: Syntax for 40 languages, plus new SyntaxNet capabilities\n", "author": ["Posted by Chris Alberti, Dave Orr & Slav Petrov, Google Natural Language Understanding Team"], "link": "http://ai.googleblog.com/2016/08/meet-parseys-cousins-syntax-for-40.html", "abstract": "", "date": "\nMonday, August 8, 2016\n"},
{"website": "Google-AI", "title": "\nACL 2016 & Research at Google\n", "author": ["Posted by Slav Petrov, Research Scientist"], "link": "http://ai.googleblog.com/2016/08/acl-2016-research-at-google.html", "abstract": "", "date": "\nSunday, August 7, 2016\n"},
{"website": "Google-AI", "title": "\nComputational Thinking for All Students\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2016/08/computational-thinking-for-all-students.html", "abstract": "", "date": "\nWednesday, August 3, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing an Open Source ADC board for BeagleBone\n", "author": ["Posted by Jason Holt, Software Engineer"], "link": "http://ai.googleblog.com/2016/07/announcing-open-source-adc-board-for.html", "abstract": "", "date": "\nWednesday, July 20, 2016\n"},
{"website": "Google-AI", "title": "\nTowards an exact (quantum) description of chemistry\n", "author": ["Posted by Ryan Babbush, Quantum Software Engineer"], "link": "http://ai.googleblog.com/2016/07/towards-exact-quantum-description-of.html", "abstract": "", "date": "\nMonday, July 18, 2016\n"},
{"website": "Google-AI", "title": "\nWide & Deep Learning: Better Together with TensorFlow\n", "author": ["Posted by Heng-Tze Cheng, Senior Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html", "abstract": "", "date": "\nWednesday, June 29, 2016\n"},
{"website": "Google-AI", "title": "\nCVPR 2016 & Research at Google\n", "author": ["Posted by Rahul Sukthankar, Research Scientist"], "link": "http://ai.googleblog.com/2016/06/cvpr-2016-research-at-google.html", "abstract": "", "date": "\nTuesday, June 28, 2016\n"},
{"website": "Google-AI", "title": "\nProject Bloks: Making code physical for kids\n", "author": ["Posted by Steve Vranakis and Jayme Goldstein, Executive Creative Director and Project Lead, Google Creative Lab"], "link": "http://ai.googleblog.com/2016/06/project-bloks-making-code-physical-for.html", "abstract": "", "date": "\nMonday, June 27, 2016\n"},
{"website": "Google-AI", "title": "\nBringing Precision to the AI Safety Discussion\n", "author": ["Posted by Chris Olah, Google Research"], "link": "http://ai.googleblog.com/2016/06/bringing-precision-to-ai-safety.html", "abstract": "", "date": "\nTuesday, June 21, 2016\n"},
{"website": "Google-AI", "title": "\nICML 2016 & Research at Google\n", "author": ["Posted by Afshin Rostamizadeh, Research Scientist"], "link": "http://ai.googleblog.com/2016/06/icml-2016-research-at-google.html", "abstract": "", "date": "\nMonday, June 20, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing Google Research, Europe\n", "author": ["Posted by Emmanuel Mogenet, Head of Google Research, Europe"], "link": "http://ai.googleblog.com/2016/06/announcing-google-research-europe.html", "abstract": "", "date": "\nThursday, June 16, 2016\n"},
{"website": "Google-AI", "title": "\nQuantum annealing with a digital twist\n", "author": ["Posted by Rami Barends and Alireza Shabani, Quantum Electronics Engineers"], "link": "http://ai.googleblog.com/2016/06/quantum-annealing-with-digital-twist.html", "abstract": "", "date": "\nWednesday, June 8, 2016\n"},
{"website": "Google-AI", "title": "\nMotion Stills \u2013 Create beautiful GIFs from Live Photos\n", "author": ["Posted by Ken Conley and Matthias Grundmann, Machine Perception"], "link": "http://ai.googleblog.com/2016/06/motion-stills-create-beautiful-gifs.html", "abstract": "", "date": "\nTuesday, June 7, 2016\n"},
{"website": "Google-AI", "title": "\n\"Aw, so cute!\": Allo helps you respond to shared photos\n", "author": ["by Ariel Fuxman, Research Scientist"], "link": "http://ai.googleblog.com/2016/05/so-cute-allo-helps-you-respond-to.html", "abstract": "", "date": "\nWednesday, May 18, 2016\n"},
{"website": "Google-AI", "title": "\nChat Smarter with Allo\n", "author": ["Posted by Pranav Khaitan, Google Research"], "link": "http://ai.googleblog.com/2016/05/chat-smarter-with-allo.html", "abstract": "", "date": "\nWednesday, May 18, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing SyntaxNet: The World\u2019s Most Accurate Parser Goes Open Source\n", "author": ["Posted by Slav Petrov, Senior Staff Research Scientist"], "link": "http://ai.googleblog.com/2016/05/announcing-syntaxnet-worlds-most.html", "abstract": "", "date": "\nThursday, May 12, 2016\n"},
{"website": "Google-AI", "title": "\nResearch at Google and ICLR 2016\n", "author": ["Posted by Dumitru Erhan, Gentleman Scientist"], "link": "http://ai.googleblog.com/2016/05/research-at-google-and-iclr-2016.html", "abstract": "", "date": "\nSunday, May 1, 2016\n"},
{"website": "Google-AI", "title": "\nDeepMind moves to TensorFlow\n", "author": ["Posted by Koray Kavukcuoglu, Research Scientist, Google DeepMind"], "link": "http://ai.googleblog.com/2016/04/deepmind-moves-to-tensorflow.html", "abstract": "", "date": "\nFriday, April 29, 2016\n"},
{"website": "Google-AI", "title": "\nComputer Science Education for All Students\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2016/04/computer-science-education-for-all.html", "abstract": "", "date": "\nTuesday, April 26, 2016\n"},
{"website": "Google-AI", "title": "\nHelping webmasters re-secure their sites\n", "author": ["Posted by Kurt Thomas and Yuan Niu, Spam & Abuse Research"], "link": "http://ai.googleblog.com/2016/04/helping-webmasters-re-secure-their-sites.html", "abstract": "", "date": "\nMonday, April 18, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing TensorFlow 0.8 \u2013 now with distributed computing support!\n", "author": ["Posted by Derek Murray, Software Engineer"], "link": "http://ai.googleblog.com/2016/04/announcing-tensorflow-08-now-with.html", "abstract": "", "date": "\nWednesday, April 13, 2016\n"},
{"website": "Google-AI", "title": "\nAll of Google\u2019s CS Education Programs and Tools in One Place \n", "author": ["Posted by Chris Stephenson, Head of Computer Science Education Programs"], "link": "http://ai.googleblog.com/2016/04/all-of-googles-cs-education-programs.html", "abstract": "", "date": "\nTuesday, April 12, 2016\n"},
{"website": "Google-AI", "title": "\nGenomic Data Processing on Google Cloud Platform\n", "author": ["Posted by Dr. Stacey Gabriel, Director of the Genomics Platform at the Broad Institute of MIT and Harvard"], "link": "http://ai.googleblog.com/2016/04/genomic-data-processing-on-google-cloud.html", "abstract": "", "date": "\nTuesday, April 5, 2016\n"},
{"website": "Google-AI", "title": "\nLessons learned while protecting Gmail\n", "author": ["Posted by Elie Bursztein - anti-abuse & security research,  Nicolas Lidzborski - Gmail security engineering, and Vijay Eranti - Gmail anti-abuse engineering"], "link": "http://ai.googleblog.com/2016/03/lessons-learned-while-protecting-gmail.html", "abstract": "", "date": "\nTuesday, March 29, 2016\n"},
{"website": "Google-AI", "title": "\nMachine Learning in the Cloud, with TensorFlow\n", "author": ["Posted by Slaven Bilac, Software Engineer, Google Research"], "link": "http://ai.googleblog.com/2016/03/machine-learning-in-cloud-with.html", "abstract": "", "date": "\nWednesday, March 23, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2016 Google PhD Fellows for North America, Europe and the Middle East\n", "author": ["Posted by Michael Rennaker, Google PhD Fellowships Lead"], "link": "http://ai.googleblog.com/2016/03/announcing-2016-google-phd-fellows-for.html", "abstract": "", "date": "\nThursday, March 10, 2016\n"},
{"website": "Google-AI", "title": "\nTrain your own image classifier with Inception in TensorFlow\n", "author": ["Posted by Jon Shlens, Senior Research Scientist"], "link": "http://ai.googleblog.com/2016/03/train-your-own-image-classifier-with.html", "abstract": "", "date": "\nWednesday, March 9, 2016\n"},
{"website": "Google-AI", "title": "\nDeep Learning for Robots: Learning from Large-Scale Interaction\n", "author": ["Posted by Sergey Levine, Research Scientist"], "link": "http://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html", "abstract": "", "date": "\nTuesday, March 8, 2016\n"},
{"website": "Google-AI", "title": "\nAn Update on fast Transit Routing with Transfer Patterns\n", "author": ["Arno Eigenwillig, Software Engineer on Google Maps Directions"], "link": "http://ai.googleblog.com/2016/03/an-update-on-fast-transit-routing-with.html", "abstract": "", "date": "\nWednesday, March 2, 2016\n"},
{"website": "Google-AI", "title": "\nAnd the winner of the $1 Million Little Box Challenge is\u2026CE+T Power\u2019s Red Electrical Devils\n", "author": ["Posted by Ross Koningstein, Engineering Director Emeritus, Google Research"], "link": "http://ai.googleblog.com/2016/02/and-winner-of-1-million-little-box.html", "abstract": "", "date": "\nMonday, February 29, 2016\n"},
{"website": "Google-AI", "title": "\nOn the Personalities of Dead Authors\n", "author": ["Posted by Marc Pickett, Software Engineer, Chris Tar, Engineering Manager and Brian Strope, Research Scientist"], "link": "http://ai.googleblog.com/2016/02/on-personalities-of-dead-authors.html", "abstract": "", "date": "\nWednesday, February 24, 2016\n"},
{"website": "Google-AI", "title": "\nGoogle Science Fair 2016: #howcanwe make things better with science?\n", "author": ["Posted by Olivia Hallisey, 2015 Grand Prize winner, Google Science Fair"], "link": "http://ai.googleblog.com/2016/02/google-science-fair-2016-howcanwe-make.html", "abstract": "", "date": "\nTuesday, February 23, 2016\n"},
{"website": "Google-AI", "title": "\nExploring the Intersection of Art and Machine Intelligence\n", "author": ["Posted by Mike Tyka, Software Engineer"], "link": "http://ai.googleblog.com/2016/02/exploring-intersection-of-art-and.html", "abstract": "", "date": "\nMonday, February 22, 2016\n"},
{"website": "Google-AI", "title": "\nText-to-Speech for Low-Resource Languages (Episode 3): But can it say \u201cGoogle\u201d?\n", "author": ["Posted by Martin Jansche, Software Engineer, Google Research for Low Resource Languages"], "link": "http://ai.googleblog.com/2016/02/text-to-speech-for-low-resource.html", "abstract": "", "date": "\nFriday, February 19, 2016\n"},
{"website": "Google-AI", "title": "\nRunning your models in production with TensorFlow Serving\n", "author": ["Posted by Noah Fiedel, Software Engineer"], "link": "http://ai.googleblog.com/2016/02/running-your-models-in-production-with.html", "abstract": "", "date": "\nTuesday, February 16, 2016\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Fall 2015\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2016/02/google-research-awards-fall-2015.html", "abstract": "", "date": "\nFriday, February 12, 2016\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Google Internet of Things (IoT) Technology Research Award Pilot\n", "author": ["Posted Vint Cerf, Chief Internet Evangelist, and Max Senges, Google Research "], "link": "http://ai.googleblog.com/2016/02/announcing-google-internet-of-things.html", "abstract": "", "date": "\nWednesday, February 10, 2016\n"},
{"website": "Google-AI", "title": "\nAlphaGo: Mastering the ancient game of Go with Machine Learning\n", "author": ["Posted by David Silver and Demis Hassabis, Google DeepMind"], "link": "http://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html", "abstract": "", "date": "\nWednesday, January 27, 2016\n"},
{"website": "Google-AI", "title": "\nTeach Yourself Deep Learning with TensorFlow and Udacity\n", "author": ["Posted by Vincent Vanhoucke, Principal Research Scientist"], "link": "http://ai.googleblog.com/2016/01/teach-yourself-deep-learning-with.html", "abstract": "", "date": "\nThursday, January 21, 2016\n"},
{"website": "Google-AI", "title": "\nWhy attend USENIX Enigma?\n", "author": ["Parisa Tabriz, Security Princess & Enigma Program Co-Chair"], "link": "http://ai.googleblog.com/2016/01/why-attend-usenix-enigma.html", "abstract": "", "date": "\nMonday, January 11, 2016\n"},
{"website": "Google-AI", "title": "\nFour years of Schema.org - Recent Progress and Looking Forward\n", "author": ["Posted by Ramanathan Guha, Google Fellow"], "link": "http://ai.googleblog.com/2015/12/four-years-of-schemaorg-recent-progress.html", "abstract": "", "date": "\nThursday, December 17, 2015\n"},
{"website": "Google-AI", "title": "\nText-to-Speech for Low-Resource Languages (Episode 2): Building a Parametric Voice\n", "author": ["Posted by Alexander Gutkin, Google Speech Team"], "link": "http://ai.googleblog.com/2015/12/text-to-speech-for-low-resource.html", "abstract": "", "date": "\nTuesday, December 15, 2015\n"},
{"website": "Google-AI", "title": "\n Making online learning even easier with a re-envisioned Course Builder\n", "author": ["Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"], "link": "http://ai.googleblog.com/2015/12/making-online-learning-even-easier-with.html", "abstract": "", "date": "\nMonday, December 14, 2015\n"},
{"website": "Google-AI", "title": "\nWhen can Quantum Annealing win?\n", "author": ["Posted by Hartmut Neven, Director of Engineering"], "link": "http://ai.googleblog.com/2015/12/when-can-quantum-annealing-win.html", "abstract": "", "date": "\nTuesday, December 8, 2015\n"},
{"website": "Google-AI", "title": "\nHow to Classify Images with TensorFlow\n", "author": ["Posted by Pete Warden, Software Engineer"], "link": "http://ai.googleblog.com/2015/12/how-to-classify-images-with-tensorflow.html", "abstract": "", "date": "\nMonday, December 7, 2015\n"},
{"website": "Google-AI", "title": "\nNIPS 2015 and Machine Learning Research at Google\n", "author": ["Posted by Sanjiv Kumar, Research Scientist"], "link": "http://ai.googleblog.com/2015/12/nips-2015-and-machine-learning-research.html", "abstract": "", "date": "\nSunday, December 6, 2015\n"},
{"website": "Google-AI", "title": "\nTensorFlow - Google\u2019s latest machine learning system, open sourced for everyone\n", "author": ["Posted by Jeff Dean, Senior Google Fellow, and Rajat Monga, Technical Lead"], "link": "http://ai.googleblog.com/2015/11/tensorflow-googles-latest-machine.html", "abstract": "", "date": "\nMonday, November 9, 2015\n"},
{"website": "Google-AI", "title": "\nComputer, respond to this email.\n", "author": ["Posted by Greg Corrado", ", Senior Research Scientist"], "link": "http://ai.googleblog.com/2015/11/computer-respond-to-this-email.html", "abstract": "", "date": "\nTuesday, November 3, 2015\n"},
{"website": "Google-AI", "title": "\nHow to measure translation quality in your user interfaces\n", "author": ["Posted by Javier Bargas-Avila, User Experience Research at Google"], "link": "http://ai.googleblog.com/2015/10/how-to-measure-translation-quality-in.html", "abstract": "", "date": "\nFriday, October 30, 2015\n"},
{"website": "Google-AI", "title": "\nImproving YouTube video thumbnails with deep neural nets\n", "author": ["Posted by Weilong Yang and Min-hsuan Tsai, Video Content Analysis team and the YouTube Creator team"], "link": "http://ai.googleblog.com/2015/10/improving-youtube-video-thumbnails-with.html", "abstract": "", "date": "\nThursday, October 8, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle voice search: faster and more accurate\n", "author": ["Posted by Ha\u015fim Sak, Andrew Senior, Kanishka Rao, Fran\u00e7oise Beaufays and Johan Schalkwyk \u2013 Google Speech Team"], "link": "http://ai.googleblog.com/2015/09/google-voice-search-faster-and-more.html", "abstract": "", "date": "\nThursday, September 24, 2015\n"},
{"website": "Google-AI", "title": "\nA Beginner\u2019s Guide to Deep Neural Networks\n", "author": ["Posted by Natalie Hammel and Lorraine Yurshansky, creators of Nat & Lo\u2019s 20% Project"], "link": "http://ai.googleblog.com/2015/09/a-beginners-guide-to-deep-neural.html", "abstract": "", "date": "\nTuesday, September 22, 2015\n"},
{"website": "Google-AI", "title": "\nInformation sharing for more efficient network utilization and management\n", "author": ["Andreas Terzis, Software Engineer"], "link": "http://ai.googleblog.com/2015/09/information-sharing-for-more-efficient.html", "abstract": "", "date": "\nThursday, September 17, 2015\n"},
{"website": "Google-AI", "title": "\nCrowdsourcing a Text-to-Speech Voice for Low-Resource Languages (Episode 1)\n", "author": ["Posted by Linne Ha, Senior Program Manager, Google Research for Low Resource Languages"], "link": "http://ai.googleblog.com/2015/09/crowdsourcing-text-to-speech-voice-for.html", "abstract": "", "date": "\nTuesday, September 8, 2015\n"},
{"website": "Google-AI", "title": "\nVLDB 2015 and Database Research at Google\n", "author": ["Posted by Corinna Cortes, Head of Google Research NY and Cong Yu, Research Scientist"], "link": "http://ai.googleblog.com/2015/08/vldb-2015-and-database-research-at.html", "abstract": "", "date": "\nMonday, August 31, 2015\n"},
{"website": "Google-AI", "title": "\nAnnouncing Google\u2019s 2015 Global PhD Fellows\n", "author": ["Posted by Michael Rennaker, Google University Relations"], "link": "http://ai.googleblog.com/2015/08/announcing-googles-2015-global-phd.html", "abstract": "", "date": "\nFriday, August 28, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Faculty Research Awards: Summer 2015\n", "author": ["posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2015/08/google-faculty-research-awards-summer.html", "abstract": "", "date": "\nFriday, August 21, 2015\n"},
{"website": "Google-AI", "title": "\nThe Next Chapter for Flu Trends\n", "author": ["Posted by The Flu Trends Team"], "link": "http://ai.googleblog.com/2015/08/the-next-chapter-for-flu-trends.html", "abstract": "", "date": "\nThursday, August 20, 2015\n"},
{"website": "Google-AI", "title": "\nPulling Back the Curtain on Google\u2019s Network Infrastructure\n", "author": ["Posted by Amin Vahdat, Google Fellow"], "link": "http://ai.googleblog.com/2015/08/pulling-back-curtain-on-googles-network.html", "abstract": "", "date": "\nTuesday, August 18, 2015\n"},
{"website": "Google-AI", "title": "\nSay hello to the Enigma conference\n", "author": ["Posted by Elie Bursztein - Anti-abuse team, Parisa Tabriz - Chrome Security and Niels Provos - Security team"], "link": "http://ai.googleblog.com/2015/08/say-hello-to-enigma-conference.html", "abstract": "", "date": "\nTuesday, August 18, 2015\n"},
{"website": "Google-AI", "title": "\nKDD 2015 Best Research Paper Award: \u201cAlgorithms for Public-Private Social Networks\u201d\n", "author": ["Posted by Posted by Corinna Cortes, Head, Google Research NY"], "link": "http://ai.googleblog.com/2015/08/kdd-2015-best-research-paper-award.html", "abstract": "", "date": "\nMonday, August 17, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle\u2019s Course Builder 1.9 improves instructor experience and takes Skill Maps to the next level \n", "author": ["Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"], "link": "http://ai.googleblog.com/2015/08/googles-course-builder-19-improves.html", "abstract": "", "date": "\nThursday, August 13, 2015\n"},
{"website": "Google-AI", "title": "\nThe neural networks behind Google Voice transcription\n", "author": ["Posted by Fran\u00e7oise Beaufays, Research Scientist"], "link": "http://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html", "abstract": "", "date": "\nTuesday, August 11, 2015\n"},
{"website": "Google-AI", "title": "\nThe reusable holdout: Preserving validity in adaptive data analysis\n", "author": ["Posted by Moritz Hardt, Research Scientist"], "link": "http://ai.googleblog.com/2015/08/the-reusable-holdout-preserving.html", "abstract": "", "date": "\nThursday, August 6, 2015\n"},
{"website": "Google-AI", "title": "\nYoung people who are changing the world through science\n", "author": ["Posted by Andrea Cohan, Google Science Fair Program Manager "], "link": "http://ai.googleblog.com/2015/08/young-people-who-are-changing-world.html", "abstract": "", "date": "\nTuesday, August 4, 2015\n"},
{"website": "Google-AI", "title": "\nSee through the clouds with Earth Engine and Sentinel-1 Data\n", "author": ["Posted by Luc Vincent, Engineering Director, Geo Imagery"], "link": "http://ai.googleblog.com/2015/08/see-through-clouds-with-earth-engine.html", "abstract": "", "date": "\nMonday, August 3, 2015\n"},
{"website": "Google-AI", "title": "\nICSE 2015 and Software Engineering Research at Google\n", "author": ["Posted by Mohsen Vakilian, Software Engineer"], "link": "http://ai.googleblog.com/2015/07/icse-2015-and-software-engineering.html", "abstract": "", "date": "\nThursday, July 30, 2015\n"},
{"website": "Google-AI", "title": "\nHow Google Translate squeezes deep learning onto a phone\n", "author": ["Posted by Otavio Good, Software Engineer, Google Translate"], "link": "http://ai.googleblog.com/2015/07/how-google-translate-squeezes-deep.html", "abstract": "", "date": "\nWednesday, July 29, 2015\n"},
{"website": "Google-AI", "title": "\nThe Thorny Issue of CS Teacher Certification\n", "author": ["Posted by Chris Stephenson, Head of Computer Science Education Programs"], "link": "http://ai.googleblog.com/2015/07/the-thorny-issue-of-cs-teacher.html", "abstract": "", "date": "\nThursday, July 16, 2015\n"},
{"website": "Google-AI", "title": "\nShould My Kid Learn to Code?\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2015/07/should-my-kid-learn-to-code.html", "abstract": "", "date": "\nTuesday, July 14, 2015\n"},
{"website": "Google-AI", "title": "\nSimulating fermionic particles with superconducting quantum hardware\n", "author": ["Posted by Rami Barends and Julian Kelly, Quantum Electronics Engineers and John Martinis, Research Scientist"], "link": "http://ai.googleblog.com/2015/07/simulating-fermionic-particles-with.html", "abstract": "", "date": "\nMonday, July 13, 2015\n"},
{"website": "Google-AI", "title": "\nThe Computer Science Pipeline and Diversity: Part 2 - Some positive signs, and looking towards the future\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2015/07/the-computer-science-pipeline-and.html", "abstract": "", "date": "\nThursday, July 9, 2015\n"},
{"website": "Google-AI", "title": "\nThe Computer Science Pipeline and Diversity: Part 1 - How did we get here?\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, Google"], "link": "http://ai.googleblog.com/2015/07/the-computer-science-pipeline-and_8.html", "abstract": "", "date": "\nWednesday, July 8, 2015\n"},
{"website": "Google-AI", "title": "\nICML 2015 and Machine Learning Research at Google\n", "author": ["Posted by Corinna Cortes, Head, Google Research NY"], "link": "http://ai.googleblog.com/2015/07/icml-2015-and-machine-learning-research.html", "abstract": "", "date": "\nSunday, July 5, 2015\n"},
{"website": "Google-AI", "title": "\nDeepDream - a code example for visualizing Neural Networks\n", "author": ["Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer"], "link": "http://ai.googleblog.com/2015/07/deepdream-code-example-for-visualizing.html", "abstract": "", "date": "\nWednesday, July 1, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Computational Journalism Research Awards launch in Europe\n", "author": ["Posted by Andrea Held, Google University Relations & Matt Cooke, Google News Lab Europe"], "link": "http://ai.googleblog.com/2015/06/google-computational-journalism.html", "abstract": "", "date": "\nThursday, June 18, 2015\n"},
{"website": "Google-AI", "title": "\nInceptionism: Going Deeper into Neural Networks\n", "author": ["Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer"], "link": "http://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html", "abstract": "", "date": "\nWednesday, June 17, 2015\n"},
{"website": "Google-AI", "title": "\nNew ways to add Reminders in Inbox by Gmail\n", "author": ["Posted by Dave Orr, Google Research Product Manager"], "link": "http://ai.googleblog.com/2015/06/new-ways-to-add-reminders-in-inbox-by.html", "abstract": "", "date": "\nWednesday, June 17, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Computer Vision research at CVPR 2015\n", "author": ["Posted by Vincent Vanhoucke, Google Research Scientist"], "link": "http://ai.googleblog.com/2015/06/google-computer-vision-research-at-cvpr.html", "abstract": "", "date": "\nSunday, June 7, 2015\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2015 Google European Doctoral Fellows\n", "author": ["Posted by David Harper, University Relations and Beate List, Research Programs"], "link": "http://ai.googleblog.com/2015/06/announcing-2015-google-european.html", "abstract": "", "date": "\nFriday, June 5, 2015\n"},
{"website": "Google-AI", "title": "\nA Multilingual Corpus of Automatically Extracted Relations from Wikipedia\n", "author": ["Posted by Shankar Kumar, Google Research Scientist and Manaal Faruqui, Carnegie Mellon University PhD candidate"], "link": "http://ai.googleblog.com/2015/06/a-multilingual-corpus-of-automatically.html", "abstract": "", "date": "\nTuesday, June 2, 2015\n"},
{"website": "Google-AI", "title": "\nSergey and Larry awarded the Seoul Test-of-Time Award from WWW 2015\n", "author": ["Posted by Andrei Broder, Google Distinguished Scientist"], "link": "http://ai.googleblog.com/2015/05/sergey-and-larry-awarded-seoul-test-of.html", "abstract": "", "date": "\nFriday, May 22, 2015\n"},
{"website": "Google-AI", "title": "\nTone: An experimental Chrome extension for instant sharing over audio\n", "author": ["Posted by Alex Kauffmann, Interaction Researcher, and Boris Smus, Software Engineer"], "link": "http://ai.googleblog.com/2015/05/tone-experimental-chrome-extension-for.html", "abstract": "", "date": "\nTuesday, May 19, 2015\n"},
{"website": "Google-AI", "title": "\nPaper to Digital in 200+ languages\n", "author": ["Posted by Dmitriy Genzel and Ashok Popat, Research Scientists and Dhyanesh Narayanan, Product Manager"], "link": "http://ai.googleblog.com/2015/05/paper-to-digital-in-200-languages.html", "abstract": "", "date": "\nWednesday, May 6, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Handwriting Input in 82 languages on your Android mobile device\n", "author": ["Posted by Thomas Deselaers, Daniel Keysers, Henry Rowley, Li-Lun Wang, Victor C\u0103rbune, Ashok Popat, Dhyanesh Narayanan, Handwriting Team, Google Research"], "link": "http://ai.googleblog.com/2015/04/google-handwriting-input-in-82.html", "abstract": "", "date": "\nWednesday, April 15, 2015\n"},
{"website": "Google-AI", "title": "\nBeyond Short Snippets: Deep Networks for Video Classification\n", "author": ["Posted by Software Engineers George Toderici and Sudheendra Vijayanarasimhan"], "link": "http://ai.googleblog.com/2015/04/beyond-short-snippets-deep-networks-for.html", "abstract": "", "date": "\nWednesday, April 8, 2015\n"},
{"website": "Google-AI", "title": "\nSkill maps, analytics and more with Google\u2019s Course Builder 1.8\n", "author": ["Posted by Adam Feldman, Product Manager and Pavel Simakov, Technical Lead, Course Builder Team"], "link": "http://ai.googleblog.com/2015/04/skill-maps-analytics-and-more-with.html", "abstract": "", "date": "\nMonday, April 6, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Computer Science Capacity Awards\n", "author": ["By Maggie Johnson, Director of Education and University Relations and Chris Busselle, Google.org"], "link": "http://ai.googleblog.com/2015/03/google-computer-science-capacity-awards.html", "abstract": "", "date": "\nMonday, March 16, 2015\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Google MOOC Focused Research Awards\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations, and Aimin Zhu, University Relations Manager, APAC"], "link": "http://ai.googleblog.com/2015/03/announcing-google-mooc-focused-research.html", "abstract": "", "date": "\nMonday, March 9, 2015\n"},
{"website": "Google-AI", "title": "\nA step closer to quantum computation with Quantum Error Correction\n", "author": ["Posted by Julian Kelly, Rami Barends, and Austin Fowler, Quantum Electronics Engineers"], "link": "http://ai.googleblog.com/2015/03/a-step-closer-to-quantum-computation.html", "abstract": "", "date": "\nWednesday, March 4, 2015\n"},
{"website": "Google-AI", "title": "\nLarge-Scale Machine Learning for Drug Discovery\n", "author": ["Posted by Patrick Riley and Dale Webster, Google Research and ", ", Google Research Intern and Stanford Ph.D. candidate"], "link": "http://ai.googleblog.com/2015/03/large-scale-machine-learning-for-drug.html", "abstract": "", "date": "\nMonday, March 2, 2015\n"},
{"website": "Google-AI", "title": "\nFrom Pixels to Actions: Human-level control through Deep Reinforcement Learning\n", "author": ["Posted by Dharshan Kumaran and Demis Hassabis, Google DeepMind, London"], "link": "http://ai.googleblog.com/2015/02/from-pixels-to-actions-human-level.html", "abstract": "", "date": "\nWednesday, February 25, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Faculty Research Awards: Winter 2015\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2015/02/google-faculty-research-awards-winter.html", "abstract": "", "date": "\nThursday, February 19, 2015\n"},
{"website": "Google-AI", "title": "\nGoogle Science Fair 2015: what will you try?\n", "author": ["Posted by Miriam Schneider, Google for Education team"], "link": "http://ai.googleblog.com/2015/02/google-science-fair-2015-what-will-you.html", "abstract": "", "date": "\nWednesday, February 18, 2015\n"},
{"website": "Google-AI", "title": "\nAnnouncing the 2015 North American Google PhD Fellows\n", "author": ["Posted by Michael Rennaker, Google University Relations"], "link": "http://ai.googleblog.com/2015/02/announcing-2015-north-american-google.html", "abstract": "", "date": "\nWednesday, February 18, 2015\n"},
{"website": "Google-AI", "title": "\nMap of Life: A preview of how to evaluate species conservation with Google Earth Engine\n", "author": ["Posted by Walter Jetz, Dept. of Ecology and Evolutionary Biology, Yale University, and Dave Thau, Developer Advocate, Google Earth Engine, with support from Robert Guralnick, Dept. of Natural History, University of Florida"], "link": "http://ai.googleblog.com/2015/01/map-of-life-preview-of-how-to-evaluate.html", "abstract": "", "date": "\nThursday, January 8, 2015\n"},
{"website": "Google-AI", "title": "\nLittle Box Challenge Academic Awards\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2014/12/little-box-challenge-academic-awards.html", "abstract": "", "date": "\nTuesday, December 16, 2014\n"},
{"website": "Google-AI", "title": "\nCall for Research Proposals to participate in the Open Web of Things Expedition\n", "author": ["Posted Vint Cerf, Chief Internet Evangelist, Roy Want and Max Senges, Google Research"], "link": "http://ai.googleblog.com/2014/12/call-for-research-proposals-to.html", "abstract": "", "date": "\nFriday, December 12, 2014\n"},
{"website": "Google-AI", "title": "\nLearning Digital Skills online with Google Activate\n", "author": ["Posted by Michel Benard, University Relations Manager and Cova Soto, Product Marketing Manager, Business Marketing Madrid"], "link": "http://ai.googleblog.com/2014/12/learning-digital-skills-online-with.html", "abstract": "", "date": "\nThursday, December 11, 2014\n"},
{"website": "Google-AI", "title": "\nMOOC Research and Innovation\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2014/12/mooc-research-and-innovation.html", "abstract": "", "date": "\nTuesday, December 9, 2014\n"},
{"website": "Google-AI", "title": "\nHigh Quality Object Detection at Scale\n", "author": ["Google Engineers Christian Szegedy, Scott Reed", ", Dumitru Erhan, and Dragomir Anguelov"], "link": "http://ai.googleblog.com/2014/12/high-quality-object-detection-at-scale.html", "abstract": "", "date": "\nMonday, December 8, 2014\n"},
{"website": "Google-AI", "title": "\nWhat we can learn about effective, meaningful and diverse organizations\n", "author": ["Posted by Beryl Nelson, Software Engineering Manager"], "link": "http://ai.googleblog.com/2014/12/what-we-can-learn-about-effective.html", "abstract": "", "date": "\nThursday, December 4, 2014\n"},
{"website": "Google-AI", "title": "\nAutomatically making sense of data\n", "author": ["Posted by Kevin Murphy, Research Scientist and David Harper, Head of University Relations, EMEA"], "link": "http://ai.googleblog.com/2014/12/automatically-making-sense-of-data.html", "abstract": "", "date": "\nTuesday, December 2, 2014\n"},
{"website": "Google-AI", "title": "\nAdvances in Variational Inference: Working Towards Large-scale Probabilistic Machine Learning at NIPS 2014\n", "author": ["Posted by Shakir Mohamed and Charles Blundell, Google DeepMind, London"], "link": "http://ai.googleblog.com/2014/12/advances-in-variational-inference.html", "abstract": "", "date": "\nMonday, December 1, 2014\n"},
{"website": "Google-AI", "title": "\nA picture is worth a thousand (coherent) words: building a natural description of images\n", "author": ["Posted by Google Research Scientists Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan"], "link": "http://ai.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html", "abstract": "", "date": "\nMonday, November 17, 2014\n"},
{"website": "Google-AI", "title": "\nThe World Parks Congress: Using technology to protect our natural environment\n", "author": ["Posted by Dave Thau, Developer Advocate for Google Earth Engine and Karin Tuxen-Bettman, Program Manager, Google Earth Outreach"], "link": "http://ai.googleblog.com/2014/11/the-world-parks-congress-using.html", "abstract": "", "date": "\nWednesday, November 12, 2014\n"},
{"website": "Google-AI", "title": "\nGoogler Shumin Zhai awarded with the ACM UIST Lasting Impact Award\n", "author": ["Posted by Alfred Spector, Vice President, Engineering"], "link": "http://ai.googleblog.com/2014/11/googler-shumin-zhai-awarded-with-acm.html", "abstract": "", "date": "\nMonday, November 3, 2014\n"},
{"website": "Google-AI", "title": "\nGoogle Flu Trends gets a brand new engine\n", "author": ["Posted by Christian Stefansen, Senior Software Engineer"], "link": "http://ai.googleblog.com/2014/10/google-flu-trends-gets-brand-new-engine.html", "abstract": "", "date": "\nFriday, October 31, 2014\n"},
{"website": "Google-AI", "title": "\nLearning Statistics with Privacy, aided by the Flip of a Coin\n", "author": ["Posted by \u00dalfar Erlingsson, Tech Lead Manager, Security Research"], "link": "http://ai.googleblog.com/2014/10/learning-statistics-with-privacy-aided.html", "abstract": "", "date": "\nThursday, October 30, 2014\n"},
{"website": "Google-AI", "title": "\nHDR+: Low Light and High Dynamic Range photography in the Google Camera App\n", "author": ["Posted by Marc Levoy, Google[x] Software Engineering Manager and "], "link": "http://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html", "abstract": "", "date": "\nMonday, October 27, 2014\n"},
{"website": "Google-AI", "title": "\nHelping teachers teach computer science\n", "author": [], "link": "http://ai.googleblog.com/2014/10/helping-teachers-teach-computer-science.html", "abstract": "", "date": "\nFriday, October 24, 2014\n"},
{"website": "Google-AI", "title": "\nSmart Autofill - Harnessing the Predictive Power of Machine Learning in Google Sheets\n", "author": ["Posted by Konstantin Davydov, Software Engineer and Afshin Rostamizadeh, Research Scientist"], "link": "http://ai.googleblog.com/2014/10/smart-autofill-harnessing-predictive.html", "abstract": "", "date": "\nMonday, October 13, 2014\n"},
{"website": "Google-AI", "title": "\nAll the News that's Fit to Read: A Study of Social Annotations for News Reading\n", "author": ["Posted by Chinmay Kulkarni, Stanford University Ph.D candidate and former Google Intern, and Ed H. Chi, Google Research Scientist"], "link": "http://ai.googleblog.com/2014/10/all-news-that-fit-to-read-study-of.html", "abstract": "", "date": "\nWednesday, October 8, 2014\n"},
{"website": "Google-AI", "title": "\nAnnouncing the Google CS Engagement Small Awards Program\n", "author": ["Posted by Leslie Yeh Johnson, University Relations"], "link": "http://ai.googleblog.com/2014/10/announcing-google-cs-engagement-small.html", "abstract": "", "date": "\nMonday, October 6, 2014\n"},
{"website": "Google-AI", "title": "\nSudoku, Linear Optimization, and the Ten Cent Diet\n", "author": ["Posted by Jon Orwant, Engineering Manager"], "link": "http://ai.googleblog.com/2014/09/sudoku-linear-optimization-and-ten-cent.html", "abstract": "", "date": "\nTuesday, September 30, 2014\n"},
{"website": "Google-AI", "title": "\nCollaborative Mathematics with SageMathCloud and Google Cloud Platform\n", "author": ["Posted by Craig Citro, Software Engineer"], "link": "http://ai.googleblog.com/2014/09/collaborative-mathematics-with.html", "abstract": "", "date": "\nMonday, September 29, 2014\n"},
{"website": "Google-AI", "title": "\nIntroducing Structured Snippets, now a part of Google Web Search\n", "author": ["Posted by Corinna Cortes, Boulos Harb, Afshin Rostamizadeh, Ken Wilder, and Cong Yu, Google Research"], "link": "http://ai.googleblog.com/2014/09/introducing-structured-snippets-now.html", "abstract": "", "date": "\nMonday, September 22, 2014\n"},
{"website": "Google-AI", "title": "\nSign in to edx.org with Google (and Facebook, and...)\n", "author": ["Posted by John Cox, Software Engineer"], "link": "http://ai.googleblog.com/2014/09/sign-in-to-edxorg-with-google-and.html", "abstract": "", "date": "\nThursday, September 18, 2014\n"},
{"website": "Google-AI", "title": "\nCourse Builder now supports the Learning Tools Interoperability (LTI) Specification\n", "author": ["Posted by John Cox, Software Engineer"], "link": "http://ai.googleblog.com/2014/09/course-builder-now-supports-learning.html", "abstract": "", "date": "\nThursday, September 11, 2014\n"},
{"website": "Google-AI", "title": "\nBuilding a deeper understanding of images\n", "author": ["Posted by Christian Szegedy, Software Engineer"], "link": "http://ai.googleblog.com/2014/09/building-deeper-understanding-of-images.html", "abstract": "", "date": "\nFriday, September 5, 2014\n"},
{"website": "Google-AI", "title": "\nWorking Together to Support Computer Science Education\n", "author": ["Posted by Chris Stephenson, Computer Science Education Program Manager"], "link": "http://ai.googleblog.com/2014/09/working-together-to-support-computer.html", "abstract": "", "date": "\nWednesday, September 3, 2014\n"},
{"website": "Google-AI", "title": "\nHardware Initiative at Quantum Artificial Intelligence Lab\n", "author": ["Posted by Hartmut Neven, Director of Engineering"], "link": "http://ai.googleblog.com/2014/09/hardware-initiative-at-quantum.html", "abstract": "", "date": "\nTuesday, September 2, 2014\n"},
{"website": "Google-AI", "title": "\nTeaching machines to read between the lines (and a new corpus with entity salience annotations)\n", "author": ["Posted by Dan Gillick, Research Scientist, and Dave Orr, Product Manager"], "link": "http://ai.googleblog.com/2014/08/teaching-machines-to-read-between-lines.html", "abstract": "", "date": "\nMonday, August 25, 2014\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Summer 2014\n", "author": ["posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2014/08/google-research-awards-summer-2014.html", "abstract": "", "date": "\nWednesday, August 20, 2014\n"},
{"website": "Google-AI", "title": "\nSummer Games: Learn to Program\n", "author": ["Posted by Jennifer Vaden Barth, Executive Assistant"], "link": "http://ai.googleblog.com/2014/08/summer-games-learn-to-program.html", "abstract": "", "date": "\nMonday, August 11, 2014\n"},
{"website": "Google-AI", "title": "\nDoing Data Science with coLaboratory\n", "author": ["Posted by Kayur Patel, Kester Tong, Mark Sandler, and Corinna Cortes, Google Research"], "link": "http://ai.googleblog.com/2014/08/doing-data-science-with-colaboratory.html", "abstract": "", "date": "\nFriday, August 8, 2014\n"},
{"website": "Google-AI", "title": "\nFacilitating Genomics Research with Google Cloud Platform\n", "author": ["Posted by Paul C. Boutros, Ontario Institute for Cancer Research, Josh Stuart, UC Santa Cruz, Adam Margolin, Oregon Health & Science University; Nicole Deflaux and Jonathan Bingham, Google Cloud Platform and Google Genomics"], "link": "http://ai.googleblog.com/2014/07/facilitating-genomics-research-with.html", "abstract": "", "date": "\nWednesday, July 30, 2014\n"},
{"website": "Google-AI", "title": "\nFocus Areas for Policy & Standards Research Proposals\n", "author": ["Posted by Vint Cerf, VP & Chief Internet Evangelist"], "link": "http://ai.googleblog.com/2014/07/focus-areas-for-policy-standards.html", "abstract": "", "date": "\nThursday, July 24, 2014\n"},
{"website": "Google-AI", "title": "\nAcademics and the Little Box Challenge\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2014/07/academics-and-little-box-challenge.html", "abstract": "", "date": "\nTuesday, July 22, 2014\n"},
{"website": "Google-AI", "title": "\nSimple is better - Making your web forms easy to use pays off\n", "author": ["Posted by Javier Bargas-Avila and Mirjam Seckler, User Experience Research at Google"], "link": "http://ai.googleblog.com/2014/07/simple-is-better-making-your-web-forms.html", "abstract": "", "date": "\nMonday, July 14, 2014\n"},
{"website": "Google-AI", "title": "\nInfluential Papers for 2013\n", "author": ["Posted by Corinna Cortes and Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2014/06/influential-papers-for-2013.html", "abstract": "", "date": "\nMonday, June 30, 2014\n"},
{"website": "Google-AI", "title": "\n2014 Google PhD Fellowships: Supporting the Future of Computer Science\n", "author": ["Posted by David Harper, Google University Relations & Beate List, Google Research Programs"], "link": "http://ai.googleblog.com/2014/06/2014-google-phd-fellowships-supporting.html", "abstract": "", "date": "\nWednesday, June 18, 2014\n"},
{"website": "Google-AI", "title": "\nA skill-based approach to creating open online courses\n", "author": ["Posted by Sean Lip, Software Engineer, Open Online Education"], "link": "http://ai.googleblog.com/2014/05/a-skill-based-approach-to-creating-open.html", "abstract": "", "date": "\nTuesday, May 27, 2014\n"},
{"website": "Google-AI", "title": "\nA Billion Words: Because today's language modeling standard should be higher\n", "author": ["Posted by Dave Orr, Product Manager, and Ciprian Chelba, Research Scientist"], "link": "http://ai.googleblog.com/2014/04/a-billion-words-because-today-language.html", "abstract": "", "date": "\nWednesday, April 30, 2014\n"},
{"website": "Google-AI", "title": "\nLens Blur in the new Google Camera app\n", "author": ["Posted by Carlos Hern\u00e1ndez, Software Engineer"], "link": "http://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html", "abstract": "", "date": "\nWednesday, April 16, 2014\n"},
{"website": "Google-AI", "title": "\nSawasdeee ka Voice Search\n", "author": ["Posted by Keith Hall and Richard Sproat, Staff Research Scientists, Speech"], "link": "http://ai.googleblog.com/2014/04/sawasdeee-ka-voice-search.html", "abstract": "", "date": "\nWednesday, April 2, 2014\n"},
{"website": "Google-AI", "title": "\nMaking Blockly Universally Accessible\n", "author": ["Posted by Neil Fraser, Chief Interplanetary Liaison"], "link": "http://ai.googleblog.com/2014/04/making-blockly-universally-accessible.html", "abstract": "", "date": "\nTuesday, April 1, 2014\n"},
{"website": "Google-AI", "title": "\nCelebrating the First Set of Google Geo Education Awardees and Announcing Round Two\n", "author": ["Posted by Dave Thau, Senior Developer Advocate"], "link": "http://ai.googleblog.com/2014/03/celebrating-first-set-of-google-geo.html", "abstract": "", "date": "\nMonday, March 31, 2014\n"},
{"website": "Google-AI", "title": "\nMaking Sense of MOOC Data\n", "author": ["Posted by Julia Wilkowski, Staff Instructional Designer"], "link": "http://ai.googleblog.com/2014/03/making-sense-of-mooc-data.html", "abstract": "", "date": "\nThursday, March 27, 2014\n"},
{"website": "Google-AI", "title": "\nBerkeley Earth Maps Powered by Google Maps Engine now available in the Google Maps Gallery\n", "author": ["Posted by Dr. Robert Rohde, Berkeley Earth"], "link": "http://ai.googleblog.com/2014/03/berkeley-earth-maps-powered-by-google.html", "abstract": "", "date": "\nThursday, March 20, 2014\n"},
{"website": "Google-AI", "title": "\nComputer Science Education Recharged!\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2014/03/computer-science-education-recharged.html", "abstract": "", "date": "\nTuesday, March 11, 2014\n"},
{"website": "Google-AI", "title": "\nGoogle joins the Global Alliance for Genomics and Health\n", "author": ["Posted by Jonathan Bingham, Product Manager"], "link": "http://ai.googleblog.com/2014/02/google-joins-global-alliance-for.html", "abstract": "", "date": "\nThursday, February 27, 2014\n"},
{"website": "Google-AI", "title": "\nMaking Sense of Data with Google\n", "author": ["Posted by John Atwood, Program Manager"], "link": "http://ai.googleblog.com/2014/02/making-sense-of-data-with-google.html", "abstract": "", "date": "\nTuesday, February 25, 2014\n"},
{"website": "Google-AI", "title": "\nMonitoring the World's Forests with Global Forest Watch\n", "author": ["Posted by Crystal Davis, Director of Global Forest Watch, the World Resources Institute, and Dave Thau, Developer Advocate, Google Earth Engine"], "link": "http://ai.googleblog.com/2014/02/monitoring-world-forests-with-global.html", "abstract": "", "date": "\nThursday, February 20, 2014\n"},
{"website": "Google-AI", "title": "\nGoogle Award Program stimulates Journalism and CS collaboration\n", "author": ["Posted by Krishna Bharat, Distinguished Research Scientist"], "link": "http://ai.googleblog.com/2014/02/google-award-program-stimulates.html", "abstract": "", "date": "\nWednesday, February 19, 2014\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Winter 2014\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2014/02/google-research-awards-winter-2014.html", "abstract": "", "date": "\nTuesday, February 18, 2014\n"},
{"website": "Google-AI", "title": "\nExplore the history of Pop -- and Punk, Jazz, and Folk -- with the Music Timeline\n", "author": ["Posted by Alison Cichowlas and Tony Lam, Google Research"], "link": "http://ai.googleblog.com/2014/01/explore-history-of-pop-and-punk-jazz.html", "abstract": "", "date": "\nThursday, January 16, 2014\n"},
{"website": "Google-AI", "title": "\nPiloting after school clubs to ignite interest in Computer Science\n", "author": ["Posted by JamieSue Goodman, Program Lead, CS First"], "link": "http://ai.googleblog.com/2014/01/piloting-after-school-clubs-to-ignite.html", "abstract": "", "date": "\nWednesday, January 15, 2014\n"},
{"website": "Google-AI", "title": "\nGroundbreaking simulations by Google Exacycle Visiting Faculty \n", "author": ["Posted by David Konerding, Staff Software Engineer"], "link": "http://ai.googleblog.com/2013/12/groundbreaking-simulations-by-google.html", "abstract": "", "date": "\nMonday, December 16, 2013\n"},
{"website": "Google-AI", "title": "\nGoogler Moti Yung elected as 2013 ACM Fellow\n", "author": ["Posted by Alfred Spector, VP of Engineering"], "link": "http://ai.googleblog.com/2013/12/googler-moti-yung-elected-as-2013-acm.html", "abstract": "", "date": "\nWednesday, December 11, 2013\n"},
{"website": "Google-AI", "title": "\nFree Language Lessons for Computers\n", "author": ["Posted by Dave Orr, Google Research Product Manager"], "link": "http://ai.googleblog.com/2013/12/free-language-lessons-for-computers.html", "abstract": "", "date": "\nTuesday, December 3, 2013\n"},
{"website": "Google-AI", "title": "\nReleased Data Set: Features Extracted From YouTube Videos for Multiview Learning\n", "author": ["Posted by Omid Madani, Senior Software Engineer"], "link": "http://ai.googleblog.com/2013/11/released-data-set-features-extracted.html", "abstract": "", "date": "\nTuesday, November 26, 2013\n"},
{"website": "Google-AI", "title": "\nThe MiniZinc Challenge\n", "author": ["Posted by Jon Orwant, Engineering Manager"], "link": "http://ai.googleblog.com/2013/11/the-minizinc-challenge.html", "abstract": "", "date": "\nMonday, November 25, 2013\n"},
{"website": "Google-AI", "title": "\nNew Research Challenges in Language Understanding\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2013/11/new-research-challenges-in-language.html", "abstract": "", "date": "\nFriday, November 22, 2013\n"},
{"website": "Google-AI", "title": "\nUnique Strategies for Scaling Teacher Professional Development\n", "author": ["Posted by Candice Reimers, Senior Program Manager"], "link": "http://ai.googleblog.com/2013/11/unique-strategies-for-scaling-teacher.html", "abstract": "", "date": "\nTuesday, November 19, 2013\n"},
{"website": "Google-AI", "title": "\nMoore\u2019s Law Part 4: Moore's Law in other domains \n", "author": [], "link": "http://ai.googleblog.com/2013/11/moores-law-part-4-moore-law-in-other.html", "abstract": "", "date": "\nFriday, November 15, 2013\n"},
{"website": "Google-AI", "title": "\nThe first detailed maps of global forest change\n", "author": ["Posted by Matt Hansen and Peter Potapov, University of Maryland; Rebecca Moore and Matt Hancher, Google "], "link": "http://ai.googleblog.com/2013/11/the-first-detailed-maps-of-global.html", "abstract": "", "date": "\nThursday, November 14, 2013\n"},
{"website": "Google-AI", "title": "\nMoore\u2019s Law, Part 3: Possible extrapolations over the next 15 years and impact\n", "author": [], "link": "http://ai.googleblog.com/2013/11/moores-law-part-3-possible.html", "abstract": "", "date": "\nWednesday, November 13, 2013\n"},
{"website": "Google-AI", "title": "\nMoore\u2019s Law, Part 2: More Moore and More than Moore\n", "author": [], "link": "http://ai.googleblog.com/2013/11/moores-law-part-2-more-moore-and-more.html", "abstract": "", "date": "\nTuesday, November 12, 2013\n"},
{"website": "Google-AI", "title": "\nMoore\u2019s Law, Part 1: Brief history of Moore's Law and current state\n", "author": [], "link": "http://ai.googleblog.com/2013/11/moores-law-part-1-brief-history-of.html", "abstract": "", "date": "\nMonday, November 11, 2013\n"},
{"website": "Google-AI", "title": "\nEnhancing Linguistic Search with the Google Books Ngram Viewer\n", "author": ["Posted by Slav Petrov and Dipanjan Das, Research Scientists"], "link": "http://ai.googleblog.com/2013/10/enhancing-linguistic-search-with-google.html", "abstract": "", "date": "\nThursday, October 17, 2013\n"},
{"website": "Google-AI", "title": "\nOpening up Course Builder data\n", "author": ["Posted by John Cox and Pavel Simakov, Course Builder Team, Google Research"], "link": "http://ai.googleblog.com/2013/10/opening-up-course-builder-data.html", "abstract": "", "date": "\nWednesday, October 9, 2013\n"},
{"website": "Google-AI", "title": "\nProjecting without a projector: sharing your smartphone content onto an arbitrary display\n", "author": ["Posted by Yang Li, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2013/09/projecting-without-projector-sharing.html", "abstract": "", "date": "\nThursday, September 26, 2013\n"},
{"website": "Google-AI", "title": "\nBroadening Google Patents\n", "author": ["Posted by Jon Orwant, Engineering Manager"], "link": "http://ai.googleblog.com/2013/09/broadening-google-patents.html", "abstract": "", "date": "\nTuesday, September 17, 2013\n"},
{"website": "Google-AI", "title": "\nWe are joining the Open edX platform\n", "author": ["Posted by Dan Clancy, Director of Research"], "link": "http://ai.googleblog.com/2013/09/we-are-joining-open-edx-platform.html", "abstract": "", "date": "\nTuesday, September 10, 2013\n"},
{"website": "Google-AI", "title": "\nMake Your Websites More Accessible to More Users with Introduction to Web Accessibility\n", "author": ["Eve Andersson, Manager, Accessibility Engineering"], "link": "http://ai.googleblog.com/2013/09/make-your-websites-more-accessible-to.html", "abstract": "", "date": "\nTuesday, September 10, 2013\n"},
{"website": "Google-AI", "title": "\nA Comparison of Five Google Online Courses\n", "author": ["Posted by Julia Wilkowski, Senior Instructional Designer"], "link": "http://ai.googleblog.com/2013/09/a-comparison-of-five-google-online.html", "abstract": "", "date": "\nThursday, September 5, 2013\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Summer 2013\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2013/08/google-research-awards-summer-2013.html", "abstract": "", "date": "\nMonday, August 12, 2013\n"},
{"website": "Google-AI", "title": "\nComputer Science Teaching Fellows Starting Up in Charleston, SC\n", "author": ["Posted by Cameron Fadjo, Program Lead, Computer Science Teaching Fellows"], "link": "http://ai.googleblog.com/2013/08/computer-science-teaching-fellows.html", "abstract": "", "date": "\nWednesday, August 7, 2013\n"},
{"website": "Google-AI", "title": "\nUnder the hood of Croatian, Filipino, Ukrainian, and Vietnamese in Google Voice Search\n", "author": ["Posted by Eugene Weinstein and Pedro Moreno, Google Speech Team"], "link": "http://ai.googleblog.com/2013/07/under-hood-of-croatian-filipino.html", "abstract": "", "date": "\nThursday, July 25, 2013\n"},
{"website": "Google-AI", "title": "\n11 Billion Clues in 800 Million Documents: A Web Research Corpus Annotated with Freebase Concepts\n", "author": ["Posted by Dave Orr, Amar Subramanya, Evgeniy Gabrilovich, and Michael Ringgaard, Google Research"], "link": "http://ai.googleblog.com/2013/07/11-billion-clues-in-800-million.html", "abstract": "", "date": "\nWednesday, July 17, 2013\n"},
{"website": "Google-AI", "title": "\nNew research from Google shows that 88% of the traffic generated by mobile search ads is not replaced by traffic originating from mobile organic search\n", "author": ["Posted by Shaun Lysen, Statistician at Google"], "link": "http://ai.googleblog.com/2013/07/new-research-from-google-shows-that-88.html", "abstract": "", "date": "\nTuesday, July 16, 2013\n"},
{"website": "Google-AI", "title": "\nGoogle Databoard: A new way to explore industry research\n", "author": ["Posted by Adam Grunewald, Mobile Marketing Manager"], "link": "http://ai.googleblog.com/2013/07/google-databoard-new-way-to-explore.html", "abstract": "", "date": "\nTuesday, July 9, 2013\n"},
{"website": "Google-AI", "title": "\nConference Report: USENIX Annual Technical Conference (ATC) 2013\n", "author": ["Posted by Murray Stokely, Google Storage Analytics Team"], "link": "http://ai.googleblog.com/2013/07/conference-report-usenix-annual.html", "abstract": "", "date": "\nWednesday, July 3, 2013\n"},
{"website": "Google-AI", "title": "\nNatural Language Understanding-focused awards announced\n", "author": ["Posted by Massimiliano Ciaramita, Research Scientist and David Harper, Head University Relations (EMEA)"], "link": "http://ai.googleblog.com/2013/07/natural-language-understanding-focused.html", "abstract": "", "date": "\nTuesday, July 2, 2013\n"},
{"website": "Google-AI", "title": "\nFast, Accurate Detection of 100,000 Object Classes on a Single Machine\n", "author": ["Posted by Tom Dean, Google Research"], "link": "http://ai.googleblog.com/2013/06/fast-accurate-detection-of-100000.html", "abstract": "", "date": "\nThursday, June 27, 2013\n"},
{"website": "Google-AI", "title": "\nSome Innovative MOOCs\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2013/06/some-innovative-moocs.html", "abstract": "", "date": "\nTuesday, June 18, 2013\n"},
{"website": "Google-AI", "title": "\nExcellent Papers for 2012\n", "author": ["Posted by Corinna Cortes and Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2013/06/excellent-papers-for-2012.html", "abstract": "", "date": "\nThursday, June 13, 2013\n"},
{"website": "Google-AI", "title": "\nImproving Photo Search: A Step Across the Semantic Gap\n", "author": ["Posted by Chuck Rosenberg, Image Search Team"], "link": "http://ai.googleblog.com/2013/06/improving-photo-search-step-across.html", "abstract": "", "date": "\nWednesday, June 12, 2013\n"},
{"website": "Google-AI", "title": "\n2013 Google PhD Fellowships: 5 Years of Supporting the Future of Computer Science\n", "author": ["Posted by Michael Rennaker, Google University Relations"], "link": "http://ai.googleblog.com/2013/06/2013-google-phd-fellowships-5-years-of.html", "abstract": "", "date": "\nTuesday, June 11, 2013\n"},
{"website": "Google-AI", "title": "\nBuilding A Visual Planetary Time Machine\n", "author": ["Posted by Randy Sargent, Google/Carnegie Mellon University;  Matt Hancher and Eric Nguyen, Google; and Illah Nourbakhsh, Carnegie Mellon University "], "link": "http://ai.googleblog.com/2013/06/building-visual-planetary-time-machine.html", "abstract": "", "date": "\nMonday, June 10, 2013\n"},
{"website": "Google-AI", "title": "\nThe Story Behind Course Builder\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2013/06/the-story-behind-course-builder.html", "abstract": "", "date": "\nMonday, June 3, 2013\n"},
{"website": "Google-AI", "title": "\nDistributing the Edit History of Wikipedia Infoboxes\n", "author": ["Posted by Enrique Alfonseca, Google Research "], "link": "http://ai.googleblog.com/2013/05/distributing-edit-history-of-wikipedia.html", "abstract": "", "date": "\nThursday, May 30, 2013\n"},
{"website": "Google-AI", "title": "\nOpen Access for Publications\n", "author": ["Posted by Alfred Spector, Vice President, Engineering"], "link": "http://ai.googleblog.com/2013/05/open-access-for-publications.html", "abstract": "", "date": "\nWednesday, May 29, 2013\n"},
{"website": "Google-AI", "title": "\nExplore more with Mapping with Google\n", "author": ["Posted by Tina Ornduff, Program Manager "], "link": "http://ai.googleblog.com/2013/05/explore-more-with-mapping-with-google.html", "abstract": "", "date": "\nTuesday, May 28, 2013\n"},
{"website": "Google-AI", "title": "\nSyntactic Ngrams over Time\n", "author": ["Posted by Yoav Goldberg, Professor at Bar Ilan University & Post-doc at Google 2011-2013"], "link": "http://ai.googleblog.com/2013/05/syntactic-ngrams-over-time.html", "abstract": "", "date": "\nThursday, May 23, 2013\n"},
{"website": "Google-AI", "title": "\nLaunching the Quantum Artificial Intelligence Lab\n", "author": ["Posted by Hartmut Neven, Director of Engineering"], "link": "http://ai.googleblog.com/2013/05/launching-quantum-artificial.html", "abstract": "", "date": "\nThursday, May 16, 2013\n"},
{"website": "Google-AI", "title": "\nTwo Googlers elected to the American Academy of Arts and Sciences\n", "author": ["Posted by Alfred Spector, Vice President, Engineering"], "link": "http://ai.googleblog.com/2013/04/two-googlers-elected-to-american.html", "abstract": "", "date": "\nThursday, April 25, 2013\n"},
{"website": "Google-AI", "title": "\n50,000 Lessons on How to Read: a Relation Extraction Corpus\n", "author": ["Posted by Dave Orr, Product Manager, Google Research"], "link": "http://ai.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html", "abstract": "", "date": "\nThursday, April 11, 2013\n"},
{"website": "Google-AI", "title": "\nAdvanced Power Searching with Google: Lessons Learned\n", "author": ["Posted by Dan Russell, Uber Tech Lead, Search Quality & User Happiness and Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2013/04/advanced-power-searching-with-google.html", "abstract": "", "date": "\nTuesday, April 9, 2013\n"},
{"website": "Google-AI", "title": "\nEducation Awards on Google App Engine\n", "author": ["Posted by Andrea Held, Google University Relations"], "link": "http://ai.googleblog.com/2013/03/education-awards-on-google-app-engine.html", "abstract": "", "date": "\nWednesday, March 27, 2013\n"},
{"website": "Google-AI", "title": "\nScaling Computer Science Education\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2013/03/scaling-computer-science-education.html", "abstract": "", "date": "\nWednesday, March 13, 2013\n"},
{"website": "Google-AI", "title": "\nOur Commitment to Social Computing Research: Social Interactions Focused Awards Announcement\n", "author": ["Ed H. Chi, Staff Research Scientist"], "link": "http://ai.googleblog.com/2013/03/our-commitment-to-social-computing.html", "abstract": "", "date": "\nTuesday, March 12, 2013\n"},
{"website": "Google-AI", "title": "\nLearning from Big Data: 40 Million Entities in Context\n", "author": ["Posted by Dave Orr, Amar Subramanya, and Fernando Pereira, Google Research"], "link": "http://ai.googleblog.com/2013/03/learning-from-big-data-40-million.html", "abstract": "", "date": "\nFriday, March 8, 2013\n"},
{"website": "Google-AI", "title": "\nApplauding the White House Memorandum on Open Access\n", "author": ["Posted by Alfred Spector, Vice President of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2013/02/applauding-white-house-memorandum-on.html", "abstract": "", "date": "\nMonday, February 25, 2013\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Winter, 2013\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2013/02/google-research-awards-winter-2013.html", "abstract": "", "date": "\nFriday, February 22, 2013\n"},
{"website": "Google-AI", "title": "\nMobile interaction research at Google\n", "author": ["Posted by Xiaojun Bi, Ciprian Chelba, Tom Ouyang, Kurt Partridge and Shumin Zhai"], "link": "http://ai.googleblog.com/2013/02/mobile-interaction-research-at-google.html", "abstract": "", "date": "\nFriday, February 15, 2013\n"},
{"website": "Google-AI", "title": "\nResearch Projects on Google App Engine\n", "author": ["By Andrea Held, Program Manager, Google University Relations"], "link": "http://ai.googleblog.com/2013/02/research-projects-on-google-app-engine.html", "abstract": "", "date": "\nTuesday, February 12, 2013\n"},
{"website": "Google-AI", "title": "\nAdvanced Power Searching with Google -- Registration Opens Today\n", "author": ["Posted by Daniel Russell, \u00dcber Tech Lead for Search Quality and User Happiness"], "link": "http://ai.googleblog.com/2013/01/advanced-power-searching-with-google.html", "abstract": "", "date": "\nThursday, January 10, 2013\n"},
{"website": "Google-AI", "title": "\nConference Report: Workshop on Internet and Network Economics (WINE) 2012\n", "author": ["Posted by Vahab Mirrokni, Research Scientist, Google Research New York"], "link": "http://ai.googleblog.com/2012/12/conference-report-workshop-on-internet.html", "abstract": "", "date": "\nWednesday, December 19, 2012\n"},
{"website": "Google-AI", "title": "\nUsing online courses in Spain to teach entrepreneurship\n", "author": ["Posted by Francisco Ruiz Anton, Policy Manager, Google Spain"], "link": "http://ai.googleblog.com/2012/12/using-online-courses-in-spain-to-teach.html", "abstract": "", "date": "\nTuesday, December 18, 2012\n"},
{"website": "Google-AI", "title": "\nMillions of Core-Hours Awarded to Science\n", "author": ["Posted by Andrea Held, Program Manager, University Relations"], "link": "http://ai.googleblog.com/2012/12/millions-of-core-hours-awarded-to.html", "abstract": "", "date": "\nMonday, December 17, 2012\n"},
{"website": "Google-AI", "title": "\nContinuing the quest for future computer scientists with CS4HS\n", "author": ["Erin Mindell, Program Manager, Google Education"], "link": "http://ai.googleblog.com/2012/12/continuing-quest-for-future-computer.html", "abstract": "", "date": "\nThursday, December 13, 2012\n"},
{"website": "Google-AI", "title": "\nLarge Scale Language Modeling in Automatic Speech Recognition\n", "author": ["Posted by Ciprian Chelba, Research Scientist"], "link": "http://ai.googleblog.com/2012/10/large-scale-language-modeling-in.html", "abstract": "", "date": "\nWednesday, October 31, 2012\n"},
{"website": "Google-AI", "title": "\nNgram Viewer 2.0\n", "author": ["Posted by Jon Orwant, Engineering Manager"], "link": "http://ai.googleblog.com/2012/10/ngram-viewer-20.html", "abstract": "", "date": "\nThursday, October 18, 2012\n"},
{"website": "Google-AI", "title": "\nReFr: A New Open-Source Framework for Building Reranking Models\n", "author": ["Posted by ", " and ", ", Research Scientists at Google"], "link": "http://ai.googleblog.com/2012/10/refr-new-open-source-framework-for.html", "abstract": "", "date": "\nThursday, October 4, 2012\n"},
{"website": "Google-AI", "title": "\nEMEA Faculty Summit 2012\n", "author": ["Michel Benard, University Relations Manager"], "link": "http://ai.googleblog.com/2012/10/emea-faculty-summit-2012.html", "abstract": "", "date": "\nTuesday, October 2, 2012\n"},
{"website": "Google-AI", "title": "\nRunning Continuous Geo Experiments to Assess Ad Effectiveness\n", "author": ["Posted by Jon Vaver, Research Scientist and Lizzy Van Alstine, Marketing Manager"], "link": "http://ai.googleblog.com/2012/09/running-continuous-geo-experiments-to.html", "abstract": "", "date": "\nTuesday, September 18, 2012\n"},
{"website": "Google-AI", "title": "\nPower Searching with Google is back\n", "author": ["Posted by Dan Russell, Uber Tech Lead, Search Quality & User Happiness"], "link": "http://ai.googleblog.com/2012/09/power-searching-with-google-is-back.html", "abstract": "", "date": "\nTuesday, September 11, 2012\n"},
{"website": "Google-AI", "title": "\nHelping the World to Teach\n", "author": ["Posted by Peter Norvig, Director of Research"], "link": "http://ai.googleblog.com/2012/09/helping-world-to-teach.html", "abstract": "", "date": "\nTuesday, September 11, 2012\n"},
{"website": "Google-AI", "title": "\nUsers love simple and familiar designs \u2013 Why websites need to make a great first impression\n", "author": ["Posted by Javier Bargas-Avila, Senior User Experience Researcher at YouTube UX Research "], "link": "http://ai.googleblog.com/2012/08/users-love-simple-and-familiar-designs.html", "abstract": "", "date": "\nWednesday, August 29, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle at UAI 2012\n", "author": ["Posted by Kevin Murphy, Research Scientist"], "link": "http://ai.googleblog.com/2012/08/google-at-uai-2012.html", "abstract": "", "date": "\nTuesday, August 28, 2012\n"},
{"website": "Google-AI", "title": "\nBetter table search through Machine Learning and Knowledge\n", "author": ["Posted By Johnny Chen, Product Manager, Google Research"], "link": "http://ai.googleblog.com/2012/08/better-table-search-through-machine.html", "abstract": "", "date": "\nThursday, August 23, 2012\n"},
{"website": "Google-AI", "title": "\nMachine Learning Book for Students and Researchers\n", "author": ["Posted by Afshin Rostamizadeh, Google Research"], "link": "http://ai.googleblog.com/2012/08/machine-learning-book-for-students-and.html", "abstract": "", "date": "\nWednesday, August 22, 2012\n"},
{"website": "Google-AI", "title": "\nFaculty Summit 2012: Online Education Panel\n", "author": ["Posted by ", ", Director of Research"], "link": "http://ai.googleblog.com/2012/08/faculty-summit-2012-online-education.html", "abstract": "", "date": "\nMonday, August 20, 2012\n"},
{"website": "Google-AI", "title": "\nImproving Google Patents with European Patent Office patents and the Prior Art Finder \n", "author": ["Posted by Jon Orwant, Engineering Manager"], "link": "http://ai.googleblog.com/2012/08/improving-google-patents-with-european.html", "abstract": "", "date": "\nTuesday, August 14, 2012\n"},
{"website": "Google-AI", "title": "\nTeaching the World to Search\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2012/08/teaching-world-to-search.html", "abstract": "", "date": "\nWednesday, August 8, 2012\n"},
{"website": "Google-AI", "title": "\nSpeech Recognition and Deep Learning\n", "author": ["Posted by Vincent Vanhoucke, Research Scientist, Speech Team"], "link": "http://ai.googleblog.com/2012/08/speech-recognition-and-deep-learning.html", "abstract": "", "date": "\nMonday, August 6, 2012\n"},
{"website": "Google-AI", "title": "\nReflections on Digital Interactions: Thoughts from the 2012 NA Faculty Summit\n", "author": ["Posted by Alfred Spector, Vice President of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2012/08/reflections-on-digital-interactions.html", "abstract": "", "date": "\nThursday, August 2, 2012\n"},
{"website": "Google-AI", "title": "\nNatural Language in Voice Search\n", "author": ["Posted by Jakob Uszkoreit, Software Engineer"], "link": "http://ai.googleblog.com/2012/07/natural-language-in-voice-search.html", "abstract": "", "date": "\nTuesday, July 31, 2012\n"},
{"website": "Google-AI", "title": "\nNew Challenges in Computer Science Research\n", "author": ["Posted by Jeff Walz, Head of University Relations"], "link": "http://ai.googleblog.com/2012/07/new-challenges-in-computer-science.html", "abstract": "", "date": "\nFriday, July 27, 2012\n"},
{"website": "Google-AI", "title": "\nEducation in the Cloud\n", "author": ["Posted by Andrea Held, University Relations"], "link": "http://ai.googleblog.com/2012/07/education-in-cloud.html", "abstract": "", "date": "\nFriday, July 27, 2012\n"},
{"website": "Google-AI", "title": "\nBig Pictures with Big Messages\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2012/07/big-pictures-with-big-messages.html", "abstract": "", "date": "\nThursday, July 26, 2012\n"},
{"website": "Google-AI", "title": "\nSite Reliability Engineers: \u201csolving the most interesting problems\u201d\n", "author": ["Posted by Chris Reid, Sydney Staffing team"], "link": "http://ai.googleblog.com/2012/07/site-reliability-engineers-solving-most.html", "abstract": "", "date": "\nWednesday, July 25, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle at SIGMOD/PODS 2012\n", "author": ["Posted by ", ", Research Scientist and Jeff Shute, Software Engineer"], "link": "http://ai.googleblog.com/2012/07/google-at-sigmodpods-2012.html", "abstract": "", "date": "\nFriday, July 13, 2012\n"},
{"website": "Google-AI", "title": "\nReflections on the Google Faculty Institute\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2012/07/reflections-on-google-faculty-institute.html", "abstract": "", "date": "\nThursday, July 12, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle Research Awards: Summer, 2012\n", "author": ["Posted by Maggie Johnson, Director of Education, University Relations"], "link": "http://ai.googleblog.com/2012/07/google-research-awards-summer-2012.html", "abstract": "", "date": "\nTuesday, July 3, 2012\n"},
{"website": "Google-AI", "title": "\nOur Unique Approach to Research\n", "author": ["Posted by\u00a0", ", Vice President of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2012/07/our-unique-approach-to-research.html", "abstract": "", "date": "\nMonday, July 2, 2012\n"},
{"website": "Google-AI", "title": "\nIntroducing new Fusion Tables API\n", "author": ["Posted by Warren Shen, Fusion Tables team"], "link": "http://ai.googleblog.com/2012/06/introducing-new-fusion-tables-api.html", "abstract": "", "date": "\nTuesday, June 26, 2012\n"},
{"website": "Google-AI", "title": "\nBecome a Google Power Searcher\n", "author": ["Posted by Terry Ednacot, Education Program Manager"], "link": "http://ai.googleblog.com/2012/06/become-google-power-searcher.html", "abstract": "", "date": "\nTuesday, June 26, 2012\n"},
{"website": "Google-AI", "title": "\nThird Market Algorithms and Optimization Workshop at Google NYC\n", "author": ["Posted by Nitish Korula and Vahab Mirrokni, Google Research, New York"], "link": "http://ai.googleblog.com/2012/06/third-market-algorithms-and.html", "abstract": "", "date": "\nFriday, June 15, 2012\n"},
{"website": "Google-AI", "title": "\nRecap of NAACL-12 including two Best Paper awards for Googlers\n", "author": ["Posted by Ryan McDonald, Research Scientist, Google Research"], "link": "http://ai.googleblog.com/2012/06/recap-of-naacl-12-including-two-best.html", "abstract": "", "date": "\nThursday, June 14, 2012\n"},
{"website": "Google-AI", "title": "\n2012 Google PhD Fellowships\n", "author": ["Posted by Leslie Yeh Johnson, University Relations Manager"], "link": "http://ai.googleblog.com/2012/06/2012-google-phd-fellowships.html", "abstract": "", "date": "\nMonday, June 11, 2012\n"},
{"website": "Google-AI", "title": "\nHello science\u2014meet HR\n", "author": ["Posted by Jennifer Kurkoski, Ph.D., Manager, People & Innovation Lab"], "link": "http://ai.googleblog.com/2012/06/hello-sciencemeet-hr.html", "abstract": "", "date": "\nWednesday, June 6, 2012\n"},
{"website": "Google-AI", "title": "\nResearch at Google on G+: Featuring Excellent Papers for 2011\n", "author": ["Posted by Corinna Cortes, Google Research"], "link": "http://ai.googleblog.com/2012/06/research-at-google-on-g-featuring.html", "abstract": "", "date": "\nMonday, June 4, 2012\n"},
{"website": "Google-AI", "title": "\nFrom Words to Concepts and Back: Dictionaries for Linking Text, Entities and Ideas\n", "author": ["Posted by Valentin Spitkovsky and Peter Norvig, Research Team"], "link": "http://ai.googleblog.com/2012/05/from-words-to-concepts-and-back.html", "abstract": "", "date": "\nFriday, May 18, 2012\n"},
{"website": "Google-AI", "title": "\nJoining forces to support computer science majors\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2012/04/joining-forces-to-support-computer.html", "abstract": "", "date": "\nThursday, April 26, 2012\n"},
{"website": "Google-AI", "title": "\nWorking with your Data: Easier and More Fun\n", "author": ["Posted by Rebecca Shapley, Fusion Tables Team"], "link": "http://ai.googleblog.com/2012/04/working-with-your-data-easier-and-more.html", "abstract": "", "date": "\nThursday, April 12, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle App Engine Research Awards for scientific discovery\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations and Andrea Held, University Relations Program Manager"], "link": "http://ai.googleblog.com/2012/03/google-app-engine-research-awards-for.html", "abstract": "", "date": "\nThursday, March 29, 2012\n"},
{"website": "Google-AI", "title": "\nImpact of Organic Ranking on Ad Click Incrementality\n", "author": ["Posted by David Chan, Statistician and Lizzy Van Alstine, Research Evangelist"], "link": "http://ai.googleblog.com/2012/03/impact-of-organic-ranking-on-ad-click.html", "abstract": "", "date": "\nTuesday, March 27, 2012\n"},
{"website": "Google-AI", "title": "\nExcellent Papers for 2011\n", "author": ["Posted by Corinna Cortes and Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2012/03/excellent-papers-for-2011.html", "abstract": "", "date": "\nThursday, March 22, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle at INFOCOM 2012\n", "author": ["Posted by Emilie Danna, Google Research & Michal Segalov,Networking Software"], "link": "http://ai.googleblog.com/2012/03/google-at-infocom-2012.html", "abstract": "", "date": "\nWednesday, March 21, 2012\n"},
{"website": "Google-AI", "title": "\nGamification for Improved Search Ranking for YouTube Topics\n", "author": ["Posted by Charles DuHadway and Sanketh Shetty, Google Research"], "link": "http://ai.googleblog.com/2012/03/gamification-for-improved-search.html", "abstract": "", "date": "\nMonday, March 19, 2012\n"},
{"website": "Google-AI", "title": "\nSearch Ads Pause Studies Update\n", "author": ["Posted by Lizzy Van Alstine, Research Evangelist and David Chan, Statistician"], "link": "http://ai.googleblog.com/2012/03/search-ads-pause-studies-update.html", "abstract": "", "date": "\nMonday, March 12, 2012\n"},
{"website": "Google-AI", "title": "\nKeeping an \u201cOER mind\u201d about shared resources for education\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2012/03/keeping-oer-mind-about-shared-resources.html", "abstract": "", "date": "\nMonday, March 5, 2012\n"},
{"website": "Google-AI", "title": "\nAnnouncing Google-hosted workshop videos from NIPS 2011\n", "author": ["Posted by John Blitzer and Douglas Eck, Google Research"], "link": "http://ai.googleblog.com/2012/02/announcing-google-hosted-workshop.html", "abstract": "", "date": "\nThursday, February 23, 2012\n"},
{"website": "Google-AI", "title": "\n2011 EMEA Android Educational Outreach Program Awards Mobile Phones to Universities\n", "author": ["Posted by David Harper, Head of University Relations, EMEA"], "link": "http://ai.googleblog.com/2012/02/2011-emea-android-educational-outreach.html", "abstract": "", "date": "\nWednesday, February 22, 2012\n"},
{"website": "Google-AI", "title": "\nQuantifying comedy on YouTube: why the number of o\u2019s in your LOL matter\n", "author": ["Posted by Sanketh Shetty, YouTube Slam Team, Google Research\u00a0"], "link": "http://ai.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html", "abstract": "", "date": "\nThursday, February 9, 2012\n"},
{"website": "Google-AI", "title": "\nData and code open sourced from Google's Renewable Energy Cheaper than Coal project\n", "author": ["Posted by Ross Koningstein, Engineer, Google RE<C team"], "link": "http://ai.googleblog.com/2012/01/data-and-code-open-sourced-from-google.html", "abstract": "", "date": "\nMonday, January 30, 2012\n"},
{"website": "Google-AI", "title": "\nOpen-sourcing Sky Map and collaborating with Carnegie Mellon University\n", "author": ["Posted by John Taylor and Kevin Serafini"], "link": "http://ai.googleblog.com/2012/01/open-sourcing-sky-map-and-collaborating.html", "abstract": "", "date": "\nFriday, January 20, 2012\n"},
{"website": "Google-AI", "title": "\nCDC Birth Vital Statistics in BigQuery\n", "author": ["Posted by Dan Vanderkam, Software Engineer"], "link": "http://ai.googleblog.com/2012/01/cdc-birth-vital-statistics-in-bigquery.html", "abstract": "", "date": "\nFriday, January 13, 2012\n"},
{"website": "Google-AI", "title": "\nGoogle Correlate expands to 49 additional countries\n", "author": ["Posted by Matt Mohebbi, Software Engineer"], "link": "http://ai.googleblog.com/2012/01/google-correlate-expands-to-49.html", "abstract": "", "date": "\nTuesday, January 3, 2012\n"},
{"website": "Google-AI", "title": "\nAcademic Successes in Cluster Computing\n", "author": ["Posted by Alfred Spector, VP of Research"], "link": "http://ai.googleblog.com/2011/12/academic-successes-in-cluster-computing.html", "abstract": "", "date": "\nThursday, December 22, 2011\n"},
{"website": "Google-AI", "title": "\nMeasuring Ad Effectiveness Using Geo Experiments\n", "author": ["Posted by Lizzy Van Alstine and Jon Vaver, Quantitative Analysis Team"], "link": "http://ai.googleblog.com/2011/12/measuring-ad-effectiveness-using-geo.html", "abstract": "", "date": "\nFriday, December 9, 2011\n"},
{"website": "Google-AI", "title": "\nACM Fellows for 2011\n", "author": ["Posted by Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2011/12/acm-fellows-for-2011.html", "abstract": "", "date": "\nThursday, December 8, 2011\n"},
{"website": "Google-AI", "title": "\nOur second round of Google Research Awards for 2011\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2011/12/our-second-round-of-google-research.html", "abstract": "", "date": "\nTuesday, December 6, 2011\n"},
{"website": "Google-AI", "title": "\n2011 Google China Faculty Summit in Hangzhou\n", "author": ["Posted by Aimin Zhu, University Relationship Manager, Google China"], "link": "http://ai.googleblog.com/2011/12/2011-google-china-faculty-summit-in.html", "abstract": "", "date": "\nFriday, December 2, 2011\n"},
{"website": "Google-AI", "title": "\nMore Google Cluster Data\n", "author": ["Posted by John Wilkes, Principal Software Engineer"], "link": "http://ai.googleblog.com/2011/11/more-google-cluster-data.html", "abstract": "", "date": "\nTuesday, November 29, 2011\n"},
{"website": "Google-AI", "title": "\nDiscovering Talented Musicians with Acoustic Analysis\n", "author": ["Posted by Charles DuHadway, YouTube Slam Team, Google Research "], "link": "http://ai.googleblog.com/2011/11/discovering-talented-musicians-with.html", "abstract": "", "date": "\nWednesday, November 2, 2011\n"},
{"website": "Google-AI", "title": "\nFresh Perspectives about People and the Web from Think Quarterly\n", "author": ["Posted by Allison Mooney, Christina Park, and Caroline McCarthy, The Think Quarterly Team"], "link": "http://ai.googleblog.com/2011/09/fresh-perspectives-about-people-and-web.html", "abstract": "", "date": "\nWednesday, September 28, 2011\n"},
{"website": "Google-AI", "title": "\nTrying on the new Dynamic Views from Blogger\n", "author": [], "link": "http://ai.googleblog.com/2011/09/trying-on-new-dynamic-views-from-blogger.html", "abstract": "", "date": "\nTuesday, September 27, 2011\n"},
{"website": "Google-AI", "title": "\nSorting Petabytes with MapReduce - The Next Episode\n", "author": ["Posted by Grzegorz Czajkowski, Mari\u00e1n Dvorsk\u00fd, Jerry Zhao, and Michael Conley, Systems Infrastructure"], "link": "http://ai.googleblog.com/2011/09/sorting-petabytes-with-mapreduce-next.html", "abstract": "", "date": "\nWednesday, September 7, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle at the Joint Statistical Meetings in Miami\n", "author": ["Posted by Marianna Dizik, Statistician"], "link": "http://ai.googleblog.com/2011/08/google-at-joint-statistical-meetings-in.html", "abstract": "", "date": "\nMonday, August 22, 2011\n"},
{"website": "Google-AI", "title": "\nA new MIT center for mobile learning, with support from Google\n", "author": ["Posted by Hal Abelson, Professor of Computer Science and Engineering, MIT"], "link": "http://ai.googleblog.com/2011/08/a-new-mit-center-for-mobile-learning.html", "abstract": "", "date": "\nTuesday, August 16, 2011\n"},
{"website": "Google-AI", "title": "\nOur Faculty Institute brings faculty back to the drawing board\n", "author": [], "link": "http://ai.googleblog.com/2011/08/our-faculty-institute-brings-faculty.html", "abstract": "", "date": "\nFriday, August 12, 2011\n"},
{"website": "Google-AI", "title": "\nCulturomics, Ngrams and new power tools for Science\n", "author": ["Posted by Erez Lieberman Aiden and Jean-Baptiste Michel, Visiting Faculty at Google"], "link": "http://ai.googleblog.com/2011/08/culturomics-ngrams-and-new-power-tools.html", "abstract": "", "date": "\nWednesday, August 10, 2011\n"},
{"website": "Google-AI", "title": "\nPresident's Council Recommends Open Data for Federal Agencies\n", "author": ["Posted by Alon Halevy, Senior Staff Research Scientist"], "link": "http://ai.googleblog.com/2011/07/president-council-recommends-open-data.html", "abstract": "", "date": "\nThursday, July 28, 2011\n"},
{"website": "Google-AI", "title": "\nStudies Show Search Ads Drive 89% Incremental Traffic\n", "author": ["Posted by David Chan and Lizzy Van Alstine, Quantitative Management Team"], "link": "http://ai.googleblog.com/2011/07/studies-show-search-ads-drive-89.html", "abstract": "", "date": "\nThursday, July 21, 2011\n"},
{"website": "Google-AI", "title": "\nFaculty from across the Americas meet in New York for the Faculty Summit\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2011/07/faculty-from-across-americas-meet-in.html", "abstract": "", "date": "\nWednesday, July 20, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Americas Faculty Summit: Reflections from our attendees\n", "author": ["Posted by Alfred Spector, Vice President, Research"], "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit.html", "abstract": "", "date": "\nTuesday, July 19, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Americas Faculty Summit Day 2: Shopping, Coupons and Data\n", "author": ["Posted by Andrew W. Moore, Director, Google Commerce and Site Director, Pittsburgh"], "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-2.html", "abstract": "", "date": "\nMonday, July 18, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Americas Faculty Summit Day 1: Cluster Management\n", "author": ["Posted by John Wilkes, Principal Software Engineer"], "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-1.html", "abstract": "", "date": "\nFriday, July 15, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Americas Faculty Summit Day 1: Mobile Search\n", "author": ["Posted by Johan Schalkwyk, Software Engineer"], "link": "http://ai.googleblog.com/2011/07/google-americas-faculty-summit-day-1_15.html", "abstract": "", "date": "\nFriday, July 15, 2011\n"},
{"website": "Google-AI", "title": "\nWhat You Capture Is What You Get: A New Way for Task Migration Across Devices\n", "author": ["Posted by Yang Li, Research Scientist"], "link": "http://ai.googleblog.com/2011/07/what-you-capture-is-what-you-get-new.html", "abstract": "", "date": "\nTuesday, July 12, 2011\n"},
{"website": "Google-AI", "title": "\nLanguages of the World (Wide Web)\n", "author": ["Posted by Daniel Ford and Josh Batson"], "link": "http://ai.googleblog.com/2011/07/languages-of-world-wide-web.html", "abstract": "", "date": "\nThursday, July 7, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Translate welcomes you to the Indic web\n", "author": ["Posted by Ashish Venugopal, Research Scientist "], "link": "http://ai.googleblog.com/2011/06/google-translate-welcomes-you-to-indic.html", "abstract": "", "date": "\nTuesday, June 21, 2011\n"},
{"website": "Google-AI", "title": "\nAuto-Directed Video Stabilization with Robust L1 Optimal Camera Paths\n", "author": ["Posted by ", ", ", ", and ", ", Research Team"], "link": "http://ai.googleblog.com/2011/06/auto-directed-video-stabilization-with.html", "abstract": "", "date": "\nMonday, June 20, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle at CVPR 2011\n", "author": ["Posted by Mei Han and Sergey Ioffe, Research Team"], "link": "http://ai.googleblog.com/2011/06/google-at-cvpr-2011.html", "abstract": "", "date": "\nThursday, June 16, 2011\n"},
{"website": "Google-AI", "title": "\nOur first round of Google Research Awards for 2011\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2011/06/our-first-round-of-google-research.html", "abstract": "", "date": "\nThursday, June 9, 2011\n"},
{"website": "Google-AI", "title": "\nInstant Mix for Music Beta by Google\n", "author": ["Posted by Douglas Eck, Research Scientist"], "link": "http://ai.googleblog.com/2011/06/instant-mix-for-music-beta-by-google.html", "abstract": "", "date": "\nWednesday, June 8, 2011\n"},
{"website": "Google-AI", "title": "\nAfter the award: students and mentors\n", "author": ["Posted by Leslie Yeh Johnson, University Relations Manager"], "link": "http://ai.googleblog.com/2011/06/after-award-students-and-mentors.html", "abstract": "", "date": "\nMonday, June 6, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle Scribe: Now with automatic text for links and faster formatting options\n", "author": ["Posted by Kartik Singh and Kuntal Loya, Google Scribe team"], "link": "http://ai.googleblog.com/2011/05/google-scribe-now-with-automatic-text.html", "abstract": "", "date": "\nThursday, May 26, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle at ACL 2011\n", "author": ["Posted by Ryan McDonald and Fernando Pereira, Research Team"], "link": "http://ai.googleblog.com/2011/05/google-at-acl-2011.html", "abstract": "", "date": "\nWednesday, May 18, 2011\n"},
{"website": "Google-AI", "title": "\nMake beautiful interactive maps even faster with new additions to the Fusion Tables API\n", "author": ["Posted by Rebecca Shapley, Jayant Madhavan, Rod McChesney, and Kathryn Hurley, Fusion Tables team"], "link": "http://ai.googleblog.com/2011/05/make-beautiful-interactive-maps-even.html", "abstract": "", "date": "\nTuesday, May 10, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle at CHI 2011\n", "author": ["Posted by Yang Li, Research Scientist"], "link": "http://ai.googleblog.com/2011/05/google-at-chi-2011.html", "abstract": "", "date": "\nThursday, May 5, 2011\n"},
{"website": "Google-AI", "title": "\nPartnering with Tsinghua University to support education in Western China\n", "author": ["Posted by Aimin Zhu, China University Relations"], "link": "http://ai.googleblog.com/2011/04/partnering-with-tsinghua-university-to.html", "abstract": "", "date": "\nThursday, April 14, 2011\n"},
{"website": "Google-AI", "title": "\n1 billion core-hours of computational capacity for researchers\n", "author": ["Posted by Dan Belov, Principal Engineer and David Konerding, Software Engineer"], "link": "http://ai.googleblog.com/2011/04/1-billion-core-hours-of-computational.html", "abstract": "", "date": "\nThursday, April 7, 2011\n"},
{"website": "Google-AI", "title": "\nOverlapping Experiment Infrastructure: More, Better, Faster Experimentation\n", "author": ["Posted by Deirdre O'Brien and Diane Tang, Adwords Team"], "link": "http://ai.googleblog.com/2011/04/overlapping-experiment-infrastructure.html", "abstract": "", "date": "\nMonday, April 4, 2011\n"},
{"website": "Google-AI", "title": "\nIg-pay Atin-lay Oice-vay Earch-say\n", "author": ["Posted by Martin Jansche and Alex Salcianu, Google Speech Team"], "link": "http://ai.googleblog.com/2011/04/ig-pay-atin-lay-oice-vay-earch-say.html", "abstract": "", "date": "\nFriday, April 1, 2011\n"},
{"website": "Google-AI", "title": "\nWord of Mouth: Introducing Voice Search for Indonesian, Malaysian and Latin American Spanish\n", "author": [], "link": "http://ai.googleblog.com/2011/03/word-of-mouth-introducing-voice-search.html", "abstract": "", "date": "\nWednesday, March 30, 2011\n"},
{"website": "Google-AI", "title": "\nReading tea leaves in the tourism industry: A Case Study in the Gulf Oil Spill\n", "author": ["Posted by Hyunyoung Choi and Paul Liu, Senior Economists"], "link": "http://ai.googleblog.com/2011/03/reading-tea-leaves-in-tourism-industry.html", "abstract": "", "date": "\nThursday, March 24, 2011\n"},
{"website": "Google-AI", "title": "\nGames, auctions and beyond\n", "author": ["Posted by Yossi Matias, Senior Director, Head of Israel R&D Center"], "link": "http://ai.googleblog.com/2011/03/games-auctions-and-beyond.html", "abstract": "", "date": "\nWednesday, March 16, 2011\n"},
{"website": "Google-AI", "title": "\nLarge Scale Image Annotation: Learning to Rank with Joint Word-Image Embeddings\n", "author": ["Posted by Jason Weston and Samy Bengio, Research Team"], "link": "http://ai.googleblog.com/2011/03/large-scale-image-annotation-learning.html", "abstract": "", "date": "\nThursday, March 10, 2011\n"},
{"website": "Google-AI", "title": "\nBuilding resources to syntactically parse the web\n", "author": ["Posted by Slav Petrov and Ryan McDonald, Research Team"], "link": "http://ai.googleblog.com/2011/03/building-resources-to-syntactically.html", "abstract": "", "date": "\nWednesday, March 9, 2011\n"},
{"website": "Google-AI", "title": "\nSlicing and dicing data for interactive visualization\n", "author": ["Posted by Benjamin Yolken, Google Public Data Product Manager"], "link": "http://ai.googleblog.com/2011/02/slicing-and-dicing-data-for-interactive.html", "abstract": "", "date": "\nMonday, February 28, 2011\n"},
{"website": "Google-AI", "title": "\nWhere does my data live?\n", "author": ["Posted by Daniel Ford, Senior Mathematician"], "link": "http://ai.googleblog.com/2011/02/where-does-my-data-live.html", "abstract": "", "date": "\nFriday, February 25, 2011\n"},
{"website": "Google-AI", "title": "\nA Runtime Solution for Online Contention Detection and Response\n", "author": ["Posted by Jason Mars, Software Engineering Intern"], "link": "http://ai.googleblog.com/2011/02/a-runtime-solution-for-online.html", "abstract": "", "date": "\nFriday, February 25, 2011\n"},
{"website": "Google-AI", "title": "\nCongratulations to Ken Thompson\n", "author": ["Posted by Bill Coughran, Senior Vice President of Engineering"], "link": "http://ai.googleblog.com/2011/02/congratulations-to-ken-thompson.html", "abstract": "", "date": "\nTuesday, February 22, 2011\n"},
{"website": "Google-AI", "title": "\nQuery Language Modeling for Voice Search\n", "author": ["Posted by Ciprian Chelba, Research Scientist"], "link": "http://ai.googleblog.com/2011/02/query-language-modeling-for-voice-search.html", "abstract": "", "date": "\nThursday, February 17, 2011\n"},
{"website": "Google-AI", "title": "\nJulia meets HTML 5\n", "author": ["Posted by Daniel Wolf, Software Engineer"], "link": "http://ai.googleblog.com/2011/01/julia-meets-html-5.html", "abstract": "", "date": "\nMonday, January 31, 2011\n"},
{"website": "Google-AI", "title": "\nGoogle at NIPS 2010\n", "author": ["Posted by Slav Petrov, Doug Aberdeen, and Lisa McCracken, Google Research"], "link": "http://ai.googleblog.com/2011/01/google-at-nips-2010.html", "abstract": "", "date": "\nThursday, January 27, 2011\n"},
{"website": "Google-AI", "title": "\nMore Google Contributions to the Broader Scientific Community\n", "author": ["Posted by Corinna Cortes and Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2011/01/more-google-contributions-to-broader.html", "abstract": "", "date": "\nTuesday, January 25, 2011\n"},
{"website": "Google-AI", "title": "\nSupporting computer science education with CS4HS\n", "author": ["Posted by Terry Ednacot, Education Program Manager"], "link": "http://ai.googleblog.com/2011/01/supporting-computer-science-education.html", "abstract": "", "date": "\nThursday, January 20, 2011\n"},
{"website": "Google-AI", "title": "\nMore researchers dive into the digital humanities\n", "author": ["Posted by  Jon Orwant, Engineering Manager for Google Books"], "link": "http://ai.googleblog.com/2010/12/more-researchers-dive-into-digital.html", "abstract": "", "date": "\nMonday, December 20, 2010\n"},
{"website": "Google-AI", "title": "\nRobot hackathon connects with Android, browsers and the cloud\n", "author": ["Posted by Ryan Hickman and Mamie Rheingold, 20% Robotics Task Force"], "link": "http://ai.googleblog.com/2010/12/robot-hackathon-connects-with-android.html", "abstract": "", "date": "\nFriday, December 17, 2010\n"},
{"website": "Google-AI", "title": "\nFind out what\u2019s in a word, or five, with the Google Books Ngram Viewer\n", "author": ["Posted by Jon Orwant, Engineering Manager, Google Books"], "link": "http://ai.googleblog.com/2010/12/find-out-whats-in-word-or-five-with.html", "abstract": "", "date": "\nThursday, December 16, 2010\n"},
{"website": "Google-AI", "title": "\nLetting everyone do great things with App Inventor\n", "author": ["Posted by Karen Parker, App Inventor Program Manager"], "link": "http://ai.googleblog.com/2010/12/letting-everyone-do-great-things-with.html", "abstract": "", "date": "\nWednesday, December 15, 2010\n"},
{"website": "Google-AI", "title": "\n$6 million to faculty in Q4 Research Awards\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2010/12/6-million-to-faculty-in-q4-research.html", "abstract": "", "date": "\nWednesday, December 8, 2010\n"},
{"website": "Google-AI", "title": "\nFour Googlers elected ACM Fellows this year\n", "author": ["Posted by Alfred Spector, VP of Research"], "link": "http://ai.googleblog.com/2010/12/four-googlers-elected-acm-fellows-this.html", "abstract": "", "date": "\nTuesday, December 7, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Launches Cantonese Voice Search in Hong Kong\n", "author": ["Posted by Posted by Yun-hsuan Sung (\u5b8b\u96f2\u8ed2) and Martin Jansche, Google Research"], "link": "http://ai.googleblog.com/2010/12/google-launches-cantonese-voice-search.html", "abstract": "", "date": "\nThursday, December 2, 2010\n"},
{"website": "Google-AI", "title": "\nVoice Search in Underrepresented Languages\n", "author": ["Posted by Pedro J. Moreno, Staff Research Scientist and Johan Schalkwyk, Senior Staff Engineer"], "link": "http://ai.googleblog.com/2010/11/voice-search-in-underrepresented.html", "abstract": "", "date": "\nTuesday, November 9, 2010\n"},
{"website": "Google-AI", "title": "\nSuggesting a Better Remote Control\n", "author": ["Posted by Ullas Gargi and Rich Gossweiler, Research Team"], "link": "http://ai.googleblog.com/2010/11/suggesting-better-remote-control.html", "abstract": "", "date": "\nThursday, November 4, 2010\n"},
{"website": "Google-AI", "title": "\nExploring Computational Thinking\n", "author": ["Posted by Elaine Kao, Education Program Manager"], "link": "http://ai.googleblog.com/2010/10/exploring-computational-thinking.html", "abstract": "", "date": "\nMonday, October 25, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle at the Conference on Empirical Methods in Natural Language Processing (EMNLP '10)\n", "author": ["Posted by Slav Petrov, Research Scientist"], "link": "http://ai.googleblog.com/2010/10/google-at-conference-on-empirical.html", "abstract": "", "date": "\nMonday, October 18, 2010\n"},
{"website": "Google-AI", "title": "\nKuzman Ganchev Receives Presidential Award from the Republic of Bulgaria\n", "author": ["Posted by Slav Petrov, Research Scientist"], "link": "http://ai.googleblog.com/2010/10/kuzman-ganchev-receives-presidential.html", "abstract": "", "date": "\nFriday, October 15, 2010\n"},
{"website": "Google-AI", "title": "\nKorean Voice Input -- Have you Dictated your E-Mails in Korean lately?\n", "author": ["Posted by Mike Schuster & Kaisuke Nakajima, Google Research"], "link": "http://ai.googleblog.com/2010/10/korean-voice-input-have-you-dictated.html", "abstract": "", "date": "\nThursday, October 14, 2010\n"},
{"website": "Google-AI", "title": "\nClustering Related Queries Based on User Intent\n", "author": ["Posted by Jayant Madhavan and Alon Halevy"], "link": "http://ai.googleblog.com/2010/10/clustering-related-queries-based-on.html", "abstract": "", "date": "\nWednesday, October 13, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle at USENIX Symposium on Operating Systems Design and Implementation (OSDI \u201810)\n", "author": ["Posted by Murray Stokely, Software Engineer"], "link": "http://ai.googleblog.com/2010/10/google-at-usenix-symposium-on-operating.html", "abstract": "The  (OSDI \u201810) was recently held in Vancouver, B.C.  This biennial conference is one of the premiere forums for presenting innovative research in distributed systems from both academia and industry, and we were glad to be a part of it.In addition to sponsoring this conference since 2002, Googlers contributed to the exchange of scientific ideas through authoring or co-authoring 3 published papers, organizing workshops, and serving on the program committee.  A short summary of the contributions:In addition to the papers presented by current Googlers, we were also happy to see that the recipient of the , , presented her work on .Videos of all of the talks from OSDI are available on the  for attendees and current USENIX members.  There is also a  with a growing subset of the conference videos open to everyone.Google is making substantial progress on many of the grand challenge problems in computer science and artificial intelligence as part of its mission to organize the worlds information and make it useful.  Given the continuing increase in the scale of our distributed systems it\u2019s fair to say we\u2019ll have some other exciting new work to share at the next OSDI.  Hope to see you in 2012.", "date": "\nTuesday, October 12, 2010\n"},
{"website": "Google-AI", "title": "\nMaking an Impact on a Thriving Speech Research Community\n", "author": ["Posted by Vincent Vanhoucke, Google Research"], "link": "http://ai.googleblog.com/2010/10/making-impact-on-thriving-speech.html", "abstract": "", "date": "\nMonday, October 11, 2010\n"},
{"website": "Google-AI", "title": "\nBowls and Learning\n", "author": ["Posted by Phil Long, Research Team"], "link": "http://ai.googleblog.com/2010/10/bowls-and-learning.html", "abstract": "", "date": "\nThursday, October 7, 2010\n"},
{"website": "Google-AI", "title": "\nPoetic Machine Translation\n", "author": ["Posted by Dmitriy Genzel, Software Engineer"], "link": "http://ai.googleblog.com/2010/10/poetic-machine-translation.html", "abstract": "", "date": "\nTuesday, October 5, 2010\n"},
{"website": "Google-AI", "title": "\nVeni, Vidi, Verba Verti\n", "author": ["Posted by Jakob Uszkoreit, Ingeniarius Programmandi"], "link": "http://ai.googleblog.com/2010/09/veni-vidi-verba-verti.html", "abstract": "", "date": "\nThursday, September 30, 2010\n"},
{"website": "Google-AI", "title": "\nRemembering Fred Jelinek\n", "author": ["Posted by Ciprian Chelba, Research Team"], "link": "http://ai.googleblog.com/2010/09/remembering-fred-jelinek.html", "abstract": "", "date": "\nFriday, September 17, 2010\n"},
{"website": "Google-AI", "title": "\nFrowns, Sighs, and Advanced Queries -- How does search behavior change as search becomes more difficult?\n", "author": ["Posted by Anne Aula, Rehan Khan, and Zhiwei Guan, User Experience Team"], "link": "http://ai.googleblog.com/2010/09/frowns-sighs-and-advanced-queries-how.html", "abstract": "", "date": "\nFriday, September 17, 2010\n"},
{"website": "Google-AI", "title": "\nFocusing on Our Users: The Google Health Redesign\n", "author": ["Posted by Hendrik Mueller, User Experience Researcher"], "link": "http://ai.googleblog.com/2010/09/focusing-on-our-users-google-health.html", "abstract": "", "date": "\nWednesday, September 15, 2010\n"},
{"website": "Google-AI", "title": "\nDiscontinuous Seam Carving for Video Retargeting\n", "author": ["Posted by Matthias Grundmann and Vivek Kwatra, Google Research"], "link": "http://ai.googleblog.com/2010/09/discontinuous-seam-carving-for-video.html", "abstract": "", "date": "\nMonday, September 13, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Search by Voice:  A Case Study\n", "author": ["Posted by Johan Schalkwyk, Google Research"], "link": "http://ai.googleblog.com/2010/09/google-search-by-voice-case-study.html", "abstract": "", "date": "\nThursday, September 9, 2010\n"},
{"website": "Google-AI", "title": "\nTowards Energy-Proportional Datacenters\n", "author": ["Posted by Dennis Abts, Michael R. Marty, Philip M. Wells, Peter Klausler, and Hong Liu"], "link": "http://ai.googleblog.com/2010/09/towards-energy-proportional-datacenters.html", "abstract": "", "date": "\nWednesday, September 1, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle North American Faculty Summit - Day 2\n", "author": ["Posted by Andrew Tomkins, Director of Engineering, Google Research"], "link": "http://ai.googleblog.com/2010/08/google-north-american-faculty-summit.html", "abstract": "", "date": "\nWednesday, August 4, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle North American Faculty Summit - cloud computing\n", "author": ["Posted by Brian Bershad, Director of Engineering, Site Director, Google Seattle"], "link": "http://ai.googleblog.com/2010/08/google-north-american-faculty-summit_3.html", "abstract": "", "date": "\nTuesday, August 3, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Publications\n", "author": ["Posted by Corinna Cortes and Alfred Spector, Google Research"], "link": "http://ai.googleblog.com/2010/07/google-publications.html", "abstract": "", "date": "\nFriday, July 30, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle North American Faculty Summit - Day 1\n", "author": ["Posted by \u00dalfar Erlingsson, Manager, Security Research"], "link": "http://ai.googleblog.com/2010/07/google-north-american-faculty-summit.html", "abstract": "", "date": "\nFriday, July 30, 2010\n"},
{"website": "Google-AI", "title": "\nAnd the award goes to...\n", "author": ["Posted by Fernando Pereira, Research Director"], "link": "http://ai.googleblog.com/2010/07/and-award-goes-to.html", "abstract": "", "date": "\nTuesday, July 27, 2010\n"},
{"website": "Google-AI", "title": "\nGooglers receive multiple awards at the 2010 International Conference on Machine Learning\n", "author": ["Posted by Fernando Pereira, Research Director"], "link": "http://ai.googleblog.com/2010/07/googlers-receive-multiple-awards-at.html", "abstract": "", "date": "\nTuesday, July 27, 2010\n"},
{"website": "Google-AI", "title": "\nAnnouncing our Q2 Research Awards\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations"], "link": "http://ai.googleblog.com/2010/07/announcing-our-q2-research-awards.html", "abstract": "", "date": "\nThursday, July 22, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle PhD Fellowships go international\n", "author": ["Posted by Alfred Spector, VP of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2010/07/google-phd-fellowships-go-international.html", "abstract": "", "date": "\nThursday, July 15, 2010\n"},
{"website": "Google-AI", "title": "\nOur commitment to the digital humanities\n", "author": [], "link": "http://ai.googleblog.com/2010/07/our-commitment-to-digital-humanities.html", "abstract": "", "date": "\nWednesday, July 14, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle launches Korean Voice Search\n", "author": ["Posted by Mike Schuster & Martin Jansche, Google Research"], "link": "http://ai.googleblog.com/2010/06/google-launches-korean-voice-search.html", "abstract": "", "date": "\nWednesday, June 30, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Search by Voice now available in France, Italy, Germany and Spain\n", "author": ["Posted by Thad Hughes, Martin Jansche, and Pedro Moreno, Google Research"], "link": "http://ai.googleblog.com/2010/06/google-search-by-voice-now-available-in.html", "abstract": "", "date": "\nMonday, June 14, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Fusion Tables celebrates one year of data management\n", "author": ["Posted by Alon Halevy, Google Research and Rebecca Shapley, User Experience"], "link": "http://ai.googleblog.com/2010/06/google-fusion-tables-celebrates-one.html", "abstract": "", "date": "\nWednesday, June 9, 2010\n"},
{"website": "Google-AI", "title": "\nRecent Accomplishments by Research Award Recipients\n", "author": ["Posted by Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2010/05/recent-accomplishments-by-research.html", "abstract": "", "date": "\nWednesday, May 19, 2010\n"},
{"website": "Google-AI", "title": "\nFive more languages on translate.google.com\n", "author": ["Posted by Ashish Venugopal, Research Scientist"], "link": "http://ai.googleblog.com/2010/05/five-more-languages-on.html", "abstract": "", "date": "\nThursday, May 13, 2010\n"},
{"website": "Google-AI", "title": "\nLessons learned developing a practical large scale machine learning system\n", "author": ["Posted by Simon Tong, Google Research"], "link": "http://ai.googleblog.com/2010/04/lessons-learned-developing-practical.html", "abstract": "", "date": "\nTuesday, April 6, 2010\n"},
{"website": "Google-AI", "title": "\nHopping on a Face Manifold via People Hopper\n", "author": ["Posted by Sanjiv Kumar and Henry Rowley, Google Research"], "link": "http://ai.googleblog.com/2010/03/hopping-on-face-manifold-via-people.html", "abstract": "", "date": "\nWednesday, March 3, 2010\n"},
{"website": "Google-AI", "title": "\nAnnouncing Google's Focused Research Awards\n", "author": ["Posted by Alfred Spector, Vice President of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2010/02/announcing-google-focused-research.html", "abstract": "", "date": "\nTuesday, February 2, 2010\n"},
{"website": "Google-AI", "title": "\nResearch Areas of Interest:  Building scalable, robust cluster applications\n", "author": ["Posted by Brad Chen, Technical Lead/Manager"], "link": "http://ai.googleblog.com/2010/01/research-areas-of-interest-building.html", "abstract": "", "date": "\nWednesday, January 27, 2010\n"},
{"website": "Google-AI", "title": "\nGoogle Cluster Data\n", "author": ["Posted by Joseph L. Hellerstein, Manager of Google Performance Analytics"], "link": "http://ai.googleblog.com/2010/01/google-cluster-data.html", "abstract": "", "date": "\nThursday, January 7, 2010\n"},
{"website": "Google-AI", "title": "\nAnnouncing our Q4 Research Awards\n", "author": ["Posted by Maggie Johnson, Director of Education & University Relations and Jeff Walz, Head of University Relations"], "link": "http://ai.googleblog.com/2009/12/announcing-our-q4-research-awards.html", "abstract": "", "date": "\nTuesday, December 22, 2009\n"},
{"website": "Google-AI", "title": "\nTeaching a Computer to Understand Japanese\n", "author": ["Posted by Mike Schuster, Google Research and Kaisuke Nakajima, Google Japan"], "link": "http://ai.googleblog.com/2009/12/teaching-computer-to-understand-japanese.html", "abstract": "", "date": "\nTuesday, December 15, 2009\n"},
{"website": "Google-AI", "title": "\nResearch Areas of Interest - Multimedia\n", "author": ["Posted by Michele Covell, Vision Research Team"], "link": "http://ai.googleblog.com/2009/12/research-areas-of-interest-multimedia.html", "abstract": "", "date": "\nThursday, December 10, 2009\n"},
{"website": "Google-AI", "title": "\nMachine Learning with Quantum Algorithms\n", "author": ["Posted by Hartmut Neven, Technical Lead Manager Image Recognition"], "link": "http://ai.googleblog.com/2009/12/machine-learning-with-quantum-algorithms.html", "abstract": "", "date": "\nTuesday, December 8, 2009\n"},
{"website": "Google-AI", "title": "\nCelebrating Computer Science Education Week\n", "author": ["Posted by Alfred Spector, VP Research and Special Initiatives and Maggie Johnson, Director of Education and University Relations"], "link": "http://ai.googleblog.com/2009/12/celebrating-computer-science-education.html", "abstract": "", "date": "\nMonday, December 7, 2009\n"},
{"website": "Google-AI", "title": "\nJoin us for the 2010 Google GRAD CS Forum!\n", "author": ["Posted by Hanah Kim, University Programs"], "link": "http://ai.googleblog.com/2009/12/join-us-for-2010-google-grad-cs-forum.html", "abstract": "", "date": "\nMonday, December 7, 2009\n"},
{"website": "Google-AI", "title": "\nAutomatic Captioning in YouTube\n", "author": ["Posted by Christopher Alberti and Michiel Bacchiani, Google Research"], "link": "http://ai.googleblog.com/2009/12/automatic-captioning-in-youtube.html", "abstract": "", "date": "\nFriday, December 4, 2009\n"},
{"website": "Google-AI", "title": "\nFour Googlers elected ACM Fellows\n", "author": ["Posted by Alfred Spector, VP of Research"], "link": "http://ai.googleblog.com/2009/12/four-googlers-elected-acm-fellows.html", "abstract": "", "date": "\nTuesday, December 1, 2009\n"},
{"website": "Google-AI", "title": "\nExplore Images with Google Image Swirl\n", "author": ["Posted by Yushi Jing and Henry Rowley, Google Research"], "link": "http://ai.googleblog.com/2009/11/explore-images-with-google-image-swirl.html", "abstract": "", "date": "\nMonday, November 23, 2009\n"},
{"website": "Google-AI", "title": "\nThe 50th Symposium on Foundations of Computer Science (FOCS)\n", "author": ["Posted by ", " and ", ", Google Research, NY"], "link": "http://ai.googleblog.com/2009/11/the-50th-symposium-on-foundations-of.html", "abstract": "", "date": "\nFriday, November 13, 2009\n"},
{"website": "Google-AI", "title": "\nA 2x Faster Web\n", "author": ["Posted by Mike Belshe, Software Engineer and Roberto Peon, Software Engineer"], "link": "http://ai.googleblog.com/2009/11/a-2x-faster-web.html", "abstract": "", "date": "\nThursday, November 12, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle Search by Voice Learns Mandarin Chinese\n", "author": ["Posted by Pedro J. Moreno, Research Scientist"], "link": "http://ai.googleblog.com/2009/11/google-search-by-voice-learns-mandarin.html", "abstract": "", "date": "\nMonday, November 2, 2009\n"},
{"website": "Google-AI", "title": "\n51 Languages in Google Translate\n", "author": ["Posted by Franz Och, Principal Scientist"], "link": "http://ai.googleblog.com/2009/08/51-languages-in-google-translate.html", "abstract": "", "date": "\nMonday, August 31, 2009\n"},
{"website": "Google-AI", "title": "\nOn the predictability of Search Trends\n", "author": ["Posted by Yossi Matias, Niv Efron, and Yair Shimshoni, Google Labs, Israel."], "link": "http://ai.googleblog.com/2009/08/on-predictability-of-search-trends.html", "abstract": "", "date": "\nMonday, August 17, 2009\n"},
{"website": "Google-AI", "title": "\nUnder the Hood of App Inventor for Android\n", "author": ["Posted by Bill Magnuson, Hal Abelson, and Mark Friedman"], "link": "http://ai.googleblog.com/2009/08/under-hood-of-app-inventor-for-android.html", "abstract": "", "date": "\nTuesday, August 11, 2009\n"},
{"website": "Google-AI", "title": "\nTwo Views from the 2009 Google Faculty Summit\n", "author": ["Posted by Alfred Spector, Vice President of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2009/08/two-views-from-2009-google-faculty.html", "abstract": "", "date": "\nMonday, August 3, 2009\n"},
{"website": "Google-AI", "title": "\nApp Inventor for Android\n", "author": ["Posted by Hal Abelson, Visiting Faculty"], "link": "http://ai.googleblog.com/2009/07/app-inventor-for-android.html", "abstract": "", "date": "\nFriday, July 31, 2009\n"},
{"website": "Google-AI", "title": "\nPredicting Initial Claims for Unemployment Benefits\n", "author": ["Posted by Hal Varian, Chief Economist and Hyunyoung Choi, Sr. Economist"], "link": "http://ai.googleblog.com/2009/07/predicting-initial-claims-for.html", "abstract": "", "date": "\nWednesday, July 22, 2009\n"},
{"website": "Google-AI", "title": "\nACM EC Conference and Workshop on Ad Auctions\n", "author": ["By ", " and ", ", Google Research, NY"], "link": "http://ai.googleblog.com/2009/07/acm-ec-conference-and-workshop-on-ad.html", "abstract": "", "date": "\nTuesday, July 21, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle's Research Awards Program Update\n", "author": ["Posted by Posted by Juan E. Vargas, University Relations"], "link": "http://ai.googleblog.com/2009/07/google-research-awards-program-update.html", "abstract": "", "date": "\nTuesday, July 14, 2009\n"},
{"website": "Google-AI", "title": "\nInternational Conference on Machine Learning (ICML 2009) in Montreal\n", "author": ["Posted by ", " and ", ", Google Research, NY"], "link": "http://ai.googleblog.com/2009/07/international-conference-on-machine.html", "abstract": "", "date": "\nThursday, July 2, 2009\n"},
{"website": "Google-AI", "title": "\nSpeed Matters\n", "author": ["Posted by Jake Brutlag, Web Search Infrastructure"], "link": "http://ai.googleblog.com/2009/06/speed-matters.html", "abstract": "", "date": "\nTuesday, June 23, 2009\n"},
{"website": "Google-AI", "title": "\nA new landmark in computer vision\n", "author": ["Posted by Jay Yagnik, Head of Computer Vision Research"], "link": "http://ai.googleblog.com/2009/06/a-new-landmark-in-computer-vision.html", "abstract": "", "date": "\nMonday, June 22, 2009\n"},
{"website": "Google-AI", "title": "\nLarge-scale graph computing at Google\n", "author": ["Posted by Grzegorz Czajkowski, Systems Infrastructure Team"], "link": "http://ai.googleblog.com/2009/06/large-scale-graph-computing-at-google.html", "abstract": "", "date": "\nMonday, June 15, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle Fusion Tables\n", "author": ["Posted by Alon Halevy, Google Research and Rebecca Shapley, User Experience"], "link": "http://ai.googleblog.com/2009/06/google-fusion-tables.html", "abstract": "", "date": "\nTuesday, June 9, 2009\n"},
{"website": "Google-AI", "title": "\nRemembering Rajeev Motwani\n", "author": ["Posted by Alfred Spector, VP of Research"], "link": "http://ai.googleblog.com/2009/06/remembering-rajeev-motwani.html", "abstract": "", "date": "\nMonday, June 8, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle Fellowships, the Nuts and Bolts\n", "author": ["Posted by Leslie Yeh Johnson, Google University Relations"], "link": "http://ai.googleblog.com/2009/05/google-fellowships-nuts-and-bolts.html", "abstract": "", "date": "\nFriday, May 15, 2009\n"},
{"website": "Google-AI", "title": "\nThe best and the brightest\n", "author": ["Posted by Leslie Yeh Johnson, Google University Relations"], "link": "http://ai.googleblog.com/2009/05/the-best-and-brightest.html", "abstract": "", "date": "\nFriday, May 15, 2009\n"},
{"website": "Google-AI", "title": "\nACM Multimedia 2009 Grand Challenges\n", "author": ["Posted by Jay Yagnik, Head of Computer Vision Research"], "link": "http://ai.googleblog.com/2009/05/acm-multimedia-2009-grand-challenges.html", "abstract": "", "date": "\nTuesday, May 12, 2009\n"},
{"website": "Google-AI", "title": "\nThe bar-bet phenomenon:  increasing diversity in mobile searches\n", "author": ["Posted by Maryam Kamvar, Melanie Kellar, Rajan Patel and Ya Xu, Google Research"], "link": "http://ai.googleblog.com/2009/05/the-bar-bet-phenomenon-increasing.html", "abstract": "", "date": "\nThursday, May 7, 2009\n"},
{"website": "Google-AI", "title": "\nCloud Computing and the Internet\n", "author": ["Posted by Vinton Cerf, Chief Internet Evangelist"], "link": "http://ai.googleblog.com/2009/04/cloud-computing-and-internet.html", "abstract": "", "date": "\nTuesday, April 28, 2009\n"},
{"website": "Google-AI", "title": "\nThe Continuing Metamorphosis of the Web\n", "author": ["Posted by Alfred Spector, VP Research and Special Initiatives"], "link": "http://ai.googleblog.com/2009/04/the-continuing-metamorphosis-of-web.html", "abstract": "", "date": "\nMonday, April 27, 2009\n"},
{"website": "Google-AI", "title": "\nCongratulations to NSF CLuE Grant awardees\n", "author": ["Posted by Jeff Walz and Andrea Held"], "link": "http://ai.googleblog.com/2009/04/congratulations-to-nsf-clue-grant.html", "abstract": "", "date": "\nThursday, April 23, 2009\n"},
{"website": "Google-AI", "title": "\nSocially Adjusted CAPTCHAs\n", "author": ["Posted by Rich Gossweiler, Maryam Kamvar, Shumeet Baluja"], "link": "http://ai.googleblog.com/2009/04/socially-adjusted-captchas.html", "abstract": "", "date": "\nThursday, April 16, 2009\n"},
{"website": "Google-AI", "title": "\nThe Grill: Google's Alfred Spector on the hot seat\n", "author": ["Posted by Ben Bayer, Google Research"], "link": "http://ai.googleblog.com/2009/04/the-grill-google-alfred-spector-on-hot.html", "abstract": "", "date": "\nWednesday, April 15, 2009\n"},
{"website": "Google-AI", "title": "\nPredicting the Present with Google Trends\n", "author": ["Posted by Hal Varian, Chief Economist and Hyunyoung Choi, Decision Support Engineering Analyst"], "link": "http://ai.googleblog.com/2009/04/predicting-present-with-google-trends.html", "abstract": "", "date": "\nThursday, April 2, 2009\n"},
{"website": "Google-AI", "title": "\nThe Unreasonable Effectiveness of Data\n", "author": ["Posted by Fernando Pereira, Google Research"], "link": "http://ai.googleblog.com/2009/03/the-unreasonable-effectiveness-of-data.html", "abstract": "", "date": "\nWednesday, March 25, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle and WPP Marketing Research Awards: Improving industry understanding and practices in online marketing\n", "author": ["Posted by Jeff Walz, University Relations and Anne Bray, Head of Agency, WPP"], "link": "http://ai.googleblog.com/2009/03/google-and-wpp-marketing-research.html", "abstract": "", "date": "\nThursday, March 19, 2009\n"},
{"website": "Google-AI", "title": "\nAnd the award goes to...\n", "author": ["Posted by Fernando Pereira, Research Director"], "link": "http://ai.googleblog.com/2009/03/and-award-goes-to.html", "abstract": "", "date": "\nWednesday, March 18, 2009\n"},
{"website": "Google-AI", "title": "\nBeyond Web-2.0\n", "author": ["Posted by T.V Raman, Research Scientist"], "link": "http://ai.googleblog.com/2009/02/beyond-web-20.html", "abstract": "", "date": "\nWednesday, February 18, 2009\n"},
{"website": "Google-AI", "title": "\nMarket Algorithms and Optimization Meeting\n", "author": ["Posted by\u00a0"], "link": "http://ai.googleblog.com/2009/01/market-algorithms-and-optimization.html", "abstract": "", "date": "\nWednesday, January 28, 2009\n"},
{"website": "Google-AI", "title": "\nGoogle University Research Awards\n", "author": ["Posted by Juan Vargas, "], "link": "http://ai.googleblog.com/2009/01/google-university-research-awards.html", "abstract": "", "date": "\nWednesday, January 28, 2009\n"},
{"website": "Google-AI", "title": "\nSmart Thumbnails on YouTube\n", "author": ["Posted by\u00a0Tom\u00e1\u0161 I\u017eo (Software Engineer) and Jay Yagnik (Head of Computer Vision Research)"], "link": "http://ai.googleblog.com/2009/01/smart-thumbnails-on-youtube.html", "abstract": "", "date": "\nMonday, January 19, 2009\n"},
{"website": "Google-AI", "title": "\nMaybe your computer just needs a hug\n", "author": [], "link": "http://ai.googleblog.com/2009/01/maybe-your-computer-just-needs-hug.html", "abstract": "", "date": "\nMonday, January 12, 2009\n"},
{"website": "Google-AI", "title": "\nTranslation is Risky Business\n", "author": ["Posted by Shankar Kumar and Wolfgang Macherey"], "link": "http://ai.googleblog.com/2008/12/translation-is-risky-business.html", "abstract": "", "date": "\nTuesday, December 30, 2008\n"},
{"website": "Google-AI", "title": "\nplop: Probabilistic Learning of Programs\n", "author": ["Posted by Moshe Looks"], "link": "http://ai.googleblog.com/2008/11/plop-probabilistic-learning-of-programs.html", "abstract": "", "date": "\nMonday, November 10, 2008\n"},
{"website": "Google-AI", "title": "\nNew Technology Roundtable Series\n", "author": ["Posted by ", ", VP of Research and Special Initiatives"], "link": "http://ai.googleblog.com/2008/10/new-technology-roundtable-series.html", "abstract": "", "date": "\nFriday, October 3, 2008\n"},
{"website": "Google-AI", "title": "\nDoubling Up\n", "author": ["Posted by Franz Josef Och"], "link": "http://ai.googleblog.com/2008/09/doubling-up.html", "abstract": "", "date": "\nMonday, September 29, 2008\n"},
{"website": "Google-AI", "title": "\nRemembering Randy Pausch\n", "author": ["Posted by Kevin McCurley, Research Team"], "link": "http://ai.googleblog.com/2008/07/remembering-randy-pausch.html", "abstract": "", "date": "\nSaturday, July 26, 2008\n"},
{"website": "Google-AI", "title": "\nMachine Learning Meeting\n", "author": ["Posted by "], "link": "http://ai.googleblog.com/2008/05/machine-learning-meeting.html", "abstract": "", "date": "\nTuesday, May 20, 2008\n"},
{"website": "Google-AI", "title": "\nCan You Publish at Google?\n", "author": ["Posted by "], "link": "http://ai.googleblog.com/2008/05/can-you-publish-at-google.html", "abstract": "", "date": "\nTuesday, May 6, 2008\n"},
{"website": "Google-AI", "title": "\nVisualRank\n", "author": ["Posted by Shumeet Baluja and Yushi Jing"], "link": "http://ai.googleblog.com/2008/05/visualrank.html", "abstract": "", "date": "\nThursday, May 1, 2008\n"},
{"website": "Google-AI", "title": "\nResearch in the Cloud: Providing Cutting Edge Computational Resources to Scientists\n", "author": ["Posted by Christophe Bisciglia, Senior Software Engineer, and Alfred Spector, Vice President of Research"], "link": "http://ai.googleblog.com/2008/04/research-in-cloud-providing-cutting.html", "abstract": "", "date": "\nWednesday, April 23, 2008\n"},
{"website": "Google-AI", "title": "\nDeploying Goog411\n", "author": ["Posted by Francoise Beaufays"], "link": "http://ai.googleblog.com/2008/03/deploying-goog411.html", "abstract": "", "date": "\nFriday, March 28, 2008\n"},
{"website": "Google-AI", "title": "\nThis year's scalability conference\n", "author": ["Posted by Andrew Schwerin, Software Engineer"], "link": "http://ai.googleblog.com/2008/02/this-year-scalability-conference.html", "abstract": "", "date": "\nMonday, February 11, 2008\n"},
{"website": "Google-AI", "title": "\nGoogle Education Summit\n", "author": ["Posted by Jeff Walz and Kevin McCurley"], "link": "http://ai.googleblog.com/2007/10/google-education-summit.html", "abstract": "", "date": "\nThursday, October 18, 2007\n"},
{"website": "Google-AI", "title": "\nOpenHTMM Released\n", "author": ["Posted by Ashok C. Popat, Research Scientist"], "link": "http://ai.googleblog.com/2007/09/openhtmm-released.html", "abstract": "", "date": "\nSunday, September 23, 2007\n"},
{"website": "Google-AI", "title": "\nThe Sky is Open\n", "author": ["Posted by Jeremy Brewer"], "link": "http://ai.googleblog.com/2007/09/the-sky-is-open.html", "abstract": "", "date": "\nWednesday, September 19, 2007\n"},
{"website": "Google-AI", "title": "\nIntroducing Sky in Google Earth\n", "author": ["Posted by Andy Connolly and Ryan Scranton"], "link": "http://ai.googleblog.com/2007/08/introducing-sky-in-google-earth.html", "abstract": "", "date": "\nTuesday, August 21, 2007\n"},
{"website": "Google-AI", "title": "\nDrink from the firehose with University Research Programs\n", "author": ["Posted by Michael Lancaster and Josh Estelle, Software Engineers"], "link": "http://ai.googleblog.com/2007/07/drink-from-firehose-with-university.html", "abstract": "", "date": "\nThursday, July 26, 2007\n"},
{"website": "Google-AI", "title": "\nNew Conference on Web Search and Data Mining\n", "author": ["Posted by Ziv Bar-Yossef and Kevin McCurley, Research Team"], "link": "http://ai.googleblog.com/2007/06/new-conference-on-web-search-and-data.html", "abstract": "", "date": "\nMonday, June 18, 2007\n"},
{"website": "Google-AI", "title": "\nVideos of talks\n", "author": ["Posted by Kevin McCurley, Research Team"], "link": "http://ai.googleblog.com/2007/06/videos-of-talks.html", "abstract": "One of the best features of working at Google is the rich variety of talks that we can attend, both technical and general interest.  Most of these are videotaped for later viewing.  This has multiple benefits:", "date": "\nMonday, June 18, 2007\n"},
{"website": "Google-AI", "title": "\nSeattle conference on scalability\n", "author": ["Posted by Amanda Camp, Software Engineer"], "link": "http://ai.googleblog.com/2007/02/seattle-conference-on-scalability.html", "abstract": "", "date": "\nFriday, February 16, 2007\n"},
{"website": "Google-AI", "title": "\nHear, here.   A Sample of Audio Processing at Google.\n", "author": ["Posted by Shumeet Baluja, Michele Covell, Pedro Moreno & Eugene Weinstein"], "link": "http://ai.googleblog.com/2007/02/hear-here-sample-of-audio-processing-at.html", "abstract": "", "date": "\nWednesday, February 14, 2007\n"},
{"website": "Google-AI", "title": "\nGoogle Research Picks for Videos of the Year\n", "author": ["Posted by Peter Norvig"], "link": "http://ai.googleblog.com/2006/12/google-research-picks-for-videos-of-year.html", "abstract": "", "date": "\nMonday, December 11, 2006\n"},
{"website": "Google-AI", "title": "\nCSCW 2006: Collaborative editing 20 years later\n", "author": ["Posted by Lilly Irani & Jens Riegelsberger, User Experience team"], "link": "http://ai.googleblog.com/2006/11/cscw-2006-collaborative-editing-20.html", "abstract": "", "date": "\nTuesday, November 28, 2006\n"},
{"website": "Google-AI", "title": "\nAnd the Awards Go To ...\n", "author": ["Posted by Proud Googlers"], "link": "http://ai.googleblog.com/2006/09/and-awards-go-to.html", "abstract": "", "date": "\nFriday, September 22, 2006\n"},
{"website": "Google-AI", "title": "\nAll Our N-gram are Belong to You\n", "author": ["Posted by Alex Franz and Thorsten Brants, Google Machine Translation Team"], "link": "http://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html", "abstract": "", "date": "\nThursday, August 3, 2006\n"},
{"website": "Google-AI", "title": "\nCall for attendees - Conference on Test Automation\n", "author": ["Posted by Allen Hutchison, Engineering Manager"], "link": "http://ai.googleblog.com/2006/07/call-for-attendees-conference-on-test.html", "abstract": "", "date": "\nWednesday, July 12, 2006\n"},
{"website": "Google-AI", "title": "\nInteractive TV: Conference and Best Paper\n", "author": ["Posted by Michele Covell & Shumeet Baluja, Research Scientists"], "link": "http://ai.googleblog.com/2006/06/interactive-tv-conference-and-best-paper.html", "abstract": "", "date": "\nTuesday, June 6, 2006\n"},
{"website": "Google-AI", "title": "\nExtra, Extra - Read All About It: Nearly All Binary Searches and Mergesorts are Broken\n", "author": ["Posted by Joshua Bloch, Software Engineer"], "link": "http://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html", "abstract": "", "date": "\nFriday, June 2, 2006\n"},
{"website": "Google-AI", "title": "\nStatistical machine translation live\n", "author": ["Posted by Franz Och, Research Scientist"], "link": "http://ai.googleblog.com/2006/04/statistical-machine-translation-live.html", "abstract": "", "date": "\nFriday, April 28, 2006\n"},
{"website": "Google-AI", "title": "\nOur conference on automated testing\n", "author": ["Posted by Allen Hutchison, Engineering Manager"], "link": "http://ai.googleblog.com/2006/04/our-conference-on-automated-testing.html", "abstract": "", "date": "\nThursday, April 27, 2006\n"},
{"website": "Google-AI", "title": "\nSee you at CHI\n", "author": ["Posted by Rick Boardman, User Experience Researcher"], "link": "http://ai.googleblog.com/2006/04/see-you-at-chi.html", "abstract": "", "date": "\nSunday, April 23, 2006\n"},
{"website": "Google-AI", "title": "\nFirst Robots\n", "author": ["Posted by  Sumit Agarwal, Maryam Kamvar, & Michael Stoppelman "], "link": "http://ai.googleblog.com/2006/03/first-robots.html", "abstract": "", "date": "\nWednesday, March 22, 2006\n"},
{"website": "Google-AI", "title": "\nHiring: The Lake Wobegon Strategy\n", "author": ["Posted by Peter Norvig, Director, Google Research"], "link": "http://ai.googleblog.com/2006/03/hiring-lake-wobegon-strategy.html", "abstract": "", "date": "\nSaturday, March 11, 2006\n"},
{"website": "Google-AI", "title": "\nAn experimental study of P2P VoIP\n", "author": ["Posted by Neil Daswani & Ravi Jain, Google; and Saikat Guha, Cornell University"], "link": "http://ai.googleblog.com/2006/03/an-experimental-study-of-p2p-voip.html", "abstract": "", "date": "\nTuesday, March 7, 2006\n"},
{"website": "Google-AI", "title": "\nTeamwork for problem-solving\n", "author": ["Posted by Corinna Cortes, Head, Google Research NY "], "link": "http://ai.googleblog.com/2006/03/teamwork-for-problem-solving.html", "abstract": "", "date": "\nSaturday, March 4, 2006\n"},
{"website": "Google-AI", "title": "\nMaking a difference\n", "author": ["Posted by Peter Norvig, Director, Google Research"], "link": "http://ai.googleblog.com/2006/02/making-difference.html", "abstract": "", "date": "\nFriday, February 17, 2006\n"}
][
{"website": "Google-Security", "title": "\nHow we fought bad apps and developers in 2020\n", "author": ["Posted by Krish Vitaldevara, Director of Product Management Trust & Safety, Google Play"], "link": "https://security.googleblog.com/2021/04/how-we-fought-bad-apps-and-developers.html", "abstract": "Providing safe experiences to billions of users and millions of Android developers has been one of the highest priorities for Google Play for many years. Last year we introduced new policies, improved our systems, and further optimized our processes to better protect our users, assist good developers and strengthen our guard against bad apps and developers. Additionally, in 2020, Google Play Protect scanned over 100B installed apps each day for malware across billions of devices.\n\nUsers come to Google Play to find helpful, reliable apps on everything from COVID-19 vaccine information to new forms of entertainment, grocery delivery, communication and more.\n\nAs such, we introduced a series of policies and new developer support to continue to elevate  information quality on the platform and reduce the risk of user harm from misinformation.\n\nOur core efforts around identifying and mitigating bad apps and developers continued to evolve to address new adversarial behaviors and forms of abuse. Our machine-learning detection capabilities and enhanced app review processes prevented over 962k policy-violating app submissions from getting published to Google Play. We also banned 119k malicious and spammy developer accounts. Additionally, we significantly increased our focus on SDK enforcement, as we've found these violations have an outsized impact on security and user data privacy.\n\nLast year, we continued to reduce developer access to sensitive permissions. In February, we  a new background location policy to ensure that apps requesting this permission need the data in order to provide clear user benefit. As a result of the new policy, developers now have to demonstrate that benefit and prominently tell users about it or face possible removal from Google Play. We've begun enforcement on apps not meeting new policy guidelines and will provide an update on the usage of this permission in a future blog post. \n\nWe've also continued to invest in protecting kids and helping parents find great content. In 2020 we  a new kids tab filled with \u201cTeacher approved\u201d apps. To evaluate apps, we teamed with academic experts and teachers across the country, including our lead advisors, Joe Blatt (Harvard Graduate School of Education) and Dr. Sandra Calvert (Georgetown University).\n\nAs we continue to invest in protecting people from apps with harmful content, malicious behaviors, or threats to user privacy, we are also equally motivated to . For example, we\u2019ve improved our process for providing relevant information about enforcement actions we\u2019ve taken, resulting in significant reduction in appeals and increased developer satisfaction. We will continue to enhance the speed and quality of our communications to developers, and continue listening to feedback about how we can further engage and elevate trusted developers. Android developers can expect to see more on this front in the coming year. \n\nOur global teams of product managers, engineers, policy experts, and operations leaders are more excited than ever to advance the safety of the platform and forge a sustaining trust with our users. We look forward to building an even better Google Play experience.", "date": "\nApril 21, 2021\n"},
{"website": "Google-Security", "title": "\n A New Standard for Mobile App Security \n", "author": ["Posted by Brooke Davis and Eugene Liderman, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2021/04/a-new-standard-for-mobile-app-security.html", "abstract": "With all of the challenges from this past year, users have become increasingly dependent on their mobile devices to create fitness routines, stay connected with loved ones, work remotely, and order things like groceries with ease. According to , in 2020 users spent over three and a half hours per day using mobile apps. With so much time spent on mobile devices, ensuring the safety of mobile apps is more important than ever. Despite the importance of digital security, there isn\u2019t a consistent industry standard for assessing mobile apps. Existing guidelines tend to be either too lightweight or too onerous for the average developer, and lack a compliance arm. That\u2019s why we're excited to share  of a new  which provides a set of security and privacy requirements with defined acceptance criteria which developers can certify their apps against. \n\nOver 20 industry stakeholders, including , , and a number of certified labs such as  and , as well as automated mobile app security testing vendors like  collaborated to develop this new security standard for mobile apps. We\u2019ve seen early interest from Internet of Things (IoT) and virtual private network (VPN) developers, however the standard is appropriate for any cloud connected service such as social, messaging, fitness, or productivity apps.\n\nThe  manages a security compliance assessment program for connected devices. ioXt has over 300 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, and webcams, and since most smart devices are managed through apps, they have expanded coverage to include mobile apps with the launch of this profile.\n\nThe ioXt  provides a minimum set of commercial best practices for all cloud connected apps running on mobile devices.  This security baseline helps mitigate against common threats and reduces the probability of significant vulnerabilities.  The profile leverages existing standards and principles set forth by  and the , and allows developers to differentiate security capabilities around cryptography, authentication, network security, and vulnerability disclosure program quality. The profile also provides a framework to evaluate app category specific requirements which may be applied based on the features contained in the app. For example, an IoT app only needs to certify under the Mobile Application profile, whereas a VPN app must comply with the Mobile Application profile, plus the VPN extension.\n\nCertification allows developers to demonstrate product safety and we\u2019re excited about the opportunity for this standard to push the industry forward. We observed that app developers were very quick to resolve any issues that were identified during their blackbox evaluations against this new standard, oftentimes with turnarounds in a matter of days. At launch, the following apps have been certified: , , , , , , , , , as well as the  app, including .\n\nWe look forward to seeing adoption of the standard grow over time and for those app developers that are already investing in security best practices to be able to highlight their efforts. The standard also serves as a guiding light to inspire more developers to invest in mobile app security. If you are interested in learning more about the ioXt Alliance and how to get your app certified, visit  and check out Android\u2019s guidelines for building secure apps .", "date": "\nApril 15, 2021\n"},
{"website": "Google-Security", "title": "\n Rust in the Linux kernel\n", "author": ["Posted by Wedson Almeida Filho, Android Team"], "link": "https://security.googleblog.com/2021/04/rust-in-linux-kernel.html", "abstract": "In our , we announced that Android now supports the  programming language for developing the OS itself. Related to this, we are also participating in the effort to evaluate the use of Rust as a supported language for developing the Linux kernel. In this post, we discuss some technical aspects of this work using a few simple examples.\n\nC has been the language of choice for writing kernels for almost half a century because it offers the level of control and predictable performance required by such a critical component. Density of memory safety bugs in the Linux kernel is generally quite low due to high code quality, high standards of code review, and carefully implemented safeguards. However, . On Android, vulnerabilities in the kernel are generally considered high-severity because they can result in a security model bypass due to the privileged mode that the kernel runs in.\n\nWe feel that Rust is now ready to join C as a practical language for implementing the kernel. It can help us reduce the number of potential bugs and security vulnerabilities in privileged code while playing nicely with the core kernel and preserving its performance characteristics.\n\nWe developed an  of the Binder driver to allow us to make meaningful comparisons between the safety and performance characteristics of the existing C version and its Rust counterpart. The Linux kernel has over 30 million lines of code, so naturally our goal is not to convert it all to Rust but rather to allow new code to be written in Rust. We believe this incremental approach allows us to benefit from the kernel\u2019s existing high-performance implementation while providing kernel developers with new tools to improve memory safety and maintain performance going forward.\nWe joined the  organization, where the community had already done and continues to do great work toward adding Rust support to the Linux kernel build system. We also need designs that allow code in the two languages to interact with each other: we're particularly interested in safe, zero-cost abstractions that allow Rust code to use kernel functionality written in C, and how to implement functionality in idiomatic Rust that can be called seamlessly from the C portions of the kernel.\nSince Rust is a new language for the kernel, we also have the opportunity to enforce best practices in terms of documentation and uniformity. For example, we have specific machine-checked requirements around the usage of unsafe code: for every unsafe function, the developer must document the requirements that need to be satisfied by callers to ensure that its usage is safe; additionally, for every call to unsafe functions (or usage of unsafe constructs like dereferencing a raw pointer), the developer must document the justification for why it is safe to do so.\n\nJust as important as safety, Rust support needs to be convenient and helpful for developers to use. Let\u2019s get into a few examples of how Rust can assist kernel developers in writing drivers that are safe and correct.\n\nWe'll use an implementation of a  character device. Each device has a current value; writes of  bytes result in the device value being incremented by ; reads decrement the value by 1 unless the value is 0, in which case they will block until they can decrement the count without going below 0.\n\n  Suppose  is a file representing our device. We can interact with it from the shell as follows:\n > cat semaphore \n  When  is a newly initialized device, the command above will block because the device's current value is 0. It will be unblocked if we run the following command from another shell because it increments the value by 1, which allows the original read to complete:\n\n> echo -n a > semaphore\nWe could also increment the count by more than 1 if we write more data, for example:\n\n> echo -n abc > semaphore\n\nincrements the count by 3, so the next 3 reads won't block.\n\nTo allow us to show a few more aspects of Rust, we'll add the following features to our driver: remember what the maximum value was throughout the lifetime of a device, and remember how many reads each file issued on the device.\n\nWe'll now show how such a driver would be , contrasting it with a . We note, however, we are still early on so this is all subject to change in the future. How Rust can assist the developer is the aspect that we'd like to emphasize. For example, at compile time it allows us to eliminate or greatly reduce the chances of introducing classes of bugs, while at the same time remaining flexible and having minimal overhead.\n\nA developer needs to do the following to implement a driver for a new character device in Rust:\n\nThe following outlines how the first two steps of our example compare in Rust and C:\n\nCharacter devices in Rust benefit from a number of safety features:\n\nFor a driver to provide a custom  handler, it needs to implement the\u00a0 function that is part of the  trait, as exemplified in the table below.\n\n \n\nIoctl commands are standardized such that, given a command, we know whether a user buffer is provided, its intended use (read, write, both, none), and its size. In Rust, we provide a  (accessible by calling ) that uses this information to automatically create user memory access helpers and pass them to the caller.\n\nA driver is not  to use this though. If, for example, it doesn't use the standard ioctl encoding, Rust offers the flexibility of simply calling  to extract the raw arguments and using them to handle the ioctl (potentially with unsafe code, which will need to be justified).\n\nHowever, if a driver implementation does use the standard dispatcher, it will benefit from not having to implement any unsafe code, and:\n\nAll of the above could potentially also be done in C, but it's very easy for developers to (likely unintentionally) break contracts that lead to unsafety; Rust requires  blocks for this, which should only be used in rare cases and brings additional scrutiny. Additionally, Rust offers the following:\nWe allow developers to use mutexes and spinlocks to provide interior mutability. In our example, we use a mutex to protect mutable data; in the tables below we show the data structures we use in C and Rust, and how we implement a wait until the count is nonzero so that we can satisfy a read:\n  \nWe note that such waits are not uncommon in the existing C code, for example, a  for a \"partner\" to write, a  for data, an  for completion of a delete, or a  for state change.\nThe following are benefits from the Rust implementation:\n\nIn the tables below, we show how , , and  are implemented in our example driver:\n\nThey illustrate other benefits brought by Rust:\n\nThe examples above are only a small part of the whole project. We hope it gives readers a glimpse of the kinds of benefits that Rust brings. At the moment we have nearly all generic kernel functionality needed by Binder neatly wrapped in safe Rust abstractions, so we are in the process of  with the intent of upstreaming the existing Rust support.\n\nWe also continue to make progress on our Binder prototype, implement additional abstractions, and smooth out some rough edges. This is an exciting time and a rare opportunity to potentially influence how the Linux kernel is developed, as well as inform the evolution of the Rust language. We invite those interested to join us in  and attend our planned talk at !", "date": "\nApril 14, 2021\n"},
{"website": "Google-Security", "title": "\n Rust in the Android platform \n", "author": ["Posted by Jeff Vander Stoep and Stephen Hines, Android Team "], "link": "https://security.googleblog.com/2021/04/rust-in-android-platform.html", "abstract": "Correctness of code in the Android platform is a top priority for the security, stability, and quality of each Android release. Memory safety bugs in C and C++ continue to be the most-difficult-to-address source of incorrectness. We invest a great deal of effort and resources into detecting, fixing, and mitigating this class of bugs, and these efforts are effective in preventing a large number of bugs from making it into Android releases. Yet in spite of these efforts, memory safety bugs continue to be a top contributor of stability issues, and consistently represent ~ of Android\u2019s high severity security vulnerabilities.\n\nIn addition to  and  efforts to improve detection of memory bugs, we are ramping up efforts to prevent them in the first place. Memory-safe languages are the most cost-effective means for preventing memory bugs. In addition to memory-safe languages like Kotlin and Java, we\u2019re excited to announce that the Android Open Source Project (AOSP) now supports the Rust programming language for developing the OS itself.\n\nManaged languages like Java and Kotlin are the best option for Android app development. These languages are designed for ease of use, portability, and safety. The  manages memory on behalf of the developer. The Android OS uses Java extensively, effectively protecting large portions of the Android platform from memory bugs. Unfortunately, for the lower layers of the OS, Java and Kotlin are not an option.\n\nLower levels of the OS require systems programming languages like C, C++, and Rust. These languages are designed with control and predictability as goals. They provide access to low level system resources and hardware. They are light on resources and have more predictable performance characteristics.For C and C++, the developer is responsible for managing memory lifetime. Unfortunately,  when doing this, especially in complex and multithreaded codebases.\n\nRust provides memory safety guarantees by using a combination of compile-time checks to enforce object lifetime/ownership and runtime checks to ensure that memory accesses are valid. This safety is achieved while providing equivalent performance to C and C++.\n\nC and C++ languages don\u2019t provide these same safety guarantees and require robust isolation. All Android processes are sandboxed and we follow the  to decide if functionality necessitates additional isolation and deprivileging. The Rule of 2 is simple: given three options, developers may only select two of the following three options.\n\nFor Android, this means that if code is written in C/C++ and parses untrustworthy input, it should be contained within a tightly constrained and unprivileged sandbox. While  has been effective in reducing the severity and reachability of security vulnerabilities, it does come with limitations. Sandboxing is expensive: the new processes it requires  due to IPC and additional memory usage. Sandboxing doesn\u2019t eliminate vulnerabilities from the code and its efficacy is reduced by , allowing attackers to chain multiple vulnerabilities together.\n\nMemory-safe languages like Rust help us overcome these limitations in two ways:\n\nOf course, introducing a new programming language does nothing to address bugs in our existing C/C++ code. Even if we redirected the efforts of every software engineer on the Android team, rewriting tens of millions of lines of code is simply not feasible. \n\nThe above analysis of the age of memory safety bugs in Android (measured from when they were first introduced) demonstrates why our memory-safe language efforts are best focused on new development and not on rewriting mature C/C++ code. Most of our memory bugs occur in new or recently modified code, with about 50% being less than a year old. \n\nThe comparative rarity of older memory bugs may come as a surprise to some, but we\u2019ve found that old code is not where we most urgently need improvement. Software bugs are found and fixed over time, so we would expect the number of bugs in code that is being maintained but not actively developed to go down over time. Just as reducing the number and density of bugs improves the effectiveness of sandboxing, it also improves the effectiveness of bug detection.\n\nBug detection via robust testing, , and  is crucial for improving the quality and correctness of all software, including software written in Rust. A key limitation for the most effective memory safety detection techniques is that the erroneous state must actually be triggered in instrumented code in order to be detected. Even in code bases with excellent test/fuzz coverage, this results in a lot of bugs going undetected.\n\nAnother limitation is that . In some projects, . Bug fixing is a long and costly process.\n\n\n Each of these steps is costly, and missing any one of them can result in the bug going unpatched for some or all users. For complex C/C++ code bases, often there are only a handful of people capable of developing and reviewing the fix, and even with a high amount of effort spent on fixing bugs, .\n\nBug detection is most effective when bugs are relatively rare and dangerous bugs can be given the urgency and priority that they merit. Our ability to reap the benefits of improvements in bug detection require that we prioritize preventing the introduction of new bugs. \n\nRust modernizes a range of other language aspects, which results in improved correctness of code:\n\nAdding a new language to the Android platform is a large undertaking. There are toolchains and dependencies that need to be maintained, test infrastructure and tooling that must be updated, and developers that need to be trained. For the past 18 months we have been adding Rust support to the Android Open Source Project, and we have a few early adopter projects that we will be sharing in the coming months. Scaling this to more of the OS is a multi-year project. Stay tuned, we will be posting more updates on this blog.", "date": "\nApril 6, 2021\n"},
{"website": "Google-Security", "title": "\nAnnouncing the Android Ready SE Alliance\n", "author": ["Posted by Sudhi Herle and Jason Wong, Android Team"], "link": "https://security.googleblog.com/2021/03/announcing-android-ready-se-alliance.html", "abstract": "When the Pixel 3 launched in 2018, it had a  called . In addition to being a root-of-trust for Pixel software and firmware, it also enabled tamper-resistant key storage for Android Apps using . StrongBox is an implementation of the Keymaster HAL that resides in a hardware security module. It is an important security enhancement for Android devices and paved the way for us to consider features that were previously not possible.\n\nStrongBox and tamper-resistant hardware are becoming important requirements for emerging user features, including:\n\nAll these features need to run on tamper-resistant hardware to protect the integrity of the application executables and a user\u2019s data, keys, wallet, and more. Most modern phones now include discrete tamper-resistant hardware called a Secure Element (SE). We believe this SE offers the best path for introducing these new consumer use cases in Android.\n\nIn order to accelerate adoption of these new Android use cases, we are announcing the formation of the . SE vendors are joining hands with Google to create a set of open-source, validated, and ready-to-use SE Applets. Today, we are launching the General Availability (GA) version of StrongBox for SE. This applet is qualified and ready for use by our OEM partners. It is currently available from , , , , and .\n\nIt is important to note that these features are not just for phones and tablets. StrongBox is also applicable to WearOS, Android Auto Embedded, and Android TV. \n\nUsing Android Ready SE in a device requires the OEM to:\n\nWe are working with our ecosystem to prioritize and deliver the following Applets in conjunction with corresponding Android feature releases:\n\nWe already have several Android OEMs adopting  for their devices. We look forward to working with our OEM partners to bring these next generation features for our users.\n\nPlease visit our Android Security and Privacy developer  for more info.", "date": "\nMarch 25, 2021\n"},
{"website": "Google-Security", "title": "\nAnnouncing the winners of the 2020 GCP VRP Prize \n", "author": ["Posted by Harshvardhan Sharma, Information Security Engineer, Google\u00a0"], "link": "https://security.googleblog.com/2021/03/announcing-winners-of-2020-gcp-vrp-prize.html", "abstract": "", "date": "\nMarch 17, 2021\n"},
{"website": "Google-Security", "title": "\n Google, HTTPS, and device compatibility\n", "author": [], "link": "https://security.googleblog.com/2021/03/google-https-and-device-compatibility.html", "abstract": "", "date": "\nMarch 15, 2021\n"},
{"website": "Google-Security", "title": "\n A Spectre proof-of-concept for a Spectre-proof web \n", "author": [], "link": "https://security.googleblog.com/2021/03/a-spectre-proof-of-concept-for-spectre.html", "abstract": "", "date": "\nMarch 12, 2021\n"},
{"website": "Google-Security", "title": "\n Continuing to Raise the Bar for Verifiable Security on Pixel\n", "author": ["Posted by Eugene Liderman, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2021/03/continuing-to-raise-bar-for-verifiable.html", "abstract": "Evaluating the security of mobile devices is difficult, and a trusted way to validate a company\u2019s claims is through independent, industry certifications. When it comes to smartphones one of the most rigorous end-to-end certifications is the .  is the driving force for establishing widespread mutual recognition of secure IT products across 31 countries . Over the past few years only three smartphone manufacturers have continually been certified on every OS version: Google, Samsung, and Apple. At the beginning of February, we successfully completed this certification for all currently supported . Google is the first manufacturer to be certified on the latest OS version. \nThis specific certification is designed to evaluate how a device defends against the real-world threats facing both consumers and businesses. The table below outlines the threats and mitigations provided in the CC MDF protection profile: An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructure An attacker is positioned on a wireless communications channel or elsewhere on the network infrastructure  Standard protocols such as IPsec, DTLS, TLS, HTTPS, and Bluetooth to ensure encrypted communications are secure Secure authentication for networks and backends - Capabilities for configuring and applying security policies defined by the user and/or Enterprise Administrator An attacker, with physical access, may attempt to retrieve user data on the mobile device, including credentials  Application isolation/sandboxing and framework permissions provide separation and privacy between user activities Application isolation/sandboxing and framework permissions provide separation and privacy between user activities\nWhat makes this certification important is the fact that it is a hands on evaluation done by an  authorized lab to evaluate the device and perform a variety of tests to ensure that:\n\nAt a high level, the target of evaluation (TOE) is the combination of device hardware (i.e. system on chip) and operating system (i.e. Android). In order to validate our mitigations for the threats listed above, the lab looks at the following security functionality:\n\nIt\u2019s incredibly important to ensure Pixel security can specifically support enterprise needs. Many regulated industries require the use of Common Criteria certified devices to ensure that sensitive data is backed by the strongest possible protections. The Android Enterprise management framework enables enterprises to do things like control devices by setting restrictions around what the end user can do and audit devices to ensure all software settings are configured properly. For example, enterprise IT admins wish to enforce policies for features like the camera, location services or app installation process. \n\nSecurity isn\u2019t just an enterprise concern and many of the protections validated by Common Criteria certification apply to consumers as well. For example, when you\u2019re connecting to Wi-Fi, you want to ensure no one can spy on your web browsing. If your device is lost or stolen, you want to be confident that your lock screen can reduce the chances of someone accessing your personal information. \n\nWe believe in making security & privacy accessible to all of our users. This is why we  take care to ensure that Pixel devices meet or exceed these certification standards.. We\u2019re committed to meeting these standards moving forward, so you can rest assured that your Pixel phone comes with top-of-the-line security built in, from the moment you turn it on.\n\nWhile certifications are a great form of third party validation, they often fall under what we like to call the 3 C\u2019s:\n\nWe have been working these last three years to reduce this complexity for our OEM partners. We are excited to tell you that the features required to satisfy the necessary security requirements are baked directly into the Android Open Source Project. We\u2019ve also added all of the management and auditability requirements into the Android Enterprise Management framework. Last year we started publishing the tools we have developed for this on  to allow other Android OEMs to take advantage of our efforts as they go through their certification.\n\nWhile we continue certifying Pixel smartphones with new Android OS versions, we have worked to enable other Android OEMs to achieve this certification as well as others, such as:\n\nWe\u2019ll continue to invest in additional ways to measure security for both enterprises and consumers, and we welcome the industry to join us in this effort.", "date": "\nMarch 11, 2021\n"},
{"website": "Google-Security", "title": "\n#ShareTheMicInCyber: Brooke Pearson \n", "author": ["Posted by Parisa Tabriz, Head of Chrome Product, Engineering and UX\u00a0"], "link": "https://security.googleblog.com/2021/03/sharethemicincyber-brooke-pearson.html", "abstract": "", "date": "\nMarch 11, 2021\n"},
{"website": "Google-Security", "title": "\nFuzzing Java in OSS-Fuzz\n", "author": [], "link": "https://security.googleblog.com/2021/03/fuzzing-java-in-oss-fuzz.html", "abstract": "", "date": "\nMarch 10, 2021\n"},
{"website": "Google-Security", "title": "\nIntroducing sigstore: Easy Code Signing & Verification for Supply Chain Integrity\n", "author": ["Posted by\u00a0Kim Lewandowski & Dan Lorenc, Google Open Source Security Team"], "link": "https://security.googleblog.com/2021/03/introducing-sigstore-easy-code-signing.html", "abstract": "", "date": "\nMarch 9, 2021\n"},
{"website": "Google-Security", "title": "\n#ShareTheMicInCyber: Rob Duhart \n", "author": [], "link": "https://security.googleblog.com/2021/03/sharethemicincyber-rob-duhart.html", "abstract": "", "date": "\nMarch 1, 2021\n"},
{"website": "Google-Security", "title": "\nCelebrating the influence and contributions of Black+ Security & Privacy Googlers\n", "author": [], "link": "https://security.googleblog.com/2021/02/celebrating-influence-and-contributions.html", "abstract": "", "date": "\nFebruary 25, 2021\n"},
{"website": "Google-Security", "title": "\nNew Password Checkup Feature Coming to Android\n", "author": ["Posted by Arvind Kumar Sugumar, Software Engineer, Android Team"], "link": "https://security.googleblog.com/2021/02/new-password-checkup-feature-coming-to.html", "abstract": "To make this easier, Chrome introduced the  in 2019, which notifies you when one of the passwords you\u2019ve saved in Chrome is exposed. We\u2019re now bringing this functionality to your Android apps through Autofill with Google. Whenever you fill or save credentials into an app, we\u2019ll check those credentials against a list of known compromised credentials and alert you if your password has been compromised. The prompt can also take you to your , where you can do a comprehensive review of your saved passwords. Password Checkup on Android apps is available on Android 9 and above, for users of Autofill with Google. \n\nFollow the instructions below to enable Autofill with Google on your Android device:\n\nIf you can\u2019t find these options, check out  with details on how to get information from your device manufacturer.\n\n\n\nUser privacy is top of mind, especially when it comes to features that handle sensitive data such as passwords. Autofill with Google is built on the Android autofill framework which enforces strict privacy & security invariants that ensure that we have access to the user\u2019s credentials only in the following two cases: 1) the user has already saved said credential to their Google account; 2) the user was offered to save a new credential by the Android OS and chose to save it to their account. \n\nWhen the user interacts with a credential by either filling it into a form or saving it for the first time, we use the same privacy preserving API that powers the feature in Chrome to check if the credential is part of the list of known compromised passwords tracked by Google. \n\nThis implementation ensures that:\n\nFor more information on how this API is built under the hood, check out  from the Chrome team.\n\n\n\nIn addition to Password Checkup, Autofill with Google offers other features to help you keep your data secure:\n\nAs always, stay tuned to the Google Security blog to keep up to date on the latest ways we\u2019re improving security across our products.", "date": "\nFebruary 23, 2021\n"},
{"website": "Google-Security", "title": "\nMitigating Memory Safety Issues in Open Source Software\n", "author": ["Posted by Dan Lorenc, Infrastructure Security Team"], "link": "https://security.googleblog.com/2021/02/mitigating-memory-safety-issues-in-open.html", "abstract": "", "date": "\nFebruary 17, 2021\n"},
{"website": "Google-Security", "title": "\nLaunching OSV - Better vulnerability triage for open source\n", "author": ["Posted by Oliver Chang and Kim Lewandowski, Google Security Team"], "link": "https://security.googleblog.com/2021/02/launching-osv-better-vulnerability.html", "abstract": "", "date": "\nFebruary 5, 2021\n"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2020 Year in Review \n", "author": ["Posted by Anna Hupa, Senior Strategist, Vulnerability Rewards Team"], "link": "https://security.googleblog.com/2021/02/vulnerability-reward-program-2020-year.html", "abstract": "Despite the challenges of this unprecedented year, our vulnerability researchers have achieved more than ever before, partnering with our Vulnerability Reward Programs (VRPs) to protect Google\u2019s users by discovering security and abuse bugs and reporting them to us for remediation. Their diligence helps us keep our users, and the internet at large, safe, and enables us to fix security issues before they can be exploited.\n\nThe incredibly hard work, dedication, and expertise of our researchers in 2020 resulted in a record-breaking payout of over  million in rewards, with an additional $280,000 given to charity. We\u2019d like to extend a big  to our community of researchers for collaborating with us. It\u2019s your excellent work that brings our programs to life, so we wanted to take a moment to look back on last year\u2019s successes.\n\nOur rewards programs span several Google product areas, including Chrome, Android, and the Google Play Store. As in , we are sharing our 2020 Year in Review statistics across all of these programs. \n\n\n2020 was a fantastic year for the Android VRP, and in response to the valiant efforts of multiple teams of researchers, we paid out  in rewards. Following our increase in exploit payouts in , we received a record 13 working exploit submissions in 2020, representing  in exploit reward payouts. Some highlights include:\n\nIn addition, we launched a number of pilot rewards programs to guide security researchers toward additional areas of interest, including Android Auto OS, writing fuzzers for Android code, and a reward program for Android chipsets. And in 2021, we'll be working on additional improvements and exciting initiatives related to our programs.\n\n\nChrome has also seen a record year of VRP payouts! We increased our reward amounts in July 2019, and as a result, 2020 has seen us pay out 83% more than 2019, totalling  across 300 bugs.\n\nIn 2019, 14% of our payouts were for V8 bugs. This decreased to just 6% in 2020. At the end of 2020, we , so we expect to see this amount increase again in 2021.\n\n\nIt\u2019s been another stellar year for the Google Play Security Rewards Program! This year, we expanded the criteria for qualifying Android apps to include apps utilizing the  and performing contact tracing to help combat Covid-19. We also increased our maximum bounty award amount to $20,000 for qualifying vulnerabilities. \n\nIn 2020, the Google Play Security Rewards Program and Developer Data Protection Reward Program awarded over  to Android researchers around the world. \n\n\n\nBeyond typical security vulnerabilities, we remain interested in research focused on abuse-related risks.\n\nThe Abuse program released an official definition describing what an  is and how abuse-related reports are assessed. We also  increased rewards for reports focused on abuse-related methodologies. These efforts led to a huge spike of abuse-related reports. In fact, we received more than twice as many reports in 2020 as in 2019, a level of growth we\u2019ve never seen before. The fantastic work of our researchers in 2020 allowed us to identify and fix over 100 issues across more than 60 different products.\n\n\n\nBesides reward payouts, in 2020 we also awarded over  in grants to more than 180 security researchers around the world, which is a record for this . More than a third of these grants were awarded in , to extend our support to researchers and enable them to continue with their work. Our researchers got back to us with over 200 reports which resulted in more than 100 identified vulnerabilities. \n\n\n\n\n\nFinally, because of the ongoing Covid-19 pandemic and related restrictions on travel last year, we couldn\u2019t keep our tradition of meeting our bug hunters in person and organizing events like , where we can engage with our incredible community of researchers. Like everyone else, we are full of hope that 2021 will allow us to meet in person again, and celebrate the 10 year VRP anniversary and the fantastic work our researchers have contributed during this time. \n\nWe look forward to another year of working with our security researchers to make Google, Android, Chrome and the Google Play Store safer for everyone. Follow us on  to keep tabs on the latest.", "date": "\nFebruary 4, 2021\n"},
{"website": "Google-Security", "title": "\nKnow, Prevent, Fix: A framework for shifting the discussion around vulnerabilities in open source\n", "author": ["Posted by Eric Brewer, Rob Pike, Abhishek Arya, Anne Bertucio and Kim Lewandowski\u00a0"], "link": "https://security.googleblog.com/2021/02/know-prevent-fix-framework-for-shifting.html", "abstract": "The security of open source software has rightfully garnered the industry\u2019s attention, but solutions require consensus about the challenges and cooperation in the execution. The problem is complex and there are many facets to cover: supply chain, dependency management, identity, and build pipelines. Solutions come faster when the problem is well-framed; we propose a framework (\u201cKnow, Prevent, Fix\u201d) for how the industry can think about vulnerabilities in open source and concrete areas to address first, including:\n\nThe following framework and goals are proposed with the intention of sparking industry-wide discussion and progress on the security of open source software.\n\n\n\n \n\nDue to , the software world gained a deeper understanding about the real risk of supply-chain attacks. Open source software  be less risky on the security front, as all of the code and dependencies are in the open and available for inspection and verification. And while that is generally true, it assumes people are actually looking. With so many dependencies, it is impractical to monitor them all, and many open source packages are not well maintained. \n\nIt is common for a program to depend, directly or indirectly, on thousands of packages and libraries. For example, Kubernetes now depends on about 1,000 packages. Open source likely makes  use of  than closed source, and from a wider range of suppliers; the number of distinct entities that need to be trusted can be . This makes it extremely difficult to understand how open source is used in products and what vulnerabilities might be relevant. There is also no assurance that what is built matches the source code. \n\nTaking a step back, although supply-chain attacks are a risk, the vast majority of vulnerabilities are mundane and unintentional\u2014honest errors made by well-intentioned developers. Furthermore, bad actors are more likely to exploit known vulnerabilities than to find their own: it\u2019s just easier. As such, we must focus on making fundamental changes to address the majority of vulnerabilities, as doing so will move the entire industry far along in addressing the complex cases as well, including supply-chain attacks.\n\nFew organizations can verify all of the packages they use, let alone all of the updates to those packages. In the current landscape, tracking these packages takes a non-trivial amount of infrastructure, and significant manual effort. At Google, we have those resources and go to extraordinary lengths to manage the open source packages we use\u2014including keeping a private repo of all open source packages we use internally\u2014and it is challenging to track all of the updates. The sheer flow of updates is daunting. A core part of any solution will be more automation, and this will be a key theme for our open source security work in 2021 and beyond. \n\nBecause this is a complex problem that needs industry cooperation, our purpose here is to focus the conversation around concrete goals. Google co-founded the  to be a focal point for this collaboration, but to make progress, we need participation across the industry, and agreement on what the problems are and how we might address them. To get the discussion started, we present one way to frame this problem, and a set of concrete goals that we hope will accelerate industry-wide solutions.\n\nWe suggest framing the challenge as three largely independent problem areas, each with concrete objectives:\n\nA related but separate problem, which is critical to securing the supply chain, is improving the security of the development process. We\u2019ve outlined the challenges of this problem and proposed goals in the fourth section, Prevention for Critical Software.\n\nKnowing your vulnerabilities is harder than expected for many reasons. Although there are mechanisms for reporting vulnerabilities, it is hard to know if they actually affect the specific versions of software you are using. \n\n\n\nFirst, it is crucial to capture precise vulnerability metadata from all available data sources. For example, knowing which version introduced a vulnerability helps determine if one's software is affected, and knowing when it was fixed results in accurate and timely patching (and a reduced window for potential exploitation). Ideally, this triaging workflow should be automated.\n\nSecond, most vulnerabilities are in your dependencies, rather than the code you write or control directly. Thus, even when your code is not changing, there can be a constant churn in your vulnerabilities: some get fixed and others get added.\n\n\n\nInfrastructure and industry standards are needed to track and maintain open source vulnerabilities, understand their consequences, and manage their mitigations. A standard vulnerability schema would allow common tools to work across multiple vulnerability databases and simplify the task of tracking, especially when vulnerabilities touch multiple languages or subsystems.\n\n\n\nBetter tooling is needed to understand quickly what software is affected by a newly discovered vulnerability, a problem made harder by the scale and dynamic nature of large dependency trees. Current practices also often make it difficult to predict exactly what versions are used without actually doing an installation, as the software for version resolution is only available through the installer.\n\nIt would be ideal to prevent vulnerabilities from ever being created, and although testing and analysis tools can help, prevention will always be a hard problem. Here we focus on two specific aspects:\n\n\n\nThe first category is essentially knowing about vulnerabilities at the time you decide to use a package. Taking on a new dependency has inherent risk and it needs to be an informed decision. Once you have a dependency, it generally becomes harder to remove over time.\n\nKnowing about vulnerabilities is a great start, but there is more that we can do.\n\nMany vulnerabilities arise from lack of adherence to security best practices in software development processes. Are all contributors using two-factor authentication (2FA)? Does the project have continuous integration set up and running tests? Is fuzzing integrated? These are the types of security checks that would help consumers understand the risks they\u2019re taking on with new dependencies. Packages with a low \u201cscore\u201d warrant a closer review, and a plan for remediation.\n\nThe recently announced  project from OpenSSF attempts to generate these data points in a fully automated way. Using scorecards can also help defend against  (malevolent packages with names similar to popular packages), since they would score much lower and fail many security checks.\n\nImproving the development processes for critical software is related to vulnerability prevention, but deserves its own discussion further down in our post.\n\nThe general problem of fixing vulnerabilities is beyond our scope, but there is much we can do for the specific problem of managing vulnerabilities in software dependencies. Today there is little help on this front, but as we improve precision it becomes worthwhile to invest in new processes and tooling.\n\nOne option of course is to fix the vulnerability directly. If you can do this in a backwards-compatible way, then the fix is available for everyone. But a challenge is that you are unlikely to have expertise on the problem, nor the direct ability to make changes. Fixing a vulnerability also assumes the software maintainers are aware of the issue, and have the knowledge and resources for vulnerability disclosure.\n\n \n\nConversely, if you simply remove the dependency that contains the vulnerability, then it is fixed for you and those that import or use your software, but not for anyone else. This is a change that is under your direct control.\n\nThese scenarios represent the two ends of the chain of dependencies between your software and the vulnerability, but in practice there can be many intervening packages. The general hope is that someone along that dependency chain will fix it. Unfortunately, fixing a link is not enough: Every link of the dependency chain between you and the vulnerability needs to be updated before your software will be fixed. Each link must include the fixed version of the thing below it to purge the vulnerability. Thus, the updates need to be done from the bottom up, unless you can eliminate the dependency altogether, which may require similar heroics and is rarely possible\u2014but is the best solution when it is. \n\n \n\nToday, we lack clarity on this process: what progress has been made by others and what upgrades should be applied at what level? And where is the process stuck? Who is responsible for fixing the vulnerability itself? Who is responsible for propagating the fix?\n\n\n\nEventually, your dependencies will be fixed and you can locally upgrade to the new versions. Knowing when this happens is an important goal as it accelerates reducing the exposure to vulnerabilities. We also need a notification system for the actual discovery of vulnerabilities; often new vulnerabilities represent latent problems that are newly discovered even though the actual code has not changed (such as this ). For large projects, most such issues will arise in the indirect dependencies. Today, we lack the precision required to do notification well, but as we improve vulnerability precision and metadata (as above), we should also drive notification. \n\nSo far, we have only described the easy case: a sequence of upgrades that are all backwards compatible, implying that the behavior is the same except for the absence of the vulnerability. \n\nIn practice, an upgrade is often not backward compatible, or is blocked by restrictive  requirements. These issues mean that updating a package deep in the dependency tree must cause some churn, or at least requirement updates, in the things above it. The situation often arises when the fix is made to the latest version, say 1.3, but your software or intervening packages request 1.2. We see this situation often, and it remains a big challenge that is made even harder by the difficulty of getting owners to update intervening packages. Moreover, if you use a package in a thousand places, which is not crazy for a big enterprise, you might need to go through the update process a thousand times.\n\n\n\nIt\u2019s also important to fix the vulnerability in the older versions, especially those in heavy use. Such repair is common practice for the subset of software that has long-term support, but ideally all widely used versions should be fixed, especially for security risks.\n\nAutomation could help: given a fix for one version, perhaps we can generate good candidate fixes for other versions. This process is sometimes done by hand today, but if we can make it significantly easier, more versions will actually get patched, and there will be less work to do higher in the chain.\n\nTo summarize, we need ways to make fixing vulnerabilities, especially in dependencies, both easier and more timely. We need to increase the chance that there is a fix for widely used versions and not just for the latest version, which is often hard to adopt due to the other changes it includes.\n\nFinally, there are many other options on the \u201cfixing\u201d front, including various kinds of mitigations, such as avoiding certain methods, or limiting risk through sandboxes or access controls. These are important practical options that need more discussion and support.\n\nThe framing above applies broadly to vulnerabilities, regardless of whether they are due to bad actors or are merely innocent mistakes. Although the suggested goals cover most vulnerabilities, they are not sufficient to prevent malicious behavior. To have a meaningful impact on prevention for bad actors, including supply-chain attacks, we need to improve the processes used for development.\n\nThis is a big task, and currently unrealistic for the majority of open source. Part of the beauty of open source is its lack of constraints on the process, which encourages a wide range of contributors. However, that flexibility can hinder security considerations. We want contributors, but we cannot expect everyone to be equally focused on security. Instead, we must identify critical packages and protect them. Such critical packages must be held to a range of higher development standards, even though that might add developer friction.\n\n\n\nIt is important to identify the \u201ccritical\u201d packages that we all depend upon and whose compromise would endanger critical infrastructure or user privacy. These packages need to be held to higher standards, some of which we outline below. \n\nIt is not obvious how to define \u201ccritical\u201d and the definition will likely expand over time. Beyond obvious software, such as OpenSSL or key cryptographic libraries, there are widely used packages where their sheer reach makes them worth protecting. We started the  to brainstorm this problem with the community, as well collaborating with Harvard on the Open Source Census efforts.\n\n\n\nOne principle that we follow across Google is that changes should not be unilateral\u2014that is, every change involves at least an author and a reviewer or approver. The goal is to limit what an adversary can do on their own\u2014we need to make sure someone is actually looking at the changes. To do this well for open source is actually quite a bit harder than just within a single company, which can have strong authentication and enforce code reviews and other checks.\n\nAvoiding unilateral changes can be broken down into two sub-goals:\n\n\n    \n\n\n    Besides being a great process for improving code, reviews ensure that at least one person other than the author is looking at every change. Code reviews are a standard practice for all changes within Google.\n\n\n    \n\n\n    To really achieve the \u201csomeone is looking\u201d goal, we need the reviewer to be independent from the contributor. And for critical changes, we probably want more than one independent review. We need to sort out what counts as \u201cindependent\u201d review, of course, but the idea of independence is fundamental to reviews in most industries.  \n\n\n\nAny notion of independence also implies that you know the actors\u2014an anonymous actor cannot be assumed to be independent or trustworthy. Today, we essentially have pseudonyms: the same person uses an identity repeatedly and thus can have a reputation, but we don\u2019t always know the individual\u2019s trustworthiness. This leads to a range of subgoals:\n\n\n    \n\n\n    Attackers like to have anonymity. There have been past supply-chain attacks where attackers capitalized on anonymity and worked their way through package communities to become maintainers, without anyone realizing this \u201cnew maintainer\u201d had malicious intent (compromising source code was eventually injected upstream). To mitigate this risk, our view is that owners and maintainers of critical software must not be anonymous.\n\n\n    It is conceivable that contributors, unlike owners and maintainers, could be anonymous, but only if their code has passed multiple reviews by trusted parties. \n\n\n    It is also conceivable that we could have \u201cverified\u201d identities, in which a trusted entity knows the real identity, but for privacy reasons the public does not. This would enable decisions about independence as well as prosecution for illegal behavior.\n\n\n    \n\n\n    Malicious actors look for easy attack vectors, so phishing attacks and other forms of theft related to credentials are common. One obvious improvement would be the required use of two-factor authentication, especially for owners and maintainers. \n\n\n    \n\n\n    To continue the inclusive nature of open source, we need to be able to trust a wide range of identities, but still with verified integrity. This implies a federated model for identities, perhaps similar to how we support federated SSL certificates today\u2014a range of groups can generate valid certificates, but with strong auditing and mutual oversight. \n\n on this topic are starting to take place in the OpenSSF\u2019s .\n\n\n\nWe should extend notifications to cover changes in risk. The most obvious is ownership changes, which can be a prelude to new attacks (such as the recent NPM ). Other examples include discovery of stolen credentials, collusion, or other bad actor behavior.\n\n\n\nIt is common to use secure hashes to detect if an artifact has arrived intact, and digital signatures to prove authenticity. Adding \u201ctransparency\u201d means that these attestations are logged publicly and thus document what was intended. In turn, external parties can monitor the logs for fake versions even if users are unaware. Going a step further, when credentials are stolen, we can know what artifacts were signed using those credentials and work to remove them. This kind of transparency, including the durable public logs and the third-party monitoring, has been used to great success for , and we have  one way to do this for package managers. Knowing you have the right package or binary is similar to knowing you are visiting the real version of a web site.\n\n\n\nKen Thompson's Turing Award  famously demonstrated in 1984 that authentic source code alone is not enough, and  have shown this attack is a real threat. How do you trust your build system? All the components of it must be trusted and verified through a continuous process of building trust.\n\nReproducible builds help\u2014there is a deterministic outcome for the build and we can thus verify that we got it right\u2014but are harder to achieve due to ephemeral data (such as timestamps) ending up in the release artifact. And safe reproducible builds require verification tools, which in turn must be built verifiably and reproducibly, and so on. We must construct a network of trusted tools and build products.\n\nTrust in both the artifacts and the tools can be established via \u201cdelegation\u201d, through a variant of the transparency process described above called . Internally, the Google build system signs all artifacts and produces a manifest that ties it to the source code. For open source, one or more trusted agents could run the build as a service, signing the artifact to prove that they are accountable for its integrity. This kind of ecosystem should exist and mostly needs awareness and some agreements on the format of attestations, so that we can automate the processes securely.\n\nThe actions in this section are great for software in general, and are essentially in use today within Google, but they are heavier weight than usual for open source. Our hope is that by focusing on the subset of software that is critical, we can achieve these goals at least for that set. As the tooling and automation get better, these goals will become easier to adopt more widely.\n\nThe nature of open source requires that we solve problems through consensus and collaboration. For complex topics such as vulnerabilities, this implies focused discussion around the key issues. We presented one way to frame this discussion, and defined a set of goals that we hope will accelerate industry-wide discourse and the ultimate solutions. The first set of goals apply broadly to vulnerabilities and are really about enabling automation and reducing risk and toil.\nHowever, these goals are not enough in the presence of adversaries or to prevent \u201csupply chain\u201d attacks. Thus we propose a second set of goals for critical software. The second set is more onerous and therefore will meet some resistance, but we believe the extra constraints are fundamental for security. The intention is to define collectively the set of \u201ccritical\u201d software packages, and apply these higher standards only to this set.Although we have various opinions on how to meet both sets of goals, we are but one voice in a space where consensus and sustainable solutions matter most of all. We look forward to this discussion, to promoting the best ideas, and eventually to solutions that both strengthen and streamline the security of open source that we all depend on.\n     Ideally, depended-upon versions should be stable absent an explicit upgrade, but behavior varies depending on the packaging system. Two that aim for stability rather than fast upgrades are Go Modules and NuGet, both of which by default install upgrades only when the requirements are updated; the dependencies might be wrong, but they only  with explicit updates.", "date": "\nFebruary 3, 2021\n"},
{"website": "Google-Security", "title": "\n Data Driven Security Hardening in Android\n", "author": ["Posted by Kevin Deus, Joel Galenson, Billy Lau and Ivan Lozano, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2021/01/data-driven-security-hardening-in.html", "abstract": "The Android platform team is committed to securing Android for every user across every device. In addition to  to patch vulnerabilities reported to us through our , we also proactively architect Android to protect against undiscovered vulnerabilities through hardening measures such as applying  and improving sandboxing. This post focuses on the decision-making process that goes into these proactive measures: in particular, how we choose which hardening techniques to deploy and where they are deployed. As device capabilities vary widely within the Android ecosystem, these decisions must be made carefully, guided by data available to us to maximize the value to the ecosystem as a whole.\n\nThe overall approach to Android Security is multi-pronged and leverages several principles and techniques to arrive at data-guided solutions to make future exploitation more difficult. In particular, when it comes to hardening the platform, we try to answer the following questions:\n\nBy shedding some light on the process we use to choose security features for Android, we hope to provide a better understanding of Android's overall approach to protecting our users.\n\nWe use a variety of sources to determine what areas of the platform would benefit the most from different types of security mitigations. The  (VRP) is one very informative source: all vulnerabilities submitted through this program are analyzed by our security engineers to determine the root cause of each vulnerability and its overall severity (based on ). Other sources are internal and external bug-reports, which identify vulnerable components and reveal coding practices that commonly lead to errors. Knowledge of problematic code patterns combined with the prevalence and severity of the vulnerabilities they cause can help inform decisions about which mitigations are likely to be the most beneficial.\n\n\nRelying purely on vulnerability reports is not sufficient as the data are inherently biased: often, security researchers flock to \"hot\" areas, where other researchers have already found vulnerabilities (e.g. ). Or they may focus on areas where readily-available tools make it easier to find bugs (for instance, if a security research tool is posted to Github, other researchers commonly utilize that tool to explore deeper).\n\nTo ensure that mitigation efforts are not biased only toward areas where bugs and vulnerabilities have been reported, internal Red Teams analyze less scrutinized or more complex parts of the platform. Also, continuous automated fuzzers run at-scale on both Android virtual machines and physical devices. This also ensures that bugs can be found and fixed early in the development lifecycle. Any vulnerabilities uncovered through this process are also analyzed for root cause and severity, which inform mitigation deployment decisions.\n\nThe Android VRP rewards submissions of  that demonstrate a full end-to-end attack. These exploit-chains, which generally utilize multiple vulnerabilities, are very informative in demonstrating techniques that attackers use to chain vulnerabilities together to accomplish their goals. Whenever a researcher submits a full exploit chain, a team of security engineers analyzes and documents the overall approach, each link in the chain, and any innovative attack strategies used. This analysis informs which exploit mitigation strategies could be employed to prevent pivoting directly from one vulnerability to another (some examples include  and ) and whether the process\u2019s attack surface could be reduced if it has unnecessary access to resources.\n\nThere are often multiple different ways to use a collection of vulnerabilities to create an exploit chain. Therefore a defense-in-depth approach is beneficial, with the goal of reducing the usefulness of some vulnerabilities and lengthening exploit chains so that successful exploitation requires more vulnerabilities. This increases the cost for an attacker to develop a full exploit chain.\n\nKeeping up with developments in the wider security community helps us understand the current threat landscape, what techniques are currently used for exploitation, and what future trends look like. This involves but is not limited to:\n\nAll of these data sources provide feedback for the overall security hardening strategy, where new mitigations should be deployed, and what existing security mitigations should be improved.\n\nAnalyzing the data reveals areas where broader mitigations can eliminate entire classes of vulnerabilities.  For instance, if parts of the platform show a large number of vulnerabilities due to integer overflow bugs, they are good candidates to enable Undefined Behavior Sanitizer () mitigations such as the Integer Overflow Sanitizer.  When common patterns in memory access vulnerabilities appear, they inform efforts to build  (enabled by default in ) and implement mitigations (such as ) against exploitation techniques that provide better resilience against memory overflows or Use-After-Free vulnerabilities.\n\nBefore discussing how the data can be used, it is important to understand how we classify our overall efforts in hardening the platform. There are a few broadly defined buckets that hardening techniques and mitigations fit into (though sometimes a particular mitigation may not fit cleanly into any single one):\n\nWith the broad arsenal of mitigation techniques available, which of these to employ and where to apply them depends on the type of problem being solved. For instance, a monolithic process that handles a lot of untrusted data and does complex parsing would be a good candidate for all of these. The media frameworks provide an excellent historical example where an architectural decomposition enabled incrementally turning on more exploit mitigations and deprivileging.\n\n\nRemotely reachable attack surfaces such as NFC, Bluetooth, WiFi, and media components have historically housed the most severe vulnerabilities, and as such these components are also prioritized for hardening. These components often contain some of the most common vulnerability root causes that are reported in the VRP, and we have recently enabled sanitizers in all of them.\n\nLibraries and processes that enforce or sit at security boundaries, such as , and widely-used core libraries such as , , and  are good targets for exploit mitigations since these are not process-specific. However, due to performance and stability sensitivities around these core libraries, mitigations need to be supported by strong evidence of their security impact. \n\nFinally, the kernel\u2019s high level of privilege makes it an important target for hardening as well. Because different codebases have different characteristics and functionality, susceptibility to and prevalence of certain kinds of vulnerabilities will differ. Stability and performance of mitigations here are exceptionally important to avoid negatively impacting the user experience, and some mitigations that make sense to deploy in user space may not be applicable or effective. Therefore our considerations for which hardening strategies to employ in the kernel are based on a separate analysis of the available kernel-specific data. \n\nThis data-driven approach has led to tangible and measurable results. Starting in 2015 with Stagefright, a large number of  vulnerabilities were reported in Android's media framework. These were especially sensitive because many of these vulnerabilities were remotely reachable. This led to , followed by additional efforts to . Thanks to these changes, in 2020 we had no internet-reachable Critical severity vulnerabilities reported to us in the media frameworks.\n\nSome of these mitigations provide more value than others, so it is important to focus engineering resources where they are most effective. This involves weighing the performance cost of each mitigation as well as how much work is required to deploy it and support it without negatively affecting device stability or user experience.\n\nUnderstanding the performance impact of a mitigation is a critical step toward enabling it. Adding too much overhead to some components or the entire system can negatively impact user experience by reducing battery life and making the device less responsive. This is especially true for entry-level devices, which should benefit from hardening as well. We thus want to prioritize engineering efforts on impactful mitigations with acceptable overheads.\n\nWhen investigating performance, important factors include not just CPU time but also memory increase, code size, battery life, and . These factors are especially important to consider for more constrained entry-level devices, to ensure that the mitigations perform well across the entire Android ecosystem.\n\nThe system-wide performance impact of a mitigation is also dependent on where that mitigation is enabled, as certain components are more performance-sensitive than others. For example, binder is one of the most used paths for interprocess communication, so even small additional overhead could significantly impact user experience on a device. On the other hand, video players only need to ensure that frames are rendered at the source framerate; if frames are rendered much faster than the rate at which they are displayed, additional overhead may be more acceptable.\n\nBenchmarks, if available, can be extremely useful to evaluate the performance impact of a mitigation. If there are no benchmarks for a certain component, new ones should be created, for instance by calling impacted codec code to decode a media file. If this testing reveals unacceptable overhead, there are often a few options to address it:\n\nMost of these improvements involve changes or contributions to the LLVM project. By working with upstream LLVM, these improvements have impact and benefit beyond Android. At the same time Android benefits from upstream improvements when others in the LLVM community make improvements as well.\n\nThere is more to consider when enabling a mitigation than its security benefit and performance cost, such as the cost of short-term deployment and long-term support.\n\nOne important issue is whether a mitigation can contain false positives. For example, if the Bounds Sanitizer produces an error, there is definitely an out-of-bounds access (although it might not be exploitable). But the Integer Overflow Sanitizer can produce false positives, as many integer overflows are harmless or even perfectly expected and correct.\n\nIt is thus important to consider the impact of a mitigation on the stability of the system. Whether a crash is due to a false positive or a legitimate security issue, it still disrupts the user experience and so is undesirable. This is another reason to carefully consider which components should have which mitigations, as crashes in some components are worse than others. If a mitigation causes a crash in a media codec, the user\u2019s video playback will be stopped, but if  crashes during an update, the phone could be bricked. For a mitigation like Bounds Sanitizer, where false positives are not an issue, we still need to perform extensive testing to ensure the device remains stable. Off-by-one errors, for example, may not crash during normal operation, but Bounds Sanitizer would abort execution and result in instability.\n\nAnother consideration is whether it is possible to enumerate everything a mitigation might break. For example, it is not easy to contain the risk of the Integer Overflow Sanitizer without extensive testing, as it is difficult to determine which overflows are intentional/benign (and thus should be allowed) and which could lead to vulnerabilities.\n\nWe must consider not just issues caused by deploying mitigations but also how to support them long-term. This includes the developer time to integrate a mitigation into existing systems, enable and debug it, deploy it onto devices, and support it after launch. SELinux is a good example of this; it takes a significant amount of effort to write the policy for a new device, and even once enforcing mode is enabled, the policy must be supported for years as code changes and functionality is added or removed.\n\nWe try to make mitigations less disruptive and spread awareness of how they affect developers. This is done by making documentation available on  and by improving existing algorithms to reduce false positives. Making it easier to debug mitigations when something goes wrong reduces the developer maintenance burden that can accompany mitigations. For example, when developers found it difficult to identify UBSan errors, we enabled  for the UBSan Minimal Runtime by default in the Android build system. The minimal runtime itself was first  by others at Google specifically for this purpose. When the Integer Overflow Sanitizer crashes a program, that adds the following hint to the generic SIGABRT crash message:\n\nDevelopers who see this message then know to , which prints out details about the crash:\n\nSimilarly, upstream SELinux provides a tool called audit2allow that can be used to suggest rules to allow blocked behaviors:\n\nA debugging tool does not need to be perfect to be helpful; audit2allow does not always suggest the correct options, but for developers without detailed knowledge of SELinux it provides a strong starting point.\n\nWith every Android release, our team works hard to balance security improvements that benefit the entire ecosystem with performance and stability, drawing heavily from the data that are available to us. We hope that this sheds some light on the particular challenges involved and the overall process that leads to mitigations introduced in each Android release.", "date": "\nJanuary 29, 2021\n"},
{"website": "Google-Security", "title": "\nNew Year, new password protections in Chrome\n", "author": ["Posted by Ali Sarraf, Product Manager, Chrome"], "link": "https://security.googleblog.com/2021/01/new-year-new-password-protections-in.html", "abstract": "Passwords help protect our online information, which is why it\u2019s never been more important to keep them safe. But when we\u2019re juggling dozens (if not hundreds!) of passwords across various websites\u2014from shopping, to entertainment to personal finance\u2014it feels like there\u2019s always a new account to set up or manage. While it\u2019s definitely a best practice to have a strong, unique password for each account, it can be really difficult to remember them all\u2014that\u2019s why we have a password manager in Chrome to back you up.\n\nAs you browse the web, on your phone, computer or tablet, Chrome can create, store and fill in your passwords with a single click. We'll warn you if your passwords have been compromised after logging in to sites, and you can always check for yourself in Chrome Settings. As we kick off the New Year, we\u2019re excited to announce new updates that will give you even greater control over your passwords:\n\n\n\nWe\u2019ve all had moments where we\u2019ve rushed to set up a new login, choosing a simple \u201cname-of-your-pet\u201d password to get set up quickly. However, weak passwords expose you to security risks and should be avoided. In Chrome 88, you can now complete a simple check to identify any weak passwords and take action easily.\n\nChrome can already prompt you to update your saved passwords when you log in to websites. However, you may want to update multiple usernames and passwords easily, in one convenient place. That\u2019s why starting in Chrome 88, you can manage all of your passwords even faster and easier in Chrome Settings on desktop and iOS (Chrome\u2019s Android app will be getting this feature soon, too).\nThese new updates come on top of many improvements from last year which have all contributed to your online safety and make browsing the web even easier: \n\nThe new features with Chrome 88 will be rolled out over the coming weeks, so take advantage of the new updates to keep your passwords secure. Stay tuned for more great password features throughout 2021.", "date": "\nJanuary 19, 2021\n"},
{"website": "Google-Security", "title": "\nHow the Atheris Python Fuzzer Works\n", "author": ["Posted by Ian Eldred Pudney, Google Information Security\u00a0"], "link": "https://security.googleblog.com/2020/12/how-atheris-python-fuzzer-works.html", "abstract": "Atheris is a native Python extension, and uses  to provide its code coverage and input generation capabilities. The entry point passed to atheris.Setup() is wrapped in the C++ entry point that\u2019s actually passed to libFuzzer. This wrapper will then be invoked by libFuzzer repeatedly, with its data proxied back to Python.Atheris is a native Python extension, and is typically compiled with libFuzzer linked in. When you initialize Atheris, it registers a  with CPython to collect information about Python code flow. This tracer can keep track of every line reached and every function executed.We need to get this trace information to libFuzzer, which is responsible for generating code coverage information. There\u2019s a problem, however: libFuzzer assumes that the amount of code is known at compile-time. The two primary code coverage mechanisms are __sanitizer_cov_pcs_init (which registers a set of program counters that might be visited) and __sanitizer_cov_8bit_counters_init (which registers an array of booleans that are to be incremented when a basic block is visited). Both of these need to know at initialization time how many program counters or basic blocks exist. But in Python, that isn\u2019t possible, since code isn\u2019t loaded until well after Python starts. We can\u2019t even know it when we start the fuzzer: it\u2019s possible to dynamically import code later, or even generate code on the fly.Thankfully, libFuzzer supports fuzzing shared libraries loaded at runtime. Both __sanitizer_cov_pcs_init and __sanitizer_cov_8bit_counters_init are able to be safely called from a shared library in its constructor (called when the library is loaded). So, Atheris simulates loading shared libraries! When tracing is initialized, Atheris first calls those functions with an array of 8-bit counters and completely made-up program counters. Then, whenever a new Python line is reached, Atheris allocates a PC and 8-bit counter to that line; Atheris will always report that line the same way from then on. Once Atheris runs out of PCs and 8-bit counters, it simply loads a new \u201cshared library\u201d by calling those functions again. Of course, exponential growth is used to ensure that the number of shared libraries doesn\u2019t become excessive.In the , we advise users to use Python 3.8+ where possible. This is because Python 3.8 added a new feature: opcode tracing. Not only can we monitor when every line is visited and every function is called, but we can actually monitor every operation that Python performs, and what arguments it uses. This allows Atheris to find its way through if statements much better.When a COMPARE_OP opcode is encountered, indicating a boolean comparison between two values, Atheris inspects the types of the values. If the values are bytes or Unicode, Atheris is able to report the comparison to libFuzzer via __sanitizer_weak_hook_memcmp. For integer comparison, Atheris uses the appropriate function to report integer comparisons, such as __sanitizer_cov_trace_cmp8.In recent Python versions, a Unicode string is actually represented as an array of 1-byte, 2-byte, or 4-byte characters, based on the size of the largest character in the string. The obvious solution for coverage is to:", "date": "\nDecember 9, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing Bonus Rewards for V8 Exploits\n", "author": ["Posted by Martin Barbella, Chrome Vulnerability Rewards Panelist"], "link": "https://security.googleblog.com/2020/12/announcing-bonus-rewards-for-v8-exploits.html", "abstract": "Starting today, the Chrome Vulnerability Rewards Program is offering a new bonus for reports which demonstrate exploitability in , Chrome\u2019s JavaScript engine. We have historically had many great V8 bugs reported (thank you to all of our reporters!) but we'd like to know more about the exploitability of different V8 bug classes, and what mechanisms are effective to go from an initial bug to a full exploit. That's why we're offering this additional reward for bugs that show how a V8 vulnerability could be used as part of a real world attack.\n\nIn the past, exploits had to be fully functional to be rewarded at our highest tier, . Demonstration of how a bug might be exploited is one factor that the panel may use to determine that a report is , our second highest tier, but we want to encourage more of this type of analysis. This information is very useful for us when planning future mitigations, making release decisions, and fixing bugs faster. We also know it requires a bit more effort for our reporters, and that effort should be rewarded. For the time being this only applies to V8 bugs, but we\u2019re curious to see what our reporters come up with!\n\nThe full details are available on the . At a high-level, we\u2019re offering increased reward amounts, up to double, for qualifying V8 bugs.\n\nThe following table shows the updated reward amounts for reports qualifying for this new bonus. These new, higher values replace the normal reward. If a bug in V8 doesn\u2019t fit into one of these categories, it may still qualify for an increased reward at the panel\u2019s discretion.\n\nSo what does a report need to do to demonstrate that a bug is likely exploitable? Any V8 bug report which would have previously been rewarded at the  level will likely qualify with no additional effort from the reporter. By definition, these demonstrate that the issue was exploitable. V8 reports at the  level may also qualify if they include evidence that the bug is exploitable as part of their analysis. See the  for more information about our reward levels.\n\nThe following are some examples of how a report could demonstrate that exploitation is likely, but any analysis or proof of concept will be considered by the panel:\n\nFor example reports, see issues  and .\n\nWe\u2019d like to thank all of our VRP reporters for helping us keep Chrome users safe! We look forward to seeing what you find.\n\n-The Chrome Vulnerability Rewards Panel", "date": "\nDecember 8, 2020\n"},
{"website": "Google-Security", "title": "\nOpenTitan at One Year: the Open Source Journey to Secure Silicon\n", "author": ["Posted by Dominic Rizzo, OpenTitan Lead, Google\u00a0"], "link": "https://security.googleblog.com/2020/12/opentitan-at-one-year-open-source.html", "abstract": "", "date": "\nDecember 7, 2020\n"},
{"website": "Google-Security", "title": "\nImproving open source security during the Google summer internship program \n", "author": ["Posted by the Information Security Engineering team at Google\u00a0"], "link": "https://security.googleblog.com/2020/12/improving-open-source-security-during.html", "abstract": "", "date": "\nDecember 7, 2020\n"},
{"website": "Google-Security", "title": "\nFostering research on new web security threats\n", "author": ["Posted by Artur Janc and terjanq, Information Security Engineers\u00a0"], "link": "https://security.googleblog.com/2020/12/fostering-research-on-new-web-security.html", "abstract": "", "date": "\nDecember 4, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing our open source security key test suite\n", "author": ["Posted by Fabian Kaczmarczyck, Software Engineer, Jean-Michel Picod, Software Engineer and Elie Bursztein, Security and Anti-abuse Research Lead"], "link": "https://security.googleblog.com/2020/11/announcing-our-open-source-security-key.html", "abstract": "", "date": "\nNovember 13, 2020\n"},
{"website": "Google-Security", "title": "\n Privacy-preserving features in the Mobile Driving License\n", "author": ["Posted by David Zeuthen, Shawn Willden and Ren\u00e9 Mayrhofer, Android Security and Privacy team "], "link": "https://security.googleblog.com/2020/10/privacy-preserving-features-in-mobile.html", "abstract": "In the United States and other countries a  is not only used to convey driving privileges, it is also commonly used to prove identity or personal details.\n\nPresenting a Driving License is simple, right? You hand over the card to the individual wishing to confirm your identity (the so-called \u201c\u201d or \u201c\u201d); they check the  (hologram, micro-printing, etc.) to ensure it\u2019s not counterfeit; they check that it\u2019s really your license, making sure you look like the portrait image printed on the card; and they read the data they\u2019re interested in, typically your age, legal name, address etc. Finally, the verifier needs to hand back the plastic card.\n\n \n\nMost people are so familiar with this process that they don\u2019t think twice about it, or consider the privacy implications. In the following we\u2019ll discuss how the new and soon-to-be-released  standard will improve on nearly every aspect of the process, and what it has to do with Android.\n\nThe  standard has been written by a diverse group of people representing driving license issuers (e.g. state governments in the US), relying parties (federal and state governments, including law enforcement), academia, industry (including Google), and many others. This ISO standard allows for construction of Mobile Driving License (mDL) applications which users can carry in their phone and can use instead of the plastic card.\n\nInstead of handing over your plastic card, you open the mDL application on your phone and press a button to share your mDL. The Verifier (aka \u201cRelying Party\u201d) has their own device with an mDL reader application and they either scan a QR code shown in your mDL app or do an NFC tap. The QR code (or NFC tap) conveys an ephemeral cryptographic public key and hardware address the mDL reader can connect to.\n\nOnce the mDL reader obtains the cryptographic key it creates its own ephemeral keypair and establishes an encrypted and authenticated, secure wireless channel (BLE, Wifi Aware or NFC)). The mDL reader uses this secure channel to request data, such as the portrait image or what kinds of vehicles you're allowed to drive, and can also be used to  such as \u201cis the holder older than 18?\u201d\n\nCrucially, the mDL application can ask the user to approve which data to release and may require the user to authenticate with fingerprint or face \u2014 none of which a passive plastic card could ever do.\n\n \n\nWith this explanation in mind, let\u2019s see how presenting an mDL application compares with presenting a plastic-card driving license:\n\nThese are some of the reasons why we think mDL is a big win for end users in terms of privacy.\n\nOne commonality between plastic-card driving licences and the mDL is how the relying party verifies that the person presenting the license is the authorized holder. In both cases, the verifier manually compares the appearance of the individual against a portrait photo, either printed on the plastic or transmitted electronically and  that it\u2019s hard for individuals to match strangers to portrait images.\n\nThe initial version of ISO 18013-5 won\u2019t improve on this but the ISO committee working on the standard is already investigating ways to utilize on-device biometrics sensors to perform this match in a secure and privacy-protecting way. The hope is that improved fidelity in the process helps reduce unauthorized use of identity documents.\n\nThrough facilities such as hardware-based Keystore, Android already offers excellent support for security and privacy-sensitive applications and in fact it\u2019s already possible to implement the  standard on Android without further platform changes. Many organizations participating in the ISO committee have already implemented 18013-5 Android apps.\n\nThat said, with purpose-built support in the operating system it is possible to provide better security and privacy properties. Android 11 includes the  at the Framework level along with a  which can be implemented by Android OEMs to enable identity credential support in Secure Hardware. Using the Identity Credential API, the  of mDL applications does not include the application or even Android itself. This will be particularly important for future versions where the verifier must trust the device to identify and authenticate the user, for example through fingerprint or face matching on the holder's own device. It\u2019s likely such a solution will require  hardware and/or software and certification is not practical if the TCB includes the hundreds of millions of lines of code in Android and the Linux kernel.\n\nOne advantage of plastic cards is that they don't require power or network communication to be useful. Putting all your licenses on your phone could seem inconvenient in cases where your device is low on battery, or does not have enough battery life to start. The Android Identity Credential HAL therefore provides support for a mode called , where the license is still available through an NFC tap even when the phone's battery is too low to boot it up. Device makers can implement this mode, but it will require hardware support that will take several years to roll out.\n\nFor devices without the Identity Credential HAL, we have an  which implements the same API and works on nearly every Android device in the world (API level 24 or later). If the device has hardware-backed Identity Credential support then this Jetpack simply forwards calls to the platform API. Otherwise, an Android Keystore-backed implementation will be used. While the Android Keystore-backed implementation does not provide the same level of security and privacy, it is perfectly adequate for both holders and issuers in cases where all data is issuer-signed. Because of this, the Jetpack is the preferred way to use the Identity Credential APIs. We also made available using the Identity Credential APIs.\n\nAndroid now includes APIs for managing and presenting with identity documents in a more secure and privacy-focused way than was previously possible. These can be used to implement ISO 18013-5 mDLs but the APIs are generic enough to be usable for other kinds of electronic documents, from school ID or bonus program club cards to passports.\n\nAdditionally, the Android Security and Privacy team actively participates in the ISO committees where these standards are written and also works with civil liberties groups to .", "date": "\nOctober 28, 2020\n"},
{"website": "Google-Security", "title": "\nFuzzing internships for Open Source Software\n", "author": ["Posted by Abhishek Arya, Chrome Security team"], "link": "https://security.googleblog.com/2020/10/fuzzing-internships-for-open-source.html", "abstract": "Open source software is the foundation of many modern software products. Over the years, developers increasingly have  reusable open source components for their applications. It is paramount that these open source components are secure and reliable, as weaknesses impact those that build upon it.\n\nGoogle cares deeply about the security of the open source ecosystem and recently launched the  with other industry partners.  is an automated testing technique to find bugs by feeding unexpected inputs to a target program. At Google, we leverage fuzzing at scale to find tens of thousands of security vulnerabilities and stability bugs. This summer, as part of , we hosted 50 interns to improve the state of fuzz testing in the open source ecosystem.\n\nThe fuzzing interns worked towards integrating new projects and improving existing ones in , our continuous fuzzing service for the open source community (which has  projects,  bugs, 89% fixed). Several widely used open source libraries including but not limited to , , , and , now have continuous fuzzing coverage as a result of these efforts. \n\nAnother group of interns focused on improving the security of the . , a kernel fuzzing tool from Google, has been instrumental in  in various operating systems. The interns were tasked with improving the fuzzing coverage by adding new descriptions to syzkaller like , , and  for example, refining the , and advancing kernel .\n\nSome interns chose to write fuzzers for Android and Chrome, which are open source projects that billions of internet users rely on. For Android, the interns contributed several new fuzzers for uncovered areas - network protocols such as  and , audio codecs like , , and . On the Chrome side, interns improved existing blackbox fuzzers, particularly in the areas: DOM, IPC, media, extensions, and added new .\n\nOur last set of interns researched quite a few under-explored areas of fuzzing, some of which were , , ,  and made useful contributions.\n\nOver the course of the internship, our interns have reported over 150 security vulnerabilities and 750 functional bugs. Given the overall success of these efforts, we plan to continue hosting fuzzing internships every year to help secure the open source ecosystem and teach incoming open source contributors about the importance of fuzzing. For more information on the Google internship program and other student opportunities, check out . We encourage you to apply.", "date": "\nOctober 9, 2020\n"},
{"website": "Google-Security", "title": "\nPrivacy-Preserving Smart Input with Gboard\n", "author": ["Posted by Yang Lu, Software Engineer, Angana Ghosh, Group Product Manager, and Xu Liu, Director of Engineering, Gboard team\n"], "link": "https://security.googleblog.com/2020/10/privacy-preserving-smart-input-with.html", "abstract": "Google Keyboard (a.k.a Gboard) has a critical mission to provide frictionless input on Android to empower users to communicate accurately and express themselves effortlessly. In order to accomplish this mission, Gboard must also protect users' private and sensitive data. Nothing users type is sent to Google servers.  We recently launched privacy-preserving input by further advancing the latest . In Android 11, Gboard also launched the contextual input suggestion experience by integrating on-device smarts into the user's daily communication in a privacy-preserving way.\n\n\nBefore Android 11, input suggestions were surfaced to users in several different places. In Android 11, Gboard launched a consistent and coordinated approach to access contextual input suggestions. For the first time, we've brought Smart Replies to the keyboard suggestions - powered by system intelligence running entirely on device. The smart input suggestions are rendered with a transparent layer on top of Gboard\u2019s suggestion strip.  This structure maintains the trust boundaries between the Android platform and Gboard, meaning sensitive personal content cannot be not accessed by Gboard. The suggestions are only sent to the app after the user taps to accept them.\n\nFor instance, when a user receives the message \u201cHave a virtual coffee at 5pm?\u201d in Whatsapp, on-device system intelligence predicts smart text and emoji replies \u201cSounds great!\u201d and \u201c\ud83d\udc4d\u201d. Android system intelligence can see the incoming message but Gboard cannot. In Android 11, these Smart Replies are rendered by the Android platform on Gboard\u2019s suggestion strip as a transparent layer. The suggested reply is generated by the system intelligence. When the user taps the suggestion, Android platform sends it to the input field directly. If the user doesn't tap the suggestion, gBoard and the app cannot see it. In this way, Android and Gboard surface the best of Google smarts whilst keeping users' data private: none of their data goes to any app, including the keyboard, unless they've tapped a suggestion.\n\nAdditionally,  has enabled Gboard to train intelligent input models across many devices while keeping everything individual users type on their device. Today, the emoji is as common as punctuation - and have become the way for our users to express themselves in messaging. Our users want a way to have fresh and diversified emojis to better express their thoughts in messaging apps. Recently, we launched new on-device transformer models that are fine-tuned with federated learning in Gboard, to produce more contextual emoji predictions for English, Spanish and Portuguese.\n\nFurthermore, following the success of privacy-preserving machine learning techniques, Gboard continues to leverage  to understand how Gboard is used from decentralized data. What we've learned from privacy-preserving analysis has let us make better decisions in our product.\n\nWhen a user shares an emoji in a conversation, their phone keeps an ongoing count of which emojis are used. Later, when the phone is idle, plugged in, and connected to WiFi, Google\u2019s  invites the device to join a \u201cround\u201d of federated analytics data computation with hundreds of other participating phones. Every device involved in one round will compute the emoji share frequency, encrypt the result and send it a federated analytics server. Although the server can\u2019t decrypt the data individually, the final tally of total emoji counts can be decrypted when combining encrypted data across devices. The aggregated data shows that the most popular emoji is \ud83d\ude02 in Whatsapp,  \ud83d\ude2d in Roblox(gaming), and \u2714 in Google Docs. Emoji \ud83d\ude37 moved up from 119th to 42nd in terms of frequency during COVID-19.\n\nGboard always has a strong commitment to Google\u2019s . Gboard strives to build privacy-preserving effortless input products for users to freely express their thoughts in 900+ languages while safeguarding user data. We will keep pushing the state of the art in smart  input technologies on Android while safeguarding user data. Stay tuned!", "date": "\nOctober 7, 2020\n"},
{"website": "Google-Security", "title": "\nNew Password Protections (and more!) in Chrome\n", "author": ["Posted by AbdelKarim Mardini, Senior Product Manager, Chrome "], "link": "https://security.googleblog.com/2020/10/new-password-protections-and-more-in.html", "abstract": "To check whether you have any compromised passwords, Chrome sends a copy of your usernames and passwords to Google using a . This lets Google check them against lists of credentials known to be compromised, but Google cannot derive your username or password from this encrypted copy. \n\nWe notify you when you have compromised passwords on websites, but it can be time-consuming to go find the relevant form to change your password. To help, we\u2019re adding support for  that let Chrome take users directly to the right \u201cchange password\u201d form after they\u2019ve been alerted that their password has been compromised.\n\nAlong with these improvements, Chrome is also bringing  to mobile. In our next release, we will launch Safety Check on iOS and Android, which includes checking for compromised passwords, telling you if Safe Browsing is enabled, and whether the version of Chrome you are running is updated with the latest security protections. You will also be able to use Chrome on iOS to autofill saved login details into other apps or browsers.\n\nIn Chrome 86 we\u2019ll also be launching a number of additional features to improve user security, including: \n\n\n\nEarlier this year, we launched  for desktop, which gives Chrome users the option of more advanced security protections. \n\nWhen you turn on Enhanced Safe Browsing, Chrome can proactively protect you against phishing, malware, and other dangerous sites by sharing real-time data with Google\u2019s Safe Browsing service. Among our users who have enabled checking websites and downloads in real time, our  see a roughly 20% drop in users typing their passwords into phishing sites.\n\n\n\nWe recently launched  on Android to prevent phishing attacks. To improve security on iOS too, we\u2019re introducing a biometric authentication step before autofilling passwords. On iOS, you\u2019ll now be able to authenticate using Face ID, Touch ID, or your phone passcode. Additionally, Chrome Password Manager allows you to autofill saved passwords into iOS apps or browsers if you enable Chrome autofill in Settings.\n\n\n\nSecure HTTPS pages may sometimes still have non-secure features. Earlier this year, Chrome began , when secure pages incorporate insecure content. But there are still other ways that HTTPS pages can create security risks for users, such as offering downloads over non-secure links, or using forms that don\u2019t submit data securely.  \n\nTo better protect users from these threats, Chrome 86 is introducing  on desktop and Android to alert and warn users before submitting a non-secure form that\u2019s embedded in an HTTPS page.\n\nAdditionally, Chrome 86 will block or warn on some insecure downloads initiated by secure pages. Currently, this change affects commonly abused file types, but eventually secure pages will only be able to initiate secure downloads of any type. For more details, see  to gradually block mixed downloads altogether\n\nWe encourage developers to update their forms and downloads to use secure connections for the safety and privacy of their users.", "date": "\nOctober 6, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing the launch of the Android Partner Vulnerability Initiative\n", "author": [], "link": "https://security.googleblog.com/2020/10/announcing-launch-of-android-partner.html", "abstract": "Google\u2019s Android Security & Privacy team has launched the  (APVI) to manage security issues specific to Android OEMs. The APVI is designed to drive remediation and provide transparency to users about issues we have discovered at Google that affect device models shipped by Android partners.\n\nAndroid incorporates industry-leading security features and every day we work with developers and device implementers to keep the Android platform and ecosystem safe. As part of that effort, we have a range of existing programs to enable security researchers to report security issues they have found. For example, you can report vulnerabilities in Android code via the  (ASR), and vulnerabilities in popular third-party Android apps through the . Google releases ASR reports in Android Open Source Project (AOSP) based code through the  (ASB). These reports are issues that could impact all Android based devices. All Android partners must adopt ASB changes in order to declare the current month\u2019s Android security patch level (SPL). But until recently, we didn\u2019t have a clear way to process Google-discovered security issues outside of AOSP code that are unique to a much smaller set of specific Android OEMs. The APVI aims to close this gap, adding another layer of security for this targeted set of Android OEMs.\n\nThe APVI covers Google-discovered issues that could potentially affect the security posture of an Android device or its user and is aligned to  Information technology -- Security techniques -- Vulnerability disclosure recommendations. The initiative covers a wide range of issues impacting device code that is not serviced or maintained by Google (these are handled by the Android Security Bulletins).\n\nThe APVI has already processed a number of security issues, improving user protection against permissions bypasses, execution of code in the kernel, credential leaks and generation of unencrypted backups. Below are a few examples of what we\u2019ve found, the impact and OEM remediation efforts.\n\nIn some versions of a third-party pre-installed over-the-air (OTA) update solution, a custom system service in the Android framework exposed privileged APIs directly to the OTA app. The service ran as the system user and did not require any permissions to access, instead checking for knowledge of a hardcoded password. The operations available varied across versions, but always allowed access to sensitive APIs, such as silently installing/uninstalling APKs, enabling/disabling apps and granting app permissions. This service appeared in the code base for many device builds across many OEMs, however it wasn\u2019t always registered or exposed to apps. We\u2019ve worked with impacted OEMs to make them aware of this security issue and provided guidance on how to remove or disable the affected code.\n\nA popular web browser pre-installed on many devices included a built-in password manager for sites visited by the user. The interface for this feature was exposed to WebView through JavaScript loaded in the context of each web page. A malicious site could have accessed the full contents of the user\u2019s credential store. The credentials are encrypted at rest, but used a weak algorithm (DES) and a known, hardcoded key. This issue was reported to the developer and updates for the app were issued to users.\n\nThe  method in the  class was modified in the framework code for some devices to allow special permissions access to some apps. In one version, the method granted apps with the shared user ID  any permission they requested and apps signed with the same key as the  package any permission in their manifest. Another version of the modification allowed apps matching a list of package names and signatures to pass runtime permission checks even if the permission was not in their manifest. These issues have been fixed by the OEMs.\n\n\nKeep an eye out at  for future disclosures of Google-discovered security issues under this program, or find more information there on issues that have already been disclosed.", "date": "\nOctober 2, 2020\n"},
{"website": "Google-Security", "title": "\nLockscreen and Authentication  Improvements in Android 11\n", "author": ["Posted by Haining Chen, Vishwath Mohan, Kevin Chyn and Liz Louis, Android Security Team"], "link": "https://security.googleblog.com/2020/09/lockscreen-and-authentication.html", "abstract": "This trust is paramount to the Android Security team. The team focuses on ensuring that Android devices respect the privacy and sensitivity of user data. A fundamental aspect of this work centers around the lockscreen, which acts as the proverbial front door to our devices. After all, the lockscreen  that only the intended user(s) of a device can access their private data. \n\nThis blog post outlines recent improvements around how users interact with the lockscreen on Android devices and more generally with authentication. In particular, we focus on two categories of authentication that present both immense potential as well as potentially immense risk if not designed well: biometrics and environmental modalities. \n\nBefore getting into the details of lockscreen and authentication improvements, we first want to establish some context to help relate these improvements to each other. A good way to envision these changes is to fit them into the framework of the , a conceptual classification of all the different authentication modalities on Android, how they relate to each other, and how they are constrained based on this classification. \n\nThe model itself is fairly simple, classifying authentication modalities into three buckets of decreasing levels of security and commensurately increasing constraints. The primary tier is the least constrained in the sense that users only need to re-enter a primary modality under certain situations (for example, after each boot or every 72 hours) in order to use its capability. The secondary and tertiary tiers are more constrained because they cannot be set up and used without having a primary modality enrolled first and they have more constraints further restricting their capabilities. \n\n\n    Knowledge factors are especially useful on Android becauses devices offer hardware backed brute-force protection with exponential-backoff, meaning Android devices prevent attackers from repeatedly guessing a PIN, pattern, or password by having hardware backed timeouts after every 5 incorrect attempts. Knowledge factors also confer additional benefits to all users that use them, such as  (FBE) and .\n\n\tWe will delve into Android biometrics in the next section.\n\n\t\n\n\n    While both Trusted Places and Trusted Devices (and tertiary modalities in general) offer convenient ways to get access to the contents of your device, the fundamental issue they share is that they are ultimately a . For example, an attacker could unlock a misplaced phone that uses Trusted Place simply by driving it past the user's home, or with moderate amount of effort, spoofing a GPS signal using off-the-shelf Software Defined Radios and some mild scripting. Similarly with Trusted Device, access to a safelisted bluetooth device also gives access to all data on the user\u2019s phone. \n\n\n    Because of this, a major improvement has been made to the environmental tier in Android 10. . In this new mode, a tertiary tier modality can no longer unlock a locked device. Instead, if the device is first unlocked using either a primary or secondary modality, it can continue to keep it in the unlocked state for a maximum of four hours.\n\nBiometric implementations come with a wide variety of security characteristics, so we rely on the following two key factors to determine the security of a particular implementation:\n\nWe use these two factors to classify biometrics into one of three different classes in decreasing order of security:\n\nEach class comes with an associated set of constraints that aim to balance their ease of use with the level of security they offer. \n\nThese constraints reflect the length of time before a biometric falls back to primary authentication, and the allowed application integration. For example, a Class 3 biometric enjoys the longest timeouts and offers all integration options for apps, while a Class 1 biometric has the shortest timeouts and no options for app integration. You can see a summary of the details in the table below, or the full details in the  (CDD).\n\n App integration means exposing an API to apps (e.g., via integration with BiometricPrompt/BiometricManager, androidx.biometric, or FIDO2 APIs)\n\n Keystore integration means integrating Keystore, e.g., to release app auth-bound keys\n\nBiometrics provide convenience to users while maintaining a high level of security. Because users need to set up a primary authentication modality in order to use biometrics, it helps boost the lockscreen adoption (we see an average of 20% higher lockscreen adoption on devices that offer biometrics versus those that do not). This allows more users to benefit from the security features that  the lockscreen provides: gates unauthorized access to sensitive user data and also confers other advantages of a primary authentication modality to these users, such as encrypted backups. Finally, biometrics also help reduce  attacks in which an attacker tries to reproduce a PIN, pattern, or password after observing a user entering the credential.\n\nHowever, it is important that users understand the trade-offs involved with the use of biometrics. Primary among these is that  For example, a face biometric implementation might be fooled by family members who resemble the user or a 3D mask of the user. A fingerprint biometric implementation could potentially be bypassed by a spoof made from latent fingerprints of the user. Although anti-spoofing or Presentation Attack Detection (PAD) technologies have been actively developed to mitigate such spoofing attacks, they are mitigations, not preventions. \n\nOne effort that Android has made to mitigate the potential risk of using biometrics is the lockdown mode introduced in Android P. Android users can use this feature to temporarily disable biometrics, together with Smart Lock (for example, Trusted Places and Trusted Devices) as well as notifications on the lock screen, when they feel the need to do so. \n\nTo use the lockdown mode, users first need to set up a primary authentication modality and then enable it in settings. The exact setting where the lockdown mode can be enabled varies by device models, and on a Google Pixel 4 device it is under Settings > Display > Lock screen > Show lockdown option. Once enabled, users can trigger the lockdown mode by holding the power button and then clicking the Lockdown icon on the power menu. A device in lockdown mode will return to the non-lockdown state after a primary authentication modality (such as a PIN, pattern, or password) is used to unlock the device.\n\nIn order for developers to benefit from the security guarantee provided by Android biometrics and to easily integrate biometric authentication into their apps to better protect sensitive user data, we introduced the  APIs in Android P.\n\n\nThere are several benefits of using the BiometricPrompt APIs. Most importantly, these APIs allow app developers to target biometrics in a modality-agnostic way across different Android devices (that is, BiometricPrompt can be used as a single integration point for various biometric modalities supported on devices), while controlling the security guarantees that the authentication needs to provide (such as requiring Class 3 or Class 2 biometrics, with device credential as a fallback). In this way, it helps protect app data with a second layer of defenses (in addition to the lockscreen) and in turn respects the sensitivity of user data. Furthermore, BiometricPrompt provides a persistent UI with customization options for certain information (for example, title and description), offering a consistent user experience across biometric modalities and across Android devices.\n\nAs shown in the following architecture diagram, apps can integrate with biometrics on Android devices through either the framework API or the support library (that is,  for backward compatibility). One thing to note is that  is deprecated because developers are encouraged to migrate to  for modality-agnostic authentications.\n\nAndroid 10 introduced the  class that developers can use to query the availability of biometric authentication and included fingerprint and face authentication integration for .\n\n\nIn Android 11, we introduce new features such as the interface which allows developers to specify the authentication types accepted by their apps, as well as additional support for auth-per-use keys within the  class. \n\nMore details can be found in the  and  documentation. Read more about  API usage in our blog post  and our codelab .", "date": "\nSeptember 22, 2020\n"},
{"website": "Google-Security", "title": "\nImproved malware protection for users in the Advanced Protection Program\n", "author": ["Posted by Daniel Rubery, Software Engineer, Chrome, Ryan Rasti, Software Engineer, Safe Browsing, and Eric Mill, Product Manager, Chrome Security"], "link": "https://security.googleblog.com/2020/09/improved-malware-protection-for-users.html", "abstract": "Advanced Protection users are already well-protected from phishing. As a result, we\u2019ve seen that attackers target these users through other means, such as leading them to download malware. In August 2019, Chrome .\n\nNow, in addition to this warning, Chrome is giving Advanced Protection users the ability to send risky files to be scanned by Google Safe Browsing\u2019s full suite of malware detection technology before opening the file. We expect these cloud-hosted scans to significantly improve our ability to detect when these files are malicious. \n\n  \n\nWhen a user downloads a file, Safe Browsing will perform a quick check using metadata, such as hashes of the file, to evaluate whether it appears potentially suspicious. For any downloads that Safe Browsing deems risky, but not clearly unsafe, the user will be presented with a warning and the ability to send the file to be scanned. If the user chooses to send the file, Chrome will upload it to Google Safe Browsing, which will scan it using its static and dynamic analysis techniques in real time. After a short wait, if Safe Browsing determines the file is unsafe, Chrome will warn the user. As always, users can bypass the warning and open the file without scanning, if they are confident the file is safe. Safe Browsing deletes uploaded files a short time after scanning.", "date": "\nSeptember 16, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing new reward amounts for abuse risk researchers\n", "author": ["Posted by Marc Henson, Lead and Program Manager, Trust & Safety; Anna Hupa, Senior Strategist, at Google"], "link": "https://security.googleblog.com/2020/09/announcing-new-reward-amounts-for-abuse.html", "abstract": "", "date": "\nSeptember 1, 2020\n"},
{"website": "Google-Security", "title": "\nPixel 4a is the first device to go through ioXt at launch\n", "author": ["Posted by Eugene Liderman and Xevi Miro Bruix, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2020/08/pixel-4a-is-first-device-to-go-through.html", "abstract": "Trust is very important when it comes to the relationship between a user and their smartphone. While phone functionality and design can enhance the user experience, security is fundamental and foundational to our relationship with our phones.There are multiple ways to build trust around the security capabilities that a device provides and we continue to invest in verifiable ways to do just that.\n\n\n\nToday we are happy to announce that the  and the are the first Android smartphones to go through  against the  \n\nThe  manages a security compliance assessment program for connected devices. ioXt has over 200 members across various industries, including Google, Amazon, Facebook, T-Mobile, Comcast, Zigbee Alliance, Z-Wave Alliance, Legrand, Resideo, Schneider Electric, and many others. With so many companies involved, ioXt covers a wide range of device types, including smart lighting, smart speakers, webcams, and Android smartphones.\n\nThe core focus of ioXt is \u201cto set security standards that bring  and  to the market and directly into the hands of consumers.\u201d This is accomplished by assessing devices against a  and relying on publicly available evidence. The goal of ioXt\u2019s approach is to enable users, enterprises, regulators, and other stakeholders to understand the security in connected products to drive better awareness towards how these products are protecting the security and privacy of users.\n\nioXt\u2019s baseline security requirements are tailored for product classes, and the  enables smartphone manufacturers to differentiate security capabilities, including biometric authentication strength, security update frequency, length of security support lifetime commitment, vulnerability disclosure program quality, and preloaded app risk minimization.\n\nWe believe that using a widely known industry consortium standard for Pixel certification provides increased trust in the security claims we make to our users.  has published an audit report that can be downloaded . The report documents the evaluation of / and  against the ioXt Android Profile. \n\n is one of the most important criteria used in the ioXt Android profile. Security by Default rates devices by cumulatively scoring the risk for all preloads on a particular device. For this particular measurement, we worked with a team of university experts from the University of Cambridge, University of Strathclyde, and Johannes Kepler University in Linz to create a formula that considers the risk of platform signed apps, pregranted permissions on preloaded apps, and apps communicating using cleartext traffic.\nScreenshot of the  of the  at the \nIn partnership with those teams, Google created , an open source tool that collects necessary attributes from the device and runs it through this formula to come up with a raw score. NCC Group leveraged Uraniborg to conduct the assessment for the ioXt Security by Default category.\n\nAs part of our ongoing certification efforts, we look forward to submitting future Pixel smartphones through the ioXt standard, and we encourage the Android device ecosystem to participate in similar transparency efforts for their devices.", "date": "\nAugust 10, 2020\n"},
{"website": "Google-Security", "title": "\nTowards native security defenses for the web ecosystem\n", "author": ["Posted by Artur Janc and Lukas Weichselbaum, Information Security Engineers"], "link": "https://security.googleblog.com/2020/07/towards-native-security-defenses-for.html", "abstract": "", "date": "\nJuly 22, 2020\n"},
{"website": "Google-Security", "title": "\nSystem hardening in Android 11\n", "author": ["Posted by Platform Hardening Team "], "link": "https://security.googleblog.com/2020/06/system-hardening-in-android-11.html", "abstract": "In Android 11 we continue to increase the security of the Android platform.  We have moved to safer default settings, migrated to a hardened memory allocator, and expanded the use of compiler mitigations that defend against classes of vulnerabilities and frustrate exploitation techniques. \nWe\u2019ve enabled forms of automatic memory initialization in both Android 11\u2019s userspace and the Linux kernel. Uninitialized memory bugs occur in C/C++ when memory is used without having first been initialized to a known safe value. These types of bugs can be confusing, and even the term \u201cuninitialized\u201d is misleading. Uninitialized may seem to imply that a variable has a random value.  In reality it isn\u2019t random. It has whatever value was previously placed there. This value may be predictable or even attacker controlled. Unfortunately this behavior can result in a serious vulnerability such as information disclosure bugs like  bypasses, or control flow hijacking via a . Another possible side effect of using uninitialized values is advanced compiler optimizations may transform the code unpredictably, as this is considered  by the relevant C standards.\n\nIn practice, uses of uninitialized memory are difficult to detect. Such errors may sit in the codebase unnoticed for years if the memory happens to be initialized with some \"safe\" value most of the time. When uninitialized memory results in a bug, it is often challenging to identify the source of the error, particularly if it is rarely triggered.Eliminating an entire class of such bugs is a lot more effective than hunting them down individually. Automatic stack variable initialization relies on a feature in the Clang compiler which allows choosing initializing local variables with either zeros or a pattern.Initializing to zero provides safer defaults for strings, pointers, indexes, and sizes. The downsides of zero init are less-safe defaults for return values, and exposing fewer bugs where the underlying code relies on zero initialization. Pattern initialization tends to expose more bugs and is generally safer for return values and less safe for strings, pointers, indexes, and sizes.\n\nAutomatic stack variable initialization is enabled throughout the entire Android userspace. During the development of Android 11, we initially selected pattern in order to uncover bugs relying on zero init and then moved to zero-init after a few months for increased safety. Platform OS developers can build with  if they want help uncovering bugs relying on zero init.\n\nAutomatic  and  initialization were recently merged in the upstream Linux kernel. We have made these features available on earlier versions of Android\u2019s kernel including , , and . These features enforce initialization of local variables and heap allocations with known values that cannot be controlled by attackers and are useless when leaked. Both features result in a performance overhead, but also prevent undefined behavior improving both stability and security.\n\nFor kernel stack initialization we adopted the  from upstream Linux. It currently relies on Clang pattern initialization for stack variables, although this is subject to change in the future. \n\nHeap initialization is controlled by two boot-time flags, init_on_alloc and init_on_free, with the former wiping freshly allocated heap objects with zeroes (think  in the whole kernel) and the latter doing the same before the objects are freed (this helps to reduce the lifetime of security-sensitive data).  is a lot more cache-friendly and has smaller performance impact (within 2%), therefore it has been chosen to protect Android kernels.\n\nIn Android 11, Scudo replaces jemalloc as the default native allocator for Android. Scudo is a hardened memory allocator designed to help detect and mitigate memory corruption bugs in the heap, such as:\n\nScudo does not fully prevent exploitation but it does add a number of sanity checks which are effective at strengthening the heap against some memory corruption bugs.\n\nIt also proactively organizes the heap in a way that makes exploitation of memory corruption more difficult, by reducing the predictability of the allocation patterns, and separating allocations by sizes. \n\nIn our internal testing, Scudo has already proven its worth by surfacing security and stability bugs that were previously undetected.\n\nAndroid 11 introduces GWP-ASan, an in-production heap memory safety bug detection tool that's integrated directly into the native allocator Scudo. GWP-ASan probabilistically detects and provides actionable reports for heap memory safety bugs when they occur, works on 32-bit and 64-bit processes, and is enabled by default for system processes and system apps.\n\nGWP-ASan is also  via a one line opt-in in an app's AndroidManifest.xml, with no complicated build support or recompilation of prebuilt libraries necessary.\n\nContinuing work on  (MTE) in Android, Android 11 includes support for kernel , also known as . Userspace  is supported since Android 10.\n\n (KASAN) is a dynamic memory error detector designed to find out-of-bound and use-after-free bugs in the Linux kernel. Its  is a software implementation of the memory tagging concept for the kernel. Software Tag-Based KASAN is available in 4.14, 4.19 and 5.4 Android kernels, and can be enabled with the CONFIG_KASAN_SW_TAGS kernel configuration option. Currently Tag-Based KASAN only supports tagging of slab memory; support for other types of memory (such as stack and globals) will be added in the future.\n\nCompared to , Tag-Based KASAN has significantly lower memory requirements (see  for details), which makes it usable on dog food testing devices. Another use case for Software Tag-Based KASAN is checking the existing kernel code for compatibility with memory tagging. As Tag-Based KASAN is based on similar concepts as the future in-kernel MTE support, making sure that kernel code works with Tag-Based KASAN will ease in-kernel MTE integration in the future.\n\nWe\u2019ve continued to expand the compiler mitigations that have been rolled out in   as well. This includes adding both integer and bounds sanitizers to some core libraries that were lacking them. For example, the libminikin fonts library and the libui rendering library are now bounds sanitized. We\u2019ve hardened the NFC stack by implementing both integer overflow sanitizer and bounds sanitizer in those components.\n\nIn addition to the hard mitigations like sanitizers, we also continue to expand our use of  as an exploit mitigation. CFI has been enabled in Android\u2019s , , and more of our core javascript libraries like libv8 and the PacProcessor.", "date": "\nJune 30, 2020\n"},
{"website": "Google-Security", "title": "\n11 Weeks of Android: Privacy and Security\n", "author": ["Posted by Charmaine D'Silva, Product Lead, Android Privacy and Framework, Narayan Kamath, Engineering Lead, Android Privacy and Framework, Stephan Somogyi, Product Lead, Android Security and Sudhi Herle, Engineering Lead, Android Security "], "link": "https://security.googleblog.com/2020/06/11-weeks-of-android-privacy-and-security_29.html", "abstract": "Privacy and security is core to how we design Android, and with every new release we increase our investment in this space. Android 11 continues to make important strides in these areas, and this week we\u2019ll be sharing a series of updates and resources about Android privacy and security. But first, let\u2019s take a quick look at some of the most important changes we\u2019ve made in Android 11 to protect user privacy and make the platform more secure.\n\nAs shared in the \u201c\u201d video, we\u2019re giving users even more control over sensitive permissions. Throughout the development of this release, we have engaged deeply and frequently with our developer community to design these features in a balanced way - amplifying user privacy while minimizing developer impact. Let\u2019s go over some of these features:\n\n: In Android 10, we introduced a granular location permission that allows users to limit access to location only when an app is in use (aka foreground only). When presented with the new runtime permissions options, users choose foreground only location more than 50% of the time. This demonstrated to us that users really wanted finer controls for permissions. So in Android 11, we\u2019ve introduced  that let users give an app access to the device microphone, camera, or location, just that one time. As an app developer, there are no changes that you need to make to your app for it to work with one time permissions, and the app can request permissions again the next time the app is used. Learn more about building privacy-friendly apps with these new changes .\n\n In Android 10 we added a background location usage reminder so users can see how apps are using this sensitive data on a regular basis. Users who interacted with the reminder either downgraded or denied the location permission over 75% of the time. In addition, we have done extensive research and believe that there are very few legitimate use cases for apps to require access to location in the background. \n\nIn Android 11, background location will no longer be a permission that a user can grant via a run time prompt and it will require a more deliberate action. If your app needs background location, the system will ensure that the app first asks for foreground location. The app can then broaden its access to background location through a separate permission request, which will cause the system to take the user to Settings in order to complete the permission grant.\n\nIn February, we  that Google Play developers will need to get approval to access background location in their app to prevent misuse. We're giving developers more time to make changes and won't be enforcing the policy for existing apps until 2021. Check out this helpful video .\n\nMost users tend to download and install over 60 apps on their device but interact with only a third of these apps on a regular basis. If users haven\u2019t used an app that targets Android 11 for an extended period of time, the system will \u201c\u201d all of the granted runtime permissions associated with the app and notify the user. The app can request the permissions again the next time the app is used. If you have an app that has a legitimate need to retain permissions, you can prompt users to turn this feature OFF for your app in Settings.\n\nAndroid encourages developers to limit their access to sensitive data, even if they have been granted permission to do so. In Android 11, developers will have access to  that will give them more transparency into their app\u2019s usage of private and protected data. The APIs will enable apps to track when the system records the app\u2019s access to private user data.\n\nIn Android 10, we introduced  which provides a filtered view into external storage, giving access to app-specific files and media collections. This change protects user privacy by limiting broad access to shared storage in many ways including changing the storage permission to only give read access to photos, videos and music and improving app storage attribution. Since Android 10, we\u2019ve incorporated developer feedback and made many improvements to help developers adopt scoped storage, including: updated permission UI to enhance user experience, direct file path access to media to improve compatibility with existing libraries, updated APIs for modifying media,  permission to enable select use cases that need broad files access, and protected external app directories. In Android 11, scoped storage will be mandatory for all apps that target API level 30. Learn more in this  and check out the  for further details. \n\nGoogle Play system updates were introduced with Android 10 as part of . Their main benefit is to increase the modularity and granularity of platform subsystems within Android so we can update core OS components without needing a full OTA update from your phone manufacturer. Earlier this year, thanks to Project Mainline, we were able to quickly fix a critical vulnerability in the media decoding subsystem. Android 11 adds new modules, and maintains the security properties of existing ones. For example, Conscrypt, which provides cryptographic primitives, maintained its FIPS validation in Android 11 as well.\n\n Developers can now use the  to specify the biometric authenticator strength required by their app to unlock or access sensitive parts of the app. We are planning to add this to the  to allow for backward compatibility and will share further updates on this work as it progresses. \n\nThis will unlock new use cases such as mobile drivers licences, National ID, and Digital ID. It\u2019s being built by our security team to ensure this information is stored safely, using security hardware to secure and control access to the data, in a way that  as compared to traditional physical documents. We\u2019re working with various government agencies and industry partners to make sure that Android 11 is ready for such digital-first identity experiences.\n\nThank you for your flexibility and feedback as we continue to build an increasingly more private and secure platform. You can learn about more features in the . You can also learn about general best practices related to  and .  \n\nPlease follow Android Developers on  and  to catch helpful content and materials in this area all this week.", "date": "\nJune 29, 2020\n"},
{"website": "Google-Security", "title": "\nMaking the Advanced Protection Program and Titan Security Keys easier to use on Apple iOS devices\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2020/06/making-advanced-protection-program-and.html", "abstract": "", "date": "\nJune 3, 2020\n"},
{"website": "Google-Security", "title": "\nThe Advanced Protection Program comes to Google Nest\n", "author": ["Posted by Shuvo Chatterjee, Product Manager, Advanced Protection Program"], "link": "https://security.googleblog.com/2020/06/the-advanced-protection-program-comes.html", "abstract": "", "date": "\nJune 1, 2020\n"},
{"website": "Google-Security", "title": "\nExpanding our work with the open source security community\n", "author": ["Posted by Eduardo Vela, Vulnerability Collector, Google\u00a0"], "link": "https://security.googleblog.com/2020/05/expanding-our-work-with-open-source.html", "abstract": "", "date": "\nMay 28, 2020\n"},
{"website": "Google-Security", "title": "\nEnhanced Safe Browsing Protection now available in Chrome\n", "author": ["Posted by Nathan Parker, Varun Khaneja, Eric Mill and Kiran C Nair - Chrome Safe Browsing team\n"], "link": "https://security.googleblog.com/2020/05/enhanced-safe-browsing-protection-now.html", "abstract": "Over the past few years we\u2019ve seen threats on the web becoming increasingly sophisticated. Phishing sites rotate domains very quickly to avoid being blocked, and malware campaigns are directly targeting at-risk users. We\u2019ve realized that to combat these most effectively, security cannot be one-size-fits-all anymore: That\u2019s why today we are announcing Enhanced Safe Browsing protection in Chrome, a new option for users who require or want a more advanced level of security while browsing the web.\n\nTurning on Enhanced Safe Browsing will substantially increase protection from dangerous websites and downloads. By sharing real-time data with Google Safe Browsing, Chrome can proactively protect you against dangerous sites. If you\u2019re signed in, Chrome and other Google apps you use (Gmail, Drive, etc) will be able to provide improved protection based on a holistic view of threats you encounter on the web and attacks against your Google Account. In other words, we\u2019re bringing the intelligence of Google\u2019s cutting-edge security tools directly into your browser.  \n\nOver the next year, we\u2019ll be adding even more protections to this mode, including tailored warnings for phishing sites and file downloads and cross-product alerts. \n\n\n\nSafe Browsing\u2019s blocklist API is an existing security protocol that protects billions of devices worldwide. Every day, Safe Browsing discovers thousands of new unsafe sites and adds them to the blocklist API that is shared with the web industry. Chrome checks the URL of each site you visit or file you download against a local list, which is updated approximately every 30 minutes. Increasingly, some sophisticated phishing sites slip through that 30-minute refresh window by switching domains very quickly. \n\nThis protocol is designed so that Google cannot determine the actual URL Chrome visited from this information, and thus by necessity the same verdict is returned regardless of the user\u2019s situation. This means Chrome can\u2019t adjust protection based on what kinds of threats a particular user is seeing or the type of sites they normally visit. So while the Safe Browsing blocklist API remains very powerful and will continue to protect users, we\u2019ve been looking for ways to provide more proactive and tailored protections. \n\n\n\nWhen you switch to Enhanced Safe Browsing, Chrome will share additional security data directly with Google Safe Browsing to enable more accurate threat assessments. For example, Chrome will check uncommon URLs in real time to detect whether the site you are about to visit may be a phishing site. Chrome will also send a small sample of pages and suspicious downloads to help discover new threats against you and other Chrome users.\n\nIf you are signed in to Chrome, this data is temporarily linked to your Google Account. We do this so that when an attack is detected against your browser or account, Safe Browsing can tailor its protections to your situation. In this way, we can provide the most precise protection without unnecessary warnings. After a short period, Safe Browsing anonymizes this data so it is no longer connected to your account.\n\nYou can opt in to this mode by visiting Privacy and Security settings > Security > and selecting the \u201cEnhanced protection\u201d mode under Safe Browsing. It will be rolled out gradually in M83 on desktop platforms, with Android support coming in a future release. Enterprise administrators can control this setting via the SafeBrowsingProtectionLevel policy. \n\n\n\nChrome\u2019s billions of users are incredibly diverse, with a full spectrum of needs and perspectives in security and privacy. We will continue to invest in both Standard and Enhanced Safe Browsing with the goal to expand Chrome\u2019s security offerings to cover all users.", "date": "\nMay 19, 2020\n"},
{"website": "Google-Security", "title": "\n Introducing portability of Google Authenticator 2SV codes across Android devices\n", "author": ["Posted by Dongjing He, Software Engineer; Teddy Katz, Software Engineer; Christiaan Brand, Product Manager"], "link": "https://security.googleblog.com/2020/05/introducing-portability-of-google.html", "abstract": "", "date": "\nMay 7, 2020\n"},
{"website": "Google-Security", "title": "\nResearch Grants to support Google VRP Bug Hunters during COVID-19\n", "author": ["Posted by Anna Hupa, Senior Strategist, Trust & Safety at Google"], "link": "https://security.googleblog.com/2020/04/research-grants-to-support-google-vrp_20.html", "abstract": "", "date": "\nApril 20, 2020\n"},
{"website": "Google-Security", "title": "\nIntroducing our new book \u201cBuilding Secure and Reliable Systems\u201d\n", "author": ["Posted by Royal Hansen, VP of Security Engineering, Google"], "link": "https://security.googleblog.com/2020/04/introducing-our-new-book-building.html", "abstract": "", "date": "\nApril 8, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing our first GCP VRP Prize winner and updates to 2020 program\n", "author": ["Posted by Harshvardan Sharma, Information Security Engineer, Google"], "link": "https://security.googleblog.com/2020/03/announcing-our-first-gcp-vrp-prize.html", "abstract": "", "date": "\nMarch 11, 2020\n"},
{"website": "Google-Security", "title": "\nHow Google Play Protect kept users safe in 2019\n", "author": ["Posted by Rahul Mishra, Program Manager, Android Security and Privacy Team"], "link": "https://security.googleblog.com/2020/03/how-google-play-protect-kept-users-safe.html", "abstract": "", "date": "\nMarch 10, 2020\n"},
{"website": "Google-Security", "title": "\nHow Google does certificate lifecycle management\n", "author": ["Posted by Siddharth Bhai and Ryan Hurst, Product Managers, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2020/03/how-google-does-certificate-lifecycle.html", "abstract": "", "date": "\nMarch 10, 2020\n"},
{"website": "Google-Security", "title": "\nFuzzBench: Fuzzer Benchmarking as a Service\n", "author": ["Posted by Jonathan Metzman, Abhishek Arya, Google OSS-Fuzz Team and L\u00e1szl\u00f3 Szekeres\u200e, Google Software Analysis Team"], "link": "https://security.googleblog.com/2020/03/fuzzbench-fuzzer-benchmarking-as-service.html", "abstract": "", "date": "\nMarch 2, 2020\n"},
{"website": "Google-Security", "title": "\nHelping Developers with Permission Requests\n", "author": ["Posted by Sai Teja Peddinti, Nina Taft and Igor Bilogrevic from PDPO Applied Privacy Research, and Pauline Anthonysamy from Android Security and Privacy."], "link": "https://security.googleblog.com/2020/02/helping-developers-with-permission.html", "abstract": "", "date": "\nFebruary 27, 2020\n"},
{"website": "Google-Security", "title": "\nData Encryption on Android with Jetpack Security\n", "author": [], "link": "https://security.googleblog.com/2020/02/data-encryption-on-android-with-jetpack.html", "abstract": "Have you ever tried to encrypt data in your app?  As a developer, you want to keep data safe, and in the hands of the party intended to use. But if you\u2019re like most Android developers, you don\u2019t have a dedicated security team to help encrypt your app\u2019s data properly. By searching the web to learn how to encrypt data, you might get answers that are several years out of date and provide incorrect examples.\n\nThe  (JetSec) crypto library provides abstractions for encrypting Files and SharedPreferences objects. The library promotes the use of the  while using safe and well-known . Using EncryptedFile and EncryptedSharedPreferences allows you to locally protect files that may contain sensitive data, API keys, OAuth tokens, and other types of secrets. \n\nWhy would you want to encrypt data in your app? Doesn\u2019t Android, since 5.0,  by default? It certainly does, but there are some use cases where you may want an extra level of protection. If your app uses , you should encrypt the data. In the app home directory, your app should encrypt data if your app handles sensitive information including but not limited to personally identifiable information (PII), health records, financial details, or enterprise data. When possible, we recommend that you tie this information to biometrics for an extra level of protection. \n\nJetpack Security is based on , an open-source, cross-platform security project from Google. Tink might be appropriate if you need general encryption, hybrid encryption, or something similar. Jetpack Security data structures are fully compatible with Tink.\n\nBefore we jump into encrypting your data, it\u2019s important to understand how your encryption keys will be kept safe. Jetpack Security uses a , which encrypts all subkeys that are used for each cryptographic operation. JetSec provides a recommended default master key in the MasterKeys class. This class uses a basic AES256-GCM key which is generated and stored in the AndroidKeyStore. The AndroidKeyStore is a container which stores cryptographic keys in the TEE or StrongBox, making them hard to extract. Subkeys are stored in a configurable  SharedPreferences object. \n\nPrimarily, we use the AES256_GCM_SPEC specification in Jetpack Security, which is recommended for general use cases. AES256-GCM is symmetric and generally fast on modern devices. \n\nFor apps that require more configuration, or handle very sensitive data, it\u2019s recommended to build your , choosing options that make sense for your use. Time-bound keys with  can provide an extra level of protection against rooted or compromised devices.\n\nImportant options:\n\n If your app needs to encrypt data in the background, you should not use time-bound keys or require that the device is unlocked, as you will not be able to accomplish this without a user present.\n\nYou must use BiometricPrompt to authorize the device if your key was created with the following options:\n\nAfter the user authenticates, the keys are unlocked for the amount of time set in the validity seconds field. The AndroidKeystore does not have an API to query key settings, so your app  must keep track of these settings. You should build your BiometricPrompt instance in the  method of the activity where you present the dialog to the user.\n\nBiometricPrompt code to unlock time-bound keys\n\nJetpack Security includes an EncryptedFile class, which removes the challenges of encrypting file data. Similar to File, EncryptedFile provides a FileInputStream object for reading and a FileOutputStream object for writing. Files are encrypted using . The data is divided into chunks and encrypted using AES256-GCM in such a way that it's not possible to reorder.\n\nIf your application needs to save Key-value pairs - such as API keys - JetSec provides the EncryptedSharedPreferences class, which uses the same SharedPreferences interface that you\u2019re used to.\n\nBoth keys and values are encrypted. Keys are encrypted using , which provides a deterministic cipher text; values are encrypted with AES256-GCM and are bound to the encrypted key. This scheme allows the key data to be encrypted safely, while still allowing lookups.\n\n is a sample app on the Android Security GitHub samples page. It\u2019s a great example of how to use File encryption using Jetpack Security.\n\nHappy Encrypting!", "date": "\nFebruary 25, 2020\n"},
{"website": "Google-Security", "title": "\nImproving Malicious Document Detection in Gmail with Deep Learning\n", "author": ["Posted by Elie Bursztein, Security & Anti-Abuse Research Lead; David Tao, Software Engineer; Neil Kumaran, Product Manager, Gmail Security\u00a0"], "link": "https://security.googleblog.com/2020/02/improving-malicious-document-detection.html", "abstract": "", "date": "\nFebruary 25, 2020\n"},
{"website": "Google-Security", "title": "\nDisruptive ads enforcement and our new approach\n", "author": ["Posted by Per Bjorke, Senior Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2020/02/disruptive-ads-enforcement-and-our-new.html", "abstract": "", "date": "\nFebruary 20, 2020\n"},
{"website": "Google-Security", "title": "\nTitan Security Keys - now available in Austria, Canada, France, Germany, Italy, Japan, Spain, Switzerland, and the UK \n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2020/02/titan-security-keys-now-available-in.html", "abstract": "", "date": "\nFebruary 18, 2020\n"},
{"website": "Google-Security", "title": "\nHow we fought bad apps and malicious developers in 2019\n", "author": [], "link": "https://security.googleblog.com/2020/02/how-we-fought-bad-apps-and-malicious.html", "abstract": "", "date": "\nFebruary 11, 2020\n"},
{"website": "Google-Security", "title": "\nProtecting users from insecure downloads in Google Chrome\n", "author": ["Posted by Joe DeBlasio, Chrome security team"], "link": "https://security.googleblog.com/2020/02/protecting-users-from-insecure_6.html", "abstract": "", "date": "\nFebruary 6, 2020\n"},
{"website": "Google-Security", "title": "\nSay hello to OpenSK: a fully open-source security key implementation\n", "author": ["Posted by Elie Bursztein, Security & Anti-abuse Research Lead, and Jean-Michel Picod, Software Engineer, Google\u00a0"], "link": "https://security.googleblog.com/2020/01/say-hello-to-opensk-fully-open-source.html", "abstract": "", "date": "\nJanuary 30, 2020\n"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2019 Year in Review\n", "author": ["Posted by Natasha Pabrai, Jan Keller, Jessica Lin, Anna Hupa, and Adam Bacchus, Vulnerability Reward Programs at Google"], "link": "https://security.googleblog.com/2020/01/vulnerability-reward-program-2019-year.html", "abstract": "", "date": "\nJanuary 28, 2020\n"},
{"website": "Google-Security", "title": "\nHave an iPhone? Use it to protect your Google Account with the Advanced Protection Program\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud and Kaiyu Yan, Software Engineer, Google"], "link": "https://security.googleblog.com/2020/01/have-iphone-use-it-to-protect-your.html", "abstract": "", "date": "\nJanuary 15, 2020\n"},
{"website": "Google-Security", "title": "\nSecuring open-source: how Google supports the new Kubernetes bug bounty\n", "author": ["Posted by Maya Kaczorowski, Product Manager, Container Security and Aaron Small, Product Manager, GKE On-Prem Security"], "link": "https://security.googleblog.com/2020/01/securing-open-source-how-google.html", "abstract": "", "date": "\nJanuary 14, 2020\n"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Bread (and Friends)\n", "author": ["Posted by Alec Guertin and Vadim Kotov, Android Security & Privacy Team \n", "\nIn this edition of our ", " series we introduce Bread, a large-scale billing fraud family. We first started tracking Bread (also known as Joker) in early 2017, identifying apps designed solely for ", ". As the Play Store has introduced new policies and Google Play Protect has scaled defenses, Bread apps were forced to continually iterate to search for gaps. They have at some point used just about every cloaking and obfuscation technique under the sun in an attempt to go undetected. Many of these samples appear to be designed specifically to attempt to slip into the Play Store undetected and are not seen elsewhere. In this post, we show how Google Play Protect has defended against a well organized, persistent attacker and share examples of their techniques.\n", "\n\n", "\nTL;DR", "\n", "\n", "\n", "\n", "\n", "\n", "\nBILLING FRAUD", "\nBread apps typically fall into two categories: SMS fraud (older versions) and toll fraud (newer versions). Both of these types of fraud take advantage of mobile billing techniques involving the user\u2019s carrier.\n", "\n\n", "\nCarriers may partner with vendors to allow users to pay for services by SMS. The user simply needs to text a prescribed keyword to a prescribed number (shortcode). A charge is then added to the user\u2019s bill with their mobile service provider.\n", "\n\n", "\n", "\nCarriers may also provide payment endpoints over a web page. The user visits the URL to complete the payment and enters their phone number. Verification that the request is coming from the user\u2019s device is completed using two possible methods:\n", "\n\n", "\n", "\nBoth of the billing methods detailed above provide device verification, but not user verification. The carrier can determine that the request originates from the user\u2019s device, but does not require any interaction from the user that cannot be automated. Malware authors use injected clicks, custom HTML parsers and SMS receivers to automate the billing process without requiring any interaction from the user.\n", "\n\n", "\nBread apps have used many innovative and classic techniques to hide strings from analysis engines. Here are some highlights.\n", "\n\n", "\nFrequently, Bread apps take advantage of standard crypto libraries in `java.util.crypto`. We have discovered apps using AES, Blowfish, and DES as well as combinations of these to encrypt their strings.\n", "\n\n", "\nOther variants have used custom-implemented encryption algorithms. Some common techniques include: basic XOR encryption, nested XOR and custom key-derivation methods. Some variants have gone so far as to use a different key for the strings of each class.\n", "\n\n", "\nEncrypted strings can be a signal that the code is trying to hide something. Bread has used a few tricks to keep strings in plaintext while preventing basic string matching.\n", "\n\n\n\n\n", "\nGoing one step further, these substrings are sometimes scattered throughout the code, retrieved from static variables and method calls. Various versions may also change the index of the split (e.g. \u201c.clic\u201d and \u201ck();\u201d).\n", "\n\n", "\nAnother technique to obfuscate unencrypted strings uses repeated delimiters. A short, constant string of characters is inserted at strategic points to break up keywords:\n", "\n\n\n\n\n", "\nAt runtime, the delimiter is removed before using the string:\n", "\n\n\n\n\n", "\n", "\nSMS and toll fraud generally requires a few basic behaviors (for example, disabling WiFi or accessing SMS), which are accessible by a handful of APIs. Given that there are a limited number of behaviors required to identify billing fraud, Bread apps have had to try a wide variety of techniques to mask usage of these APIs.\n", "\n\n", "\nMost methods for hiding API usage tend to use Java reflection in some way. In some samples, Bread has simply directly called the Reflect API on strings decrypted at runtime.\n", "\n\n\n\n\n", "\n", "\nBread has also tested our ability to analyze native code. In one sample, no SMS-related code appears in the DEX file, but there is a native method registered.\n", "\n\n\n\n\n", "\nTwo strings are passed into the call, the shortcode and keyword used for SMS billing (getter methods renamed here for clarity).\n", "\n\n\n\n\n", "\nIn the native library, it stores the strings to access the SMS API.\n", "\n", "\nThe ", " method uses the Java Native Interface (JNI) to fetch and call the Android SMS API. The following is a screenshot from IDA with comments showing the strings and JNI functions.\n", "\n\n", "\n", "\nContinuing on the theme of cross-language bridges, Bread has also tried out some obfuscation methods utilizing JavaScript in WebViews. The following method is declared in the DEX.\n", "\n\n\n\n\n", "\nWithout context, this method does not reveal much about its intended behavior, and there are no calls made to it anywhere in the DEX. However, the app does create a WebView and registers a JavaScript interface to this class.\n", "\n\n\n\n\n", "\nThis gives JavaScript run in the WebView access to this method. The app loads a URL pointing to a Bread-controlled server. The response contains some basic HTML and JavaScript.\n", "\n", "\nIn green, we can see the references to the SMS API. In red, we see those values being passed into the suspicious Java method through the registered interface. Now, using these strings ", " can use reflection to call ", " and process the payment.\n", "\n\n", "\nIn addition to implementing custom obfuscation techniques, apps have used several commercially available packers including: Qihoo360, AliProtect and SecShell.\n", "\n\nMore recently, we have seen Bread-related apps trying to hide malicious code in a native library shipped with the APK. Earlier this year, we discovered apps hiding a JAR in the data section of an ELF file which it then dynamically loads using ", ".\n", "\n\nThe figure below shows a fragment of encrypted JAR stored in .rodata section of a shared object shipped with the APK as well as the XOR key used for decryption.\n", "\n\n", "\nAfter we blocked those samples, they moved a significant portion of malicious functionality into the native library, which resulted in a rather peculiar back and forth between Dalvik and native code:\n", "\n\n", "\n", "\n", "\nEarly versions of Bread utilized a basic command and control infrastructure to dynamically deliver content and retrieve billing details. In the example server response below, the green fields show text to be shown to the user. The red fields are used as the shortcode and keyword for SMS billing.\n", "\n\n", "\n", "\nSince various carriers implement the billing process differently, Bread has developed several variants containing generalized state machines implementing all possible steps. At runtime, the apps can check which carrier the device is connected to and fetch a configuration object from the command and control server. The configuration contains a list of steps to execute with URLs and JavaScript.\n", "\n\n\n\n\n", "\nThe steps implemented include:\n", "\n\n", "\n", "\nOne of the more interesting states implements the ability to solve basic captchas (obscured letters and numbers). First, the app creates a JavaScript function to call a Java method, ", ", exposed to WebView using ", ".\n", "\n\n", "\nThe value used to replace ", " comes from the JSON configuration.\n", "\n\n\n\n\n", "\nThe app then uses JavaScript injection to create a new script in the carrier\u2019s web page to run the new function.\n", "\n\n", "\nThe base64-encoded image is then uploaded to an image recognition service. If the text is retrieved successfully, the app uses JavaScript injection again to submit the HTML form with the captcha answer.\n", "\n\n", "\n", "\nIn our basic command & control example above, we didn\u2019t address the (incorrectly labeled) \u201cimei\u201d field.\n", "\n\n\n\n\n", "\nThis contains the Mobile Country Code (MCC) and Mobile Network Code (MNC) values that the billing process will work for. In this example, the server response contains several values for Thai carriers. The app checks if the device\u2019s network matches one of those provided by the server. If it does, it will commence with the billing process. If the value does not match, the app skips the \u201cdisclosure\u201d page and billing process and brings the user straight to the app content.\n", "\n\nIn some versions, the server would only return valid responses several days after the apps were submitted.\n", "\n\n", "\nIn the JavaScript bridge API obfuscation example covered above, the server supplied the app with the necessary strings to complete the billing process. However, analysts may not always see the indicators of compromise in the server\u2019s response.\n", "\n\nIn this example, the requests to the server take the following form:\n", "\n\n\n\n\n", "\nHere, the \u201coperator\u201d query parameter is the Mobile Country Code and Mobile Network Code . The server can use this information to determine if the user\u2019s carrier is one of Bread\u2019s targets. If not, the response is scrubbed of the strings used to complete the billing fraud.\n", "\n\n\n\n\n", "\n", "\nBread apps sometimes display a pop-up to the user that implies some form of compliance or disclosure, showing ", " or a ", " button. However, the actual text would often only display a basic welcome message.\n", "\n\n", "\n", "\n", "\n\nOther versions included all the pieces needed for a valid disclosure message.\n", "\n\n", "\nWhen translated the disclosure reads:\n", "\n\n", "\n", "\n\nHowever, there are still two issues here:\n", "\n\n", "\nEven if the disclosure here displayed accurate information, the user would often find that the advertised functionality of the app did not match the actual content. Bread apps frequently contain no functionality beyond the billing process or simply clone content from other popular apps.\n", "\n\n", "\nBread has also leveraged an abuse tactic unique to app stores: versioning. Some apps have started with clean versions, in an attempt to grow user bases and build the developer accounts\u2019 reputations. Only later is the malicious code introduced, through an update. Interestingly, early \u201cclean\u201d versions contain varying levels of signals that the updates will include malicious code later. Some are first uploaded with all the necessary code except the one line that actually initializes the billing process. Others may have the necessary permissions, but are missing the classes containing the fraud code. And others have all malicious content removed, except for log comments referencing the payment process. All of these methods attempt to space out the introduction of possible signals in various stages, testing for gaps in the publication process. However, GPP does not treat new apps and updates any differently from an analysis perspective.\n", "\n\n", "\nWhen early versions of apps are first published, many five star reviews appear with comments like:\n", "\n", "\n\n"], "link": "https://security.googleblog.com/2020/01/pha-family-highlights-bread-and-friends.html", "abstract": "", "date": "\nJanuary 9, 2020\n"},
{"website": "Google-Security", "title": "\nAnnouncing updates to our Patch Rewards program in 2020 \n", "author": ["Posted by Jan Keller, Technical Program Manager, Security\u00a0"], "link": "https://security.googleblog.com/2019/12/announcing-updates-to-our-patch-rewards.html", "abstract": "", "date": "\nDecember 18, 2019\n"},
{"website": "Google-Security", "title": "\nProtecting programmatic access to user data with Binary Authorization for Borg\n", "author": ["Posted by Daniel Rebolledo Samper and Mark Lodato, Software Engineers, Security & Privacy"], "link": "https://security.googleblog.com/2019/12/protecting-programmatic-access-to-user.html", "abstract": "", "date": "\nDecember 17, 2019\n"},
{"website": "Google-Security", "title": "\nBetter password protections in Chrome - How it works\n", "author": ["Posted by Patrick Nepper, Kiran C. Nair, Vasilii Sukhanov and Varun Khaneja, Chrome Team"], "link": "https://security.googleblog.com/2019/12/better-password-protections-in-chrome.html", "abstract": "", "date": "\nDecember 10, 2019\n"},
{"website": "Google-Security", "title": "\nDetecting unsafe path access patterns with PathAuditor\n", "author": ["Posted by Marta Ro", "ek, Google Summer Intern 2019, and Stephen R", "ttger, Software Engineer\u00a0"], "link": "https://security.googleblog.com/2019/12/detecting-unsafe-path-access-patterns.html", "abstract": "", "date": "\nDecember 9, 2019\n"},
{"website": "Google-Security", "title": "\n An Update on Android TLS Adoption\n", "author": [], "link": "https://security.googleblog.com/2019/12/an-update-on-android-tls-adoption.html", "abstract": "Android is committed to keeping users, their devices, and their data safe. One of the ways that we keep data safe is by protecting network traffic that enters or leaves an Android device with Transport Layer Security (TLS). \n\nAndroid 7 (API level 24)  the  in 2016, allowing app developers to configure the network security policy for their app through a declarative configuration file. To ensure apps are safe, apps targeting Android 9 (API level 28) or higher automatically have a  set by default that prevents unencrypted traffic for every domain.\n\nToday, we\u2019re happy to announce that 80% of Android apps are encrypting traffic by default. The percentage is even greater for apps targeting Android 9 and higher, with 90% of them encrypting traffic by default.\n\nPercentage of apps that block cleartext by default.\n\nSince November 1 2019, . As a result, we expect these numbers to continue improving. Network traffic from these apps is secure by default and any use of unencrypted connections is the result of an explicit choice by the developer.\n\nThe latest releases of Android Studio and Google Play\u2019s  warn developers when their app includes a potentially insecure Network Security Configuration (for example, when they allow unencrypted traffic for all domains or when they accept user provided certificates outside of debug mode). This encourages the adoption of HTTPS across the Android ecosystem and ensures that developers are aware of their security configuration.\n\nExample of a warning shown to developers in Android Studio.\n\nExample of a warning shown to developers as part of the .\n\nFor apps targeting Android 9 and higher, the out-of-the-box default is to encrypt all network traffic in transit and trust only certificates issued by an authority in the standard Android CA set without requiring any extra configuration. Apps can provide an exception to this only by including a separate Network Security Config file with carefully selected exceptions.\n\nIf your app needs to allow traffic to certain domains, it can do so by including a Network Security Config file that only includes these exceptions to the default secure policy. \n\nIf your app needs to be able to accept user specified certificates for testing purposes (for example, connecting to a local server during testing), make sure to wrap your  element inside a  element. This ensures the connections in the production version of your app are secure.\n\nIf your library directly creates secure/insecure connections, make sure that it honors the app's cleartext settings by checking   opening any cleartext connection.\n\n\nAndroid\u2019s  and other popular HTTP libraries such as  or  have built-in Network Security Config support.", "date": "\nDecember 3, 2019\n"},
{"website": "Google-Security", "title": "\n Expanding the Android Security Rewards Program\n", "author": ["Posted by Jessica Lin, Android Security Team"], "link": "https://security.googleblog.com/2019/11/expanding-android-security-rewards.html", "abstract": "The  (ASR) program was created in 2015 to reward researchers who find and report security issues to help keep the Android ecosystem safe. Over the past 4 years, we have awarded over 1,800 reports, and paid out over four million dollars.\n\nToday, we\u2019re expanding the program and increasing reward amounts. We are introducing a top prize of $1 million for a full chain remote code execution exploit with persistence which compromises the Titan M secure element on Pixel devices. Additionally, we will be launching a specific program offering a 50% bonus for exploits found on specific developer preview versions of Android, meaning our top prize is now $1.5 million.\n\nAs mentioned in a  in 2019 Gartner rated the Pixel 3 with Titan M as having the most \u201cstrong\u201d ratings in the built-in security section out of all devices evaluated. This is why we\u2019ve created a dedicated prize to reward researchers for exploits found to circumvent the secure elements protections.\n\nIn addition to exploits involving Pixel Titan M, we have added other categories of exploits to the rewards program, such as those involving data exfiltration and lockscreen bypass. These rewards go up to $500,000 depending on the exploit category. For full details, please refer to the . \n\nNow that we\u2019ve covered some of what\u2019s new, let\u2019s take a look back at some milestones from this year. Here are some highlights from 2019:\n\nThe highest reward paid out to a member of the research community was for a report from Guang Gong () of Alpha Lab, Qihoo 360 Technology Co. Ltd. This report detailed the first reported 1-click remote code execution exploit chain on the Pixel 3 device. Guang Gong was awarded $161,337 from the  and $40,000 by  for a total of $201,337. The $201,337 combined reward is also the highest reward for a single exploit chain across all Google VRP programs. The Chrome vulnerabilities leveraged in this report were fixed in  and released in September, protecting users against this exploit chain. \n\nWe\u2019d like to thank all of our researchers for contributing to the security of the Android ecosystem. If you\u2019re interested in becoming a researcher, check out our  for information on how to get started. \n\nStarting today November 21, 2019 the new rewards take effect. Any reports that were submitted before November 21, 2019 will be rewarded based on the previously existing rewards table. \n\nHappy bug hunting!", "date": "\nNovember 21, 2019\n"},
{"website": "Google-Security", "title": "\nUsing a built-in FIDO authenticator on latest-generation Chromebooks \n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2019/11/using-built-in-fido-authenticator-on.html", "abstract": "", "date": "\nNovember 19, 2019\n"},
{"website": "Google-Security", "title": "\nGWP-ASan: Sampling heap memory error detection in-the-wild\n", "author": ["Posted by Vlad Tsyrklevich, Dynamic Tools Team"], "link": "https://security.googleblog.com/2019/11/gwp-asan-sampling-heap-memory-error.html", "abstract": "Memory safety errors, like use-after-frees and out-of-bounds reads/writes, are a leading source of vulnerabilities in C/C++ applications. Despite investments in preventing and detecting these errors in Chrome, over 60% of high severity vulnerabilities in Chrome are memory safety errors. Some memory safety errors don\u2019t lead to security vulnerabilities but simply cause crashes and instability.\n\nChrome uses state-of-the-art techniques to prevent these errors, including:\n\nChrome also makes use of sandboxing and exploit mitigations to complicate exploitation of memory errors that go undetected by the methods above.\n\nAddressSanitizer is a compiler instrumentation that finds memory errors occurring on the heap, stack, or in globals. ASan is highly effective and one of the lowest overhead instrumentations available that detects the errors that it does; however, it still incurs an average 2-3x performance and memory overhead. This makes it suitable for use with unit tests or fuzzing, but not deployment to end users. Chrome used to deploy  to detect memory errors. SyzyASAN had a similar overhead so it was only deployed to a small subset of users on the canary channel. It was discontinued after the Windows toolchain switched to LLVM.\n\nGWP-ASan, also known by its recursive backronym, GWP-ASan Will Provide Allocation Sanity, is a sampling allocation tool designed to detect heap memory errors occurring in production with negligible overhead. Because of its negligible overhead we can deploy GWP-ASan to the entire Chrome user base to find memory errors happening in the real world that are not caught by fuzzing or testing with ASan. Unlike ASan, GWP-ASan can not find memory errors on the stack or in globals.\n\nGWP-ASan is currently enabled for all Windows and macOS users for allocations made using malloc() and PartitionAlloc. It is only enabled for a small fraction of allocations and processes to reduce performance and memory overhead to a negligible amount. At the time of writing it has found  (many are still restricted view). About 90% of the issues GWP-ASan has found are use-after-frees. The remaining are out-of-bounds reads and writes.\n\nTo learn more, check out our full write up on GWP-ASan .", "date": "\nNovember 7, 2019\n"},
{"website": "Google-Security", "title": "\nThe App Defense Alliance: Bringing the security industry together to fight bad apps\n", "author": ["Posted by Dave Kleidermacher, VP, Android Security & Privacy "], "link": "https://security.googleblog.com/2019/11/the-app-defense-alliance-bringing.html", "abstract": "", "date": "\nNovember 6, 2019\n"},
{"website": "Google-Security", "title": "\nOpenTitan - open sourcing transparent, trustworthy, and secure silicon\n", "author": ["Posted by Royal Hansen, Vice President, Google and Dominic Rizzo, OpenTitan Lead, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2019/11/opentitan-open-sourcing-transparent.html", "abstract": "", "date": "\nNovember 5, 2019\n"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 4 (services)\n", "author": ["Posted by Guilherme Gon\u00e7alves, Site Reliability Engineer and Kyle O'Malley, Security Engineer\u00a0"], "link": "https://security.googleblog.com/2019/10/how-google-adopted-beyondcorp-part-4.html", "abstract": "", "date": "\nOctober 31, 2019\n"},
{"website": "Google-Security", "title": "\nProtecting against code reuse in the Linux kernel with Shadow Call Stack\n", "author": ["Posted by Sami Tolvanen, Staff Software Engineer, Android Security & Privacy Team\n"], "link": "https://security.googleblog.com/2019/10/protecting-against-code-reuse-in-linux_30.html", "abstract": "By default, shadow stacks are not virtually allocated to minimize memory overhead, but CONFIG_SHADOW_CALL_STACK_VMAP can be enabled for better stack exhaustion protection. With CONFIG_DEBUG_STACK_USAGE, the kernel will also print out shadow stack usage in addition to normal stack usage which can be helpful when debugging issues.", "date": "\nOctober 30, 2019\n"},
{"website": "Google-Security", "title": "\nImproving Site Isolation for Stronger Browser Security\n", "author": ["Posted by Charlie Reis, Site Isolator "], "link": "https://security.googleblog.com/2019/10/improving-site-isolation-for-stronger.html", "abstract": "The Chrome Security team values having multiple lines of defense. Web browsers are complex, and malicious web pages may try to find and exploit browser bugs to steal data. Additional lines of defense, like , make it harder for attackers to access your computer, even if bugs in the browser are exploited. With , Chrome has gained a new line of defense that helps protect your accounts on the Web as well.\n\nSite Isolation ensures that pages from different sites end up in different sandboxed processes in the browser. Chrome can thus limit the entire process to accessing data from only one site, making it harder for an attacker to steal cross-site data. We started isolating all sites , and now we\u2019re excited to enable it on Android for sites that users log into . We've also strengthened Site Isolation on desktop to help defend against even fully compromised processes.\n\nSite Isolation helps defend against two types of threats. First, attackers may try to use advanced \"side channel\" attacks to leak sensitive data from a process through unexpected means. For example,  attacks take advantage of CPU performance features to access data that should be off limits. With Site Isolation, it is harder for the attacker to get cross-site data into their process in the first place.\n\nSecond, even more powerful attackers may discover security bugs in the browser, allowing them to completely hijack the sandboxed process. On desktop platforms, Site Isolation can now catch these misbehaving processes and limit their access to cross-site data. We're working to bring this level of hijacked process protection to Android in the future as well.\n\nThanks to this extra line of defense, Chrome can now help keep your web accounts even more secure. We are still making improvements to get the full benefits of Site Isolation, but this change gives Chrome a solid foundation for protecting your data.\n\nIf you\u2019d like to learn more, check out our  on the Chromium blog.", "date": "\nOctober 17, 2019\n"},
{"website": "Google-Security", "title": "\nUSB-C Titan Security Keys - available tomorrow in the US\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2019/10/usb-c-titan-security-keys-available.html", "abstract": "", "date": "\nOctober 14, 2019\n"},
{"website": "Google-Security", "title": "\nNo More Mixed Messages About HTTPS\n", "author": ["Posted by Emily Stark and Carlos Joan Rafael Ibarra Lopez, Chrome security team"], "link": "https://security.googleblog.com/2019/10/no-more-mixed-messages-about-https_3.html", "abstract": "", "date": "\nOctober 3, 2019\n"},
{"website": "Google-Security", "title": "\nChrome UI for Deprecating Legacy TLS Versions\n", "author": ["Posted by Chris Thompson, Chrome security team"], "link": "https://security.googleblog.com/2019/10/chrome-ui-for-deprecating-legacy-tls.html", "abstract": "", "date": "\nOctober 1, 2019\n"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 3 (tiered access)\n", "author": ["Posted by Daniel Ladenheim, Software Engineer, and Hunter King, Security Engineer\u00a0"], "link": "https://security.googleblog.com/2019/09/how-google-adopted-beyondcorp-part-3.html", "abstract": "", "date": "\nSeptember 17, 2019\n"},
{"website": "Google-Security", "title": "\n Trust but verify attestation with revocation\n", "author": [], "link": "https://security.googleblog.com/2019/09/trust-but-verify-attestation-with.html", "abstract": "", "date": "\nSeptember 6, 2019\n"},
{"website": "Google-Security", "title": "\nExpanding bug bounties on Google Play\n", "author": [], "link": "https://security.googleblog.com/2019/08/expanding-bug-bounties-on-google-play.html", "abstract": "We\u2019re constantly looking for ways to further improve the security and privacy of our products, and the ecosystems they support. At Google, we understand the strength of open platforms and ecosystems, and that the best ideas don\u2019t always come from within. It is for this reason that we offer a broad range of vulnerability reward programs, encouraging the community to help us improve security for everyone. Today, we\u2019re expanding on those efforts with some big changes to, as well as the launch of the new.\n\nWe are increasing the scope of GPSRP to include all apps in Google Play with 100 million or more installs. These apps are now eligible for rewards, even if the app developers don\u2019t have their own vulnerability disclosure or bug bounty program. In these scenarios, Google helps responsibly disclose identified vulnerabilities to the affected app developer. This opens the door for security researchers to help hundreds of organizations identify and fix vulnerabilities in their apps. If the developers already have their own programs, researchers can collect rewards directly from them on top of the rewards from Google. We encourage app developers to start their own vulnerability disclosure or bug bounty program to work directly with the security researcher community.\n\nVulnerability data from GPSRP helps Google create automated checks that scan all apps available in Google Play for similar vulnerabilities. Affected app developers are notified through the Play Console as part of the program, which provides information on the vulnerability and how to fix it. Over its lifetime, ASI has helped more than 300,000 developers fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users until the issue is fixed.\n\nTo date, GPSRP has paid out over $265,000 in bounties. Recent scope and have resulted in $75,500 in rewards across July & August alone. With these changes, we anticipate even further engagement from the security research community to bolster the success of the program.\n\nToday, we are also launching the. DDPRP is a bounty program, in collaboration with HackerOne, meant to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions. It the contributions of individuals who help report apps that are violating Google Play, Google API, or Google Chrome Web Store Extensions program policies.\n\nThe program aims to reward anyone who can provide verifiably and unambiguous evidence of data abuse, in a similar model as Google\u2019s other vulnerability reward programs. In particular, the program aims to identify situations where user data is being used or sold unexpectedly, or repurposed in an illegitimate way without user consent. If data abuse is identified related to an app or Chrome extension, that app or extension will accordingly be removed from Google Play or Google Chrome Web Store. In the case of an app developer abusing access to Gmail restricted scopes, their API access will be removed. While no reward table or maximum reward is listed at this time, depending on impact, a single report could net as large as a $50,000 bounty.\n\nAs 2019 continues, we look forward to seeing what researchers find next. Thank you to the entire community for contributing to keeping our platforms and ecosystems safe. Happy bug hunting!", "date": "\nAugust 29, 2019\n"},
{"website": "Google-Security", "title": "\nProtecting Chrome users in Kazakhstan\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2019/08/protecting-chrome-users-in-kazakhstan.html", "abstract": "", "date": "\nAugust 21, 2019\n"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp: Part 2 (devices)\n", "author": ["Posted by Matt McDonald, Software Engineer, and Sebastian Harl, Software Engineer\u00a0"], "link": "https://security.googleblog.com/2019/08/how-google-adopted-beyondcorp-part-2.html", "abstract": "", "date": "\nAugust 20, 2019\n"},
{"website": "Google-Security", "title": "\nNew Research: Lessons from Password Checkup in action\n", "author": ["Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Spam and Abuse research"], "link": "https://security.googleblog.com/2019/08/new-research-lessons-from-password.html", "abstract": "", "date": "\nAugust 15, 2019\n"},
{"website": "Google-Security", "title": "\nMaking authentication even easier with FIDO2-based local user verification for Google Accounts\n", "author": ["Posted by Dongjing He, Software Engineer and Christiaan Brand, Product Manager\u00a0"], "link": "https://security.googleblog.com/2019/08/making-authentication-even-easier-with_12.html", "abstract": "", "date": "\nAugust 12, 2019\n"},
{"website": "Google-Security", "title": "\nAwarding Google Cloud Vulnerability Research\n", "author": ["Posted by Felix Groebert, Information Security Engineering"], "link": "https://security.googleblog.com/2019/08/awarding-google-cloud-vulnerability.html", "abstract": "", "date": "\nAugust 8, 2019\n"},
{"website": "Google-Security", "title": "\nUnderstanding why phishing attacks are so effective and how to mitigate them\n", "author": ["Posted by", "\u00a0"], "link": "https://security.googleblog.com/2019/08/understanding-why-phishing-attacks-are.html", "abstract": "", "date": "\nAugust 8, 2019\n"},
{"website": "Google-Security", "title": "\nAdopting the Arm Memory Tagging Extension in Android\n", "author": ["Posted by Kostya Serebryany, Google Core Systems, and Sudhi Herle, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/08/adopting-arm-memory-tagging-extension.html", "abstract": "As part of our continuous commitment to improve the security of the Android ecosystem, we are partnering with Arm to design the  (MTE). , common in C and C++, remain one of the largest vulnerabilities in the Android platform and although there have been previous , memory safety bugs comprised more than half of the high priority security bugs in Android 9. Additionally, memory safety bugs manifest as hard to diagnose reliability problems, including sporadic crashes or silent data corruption. This reduces user satisfaction and . Software testing tools, such as  and  help, but their applicability on current hardware is limited due to noticeable overheads.\n\nMTE, a hardware feature, aims to further mitigate these memory safety bugs by enabling us to detect them with low overhead. It has two execution modes: \n\nArm recently published a  and has added documentation to the Arm v8.5 . \n\nWe envision several different usage modes for MTE. \n\nWe believe that memory tagging will detect the most common classes of memory safety bugs in the wild, helping vendors identify and fix them, discouraging malicious actors from exploiting them. During the past year, our team has been working to ensure readiness of the Android platform and application software for MTE. We have deployed HWASAN, a software implementation of the memory tagging concept, to test our entire platform and a few select apps. This deployment has uncovered close to 100 memory safety bugs. The majority of these bugs were detected on HWASAN enabled phones in everyday use. MTE will greatly improve upon this in terms of overhead, ease of deployment, and scale. In parallel, we have been working on supporting MTE  and . The Android platform support for MTE will be complete by the time of silicon availability. \n\nGoogle is committed to supporting MTE throughout the Android software stack. We are working with select Arm System On Chip (SoC) partners to test MTE support and look forward to wider deployment of MTE in the Android software and hardware ecosystem. Based on the current data points, MTE provides tremendous benefits at acceptable performance costs. We are considering MTE as a possible foundational requirement for certain tiers of Android devices.", "date": "\nAugust 2, 2019\n"},
{"website": "Google-Security", "title": "\nTitan Security Keys are now available in Canada, France, Japan, and the UK\n", "author": [], "link": "https://security.googleblog.com/2019/07/titan-security-keys-are-now-available.html", "abstract": "", "date": "\nJuly 31, 2019\n"},
{"website": "Google-Security", "title": "\nChrome Fuzzer Program Update And How-To\n", "author": ["Posted by Max Moroz, Fuzzing Evangelist, and Ned Williamson, Fuzzing Entrepreneur"], "link": "https://security.googleblog.com/2019/07/chrome-fuzzer-program-update-and-how-to.html", "abstract": "We increased the Chrome Fuzzer Program bonus from $500 to $1,000 as part of our recent .\n\n is a part of the Google Chrome Vulnerability Reward Program that lets security researchers run their fuzzers at scale on the  infrastructure. It makes bug reporting fully automated, and the fuzzer authors get the same rewards as if they reported the bugs manually, plus an extra bonus ($1,000 as of now) on top of it for every new vulnerability.\n\nWe run fuzzers indefinitely, and some of the fuzzers contributed years ago are still finding security issues in ever changing Chrome code. This is a win-win for both sides, as security researchers do not have to spend time analyzing the crashes, and Chrome developers receive high quality bug reports automatically.\n\nTo learn more about the Chrome Fuzzer Program, let\u2019s talk to Ned Williamson, who\u2019s been a participant since 2017 and now works on the Google Security team.\n\n Hey Ned! It looks like you\u2019ve received over $50,000 by participating in the Google Chrome Vulnerability Reward Program with your . \n\n Yes, it\u2019s true. I wrote a fuzzer for QUIC which helped me find and report two critical vulnerabilities, each worth $10,000. Because I knew my fuzzer worked well, I submitted it to the Chrome Fuzzer Program. Then, in the next few months, I received that reward three more times (plus a bonus), as the fuzzer caught several security regressions on ClusterFuzz soon after they happened.\n\n Have you intentionally focused on the areas that yield  issues and bigger rewards?\n\n Yes. While vulnerabilities in code that is more critical to user security yield larger reward amounts, I actually started by looking at lower severity bugs and incrementally began looking for more severe bugs until I could find critical ones. You can see this progression by looking at the  as an external researcher.\n\n Would you suggest starting by looking for non-critical bugs?\n\n I would say so. Security-critical code is generally better designed and more thoroughly audited, so it might be discouraging to start from there. Finding less critical security bugs and winning bounties is a good way to build confidence and stay motivated.\n\n Can you share an algorithm on how to find security bugs in Chrome?\n\n Looking at previous and existing bug reports, even for non-security crashes, is a great way to tell which code is security-critical and potentially buggy. From there, if some code looks like it\u2019s exposed to user inputs, I\u2019d set up a fuzzing campaign against that component. After you gain experience you will not need to rely on existing reports to find new attack surface, which in turn helps you find places that have not been considered by previous researchers. This was the case for my QUIC fuzzer.\n\n How did you learn to write fuzzers?\n\n I didn\u2019t have any special knowledge about fuzzing before I started looking for vulnerabilities in Chrome. I followed the  in the repository and I still follow the same process today.\n\n  isn\u2019t very simple compared to . How did you get to that implementation?\n\n The key insight in the QUIC fuzzer was realizing that the parts of the code that handled plaintext messages after decryption were prone to memory corruption. Typically, fuzzing does not perform well with encrypted inputs (it\u2019s pretty hard to \u201crandomly\u201d generate a packet that can be successfully decrypted), so I extended the QUIC testing code to allow for testing with encryption disabled.\n\n: Are there any other good examples of fuzz targets employing a similar logic?\n\n: Another example is  that wraps the fuzzing input around with a valid hardcoded PDF file, therefore focusing fuzzing only on the XFA script part of it. As a researcher, you just need to choose what exactly you want to fuzz, and then understand how to execute that code properly. Looking at the unit tests is usually the easiest way to get such an understanding.\n\nUseful links:\n\nHappy fuzzing and bug hunting!", "date": "\nJuly 30, 2019\n"},
{"website": "Google-Security", "title": "\nBigger Rewards for Security Bugs\n", "author": ["Posted by Natasha Pabrai and Andrew Whalley, Chrome Security Team\n"], "link": "https://security.googleblog.com/2019/07/bigger-rewards-for-security-bugs.html", "abstract": "Chrome has always been built with security at its core, by a passionate worldwide community as part of the  open source project. We're proud that community includes world class security researchers who help defend Chrome, and other Chromium based browsers.\n\nBack in 2010  the Chrome Vulnerability Rewards Program which provides cash rewards to researchers for finding and reporting security bugs that help keep our users safe. Since its inception the program has received over 8,500 reports and paid out over five million dollars! A big thank you to every one of the researchers - it's an honor working with you.\n\nOver the years we've expanded the program, including rewarding full chain exploits on Chrome OS, and the , where we run researchers' fuzzers on thousands of Google cores and automatically submit bugs they find for reward.\n\nToday, we're delighted to announce an across the board increase in our reward amounts! Full details can be found on our  but highlights include tripling the maximum baseline reward amount from $5,000 to $15,000 and doubling the maximum reward amount for high quality reports from $15,000 to $30,000. The additional bonus given to bugs found by fuzzers running under Chrome Fuzzer Program is also doubling to $1,000.\n\nWe've also clarified what we consider a , to help reporters get the highest possible reward, and we've updated the bug categories to better reflect the types of bugs that are reported and that we are most interested in.\n\nBut that's not all! On Chrome OS we're increasing our standing reward to $150,000 for exploit chains that can compromise a Chromebook or Chromebox with persistence in guest mode. Security bug in firmware and lock screen bypasses also get their own reward categories.\n\nThese new reward amounts will apply to bugs submitted after today on the . As always, see the  for full details about the program.\n\nIn other news, our friends over at the  have increased their rewards for remote code execution bugs from $5,000 to $20,000, theft of insecure private data from $1,000 to $3,000, and access to protected app components from $1,000 to $3,000. The Google Play Security Reward Program also pays bonus rewards for responsibly disclosing vulnerabilities to participating app developers. Check out the  to learn more and see which apps are in scope.\n\nHappy bug hunting!", "date": "\nJuly 18, 2019\n"},
{"website": "Google-Security", "title": "\nHow Google adopted BeyondCorp\n", "author": ["Posted by Lior Tishbi, Program Manager and Puneet Goel, Product Manager, Justin McWilliams, Engineering Manager"], "link": "https://security.googleblog.com/2019/06/how-google-adopted-beyondcorp.html", "abstract": "", "date": "\nJune 27, 2019\n"},
{"website": "Google-Security", "title": "\nGoogle Public DNS over HTTPS (DoH) supports RFC 8484 standard\n", "author": ["Posted by Marshall Vale, Product Manager and Alexander Dupuy, Software Engineer"], "link": "https://security.googleblog.com/2019/06/google-public-dns-over-https-doh.html", "abstract": "", "date": "\nJune 26, 2019\n"},
{"website": "Google-Security", "title": "\nHelping organizations do more without collecting more data\n", "author": ["Posted by Amanda Walker, Engineering Director; Sarvar Patel, Software Engineer; and Moti Yung, Research Scientist, Private Computing"], "link": "https://security.googleblog.com/2019/06/helping-organizations-do-more-without-collecting-more-data.html", "abstract": "", "date": "\nJune 19, 2019\n"},
{"website": "Google-Security", "title": "\nNew Chrome Protections from Deception\n", "author": ["Posted by Emily Schechter, Chrome Product Manager\n"], "link": "https://security.googleblog.com/2019/06/new-chrome-protections-from-deception.html", "abstract": "", "date": "\nJune 18, 2019\n"},
{"website": "Google-Security", "title": "\nImproving Security and Privacy for Extensions Users\n", "author": ["Posted by Devlin Cronin, Chrome Extensions Team"], "link": "https://security.googleblog.com/2019/06/improving-security-and-privacy-for.html", "abstract": "", "date": "\nJune 12, 2019\n"},
{"website": "Google-Security", "title": "\nUse your Android phone\u2019s built-in security key to verify sign-in on iOS devices \n", "author": ["Posted by Kaiyu Yan and Christiaan Brand"], "link": "https://security.googleblog.com/2019/06/use-your-android-phones-built-in.html", "abstract": "", "date": "\nJune 12, 2019\n"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Triada\n", "author": ["Posted by Lukasz Siewierski, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/06/pha-family-highlights-triada.html", "abstract": "", "date": "\nJune 6, 2019\n"},
{"website": "Google-Security", "title": "\nNew research: How effective is basic account hygiene at preventing hijacking\n", "author": ["Posted by Kurt Thomas and Angelika Moscicki"], "link": "https://security.googleblog.com/2019/05/new-research-how-effective-is-basic.html", "abstract": "", "date": "\nMay 17, 2019\n"},
{"website": "Google-Security", "title": "\nAdvisory: Security Issue with Bluetooth Low Energy (BLE) Titan Security Keys\n", "author": ["Posted by Christiaan Brand, Product Manager, Google Cloud"], "link": "https://security.googleblog.com/2019/05/titan-keys-update.html", "abstract": "", "date": "\nMay 15, 2019\n"},
{"website": "Google-Security", "title": "\nWhat\u2019s New in Android Q Security\n", "author": [], "link": "https://security.googleblog.com/2019/05/whats-new-in-android-q-security.html", "abstract": "With every new version of Android, one of our top priorities is raising the bar for security. Over the last few years, these improvements have led to measurable progress across the ecosystem, and 2018 was no different. \n\nIn the 4th quarter of 2018, we had 84% more devices receiving a security update than in the same quarter the prior year. At the same time, no critical security vulnerabilities affecting the Android platform were publicly disclosed without a security update or mitigation available in 2018, and we saw a 20% year-over-year decline in the proportion of devices that installed a . In the spirit of transparency, we released this data and more in our \n\nBut now you may be asking, what\u2019s next? \n\nToday at Google I/O we lifted the curtain on all the new security features being integrated into Android Q. We plan to go deeper on each feature in the coming weeks and months, but first wanted to share a quick summary of all the security goodness we\u2019re adding to the platform. \n\n\nStorage encryption is one of the most fundamental (and effective) security technologies, but current encryption standards require devices have cryptographic acceleration hardware. Because of this requirement many devices are not capable of using storage encryption. The launch of Adiantum changes that in the Android Q release. We announced  in February. Adiantum is designed to run efficiently without specialized hardware, and can work across everything from smart watches to internet-connected medical devices.\n\nOur commitment to the importance of encryption continues with the Android Q release. All compatible Android devices newly launching with Android Q are required to encrypt user data, with no exceptions. This includes phones, tablets, televisions, and automotive devices. This will ensure the next generation of devices are more secure than their predecessors, and allow the next billion people coming online for the first time to do so safely. \n\nHowever, storage encryption is just one half of the picture, which is why we are also enabling TLS 1.3 support by default in Android Q. TLS 1.3 is a major revision to the TLS standard finalized by the IETF in August 2018. It is faster, more secure, and more private.  TLS 1.3 can often complete the handshake in fewer roundtrips, making the connection time up to 40% faster for those sessions. From a security perspective, TLS 1.3 removes support for weaker cryptographic algorithms, as well as some insecure or obsolete features. It uses a newly-designed handshake which fixes several weaknesses in TLS 1.2. The new protocol is cleaner, less error prone, and more resilient to key compromise. Finally, from a privacy perspective, TLS 1.3 encrypts more of the handshake to better protect the identities of the participating parties.\n\n\nAndroid utilizes a strategy of defense-in-depth to ensure that individual implementation bugs are insufficient for bypassing our security systems. We apply process isolation, attack surface reduction, architectural decomposition, and exploit mitigations to render vulnerabilities more difficult or impossible to exploit, and to increase the number of vulnerabilities needed by an attacker to achieve their goals.\n\nIn Android Q, we have applied these strategies to security critical areas such as media, Bluetooth, and the kernel. We describe these improvements more extensively in a separate , but some highlights include:\n\n\nAndroid Pie introduced the BiometricPrompt API to help apps utilize biometrics, including face, fingerprint, and iris. Since the launch, we\u2019ve seen a lot of apps embrace the new API, and now with Android Q, we\u2019ve updated the underlying framework with robust support for face and fingerprint. Additionally, we expanded the API to support additional use-cases, including both implicit and explicit authentication.\n\nIn the explicit flow, the user must perform an action to proceed, such as tap their finger to the fingerprint sensor.  If they\u2019re using face or iris to authenticate, then the user must click an additional button to proceed. The explicit flow is the default flow and should be used for all high-value transactions such as payments.\n\nImplicit flow does not require an additional user action. It is used to provide a lighter-weight, more seamless experience for transactions that are readily and easily reversible, such as sign-in and autofill. \n\nAnother handy new feature in BiometricPrompt is the ability to check if a device supports biometric authentication prior to invoking BiometricPrompt. This is useful when the app wants to show an \u201cenable biometric sign-in\u201d or similar item in their sign-in page or in-app settings menu.  To support this, we\u2019ve added a new  class. You can now call the  method in it to determine whether the device supports biometric authentication and whether the user is enrolled.\n\n\nBeyond Android Q, we are looking to add Electronic ID support for mobile apps, so that your phone can be used as an ID, such as a driver\u2019s license. Apps such as these have a lot of security requirements and involves integration between the client application on the holder\u2019s mobile phone, a reader/verifier device, and issuing authority backend systems used for license issuance, updates, and revocation. \n\nThis initiative requires expertise around cryptography and standardization from the  and is being led by the Android Security and Privacy team. We will be providing APIs and a reference implementation of HALs for Android devices in order to ensure the platform provides the building blocks for similar security and privacy sensitive applications. You can expect to hear more updates from us on Electronic ID support in the near future.", "date": "\nMay 9, 2019\n"},
{"website": "Google-Security", "title": "\nQueue the Hardening Enhancements\n", "author": [], "link": "https://security.googleblog.com/2019/05/queue-hardening-enhancements.html", "abstract": "Android Q Beta versions are now publicly . Among the various new features introduced in Android Q are some important security hardening changes. While exciting are added in each Android release, hardening generally refers to security improvements made to existing components.\n\nWhen prioritizing platform hardening, we analyze data from a number of sources including our  (VRP). Past security issues provide useful insight into which components can use additional hardening. Android publishes  which include fixes for all the high/critical severity vulnerabilities in the Android Open Source Project (AOSP) reported through our VRP. While fixing vulnerabilities is necessary, we also get a lot of value from the metadata - analysis on the location and class of vulnerabilities. With this insight we can apply the following strategies to our existing components:\n\nHere\u2019s a look at high severity vulnerabilities by component and cause from 2018:\n\n\n\nMost of Android\u2019s vulnerabilities occur in the media and bluetooth components. Use-after-free (UAF), integer overflows, and out of bounds (OOB) reads/writes comprise 90% of vulnerabilities with OOB being the most common.\n\nIn Android Q, we moved software codecs out of the main mediacodec service into a . This is a big step forward in our effort to improve security by isolating various media components into less privileged sandboxes. As Mark Brand of Project Zero points out in his  blog post, constrained sandboxes are not where an attacker wants to end up. In 2018, approximately 80% of the critical/high severity vulnerabilities in media components occurred in software codecs, meaning further isolating them is a big improvement. Due to the increased protection provided by the new mediaswcodec sandbox, these same vulnerabilities will receive a lower severity based on Android\u2019s .\n\nThe following figure shows an overview of the evolution of media services layout in the recent Android releases. \n\n\nWith this move, we now have the two primary sources for media vulnerabilities tightly sandboxed within constrained processes. Software codecs are similar to extractors in that they both have extensive code parsing bitstreams from untrusted sources. Once a vulnerability is identified in the source code, it can be triggered by sending a crafted media file to media APIs (such as MediaExtractor or MediaCodec). Sandboxing these two services allows us to reduce the severity of potential security vulnerabilities without compromising performance.\n\nIn addition to constraining riskier codecs, a lot of work has also gone into preventing common types of vulnerabilities.\n\nIncorrect or missing memory bounds checking on arrays account for about 34% of Android\u2019s userspace vulnerabilities. In cases where the array size is known at compile time, LLVM\u2019s bound sanitizer (BoundSan) can automatically instrument arrays to prevent overflows and fail safely.\n\nBoundSan instrumentation\nBoundSan is enabled in 11 media codecs and throughout the Bluetooth stack for Android Q. By optimizing away a number of   the performance overhead was reduced to less than 1%. BoundSan has already found/prevented potential vulnerabilities in codecs and Bluetooth.\n\nAndroid  the production use of sanitizers in Android Nougat when we first started rolling out integer sanization (IntSan) in the media frameworks. This work has continued with each release and has been very successful in preventing otherwise exploitable vulnerabilities. For example, new IntSan coverage in Android Pie mitigated 11 critical vulnerabilities. Enabling IntSan is challenging because overflows are generally benign and unsigned integer overflows are well defined and sometimes intentional. This is quite different from the bound sanitizer where OOB reads/writes are always unintended and often exploitable. Enabling Intsan has been a multi year project, but with Q we have fully enabled it across the media frameworks with the inclusion of 11 more codecs.\n\n\nIntSan Instrumentation\nIntSan works by instrumenting arithmetic operations to abort when an overflow occurs. This instrumentation can have an impact on performance, so evaluating the impact on CPU usage is necessary. In cases where performance impact was too high, we identified hot functions and individually disabled IntSan on those functions after manually reviewing them for integer safety.\n\nBoundSan and IntSan are considered strong mitigations because (where applied) they prevent the root cause of memory safety vulnerabilities. The class of mitigations described next target common exploitation techniques. These mitigations are considered to be probabilistic because they make exploitation more difficult by limiting how a vulnerability may be used. \n\nLLVM\u2019s Control Flow Integrity (CFI) was enabled in the media frameworks, Bluetooth, and NFC in . CFI makes code reuse attacks more difficult by protecting the forward-edges of the call graph, such as function pointers and virtual functions. Android Q uses LLVM\u2019s Shadow Call Stack (SCS) to protect return addresses, protecting the backwards-edge of control flow graph. SCS accomplishes this by storing return addresses in a separate shadow stack which is protected from leakage by storing its location in the x18 register, which is now reserved by the compiler.\n\n\nSCS Instrumentation\nSCS has negligible performance overhead and a small memory increase due to the separate stack. In Android Q, SCS has been turned on in portions of the Bluetooth stack and is also available for the kernel. We\u2019ll share more on that in an upcoming post.\n\nLike SCS, eXecute-Only Memory (XOM) aims at making common exploitation techniques more expensive. It does so by strengthening the protections already provided by address space layout randomization (ASLR) which in turn makes code reuse attacks more difficult by requiring attackers to first leak the location of the code they intend to reuse. This often means that an attacker now needs two vulnerabilities, a read primitive and a write primitive, where previously just a write primitive was necessary in order to achieve their goals. XOM protects against leaks (memory disclosures of code segments) by making code unreadable. Attempts to read execute-only code results in the process aborting safely.\n\n\nTombstone from a XOM abort\nStarting in Android Q, platform-provided AArch64 code segments in binaries and libraries are loaded as execute-only. Not all devices will immediately receive the benefit as this enforcement has hardware dependencies (ARMv8.2+) and kernel dependencies (Linux 4.9+, CONFIG_ARM64_UAO). For apps with a targetSdkVersion lower than Q, Android\u2019s zygote process will relax the protection in order to avoid potential app breakage, but 64 bit system processes (for example, mediaextractor, init, vold, etc.) are protected. XOM protections are applied at compile-time and have no memory or CPU overhead.\n\nScudo is a dynamic heap allocator designed to be resilient against heap related vulnerabilities such as:\n\nScudo does not prevent exploitation but rather proactively manages memory in a way to make exploitation more difficult. It is configurable on a per-process basis depending on performance requirements. Scudo is enabled in extractors and codecs in the media frameworks.\n\nTombstone from Scudo aborts\nAOSP makes use of a number of Open Source Projects to build and secure Android. Google is actively contributing back to these projects in a number of security critical areas:", "date": "\nMay 9, 2019\n"},
{"website": "Google-Security", "title": "\nQuantifying Measurable Security\n", "author": ["Posted by Eugene Liderman, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/05/quantifying-measurable-security.html", "abstract": "", "date": "\nMay 8, 2019\n"},
{"website": "Google-Security", "title": "\nGoogle CTF 2019 is here\n", "author": ["Posted by Jan Keller, Security Technical Program Manager"], "link": "https://security.googleblog.com/2019/05/google-ctf-2019-is-here.html", "abstract": "", "date": "\nMay 3, 2019\n"},
{"website": "Google-Security", "title": "\nBetter protection against Man in the Middle phishing attacks\n", "author": ["Posted by Jonathan Skelker, Product Manager, Account Security"], "link": "https://security.googleblog.com/2019/04/better-protection-against-man-in-middle.html", "abstract": "", "date": "\nApril 18, 2019\n"},
{"website": "Google-Security", "title": "\nThe Android Platform Security Model\n", "author": ["Posted by Jeff Vander Stoep, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/04/the-android-platform-security-model.html", "abstract": "", "date": "\nApril 18, 2019\n"},
{"website": "Google-Security", "title": "\nGmail making email more secure with MTA-STS standard\n", "author": ["Posted by Nicolas Lidzborski, Senior Staff Software Engineer, Google Cloud and\u00a0Nicolas Kardas, Senior Product Manager, Google Cloud\u00a0"], "link": "https://security.googleblog.com/2019/04/gmail-making-email-more-secure-with-mta.html", "abstract": "", "date": "\nApril 10, 2019\n"},
{"website": "Google-Security", "title": "\nAndroid Security & Privacy Year in Review 2018: Keeping two billion users, and their data, safe and sound\n", "author": ["Posted by Meghan Kelly, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/03/android-security-privacy-year-in-review.html", "abstract": "", "date": "\nMarch 29, 2019\n"},
{"website": "Google-Security", "title": "\nManaged Google Play earns key certifications for security and privacy\n", "author": [], "link": "https://security.googleblog.com/2019/03/managed-google-play-earns-key.html", "abstract": "", "date": "\nMarch 21, 2019\n"},
{"website": "Google-Security", "title": "\nOpen-sourcing Sandboxed API\n", "author": ["Posted by Christian Blichmann & Robert Swiecki, ISE Sandboxing team"], "link": "https://security.googleblog.com/2019/03/open-sourcing-sandboxed-api.html", "abstract": "", "date": "\nMarch 18, 2019\n"},
{"website": "Google-Security", "title": "\nDisclosing vulnerabilities to protect users across platforms\n", "author": ["Posted by Clement Lecigne, Threat Analysis Group"], "link": "https://security.googleblog.com/2019/03/disclosing-vulnerabilities-to-protect.html", "abstract": "", "date": "\nMarch 7, 2019\n"},
{"website": "Google-Security", "title": "\nAndroid Security Improvement update: Helping developers harden their apps, one thwarted vulnerability at a time\n", "author": [], "link": "https://security.googleblog.com/2019/02/android-security-improvement-update.html", "abstract": "Helping Android app developers build secure apps, free of known vulnerabilities, means helping the overall ecosystem thrive. This is why we launched the  five years ago, and why we're still so invested in its success today. \n\nWhen an app is submitted to the Google Play store, we scan it to determine if a variety of vulnerabilities are present. If we find something concerning, we flag it to the developer and then help them to remedy the situation. \n\nThink of it like a routine physical. If there are no problems, the app runs through our normal tests and continues on the process to being published in the Play Store. If there is a problem, however, we provide a diagnosis and next steps to get back to healthy form. \n\nOver its lifetime, the program has helped more than 300,000 developers to fix more than 1,000,000 apps on Google Play. In 2018 alone, the program helped over 30,000 developers fix over 75,000 apps. The downstream effect means that those 75,000 vulnerable apps are not distributed to users with the same security issues present, which we consider a win. \n\nThe App Security Improvement program covers a broad range of security issues in Android apps. These can be as specific as security issues in certain versions of popular libraries () and as broad as . \n\nWe are continuously improving this program's capabilities by improving the existing checks and launching checks for more classes of security vulnerability. In 2018, we deployed warnings for six additional security vulnerability classes including:\n\nEnsuring that we're continuing to evolve the program as new exploits emerge is a top priority for us. We are continuing to work on this throughout 2019.\n\nKeeping Android users safe is important to Google. We know that app security is often tricky and that developers can make mistakes. We hope to see this program grow in the years to come, helping developers worldwide build apps users can truly trust.", "date": "\nFebruary 28, 2019\n"},
{"website": "Google-Security", "title": "\nGoogle Play Protect in 2018: New updates to keep Android users secure\n", "author": [], "link": "https://security.googleblog.com/2019/02/google-play-protect-in-2018-new-updates.html", "abstract": "", "date": "\nFebruary 26, 2019\n"},
{"website": "Google-Security", "title": "\nHow we fought bad apps and malicious developers in 2018\n", "author": [], "link": "https://security.googleblog.com/2019/02/how-we-fought-bad-apps-and-malicious.html", "abstract": "", "date": "\nFebruary 13, 2019\n"},
{"website": "Google-Security", "title": "\nOpen sourcing ClusterFuzz\n", "author": ["Posted by Abhishek Arya, Oliver Chang, Max Moroz, Martin Barbella and Jonathan Metzman (ClusterFuzz team)"], "link": "https://security.googleblog.com/2019/02/open-sourcing-clusterfuzz.html", "abstract": "", "date": "\nFebruary 7, 2019\n"},
{"website": "Google-Security", "title": "\n Introducing Adiantum: Encryption for the Next Billion Users\n", "author": ["Posted by Paul Crowley and Eric Biggers, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2019/02/introducing-adiantum-encryption-for.html", "abstract": "", "date": "\nFebruary 7, 2019\n"},
{"website": "Google-Security", "title": "\nProtect your accounts from data breaches with Password Checkup\n", "author": ["Posted by Jennifer Pullman, Kurt Thomas, and Elie Bursztein, Security and Anti-abuse research"], "link": "https://security.googleblog.com/2019/02/protect-your-accounts-from-data.html", "abstract": "", "date": "\nFebruary 5, 2019\n"},
{"website": "Google-Security", "title": "\nPHA Family Highlights: Zen and its cousins\n", "author": [], "link": "https://security.googleblog.com/2019/01/pha-family-highlights-zen-and-its.html", "abstract": "", "date": "\nJanuary 11, 2019\n"},
{"website": "Google-Security", "title": "\nGoogle Public DNS now supports DNS-over-TLS\n", "author": ["Posted by Marshall Vale, Product Manager and Puneet Sood, Software Engineer"], "link": "https://security.googleblog.com/2019/01/google-public-dns-now-supports-dns-over.html", "abstract": "", "date": "\nJanuary 9, 2019\n"},
{"website": "Google-Security", "title": "\nAndroid Pie \u00e0 la mode: Security & Privacy\n", "author": [], "link": "https://security.googleblog.com/2018/12/android-pie-la-mode-security-privacy.html", "abstract": "", "date": "\nDecember 20, 2018\n"},
{"website": "Google-Security", "title": "\nNew Keystore features keep your slice of Android Pie a little safer\n", "author": [], "link": "https://security.googleblog.com/2018/12/new-keystore-features-keep-your-slice.html", "abstract": "", "date": "\nDecember 12, 2018\n"},
{"website": "Google-Security", "title": "\nTackling ads abuse in apps and SDKs\n", "author": ["Posted by Dave Kleidermacher, VP, Head of Security & Privacy - Android & Play"], "link": "https://security.googleblog.com/2018/12/tackling-ads-abuse-in-apps-and-sdks.html", "abstract": "", "date": "\nDecember 7, 2018\n"},
{"website": "Google-Security", "title": "\nASPIRE to keep protecting billions of Android users\n", "author": ["Posted by Billy Lau and Ren\u00e9 Mayrhofer, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/12/aspire-to-keep-protecting-billions-of.html", "abstract": "", "date": "\nDecember 5, 2018\n"},
{"website": "Google-Security", "title": "\nAnnouncing the Google Security and Privacy Research Awards\n", "author": ["Posted by Elie Bursztein and Oxana Comanescu, Google Security and Privacy Group"], "link": "https://security.googleblog.com/2018/11/announcing-google-security-and-privacy.html", "abstract": "", "date": "\nNovember 29, 2018\n"},
{"website": "Google-Security", "title": "\nIndustry collaboration leads to takedown of the \u201c3ve\u201d ad fraud operation\n", "author": ["Posted by Per Bjorke, Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2018/11/industry-collaboration-leads-to.html", "abstract": "", "date": "\nNovember 27, 2018\n"},
{"website": "Google-Security", "title": "\nCombating Potentially Harmful Applications with Machine Learning at Google: Datasets and Models\n", "author": ["Posted by Mo Yu, Damien Octeau, and Chuangang Ren, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/11/combating-potentially-harmful_14.html", "abstract": "", "date": "\nNovember 15, 2018\n"},
{"website": "Google-Security", "title": "\nIntroducing the Android Ecosystem Security Transparency Report\n", "author": ["Posted by Jason Woloz and Eugene Liderman, Android Security & Privacy Team"], "link": "https://security.googleblog.com/2018/11/introducing-android-ecosystem-security.html", "abstract": "", "date": "\nNovember 8, 2018\n"},
{"website": "Google-Security", "title": "\nA New Chapter for OSS-Fuzz\n", "author": ["Posted by Matt Ruhstaller, TPM and Oliver Chang, Software Engineer, Google Security Team"], "link": "https://security.googleblog.com/2018/11/a-new-chapter-for-oss-fuzz.html", "abstract": "", "date": "\nNovember 6, 2018\n"},
{"website": "Google-Security", "title": "\nAnnouncing some security treats to protect you from attackers\u2019 tricks\n", "author": ["Posted by Jonathan Skelker, Product Manager"], "link": "https://security.googleblog.com/2018/10/announcing-some-security-treats-to.html", "abstract": "", "date": "\nOctober 31, 2018\n"},
{"website": "Google-Security", "title": "\nIntroducing reCAPTCHA v3: the new way to stop bots\n", "author": ["Posted by Wei Liu, Google Product Manager"], "link": "https://security.googleblog.com/2018/10/introducing-recaptcha-v3-new-way-to.html", "abstract": "", "date": "\nOctober 29, 2018\n"},
{"website": "Google-Security", "title": "\nGoogle tackles new ad fraud scheme\n", "author": ["Posted by Per Bjorke, Product Manager, Ad Traffic Quality"], "link": "https://security.googleblog.com/2018/10/google-tackles-new-ad-fraud-scheme.html", "abstract": "", "date": "\nOctober 23, 2018\n"},
{"website": "Google-Security", "title": "\nAndroid Protected Confirmation: Taking transaction security to the next level\n", "author": ["Posted by Janis Danisevskis, Information Security Engineer, Android Security"], "link": "https://security.googleblog.com/2018/10/android-protected-confirmation-taking.html", "abstract": "", "date": "\nOctober 19, 2018\n"},
{"website": "Google-Security", "title": "\nBuilding a Titan: Better security through a tiny chip\n", "author": [], "link": "https://security.googleblog.com/2018/10/building-titan-better-security-through.html", "abstract": "", "date": "\nOctober 17, 2018\n"},
{"website": "Google-Security", "title": "\nModernizing Transport Security\n", "author": ["Posted by David Benjamin, Chrome networking"], "link": "https://security.googleblog.com/2018/10/modernizing-transport-security.html", "abstract": "", "date": "\nOctober 15, 2018\n"},
{"website": "Google-Security", "title": "\nGoogle and Android have your back by protecting your backups\n", "author": ["Posted by Troy Kensinger, Technical Program Manager, Android Security and Privacy"], "link": "https://security.googleblog.com/2018/10/google-and-android-have-your-back-by.html", "abstract": "", "date": "\nOctober 12, 2018\n"},
{"website": "Google-Security", "title": "\nControl Flow Integrity in the Android kernel\n", "author": [], "link": "https://security.googleblog.com/2018/10/posted-by-sami-tolvanen-staff-software.html", "abstract": "", "date": "\nOctober 10, 2018\n"},
{"website": "Google-Security", "title": "\nTrustworthy Chrome Extensions, by Default\n", "author": ["Posted by James Wagner, Chrome Extensions Product Manager"], "link": "https://security.googleblog.com/2018/10/trustworthy-chrome-extensions-by-default.html", "abstract": "", "date": "\nOctober 1, 2018\n"},
{"website": "Google-Security", "title": "\nAndroid and Google Play Security Rewards Programs surpass $3M in payouts\n", "author": [], "link": "https://security.googleblog.com/2018/09/android-and-google-play-security_20.html", "abstract": "", "date": "\nSeptember 20, 2018\n"},
{"website": "Google-Security", "title": "\nIntroducing the Tink cryptographic software library\n", "author": ["Posted by Thai Duong, Information Security Engineer, on behalf of Tink team"], "link": "https://security.googleblog.com/2018/08/introducing-tink-cryptographic-software.html", "abstract": "", "date": "\nAugust 30, 2018\n"},
{"website": "Google-Security", "title": "\nEvolution of Android Security Updates\n", "author": ["Posted by Dave Kleidermacher, VP, Head of Security - Android, Chrome OS, Play"], "link": "https://security.googleblog.com/2018/08/evolution-of-android-security-updates.html", "abstract": "", "date": "\nAugust 22, 2018\n"},
{"website": "Google-Security", "title": "\nA reminder about government-backed phishing\n", "author": ["Posted by Shane Huntley, Threat Analysis Group"], "link": "https://security.googleblog.com/2018/08/a-reminder-about-government-backed.html", "abstract": "", "date": "\nAugust 20, 2018\n"},
{"website": "Google-Security", "title": "\nExpanding our Vulnerability Reward Program to combat platform abuse\n", "author": ["Posted by Eric Brown and Marc Henson, Trust & Safety"], "link": "https://security.googleblog.com/2018/08/expanding-our-vulnerability-reward.html", "abstract": "", "date": "\nAugust 15, 2018\n"},
{"website": "Google-Security", "title": "\nGoogle Public DNS turns 8.8.8.8 years old\n", "author": ["Posted by Alexander Dupuy, Software Engineer"], "link": "https://security.googleblog.com/2018/08/google-public-dns-turns-8888-years-old.html", "abstract": "", "date": "\nAugust 10, 2018\n"},
{"website": "Google-Security", "title": "\nMitigating Spectre with Site Isolation in Chrome\n", "author": ["Posted by Charlie Reis, Site Isolator"], "link": "https://security.googleblog.com/2018/07/mitigating-spectre-with-site-isolation.html", "abstract": "", "date": "\nJuly 11, 2018\n"},
{"website": "Google-Security", "title": "\nCompiler-based security mitigations in Android P\n", "author": ["Posted by Ivan Lozano, Information Security Engineer"], "link": "https://security.googleblog.com/2018/06/compiler-based-security-mitigations-in.html", "abstract": "", "date": "\nJune 27, 2018\n"},
{"website": "Google-Security", "title": "\nBetter Biometrics in Android P\n", "author": ["Posted by Vishwath Mohan, Security Engineer"], "link": "https://security.googleblog.com/2018/06/better-biometrics-in-android-p.html", "abstract": "", "date": "\nJune 21, 2018\n"},
{"website": "Google-Security", "title": "\nEnd-to-end encryption for push messaging, simplified\n", "author": [], "link": "https://security.googleblog.com/2018/06/end-to-end-encryption-for-push.html", "abstract": "", "date": "\nJune 5, 2018\n"},
{"website": "Google-Security", "title": "\nInsider attack resistance\n", "author": ["Posted by Shawn Willden, Staff Software Engineer"], "link": "https://security.googleblog.com/2018/06/insider-attack-resistance.html", "abstract": "", "date": "\nJune 1, 2018\n"},
{"website": "Google-Security", "title": "\nKeeping 2 billion Android devices safe with machine learning\n", "author": [], "link": "https://security.googleblog.com/2018/05/keeping-2-billion-android-devices-safe.html", "abstract": "", "date": "\nMay 24, 2018\n"},
{"website": "Google-Security", "title": "\n Google CTF 2018 is here\n", "author": ["Posted by Jan Keller, Security TPM"], "link": "https://security.googleblog.com/2018/05/google-ctf-2018-is-here.html", "abstract": "", "date": "\nMay 8, 2018\n"},
{"website": "Google-Security", "title": "\nLeveraging AI to protect our users and the web\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead - Ian Goodfellow, Adversarial Machine Learning Research Lead"], "link": "https://security.googleblog.com/2018/04/leveraging-ai-to-protect-our-users-and.html", "abstract": "", "date": "\nApril 20, 2018\n"},
{"website": "Google-Security", "title": "\nDNS over TLS support in Android P Developer Preview\n", "author": ["Posted by Erik Kline, Android software engineer, and Ben Schwartz, Jigsaw software engineer"], "link": "https://security.googleblog.com/2018/04/dns-over-tls-support-in-android-p.html", "abstract": "", "date": "\nApril 17, 2018\n"},
{"website": "Google-Security", "title": "\nProtecting users with TLS by default in Android P\n", "author": ["Posted by Chad Brubaker, Senior Software Engineer Android Security"], "link": "https://security.googleblog.com/2018/04/protecting-users-with-tls-by-default-in.html", "abstract": "", "date": "\nApril 12, 2018\n"},
{"website": "Google-Security", "title": "\nAndroid Security 2017 Year in Review\n", "author": ["Posted by Dave Kleidermacher, Vice President of Security for Android, Play, ChromeOS"], "link": "https://security.googleblog.com/2018/03/android-security-2017-year-in-review.html", "abstract": "", "date": "\nMarch 15, 2018\n"},
{"website": "Google-Security", "title": "\nDistrust of the Symantec PKI: Immediate action needed by site operators\n", "author": ["Posted by Devon O\u2019Brien, Ryan Sleevi, Emily Stark, Chrome security team"], "link": "https://security.googleblog.com/2018/03/distrust-of-symantec-pki-immediate.html", "abstract": "", "date": "\nMarch 7, 2018\n"},
{"website": "Google-Security", "title": "\nA secure web is here to stay\n", "author": ["Posted by Emily Schechter, Chrome Security Product Manager"], "link": "https://security.googleblog.com/2018/02/a-secure-web-is-here-to-stay.html", "abstract": "", "date": "\nFebruary 8, 2018\n"},
{"website": "Google-Security", "title": "\nVulnerability Reward Program: 2017 Year in Review\n", "author": ["Posted by Jan Keller, Google VRP Technical Pwning Master"], "link": "https://security.googleblog.com/2018/02/vulnerability-reward-program-2017-year.html", "abstract": "", "date": "\nFebruary 7, 2018\n"},
{"website": "Google-Security", "title": "\nAnnouncing turndown of the deprecated Google Safe Browsing APIs\n", "author": ["Posted by Alex Wozniak, Software Engineer, Safe Browsing Team"], "link": "https://security.googleblog.com/2018/01/announcing-turndown-of-deprecated.html", "abstract": "", "date": "\nJanuary 24, 2018\n"},
{"website": "Google-Security", "title": "\nAndroid Security Ecosystem Investments Pay Dividends for Pixel\n", "author": ["Posted by Mayank Jain and Scott Roberts, Android security team"], "link": "https://security.googleblog.com/2018/01/android-security-ecosystem-investments.html", "abstract": "", "date": "\nJanuary 17, 2018\n"},
{"website": "Google-Security", "title": "\nMore details about mitigations for the CPU Speculative Execution issue\n", "author": ["Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager"], "link": "https://security.googleblog.com/2018/01/more-details-about-mitigations-for-cpu_4.html", "abstract": "", "date": "\nJanuary 4, 2018\n"},
{"website": "Google-Security", "title": "\nToday's CPU vulnerability: what you need to know\n", "author": ["Posted by Matt Linton, Senior Security Engineer and Pat Parseghian, Technical Program Manager"], "link": "https://security.googleblog.com/2018/01/todays-cpu-vulnerability-what-you-need.html", "abstract": "", "date": "\nJanuary 3, 2018\n"},
{"website": "Google-Security", "title": "\nSecuring communications between Google services with Application Layer Transport Security\n", "author": ["Posted by Cesar Ghali and Julien Boeuf, Engineers on the Security & Privacy Team"], "link": "https://security.googleblog.com/2017/12/securing-communications-between-google.html", "abstract": "", "date": "\nDecember 13, 2017\n"},
{"website": "Google-Security", "title": "\nAdditional protections by Safe Browsing for Android users\n", "author": ["Posted by Paul Stanton and Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2017/12/additional-protections-by-safe-browsing.html", "abstract": "", "date": "\nDecember 1, 2017\n"},
{"website": "Google-Security", "title": "\nTizi: Detecting and blocking socially engineered spyware on Android\n", "author": ["Posted by Anthony Desnos, Megan Ruthven, and Richard Neal, Google Play Protect security engineers and Clement Lecigne, Threat Analysis Group"], "link": "https://security.googleblog.com/2017/11/tizi-detecting-and-blocking-socially.html", "abstract": "", "date": "\nNovember 27, 2017\n"},
{"website": "Google-Security", "title": "\nLock it up! New hardware protections for your lock screen with the Google Pixel 2\n", "author": ["Posted by Xiaowen Xin, Android Security Team"], "link": "https://security.googleblog.com/2017/11/lock-it-up-new-hardware-protections-for.html", "abstract": "", "date": "\nNovember 14, 2017\n"},
{"website": "Google-Security", "title": "\nNew research: Understanding the root cause of account takeover\n", "author": ["Posted by Kurt Thomas, Anti-Abuse Research; Angelika Moscicki, Account Security"], "link": "https://security.googleblog.com/2017/11/new-research-understanding-root-cause.html", "abstract": "", "date": "\nNovember 9, 2017\n"},
{"website": "Google-Security", "title": "\nIntroducing the Google Play Security Reward Program\n", "author": ["Posted by Renu Chaudhary, Android Security and Rahul Mishra, Program Manager"], "link": "https://security.googleblog.com/2017/10/introducing-google-play-security-reward.html", "abstract": "", "date": "\nOctober 19, 2017\n"},
{"website": "Google-Security", "title": "\nBehind the Masq: Yet more DNS, and DHCP, vulnerabilities\n", "author": ["Posted by Fermin J. Serna, Staff Software Engineer, Matt Linton, Senior Security Engineer and Kevin Stadmeyer, Technical Program Manager"], "link": "https://security.googleblog.com/2017/10/behind-masq-yet-more-dns-and-dhcp.html", "abstract": "", "date": "\nOctober 2, 2017\n"},
{"website": "Google-Security", "title": "\nBroadening HSTS to secure more of the Web\n", "author": ["Posted by Ben McIlwain, Google Registry"], "link": "https://security.googleblog.com/2017/09/broadening-hsts-to-secure-more-of-web.html", "abstract": "", "date": "\nSeptember 27, 2017\n"},
{"website": "Google-Security", "title": "\nSafe Browsing: Protecting more than 3 billion devices worldwide, automatically\n", "author": ["Posted by Stephan Somogyi, Safe Browsing Emeritus and Allison Miller, Security & Privacy"], "link": "https://security.googleblog.com/2017/09/safe-browsing-protecting-more-than-3_11.html", "abstract": "", "date": "\nSeptember 11, 2017\n"},
{"website": "Google-Security", "title": "\nChrome\u2019s Plan to Distrust Symantec Certificates\n", "author": ["Posted by Devon O\u2019Brien, Ryan Sleevi, Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2017/09/chromes-plan-to-distrust-symantec.html", "abstract": "", "date": "\nSeptember 11, 2017\n"},
{"website": "Google-Security", "title": "\n From Chrysaor to Lipizzan: Blocking a new targeted spyware family\n", "author": ["Posted by Megan Ruthven Android Security, Ken Bodzak Threat Analysis Group, Neel Mehta Threat Analysis Group"], "link": "https://security.googleblog.com/2017/07/from-chrysaor-to-lipizzan-blocking-new.html", "abstract": "", "date": "\nJuly 26, 2017\n"},
{"website": "Google-Security", "title": "\nFinal removal of trust in WoSign and StartCom Certificates\n", "author": ["Posted by Andrew Whalley and Devon O'Brien, Chrome Security"], "link": "https://security.googleblog.com/2017/07/final-removal-of-trust-in-wosign-and.html", "abstract": "", "date": "\nJuly 20, 2017\n"},
{"website": "Google-Security", "title": "\nIdentifying Intrusive Mobile Apps Using Peer Group Analysis\n", "author": ["Posted by Martin Pelikan, Giles Hogben, and Ulfar Erlingsson of Google\u2019s Security and Privacy team"], "link": "https://security.googleblog.com/2017/07/identifying-intrusive-mobile-apps-using.html", "abstract": "", "date": "\nJuly 12, 2017\n"},
{"website": "Google-Security", "title": "\nMaking the Internet safer and faster: Introducing reCAPTCHA Android API\n", "author": ["Posted by Wei Liu,\u00a0Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2017/06/making-internet-safer-and-faster.html", "abstract": "", "date": "\nJune 9, 2017\n"},
{"website": "Google-Security", "title": "\nAnnouncing Google Capture the Flag 2017\n", "author": ["Posted by Josh Armour Security Program Manager"], "link": "https://security.googleblog.com/2017/06/announcing-google-capture-flag-2017.html", "abstract": "", "date": "\nJune 2, 2017\n"},
{"website": "Google-Security", "title": "\n2017 Android Security Rewards\n", "author": ["Posted by Mayank Jain and Scott Roberts, Android Security team"], "link": "https://security.googleblog.com/2017/06/2017-android-security-rewards.html", "abstract": "", "date": "\nJune 1, 2017\n"},
{"website": "Google-Security", "title": "\nNew Built-In Gmail Protections to Combat Malware in Attachments\n", "author": ["Posted by Sri Somanchi, Product Manager, Gmail anti-spam"], "link": "https://security.googleblog.com/2017/05/new-built-in-gmail-protections-to.html", "abstract": "", "date": "\nMay 31, 2017\n"},
{"website": "Google-Security", "title": "\nOSS-Fuzz: Five months later, and rewarding projects\n", "author": ["Posted by Oliver Chang, Abhishek Arya (Security Engineers, Chrome Security), Kostya Serebryany (Software Engineer, Dynamic Tools), and Josh Armour (Security Program Manager)"], "link": "https://security.googleblog.com/2017/05/oss-fuzz-five-months-later-and.html", "abstract": "", "date": "\nMay 8, 2017\n"},
{"website": "Google-Security", "title": "\nProtecting You Against Phishing\n", "author": ["Posted by Mark Risher, Director, Counter Abuse Technology"], "link": "https://security.googleblog.com/2017/05/protecting-you-against-phishing.html", "abstract": "", "date": "\nMay 5, 2017\n"},
{"website": "Google-Security", "title": "\nNext Steps Toward More Connection Security \n", "author": ["Posted by Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2017/04/next-steps-toward-more-connection.html", "abstract": "", "date": "\nApril 27, 2017\n"},
{"website": "Google-Security", "title": "\nNew Research: Keeping fake listings off Google Maps\n", "author": ["Posted by Doug Grundman, Maps Anti-Abuse, and Kurt Thomas, Security & Anti-Abuse Research"], "link": "https://security.googleblog.com/2017/04/new-research-keeping-fake-listings-off.html", "abstract": "", "date": "\nApril 6, 2017\n"},
{"website": "Google-Security", "title": "\nAn Investigation of Chrysaor Malware on Android\n", "author": ["Posted by Rich Cannings, Jason Woloz, Neel Mehta, Ken Bodzak, Wentao Chang, Megan Ruthven"], "link": "https://security.googleblog.com/2017/04/an-investigation-of-chrysaor-malware-on.html", "abstract": "", "date": "\nApril 3, 2017\n"},
{"website": "Google-Security", "title": "\nUpdates to the Google Safe Browsing\u2019s Site Status Tool\n", "author": ["Posted Deeksha Padma Prasad and Allison Miller, Safe Browsing"], "link": "https://security.googleblog.com/2017/03/updates-to-google-safe-browsings-site.html", "abstract": "", "date": "\nMarch 29, 2017\n"},
{"website": "Google-Security", "title": "\nReassuring our users about government-backed attack warnings\n", "author": ["Posted by Shane Huntley, Google Threat Analysis Group"], "link": "https://security.googleblog.com/2017/03/reassuring-our-users-about-government.html", "abstract": "", "date": "\nMarch 24, 2017\n"},
{"website": "Google-Security", "title": "\nDiverse protections for a diverse ecosystem: Android Security 2016 Year in Review\n", "author": ["Posted by Adrian Ludwig & Mel Miller, Android Security Team"], "link": "https://security.googleblog.com/2017/03/diverse-protections-for-diverse.html", "abstract": "", "date": "\nMarch 22, 2017\n"},
{"website": "Google-Security", "title": "\nDetecting and eliminating Chamois, a fraud botnet on Android\n", "author": ["Posted by Security Software Engineers\u2014Bernhard Grill, Megan Ruthven, and Xin Zhao"], "link": "https://security.googleblog.com/2017/03/detecting-and-eliminating-chamois-fraud.html", "abstract": "", "date": "\nMarch 13, 2017\n"},
{"website": "Google-Security", "title": "\nVRP news from Nullcon\n", "author": ["Posted by Josh Armour, Security Program Manager"], "link": "https://security.googleblog.com/2017/03/vrp-news-from-nullcon.html", "abstract": "", "date": "\nMarch 2, 2017\n"},
{"website": "Google-Security", "title": "\nExpanding protection for Chrome users on macOS\n", "author": ["Posted by Kylie McRoberts and Ryan Rasti"], "link": "https://security.googleblog.com/2017/03/expanding-protection-for-chrome-users.html", "abstract": "", "date": "\nMarch 1, 2017\n"},
{"website": "Google-Security", "title": "\nE2EMail research project has left the nest\n", "author": ["Posted by KB Sriram, Eduardo Vela Nava, and Stephan Somogyi, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/02/e2email-research-project-has-left-nest_24.html", "abstract": "", "date": "\nFebruary 24, 2017\n"},
{"website": "Google-Security", "title": "\nAnnouncing the first SHA1 collision\n", "author": ["Posted by Marc Stevens (CWI Amsterdam), Elie Bursztein (Google), Pierre Karpman (CWI Amsterdam), Ange Albertini (Google), Yarik Markov (Google), Alex Petit Bianco (Google), Clement Baisse (Google)"], "link": "https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html", "abstract": "", "date": "\nFebruary 23, 2017\n"},
{"website": "Google-Security", "title": "\nAnother option for file sharing\n", "author": ["Posted by Andrew Gerrand, Eric Grosse, Rob Pike, Eduardo Pinheiro and Dave Presotto, Google Software Engineers"], "link": "https://security.googleblog.com/2017/02/another-option-for-file-sharing.html", "abstract": "", "date": "\nFebruary 21, 2017\n"},
{"website": "Google-Security", "title": "\nUnderstanding differences between corporate and consumer Gmail threats\n", "author": ["Posted by Ali Zand and Vijay Eranti, Anti-Abuse Research and Gmail Abuse"], "link": "https://security.googleblog.com/2017/02/understanding-differences-between.html", "abstract": "", "date": "\nFebruary 16, 2017\n"},
{"website": "Google-Security", "title": "\n802.11s Security and Google Wifi\n", "author": ["Posted by Paul Devitt, Security Engineer"], "link": "https://security.googleblog.com/2017/02/80211s-security-and-google-wifi.html", "abstract": "", "date": "\nFebruary 7, 2017\n"},
{"website": "Google-Security", "title": "\nHosted S/MIME by Google provides enhanced security for Gmail in the enterprise\n", "author": ["Posted by Nicolas Kardas, Gmail Product Management and Nicolas Lidzborski, G Suite Security Engineering Lead"], "link": "https://security.googleblog.com/2017/02/hosted-smime-by-google-provides.html", "abstract": "", "date": "\nFebruary 2, 2017\n"},
{"website": "Google-Security", "title": "\nBetter and more usable protection from phishing\n", "author": ["Posted by Christiaan Brand and Guemmy Kim, Product Managers, Google Account Security"], "link": "https://security.googleblog.com/2017/02/better-and-more-usable-protection-from.html", "abstract": "", "date": "\nFebruary 1, 2017\n"},
{"website": "Google-Security", "title": "\nVulnerability Rewards Program: 2016 Year in Review\n", "author": ["Posted by Eduardo Vela Nava, VRP Technical Lead, Master of Disaster"], "link": "https://security.googleblog.com/2017/01/vulnerability-rewards-program-2016-year.html", "abstract": "", "date": "\nJanuary 30, 2017\n"},
{"website": "Google-Security", "title": "\nThe foundation of a more secure web\n", "author": ["Posted by Ryan Hurst, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/01/the-foundation-of-more-secure-web.html", "abstract": "", "date": "\nJanuary 26, 2017\n"},
{"website": "Google-Security", "title": "\nApp Security Improvements: Looking back at 2016\n", "author": [], "link": "https://security.googleblog.com/2017/01/app-security-improvements-looking-back.html", "abstract": "", "date": "\nJanuary 19, 2017\n"},
{"website": "Google-Security", "title": "\nSilence speaks louder than words when finding malware\n", "author": [], "link": "https://security.googleblog.com/2017/01/silence-speaks-louder-than-words-when.html", "abstract": "", "date": "\nJanuary 17, 2017\n"},
{"website": "Google-Security", "title": "\nSecurity Through Transparency\n", "author": ["Posted by Ryan Hurst and Gary Belvin, Security and Privacy Engineering"], "link": "https://security.googleblog.com/2017/01/security-through-transparency.html", "abstract": "", "date": "\nJanuary 12, 2017\n"},
{"website": "Google-Security", "title": "\nEnigma, The Sequel\n", "author": ["Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair"], "link": "https://security.googleblog.com/2017/01/enigma-sequel.html", "abstract": "", "date": "\nJanuary 11, 2017\n"},
{"website": "Google-Security", "title": "\nProject Wycheproof\n", "author": ["Posted by Daniel Bleichenbacher, Security Engineer and Thai Duong, Security Engineer"], "link": "https://security.googleblog.com/2016/12/project-wycheproof.html", "abstract": "", "date": "\nDecember 19, 2016\n"},
{"website": "Google-Security", "title": "\nAnnouncing OSS-Fuzz: Continuous Fuzzing for Open Source Software\n", "author": ["Posted by Mike Aizatsky, Kostya Serebryany (Software Engineers, Dynamic Tools); Oliver Chang, Abhishek Arya (Security Engineers, Google Chrome); and Meredith Whittaker (Open Research Lead)"], "link": "https://security.googleblog.com/2016/12/announcing-oss-fuzz-continuous-fuzzing.html", "abstract": "", "date": "\nDecember 1, 2016\n"},
{"website": "Google-Security", "title": "\nPixel Security: Better, Faster, Stronger\n", "author": ["Posted by Paul Crowley, Senior Software Engineer and Paul Lawrence, Senior Software Engineer"], "link": "https://security.googleblog.com/2016/11/pixel-security-better-faster-stronger.html", "abstract": "", "date": "\nNovember 17, 2016\n"},
{"website": "Google-Security", "title": "\nSHA-1 Certificates in Chrome\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2016/11/sha-1-certificates-in-chrome.html", "abstract": "", "date": "\nNovember 16, 2016\n"},
{"website": "Google-Security", "title": "\nA new site for Safe Browsing\n", "author": ["Posted by Mike Castner and Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/11/a-new-site-for-safe-browsing.html", "abstract": "", "date": "\nNovember 9, 2016\n"},
{"website": "Google-Security", "title": "\nProtecting users from repeatedly dangerous sites\n", "author": ["Posted by Brooke Heinichen, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/11/protecting-users-from-repeatedly_8.html", "abstract": "", "date": "\nNovember 8, 2016\n"},
{"website": "Google-Security", "title": "\nHere\u2019s to more HTTPS on the web!\n", "author": ["Posted by Adrienne Porter Felt and Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2016/11/heres-to-more-https-on-web.html", "abstract": "", "date": "\nNovember 3, 2016\n"},
{"website": "Google-Security", "title": "\nDistrusting WoSign and StartCom Certificates\n", "author": ["Posted by Andrew Whalley, Chrome Security"], "link": "https://security.googleblog.com/2016/10/distrusting-wosign-and-startcom.html", "abstract": "", "date": "\nOctober 31, 2016\n"},
{"website": "Google-Security", "title": "\nDisclosing vulnerabilities to protect users\n", "author": ["Posted by Neel Mehta and Billy Leonard, Threat Analysis Group"], "link": "https://security.googleblog.com/2016/10/disclosing-vulnerabilities-to-protect.html", "abstract": "", "date": "\nOctober 31, 2016\n"},
{"website": "Google-Security", "title": "\nOnHub: Powerful protection for peace of mind\n", "author": ["Posted by Chris Millikin, Public Defender (Security Engineering Manager)"], "link": "https://security.googleblog.com/2016/09/onhub-powerful-protection-for-peace-of.html", "abstract": "", "date": "\nSeptember 27, 2016\n"},
{"website": "Google-Security", "title": "\nReshaping web defenses with strict Content Security Policy\n", "author": ["Posted by Artur Janc, Michele Spagnuolo, Lukas Weichselbaum, and David Ross, Information Security Engineers"], "link": "https://security.googleblog.com/2016/09/reshaping-web-defenses-with-strict.html", "abstract": "", "date": "\nSeptember 26, 2016\n"},
{"website": "Google-Security", "title": "\nEven More Safe Browsing on Android!\n", "author": ["Posted by Stephan Somogyi, Safe Browsing Team & William Luh, Android Security Team"], "link": "https://security.googleblog.com/2016/09/even-more-safe-browsing-on-android.html", "abstract": "", "date": "\nSeptember 15, 2016\n"},
{"website": "Google-Security", "title": "\nMoving towards a more secure web\n", "author": ["Posted by Emily Schechter, Chrome Security Team"], "link": "https://security.googleblog.com/2016/09/moving-towards-more-secure-web.html", "abstract": "", "date": "\nSeptember 8, 2016\n"},
{"website": "Google-Security", "title": "\nKeeping Android safe: Security enhancements in Nougat\n", "author": ["Posted by Xiaowen Xin, Android Security Team"], "link": "https://security.googleblog.com/2016/09/keeping-android-safe-security.html", "abstract": "", "date": "\nSeptember 6, 2016\n"},
{"website": "Google-Security", "title": "\nMore Safe Browsing Help for Webmasters\n", "author": ["Posted by Kelly Hope Harrington, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/09/more-safe-browsing-help-for-webmasters.html", "abstract": "", "date": "\nSeptember 6, 2016\n"},
{"website": "Google-Security", "title": "\nGuided in-process fuzzing of Chrome components\n", "author": ["Posted by Max Moroz, ", " and Kostya Serebryany, "], "link": "https://security.googleblog.com/2016/08/guided-in-process-fuzzing-of-chrome.html", "abstract": "", "date": "\nAugust 5, 2016\n"},
{"website": "Google-Security", "title": "\nNew research: Zeroing in on deceptive software installations\n", "author": ["Posted by Kurt Thomas, Research Scientist and Juan A. Elices Crespo, Software Engineer"], "link": "https://security.googleblog.com/2016/08/new-research-zeroing-in-on-deceptive.html", "abstract": "", "date": "\nAugust 4, 2016\n"},
{"website": "Google-Security", "title": "\nAdding YouTube and Calendar to the HTTPS Transparency Report\n", "author": ["Posted by Emily Schechter, HTTPS Enthusiast"], "link": "https://security.googleblog.com/2016/08/adding-youtube-and-calendar-to-https.html", "abstract": "", "date": "\nAugust 1, 2016\n"},
{"website": "Google-Security", "title": "\nBringing HSTS to www.google.com\n", "author": ["Posted by Jay Brown, Sr. Technical Program Manager, Security"], "link": "https://security.googleblog.com/2016/07/bringing-hsts-to-wwwgooglecom.html", "abstract": "", "date": "\nJuly 29, 2016\n"},
{"website": "Google-Security", "title": "\nProtecting Android with more Linux kernel defenses\n", "author": ["Posted by Jeff Vander Stoep, Android Security team"], "link": "https://security.googleblog.com/2016/07/protecting-android-with-more-linux.html", "abstract": "", "date": "\nJuly 27, 2016\n"},
{"website": "Google-Security", "title": "\nChanges to Trusted Certificate Authorities in Android Nougat\n", "author": ["Posted by Chad Brubaker, Android Security team"], "link": "https://security.googleblog.com/2016/07/changes-to-trusted-certificate.html", "abstract": "", "date": "\nJuly 8, 2016\n"},
{"website": "Google-Security", "title": "\nExperimenting with Post-Quantum Cryptography\n", "author": ["Posted by Matt Braithwaite, Software Engineer"], "link": "https://security.googleblog.com/2016/07/experimenting-with-post-quantum.html", "abstract": "", "date": "\nJuly 7, 2016\n"},
{"website": "Google-Security", "title": "\nOne Year of Android Security Rewards\n", "author": ["Posted by Quan To, Program Manager, Android Security"], "link": "https://security.googleblog.com/2016/06/one-year-of-android-security-rewards.html", "abstract": "", "date": "\nJune 16, 2016\n"},
{"website": "Google-Security", "title": "\nEvolving the Safe Browsing API\n", "author": ["Posted by Emily Schechter and Alex Wozniak, Safe Browsing Team\u00a0"], "link": "https://security.googleblog.com/2016/05/evolving-safe-browsing-api.html", "abstract": "", "date": "\nMay 20, 2016\n"},
{"website": "Google-Security", "title": "\nHardening the media stack\n", "author": ["Posted by Dan Austin and Jeff Vander Stoep, Android Security team"], "link": "https://security.googleblog.com/2016/05/hardening-media-stack.html", "abstract": "", "date": "\nMay 5, 2016\n"},
{"website": "Google-Security", "title": "\nBringing HTTPS to all blogspot domain blogs\n", "author": ["Posted by Milinda Perera, Software Engineer, Security"], "link": "https://security.googleblog.com/2016/05/bringing-https-to-all-blogspot-domain.html", "abstract": "", "date": "\nMay 3, 2016\n"},
{"website": "Google-Security", "title": "\nProtecting against unintentional regressions to cleartext traffic in your Android apps\n", "author": ["Posted by Alex Klyubin, Android Security team"], "link": "https://security.googleblog.com/2016/04/protecting-against-unintentional.html", "abstract": "", "date": "\nApril 25, 2016\n"},
{"website": "Google-Security", "title": "\nAndroid Security 2015 Annual Report\n", "author": ["Posted by Adrian Ludwig, Lead Engineer, Android Security"], "link": "https://security.googleblog.com/2016/04/android-security-2015-annual-report.html", "abstract": "", "date": "\nApril 19, 2016\n"},
{"website": "Google-Security", "title": "\nHelping webmasters re-secure their sites\n", "author": ["Posted by Kurt Thomas and Yuan Niu, Spam & Abuse Research"], "link": "https://security.googleblog.com/2016/04/helping-webmasters-re-secure-their-sites.html", "abstract": "", "date": "\nApril 18, 2016\n"},
{"website": "Google-Security", "title": "\nGrowing Eddystone with Ephemeral Identifiers: A Privacy Aware & Secure Open Beacon Format\n", "author": ["Posted by\u00a0", "Nirdhar Khazanie, Product Manager\n    and\u00a0", "Yossi Matias, VP\n    Engineering"], "link": "https://security.googleblog.com/2016/04/growing-eddystone-with-ephemeral-identifiers.html", "abstract": "", "date": "\nApril 14, 2016\n"},
{"website": "Google-Security", "title": "\nImprovements to Safe Browsing Alerts for Network Administrators\n", "author": ["Posted by Nav Jagpal, Software Engineer"], "link": "https://security.googleblog.com/2016/04/improvements-to-safe-browsing-alerts.html", "abstract": "", "date": "\nApril 6, 2016\n"},
{"website": "Google-Security", "title": "\nMore Encryption, More Notifications, More Email Security\n", "author": ["Posted by Nicolas Lidzborski, Gmail Security Engineering Lead and Jonathan Pevarnek, ", " Engineer"], "link": "https://security.googleblog.com/2016/03/more-encryption-more-notifications-more.html", "abstract": "", "date": "\nMarch 24, 2016\n"},
{"website": "Google-Security", "title": "\nCertificate Transparency for Untrusted CAs\n", "author": ["Posted by Martin Smith, Software Engineer, Certificate Transparency"], "link": "https://security.googleblog.com/2016/03/certificate-transparency-for-untrusted.html", "abstract": "", "date": "\nMarch 21, 2016\n"},
{"website": "Google-Security", "title": "\nBinDiff now available for free\n", "author": ["Posted by Christian Blichmann, Software Engineer"], "link": "https://security.googleblog.com/2016/03/bindiff-now-available-for-free.html", "abstract": "", "date": "\nMarch 18, 2016\n"},
{"website": "Google-Security", "title": "\nSecuring the web, together\n", "author": ["Posted by Rutledge Chin Feman and Tim Willis, HTTPS Evangelists"], "link": "https://security.googleblog.com/2016/03/securing-web-together_15.html", "abstract": "", "date": "\nMarch 15, 2016\n"},
{"website": "Google-Security", "title": "\nGet Rich or Hack Tryin\u2019\n", "author": ["Posted by Nathan Parker, Chrome Defender and Tim Willis, Hacker Philanthropist"], "link": "https://security.googleblog.com/2016/03/get-rich-or-hack-tryin.html", "abstract": "", "date": "\nMarch 14, 2016\n"},
{"website": "Google-Security", "title": "\nScalable vendor security reviews\n", "author": ["Posted by Lukas Weichselbaum and Daniel Fabian, Google Security"], "link": "https://security.googleblog.com/2016/03/scalable-vendor-security-reviews.html", "abstract": "", "date": "\nMarch 7, 2016\n"},
{"website": "Google-Security", "title": "\nCVE-2015-7547: glibc getaddrinfo stack-based buffer overflow\n", "author": ["Posted by Fermin J. Serna, Staff Security Engineer and Kevin Stadmeyer, Technical Program Manager"], "link": "https://security.googleblog.com/2016/02/cve-2015-7547-glibc-getaddrinfo-stack.html", "abstract": "", "date": "\nFebruary 16, 2016\n"},
{"website": "Google-Security", "title": "\nBuilding a safer web, for everyone\n", "author": ["Posted by Gerhard Eschelbeck, VP, Security and Privacy"], "link": "https://security.googleblog.com/2016/02/building-safer-web-for-everyone.html", "abstract": "", "date": "\nFebruary 9, 2016\n"},
{"website": "Google-Security", "title": "\nNo More Deceptive Download Buttons\n", "author": ["Posted by Lucas Ballard, Safe Browsing Team"], "link": "https://security.googleblog.com/2016/02/no-more-deceptive-download-buttons.html", "abstract": "", "date": "\nFebruary 3, 2016\n"},
{"website": "Google-Security", "title": "\nGoogle Security Rewards - 2015 Year in Review\n", "author": ["Posted by Eduardo Vela Nava, Google Security"], "link": "https://security.googleblog.com/2016/01/google-security-rewards-2015-year-in.html", "abstract": "", "date": "\nJanuary 28, 2016\n"},
{"website": "Google-Security", "title": "\nWhy attend USENIX Enigma?\n", "author": ["Posted by Parisa Tabriz, Security Princess & Enigma Program Co-Chair"], "link": "https://security.googleblog.com/2016/01/why-attend-usenix-enigma.html", "abstract": "", "date": "\nJanuary 11, 2016\n"},
{"website": "Google-Security", "title": "\nAn update on SHA-1 certificates in Chrome\n", "author": ["Posted by Lucas Garron, Chrome security and David Benjamin, Chrome networking"], "link": "https://security.googleblog.com/2015/12/an-update-on-sha-1-certificates-in.html", "abstract": "", "date": "\nDecember 18, 2015\n"},
{"website": "Google-Security", "title": "\nIndexing HTTPS pages by default\n", "author": ["Posted by ", ", WTA, and the Google Security and Indexing teams"], "link": "https://security.googleblog.com/2015/12/indexing-https-pages-by-default.html", "abstract": "", "date": "\nDecember 17, 2015\n"},
{"website": "Google-Security", "title": "\nProactive measures in digital certificate security\n", "author": ["Posted by Ryan Sleevi, Software Engineer"], "link": "https://security.googleblog.com/2015/12/proactive-measures-in-digital.html", "abstract": "", "date": "\nDecember 11, 2015\n"},
{"website": "Google-Security", "title": "\nYear one: progress in the fight against Unwanted Software\n", "author": ["Posted by Moheeb Abu Rajab, Google Security Team"], "link": "https://security.googleblog.com/2015/12/year-one-progress-in-fight-against.html", "abstract": "", "date": "\nDecember 9, 2015\n"},
{"website": "Google-Security", "title": "\nA new version of Authenticator for Android\n", "author": ["Posted by Alexei Czeskis, Software Engineer"], "link": "https://security.googleblog.com/2015/12/a-new-version-of-authenticator-for.html", "abstract": "", "date": "\nDecember 7, 2015\n"},
{"website": "Google-Security", "title": "\nProtecting hundreds of millions more mobile users\n", "author": ["Posted by No\u00e9 Lutz, Nathan Parker, Stephan Somogyi; Google Chrome and Safe Browsing Teams"], "link": "https://security.googleblog.com/2015/12/protecting-hundreds-of-millions-more.html", "abstract": "", "date": "\nDecember 7, 2015\n"},
{"website": "Google-Security", "title": "\nSafe Browsing protection from even more deceptive attacks\n", "author": ["Posted by Emily Schechter, Program Manager and No\u00e9 Lutz, Software Engineer"], "link": "https://security.googleblog.com/2015/11/safe-browsing-protection-from-even-more.html", "abstract": "", "date": "\nNovember 13, 2015\n"},
{"website": "Google-Security", "title": "\nNew Research: Encouraging trends and emerging threats in email security\n", "author": [], "link": "https://security.googleblog.com/2015/11/new-research-encouraging-trends-and.html", "abstract": "", "date": "\nNovember 12, 2015\n"},
{"website": "Google-Security", "title": "\nSustaining Digital Certificate Security\n", "author": ["Posted by Ryan Sleevi, Software Engineer"], "link": "https://security.googleblog.com/2015/10/sustaining-digital-certificate-security.html", "abstract": "", "date": "\nOctober 28, 2015\n"},
{"website": "Google-Security", "title": "\nBehind the red warning: more info about online site safety\n", "author": ["Posted by\u00a0", "Adrienne Porter Felt, Chrome Security Engineer and Warning Wizard", "Emily Schechter, Safe Browsing Program Manager and Menace to Malware", "Ke Wang, Safe Browsing Engineer and Developer of Defense"], "link": "https://security.googleblog.com/2015/10/behind-red-warning-more-info-about.html", "abstract": "", "date": "\nOctober 20, 2015\n"},
{"website": "Google-Security", "title": "\nSimplifying the Page Security Icon in Chrome\n", "author": ["Posted by Lucas Garron and Chris Palmer, Chrome security team"], "link": "https://security.googleblog.com/2015/10/simplifying-page-security-icon-in-chrome.html", "abstract": "", "date": "\nOctober 13, 2015\n"},
{"website": "Google-Security", "title": "\nHTTPS support coming to Blogspot\n", "author": ["Posted by Jo-el van Bergen, Software Engineer, Security."], "link": "https://security.googleblog.com/2015/09/https-support-coming-to-blogspot.html", "abstract": "", "date": "\nSeptember 30, 2015\n"},
{"website": "Google-Security", "title": "\nNew research: The underground market fueling for-profit abuse\n", "author": ["Posted by Kurt Thomas and Elie Bursztein, Google Anti-Fraud and Abuse Research"], "link": "https://security.googleblog.com/2015/09/new-research-underground-market-fueling.html", "abstract": "", "date": "\nSeptember 24, 2015\n"},
{"website": "Google-Security", "title": "\nImproved Digital Certificate Security\n", "author": ["Posted by Stephan Somogyi, Security & Privacy PM, and Adam Eijdenberg, Certificate Transparency PM"], "link": "https://security.googleblog.com/2015/09/improved-digital-certificate-security.html", "abstract": "", "date": "\nSeptember 18, 2015\n"},
{"website": "Google-Security", "title": "\nDisabling SSLv3 and RC4\n", "author": ["Posted by Adam Langley, Security Engineer", "As the ", " ", " transition to SHA-256 certificates is nearing completion, we are planning the next changes to Google\u2019s TLS configuration. As part of those changes, we expect to disable support for SSLv3 and RC4 in the medium term.", "SSLv3 has been ", " for over 16 years and is so full of known problems that the IETF has decided that it ", ". RC4 is a 28 year old cipher that has done remarkably well, but is now the subject of ", " ", " at security conferences. The IETF has decided that RC4 also warrants a statement that it too ", ".", "Because of these issues we expect to disable both SSLv3 and RC4 support at Google\u2019s frontend servers and, over time, across our products in general, including Chrome, Android, our webcrawlers and our SMTP servers. (Indeed, SSLv3 support has already been removed from Chrome.) The ", " survey of the top 200,000 HTTPS sites finds that, already, 42% of sites have disabled RC4 and 65% of sites have disabled SSLv3.", "If your TLS client, webserver or email server requires the use of SSLv3 or RC4 then the time to update was some years ago, but better late than never. However, note that just because you might be using RC4 today doesn\u2019t mean that your client or website will stop working: TLS can negotiate cipher suites and problems will only occur if you don\u2019t support anything but RC4. (Although if you\u2019re using SSLv3 today then things will stop working when we disable it because SSLv3 is already a last resort.)", "Google's frontend servers do a lot more than terminate connections for browsers these days; there are also lots of embedded systems talking to Google using TLS. In order to reduce the amount of work that the deprecation of outdated cryptography causes, we are also announcing suggested minimum standards for TLS clients today. This applies to TLS clients in general: certainly those that are using TLS as part of HTTPS, but also, for example, SMTP servers using STARTTLS.", "We can't predict the future, but devices that meet these requirements are likely to be able to continue functioning without changes to their TLS configuration up to 2020. You should expect these standards to be required in cases where Google runs certification programs, but it\u2019s a very good idea to meet them anyway.", "Devices that don\u2019t meet these standards aren\u2019t going to stop working anytime soon (unless they depend on RC4 or SSLv3\u2014see above), but they might be affected by further TLS changes in the coming years.", "Specifically, we are requiring:"], "link": "https://security.googleblog.com/2015/09/disabling-sslv3-and-rc4.html", "abstract": "", "date": "\nSeptember 17, 2015\n"},
{"website": "Google-Security", "title": "\nCutting unwanted ad injectors out of advertising\n", "author": ["Posted by Vegard Johnsen, Product Manager, Google Ads Traffic Quality"], "link": "https://security.googleblog.com/2015/09/cutting-unwanted-ad-injectors-out-of.html", "abstract": "", "date": "\nSeptember 10, 2015\n"},
{"website": "Google-Security", "title": "\nSay hello to the Enigma conference\n", "author": [], "link": "https://security.googleblog.com/2015/08/say-hello-to-enigma-conference.html", "abstract": "", "date": "\nAugust 18, 2015\n"},
{"website": "Google-Security", "title": "\nNew research: Comparing how security experts and non-experts stay safe online\n", "author": ["Posted by\u00a0"], "link": "https://security.googleblog.com/2015/07/new-research-comparing-how-security.html", "abstract": "", "date": "\nJuly 23, 2015\n"},
{"website": "Google-Security", "title": "\nWorking Together to Filter Automated Data-Center Traffic\n", "author": ["Posted by Vegard Johnsen, Product Manager Google Ad Traffic Quality", "Today the ", " (TAG) ", " a new pilot blacklist to protect advertisers across the industry. This blacklist comprises data-center IP addresses associated with non-human ad requests. We're happy to support this effort along with other industry leaders\u2014Dstillery, Facebook, MediaMath, Quantcast, Rubicon Project, TubeMogul and Yahoo\u2014and contribute our own data-center blacklist. As mentioned to ", " and in our recent ", ", we believe that if we work together we can raise the fraud-fighting bar for the whole industry.", "Data-center traffic is one of ", " of non-human or illegitimate ad traffic. The newly shared blacklist identifies web robots or \u201cbots\u201d that are being run in data centers but that avoid detection by the ", ". Well-behaved bots announce that they're bots as they surf the web by including a bot identifier in their declared User-Agent strings. The bots filtered by this new blacklist are different. They masquerade as human visitors by using User-Agent strings that are indistinguishable from those of typical web browsers.", "In this post, we take a closer look at a few examples of data-center traffic to show why it\u2019s so important to filter this traffic across the industry.", "When observing the traffic generated by the IP addresses in the newly shared blacklist, we found significantly distorted click metrics. In May of 2015 on DoubleClick Campaign Manager alone, we found the blacklist filtered 8.9% of all clicks. Without filtering these clicks from campaign metrics, advertiser click-through rates would have been incorrect and for some advertisers this error would have been very large.", "Below is a plot that shows how much click-through rates in May would have been inflated across the most impacted of DoubleClick Campaign Manager\u2019s larger advertisers.", "There are two distinct types of invalid data-center traffic: where the intent is malicious and where the impact on advertisers is accidental. In this section we consider two interesting examples where we\u2019ve observed traffic that was likely generated with malicious intent.", "Publishers use many different strategies to increase the traffic to their sites. Unfortunately, some are willing to use any means necessary to do so. In our investigations we\u2019ve seen instances where publishers have been running software tools in data centers to intentionally mislead advertisers with fake impressions and fake clicks.", "UrlSpirit is just one example of software that some unscrupulous publishers have been using to collaboratively drive automated traffic to their websites. Participating publishers install the UrlSpirit application on Windows machines and they each submit up to three URLs through the application\u2019s interface. Submitted URLs are then distributed to other installed instances of the application, where Internet Explorer is used to automatically visit the list of target URLs. Publishers who have not installed the application can also leverage the network of installations by paying a fee.", "At the end of May more than 82% of the UrlSpirit installations were being run on machines in data centers. There were more than 6,500 data-center installations of UrlSpirit, with each data-center installation running in a separate virtual machine. In aggregate, the data-center installations of UrlSpirit were generating a monthly rate of at least half a billion ad requests\u2014 an average of 2,500 fraudulent ad requests per installation per day.", "HitLeap is another example of software that some publishers are using to collaboratively drive automated traffic to their websites. The software also runs on Windows machines, and each instance uses the Chromium Embedded Framework to automatically browse the websites of participating publishers\u2014rather than using Internet Explorer.", "Before publishers can use the network of installations to drive traffic to their websites, they need browsing minutes. Participating publishers earn browsing minutes by running the application on their computers. Alternatively, they can simply buy browsing minutes\u2014with bundles starting at $9 for 10,000 minutes or up to 1,000,000 minutes for $625.", "Publishers can specify as many target URLs as they like. The number of visits they receive from the network of installations is a function of how long they want the network of bots to spend on their sites. For example, ten browsing minutes will get a publisher five visits if the publisher requests two-minute visit durations.", "In mid-June, at least 4,800 HitLeap installations were being run in virtual machines in data centers, with a unique IP associated with each HitLeap installation. The data-center installations of HitLeap made up 16% of the total HitLeap network, which was substantially larger than the UrlSpirit network.\u00a0", "In aggregate, the data-center installations of HitLeap were generating a monthly rate of at least a billion fraudulent ad requests\u2014or an average of 1,600 ad requests per installation per day.", "Not only were these publishers collectively responsible for billions of automated ad requests, but their websites were also often extremely deceptive. For example, of the top ten webpages visited by HitLeap bots in June, nine of these included ", " -- meaning that not only was the traffic fake, but the ads couldn\u2019t have been seen even if they had been legitimate human visitors.", " is illustrative of these nine webpages with hidden ad slots. The webpage has no visible content other than a single 300\u00d7250px ad. This visible ad is actually in a 300\u00d7250px iframe that includes two ads, the second of which is hidden. Additionally, there are also twenty-seven 0\u00d70px hidden iframes on this page with each hidden iframe including two ad slots. In total there are fifty-five hidden ads on this page and one visible ad. Finally, the ads served on ", " appear to advertisers as though they have been served on legitimate websites like indiatimes.com, scotsman.com, autotrader.co.uk, allrecipes.com, dictionary.com and nypost.com, because the tags used on ", " to request the ad creatives have been deliberately spoofed.", "Unlike the traffic described above, there is also automated data-center traffic that impacts advertising campaigns but that hasn\u2019t been generated for malicious purposes. An interesting example of this is an advertising competitive intelligence company that is generating a large volume of undeclared non-human traffic.", "This company uses bots to scrape the web to find out which ad creatives are being served on which websites and at what scale. The company\u2019s scrapers also click ad creatives to analyse the landing page destinations. To provide its clients with the most accurate possible intelligence, this company\u2019s scrapers operate at extraordinary scale and they also do so without including bot identifiers in their User-Agent strings.", "While the aim of this company is not to cause advertisers to pay for fake traffic, the company\u2019s scrapers do waste advertiser spend. They not only generate non-human impressions; they also distort the metrics that advertisers use to evaluate campaign performance\u2014in particular, click metrics. Looking at the data across DoubleClick Campaign Manager this company\u2019s scrapers were responsible for 65% of the automated data-center clicks recorded in the month of May.", "Google has always invested to prevent this and other types of invalid traffic from entering our ad platforms. By contributing our data-center blacklist to TAG, we hope to help others in the industry protect themselves.", "We\u2019re excited by the collaborative spirit we\u2019ve seen working with other industry leaders on this initiative. This is an important, early step toward tackling fraudulent and illegitimate inventory across the industry and we look forward to sharing more in the future. By pooling our collective efforts and working with industry bodies, we can create strong defenses against those looking to take advantage of our ecosystem. We look forward to working with the TAG Anti-fraud working group to turn this pilot program into an industry-wide tool."], "link": "https://security.googleblog.com/2015/07/working-together-to-filter-automated.html", "abstract": "", "date": "\nJuly 21, 2015\n"},
{"website": "Google-Security", "title": "\nGoogle, the Wassenaar Arrangement, and vulnerability research\n", "author": ["Posted by\u00a0", "Neil Martin, Export Compliance Counsel,\u00a0", "As the usage and complexity of software grows, the importance of security research has grown with it. It\u2019s through diligent research that we uncover and fix bugs \u2014 like ", " and ", " \u2014 that can cause serious security issues for web users around the world.", "The time and effort it takes to uncover bugs is significant, and the marketplace for these vulnerabilities is competitive. That\u2019s why we provide cash rewards for quality security research that identifies problems in our own products or proactive improvements to open-source products. We\u2019ve ", " more than $4 million to researchers from all around the world - our current Hall of Fame includes researchers from Germany, the U.S., Japan, Brazil, and more than 30 other countries.", "With the benefits of security research in mind, there has been some public ", " and ", " around ", " put forth by the U.S. Department of Commerce that would negatively affect vulnerability research.", "The Commerce Department's proposed rules stem from U.S. membership in the ", ", a multilateral export control association. Members of the Wassenaar Arrangement have agreed to control a wide range of goods, software, and information, including technologies relating to \"intrusion software\" (as they've defined that term).", "We believe that these proposed rules, as currently written, would have a significant negative impact on the open security research community. They would also hamper our ability to defend ourselves, our users, and make the web safer. It would be a disastrous outcome if an export regulation intended to make people more secure resulted in billions of users across the globe becoming persistently less secure.", "Earlier today, we formally submitted comments on the proposed rules to the United States Commerce Department\u2019s Bureau of Industry and Security (BIS). Our comments are lengthy, but we wanted to share some of the main concerns and questions that we have officially expressed to the U.S. government today:"], "link": "https://security.googleblog.com/2015/07/google-wassenaar-arrangement-and.html", "abstract": "", "date": "\nJuly 20, 2015\n"},
{"website": "Google-Security", "title": "\nMore Visible Protection Against Unwanted Software\n", "author": ["Posted by Moheeb Abu Rajab and Stephan Somogyi, Google Safe Browsing Team"], "link": "https://security.googleblog.com/2015/07/more-visible-protection-against.html", "abstract": "", "date": "\nJuly 16, 2015\n"},
{"website": "Google-Security", "title": "\nAnnouncing Security Rewards for Android\n", "author": ["Posted by Jon Larimer, Android Security Engineer"], "link": "https://security.googleblog.com/2015/06/announcing-security-rewards-for-android.html", "abstract": "", "date": "\nJune 16, 2015\n"},
{"website": "Google-Security", "title": "\nNew Research: Some Tough Questions for \u2018Security Questions\u2019\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead and Ilan Caron, Software Engineer", "What do these seemingly random questions have in common? They\u2019re all familiar examples of \u2018security questions\u2019. Chances are you\u2019ve had to answer one these before; many online services use them to help users recover access to accounts if they forget their passwords, or as an additional layer of security to ", ".", "But, despite the prevalence of security questions, their safety and effectiveness have rarely been studied in depth. As part of our constant efforts to improve account security, we analyzed hundreds of millions of secret questions and answers that had been used for millions of account recovery claims at Google. We then worked to measure the likelihood that hackers could guess the answers.", "Our findings, summarized in a ", " that we recently presented at ", ", led us to conclude that secret questions are neither secure nor reliable enough to be used as a standalone account recovery mechanism. That\u2019s because they suffer from a fundamental flaw: their answers are either somewhat secure or easy to remember\u2014but rarely both.", "Not surprisingly, easy-to-remember answers are less secure. Easy answers often contain commonly known or publicly available information, or are in a small set of possible answers for cultural reasons (ie, a common family name in certain countries).", "Here are some specific insights:"], "link": "https://security.googleblog.com/2015/05/new-research-some-tough-questions-for.html", "abstract": "", "date": "\nMay 21, 2015\n"},
{"website": "Google-Security", "title": "\nNew Research: The Ad Injection Economy\n", "author": ["Posted by Kurt Thomas, Spam & Abuse Research", "In March, we ", " the problems with unwanted ad injectors, a common symptom of ", ". Ad injectors are programs that insert new ads, or replace existing ones, into the pages you visit while browsing the web. We\u2019ve received more than 100,000 user complaints about them in Chrome since the beginning of 2015\u2014more than any other issue. Unwanted ad injectors are not only annoying, they can pose ", " to users as well."], "link": "https://security.googleblog.com/2015/05/new-research-ad-injection-economy.html", "abstract": "", "date": "\nMay 6, 2015\n"},
{"website": "Google-Security", "title": "\nProtect your Google Account with Password Alert\n", "author": ["Posted by Drew Hintz, Security Engineer and Justin Kosslyn, Google Ideas", "\n"], "link": "https://security.googleblog.com/2015/04/protect-your-google-account-with.html", "abstract": "", "date": "\nApril 29, 2015\n"},
{"website": "Google-Security", "title": "\nA Javascript-based DDoS Attack as seen by Safe Browsing\n", "author": ["Posted by Niels Provos, Distinguished Engineer, Security Team", "To protect users from malicious content, ", " infrastructure analyzes web pages with web browsers running in virtual machines. This allows us to determine if a page contains malicious content, such as Javascript meant to exploit user machines. While machine learning algorithms select which web pages to inspect, we analyze millions of web pages every day and achieve good coverage of the web in general.", "In the middle of March, ", " ", " reported a large Distributed Denial-of-Service attack against the censorship monitoring organization GreatFire. ", " have extensively analyzed this DoS attack and found it novel because it was conducted by a network operator that intercepted benign web content to inject malicious Javascript. In this particular case, Javascript and HTML resources hosted on ", " were replaced with Javascript that would repeatedly request resources from the attacked domains.", "While Safe Browsing does not observe traffic at the network level, it affords good visibility at the HTTP protocol level. As such our infrastructure picked up this attack, too. Using Safe Browsing data, we can provide a more complete timeline of the attack and shed light on what injections occurred when.", "For this blog post, we analyzed data from March 1st to April 15th 2015. Safe Browsing first noticed injected content against ", " domains on March 3rd, 2015. The last time we observed injections during our measurement period was on April 7th, 2015. This is visible in the graph below, which plots the number of injections over time as a percentage of all injections observed:"], "link": "https://security.googleblog.com/2015/04/a-javascript-based-ddos-attack-as-seen.html", "abstract": "", "date": "\nApril 24, 2015\n"},
{"website": "Google-Security", "title": "\nAds Take a Step Towards \u201cHTTPS Everywhere\u201d\n", "author": ["Posted by\u00a0", "Neal Mohan, VP Product Management, Display and Video Ads", "Jerry Dischler, VP Product Management, AdWords"], "link": "https://security.googleblog.com/2015/04/ads-take-step-towards-https-everywhere.html", "abstract": "", "date": "\nApril 17, 2015\n"},
{"website": "Google-Security", "title": "\nBeyond annoyance: security risks of unwanted ad injectors\n", "author": ["Posted by Eric Severance, Software Engineer, Safe Browsing\u00a0"], "link": "https://security.googleblog.com/2015/04/beyond-annoyance-security-risks-of.html", "abstract": "", "date": "\nApril 16, 2015\n"},
{"website": "Google-Security", "title": "\nAndroid Security State of the Union 2014\n", "author": ["Posted by Adrian Ludwig, Lead Engineer for Android Security"], "link": "https://security.googleblog.com/2015/04/android-security-state-of-union-2014.html", "abstract": "", "date": "\nApril 2, 2015\n"},
{"website": "Google-Security", "title": "\nOut with unwanted ad injectors\n", "author": ["Posted by Nav Jagpal, Software Engineer, Safe Browsing"], "link": "https://security.googleblog.com/2015/03/out-with-unwanted-ad-injectors.html", "abstract": "", "date": "\nMarch 31, 2015\n"},
{"website": "Google-Security", "title": "\nEven more unwanted software protection via the Safe Browsing API\n", "author": ["Posted by Emily Schechter, Safe Browsing Program Manager"], "link": "https://security.googleblog.com/2015/03/even-more-unwanted-software-protection.html", "abstract": "", "date": "\nMarch 24, 2015\n"},
{"website": "Google-Security", "title": "\nMaintaining digital certificate security\n", "author": ["Posted by Adam Langley, Security Engineer\u00a0", "On Friday, March 20th, we became aware of unauthorized digital certificates for several Google domains. The certificates were issued by an intermediate certificate authority apparently held by a company called ", ". This intermediate certificate was issued by ", ".\u00a0", "CNNIC is included in all major root stores and so the misissued certificates would be trusted by almost all browsers and operating systems. Chrome on Windows, OS X, and Linux, ChromeOS, and Firefox 33 and greater would have rejected these certificates because of ", ", although misissued certificates for other sites likely exist.", "We promptly alerted CNNIC and other major browsers about the incident, and we blocked the MCS Holdings certificate in Chrome with a ", " push. CNNIC responded on the 22nd to explain that they had contracted with MCS Holdings on the basis that MCS would only issue certificates for domains that they had registered. However, rather than keep the private key in a suitable ", ", MCS installed it in a man-in-the-middle proxy. These devices intercept secure connections by masquerading as the intended destination and are sometimes used by companies to intercept their employees\u2019 secure traffic for monitoring or legal reasons. The employees\u2019 computers normally have to be configured to trust a proxy for it to be able to do this. However, in this case, the presumed proxy was given the full authority of a public CA, which is a serious breach of the CA system. This situation is similar to ", " in 2013.", "This explanation is congruent with the facts. However, CNNIC still delegated their substantial authority to an organization that was not fit to hold it.\u00a0", "Chrome users do not need to take any action to be protected by the CRLSet updates. We have no indication of abuse and we are not suggesting that people change passwords or take other action. At this time we are considering what further actions are appropriate.", "This event also highlights, again, that the ", " effort is critical for protecting the security of certificates in the future.", "(Details of the certificate chain for software vendors can be found ", ".)", ": As a result of a joint investigation of the events surrounding this incident by Google and CNNIC, we have decided that the CNNIC Root and EV CAs will no longer be recognized in Google products. This will take effect in a future Chrome update. To assist customers affected by this decision, for a limited time we will allow CNNIC\u2019s existing certificates to continue to be marked as trusted in Chrome, through the use of a publicly disclosed whitelist. While neither we nor CNNIC believe any further unauthorized digital certificates have been issued, nor do we believe the misissued certificates were used outside the limited scope of MCS Holdings\u2019 test network, CNNIC will be working to prevent any future incidents. CNNIC will implement Certificate Transparency for all of their certificates prior to any request for reinclusion. We applaud CNNIC on their proactive steps, and welcome them to reapply once suitable technical and procedural controls are in place."], "link": "https://security.googleblog.com/2015/03/maintaining-digital-certificate-security.html", "abstract": "", "date": "\nMarch 23, 2015\n"},
{"website": "Google-Security", "title": "\nSafe Browsing and Google Analytics: Keeping More Users Safe, Together\n", "author": ["Posted by Stephan Somogyi, Product Manager, Security and Privacy", "If you run a web site, you may already be familiar with ", " and how it lets you know if Safe Browsing finds something problematic on your site. For example, we\u2019ll notify you if your site is delivering malware, which is usually a sign that it\u2019s been hacked. We\u2019re extending our Safe Browsing protections to automatically display notifications to all Google Analytics users via familiar ", ".", " has been protecting people across the Internet for over eight years and we're always looking for ways to extend that protection even further. Notifications like these help webmasters like you act quickly to respond to any issues. Fast response helps keep your site\u2014and your visitors\u2014safe."], "link": "https://security.googleblog.com/2015/02/safe-browsing-and-google-analytics.html", "abstract": "", "date": "\nFebruary 26, 2015\n"},
{"website": "Google-Security", "title": "\nPwnium V: the never-ending* Pwnium\n", "author": ["Posted by Tim Willis, Hacker Philanthropist, Chrome Security Team"], "link": "https://security.googleblog.com/2015/02/pwnium-v-never-ending-pwnium.html", "abstract": "", "date": "\nFebruary 24, 2015\n"},
{"website": "Google-Security", "title": "\nMore Protection from Unwanted Software\n", "author": ["Posted by Lucas Ballard, Software Engineer"], "link": "https://security.googleblog.com/2015/02/more-protection-from-unwanted-software.html", "abstract": "", "date": "\nFebruary 23, 2015\n"},
{"website": "Google-Security", "title": "\nUsing Google Cloud Platform for Security Scanning\n", "author": ["Posted by Rob Mann, Security Engineering Manager"], "link": "https://security.googleblog.com/2015/02/using-google-cloud-platform-for.html", "abstract": "", "date": "\nFebruary 19, 2015\n"},
{"website": "Google-Security", "title": "\nFeedback and data-driven updates to Google\u2019s disclosure policy\n", "author": [], "link": "https://security.googleblog.com/2015/02/feedback-and-data-driven-updates-to.html", "abstract": "", "date": "\nFebruary 13, 2015\n"},
{"website": "Google-Security", "title": "\nSecurity Reward Programs: Year in Review, Year in Preview\n", "author": ["Posted by Eduardo Vela Nava, Security Engineer\u00a0"], "link": "https://security.googleblog.com/2015/01/security-reward-programs-year-in-review.html", "abstract": "", "date": "\nJanuary 30, 2015\n"},
{"website": "Google-Security", "title": "\nAn Update to End-To-End\n", "author": ["Posted by Stephan Somogyi, Product Manager, Security and Privacy"], "link": "https://security.googleblog.com/2014/12/an-update-to-end-to-end.html", "abstract": "", "date": "\nDecember 16, 2014\n"},
{"website": "Google-Security", "title": "\nReject the Unexpected - Content Security Policy in Gmail\n", "author": ["Posted by Danesh Irani, Software Engineer, Gmail Security"], "link": "https://security.googleblog.com/2014/12/reject-unexpected-content-security.html", "abstract": "", "date": "\nDecember 16, 2014\n"},
{"website": "Google-Security", "title": "\nAre you a robot? Introducing \u201cNo CAPTCHA reCAPTCHA\u201d\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2014/12/are-you-robot-introducing-no-captcha.html", "abstract": "", "date": "\nDecember 3, 2014\n"},
{"website": "Google-Security", "title": "\nReady, aim, fire: an open-source tool to test web security scanners\n", "author": [], "link": "https://security.googleblog.com/2014/11/ready-aim-fire-open-source-tool-to-test.html", "abstract": "", "date": "\nNovember 18, 2014\n"},
{"website": "Google-Security", "title": "\nBehind enemy lines in our war against account hijackers\n", "author": [], "link": "https://security.googleblog.com/2014/11/behind-enemy-lines-in-our-war-against.html", "abstract": "", "date": "\nNovember 6, 2014\n"},
{"website": "Google-Security", "title": "\nIntroducing nogotofail\u2014a network traffic security testing tool\n", "author": [], "link": "https://security.googleblog.com/2014/11/introducing-nogotofaila-network-traffic.html", "abstract": "", "date": "\nNovember 4, 2014\n"},
{"website": "Google-Security", "title": "\nLearning statistics with privacy, aided by the flip of a coin\n", "author": [], "link": "https://security.googleblog.com/2014/10/learning-statistics-with-privacy-aided.html", "abstract": "", "date": "\nOctober 30, 2014\n"},
{"website": "Google-Security", "title": "\nStrengthening 2-Step Verification with Security Key\n", "author": [], "link": "https://security.googleblog.com/2014/10/strengthening-2-step-verification-with.html", "abstract": "", "date": "\nOctober 21, 2014\n"},
{"website": "Google-Security", "title": "\nThis POODLE bites: exploiting the SSL 3.0 fallback\n", "author": [], "link": "https://security.googleblog.com/2014/10/this-poodle-bites-exploiting-ssl-30.html", "abstract": "", "date": "\nOctober 14, 2014\n"},
{"website": "Google-Security", "title": "\nNews from the land of patch rewards\n", "author": [], "link": "https://security.googleblog.com/2014/10/news-from-land-of-patch-rewards.html", "abstract": "", "date": "\nOctober 9, 2014\n"},
{"website": "Google-Security", "title": "\nAn update to SafeSearch options for network administrators\n", "author": [], "link": "https://security.googleblog.com/2014/10/an-update-to-safesearch-options-for.html", "abstract": "", "date": "\nOctober 2, 2014\n"},
{"website": "Google-Security", "title": "\nFewer bugs, mo\u2019 money\n", "author": [], "link": "https://security.googleblog.com/2014/09/fewer-bugs-mo-money.html", "abstract": "", "date": "\nSeptember 30, 2014\n"},
{"website": "Google-Security", "title": "\nSecurity for the people\n", "author": [], "link": "https://security.googleblog.com/2014/09/security-for-people.html", "abstract": "", "date": "\nSeptember 18, 2014\n"},
{"website": "Google-Security", "title": "\nCleaning up after password dumps\n", "author": [], "link": "https://security.googleblog.com/2014/09/cleaning-up-after-password-dumps.html", "abstract": "", "date": "\nSeptember 10, 2014\n"},
{"website": "Google-Security", "title": "\nGradually sunsetting SHA-1\n", "author": [], "link": "https://security.googleblog.com/2014/09/gradually-sunsetting-sha-1.html", "abstract": "", "date": "\nSeptember 5, 2014\n"},
{"website": "Google-Security", "title": "\nThat\u2019s not the download you\u2019re looking for...\n", "author": [], "link": "https://security.googleblog.com/2014/08/thats-not-download-youre-looking-for.html", "abstract": "", "date": "\nAugust 14, 2014\n"},
{"website": "Google-Security", "title": "\nProtecting Gmail in a global world\n", "author": [], "link": "https://security.googleblog.com/2014/08/protecting-gmail-in-global-world.html", "abstract": "", "date": "\nAugust 12, 2014\n"},
{"website": "Google-Security", "title": "\nHTTPS as a ranking signal\n", "author": [], "link": "https://security.googleblog.com/2014/08/https-as-ranking-signal_6.html", "abstract": "", "date": "\nAugust 6, 2014\n"},
{"website": "Google-Security", "title": "\nAnnouncing Project Zero\n", "author": [], "link": "https://security.googleblog.com/2014/07/announcing-project-zero.html", "abstract": "", "date": "\nJuly 15, 2014\n"},
{"website": "Google-Security", "title": "\nMaintaining digital certificate security\n", "author": [], "link": "https://security.googleblog.com/2014/07/maintaining-digital-certificate-security.html", "abstract": "", "date": "\nJuly 8, 2014\n"},
{"website": "Google-Security", "title": "\nGoogle Drive update to protect to shared links\n", "author": [], "link": "https://security.googleblog.com/2014/06/google-drive-update-to-protect-to.html", "abstract": "", "date": "\nJune 27, 2014\n"},
{"website": "Google-Security", "title": "\nSee what your apps & extensions have been up to\n", "author": [], "link": "https://security.googleblog.com/2014/06/see-what-your-apps-extensions-have-been.html", "abstract": "", "date": "\nJune 10, 2014\n"},
{"website": "Google-Security", "title": "\nMaking end-to-end encryption easier to use\n", "author": [], "link": "https://security.googleblog.com/2014/06/making-end-to-end-encryption-easier-to.html", "abstract": "", "date": "\nJune 3, 2014\n"},
{"website": "Google-Security", "title": "\nSpeeding up and strengthening HTTPS connections for Chrome on Android\n", "author": ["Posted by Elie Bursztein, Anti-Abuse Research Lead"], "link": "https://security.googleblog.com/2014/04/speeding-up-and-strengthening-https.html", "abstract": "", "date": "\nApril 24, 2014\n"},
{"website": "Google-Security", "title": "\nNew Security Measures Will Affect Older (non-OAuth 2.0) Applications \n", "author": ["Posted by Antonio Fuentes, Product Manager, Google Identity Team"], "link": "https://security.googleblog.com/2014/04/new-security-measures-will-affect-older.html", "abstract": "", "date": "\nApril 23, 2014\n"},
{"website": "Google-Security", "title": "\nStreet View and reCAPTCHA technology just got smarter \n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA\u00a0"], "link": "https://security.googleblog.com/2014/04/street-view-and-recaptcha-technology.html", "abstract": "", "date": "\nApril 16, 2014\n"},
{"website": "Google-Security", "title": "\nGoogle Services Updated to Address OpenSSL CVE-2014-0160  (the Heartbleed bug)\n", "author": ["Posted by Matthew O'Connor, Product Manager"], "link": "https://security.googleblog.com/2014/04/google-services-updated-to-address.html", "abstract": "", "date": "\nApril 9, 2014\n"},
{"website": "Google-Security", "title": "\nGoogle\u2019s Public DNS intercepted in Turkey\n", "author": ["Posted by Steven Carstensen, Software Engineer"], "link": "https://security.googleblog.com/2014/03/googles-public-dns-intercepted-in-turkey.html", "abstract": "", "date": "\nMarch 29, 2014\n"},
{"website": "Google-Security", "title": "\nIf you could tell a user three things to do to stay safe online, what would they be?\n", "author": ["Posted by Rob Reeder, User Experience Research Team"], "link": "https://security.googleblog.com/2014/03/if-you-could-tell-user-three-things-to.html", "abstract": "", "date": "\nMarch 26, 2014\n"},
{"website": "Google-Security", "title": "\nStaying at the forefront of email security and reliability: HTTPS-only and 99.978 percent availability\n", "author": ["Posted by Nicolas Lidzborski, Gmail Security Engineering Lead"], "link": "https://security.googleblog.com/2014/03/staying-at-forefront-of-email-security.html", "abstract": "", "date": "\nMarch 20, 2014\n"},
{"website": "Google-Security", "title": "\nCAPTCHAs that capture your heart\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2014/02/captchas-that-capture-your-heart.html", "abstract": "", "date": "\nFebruary 14, 2014\n"},
{"website": "Google-Security", "title": "\nSecurity Reward Programs Update\n", "author": ["Posted by Eduardo Vela Nava and Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2014/02/security-reward-programs-update.html", "abstract": "", "date": "\nFebruary 4, 2014\n"},
{"website": "Google-Security", "title": "\nKeeping YouTube Views Authentic\n", "author": ["Posted by Philipp Pfeiffenberger, Software Engineer"], "link": "https://security.googleblog.com/2014/02/keeping-youtube-views-authentic.html", "abstract": "", "date": "\nFebruary 4, 2014\n"},
{"website": "Google-Security", "title": "\nFFmpeg and a thousand fixes\n", "author": ["Posted by Mateusz Jurczyk and Gynvael Coldwind, Information Security Engineers"], "link": "https://security.googleblog.com/2014/01/ffmpeg-and-thousand-fixes.html", "abstract": "", "date": "\nJanuary 10, 2014\n"},
{"website": "Google-Security", "title": "\nFurther improving digital certificate security \n", "author": ["Posted by Adam Langley, Security Engineer"], "link": "https://security.googleblog.com/2013/12/further-improving-digital-certificate.html", "abstract": "", "date": "\nDecember 7, 2013\n"},
{"website": "Google-Security", "title": "\nInternet-wide efforts to fight email phishing are working\n", "author": ["Posted by Elie Bursztein, anti-abuse research lead and Vijay Eranti, Gmail anti-abuse technical lead"], "link": "https://security.googleblog.com/2013/12/internet-wide-efforts-to-fight-email.html", "abstract": "", "date": "\nDecember 6, 2013\n"},
{"website": "Google-Security", "title": "\nEven more patch rewards!\n", "author": ["Posted by Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2013/11/even-more-patch-rewards.html", "abstract": "About a month ago, we kicked off our . The goal is very simple: to recognize and reward proactive security improvements to third-party open-source projects that are vital to the health of the entire Internet.We started with a fairly conservative scope, but said we would expand the program soon. Today, we are adding the following to the list of projects that are eligible for rewards:For more information about eligibility, reward amounts, and the submission process, please visit . Happy patching!", "date": "\nNovember 18, 2013\n"},
{"website": "Google-Security", "title": "\nOut with the old: Stronger certificates with Google Internet Authority G2\n", "author": ["Posted by Dan Dulay, Security Engineer"], "link": "https://security.googleblog.com/2013/11/out-with-old-stronger-certificates-with.html", "abstract": "", "date": "\nNovember 18, 2013\n"},
{"website": "Google-Security", "title": "\nA roster of TLS cipher suites weaknesses\n", "author": ["Posted by Adam Langley, Software Engineer"], "link": "https://security.googleblog.com/2013/11/a-roster-of-tls-cipher-suites-weaknesses.html", "abstract": "", "date": "\nNovember 14, 2013\n"},
{"website": "Google-Security", "title": "\nDon\u2019t mess with my browser! \n", "author": [], "link": "https://security.googleblog.com/2013/10/dont-mess-with-my-browser.html", "abstract": "", "date": "\nOctober 31, 2013\n"},
{"website": "Google-Security", "title": "\nreCAPTCHA just got easier (but only if you\u2019re human)\n", "author": ["Posted by Vinay Shet, Product Manager, reCAPTCHA"], "link": "https://security.googleblog.com/2013/10/recaptcha-just-got-easier-but-only-if.html", "abstract": "", "date": "\nOctober 25, 2013\n"},
{"website": "Google-Security", "title": "\nGoing beyond vulnerability rewards\n", "author": ["Posted by Michal Zalewski, Google Security Team"], "link": "https://security.googleblog.com/2013/10/going-beyond-vulnerability-rewards.html", "abstract": "We all benefit from the amazing volunteer work done by the open source community. That\u2019s why we keep asking ourselves how to take the model pioneered with our  - and employ it to improve the security of key third-party software critical to the health of the entire Internet.We thought about simply kicking off an OSS bug-hunting program, but this approach can easily backfire. In addition to valid reports, bug bounties invite a significant volume of spurious traffic - enough to completely overwhelm a small community of volunteers. On top of this, fixing a problem often requires more effort than finding it.So we decided to try something new: provide financial incentives for down-to-earth, proactive improvements that go beyond merely fixing a known security bug. Whether you want to switch to a more secure allocator, to add privilege separation, to clean up a bunch of sketchy calls to strcat(), or even just to enable ASLR - we want to help!We intend to roll out the program gradually, based on the quality of the received submissions and the feedback from the developer community. For the initial run, we decided to limit the scope to the following projects:Before participating, please read the official rules posted on ; the document provides additional information about eligibility, rewards, and other important stuff.Please submit your patches directly to the maintainers of the individual projects. Once your patch is accepted and merged into the repository, please follow the submission process outlined . If we think that the submission has a demonstrable, positive impact on the security of the project, you will qualify for a reward ranging from $500 to $3,133.7. Happy patching!", "date": "\nOctober 9, 2013\n"},
{"website": "Google-Security", "title": "\nSecurity rewards at Google: Two MEEELLION Dollars Later\n", "author": ["Posted by Chris Evans and Adam Mein, Masters of Coin"], "link": "https://security.googleblog.com/2013/08/security-rewards-at-google-two.html", "abstract": "", "date": "\nAugust 12, 2013\n"},
{"website": "Google-Security", "title": "\nTransparency Report: Making the web a safer place\n", "author": ["Posted by Lucas Ballard, Software Engineer"], "link": "https://security.googleblog.com/2013/06/transparency-report-making-web-safer.html", "abstract": "", "date": "\nJune 25, 2013\n"},
{"website": "Google-Security", "title": "\nIranian phishing on the rise as elections approach\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2013/06/iranian-phishing-on-rise-as-elections.html", "abstract": "", "date": "\nJune 12, 2013\n"},
{"website": "Google-Security", "title": "\nIncreased rewards for Google\u2019s Web Vulnerability Reward Program\n", "author": ["Posted by Adam Mein and Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2013/06/increased-rewards-for-googles-web.html", "abstract": "", "date": "\nJune 6, 2013\n"},
{"website": "Google-Security", "title": "\nDisclosure timeline for vulnerabilities under active attack\n", "author": ["Posted by Chris Evans and Drew Hintz, Security Engineers"], "link": "https://security.googleblog.com/2013/05/disclosure-timeline-for-vulnerabilities.html", "abstract": "", "date": "\nMay 29, 2013\n"},
{"website": "Google-Security", "title": "\nChanges to our SSL Certificates\n", "author": ["Posted by Stephen McHenry, Director of Information Security Engineering"], "link": "https://security.googleblog.com/2013/05/changes-to-our-ssl-certificates.html", "abstract": "", "date": "\nMay 23, 2013\n"},
{"website": "Google-Security", "title": "\nThe results are in: Hardcode, the secure coding contest for App Engine\n", "author": ["Posted by Eduardo Vela Nava, Security Team"], "link": "https://security.googleblog.com/2013/05/the-results-are-in-hardcode-secure.html", "abstract": "", "date": "\nMay 10, 2013\n"},
{"website": "Google-Security", "title": "\nNew warnings about potentially malicious binaries\n", "author": ["Posted by Moheeb Abu Rajab, Security Team"], "link": "https://security.googleblog.com/2013/04/new-warnings-about-potentially.html", "abstract": "", "date": "\nApril 17, 2013\n"},
{"website": "Google-Security", "title": "\nGoogle Public DNS Now Supports DNSSEC Validation\n", "author": ["Posted by Yunhong Gu, Team Lead, Google Public DNS"], "link": "https://security.googleblog.com/2013/03/google-public-dns-now-supports-dnssec.html", "abstract": "", "date": "\nMarch 19, 2013\n"},
{"website": "Google-Security", "title": "\nVideos and articles for hacked site recovery\n", "author": ["Posted by ", ", Developer Programs Tech Lead"], "link": "https://security.googleblog.com/2013/03/videos-and-articles-for-hacked-site.html", "abstract": "", "date": "\nMarch 12, 2013\n"},
{"website": "Google-Security", "title": "\nAn update on our war against account hijackers \n", "author": ["Posted by Mike Hearn, Google Security Engineer\u00a0"], "link": "https://security.googleblog.com/2013/02/an-update-on-our-war-against-account.html", "abstract": "", "date": "\nFebruary 19, 2013\n"},
{"website": "Google-Security", "title": "\nCalling student coders: Hardcode, the secure coding contest for App Engine\n", "author": ["Posted by Parisa Tabriz, Security Team"], "link": "https://security.googleblog.com/2013/01/calling-student-coders-hardcode-secure.html", "abstract": "", "date": "\nJanuary 10, 2013\n"},
{"website": "Google-Security", "title": "\nEnhancing digital certificate security\n", "author": ["Posted by Adam Langley, Software Engineer"], "link": "https://security.googleblog.com/2013/01/enhancing-digital-certificate-security.html", "abstract": "", "date": "\nJanuary 3, 2013\n"},
{"website": "Google-Security", "title": "\nHelping webmasters with hacked sites\n", "author": ["Posted by ", ", Search Quality Team"], "link": "https://security.googleblog.com/2012/12/helping-webmasters-with-hacked-sites.html", "abstract": "", "date": "\nDecember 12, 2012\n"},
{"website": "Google-Security", "title": "\nAdding OAuth 2.0 support for IMAP/SMTP and XMPP to enhance auth security\n", "author": ["Posted by Ryan Troll, Application Security Team"], "link": "https://security.googleblog.com/2012/09/adding-oauth-20-support-for-imapsmtp.html", "abstract": "", "date": "\nSeptember 17, 2012\n"},
{"website": "Google-Security", "title": "\nContent hosting for the modern web\n", "author": ["Posted by Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2012/08/content-hosting-for-modern-web.html", "abstract": "", "date": "\nAugust 29, 2012\n"},
{"website": "Google-Security", "title": "\nSafe Browsing - Protecting Web Users for 5 Years and Counting\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2012/06/safe-browsing-protecting-web-users-for.html", "abstract": "", "date": "\nJune 19, 2012\n"},
{"website": "Google-Security", "title": "\nMicrosoft XML vulnerability under active exploitation\n", "author": ["Posted by Andrew Lyons, Security Engineer"], "link": "https://security.googleblog.com/2012/06/microsoft-xml-vulnerability-under.html", "abstract": "", "date": "\nJune 12, 2012\n"},
{"website": "Google-Security", "title": "\nSecurity warnings for suspected state-sponsored attacks\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2012/06/security-warnings-for-suspected-state.html", "abstract": "", "date": "\nJune 5, 2012\n"},
{"website": "Google-Security", "title": "\nNotifying users affected by the DNSChanger malware\n", "author": ["Posted by Damian Menscher, Security Engineer"], "link": "https://security.googleblog.com/2012/05/notifying-users-affected-by-dnschanger.html", "abstract": "", "date": "\nMay 22, 2012\n"},
{"website": "Google-Security", "title": "\nSpurring more vulnerability research through increased rewards\n", "author": ["Posted by Adam Mein and Michal Zalewski, Security Team"], "link": "https://security.googleblog.com/2012/04/spurring-more-vulnerability-research.html", "abstract": "", "date": "\nApril 23, 2012\n"},
{"website": "Google-Security", "title": "\nAn improved Google Authenticator app to celebrate millions of 2-step verification users\n", "author": ["Posted by Sara \"Scout\" Sinclair, Associate Product Manager, Google Security Team"], "link": "https://security.googleblog.com/2012/03/improved-google-authenticator-app-to.html", "abstract": "", "date": "\nMarch 30, 2012\n"},
{"website": "Google-Security", "title": "\nCelebrating one year of web vulnerability research\n", "author": ["Posted by Adam Mein, Technical Program Manager, Google Security Team"], "link": "https://security.googleblog.com/2012/02/celebrating-one-year-of-web.html", "abstract": "", "date": "\nFebruary 9, 2012\n"},
{"website": "Google-Security", "title": "\nAndroid and Security\n", "author": ["Posted by Adrian Ludwig, Android Security Engineer"], "link": "https://security.googleblog.com/2012/02/android-and-security.html", "abstract": "", "date": "\nFebruary 2, 2012\n"},
{"website": "Google-Security", "title": "\nLanding another blow against email phishing\n", "author": ["Posted by Adam Dawes, Product Manager"], "link": "https://security.googleblog.com/2012/01/landing-another-blow-against-email.html", "abstract": "", "date": "\nJanuary 29, 2012\n"},
{"website": "Google-Security", "title": "\nTech tips that are Good to Know\n", "author": ["Posted by Alma Whitten, Director of Privacy, Product and Engineering"], "link": "https://security.googleblog.com/2012/01/tech-tips-that-are-good-to-know.html", "abstract": "", "date": "\nJanuary 16, 2012\n"},
{"website": "Google-Security", "title": "\nExpanding Safe Browsing Alerts to include malware distribution domains\n", "author": ["Posted by Nav Jagpal, Security Team"], "link": "https://security.googleblog.com/2011/12/expanding-safe-browsing-alerts-to.html", "abstract": "", "date": "\nDecember 1, 2011\n"},
{"website": "Google-Security", "title": "\nReminder: Safe Browsing version 1 API turning down December 1\n", "author": ["Posted by Brian Ryner, Security Team"], "link": "https://security.googleblog.com/2011/11/reminder-safe-browsing-version-1-api.html", "abstract": "", "date": "\nNovember 22, 2011\n"},
{"website": "Google-Security", "title": "\nProtecting data for the long term with forward secrecy\n", "author": ["Posted by Adam Langley, Security Team"], "link": "https://security.googleblog.com/2011/11/protecting-data-for-long-term-with.html", "abstract": "", "date": "\nNovember 22, 2011\n"},
{"website": "Google-Security", "title": "\nSafe Browsing Alerts for Network Administrators is graduating from Labs\n", "author": ["Posted by Nav Jagpal, Security Team"], "link": "https://security.googleblog.com/2011/10/safe-browsing-alerts-for-network.html", "abstract": "", "date": "\nOctober 6, 2011\n"},
{"website": "Google-Security", "title": "\nGmail account security in Iran\n", "author": ["Posted by Eric Grosse, VP Security Engineering"], "link": "https://security.googleblog.com/2011/09/gmail-account-security-in-iran.html", "abstract": "", "date": "\nSeptember 8, 2011\n"},
{"website": "Google-Security", "title": "\nAn update on attempted man-in-the-middle attacks\n", "author": ["Posted by Heather Adkins, Information Security Manager"], "link": "https://security.googleblog.com/2011/08/update-on-attempted-man-in-middle.html", "abstract": "", "date": "\nAugust 29, 2011\n"},
{"website": "Google-Security", "title": "\nFour Years of Web Malware\n", "author": ["Posted by Lucas Ballard and Niels Provos, Google Security Team"], "link": "https://security.googleblog.com/2011/08/four-years-of-web-malware.html", "abstract": "", "date": "\nAugust 17, 2011\n"},
{"website": "Google-Security", "title": "\nFuzzing at scale\n", "author": ["Posted by Chris Evans, Matt Moore and Tavis Ormandy, Google Security Team"], "link": "https://security.googleblog.com/2011/08/fuzzing-at-scale.html", "abstract": "", "date": "\nAugust 12, 2011\n"},
{"website": "Google-Security", "title": "\n2-step verification: stay safe around the world in 40 languages\n", "author": ["Posted by Nishit Shah, Product Manager, Google Security"], "link": "https://security.googleblog.com/2011/07/2-step-verification-stay-safe-around.html", "abstract": "", "date": "\nJuly 28, 2011\n"},
{"website": "Google-Security", "title": "\nUsing data to protect people from malware\n", "author": ["Posted by Damian Menscher, Security Engineer"], "link": "https://security.googleblog.com/2011/07/using-data-to-protect-people-from.html", "abstract": "", "date": "\nJuly 19, 2011\n"},
{"website": "Google-Security", "title": "\nIntroducing DOM Snitch, our passive in-the-browser reconnaissance tool\n", "author": [], "link": "https://security.googleblog.com/2011/06/introducing-dom-snitch-our-passive-in.html", "abstract": "", "date": "\nJune 21, 2011\n"},
{"website": "Google-Security", "title": "\nProtecting users from malware hosted on bulk subdomain services\n", "author": ["Posted by Oliver Fisher, Google Anti-Malware Team"], "link": "https://security.googleblog.com/2011/06/protecting-users-from-malware-hosted-on.html", "abstract": "", "date": "\nJune 17, 2011\n"},
{"website": "Google-Security", "title": "\nTrying to end mixed scripting vulnerabilities\n", "author": ["Posted by Chris Evans and Tom Sepez, Google Chrome Security Team"], "link": "https://security.googleblog.com/2011/06/trying-to-end-mixed-scripting.html", "abstract": "", "date": "\nJune 16, 2011\n"},
{"website": "Google-Security", "title": "\nSafe Browsing Protocol v2 Transition\n", "author": ["Posted by Ian Fette, Google Security Team"], "link": "https://security.googleblog.com/2011/05/safe-browsing-protocol-v2-transition.html", "abstract": "", "date": "\nMay 26, 2011\n"},
{"website": "Google-Security", "title": "\nWebsite Security for Webmasters\n", "author": ["Posted by Gary Illyes, Webmaster Trends Analyst"], "link": "https://security.googleblog.com/2011/05/website-security-for-webmasters.html", "abstract": "", "date": "\nMay 5, 2011\n"},
{"website": "Google-Security", "title": "\nProtecting users from malicious downloads\n", "author": ["Posted by Moheeb Abu Rajab, Google Security Team"], "link": "https://security.googleblog.com/2011/04/protecting-users-from-malicious.html", "abstract": "", "date": "\nApril 5, 2011\n"},
{"website": "Google-Security", "title": "\nImproving SSL certificate security\n", "author": ["Posted by Ben Laurie, Google Security Team"], "link": "https://security.googleblog.com/2011/04/improving-ssl-certificate-security.html", "abstract": "", "date": "\nApril 1, 2011\n"},
{"website": "Google-Security", "title": "\nChrome warns users of out-of-date browser plugins\n", "author": ["Posted by Panayiotis Mavrommatis and No\u00e9 Lutz, Google Security Team"], "link": "https://security.googleblog.com/2011/03/chrome-warns-users-of-out-of-date.html", "abstract": "", "date": "\nMarch 31, 2011\n"},
{"website": "Google-Security", "title": "\nMHTML vulnerability under active exploitation\n", "author": ["Posted by Chris Evans, Robert Swiecki, Michal Zalewski, and Billy Rios, Google Security Team"], "link": "https://security.googleblog.com/2011/03/mhtml-vulnerability-under-active.html", "abstract": "", "date": "\nMarch 11, 2011\n"},
{"website": "Google-Security", "title": "\nAdvanced sign-in security for your Google account\n", "author": ["Posted by Nishit Shah, Product Manager, Google Security"], "link": "https://security.googleblog.com/2011/02/advanced-sign-in-security-for-your.html", "abstract": "", "date": "\nFebruary 10, 2011\n"},
{"website": "Google-Security", "title": "\nQuick update on our vulnerability reward program\n", "author": ["Posted by Matt Moore, Michal Zalewski, Adam Mein, Chris Evans; Google Security Team"], "link": "https://security.googleblog.com/2010/11/quick-update-on-our-vulnerability.html", "abstract": "", "date": "\nNovember 11, 2010\n"},
{"website": "Google-Security", "title": "\nRewarding web application security research\n", "author": ["Posted by Chris Evans, Neel Mehta, Adam Mein, Matt Moore, and Michal Zalewski; Google Security Team"], "link": "https://security.googleblog.com/2010/11/rewarding-web-application-security.html", "abstract": "", "date": "\nNovember 1, 2010\n"},
{"website": "Google-Security", "title": "\nThis Internet is Your Internet: Digital Citizenship from California to Washtenaw County\n", "author": ["Posted by Adrienne St. Aubin, Public Policy Analyst"], "link": "https://security.googleblog.com/2010/10/this-internet-is-your-internet-digital.html", "abstract": "", "date": "\nOctober 21, 2010\n"},
{"website": "Google-Security", "title": "\nProtecting your data in the cloud\n", "author": ["Posted by Priya Nayak, Consumer Operations, Google Accounts"], "link": "https://security.googleblog.com/2010/10/protecting-your-data-in-cloud.html", "abstract": "", "date": "\nOctober 15, 2010\n"},
{"website": "Google-Security", "title": "\nPhishing URLs and XML Notifications\n", "author": ["Posted by Nav Jagpal, Security team"], "link": "https://security.googleblog.com/2010/10/phishing-urls-and-xml-notifications.html", "abstract": "", "date": "\nOctober 14, 2010\n"},
{"website": "Google-Security", "title": "\nSafe Browsing Alerts for Network Administrators\n", "author": ["Posted by Nav Jagpal and Ke Wang, Security Team"], "link": "https://security.googleblog.com/2010/09/safe-browsing-alerts-for-network.html", "abstract": "", "date": "\nSeptember 28, 2010\n"},
{"website": "Google-Security", "title": "\nMoving security beyond passwords\n", "author": ["Posted by Travis McCoy, Product Manager, Google Security Team"], "link": "https://security.googleblog.com/2010/09/moving-security-beyond-passwords.html", "abstract": "", "date": "\nSeptember 20, 2010\n"},
{"website": "Google-Security", "title": "\nStay safe while browsing\n", "author": ["Posted by Panayiotis Mavrommatis and Niels Provos, Security Team"], "link": "https://security.googleblog.com/2010/09/stay-safe-while-browsing.html", "abstract": "", "date": "\nSeptember 16, 2010\n"},
{"website": "Google-Security", "title": "\nVulnerability trends: how are companies really doing?\n", "author": ["Posted by Adam Mein, Google Security Team"], "link": "https://security.googleblog.com/2010/08/vulnerability-trends-how-are-companies.html", "abstract": "", "date": "\nAugust 30, 2010\n"},
{"website": "Google-Security", "title": "\nRebooting Responsible Disclosure: a focus on protecting end users\n", "author": ["Posted by Chris Evans, Eric Grosse, Neel Mehta, Matt Moore, Tavis Ormandy, Julien Tinnes, Michal Zalewski; Google Security Team"], "link": "https://security.googleblog.com/2010/07/rebooting-responsible-disclosure-focus.html", "abstract": "", "date": "\nJuly 20, 2010\n"},
{"website": "Google-Security", "title": "\nExtending SSL to Google search\n", "author": ["Posted by Murali Viswanathan, Product Manager"], "link": "https://security.googleblog.com/2010/05/extending-ssl-to-google-search.html", "abstract": "", "date": "\nMay 21, 2010\n"},
{"website": "Google-Security", "title": "\nDo Know Evil: web application vulnerabilities\n", "author": ["Posted by Bruce Leban, Software Engineer"], "link": "https://security.googleblog.com/2010/05/do-know-evil-web-application.html", "abstract": "", "date": "\nMay 4, 2010\n"},
{"website": "Google-Security", "title": "\nThe Rise of Fake Anti-Virus\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2010/04/rise-of-fake-anti-virus.html", "abstract": "", "date": "\nApril 14, 2010\n"},
{"website": "Google-Security", "title": "\nThe chilling effects of malware\n", "author": ["Posted by Neel Mehta, Security Team"], "link": "https://security.googleblog.com/2010/03/chilling-effects-of-malware.html", "abstract": "", "date": "\nMarch 30, 2010\n"},
{"website": "Google-Security", "title": "\nPhishing phree\n", "author": ["Posted by Colin Whittaker, Anti-Phishing Team "], "link": "https://security.googleblog.com/2010/03/phishing-phree.html", "abstract": "", "date": "\nMarch 30, 2010\n"},
{"website": "Google-Security", "title": "\nDetecting suspicious account activity\n", "author": ["Posted by Pavni Diwanji, Engineering Director"], "link": "https://security.googleblog.com/2010/03/detecting-suspicious-account-activity.html", "abstract": "", "date": "\nMarch 24, 2010\n"},
{"website": "Google-Security", "title": "\nMeet skipfish, our automated web security scanner\n", "author": ["Posted by Michal Zalewski"], "link": "https://security.googleblog.com/2010/03/meet-skipfish-our-automated-web.html", "abstract": "", "date": "\nMarch 19, 2010\n"},
{"website": "Google-Security", "title": "\nFederal Support for Federated Login\n", "author": ["Posted by Eric Sachs, Senior Product Manager, Google Security"], "link": "https://security.googleblog.com/2010/03/federal-support-for-federated-login.html", "abstract": "", "date": "\nMarch 3, 2010\n"},
{"website": "Google-Security", "title": "\nMore Information about Malware Details\n", "author": ["Posted by: Lucas Ballard, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/11/more-information-about-malware-details.html", "abstract": "", "date": "\nNovember 24, 2009\n"},
{"website": "Google-Security", "title": "\nDo machines dream of electric malware?\n", "author": ["Posted by: Oliver Fisher, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/10/do-machines-dream-of-electric-malware.html", "abstract": "", "date": "\nOctober 29, 2009\n"},
{"website": "Google-Security", "title": "\nBest Practices for Verifying and Cleaning up a Compromised Site\n", "author": ["Written by Panayiotis Mavrommatis, Security Team "], "link": "https://security.googleblog.com/2009/10/best-practices-for-verifying-and.html", "abstract": "", "date": "\nOctober 22, 2009\n"},
{"website": "Google-Security", "title": "\nProtecting Users and Ads from Malware\n", "author": ["Posted by Eric Davis, Head of Anti-Malvertising"], "link": "https://security.googleblog.com/2009/10/protecting-users-and-ads-from-malware.html", "abstract": "", "date": "\nOctober 16, 2009\n"},
{"website": "Google-Security", "title": "\nShow Me the Malware!\n", "author": ["written by Lucas Ballard, on behalf of the Anti-Malware, Anti-Malvertising, and Webmaster Tools teams"], "link": "https://security.googleblog.com/2009/10/show-me-malware.html", "abstract": "", "date": "\nOctober 12, 2009\n"},
{"website": "Google-Security", "title": "\nThe Malware Warning Review Process\n", "author": ["written by Lucas Ballard and Ke Wang, Anti-Malware Team"], "link": "https://security.googleblog.com/2009/10/malware-warning-review-process.html", "abstract": "", "date": "\nOctober 9, 2009\n"},
{"website": "Google-Security", "title": "\nMalware Statistics Update\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2009/08/malware-statistics-update.html", "abstract": "", "date": "\nAugust 25, 2009\n"},
{"website": "Google-Security", "title": "\nAsk the Google Anti-Malware Team\n", "author": ["Posted by Fabrice Jaubert"], "link": "https://security.googleblog.com/2009/08/ask-google-anti-malware-team.html", "abstract": "The Google Anti-Malware engineering team knows you have many questions related to our , some with short and simple answers and some with more complex answers. The short-answer questions are already -- we hope -- adequately handled on the Webmaster Forums; now we want to do a better job at answering the more complex questions.To this end, we have created  for you to submit your questions, and to vote on other webmasters' questions. In two weeks (on Friday the 28th of August), we will close the page and select a few of the top-rated questions. Over the course of the next several weeks, we will do our best to answer each of these in a write-up, to be published here and to the .We hope to repeat this exercise (with a fresh Moderator page) in the fall to give you the opportunity to ask more questions.Thank you, and see you on !", "date": "\nAugust 14, 2009\n"},
{"website": "Google-Security", "title": "\nImproving web browser security\n", "author": ["Posted by Chris Evans, Security Team"], "link": "https://security.googleblog.com/2009/07/improving-web-browser-security.html", "abstract": "", "date": "\nJuly 22, 2009\n"},
{"website": "Google-Security", "title": "\nPassword strength and account recovery options\n", "author": ["Posted by Macduff Hughes, Engineering Director"], "link": "https://security.googleblog.com/2009/07/password-strength-and-account-recovery.html", "abstract": "", "date": "\nJuly 15, 2009\n"},
{"website": "Google-Security", "title": "\nHTTPS security for web applications\n", "author": ["Posted by Alma Whitten, Software Engineer, Security & Privacy Teams"], "link": "https://security.googleblog.com/2009/06/https-security-for-web-applications.html", "abstract": "", "date": "\nJune 16, 2009\n"},
{"website": "Google-Security", "title": "\nTop 10 Malware Sites\n", "author": ["Posted by Niels Provos, Security Team"], "link": "https://security.googleblog.com/2009/06/top-10-malware-sites.html", "abstract": "A recent surge in compromised web servers has generated many interesting discussions in online forums and blogs.  We thought we would join the conversation by sharing what we found to be the most popular malware sites in the last two months.As we've , we constantly scan our index for potentially dangerous sites.  Our automated systems found more than 4,000 different sites that appeared to be set up for distributing malware by massively compromising popular web sites.  Of these domains more than 1,400 were hosted in the .cn TLD.  Several contained plays on the name of Google such as , etc.", "date": "\nJune 3, 2009\n"},
{"website": "Google-Security", "title": "\nReducing XSS by way of Automatic Context-Aware Escaping in Template Systems\n", "author": ["Posted by Jad S. Boutros, Security Team"], "link": "https://security.googleblog.com/2009/03/reducing-xss-by-way-of-automatic.html", "abstract": "Building on our earlier posts on defenses against web application flaws [, ], we introduce Automatic Context-Aware Escaping (Auto-Escape for short), a functionality we added to two Google-developed general purpose template systems to better protect against Cross-Site Scripting (XSS).We developed Auto-Escape specifically for general purpose template systems; that is, template systems that are for the most part unaware of the structure and programming language of the content on which they operate. These template systems typically provide minimal support for web applications, possibly limited to basic escaping functions that a developer can invoke to help escape unsafe content being returned in web responses. Our observation has been that web applications of substantial size and complexity using these template systems have an increased risk of introducing XSS flaws. To see why this is the case, consider the simplified template below in which double curly brackets  and  enclose placeholders (variables) that are replaced with run-time content, presumed unsafe.In this template, four variables are used (not in this order):Each of these variable insertions requires a different escaping method or risks introducing XSS. To keep the example small, we excluded several contexts of interest, particularly style tags, HTML attributes that expect Javascript (such as ), and considerations of whether attribute values are enclosed within quotes or not (which also affects escaping).The example above demonstrates the importance of understanding the precise context in which variables are being inserted and the need for escaping functions that are both safe and correct for each. For larger and complex web applications, we notice two related vectors for XSS:Considering the sheer number of templates in large web applications and the number of untrusted content they may operate on, the process of proper escaping becomes complicated and error prone. It is also difficult to efficiently audit from a security testing perspective. We developed Auto-Escape to take that complexity away from the developer and into the template system and therefore reduce the risks of XSS that would have ensued.Auto-Escape is a functionality designed to make the Template System web application context-aware and therefore able to apply automatically and properly the escaping required. This is achieved in three parts:A simple mechanism is provided for the developer to indicate that some variables are safe and should not be escaped. This is used for variables that are either escaped through other means in source code or contain trusted markup that should be emitted intact.Auto-Escape has been released with the C++  for a while now and it continues to develop there. You can read more about it in the . We also implemented Auto-Escape for the  template system and expect it to be released in the near future. Lastly, we are in the process of integrating it into other template systems developed at Google for Java and Python and are interested in working with a few other open source template systems that may benefit from this logic. Our HTML/Javascript parser is already available with the Google Ctemplate distribution and is expected to be released as a stand-alone open source project very soon.", "date": "\nMarch 31, 2009\n"},
{"website": "Google-Security", "title": "\nWhy Googlers attend the Internet Identity Workshop\n", "author": [], "link": "https://security.googleblog.com/2009/03/why-googlers-attend-internet-identity.html", "abstract": "", "date": "\nMarch 26, 2009\n"},
{"website": "Google-Security", "title": "\nAnnouncing \"Browser Security Handbook\"\n", "author": ["Posted by Michael Zalewski, Security Team."], "link": "https://security.googleblog.com/2008/12/announcing-browser-security-handbook.html", "abstract": "", "date": "\nDecember 10, 2008\n"},
{"website": "Google-Security", "title": "\nNative Client: A Technology for Running Native Code on the Web\n", "author": ["Posted by Brad Chen, Native Client Team."], "link": "https://security.googleblog.com/2008/12/native-client-technology-for-running.html", "abstract": "", "date": "\nDecember 8, 2008\n"},
{"website": "Google-Security", "title": "\nUser Experience in the Identity Community\n", "author": [], "link": "https://security.googleblog.com/2008/12/user-experience-in-identity-community.html", "abstract": "", "date": "\nDecember 2, 2008\n"},
{"website": "Google-Security", "title": "\nGmail security and recent phishing activity\n", "author": ["Posted by Chris Evans"], "link": "https://security.googleblog.com/2008/11/gmail-security-and-recent-phishing.html", "abstract": "", "date": "\nNovember 25, 2008\n"},
{"website": "Google-Security", "title": "\nOAuth for Secure Mashups\n", "author": ["Posted by Eric Sachs, Senior Product Manager, Google Security"], "link": "https://security.googleblog.com/2008/11/oauth-for-secure-mashups.html", "abstract": "", "date": "\nNovember 18, 2008\n"},
{"website": "Google-Security", "title": "\nMalware? We don't need no stinking malware!\n", "author": ["Written by Oliver Fisher"], "link": "https://security.googleblog.com/2008/10/malware-we-dont-need-no-stinking.html", "abstract": "", "date": "\nOctober 24, 2008\n"},
{"website": "Google-Security", "title": "\nNew spam and virus trends from Enterprise\n", "author": ["Written by Amanda Kleha, Google Apps Security & Compliance team"], "link": "https://security.googleblog.com/2008/08/new-spam-and-virus-trends-from.html", "abstract": "", "date": "\nAugust 12, 2008\n"},
{"website": "Google-Security", "title": "\nKeyczar: Safe and Simple Cryptography\n", "author": ["Written by Steve Weis"], "link": "https://security.googleblog.com/2008/08/keyczar-safe-and-simple-cryptography.html", "abstract": "", "date": "\nAugust 11, 2008\n"},
{"website": "Google-Security", "title": "\nAre you using the latest web browser?\n", "author": ["Written by Thomas Duebendorfer"], "link": "https://security.googleblog.com/2008/07/are-you-using-latest-web-browser.html", "abstract": "", "date": "\nJuly 16, 2008\n"},
{"website": "Google-Security", "title": "\nMeet ratproxy, our passive web security assessment tool\n", "author": ["Posted by Michal Zalewski"], "link": "https://security.googleblog.com/2008/07/meet-ratproxy-our-passive-web-security.html", "abstract": "", "date": "\nJuly 1, 2008\n"},
{"website": "Google-Security", "title": "\nSafe Browsing Diagnostic To The Rescue\n", "author": ["Posted by Niels Provos"], "link": "https://security.googleblog.com/2008/05/safe-browsing-diagnostic-to-rescue.html", "abstract": "", "date": "\nMay 15, 2008\n"},
{"website": "Google-Security", "title": "\nContributing To Open Source Software Security\n", "author": ["Written by Will Drewry"], "link": "https://security.googleblog.com/2008/05/contributing-to-open-source-software.html", "abstract": "", "date": "\nMay 5, 2008\n"},
{"website": "Google-Security", "title": "\nAll Your ", "author": ["Written by Niels Provos, Anti-Malware Team"], "link": "https://security.googleblog.com/2008/02/all-your-iframe-are-point-to-us.html", "abstract": "", "date": "\nFebruary 11, 2008\n"},
{"website": "Google-Security", "title": "\nHelp us fill in the gaps!\n", "author": ["Posted by Ian Fette"], "link": "https://security.googleblog.com/2007/11/help-us-fill-in-gaps.html", "abstract": "", "date": "\nNovember 29, 2007\n"},
{"website": "Google-Security", "title": "\nAuditing open source software\n", "author": ["Written by Chris Evans, Security Team"], "link": "https://security.googleblog.com/2007/10/auditing-open-source-software.html", "abstract": "", "date": "\nOctober 8, 2007\n"},
{"website": "Google-Security", "title": "\nInformation flow tracing and software testing\n", "author": ["Posted by Will Drewry, Security Team"], "link": "https://security.googleblog.com/2007/09/information-flow-tracing-and-software.html", "abstract": "", "date": "\nSeptember 17, 2007\n"},
{"website": "Google-Security", "title": "\nAutomating web application security testing\n", "author": ["Posted by Srinath Anantharaju, Security Team"], "link": "https://security.googleblog.com/2007/07/automating-web-application-security.html", "abstract": "", "date": "\nJuly 16, 2007\n"},
{"website": "Google-Security", "title": "\nThe reason behind the \"We're sorry...\" message\n", "author": ["Posted by Niels Provos, Anti-Malware Team"], "link": "https://security.googleblog.com/2007/07/reason-behind-were-sorry-message.html", "abstract": "", "date": "\nJuly 9, 2007\n"},
{"website": "Google-Security", "title": "\nPhishers and Malware authors beware!\n", "author": ["Posted by Brian Rakowski and Garrett Casto, Anti-Phishing and Anti-Malware Teams"], "link": "https://security.googleblog.com/2007/06/phishers-and-malware-authors-beware.html", "abstract": "The API is still experimental, but we hope it will be useful to ISPs, web-hosting companies, and anyone building a site or an application that publishes or transmits user-generated links.  and let us know how we can make the API better. We fully expect to iterate on the design and improve the data behind the API, and we'll be paying close attention to your", "date": "\nJune 18, 2007\n"},
{"website": "Google-Security", "title": "\nThwarting a large-scale phishing attack\n", "author": ["Posted by Colin Whittaker, Anti-Phishing Team"], "link": "https://security.googleblog.com/2007/06/thwarting-large-scale-phishing-attack.html", "abstract": "In addition to targeting malware, we're interested in combating  a social engineering attack where criminals attempt to lure unsuspecting web surfers into logging into a fake website that looks like a real website, such as eBay, E-gold or an online bank. Following a successful attack, phishers can steal money out of the victims' accounts or take their identities. To protect our users against phishing, we publish a blacklist of known phishing sites. This blacklist is the basis for the anti-phishing features in the latest versions of Firefox and Google Desktop. Although blacklists are necessarily a step behind as phishers move their phishing pages around, blacklists have proved to be reasonably effective.Not all phishing attacks target sites with obvious financial value. Beginning in mid-March, we detected a five-fold increase in overall phishing page views. It turned out that the phishing pages generating 95% of the new phishing traffic targeted , the popular social networking site. While a MySpace account does not have any intrinsic monetary value, phishers had come up with ways to monetize this attack. We observed hijacked accounts being used to spread bulletin board spam for some advertising revenue. According to , phishers also logged in to the email accounts of the profile owners to harvest financial account information. In any case, phishing MySpace became profitable enough (more than phishing more traditional targets) that many of the active phishers began targeting it.Interestingly, the attack vector for this new attack appeared to be MySpace itself, rather than the usual email spam. To observe the phishers' actions, we fed them the login information for a dummy MySpace account. We saw that when phishers compromised a MySpace account, they added links to their phishing page on the stolen profile, which would in turn result in additional users getting compromised. Using a quirk of the CSS supported in MySpace profiles, the phishers injected these links invisibly as see-through images covering compromised profiles. Clicking anywhere on an infected profile, including on links that appeared normal, redirected the user to a phishing page. Here's a sample of some CSS code injected into the \"About Me\" section of an affected profile:", "date": "\nJune 11, 2007\n"},
{"website": "Google-Security", "title": "\nWeb Server Software and Malware\n", "author": [], "link": "https://security.googleblog.com/2007/06/web-server-software-and-malware.html", "abstract": "", "date": "\nJune 5, 2007\n"},
{"website": "Google-Security", "title": "\nOn virtualisation\n", "author": ["Posted by Tavis Ormandy, Security Team"], "link": "https://security.googleblog.com/2007/05/on-virtualisation.html", "abstract": "", "date": "\nMay 29, 2007\n"},
{"website": "Google-Security", "title": "\nIntroducing Google's online security efforts\n", "author": ["Posted by ", " ", " and Niels ", ", Anti-", " Team"], "link": "https://security.googleblog.com/2007/05/introducing-googles-anti-malware.html", "abstract": "", "date": "\nMay 21, 2007\n"}
][
{"website": "Cap-Gemini", "title": "User Privacy and Data Use in iOS 14\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Riccardo Freschi\n          "], "link": "https://capgemini.github.io/engineering/user-privacy-and-data-use-in-ios-14/", "abstract": "Background Digital Advertising is a form of marketing which uses the Internet to deliver promotional messages to consumers. In the Digital Advertising ecosystem there are 3 main actors: consumers, who are the recipients of the messages advertisers, who are the...", "date": "2021-03-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Elasticsearch: Deeper Dive\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Kamar Ali\n          "], "link": "https://capgemini.github.io/development/elasticsearch-deeper-dive/", "abstract": "Introduction In my previous post we looked at getting started with Elasticsearch, covering some basic concepts and getting some hands on. In this article I want to expand on that, taking a deeper dive and covering the following: Importing large...", "date": "2021-02-26T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Securing Spring Boot Config with Kubernetes\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/engineering/securing-spring-boot-config-with-kubernetes/", "abstract": "In a previous blog post I wrote about how Spring Boot and Kubernetes are widely used together when building modern microservices. This post is a natural sequel to the aforementioned post, so it\u2019s worth reading that first if you haven\u2019t...", "date": "2021-02-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Externalizing Spring Boot Config with Kubernetes\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/engineering/externalising-spring-boot-config-with-kubernetes/", "abstract": "Spring Boot and Kubernetes go hand in hand when building modern microservices. Spring Boot is a widely used JVM-based framework which allows you to build out stand-alone, production grade applications. Kubernetes is an open source container management and orchestration system,...", "date": "2021-01-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Distributed Tracing with OpenTelemetry & Jaeger", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Distributed-Tracing-with-OpenTelemetry-And-Jaeger/", "abstract": "Scientific inquiry starts with observation. The more one can see, the more one can investigate - Martin Chalfie Imagine if I was to approach you during your lunch and tell you that there has been an increase in customer raised...", "date": "2020-12-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "An Introduction to Apache NiFi, Use Cases and Best Practices", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jonathan Smith\n          "], "link": "https://capgemini.github.io/development/introduction-nifi-best-practices/", "abstract": "Overview Apache NiFi is a visual data flow based system which performs data routing, transformation and system mediation logic on data between sources or endpoints. NiFi was developed originally by the US National Security Agency. It was eventually made open...", "date": "2020-12-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "5 Things I wish I knew when I became a software developer", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jedd Hopkins\n          "], "link": "https://capgemini.github.io/learning/5-things-I-wish-I-knew-when-I-became-a-software-developer/", "abstract": "March 2018. Having recently finished a two-year contract collecting map data with a large search engine provider, I found myself at a point in my life where I had a real chance to pursue a new career. Having absolutely no...", "date": "2020-11-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Elasticsearch: Introduction\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Kamar Ali\n          "], "link": "https://capgemini.github.io/development/elasticsearch-introduction/", "abstract": "Introduction Elasticsearch is one of those technologies I have always heard about but never had the opportunity to get hands on with. Apart from making an educated guess I didn\u2019t really know what it was, or what it really provides...", "date": "2020-10-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The Engineering Collective\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/engineering/the-engineering-collective/", "abstract": "Starting out on a journey to learn something new can be both daunting and challenging. Where do you begin? How do you recognize the good materials from the bad? How do you know you\u2019re on the right path? These are...", "date": "2020-09-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Rafael: A Developers Story\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Rafael-Developers-Story/", "abstract": "As the global population live in angst about humanity\u2019s future, there are industries across the world that are being stretched to the peak of their abilities in ways never seen before. The healthcare sector is one of those industries. It...", "date": "2020-07-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Using GitHub Actions and Hugo Deploy to Deploy a Static Site to AWS", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Using-GitHub-Actions-and-Hugo-Deploy-to-Deploy-to-AWS/", "abstract": "Owning a website can be fun. Rather than thinking of it as being a bunch of words followed by a \u201c.com\u201d, instead think of it like owning a home in the world of the internet. Your own personal space that...", "date": "2020-06-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Using Route 53 to Create a New Domain for a Static Site\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Using-Route-53-to-Create-a-New-Domain-for-Static-Site/", "abstract": "Over the years there have been countless websites that offer free hosting to users, with the limitation that you use their specific URLs. As shown in previous blog posts where I create a static site and host it on S3...", "date": "2020-05-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Ask In The Channel\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/culture/ask-in-the-channel/", "abstract": "Anti-patterns in instant messaging, and why teams need to get more comfortable about being vulnerable.", "date": "2020-05-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Zero to CKA in 2\u00a0Weeks\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Matt Antley\n          "], "link": "https://capgemini.github.io/kubernetes/Zero-to-CKA-in-2-Weeks/", "abstract": "Prior to the 2 weeks leading up to my CKA exam, I had no Kubernetes knowledge to speak of. I didn\u2019t know what a Kubelet was, how to create a Pod, nor did I know what the Control Plane did....", "date": "2020-05-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Cypress, isn\u2019t that a popular holiday location?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Paul Monk\n          "], "link": "https://capgemini.github.io/testing/cypress-testing-framework/", "abstract": "Overview Cyprus is a nice sunny island and is the third most popular of the Mediterranean islands. However, as I can\u2019t go to Cyprus right now due to COVID-19, I have had to settle for something that sounds similar instead,...", "date": "2020-05-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Using AWS CloudFront as a CDN for an S3 Static Site\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Using-AWS-CloudFront-as-a-CDN-for-an-S3-Static-Site/", "abstract": "In my last post Using AWS S3 and Hugo to Create and Host a Static Website we looked at creating a static site in AWS S3 using Hugo. As previously mentioned, there are some slight disadvantages with hosting a static...", "date": "2020-04-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Quarkus meets Liquibase\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrej Petras\n          "], "link": "https://capgemini.github.io/development/Quarkus-meets-Liquibase/", "abstract": "Migrating an application to a new framework is mostly painful and challenging. We started to migrate our existing Jakarta EE applications which have been using Liquibase to manage the database changes. Quarkus has native support for Flyway which is another...", "date": "2020-03-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Ten Steps towards Cloud Native", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/cloud/cloud-native-steps/", "abstract": "Here in the Open Source Cloud Engineering (OSCE) division of Capgemini Custom Software Development, cloud native development is what we do. What we struggle with is explaining to people exactly what that is. Cloud native is a buzzword which is...", "date": "2020-03-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Using AWS S3 and Hugo to Create and Host a Static Website\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Using-S3-and-Hugo-to-Create-Hosting-Static-Website/", "abstract": "As a software engineer and consultant, I have been responsible for designing, developing and deploying custom websites and systems for a range of private and public sector companies and agencies. With most of my time being spent on other people\u2019s...", "date": "2020-03-02T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Promoting on Success: A Safe and Reliable Strategy of Promoting Serverless Applications\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Promote-On-Success/", "abstract": "Traditionally, there have been many ways of releasing applications. This relates to both the different test environments and the production environment itself. However, with serverless applications, traditional strategies may not transfer so well due to the characteristics of those architectures....", "date": "2020-02-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Introduction to Serverless Computing on AWS\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Introduction-to-Serverless-Computing-on-AWS/", "abstract": "Back in November 2014, AWS completely changed the paradigm of modern Cloud computing by releasing a service called AWS Lambda. Consequently, this changed the future product road map offerings for a lot of technology companies around the world that have...", "date": "2019-07-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Rematch: Redux Without the Bloat", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/development/rematch-redux-without-the-bloat/", "abstract": "State management. Whilst being a critical aspect of writing frontend applications, it\u2019s an ever-changing landscape. You might be reading this and thinking what even is state, and why do I need it? Fundamentally, at a high level, state management is...", "date": "2019-07-19T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Lead Developer London 2019 Roundup", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nikki Algar\n          "], "link": "https://capgemini.github.io/learning/Lead-Dev-London-Conference-2019/", "abstract": "Feeling very fortunate and full of expectation, I attended the London Lead Developer 2019 conference which was held at the iconic Barbican Centre in June. This event is focussed on technical leadership and it was my first time attending. It...", "date": "2019-07-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The top three challenges in securing Public Sector digital services", "author": ["\n            \n            \n    \n            \n            \n            \n            By Dan Harrison\n          "], "link": "https://capgemini.github.io/devsecops/Digital-Services-Public-Sector-DevSecOps/", "abstract": "When we talk to our clients, it\u2019s often understandable that they are worried about the fact that, somehow, their ability to secure digital products and services cannot keep pace with the speed at which they are built. In a previous...", "date": "2019-07-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Fast and Free AWS Web Deployment Tutorial\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Pip Turner\n          "], "link": "https://capgemini.github.io/learning/fast-and-free-aws-web-deployment-tutorial/", "abstract": "I wanted to learn how to use Amazon Web Services (AWS) so I worked through a few online courses and decided that it was time to get practical. However, I was wary of getting charged accidentally while using AWS services....", "date": "2019-06-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Devoxx 2019 Review\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Matt Smith\n          "], "link": "https://capgemini.github.io/learning/Devoxx-2019-Review/", "abstract": "Another Devoxx has come and gone and it\u2019s time to reflect on what was seen and loved! There were loads of amazing talks this year: Some new technologies, friendly faces and some wildly futuristic technology that makes it seem like...", "date": "2019-05-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "There isn't a module for that already?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/module-already/", "abstract": "Sometimes clients ask for the wrong thing. Sometimes developers build the wrong thing, because they didn\u2019t ask the right questions. If you\u2019re solving the wrong problem, it doesn\u2019t matter how elegant your solution is. One of the most important services...", "date": "2019-05-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Introduction to Quarkus: Supersonic Subatomic Java\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/development/Introduction-to-Quarkus-Supersonic-Subatomic-Java/", "abstract": "Due to the constant evolution of different languages and frameworks in the tech industry, developers are able to develop and deploy apps with faster speeds and lower footprint on the underlying systems in which they are deployed to. This simultaneously...", "date": "2019-05-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The bigger picture and the smaller details", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/agile/bigger-picture-smaller-details/", "abstract": "The importance of taking time to think about where we're going", "date": "2019-04-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Out of the Comfort Zone and into the Classroom", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/learning/out-of-the-comfort-zone-into-the-classroom/", "abstract": "Last year I decided to run a React (a frontend JavaScript Technology for building user interfaces) course within the account I\u2019m working on. This year, I decided to do the same thing again, except to a wider audience with much...", "date": "2019-04-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "What is Code Coverage and Why It Should Not Lead Development\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris Burns\n          "], "link": "https://capgemini.github.io/testing/What-Is-Code-Coverage-and-Why-It-Should-Not-Lead-Development/", "abstract": "Within the world of software development and delivery there are not only thousands of tools, frameworks and principles, but there are also many terms that have no concrete definition that tend to fluctuate depending on who you\u2019re talking to. In...", "date": "2019-04-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The Conservation of Complexity in Software\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nigel Hamer\n          "], "link": "https://capgemini.github.io/architecture/The-Conservation-of-Complexity-in-Software-Architecture/", "abstract": "In physics, the law of conservation of energy states that the total energy of an isolated system remains constant\u2014it is said to be conserved over time. Energy can neither be created nor destroyed; rather, it transforms from one form to...", "date": "2019-03-22T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "How to update data models in Drupal 8\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tijani Nasser\n          "], "link": "https://capgemini.github.io/drupal/How-to-update-data-model-on-drupal-8/", "abstract": "In this article we will see how to update data models in Drupal 8, how to make the difference between model updating and content updating, how to create default content, and finally, the procedure to adopt for successful deployments to...", "date": "2019-02-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Project Management Methodology  \u2013 Probably a necessary evil for large Agile IT Projects", "author": ["\n            \n            \n    \n            \n            \n            \n            By Imran Khan\n          "], "link": "https://capgemini.github.io/agile/project-management-methodology-agile-projects/", "abstract": "For me, who has been practicing the Agile in the IT industry for much of my career, earlier project management methodologies appeared as a big evil monster which try to control everything. After recently being certified as a PRINCE2 Practitioner,...", "date": "2019-01-24T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "A framework for progressively decoupled Drupal\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/spalp/", "abstract": "A lot of people have been jumping on the headless CMS bandwagon over the past few years, but I\u2019ve never been entirely convinced. Maybe it\u2019s partly because I don\u2019t want to give up on the sunk costs of what I\u2019ve...", "date": "2018-12-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Basic Accessibility for Web Applications", "author": ["\n            \n            \n    \n            \n            \n            \n            By Liam Giles\n          "], "link": "https://capgemini.github.io/accessibility/Basic-Accessibility-for-Web-Applications/", "abstract": "As frontend developers, it\u2019s not enough to know how to build the applications, you also need to know how to build applications for everyone to be able to use. With the number of web users continuously growing, it\u2019s important that...", "date": "2018-11-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "It's OK Not to be Agile", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/agile/ok-not-to-be-agile/", "abstract": "I love messaging platforms. Slack has made a real difference to the way we can work with remote teams and keep our disparate engineers sharing thoughts and mindsets. We\u2019re also experimenting with Teams, which is proving great, but all these...", "date": "2018-10-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Why you should try React higher-order components now", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/frontend/react-HoCs/", "abstract": "I am in my final year at university: part of the Capgemini Degree Apprenticeship. This year requires a large software project to be delivered and one component of that is a React frontend. Through it I\u2019ve been experimenting and discovered...", "date": "2018-10-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Do Repeat Yourself - returning to the Lead Developer conference", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/lead-dev-conference-2018/", "abstract": "After such a positive experience at the Lead Developer conference last year, as soon as I saw this year\u2019s announcement, I saved the date and started writing the email to get my company to pay for the ticket. Thankfully, my...", "date": "2018-08-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Designing Cross Platform Mobile Applications with Xamarin\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By James Heywood\n          "], "link": "https://capgemini.github.io/.net/designing-mobile-cross-platform-applications-with-xamarin/", "abstract": "Xamarin provides a common development experience for creating cross platform mobile applications. The aim of this post is to highlight the design techniques available and options Xamarin provides for maximising code reuse, thus ensuring cleaner code and increased productivity. The...", "date": "2018-08-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Combining OAuth and JWT to gain performance improvements", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tosin Ogunrinde\n          "], "link": "https://capgemini.github.io/architecture/combining-oauth-and-jwt-to-gain-performance-improvements/", "abstract": "For many years Simple Object Access Protocol (SOAP) was the standard approach for communicating with remote services, often via HTTP. The landscape has changed significantly in recent years with the increase in the adoption of Representational State Transfer (REST) APIs....", "date": "2018-07-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Service Fabric Cluster Creation\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jay Barden\n          "], "link": "https://capgemini.github.io/development/service-fabric-cluster-creation/", "abstract": "There are many ways to create a Service Fabric cluster and what follows, is by no means, the only one. A few posts have been written elsewhere on the internet (for example Create Service Fabric clusters from Visual Studio) and...", "date": "2018-06-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Building for Alexa at Devoxx UK", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/development/voices-for-women/", "abstract": "For our Platinum sponsorship of Devoxx UK this year, we had a theme to align with our recruitment drive of \u201cit takes all types to make a team\u201d. One attraction that we had at our stall was an application taking...", "date": "2018-05-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How fast are your React\u2011ions\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Paul Monk\n          "], "link": "https://capgemini.github.io/react/how-fast-are-your-react-ions/", "abstract": "Overview Since its conception by Facebook in 2013, React has quickly become one of the most popular libraries for building web-based user interfaces. React uses JavaScript to create a dynamic web application normally rendered directly on the client side device,...", "date": "2018-05-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A design review checklist for non\u2011designers", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/development/design-review-checklist/", "abstract": "If we can't entirely break down the wall that designs are thrown over, let's at least put a gate in it.", "date": "2018-04-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A look at Cloud-Native Apps on Azure\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ian Crow\n          "], "link": "https://capgemini.github.io/cloud/cloud-native-apps-on-azure/", "abstract": "Organisations are using software as a key differentiator and source of competitive advantage. Whilst we often think about technology-led companies, such as Netflix and Uber, it is transforming all types of organisation. The cloud supports this transformation, and we are...", "date": "2018-04-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Roslyn-Based .NET Code Analyser\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ekhor Asemota, Kriss Sulikowski\n          "], "link": "https://capgemini.github.io/.net/roslyn-based-net-code-analyser/", "abstract": "While it is relatively easy to write code, it is not so easy to write high quality maintainable code. In this post, we introduce a technology - Roslyn, which enables .NET software engineering teams to implement automated code reviews based...", "date": "2018-04-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Serverless, and the challenges using it\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jastinder Singh Atwal\n          "], "link": "https://capgemini.github.io/.net/serverless-and-using-it/", "abstract": "Recently I have been taking a look at serverless computing, trying to go beyond the headlines of why serverless is a good thing. Those headlines are something like \u201ccreate your first serverless application in minutes\u201d. While this isn\u2019t untrue, there...", "date": "2018-03-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Starting out in the world of IoT", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/starting-IoT/", "abstract": "IoT was a very daunting place for me when I started. After building an IoT scale, I\u2019m now slightly more knowledgeable. It\u2019s been one of the most interesting bits of learning I\u2019ve done for a while because you really get...", "date": "2018-03-16T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Musings of a female Software Engineer on International Women's Day", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gayathri Thiyagarajan\n          "], "link": "https://capgemini.github.io/engineering/musings_of_a_female_engineer_on_IWD/", "abstract": "On International Women\u2019s day, I pause to think about the changes that has come about since the whole Active Inclusion and Diversity drive kicked off, not just within Capgemini but in the tech industry as a whole. I wonder how...", "date": "2018-03-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Debugging into a NuGet package\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jay Barden\n          "], "link": "https://capgemini.github.io/development/debugging-into-a-nuget-package/", "abstract": "Over the past year, I have been working on a programme of work that, initially, was intended to extend an existing Azure Service Fabric solution by adding a further 14 WebAPI endpoints. However, early in the project, we took the...", "date": "2018-03-02T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Non functional requirements and Blockchain", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/blockchain/blockchain-ux/", "abstract": "Here in the Applied Innovation Exchange we get to be part of some pretty excellent events. One notable example is \u201cCapgemini Week of Innovation Networks\u201d (CWIN) which happened in November 2017 on the 8th floor of our Holborn office. As...", "date": "2018-02-23T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Encrypting configuration in Apache Karaf", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/java-cryptography-in-karaf/", "abstract": "It\u2019s very important to encrypt passwords or other sensitive information in configuration files. This stops attackers gaining easy access to sensitive information, limiting the way they can harm your system. This has been made easy in an OSGi environment with...", "date": "2018-01-19T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "2017 in review - innovation, diversity, funerals and donkeys", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/blog/end-of-year-review/", "abstract": "Our engineering team blog has continued to grow this year, with 25 authors writing more than 40,000 words in articles across a range of topics. We\u2019ve welcomed 17 first-time authors to the fold, with colleagues from other countries joining the...", "date": "2017-12-15T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Microsoft, .NET and not (necessarily) Windows\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jastinder Singh Atwal\n          "], "link": "https://capgemini.github.io/.net/core-x-platform/", "abstract": "Recently I looked into developing a cross platform .NET Core application. Up to a few years ago a .NET application was developed with the intention to run it on Windows. Not anymore. The landscape for .NET has changed dramatically over...", "date": "2017-12-13T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Seeking Frontend Developers", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/frontend/hiring-front-end/", "abstract": "Customers, whether consumer, business, employee, partner, citizen or stakeholder, are demanding the same high grade user experience they experience from the likes of Facebook, Google and Netflix, in the applications and interactions they have with organisations every day. In the...", "date": "2017-12-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "ffconf 2017 - Conference review", "author": ["\n            \n            \n    \n            \n            \n            \n            By Simon Conway\n          "], "link": "https://capgemini.github.io/engineering/ffconf-conference-review/", "abstract": "ffconf is a JavaScript conference which is held in Brighton each year. This post provides a brief summary of ffconf 2017 (held on 9 and 10 November) and links to the talks posted by the organisers. The day offered a...", "date": "2017-12-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Learning Gulp.js", "author": ["\n            \n            \n    \n            \n            \n            \n            By Leon Hassan\n          "], "link": "https://capgemini.github.io/engineering/learning-gulpjs/", "abstract": "Gulp.js is a task runner that is used to automate tasks such as compiling all your style sheets into a single file, uglifying your JavaScript and so much more. This post is going to introduce you to the world of...", "date": "2017-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Knowledge Is Dead, Long Live Learning", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/knowledge-funerals/", "abstract": "There\u2019s a certain inescapable truth that people who work with technology need to face. As time goes by, the knowledge we\u2019ve gained almost inevitably becomes obsolete. If we specialise in something, how do we deal with the fact that our...", "date": "2017-11-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Thoughts on SkillsMatter Meetup: Plugin Architecture\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/design/clean-architecture/", "abstract": "I attended a SkillsMatter meetup the other day where Uncle Bob (Robert C. Martin) explained some key features of clean architecture. It was a new talk and the content was taken from his new book \u201cClean Architecture\u201d. It was an...", "date": "2017-11-03T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "To SQL or not to SQL\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Paul Monk\n          "], "link": "https://capgemini.github.io/design/sql-vs-nosql/", "abstract": "Overview A common question that many developers and architects struggle to answer is which technology stacks should I use for my project, and more importantly why? This decision is particularly important when it comes to picking a database, as databases...", "date": "2017-10-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Dotnet, Docker, DevOps\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Jastinder Singh Atwal\n          "], "link": "https://capgemini.github.io/docker/dotnet-docker-devops/", "abstract": "Since starting on my journey down the Microsoft Open Source road, one of the things I have been introduced to is Docker. Increasingly, I have looked for opportunities where I can use Docker when developing my .NET applications. DevOps has...", "date": "2017-10-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Design Thinking: Cutting through the fluff", "author": ["\n            \n            \n    \n            \n            \n            \n            By Leon Hassan\n          "], "link": "https://capgemini.github.io/engineering/design-thinking/", "abstract": "Design thinking: What is it and why do I care about it? Design thinking is a framework for finding ideal solutions to complex problems, through observation, empathy and experimentation centered around humans. You might think that this is all rather...", "date": "2017-09-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Ways to Skin a Cat\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/frontend/react-for-java-devs/", "abstract": "I\u2019m too old to be a front-end developer. Talk of frameworks and js-library bingo makes me feel like a WWII veteran at a segway convention. But then I went to Devoxx US and heard Ben Ilegbodu\u2019s talk, \u2018Navigating the React...", "date": "2017-09-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Managing Cloud Infrastructure in Amazon Web Service using VoiceOps", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nagaraj Govindaraj, Rushil Soni\n          "], "link": "https://capgemini.github.io/cloud/Managing-Cloud-Infrastructure-in-Amazon-Web-Service-using-VoiceOps/", "abstract": "Amazon Echo\u00a0has steadily become the hottest smart home product on the market. Suddenly, every tech company wants to integrate its products with Amazon\u2019s customizable virtual assistant, Alexa. Alexa is Amazon\u2019s voice controlled service on many devices like Echo, Echo Dot,...", "date": "2017-08-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Capgemini UK Engineering's Response to the Google (Anti-)Diversity Manifesto", "author": ["\n            \n            \n    \n            \n            \n            \n            By Capgemini UK Engineering Team\n          "], "link": "https://capgemini.github.io/engineering/Capgemini-Engineering-Diversity-Manifesto/", "abstract": "A lot has been written about the 10-page memo by former Google employee James Damore. Following on from the folks at MongoDB, we, the Capgemini UK Engineers, felt the need to share our thoughts on it with all our peers...", "date": "2017-08-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "My First UI5 Application", "author": ["\n            \n            \n    \n            \n            \n            \n            By Leon Hassan\n          "], "link": "https://capgemini.github.io/engineering/My-first-UI5-application/", "abstract": "In this post we will be covering some basic set-up of our development environment (including the SAP Web IDE), going over MVC architecture with particular regard to SAP applications and finally building your first SAP application. Getting Started The first...", "date": "2017-08-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Neo Engineer or Buridan\u2019s Donkey", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gayathri Thiyagarajan\n          "], "link": "https://capgemini.github.io/engineering/becoming-a-neo-engineer/", "abstract": "\u201cThe Internet?\u00a0 Is that thing still around?\u201d\u00a0 - Homer Simpson True story - Back when I started my IT career nearly 12 years ago, fresh out of college, little did I know that my whole career was being decided by...", "date": "2017-07-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Hacking Blockchain", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders, Craig Williams, Dan Cotton\n          "], "link": "https://capgemini.github.io/blockchain/hacking-blockchain/", "abstract": "Who can resist the opportunity to join a Hackathon? Certainly not the engineers in Capgemini\u2019s Applied Innovation Exchange (AIE); fast-turnaround cutting-edge product development is what we do. So when Capgemini Financial Services announced a global virtual IBM Hyperledger Hackathon, a...", "date": "2017-06-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Change your debugging mindset", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/debugging/", "abstract": "I learnt to code at a bootcamp organised by Capgemini and delivered by QA in the first 13 weeks of my apprenticeship. Undoubtedly the best thing I got out of that was a nugget of wisdom which has stayed with...", "date": "2017-06-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Learning to Lead, Learning to Listen", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/lead-dev-conference-2017/", "abstract": "On election day I went to Westminster. Not for political reasons, but to attend the Lead Developer conference. I\u2019d been meaning to go ever since hearing good things about it from my colleague Tom Phethean, who went to the inaugural...", "date": "2017-06-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Will this Meteor hit?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Henry White\n          "], "link": "https://capgemini.github.io/development/will-this-Meteor-hit/", "abstract": "Over the past few years, the use of JavaScript as a full-stack web technology has increased through the use of libraries such as React and Angular. As a result, the open-source platform Meteor was created. This post looks at the...", "date": "2017-06-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "CQL Statement Builder", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tosin Ogunrinde\n          "], "link": "https://capgemini.github.io/apache%20cassandra/cql-statement-builder/", "abstract": "Anyone who has worked with Apache Cassandra will not underestimate the importance of selecting the right data model. However, this could be a daunting task due to the lack of a generic schema generator (in Java). For example, each time...", "date": "2017-05-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Capgemini Microsoft Dynamics 365 team at Extreme 365", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ben Hosking\n          "], "link": "https://capgemini.github.io/dynamics365/capgemini-dynamics-365-team-at-Extreme-365/", "abstract": "\"Invest in the future because that is where you are going to spend the rest of your life.\" Habeeb Akande Capgemini Microsoft Dynamics 365 UK team attended Extreme 365 hosted in Lisbon, it's important to understand where Microsoft Dynamics 365...", "date": "2017-05-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "QCon London 2017", "author": ["\n            \n            \n    \n            \n            \n            \n            By Rune Offerdal\n          "], "link": "https://capgemini.github.io/culture/qcon-london-2017/", "abstract": "I had heard great stories about QCon. It\u2019s the conference where practicing engineers find new trends and technologies and learn how to adopt them. Did the content match the inviting wrapping? This is my experience from QCon London 2017 as...", "date": "2017-04-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Devoxx UK 2017 Giveaway!", "author": ["\n            \n            \n    \n            \n            \n            \n            By Shana Dacres\n          "], "link": "https://capgemini.github.io/agile/devoxx-givaway/", "abstract": "Enjoy a free ticket to one of the most sought after developer\u2019s conference of the year, Devoxx UK 2017, courtesy of Capgemini Engineering.", "date": "2017-03-31T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "A Portable Kubernetes Cluster", "author": ["\n            \n            \n    \n            \n            \n            \n            By James Relph\n          "], "link": "https://capgemini.github.io/kubernetes/a-portable-kubernetes-cluster/", "abstract": "Kubernetes is rapidly becoming the de facto industry standard for container orchestration. Initially a development of Google\u2019s internal Borg orchestration software, Kubernetes provides a number of critical features including: Service discovery Container replication Auto scaling and load balancing Flexible and...", "date": "2017-03-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Embracing the Legacy", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/legacy/embracing-the-legacy/", "abstract": "\u201cWait what! You\u2019re still using that version of X?!\u201d We\u2019ve all been there. You\u2019re on a new project and the first thing you\u2019re made painfully aware of is that it is a \u2018legacy\u2019 project. Legacy\u2026 That single word is often...", "date": "2017-03-24T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "A blurry look into the future", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/innovation/blurry-look-future/", "abstract": "Along with some of my colleagues in the Digital Platforms team, I recently took a tour of the London lab of Capgemini\u2019s Applied Innovation Exchange. The AIE is a series of facilities around the world giving Capgemini and our clients...", "date": "2017-03-21T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "DrupalCamp London experience", "author": ["\n            \n            \n    \n            \n            \n            \n            By Luis Rodriguez\n          "], "link": "https://capgemini.github.io/drupal/drupalcamp-london-experience/", "abstract": "This weekend\u2019s DrupalCamp London wasn\u2019t my first Drupal event at all, I\u2019ve been to 3 DrupalCon Europe, 4 DrupalCamp Dublin, and a few other DrupalCamps in Ireland and lots of meetups, but in this case I experienced a lot of...", "date": "2017-03-07T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "The Best of Capgemini Engineering Blog 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By Bobby Moss\n          "], "link": "https://capgemini.github.io/blog/best-of-2016/", "abstract": "Hello! Are you new here? If so, welcome to the Capgemini Engineering blog. It\u2019s written by and for our fellow engineers across the technology industry. This site has gone from strength to strength since it was created in 2014, and...", "date": "2017-02-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "I Test, Therefore I Am", "author": ["\n            \n            \n    \n            \n            \n            \n            By Az Madujibeya\n          "], "link": "https://capgemini.github.io/development/I-Test-Robot/", "abstract": "The three laws of TDD, as espoused by Robert C. (Uncle Bob) Martin state: You are not allowed to write any production code unless it is to make a failing unit test pass. You are not allowed to write any...", "date": "2017-01-31T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Security of the Future", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/cybersecurity/security-of-the-future/", "abstract": "Cybersecurity was a huge focal point of 2016. With a new hacking scandal being highlighted in the news almost on a weekly basis, cybersecurity has become a major issue for all digital companies. Looking at some of the more severe...", "date": "2017-01-27T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Our Grade Ladder", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ant Broome\n          "], "link": "https://capgemini.github.io/culture/our-grade-ladder/", "abstract": "Last year, we identified a need to redefine the career framework for our software engineers within the UK engineering teams and started work on a Capgemini Software Engineering grade ladder. The grade ladder is our team\u2019s self-produced documentation to enable...", "date": "2017-01-20T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Talking About How We Work", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/development/how-we-work-talk/", "abstract": "Following the previous blog post about our software engineering team culture that I wrote with my colleague Andrew Harmel-Law, I spoke about the subject at the January Drupal Show & Tell last night. I\u2019ve been meaning to speak at a...", "date": "2017-01-13T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "DevOps - Why I need to know about it as an Engineer", "author": ["\n            \n            \n    \n            \n            \n            \n            By Imran Khan\n          "], "link": "https://capgemini.github.io/devops/DevOpsAndMeEngineer/", "abstract": "I recently attended DevOps days at Les Fontaines (Capgemini University in Paris). The key reasons were to understand some details of this buzzword and how it may impact me as an engineer and my existing clients. Being an engineer, my...", "date": "2016-12-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "React-ing to change", "author": ["\n            \n            \n    \n            \n            \n            \n            By Greg Wolverson\n          "], "link": "https://capgemini.github.io/react/reacting-to-change/", "abstract": "With the recent release of the \u2018The State of JavaScript\u2019 the clear big gainer of 2016 is ReactJS. So what\u2019s all the fuss about? I\u2019m going to look at the two big boys in the front end space. Angular and...", "date": "2016-11-25T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Ansible and Weave step by step", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/cloud/ansible-weave/", "abstract": "This is a pragmatic guide to Ansible for beginners. This use case will guide you on how to set up a cross-cloud software defined network for containers using Weave Net, Weave Scope and Docker. There is a full gitbook including...", "date": "2016-11-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Kubernetes, Ingress controllers and Traefik", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/kubernetes/kube-traefik/", "abstract": "When running your application services on top of an orchestration tool like Kubernetes or Mesos with Marathon there are some common necessities you\u2019ll need to satisfy. Your application will usually contain two types of services, those that should be visible...", "date": "2016-11-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "What to look for in a code review", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/what-to-look-for-in-code-review/", "abstract": "In a previous article on this blog, I talked about why code review is a good idea, and some aspects of how to conduct them. This time I want to dig deeper into the practicalities of reviewing code, and mention...", "date": "2016-10-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Fun with Drupal 8 configuration management", "author": ["\n            \n            \n    \n            \n            \n            \n            By Richard Sheppard\n          "], "link": "https://capgemini.github.io/drupal/d8config-fun/", "abstract": "It has been a few years since I have had the opportunity to build a website from the absolute beginning. The most recent project I\u2019m on continues in that vein, but it\u2019s early enough for me to consider ripping it...", "date": "2016-10-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Rearing Good Design Ideas in the Wild", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/presenting-ideas/", "abstract": "Imagine you\u2019re in a design session for a new component. The conversation is going back and forth between developers and architects but as they talk you discover the feeling of uneasiness in the current plan. It seems inflexible and prone...", "date": "2016-10-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Tips and Thoughts on Prototyping with AWS Elastic Beanstalk", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/aws-elastic-beanstalk/", "abstract": "The choice of PaaS I\u2019ve investigated a few Platform as a Service (PaaS) offerings as part of small projects or prototypes I\u2019ve done recently. I\u2019ve used Openshift (version 2), Heroku and now Amazon Web Services (AWS). This is only a...", "date": "2016-09-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Considerations for a Drupal 8 upgrade", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/considering-drupal-8-upgrade/", "abstract": "If you\u2019re migrating from a different CMS platform, the advantages of Drupal 8 seem fairly clear. But what if you\u2019re already on Drupal? There has been a lot of discussion in the Drupal community lately about upgrading to Drupal 8....", "date": "2016-09-19T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Digital Developer Meetup, Birmingham, 22nd September, 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By Richard Sheppard\n          "], "link": "https://capgemini.github.io/development/Digital-Developer-Meetup/", "abstract": "Capgemini UK is hosting an event for digital technologists and experts on the evening of Thursday 22nd September at the Old Library, Custard Factory in Birmingham\u2019s creative quarter! Whether you\u2019re a platform engineer, project manager, scrum master, technologist or tester,...", "date": "2016-09-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How We Work", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law, Malcolm Young\n          "], "link": "https://capgemini.github.io/development/how-we-work/", "abstract": "Recently within the various Capgemini UK software engineering teams we\u2019ve been looking at our approach to learning and development. Capgemini is a big corporation, and in the past there has been a focus on certification. Like many people in the...", "date": "2016-09-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "My take on Capgemini University's recent DevOps Hackathon", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gayathri Thiyagarajan\n          "], "link": "https://capgemini.github.io/cloud/my-take-on-capgemini-university-devops-hackathon/", "abstract": "Recently, I took part in a DevOps Hackathon organised by our Capgemini University in Les Fontaines. It was part of the overall Talent Week program which had an underlying theme \u201cLeading All the Way\u201d. Twenty four Capgemini Engineers with different...", "date": "2016-08-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Say Hello to KuWit", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/bots/kuwit/", "abstract": "It is not a secret that the industry is betting hard on building AI engines using the power of machine learning and big data to provide the foundation for delivering extremely personalized user experience. Platforms like FBLearner Flow, Google TensorFlow,...", "date": "2016-08-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "INVEST in User Stories", "author": ["\n            \n            \n    \n            \n            \n            \n            By Abhilash Nair\n          "], "link": "https://capgemini.github.io/agile/invest-in-user-stories/", "abstract": "An INVEST-able User Story evolves through the journey of a Sprint. Let us follow this journey through the eyes of an Agile Team Member. This post is directed at Team Members in companies planning on adopting an Agile methodology, especially...", "date": "2016-08-12T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How to write effective bug reports", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/testing/effective-bug-reports/", "abstract": "Unfortunately, software will always have bugs, and those defects need to be tracked. Whether you\u2019re doing automated or manual testing, when that testing finds a problem, you need to communicate that problem to the development team. The way to communicate...", "date": "2016-08-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Hacking on Bluemix", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ant Broome\n          "], "link": "https://capgemini.github.io/cloud/hacking-on-bluemix/", "abstract": "I\u2019m currently sitting at an airport in Paris awaiting my flight back to Cardiff and thought it would be a good time to share my experience of using IBM Bluemix. First off some background: Capgemini has its own University Campus...", "date": "2016-07-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Unit Tests are the Best Documentation", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/unit-tests-as-documentation/", "abstract": "As a relatively new developer (I\u2019ve only been writing code and learning for just over 3 years) this has been a bit of a revelation for me. I\u2019m sure for most seasoned developers this is old news but it\u2019s improved...", "date": "2016-07-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Throwing stones at Clouds", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/cloud/throwing-stones-at-clouds/", "abstract": "Once I was a serious developer, but this year all I have been allowed to do is play with Platform as a Service (PaaS) offerings. Instead of sulking, I have embraced the concept, and I think it\u2019s time a lot...", "date": "2016-07-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Digital Developer Meetup, Birmingham, 14th July, 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/development/Digital-Developer-Meetup-Birmingham/", "abstract": "After the success of our first digital developers meetup last month, we are holding another one at the Birmingham Custard Factory on the 14th July. This event will give you the opportunity to find out about the innovative projects we...", "date": "2016-07-05T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Defining a Minimum Viable Redesign", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/minimum-viable-redesign/", "abstract": "These days, it\u2019s pretty rare that we build websites that aren\u2019t some kind of redesign. Unless it\u2019s a brand new company or project, the client usually has some sort of web presence already, and for one reason or another, they\u2019ve...", "date": "2016-06-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Digital Developer Meetup, London, 30th June, 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/development/Digital-Developer-Meetup/", "abstract": "Capgemini are holding a meetup for digital developers at our recently opened Applied Innovation Exchange (AIE) in Holborn, London on the 30th June. This event will give you the opportunity to find out about the innovative projects we are working...", "date": "2016-06-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Unit Testing Timer Based Code", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Nash\n          "], "link": "https://capgemini.github.io/development/testing-timers/", "abstract": "Code that relies on timers is tricky to unit test. You have to check the functionality of your code and ensure there is the correct synchronisation between the test and the timer thread. Unless special measures are taken, exceptions thrown...", "date": "2016-06-17T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "London Calling - Devoxx UK, 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/development/Devoxx-2016-report/", "abstract": "UPDATE (21/07/2016): the Devoxx folks published the video of the sessions, so we linked them in. We came to Devoxx UK 2016 because we wanted to hire people. (A LOT of people.) But we also wanted to let the community...", "date": "2016-06-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Kubeform: Kubernetes clusters in any cloud", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/kubeform/", "abstract": "Today we are introducing Kubeform: A tool for provisioning production ready Kubernetes clusters to any cloud with security, scalability and maintainability in mind. We\u2019ve had this project open-source for a little while but have been a tad slow to get...", "date": "2016-06-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "We're Sponsoring ContainerSched", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/sponsoring-containersched/", "abstract": "The Capgemini UK Platform Engineering team is sponsoring ContainerSched 2016.", "date": "2016-06-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Cleaning the Camel", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nick Walter\n          "], "link": "https://capgemini.github.io/java/cleaning-the-camel/", "abstract": "You can almost smell bad code and it\u2019s not a nice thing to behold. It can be even worse when you have to get your hands dirty and fix the stuff. In this post I\u2019m going to take a look...", "date": "2016-06-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Win Tickets to Devoxx UK 2016!", "author": ["\n            \n            \n    \n            \n            \n            \n            By Shana Dacres\n          "], "link": "https://capgemini.github.io/agile/win-tickets-to-devoxx/", "abstract": "Capgemini UK is proud to announce another fantastic giveaway for the dev community\u2026 Tickets to Devoxx UK 2016 worth \u00a3474!", "date": "2016-05-26T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Controlling the state of your infrastructure", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/devops/Controlling-the-state-of-your-infrastructure/", "abstract": "It is now a necessity to be able to rapidly and easily deploy and evolve a platform where your apps will be running. In order to do this you need to manage your own compute resources in a given cloud...", "date": "2016-05-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Building private Ethereum networks with Docker Compose", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/blockchain/ethereum-docker-compose/", "abstract": "In my previous article about building a blockchain application, I shared some of the tools, tips and techniques I used to create an end-to-end blockchain web application. Let\u2019s hone in on a specific part of that and explain in more...", "date": "2016-05-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The ramblings of an open source newbie", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gareth Sullivan\n          "], "link": "https://capgemini.github.io/open%20source/experiences-of-open-source/", "abstract": "Some thoughts and opinions on a first foray as an open source contributor", "date": "2016-05-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "We're Sponsoring Devoxx", "author": ["\n            \n            \n    \n            \n            \n            \n            By Matt Davidson\n          "], "link": "https://capgemini.github.io/agile/were-heading-to-devoxx/", "abstract": "The Capgemini UK JVM team is sponsoring Devoxx UK 2016.", "date": "2016-05-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Musings on estimation", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/agile/on-estimating/", "abstract": "Thinking about writing a blog post about estimating but not sure how long it will take.", "date": "2016-05-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Adventures in Blockchain", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/blockchain/adventures-in-blockchain/", "abstract": "I\u2019ve recently been engaged with the Capgemini Applied Innovation Exchange (AIE) where I\u2019ve been experimenting and working with emerging technologies in conjunction with our business and our clients. As part of that we\u2019ve been looking to leverage blockchain technology for...", "date": "2016-05-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "SVG Icon Workflow for Jekyll with Gulp", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/frontend/jekyll-svg-icons/", "abstract": "The engineering blog is a great opportunity for us to make decisions without those pesky clients getting in the way. For one thing, the audience is other developers, which means that the browser profile of our visitors is very different...", "date": "2016-04-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Scala Appetizer for Java Devs", "author": ["\n            \n            \n    \n            \n            \n            \n            By Amir Aryanpour\n          "], "link": "https://capgemini.github.io/scala/scala-appetizer-for-java-devs/", "abstract": "I have been a Java developer for a long time. That goes beyond the time when EJBs or even Struts were introduced. In my madness I decided to start my PhD a few years back. As part of my research...", "date": "2016-04-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How Not To Lead A Team", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/how-not-to-lead/", "abstract": "In my previous article about good lead developers, I said that there was another story, about some of the bad leaders I\u2019ve worked with. Well, it\u2019s time to tell that story. Initially I was reluctant to focus on the negatives,...", "date": "2016-04-13T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Transparency of Things", "author": ["\n            \n            \n    \n            \n            \n            \n            By Cam Parry\n          "], "link": "https://capgemini.github.io/devops/transparency-of-things/", "abstract": "So this blog post will mainly cover what dashboards can bring to transparency, which is at the core of what most IT projects are about these days, unlocking some feature set in the business or exposing a data set previously...", "date": "2016-04-08T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How to do NFR Testing (Non Functional Testing)", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sanjay Kumar\n          "], "link": "https://capgemini.github.io/testing/how-to-do-nft/", "abstract": "On my current project I\u2019m working as an Integration developer (using among other things Camel, Hystrix and MongoDB). This post is based on a series of conversations with Andrew Harmel-Law (@al94781). I\u2019ve recently been involved in Non Functional Requirement (NFR)...", "date": "2016-03-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Techniques to improve Application Design", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gayathri Thiyagarajan\n          "], "link": "https://capgemini.github.io/architecture/application_design_made_easy/", "abstract": "Design is important because it gives an application a vision; a vision that can be shared amongst the team. It gives the team a direction; a foundation on which to build the code. It is not a means to make...", "date": "2016-03-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Understanding and Addressing our Waste", "author": ["\n            \n            \n    \n            \n            \n            \n            By Satvinder Hullait\n          "], "link": "https://capgemini.github.io/agile/understanding-and-addressing-our-waste/", "abstract": "How often do you find yourself having to wait for an event to happen so you can progress what you are trying to do? Waiting for your application to build? Waiting for someone to code review so you can commit?...", "date": "2016-03-04T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "The Thing about Things", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/infrastructure/the-thing-about-things/", "abstract": "This article looks at two sides of the Internet of Things \u2013 the emerging technologies and the cultural impacts. The other week I attended a talk by Mat Henshall, Head of Things at Thoughtworks, San Francisco. I went along hoping...", "date": "2016-02-25T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Win tickets to Voxxed Days Bristol", "author": ["\n            \n            \n    \n            \n            \n            \n            By \n          "], "link": "https://capgemini.github.io/learning/win-tickets-to-voxxeddays-bristol/", "abstract": "Want to win a free ticket to Voxxed Days\u2019 first open source conference in Bristol? We at Capgemini Engineering are giving away a few full day passes to the Voxxed Days Bristol conference, February 25th 2016 (retail value \u00a3180). As...", "date": "2016-02-19T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Seniority, strength, and serendipity - what makes a good lead developer?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/seniority-serendipity/", "abstract": "Like many other people, I fell into a lead developer role, mainly because I found myself part of a team that had no leader. The idea of self-organising teams makes a lot of sense to me, but I think that...", "date": "2016-02-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Women in Technology is an issue for everyone", "author": ["\n            \n            \n    \n            \n            \n            \n            By Shana Dacres, Mat Perry\n          "], "link": "https://capgemini.github.io/learning/women-in-tech-conf", "abstract": "Last week Thursday (28th of Jan) a few of our ladies and a gent had the opportunity to attend the inaugural Women of Silicon Roundabout conference. Over 200 women and men working in technology were present at the conference and...", "date": "2016-02-05T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Voxxed Bristol Feb. 25th 2016", "author": ["\n            \n            \n    \n            \n            \n            \n            By \n          "], "link": "https://capgemini.github.io/learning/voxxed-bristol-feb-25th-2016/", "abstract": "What\u2019s the technology / architecture / way of working that will be big this year? You\u2019re not alone if you\u2019re not sure. The Voxxed Days Bristol conference will highlight some of these big choices over the space of 18 talks,...", "date": "2016-01-29T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Docker Continuous Delivery Workflows", "author": ["\n            \n            \n    \n            \n            \n            \n            By Cam Parry\n          "], "link": "https://capgemini.github.io/devops/docker-ci-workflows/", "abstract": "So this big new world of containers has come along, and with it we are solving a lot of problems we previously had with virtual machines, and we are developing much more elegant solutions to problems as a community. One...", "date": "2016-01-26T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Specialism Constrains Throughput", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/agile/specialism-constrains-throughput/", "abstract": "Why Specialisms Within a Team Leads to Constraints on Productivity Tim, a good developer, has been working on a story. \u201cFred, I need the build configuration changed to pick up the new component I\u2019ve added.\u201d Fred responds, \u201cokay, I can...", "date": "2016-01-22T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Techniques on transforming your team to create a better culture", "author": ["\n            \n            \n    \n            \n            \n            \n            By Shana Dacres\n          "], "link": "https://capgemini.github.io/development/techniques-for-a-better-culture", "abstract": "We are all individuals - our experiences and personality affect who we are and how we see the world. We all learn in different ways and grasp concepts at different rates. Some of us need to fully understand how a...", "date": "2016-01-15T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Creating a CXF REST service with Camel Blueprint", "author": ["\n            \n            \n    \n            \n            \n            \n            By Phil Hardwick\n          "], "link": "https://capgemini.github.io/development/creating-a-cxf-restful-service-in-camel-blueprint/", "abstract": "CXF and Blueprint Before diving into the main content I\u2019ll just give some short snippets about the technology used here. Camel is a framework which implements all the (widely-used) enterprise integration patterns and allows for communication between multiple transports (JMS,...", "date": "2016-01-08T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Is REST Best in a Microservices Architecture?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Craig Williams\n          "], "link": "https://capgemini.github.io/architecture/is-rest-best-microservices/", "abstract": "During my journey into microservices, it has become apparent that the majority of online sample/howto posts regarding implementation focus solely on REST as a means for microservices to communicate with each other. Because of this, you could be forgiven for...", "date": "2015-12-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Meet our Team", "author": ["\n            \n            \n    \n            \n            \n            \n            By Levi Govaerts\n          "], "link": "https://capgemini.github.io/blog/team-promo-movie/", "abstract": "In a recent blog article, Tom spoke about the \u201cFind a Tech Job In London\u201d meetup that was hosted by Capgemini. If you missed the meetup or if you\u2019re still looking for an exciting opportunity, why not consider joining Capgemini?...", "date": "2015-12-09T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "New Year, New Career?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/blog/capgemini-hiring-fatjil/", "abstract": "If your New Year\u2019s resolution will be \u2018find a new job\u2019 now\u2019s the time to start looking for that new and exciting opportunity. On Thursday 26th November Capgemini will be hosting the Find A Tech Job In London (FATJIL) meetup...", "date": "2015-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "DevOps Legacy\n              ", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ant Broome\n          "], "link": "https://capgemini.github.io/devops/devops_legacy/", "abstract": "We all know that a DevOps culture should be a core principle of any new projects, and there\u2019s plenty of blog posts to get us started - just take a look at some of our previous posts. But what happens...", "date": "2015-11-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Stylish Unit Tests", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Nash\n          "], "link": "https://capgemini.github.io/development/unit-test-structure/", "abstract": "Back in March I went to DEVWEEK 2015 and listened to a talk by Kevlin Henney called Programming with GUTs. (You can watch the video behind the link. It\u2019s a good talk.) Since then I\u2019ve been trying to write my...", "date": "2015-10-30T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Super, Smashing, Great in Barcelona", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/super-smashing-great/", "abstract": "I\u2019ve just come back from Smashing Conference in Barcelona and first of all, I have to thank the organisers and speakers for a great event. Secondly, I must apologise for the terribly predictable (for British people of a certain age)...", "date": "2015-10-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Microservices Like Change", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nick Walter\n          "], "link": "https://capgemini.github.io/architecture/microservices-like-change/", "abstract": "Do we always need to be scared of change? I got kind of excited the other day, I got told the client wanted a change. I know, weird, right? To make matters worse it was just a month before the...", "date": "2015-10-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Better Learning Through Code Reviews", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/learning/better-learning-code-reviews/", "abstract": "One of the main reasons I wanted to join a big company was the opportunity to learn. I wanted the chance to work on bigger projects with colleagues who\u2019ve been there and done that, and to benefit from their experience....", "date": "2015-10-09T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Introducing mesos-ui: An alternative frontend for Apache Mesos", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/mesos-ui/", "abstract": "Today we are open-sourcing mesos-ui - A realtime, responsive dashboard for Apache Mesos, built with React, D3, Nodejs, Socket.io and Google Material UI for React. As part of our work on Apollo we got thinking that we could potentially improve...", "date": "2015-10-07T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Developer Automation", "author": ["\n            \n            \n    \n            \n            \n            \n            By Ant Broome\n          "], "link": "https://capgemini.github.io/devops/developer-automation/", "abstract": "The Problem As software engineers we\u2019ve all experienced the steps below when joining an established project: Walk in first day, excited at the prospect we can change the world! Get told task one is to read the documentation :/ followed...", "date": "2015-10-02T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Tooling as a service via executable images", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/devops/tooling-as-a-service/", "abstract": "This is a recap about how we use the isolation of containers and the power, portability and simplicity of Docker for providing tooling as a service via executable images using DCOS-CLI as an example. True story! The problem: We want...", "date": "2015-09-28T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Agile Cambridge 2015", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/agile/agile-cambridge-2015/", "abstract": "Failing Fast - An Autopsy of a Failed Release I\u2019ve been lucky enough to be selected to speak at Agile Cambridge 2015. My talk is titled \u201cFailing Fast - An Autopsy of a Failed Release\u201d, please read more about this...", "date": "2015-09-25T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Delivering at Devoxx", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/learning/delivering-at-devoxx/", "abstract": "There comes a point in a developer\u2019s life where one develops a solid confidence in one\u2019s opinions and actions. For me, this aligned with parenthood; the horrifying responsibility of being in charge of a little one\u2019s life makes choices such...", "date": "2015-09-18T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "The Lead Developer conference", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/learning/lead-developer/", "abstract": "On Friday, I attended The Lead Developer conference, the first outing of a new conference for technical leads. I first heard about the conference back in May, and the tag line piqued my interest immediately: \u201cWhen you\u2019re busy leading a...", "date": "2015-09-14T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "BDD Test Execution Throughput", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/development/bdd-test-execution-throughput/", "abstract": "Background This post is based on the experience of one project in particular. It is the implementation of a large enterprise solution, primarily to speed up the administration of a contract management process. The user-based part of the solution is...", "date": "2015-09-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Writing custom fields in Drupal 8", "author": ["\n            \n            \n    \n            \n            \n            \n            By Deji Akala\n          "], "link": "https://capgemini.github.io/drupal/writing-custom-fields-in-drupal-8/", "abstract": "Based on my presentations at DrupalCamp London, on Saturday 28th February 2015 and DrupalCamp Bristol, July 4th 2015 Concept of a field Fields are the data entry points to a web application. Usually, they provide HTML elements and may be...", "date": "2015-08-27T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Continuously deploying Apollo with Wercker", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela, Cam Parry\n          "], "link": "https://capgemini.github.io/open%20source/continuously-deploying-apollo/", "abstract": "When deploying Apollo it will create the cluster infrastructure in the cloud using an image built by Atlas and Packer. Additionally, it will provision and configure the new machines providing a full PAAS ready for deploying containers across the datacenter....", "date": "2015-07-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "How Apollo uses Weave and Weave Scope", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/how-apollo-uses-weave/", "abstract": "In my previous post we launched an Apollo cluster on AWS in under 5 minutes. Some of the magic around how we enable an easier developer experience around deploying and managing the communication between Docker containers is hidden in the...", "date": "2015-06-30T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Demo: Launching an Apollo cluster on AWS", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/apollo-launch-aws/", "abstract": "This post aims to show how you can get up and running with an Apollo cluster on AWS inside 5 minutes. For more background information on Apollo see our Github repo or the original post which gives an insight into...", "date": "2015-06-24T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Making ethical development decisions", "author": ["\n            \n            \n    \n            \n            \n            \n            By Rob Kerr\n          "], "link": "https://capgemini.github.io/development/making-ethical-development-choices/", "abstract": "We are moral agents. Each of us makes countless choices throughout the course of a day, and many of these choices have ethical parameters and ramifications. We seldom, if ever, stop to think about those choices and their impact on...", "date": "2015-06-04T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Last year an unconference changed my life", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/learning/last-year-an-unconf/", "abstract": "NOTE: This post first appeared on Voxxed.com. Come to Unvoxxed, the newest London Unconference brought to you by The Java Posse Roundup and Devoxx UK, 15-16 June, \u00a3125.00 - \u00a3115 if you book before the end of 21st May and...", "date": "2015-05-20T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Drupal integration patterns", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/drupal/drupal-integration-patterns/", "abstract": "As Drupal has evolved, it has become more than just a CMS. It is now a fully fledged Web Development Platform, enabling not just sophisticated content management and digital marketing capabilities but also any number of use cases involving data...", "date": "2015-05-15T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Apollo, an open-source platform for running your apps", "author": ["\n            \n            \n    \n            \n            \n            \n            By Graham Taylor\n          "], "link": "https://capgemini.github.io/devops/apollo/", "abstract": "Today we are releasing Apollo - A platform for running your next generation web services and applications. This is a project that we have been experimenting with internally to power microservices and big data platforms for our clients. Apollo is...", "date": "2015-05-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Code Beauty", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Nash\n          "], "link": "https://capgemini.github.io/development/code-beauty/", "abstract": "When was the last time you or a colleague looked at some code and made a comment about its attractiveness? You may have said that some code was \u201cugly\u201d or, if you were more fortunate, you might have called it...", "date": "2015-04-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Microservices Gotchas", "author": ["\n            \n            \n    \n            \n            \n            \n            By Nick Walter\n          "], "link": "https://capgemini.github.io/architecture/microservices-gotchas/", "abstract": "Don\u2019t get me wrong, I\u2019m not anti Microservice, far from it. Come and ask me and I\u2019ll give you a lot of reasons why they can benefit any development project. And there are plenty of posts on the web and...", "date": "2015-04-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "When It's Clever to Admit That You're Not Feeling Clever", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/development/its-sometimes-clever-to-admit/", "abstract": "On the first day of my first professional job, an experienced colleague gave me a piece of advice which has served me well for many years: \"Admit it when you don't know the answer\" Simplistically, this works because it prevents...", "date": "2015-03-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Risk Burndown", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/agile/risk-burndown/", "abstract": "Few Agile approaches actively manage risk. There are some tools to help with risk, such as sprints, Definition of Done, etc. but there are few tools that are explicit about risk. Risk burndown is an approach I have used to...", "date": "2015-03-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Why Microservices Are Right For Us", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/architecture/why-microservices-are-right-for-us-pt1/", "abstract": "I posted previously about the fact we\u2019re \u201cdoing\u201d Microservices. At that time I was hedging pretty extensively (as various commenters pointed out). This was predominantly because we we\u2019re pathfinding (or at least broadening the paths for the rest of us...", "date": "2015-03-06T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal, Symfony and friends", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alex Moreno Lopez\n          "], "link": "https://capgemini.github.io/drupal/drupal-composer-symfony/", "abstract": "There are thousands of situations in which you do not want to reinvent the wheel. It is a well known principle in Software Engineering, but not always well applied/known into the Drupal world. Let\u2019s say for example, that you have...", "date": "2015-02-27T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "An Introduction to VirtualBox", "author": ["\n            \n            \n    \n            \n            \n            \n            By Geoff Lawrence\n          "], "link": "https://capgemini.github.io/infrastructure/an-introduction-to-virtualbox/", "abstract": "What is my aim with this post? It is to give you a good introduction to Oracle\u2019s VirtualBox so you know all about it and what it can do. This is not a tutorial on how to install it and...", "date": "2015-02-11T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal 8 PSR-4 Form compatibility in Drupal 7", "author": ["\n            \n            \n    \n            \n            \n            \n            By Oliver Polden\n          "], "link": "https://capgemini.github.io/drupal/drupal-7-psr-forms/", "abstract": "Up until Drupal 8 there has been little to encourage well organised code. It now has PSR-4 autoloading so your classes are automatically included. Even though Drupal 8 is just round the corner, a lot of us will still be...", "date": "2015-01-14T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Drupal 8 in 2 steps", "author": ["\n            \n            \n    \n            \n            \n            \n            By Gabriele Manna\n          "], "link": "https://capgemini.github.io/drupal/drupal-8-in-2-steps/", "abstract": "Drupal 8 is the latest version of Drupal, a modern, PHP 5.4-boasting, REST-capable, object-oriented powerhouse. The concepts are still the same as the previous versions but the approach is now different. Drupal 8 comes with a modern Object Oriented Programming...", "date": "2015-01-07T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Little's Law and KanBan", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/agile/Littles-Law-and-KanBan/", "abstract": "This post delves into Little\u2019s Law and how it relates to KanBan for software development. Understanding these concepts and their relationship will help agile practitioners improve the delivery pipeline, in particular by knowing what to measure and inspect. (Software engineers...", "date": "2014-12-12T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Solving problems being technology agnostic", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/architecture/solving-problems/", "abstract": "We are used to building systems and solving problems under high quality requirements as developers. When you are creating a whole system with its own behaviour and idiosyncrasy your solution is probably going to have some specific characteristics due to...", "date": "2014-12-01T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Pair Programming and Bud\u014d", "author": ["\n            \n            \n    \n            \n            \n            \n            By Rob Kerr\n          "], "link": "https://capgemini.github.io/development/pair-programming-budo/", "abstract": "Confession time: I find pair programming hugely useful, productive, informative - and draining. Like many developers of my acquaintance, I\u2019m an introvert; which doesn\u2019t mean I don\u2019t like people, just that I am less than enthused about spending hours at...", "date": "2014-11-21T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Agile Analogies for Software Development", "author": ["\n            \n            \n    \n            \n            \n            \n            By Sarah Saunders\n          "], "link": "https://capgemini.github.io/agile/agile-analogies/", "abstract": "I\u2019ve recently been working in a training role, with people who have never been exposed to the software engineering lifecycle before. I was drawn to considering analogies to try and explain to them what we were trying to do. It...", "date": "2014-11-18T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Automation as a way of thinking... and docker", "author": ["\n            \n            \n    \n            \n            \n            \n            By Alberto Garcia Lamela\n          "], "link": "https://capgemini.github.io/open%20source/automation/", "abstract": "Automation is a key and essential fact when solving problems and assembling pieces together. By automating you only need to do the same thing once, reducing the possibility of human errors so it helps to increase quality, efficiency and productivity...", "date": "2014-11-10T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Keeping Drupal Secure", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/drupal/securing-drupal/", "abstract": "The recent announcement of Drupal SA-CORE-2014-005 caused a global scramble amongst Drupal site owners, developers and hosting providers to patch their sites and protect themselves from the vulnerability. Unfortunate site owners who weren\u2019t quick enough were left with a site...", "date": "2014-10-31T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Looking Forward to the Spring eXchange 2014 with Capgemini Software Engineering", "author": ["\n            \n            \n    \n            \n            \n            \n            By Scott Davies\n          "], "link": "https://capgemini.github.io/development/spring-exchange-2014/", "abstract": "It\u2019s the 2014 Spring eXchange at skillsmatter Thursday, 6th - Friday, 7th November, and Capgemini are incredibly excited to be sponsoring the event! Need Some Great Reasons to go to the Spring eXchange 2014? The Spring eXchange is the best...", "date": "2014-10-28T00:00:00+00:00"},
{"website": "Cap-Gemini", "title": "Reflections on Drupalcon Amsterdam", "author": ["\n            \n            \n    \n            \n            \n            \n            By Chris How Kin Sang\n          "], "link": "https://capgemini.github.io/drupal/drupalcon-amsterdam/", "abstract": "Along with Malcolm and other colleagues from the Capgemini Drupal team, I attended the recent Drupalcon in Amsterdam. And as well as admiring the Dutch attitude to cycling and its integration in the city (btw London, blue paint on the...", "date": "2014-10-23T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Agile and Lean", "author": ["\n            \n            \n    \n            \n            \n            \n            By John Shaw\n          "], "link": "https://capgemini.github.io/agile/agile-v-lean/", "abstract": "Or should that be Scrum and Kanban? I was in a short conversation a few days ago around whether we, in the business of software development, want to raise the profile of Lean. Perhaps even create some formal training materials....", "date": "2014-10-22T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Component Based Development for the Enterprise", "author": ["\n            \n            \n    \n            \n            \n            \n            By Malcolm Young\n          "], "link": "https://capgemini.github.io/drupal/component-based-design/", "abstract": "Recently 10 members of the Drupal development team at Capgemini went to Drupalcon Amsterdam. Having been to two Drupalcons before, I more or less knew what to expect, but something I hadn\u2019t previously given much thought to was how big...", "date": "2014-10-21T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Microservices - A Reality Check(point)", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/architecture/microservices-reality-check/", "abstract": "It\u2019s reached the point where it\u2019s even a cliche to state \u201cthere\u2019s a lot written about Microservices these days.\u201d But despite this, here\u2019s another post on the topic. Why does the internet need another? Please bear with me\u2026 We\u2019re doing...", "date": "2014-10-17T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Trade-Offs", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/architecture/trade-offs/", "abstract": "Why Stuff Goes Slow\n\n  waiting\n  blocked\n  taking the long way round\n  forgetting what you knew\n\n\nWhy Stuff Gets Complicated\n\n  Engineering around why stuff goes slow", "date": "2014-10-16T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Automated testing for POODLE", "author": ["\n            \n            \n    \n            \n            \n            \n            By Mike Wallis\n          "], "link": "https://capgemini.github.io/open%20source/testing-for-poodle/", "abstract": "Why should systems and infrastructure not be treated in the same way as other software components, especially when it comes to implementing security concerns. With today\u2019s POODLE announcement of another SSL vulnerability it makes sense to add infrastructure tests to...", "date": "2014-10-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Reflections on Symfony Live London 2014", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/open%20source/symfony-live/", "abstract": "At the end of September, I went to my first \u201cnon-Drupal\u201d PHP event, Symfony Live London 2014. With Symfony components becoming a large part of Drupal 8 it was an excellent opportunity to learn a bit about what it all...", "date": "2014-10-10T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Answering: How long will it take?", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/agile/estimation/", "abstract": "How long will it take?", "date": "2014-10-06T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Some Details on the Subtleties of Scala and the Uniform Access Principle", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Harmel-Law\n          "], "link": "https://capgemini.github.io/scala/scala-uniform-access-principle/", "abstract": "Lets start off by taking a really simple function, f, which simply makes and returns a 5-value tuple which can be captured in a val or var: def f = (1, 3.14, \u201cMouse\u201d, false, \u201cAltitude\u201d) The first element of this...", "date": "2014-10-03T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Effective BDD", "author": ["\n            \n            \n    \n            \n            \n            \n            By Andrew Larcombe\n          "], "link": "https://capgemini.github.io/bdd/effective-bdd/", "abstract": "It\u2019s always important to remember that whilst Behat can be just used as a scripting language, in order to get the many benefits associated with BDD then you should always view your scenarios as \u2018Executable Specifications\u2019 for features that deliver...", "date": "2014-09-29T00:00:00+01:00"},
{"website": "Cap-Gemini", "title": "Welcome to Capgemini Engineering", "author": ["\n            \n            \n    \n            \n            \n            \n            By Tom Phethean\n          "], "link": "https://capgemini.github.io/blog/first-post/", "abstract": "At Capgemini, we\u2019re passionate about Engineering and the many facets that make up successful engineering teams. We like to experiment with new technologies, establishing patterns that make our lives easier and more efficient. We are active in the various open...", "date": "2014-09-26T00:00:00+01:00"}
]