[
{"website": "Github-Engineering", "title": "\n\t\t\tImproving how we deploy GitHub\t\t", "author": ["\n\t\tJulian Nadeau\t"], "link": "https://github.blog/2021-01-25-improving-how-we-deploy-github/", "abstract": " Over the last year GitHub has doubled the number of developers contributing to the main GitHub.com application. While this seems like a solely positive thing on the surface, the 2x increase in folks contributing to the core software exposed some problems in terms of tooling. Tooling that worked for us a year ago no longer functioned in the same capacity. While GitHub itself has been a fantastic vehicle to drive change for GitHub, the deployment tooling and coordination has not enjoyed the same levels of success. One of those areas was our deployments. . GitHub.com is deployed primarily through chatops using a branch deploy model (we deploy branches before merging into the main branch). This meant that developers can add changes to a queue, check the status of the queue, and organize groups of pull requests to be deployed and merged. This all functioned using chatops in Slack in a room called #dotcom-ops. While this is a very simple system it started to cause some confusion while monitoring a deploy as a single chat room managed the queue, the deploy, and many other tasks for many people at the same time. All of a sudden this channel that was once a central part of a crucial information system was overwhelmed by the amount of information being pushed through it. Developers could no longer track their change through the system which resulted in a reduced capacity for that developer and an increased risk profile for GitHub. .   .  This is just one step of about a dozen spread across hundreds of messages, it is hard to keep track and validate the state of a deploy.  . At the beginning of this summer, we set out to be able to completely revamp the way we monitored deploy changes for GitHub.com. With the problem in mind –namely the multi-step, information-dense deploy process via chatops –we set out with a few main goals: . One of the main issues that we experienced with the previous system was that deploys were tracked across a number of different messages within the Slack channel. This made it very difficult to piece together the different messages that made-up a single deploy. Sometimes there could be as many as a few hundred messages in between subsequent messages from the deploy system. . The second main issue was that we had a canary stage but the stage would only deploy to up to 2% of GitHub.com traffic. This meant that there were a whole slew of problems that would never get caught in the canary stage before we rolled out to 100% of production, and would have to instead start an incident and roll back. Availability and uptime is of the utmost concern to GitHub, so this risk became crucial to fix and address as GitHub continued to grow. With this in mind, we set out to introduce a second Canary stage at a higher percent so that we could catch more issues in earlier stages which would reduce the impact of future incidents. . At the end of this project, we were able to have two canary stages. The first is at 2% and aims to capture the majority of the issues. This low percentage keeps the risk profile at a tolerable level as such a small amount of traffic would actually be impacted by an issue. A second canary stage was introduced at 20% and allows us to direct to a much larger amount of traffic while still in canary stages. This has a higher risk profile, but is mitigated by the initial 2% canary stage, and allows us to transition with less risk to the 100% production stage. . The last issue is that developers had to sit with the deploy and help poke it along at every step of the way by running independent chatops commands to queue up, deploy canary, and deploy production – making judgement calls every step of the way. This meant that developers were fumbling with chatops commands multiple times in every deployment, and often getting them wrong. . We asked ourselves “what if there was one command and everything else just happened?”. So we did that, we automated the entire process. . We already had internal deploy software that was capable of tracking a single part of a deploy. This system had a proven track record and solid foundations, and so we aimed to add the ability to automate the entire deploy sequence from a single chatops command, rather than running multiple commands per deploy, and link these records together within the existing deployment infrastructure. Moreover, this new solution would provide an easy to use interface for a quick overview of any given deployment rather than piecing together multiple chatops commands. . We came up with two basic concepts: . A deploy is made up of many stages (canary, production, etc.) . There are gates between different stages that perform some sort of check to validate we can progress to the next stage . These concepts allowed us to model the entire system in a state-machine-like fashion: .   . This diagram shows an automatic progression between a 2% canary, 20% canary, production deploy, and a ready to merge stage – separated by automated 5-minute timers. Finally, a pointer was automated to progress across the data model after the timer gates completed and stages were deployed. . What resulted was a state machine backed deploy system with a first party UI component. This was combined with the traditional chatops with which our developers were already familiar. Below, you can see an overview of deploys which have recently been deployed to GitHub.com. Rather than tracking down various messages in a noisy Slack channel, you can go to a consolidated UI. You can see the state machine progression mentioned above in the overview on the right. .   . Drilling down into a specific deployment, you can see everything that has happened during a specific deployment. You can see that each stage of this deployment has an automatic 5-minute timer between them, and the ability to pause the deployment to give a developer more time to test. We also made sure that, in the case something is wrong, we have a quick way to rollback or revert the changes with the dropdown in the top right corner. .   . Finally, the entire system could be monitored and started from Slack – just like before. We found that this is how developers typically want to start their deploys, followed by monitoring in the UI component: .   . These changes have revolutionized the way we deploy GitHub.com. Where confusion and frustration had once set in, we now have joy and content at the use of an automated system. We have received overwhelmingly positive feedback internally, and even attracted some attention from our very own Actions team. Our learnings, in this case, helped to inform and influence the decisions in the  recent GitHub Actions CD  offering announced at GitHub Universe. Our focus on our own developers means that we can apply our learnings to continue creating the best possible system for the 56M+ developers around the world. . Our work this past summer could not have been possible without the dedication and work from many people and teams. We’d like a special shoutout to the GitHub internal deploy team: their existing system, advice, and ongoing help was crucial in making sure our deploys were successful. .  Part of the    Building GitHub blog series   .  ", "date": "January 25, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tMaking GitHub’s new homepage fast and performant\t\t", "author": ["\n\t\tTobias Ahlin\t"], "link": "https://github.blog/2021-01-29-making-githubs-new-homepage-fast-and-performant/", "abstract": " This post is the third installment of our five-part series on building GitHub’s new homepage: .  How our globe is built  .  How we collect and use the data behind the globe  .  How we made the page fast and performant  .  How we illustrate at GitHub  .  How we designed the homepage and wrote the narrative  . Creating a page full of product shots, animations, and videos that still loads fast and performs well can be tricky. Throughout the process of building  GitHub’s new homepage , we’ve used the  Core Web Vitals  as one of our North Stars and measuring sticks. There are many  different   ways  of  optimizing  for these metrics, and we’ve already written about  how we optimized our WebGL globe . We’re going to take a deep-dive here into two of the strategies that produced the overall biggest performance impact for us: crafting high performance animations and serving the perfect image. . As you scroll down the GitHub homepage, we animate in certain elements to bring your attention to them: .   . Traditionally, a typical way of building this relied on listening to the scroll event, calculating the visibility of all elements that you’re tracking, and triggering animations depending on the elements’ position in the viewport: . There’s at least one big problem with an approach like this: calls to getBoundingClientRect() will  trigger reflows , and utilizing this technique might quickly create a performance bottleneck. . Luckily,   IntersectionObservers   are  supported in all modern browsers , and they can be set up to notify you of an element’s position in the viewport, without ever listening to scroll events, or without calling getBoundingClientRect.  An IntersectionObserver can be set up in just a few lines of code to track if an element is shown in the viewport, and trigger animations depending on its state, using each entry’s isIntersecting method: . As we moved over to  IntersectionObservers  for our animations, we also went through all of our animations and doubled down on one of the core tenets of optimizing animations: only animate the transform and opacity properties, since these properties are  easier for browsers to animate  (generally computationally less expensive). We thought we did a fairly good job of following this principle already, but we discovered that in some circumstances we did not, because unexpected properties were bleeding into our transitions and polluting them as elements changed state. . One might think a reasonable implementation of the “only animate transform and opacity” principle might be to define a transition in CSS like so: . In other words, we’re only explicitly changing opacity and transform, but we’re defining the transition to animate all changed properties. These transitions can lead to poor performance since other property changes can pollute the transition (you may have a global style that changes the text color on hover, for example), which can cause unnecessary style and layout calculations. To avoid this kind of animation pollution, we moved to always explicitly defining only opacity and transform as animatable: . As we rebuilt all of our animations to be triggered through IntersectionObservers and to explicitly specify only opacity and transform as animatable, we saw a drastic decrease in CPU usage and style recalculations, helping to improve our Cumulative Layout Shift score: .   . If you’re powering any animations through video elements, you likely want to do two things: only play the video while it’s visible in the viewport, and lazy-load the video when it’s needed. Sadly,  the lazy load attribute  doesn’t work on videos, but if we use  IntersectionObservers  to play videos as they appear in the viewport, we can get both of these features in one go: . Together with setting  preload  to none, this simple observer setup saves us several megabytes on each page load. . We visit web pages with a myriad of different devices, screens and browsers, and something simple as displaying an image is becoming increasingly complex if you want to cover all bases. Our particular illustration style also happens to fall between all of the classic JPG, PNG or SVG formats. Take this illustration, for example, that we use to transition from the main narrative to the footer: .   . To render this illustration, we would ideally need the transparency from PNGs but combine it with the compression from JPGs, as saving an illustration like this as a PNG would weigh in at several megabytes. Luckily,  WebP  is, as of iOS 14 and macOS Big Sur, supported in Safari on both desktops and phones, which brings  browser support up to a solid +90% . WebP does in fact give us the best of both worlds: we can create compressed, lossy images with transparency. What about support for older browsers? Even a new Mac running the latest version of Safari on macOS Catalina can’t render WebP images, so we have to do  something . . This challenge eventually led us to develop a somewhat obscure solution: two JPGs embedded in an SVG (one for the image data and one for the mask), embedded as base64 data—essentially creating a transparent JPG with one single HTTP request. Take a look at  this image . Download it, open it up, and inspect it. Yes, it’s a JPG with transparency, encoded in base64, wrapped in an SVG. . Part of the SVG specification is the  mask element . With it, you can mask out parts of an SVG. If we embed an SVG in a document, we can use the mask element in tandem with the image element to render an image with transparency: . This is great, but it won’t work as a fallback for WebP. Since the paths for these images are dynamic (see href in the example above), the SVG needs to be embedded inside the document. If we instead save this SVG in a file and set it as the src of a regular img, the images won’t be loaded, and we’ll see nothing. . We can work around this limitation by embedding the image data inside the SVG as  base64 . There are services online where you can convert an image to base64, but if you’re on a Mac, base64 is available by default in your Terminal, and you can use it like so: . Where the in-file is your image of choice, the outfile is a text file where you’ll save the base64 data. With this technique, we can embed the images inside of the SVG and use the SVG as a src on a regular image. . These are the two images that we’re using to construct the footer illustration—one for the image data and one for the mask (black is completely transparent and white is fully opaque): .   . We convert the mask and the image to base64 using the Terminal command and then paste the data into the SVG: . You can save that SVG and use it like any regular image. We can then safely use WebP with lazy loading and a solid fallback that works in all browsers: . This somewhat obscure SVG hack saves us hundreds of kilobytes on each page load, and it enables us to utilize the latest technologies for the browsers and operating systems that support them. . We’re working throughout the company to create a faster and more reliable GitHub, and these are some of the techniques that we’re utilizing. We still have a long way to go, and if you’d like to be part of that journey,  check out our careers page . .   .  \t\t Tags:   \t\t Homepage design \t ", "date": "January 29, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: January 2021\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2021-02-02-github-availability-report-january-2021/", "abstract": " In January, we experienced one incident resulting in significant impact and degraded state of availability for the GitHub Actions service. . Our service monitors detected abnormal levels of errors affecting the Actions service. This incident resulted in the failure or delay of some queued jobs for a period of time. Jobs that were queued during the incident were run successfully after the issue was resolved. . We identified the issue as caused by an infrastructure error in our SQL database layer. The database failure impacted one of the core microservices that facilitates authentication and communication between the Actions microservices, which affected queued jobs across the service. In normal circumstances, automated processes would detect that the database was unhealthy and failover with minimal or no customer impact. In this case, the failure pattern was not recognized by the automated processes, and telemetry did not show issues with the database, resulting in a longer time to determine the root cause and complete mitigation efforts. . To help avoid this class of failure in the future, we are updating the automation processes in our SQL database layer to improve error detection and failovers. Furthermore, we are continuing to invest in localizing failures to minimize the scope of impact resulting from infrastructure errors. . We’ll continue to keep you updated on the progress we’re making on ensuring reliability of our services. To learn more about how teams across GitHub identify and address opportunities to improve our engineering systems, check out the  GitHub Engineering blog . .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "February 2, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tDeployment reliability at GitHub\t\t", "author": ["\n\t\tRaffaele Di Fazio\t"], "link": "https://github.blog/2021-02-03-deployment-reliability-at-github/", "abstract": " Welcome to another deep dive of the  Building GitHub  blog series, providing a look at how teams across the GitHub engineering organization identify and address opportunities to improve our internal development tooling and infrastructure. . In a previous  post  of this series, we described how we improved the deployment experience for github.com. When we describe deployments at GitHub, the deployment experience is an important part of what it takes to ship applications to production, especially at GitHub’s scale, but there is more to it: the actual deployment mechanics need to be fast and reliable. . GitHub is deployed to two types of “targets”: multiple Kubernetes clusters and directly to bare metal hosts. Those two targets have different needs and characteristics, such as different number of replicas, different runtimes, etc. . The deployment process of GitHub is designed to be an invisible event for users—we deploy GitHub tens of times a day (yes, even on a Friday) without impact on our users. . When implementing deployments for a monolithic application, we must keep into account the impact that the deployment process has on the internal users of the tool as well. Hundreds of GitHub engineers work at the same time on new features and bug fixes on the same codebase and it’s critical that they can reliably deploy to production. If deployments take too long or if they are prone to fail (even if there is no impact on users), it will mean that developers at GitHub will spend more time getting those features out to users. . For these reasons, we asked ourselves the following questions: . One thing was sure from the beginning, we needed data to answer those questions. . We instrumented our tooling to send metrics on several important key aspects, including, but not limited to: . As well as more general metrics related to the overall delivery: . We used these metrics to implement several improvements to our deployment tooling: we generally made our deployments more reliable by analyzing those metrics, but we also introduced changes that allow us to tolerate some classes of intermittent deployment failures, introducing automatic retries in case of problems. . Additionally, instrumenting our deployment tooling allowed us to identify problems sooner when they happen so that we can react in a timely fashion. . As we mentioned, GitHub is a monolithic rails app that is deployed to Kubernetes and bare metal servers, with the customer facing part of GitHub being 100% deployed to Kubernetes. When we deploy a new version of GitHub we need to start hundreds of pods in multiple Kubernetes clusters.  . A few months ago, our deploy tooling did not print much information on what was going on behind the scenes with our Kubernetes deployments. This meant that whenever a deployment failed, for example due to an issue that we didn’t previously detect in stages before canary, we would have to dig into what happened by directly asking Kubernetes. . At GitHub, we don’t require engineers deploying to understand the internals of Kubernetes. We abstract Kubernetes in a way that is easier to deal with and we have tooling in place to debug GitHub without directly accessing specific Kubernetes clusters. . We analyzed internal support requests to our infrastructure teams and found the possibility to reduce toil by making it easier to figure out what went wrong when deploying to Kubernetes. . For those reasons, we introduced changes to our tooling to provide better information on a deployment while it is being rolled out and proactively providing specific lower level information in case of failures, which includes a view of the Kubernetes events without the need to directly access Kubernetes itself. . This change allowed us to have better detailed information on the progress of a deployment and to increase the visibility on errors in case of failures, which reduces the time to detect a problem and ultimately reduces the overall time needed to deploy to production. . When deployments fail, there is no impact for GitHub customers: deploys are automatically halted before there can be customer-facing issues and to do so we heavily rely on Kubernetes, for example by using readiness probes. . However, deploying GitHub tens of times a day means that the longer a deployment takes, the less things we can ship! . To make sure that we can successfully keep shipping new features to our customers, we defined a few service level objectives (SLOs) to keep track of how fast and reliable deploying GitHub is. . SLOs are usually defined for things like the success rate or latency of a web application, but they can be used for pretty much anything else. In our case, we started using SLOs to set reliability objectives and keep track of how much time it takes to deploy PRs to production which allows us to understand when we need to shift our focus from new features to improvements to the overall shipping flow of GitHub. . At GitHub we have a dedicated team that is responsible for the continuous deployment of applications, which means that we develop tools and best practices but ultimately also help Hubbers ship their applications. These SLOs are now an integral part of the team dynamics and influence the priorities of the team, to ensure that we can keep shipping hundreds of pull requests every week. . In this post we discussed how we make sure that we keep Hubbers shipping new features and improvements over time. Since we started taking a look at the problem, we significantly improved our deployment process, but more importantly we introduced SLOs that can guide investments to further improve our tools and processes so that GitHub users can keep getting fresh new features all year round. ", "date": "February 3, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub reduces Marketplace transaction fees, revamps Technology Partner Program\t\t", "author": ["\n\t\tRyan J. Salva\t"], "link": "https://github.blog/2021-02-04-github-reduces-marketplace-transaction-fees-revamps-technology-partner-program/", "abstract": " At GitHub, our community is at the heart of everything we do. We want to make it easier to build the things you love, with the tools you prefer to use—which is why we’re committed to maintaining an open platform for developers. Launched in 2017 and now home to the world’s largest DevOps ecosystem,  GitHub Marketplace  is the single destination for developers to find, sell, and share tools and solutions that help simplify and improve the process of building software. . Whether buying or selling, our goal is to provide the best Marketplace experience for developers as possible. Today, we’re announcing some changes worth celebrating 🎉; changes to increase your revenue, simplify the application verification process, and make it easier for everyone to build with GitHub. . In the spirit of helping developers both thrive and profit, we’re increasing developer’s take-home pay for apps sold in the marketplace from 75 to 95%. GitHub will only keep a 5% transaction fee. This change puts more revenue in the pockets of the developers, who are doing the work building tools that support the GitHub community. .  Learn more  . We know our partners are excited to get on Marketplace, and we’ve made changes to make this as easy as possible. Previously, a deep review of app security and functionality was required before an app could be added to Marketplace. Moving forward, we’ll verify your organization’s identity and common-sense security precautions by: . Validating your domain with a simple DNS TXT record . Validating the email address on record . Requiring two-factor authentication for your GitHub organization . You can track your app submission’s progress from your organization’s profile settings to fix issues faster. Now developers can get their solutions added to the Marketplace faster and the community can moderate app quality. .   . Soon, we’ll move all “verified apps” to the validated publisher model, updating the “ verified” badge to indicate publishers, and not apps are scrutinized.  Learn more  . We’ve also made some updates to our  Technology Partner Program . If you’re interested in the GitHub Marketplace but unsure how to build integrations to the GitHub platform, co-market with us, or learn about partner events and opportunities, you can get started with our technology partner program for help. You can also check out the partner-centric  resources section  or reach out to us at  partnerships@github.com . .   . You’re now one step away from the technical and go-to-market resources you need to integrate with GitHub and help improve the lives of all software developers. Looking forward to seeing you on the Marketplace. . Happy coding. 👾 ", "date": "February 4, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: February 2021\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2021-03-03-github-availability-report-february-2021/", "abstract": " In February, we experienced no incidents resulting in service downtime to our core services. . This month’s GitHub Availability Report will provide initial details around an incident from March 1 that caused significant impact and a degraded state of availability for the GitHub Actions service. . Our service monitors detected a high error rate on creating check suites for workflow runs. This incident resulted in the failure or delay of some queued jobs for a period of time. Additionally, some customers using the search/filter functionality to find workflows may have experienced incomplete search results. . In February, we experienced a few smaller and unrelated incidents affecting different parts of the GitHub Actions service. While these were localized to a subset of our customers and did not have broad impact, we take reliability very seriously and are conducting a thorough investigation into the contributing factors. . Due to the recency of the March 1 incident, we are still investigating the contributing factors and will provide a more detailed update in the March Availability Report, which will be published the first Wednesday of April. We will also share more about our efforts to minimize the impact of future incidents and increase the performance of the Actions service. . GitHub Actions has grown tremendously, and we know that the availability and performance of the service is critical to its continued success. We are committed to providing excellent service, reliability, and transparency to our users, and will continue to keep you updated on the progress we’re making to ensure this. . For more information, you can check out our  status page  for real-time updates on the availability of our systems and the  GitHub Engineering blog  for deep-dives around how we’re improving our engineering systems and infrastructure. .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "March 3, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tNew global ID format coming to GraphQL\t\t", "author": ["\n\t\tWissam Abirached\t"], "link": "https://github.blog/2021-02-10-new-global-id-format-coming-to-graphql/", "abstract": " The GitHub GraphQL API has been publicly available for  over 4 years now . Its usage has grown immensely over time, and we’ve learned a lot from running one of the largest public GraphQL APIs in the world. Today, we are introducing a new format for object identifiers and a roll out plan that brings it to our GraphQL API this year. . As GitHub grows and reaches new scale milestones, we came to the realization that the current format of  Global IDs  in our GraphQL API will not support our projected growth over the coming years. The new format gives us more flexibility and scalability in handling your requests even faster. . We are changing the Global ID format in our GraphQL API. As a result, all object identifiers in GraphQL will change and some identifiers will become longer than they are now. Since you can get an object’s Global ID via the REST API, these changes will also affect an object’s  node_id  returned via the REST API. Object identifiers will continue to be opaque strings and should not be decoded. . We understand that our APIs are a critical part of your engineering workflows, and our goal is to minimize the impact as much as possible. In order to give you time to migrate your implementations, caches, and data records to the new Global IDs, we will go through a gradual rollout and thorough deprecation process that includes three phases. . Introduce new format: This phase will introduce the new Global IDs into the wild on a type by type basis, for newly created objects only. Existing objects will continue to have the same ID. We will start by migrating the least requested object types, working our way towards the most popular types. Note that the new Global IDs may be longer and, in case you were storing the ID, you should ensure you can store the longer IDs. During this phase, as long as you can handle the potentially longer IDs, no action is required by you. The expected duration of this phase is 3 months. . Migrate: In this phase you should look to update your caches and data records. We will introduce migration tools allowing you to toggle between the two formats making it easy for you to update your caches to the new IDs. The migration tools will be detailed in a separate blog post, closer to launch date. You will be able to use the old or new IDs to refer to an object throughout this phase. The expected duration of this phase is 3 months. . Deprecate: In this phase, all REST API requests and GraphQL queries will return the new IDs. Requests made with the old IDs will continue to work, but the response will only include the new ID as well as a deprecation warning. The expected duration of this phase is 3 months. . Once the three migration phases are complete, we will sunset the old IDs. All requests made using the old IDs will result in an error. Overall, the whole process should take 9 months, with the goal of giving you plenty of time to adjust and migrate to the new format. . If you have any concerns about the rollout of this change impacting your app, please  contact us  and include information such as your app name so that we can better assist you. .  \t\t Tags:   \t\t GraphQL \t ", "date": "February 10, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tHighlights from Git 2.31\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2021-03-15-highlights-from-git-2-31/", "abstract": " The open source Git project  just released Git 2.31  with features and bug fixes from 85 contributors, 23 of them new. Last time  we caught up  with you, Git 2.29 had just been released. Two versions later, let’s take a look at the most interesting features and changes that have happened since. . Picture this: you’re at your terminal, writing commits, pulling from another repository, and pushing up the results when all of the sudden, you’re greeted by this unfriendly message: . …and, you’re stuck. Now you’ve got to wait for Git to finish running  git gc --auto  before you can get back to work. . What happened here? In the course of normal use, Git writes lots of data: objects, packfiles, references, and the like. Some of those paths are optimized for write performance. For example, it’s much quicker to  write  a  single “loose” object , but it’s faster to read a  packfile . . To keep you productive, Git makes a trade-off: in general, it optimizes for the write path while you’re working, pausing every so often to represent its internal data-structures in a way that is more efficient to read in order to keep you productive in the long-run. . Git has its own heuristics about when is a good time to perform this “pause,” but sometimes those heuristics trigger a blocking  git gc  at the worst possible time. You could manage these data-structures yourself, but you might not want to invest the time figuring out when and how to do that. . Starting in Git 2.31, you can get the best of both worlds with  background maintenance . This cross-platform feature allows Git to keep your repository healthy while not blocking any of your interactions. In particular, this will improve your  git fetch  times by pre-fetching the latest objects from your remotes once an hour. . Getting started with background maintenance couldn’t be easier. Simply navigate your terminal to any repository you want to enable background maintenance on, and run the following: . …and Git will take care of the rest. Besides pre-fetching the latest objects once an hour, Git will make sure that  its own data  is organized, too. It will update its   commit-graph  file  once an hour, and pack any loose objects (as well as incrementally repack packed objects) nightly. . Read more about this feature in  the  git maintenance  documentation  and learn how to customize it with   maintenance.* config  options . If you have any trouble, you can check the  troubleshooting documentation . . [ source ,  source ,  source ,  source ] . You may know that Git stores all data as “objects:” commits, trees, and blobs which store the contents of individual files. For efficiency, Git puts many objects into packfiles, which are essentially a concatenated stream of objects (this same stream is also how objects are transferred by  git fetch  and  git push ). In order to efficiently access individual objects, Git generates an index for each packfile. Each of these  .idx  files allows quick conversion of an object id into its byte offset within the packfile. . What happens when you want to go in the other direction? In particular, if all Git knows is what byte it’s looking at in some packfile, how does it go about figuring out which object that byte is part of? . To accomplish this, Git uses an aptly-named  reverse index : an opaque mapping between locations in a packfile, and the object each location is a part of. Prior to Git 2.31, there was no on-disk format for reverse indexes (like there is for the  .idx  file), and so it had to generate and store the reverse index in memory each time. This roughly boils down to generating an array of object-position pairs, and then sorting that array by position (for the curious, the exact details can be found  here ). . But this takes time. In the case of repositories with large packfiles, this can take a lot of time. To better understand the scale, consider an experiment which compares the time it takes to print the size of an object, versus the time it a takes to print that object’s contents. To simply print an object’s contents, Git uses the forward index to locate the desired object in a pack, and then it reassembles and prints out its contents. But to print an object’s  size  in a packfile, Git needs to locate not just the object we want to measure, but the object immediately following it, and then subtract the two to find out how much space it’s using. To find the position of the first byte in the adjacent object, Git needs to use the reverse index. . Comparing the two, it is more than  62 times slower  to print the size of an object than it is to print that entire object’s contents. You can try this at home with  hyperfine  by running: . In 2.31, Git gained the ability to serialize the reverse index into a new, on-disk format with the  .rev  extension. After generating an on-disk reverse index and repeating the above experiment, our results now show that it takes roughly the same amount of time to print an object’s contents as it does its size. . Observant readers may ask themselves why Git even needs to bother using a reverse index. After all, if you can print the contents of an object, then surely printing that object’s size is no more difficult than knowing how many bytes you wrote when printing the contents. But, this depends on the size of the object. If it’s enormous, then counting up all of its bytes is much more expensive than simply subtracting. . Reverse indexes can help beyond synthetic experiments like these: when sending objects for a fetch or push, the reverse index is used to send object bytes directly from disk. Having a reverse index computed ahead of time makes this process run faster. . Git doesn’t generate  .rev  files by default yet, but you can experiment with them yourself by running  git config pack.writeReverseIndex true , and then repacking your repository (with  git repack -Ad ). We have been using these at GitHub for the past couple of months to enable dramatic improvements in many different Git operations. . [ source ,  source ] . [ source ] . [ source ] . [ source ] . [ source ] . [ source ] . That’s just a sample of changes from the last couple of releases. For more, check out the release notes for  2.30  and  2.31 , or  any previous version  in the  Git repository . .  \t\t Tags:   \t\t Git \t ", "date": "March 15, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tImproving large monorepo performance on GitHub\t\t", "author": ["\n\t\tScott Arbeit\t"], "link": "https://github.blog/2021-03-16-improving-large-monorepo-performance-on-github/", "abstract": " Every day, GitHub serves the needs of over 56M developers, working on over 200M code repositories. All but a tiny fraction of those repositories are served with amazing performance, for customers from around the world. . Like any system as large as GitHub, some of the gaps in our coding and architecture are only discovered when they’re pushed to their limits—like having thousands of developers updating the same repo every day—and GitHub received feedback from a handful of our largest monorepo customers that they were seeing performance problems that were impacting their ability to complete push operations. . And so was GitHub.  github/github  is  our  monorepo, and we were experiencing occasional push failures ourselves. . To start our investigation, we worked with internal teams, and engaged with customers, to understand and optimize their usage of GitHub, including coalescing multiple pushes into single pushes, which reduced the number of locking write transactions they were incurring, and even helping with reworking some of their DevOps processes to eliminate unnecessary push operations. Still, we were seeing unusual levels of push failure. .  In response to this, GitHub Engineering created Project Cyclops.  Project Cyclops has been a multi-month effort across our Git Systems org (which includes our Git Storage, Git Protocols, and Git Client teams), working with GitHub’s upstream Git contributors and engineers in our Web front-end teams, to find solutions to these problems. After a lot of collaboration and hard work, and multiple improvements,  we’ve driven push errors down to nearly zero , even for our largest monorepo customers with thousands of contributors. . Project Cyclops resulted in many different approaches to improving our monorepo push performance. Together, they improved our ability to handle push traffic by at least an order of magnitude. . By default, GitHub runs a repository maintenance routine after every 50  git push  operations, or after we receive 40MB of unpacked files, which makes sure we have up-to-date packfiles for great clone/fetch performance, and which cleans up and de-duplicates data in the repo. Depending on the size of the repository, maintenance takes anywhere from a few seconds to a few minutes. . For large monorepos, with a lot of developers, 50 push operations doesn’t take long to accumulate, and therefore maintenance gets scheduled frequently, while developers are still pushing changes to the repo. We had a number of those repos failing to complete maintenance within our maximum time window. When a repo fails maintenance, performance for both push and reference updates suffers, which can lead to manual toil for our engineers to sort out those repositories again.  We’ve reduced those maintenance failures to nearly zero.  Specifically, we made improvements in  git repack , and in how we schedule maintenance retries. . During repo maintenance, we run  git repack  to compress loose objects and prepare for fast clones and fetches. . To compress a set of objects into a single pack, Git tries to find pairs of objects which are related to one another. Instead of storing all of the object’s contents verbatim, some objects are stored as deltas against other related ones. Finding these deltas takes time, and comparing every object to every other object gets infeasible quickly. Git solves this problem by searching for delta/base pairs within a sliding window over an array of all objects being packed. . Some delta candidates within the window can be rejected quickly by heuristics, but some require CPU-intensive comparisons. . We’ve implemented a parameter to limit the number of expensive comparisons we’re willing to make. By tuning this value, we’ve reduced the CPU time we spend during  git repack , while only marginally increasing the resulting packfile size. This one change  eliminated nearly all of our maintenance failures . . Before Project Cyclops, when a repo failed maintenance for any reason, we wouldn’t schedule it to run again for seven days. For many years, this rhythm served our customers well enough, but monorepos today can’t wait that long. We introduced a new  spurious-failure state  for specific repository maintenance failures—the ones that generally come from lots of push traffic happening during maintenance—that allows us to retry maintenance every four hours, up to three times. This means that we’ll get to retry during a customer’s off-hours, when many fewer pushes are happening. This change  eliminated the remaining maintenance failures , and therefore eliminated more toil from our on-call engineers. . On our file servers—the servers that actually hold git repositories—we have had a parameter in place for years that slowed down the rate of push operations we processed on each one. GitHub is a multi-tenant service, and, originally, this parameter was meant to ensure that writes from one customer won’t monopolize resources on a server, which would interfere with traffic from all of the other customers on that server. In effect, this was an  artificial cap  on the amount of work our servers could do. . After an investigation where we slowly raised the value of this parameter to allow 100% of push operations to run immediately, we found that performance with our current architecture was more than good enough, and not only did we raise the limit, we  removed the parameter from our code . This immediately improved our monorepo performance and  eliminated many push-related errors.  . GitHub, by default, writes five replicas of each repository across our three data centers to protect against failures at the server, rack, network, and data center levels. When we need to update Git references, we briefly take a lock across all of the replicas in all of our data centers, and release the lock when our  three-phase-commit  (3PC) protocol reports success. . During that lock, we compute a checksum on each replica, to ensure that they match and that all replicas are in sync. We use incremental checksums to make this faster, and during normal operation, this takes less than 50ms, but during repair operations, where we recompute the checksum from scratch, it takes longer. For large monorepos,  the lock was held for 20-30 seconds . . We made a change to compute these replica checksums prior to taking the lock. By precomputing the checksums, we’ve been able to  reduce the lock to under 1 second , allowing more write operations to succeed immediately. . One of our large monorepo customers keeps their own internal metrics on git performance, and their numbers show what ours do: their push operations have shown no failures for months. . Another monorepo customer who was experiencing too many failures was planning a migration to start with a fresh repository, minimizing the number of references in the repo, in an attempt to improve push success. After these changes, our metrics showed their push failures at nearly zero, and a survey of their developers in December found no reports of recent push failures at all. They cancelled the migration, and continue running with great performance. . Want graphs? Here are graphs from these customers showing push failures dropping to nearly zero as we rolled out fixes. .   .   . Like we said, we’ve got push failures down to  nearly  zero. Some of those failures are caused by random Internet networking issues, and are beyond our control. As for the rest, we’re looking at ways to eliminate those last annoying failures where we can, and to continue to make GitHub faster. . In the Git Systems world, we’re refreshing our storage hardware to make it faster. We’re also in the middle of a significant refactoring effort, doing our part to decompose GitHub’s famous Ruby monolith, and writing a new microservice in Go that will improve repository performance for every single user on GitHub. . Project Cyclops has led to better performance and the elimination of failures for customers with large monorepos, less wasted CPU cycles on our fleet of file servers, and has significantly improved the experience of using GitHub for thousands of developers at some of our largest customers, including our customers using GitHub Enterprise Server. . It has also made small but noticeable improvements for everyone who uses GitHub. . We’ve improved single-repository update traffic rates by at least an order of magnitude. We now have years of headroom on our current architecture to handle the growth of even the largest monorepos. . We want to say how incredibly grateful 💚 we are to our monorepo customers who have collaborated with us to make GitHub better. Their help in reporting, and sometimes even diagnosing, problems was instrumental to addressing them. ✨ Sparkles all around! ✨ .  \t\t Tags:   \t\t monorepo \t ", "date": "March 16, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we found and fixed a rare race condition in our session handling\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2021-03-18-how-we-found-and-fixed-a-rare-race-condition-in-our-session-handling/", "abstract": " On March 8, we shared that, out of an abundance of caution,  we logged all users out of GitHub.com due to a rare security vulnerability . We believe that transparency is key in earning and keeping the trust of our users and want to share more about this bug. In this post we will share the technical details of this vulnerability and how it happened, what we did to respond to it, and the steps we are taking to ensure this does not happen again. . On March 2, 2021, we received a report via our support team from a user who, while using GitHub.com logged in as their own user, was suddenly authenticated as another user. They immediately logged out, but reported the issue to us, as it rightfully worried them. . We activated our security incident response procedures and immediately investigated the  report, pulling in expertise from across the company in security, engineering, and customer support. A few hours after the initial report, we received a second report from another user with a very similar experience. After reviewing request and audit logs, we could validate the external reports received — a user’s session was indeed seen being suddenly shared across two IP addresses around the time of the reports. . Given that this bug was new behavior, we immediately suspected it to be tied to a recent change in our infrastructure and started by reviewing changes. We had recently upgraded components in our load balancing and routing tier. We identified that we had fixed an issue with HTTP keepalives which seemed like it could be related. . After investigating these changes, we were able to determine this was not the root cause. The requests of the affected users followed an entirely different path through our load balancing tier, touching different machines. We ruled out the possibility that the responses were being swapped at that level. Having ruled out the recent infrastructure changes as possible root causes, and with confidence the problem did not exist at the connection and protocol layers, we moved on to other potential causes. . The benefit of starting our investigation with recent infrastructure changes was that we did uncover that the requests that caused the incorrect session to be returned were handled on the same exact machine and in the same process. We run a multi-process setup using the  Unicorn Rack  web server for our main Rails application that handles things like viewing issues and pull requests. . From reviewing logs, we could gather that the HTTP body in the response to the client we sent was correct and only the cookies in the response to the user were wrong. The affected users from the support reports received a session cookie from a user who very recently had a request handled inside the same process. In one case, the two requests were handled sequentially, one after the other. In the second case, there were two other requests in between. . Our working theory then became that somehow something was leaking state between requests that ended up being handled in the same Ruby process. This left us wondering how this could happen. . By reviewing recently shipped features, we identified a potential thread safety issue that could be triggered by functionality that was recently rearchitected to improve performance. One such performance improvement involved moving the logic to check a user’s enabled features into a background thread that refreshed on an interval rather than checking their enablement while the request was being processed. This change seemed to touch the right areas and the undefined behavior of this thread safety issue now made it the focus of our investigation. . In order to understand the thread safety issue, let’s set some context. The main application that handles most browser interactions on GitHub.com is a Ruby on Rails application and it was known to have components that were not written to run in multiple threads (i.e., are not thread-safe). Historically, the thread unsafe behavior could lead to an incorrect value being reported in an exception internally, but not to any user facing behavior change. . Threads were already used in other places in this application, but the new background thread produced a novel and unforeseen interaction with our exception handling routines. When exceptions were reported from a background thread, such as a query timeout, the error log would contain information from both the background thread and the currently running request, showing that the data was being pulled across threads. . We initially thought this to be an internal reporting problem only and that we would see some data logged for an otherwise unrelated request from the background thread. Though inconsistent, we considered this safe since each request has its own request data and Rails creates a new controller object instance for each request. It was still unclear to us how this could cause the problems we were seeing. . The breakthrough was when the team identified that Unicorn, the underlying Rack HTTP server used in our Rails application, does not create a new and separate   env   object for each request. It instead allocates one single Ruby Hash that is then cleared (using  Hash#clear ) between each request that it handles. This led to the realization that the thread safety issue in our exception logging could result in not only inconsistent data being logged in these exceptions, but the sharing of request data on GitHub.com. . Our initial analysis led us to the hypothesis that two requests occuring within a short period of time were required to trigger the race condition. With this information, we tried to reproduce the issue in a development environment. When we attempted to sequence the exact requests, we found that one additional condition was needed and that was an anonymous request that started the whole sequence. The complete list of steps as follows: .   . In the request handling thread, an anonymous request (let’s call it Request #1) started. It registered callbacks in the current context for our internal exception reporting library. The callbacks included references to the current Rails controller object which had access to the single Rack request environment object provided by Unicorn. . In the background thread, an exception occurred. The exception reporting copied the current context in order to include it in the report. This context included the callbacks registered by Request #1 including a reference to the single Rack environment. . In the main thread, a new signed-in request started (Request #2). . In the background thread, the exception reporting processed the context callbacks. One of the callbacks reads the user session identifier, but because the request at time of the context had no authentication, this data was not yet read and therefore triggered a new call to our authentication system via the Rails controller from Request #1. This controller tried to authenticate and pulled the session cookie from the single Rack environment. Because the Rack environment is the same object for all requests, the controller found Request #2’s session cookie. . In the main thread, Request #2 finished. . Another signed-in request started (Request #3). Request #3 completed its authentication step at this time. . Back in the background thread, the controller finished the authentication step by writing a session cookie to the cookie jar that was in the Rack environment. At this point, it is the cookie jar for Request #3! . The user received the response for Request #3. But the cookie jar was updated with the session cookie from Request #2, meaning the user is now authenticated as the user from Request #2. . In summary, if an exception occurred at just the right time and if concurrent request processing happened in just the right sequence across multiple requests, we ended up replacing the session in one response with a session from an earlier response. Returning the incorrect cookie only happened for the session cookie header and as we noticed before, the rest of the response body, such as the HTML, was all still based on the user who was previously authenticated. This behavior lined up with what we saw in our request logs and we were able to clearly identify all of the pieces that made up the root cause of this race condition. . This bug required very specific conditions: a background thread, shared exception context between the main thread and the background thread, callbacks in the exception context, reusing the  env  object between requests, and our particular authentication system. This complexity is a reminder of many of the points presented in  How Complex Systems Fail  and how multiple failures are required to cause a bug like this one. . After identifying the root cause, we immediately prioritized eliminating two of the conditions that were essential in triggering this bug. First, we removed the new background thread introduced in the previously mentioned performance re-architecture. By knowing exactly what was added for this work, it was easy to revert. The change to remove this thread was deployed to production on March 5. With this change, we knew that the conditions required for the race condition could no longer be met and that our immediate risk of returning incorrect sessions was mitigated. . After we deployed the change to remove the background thread, we followed up by creating a patch for Unicorn to remove environment sharing. This patch was deployed on March 8 and further hardens the isolation between requests, even if thread safety issues occur. . In addition to fixing the underlying bug, we took action to make sure we identified affected user sessions. We examined our logging data for patterns consistent with incorrectly returned sessions. We then manually reviewed the logs matching the pattern to determine whether a session had in fact been incorrectly returned from one user to another. . As a final step, we decided to take a further preventative action to ensure the safety of our users’ data and revoked all active user sessions on GitHub.com. Considering the rarity of conditions required for this race condition, we knew that there was a very small chance of this bug occurring. While our log analysis, conducted from March 5 through March 8, confirmed that this was a rare issue, it could not rule out the possibility that a session had been incorrectly returned but then never used. This was not a risk we were willing to take, given the potential impact of even one of these incorrectly returned sessions being used. . With these two fixes in place and the revocation of sessions complete, we were confident that no new cases of the wrong session being returned could occur and that the impact of the bug had been mitigated. . While the immediate risk has been mitigated, we  worked with the maintainer of Unicorn  to upstream the change to make new requests allocate their own environment hashes. If Unicorn uses new hashes for each environment, it removes the possibility of one request mistakenly getting a hold of an object that can affect the next request. This is additional hardening that will help prevent similar thread safety bugs from turning into security vulnerabilities for other users of Unicorn. We want to sincerely thank the Unicorn maintainers for the collaboration and discussion on this issue. . Another step that we are taking is to remove the callbacks for our exception logging context. These callbacks were useful to delay execution and to avoid paying the performance overhead when not needed. The downside is that the callback also makes debugging race conditions more difficult and could potentially retain references to requests long since finished. Handling exceptions with simpler code allows us to use these routines in a safer manner now and in the future. Additionally, we are working to simplify and limit the code paths where the session cookie is managed. . Lastly, we’re looking at also improving the thread safety across our codebase in areas such as exception handling and instrumentation. We don’t want to just fix this specific bug, we want to ensure that this class of issues can’t happen again in the future. . Threaded code can be very hard to reason about and debug, especially in a codebase where threads have not been used extensively in the past. For now, we’ve stopped using long-running threads in our Rails processes. We’re taking this as an opportunity to make our code more robust for various threading contexts and to provide further protections and a thread-safe architecture to use in the future. . Taking a step back, a bug such as this is not only challenging from a technical perspective in how to identify complex interactions between multiple threads, deferred callbacks, and object sharing, but it is also a test of an organization’s ability to respond to a problem with an ambiguous cause and risk. A well-developed product security incident response team and process combined with a collaborative team of subject matter experts across our support, security, and engineering teams enabled us to quickly triage, validate, and assess the potential risk of this issue and drive prioritization across the company. This prioritization accelerated our efforts in log analysis, review of recent changes across our code and infrastructure, and ultimately the identification of the underlying issues that led to the bug. . With a clear understanding of the problem, we could then focus on shipping the correct mitigations to limit the impact to our customers. The knowledge we gained through this process allowed us to work with stakeholders across the company to confidently make the decision of logging all users out of GitHub.com. We have gained a better understanding of our “complex systems” and we will continue to use this as an opportunity to build in safeguards to prevent issues like this from happening again. ", "date": "March 18, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tEncapsulating Ruby on Rails views\t\t", "author": ["\n\t\tJoel Hawksley\t"], "link": "https://github.blog/2020-12-15-encapsulating-ruby-on-rails-views/", "abstract": " With the recent release of version 6.1, Ruby on Rails now supports the  rendering of objects  that respond to  render_in , a change  we   introduced  to the framework. It may be small (the two pull requests were less than a dozen lines), but this change has enabled us to develop a framework for building encapsulated views called  ViewComponent . . Unlike models and controllers, Rails views are not encapsulated. All Rails views in an application exist in a single execution context, meaning they can share state. This makes them hard to reason about and difficult to test, as they cannot be easily isolated. . The need for a new way of building views in our application emerged as the number of templates in the GitHub application grew into the thousands. We depended on a combination of presenters and partials with inline Ruby, tested by expensive integration tests that exercise the routing and controller layers in addition to the view layer. . Inspired by our experience building component-based UI with  React , we set off to build a framework to bring these ideas to server-rendered Rails views. . We created the ViewComponent framework for building reusable, testable &amp; encapsulated view components in Ruby on Rails. A ViewComponent is the combination of a Ruby file and a template file. For example: .  test_component.rb  .  test_component.html.erb  . Which is rendered in a view: . Returning: . Unlike traditional Rails views, ViewComponent templates are executed within the context of the ViewComponent object, encapsulating their state and allowing them to be unit tested in isolation. . For example, to test our TestComponent, we can write: . These kinds of unit tests enable us to test our view code directly, instead of via controller tests. They are also significantly faster: in the GitHub codebase, component unit tests take around 25 milliseconds each, compared to about six seconds for controller tests. . Over the past two years, we’ve made significant strides in using ViewComponent: we now have over 400 components used in over 1600 of our 4500+ templates. For example, every  Counter  and  Blankslate  on GitHub.com is rendered with a ViewComponent. . We’re seeing several significant benefits from this architecture. Because ViewComponents can be unit tested against their rendered DOM, we’ve been able to reduce duplication of test coverage for shared templates, which we previously covered with controller tests. And since ViewComponent tests are so fast, we’re finding ourselves writing more of them, leading to higher confidence in our view code. . We’ve also seen the positive impact of the consistency that component-driven view architecture can provide. When we implemented a ViewComponent for the status of pull requests, we discovered several locations where we had not updated our previous, copy-pasted implementation, to handle the then-recently-shipped draft pull request status. By standardizing on a source of truth for the UI pattern, we now have a single, consistent implementation. . To see some of the ViewComponents we’re using in the GitHub application, check out the  Primer ViewComponents  library. .  Support for 3rd-party component frameworks such as ViewComponent is just one of many contributions GitHub engineers contributed to Rails 6.1. Other notable additions include  support for horizontal sharding ,  strict loading , and  template annotations .  ", "date": "December 15, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tReducing flaky builds by 18x\t\t", "author": ["\n\t\tJordan Raine\t"], "link": "https://github.blog/2020-12-16-reducing-flaky-builds-by-18x/", "abstract": "  Part of the  Building GitHub blog series .  . It’s four o’clock in the afternoon as you push the last tweak to your branch. Your teammate already reviewed and approved your pull request and now all that’s left is to wait for CI. But, fifteen minutes later, your commit goes red. Surprised and a bit annoyed because the last five commits were green, you take a closer look only to find a failing test unrelated to your changes. When you run the test locally, it passes. . Half an hour later, after running the test again and again locally without a single failure and retrying a still-red CI, you’re no closer to deploying your code. As the clock ticks past five, you retry CI once more. But this time, something is different: it’s green! . You deploy, merge your pull request, and, an hour later than expected, close your laptop for the day. . This is the cost of a flaky test. They’re puzzling, frustrating, and waste your time. And try as we might to stop them, they’re about as common as a developer who brews pour-over coffee every morning (read: very). . Bonus points if you can guess what caused the test to fail. . Earlier this year in our monolith, 1 in 11 commits had at least one red build caused by a flaky test, or about 9 percent of commits. If you were trying to deploy something with a handful of commits, there was a good chance you’d need to retry the build or spend time diagnosing a failure, even if your code was fine. This slowed us down. . Six weeks ago, after introducing a system to manage flaky tests, the percentage of commits with flaky builds dropped to less than half a percent, or 1 in 200 commits. .   . This is an 18x improvement and the lowest rate of flaky builds since we began tracking flaky tests in 2016. . So, how does it work? .   . Say you just merged a test that would fail once every 1,000 builds. It didn’t fail during development or review but a few hours later, the test fails on your teammate’s branch. . When this failure occurs, the new system inspects it and finds it to be flaky. After ensuring the test passes when run against the same code, it  keeps the build green.  And it happens quickly: unless your teammate is watching closely, they won’t notice. . But your flaky test is still out there, failing on other branches. Every time it does, the system keeps track of where it happened and how it failed, each time learning more about the failure. If it continues to affect developers, the system identifies it as high impact: it’s time for a human to investigate. Using test failure history and  git blame , the system finds the commit most likely to have introduced the problem—your commit—and assigns an issue to you. .   .  Screenshot of GitHub’s internal CI tooling.  . From there, you can see information about the failure, including what may have caused it, where it failed, and who else might be responsible. After some poking around, you find the problem and merge a fix. . The system noticed a problem, contained it, and delegated it the right person. In short,  the only person bothered by a flaky test is the person who wrote it.  . When we set out to build this new system, our intent wasn’t to fix every flaky test or to stop developers from introducing new flaky tests. Such goals, if not impossible, seemed impractical. Similar to telling a developer to never write another bug, it would be much more costly and only slightly less flaky. Rather, we set out to manage the inevitability of flaky tests. . When inspecting the history of failures in our monolith, we found that about a quarter of our tests had failed flaky across three or more branches in the past two years. But as we filtered by occurence, we learned that the flakiness was not evenly distributed: most flaky test failed fewer than ten times and only 0.4 percent of flaky tests failed 100 times or more. .   . This made one thing clear: not every flaky failure should be investigated. . Instead, by focusing on this top 0.4 percent, we could make the most of our time. So which tests were in this group? . We define a “flaky” test result as a test that exhibits both a passing and a failing result with the same code. – John Micco, Google  . Since 2016, our CI has been able to detect whether a test failure is flaky using two complementary approaches: .  Same code, different results.  Once a build finishes, CI checks for other builds run against the same code using the root  git tree hash . If another build had different results—for example, a test failed on the first build but passed on the second—the test failure was marked as flaky. While this approach was accurate, it only worked if a build was retried. .  Retry tests that fail.  When a test failed, it was retried again later within the same build. This could be used on every build at minimal cost. If the test passed when rerun, it was marked as flaky. However, certain types of flaky tests couldn’t be detected with this approach, such as a time-based flaky test. (If your test failed because it was a leap year, rerunning it two minutes later won’t help.) . Unfortunately, these approaches were only able to identify 25 percent of the flaky failures, counting on developers to find the rest. Before delegating flaky test detection to CI, we needed an approach that was as good or better than a person. . We decided to iterate on the test retry approach by rerunning the test three times, each in a scenario targeting a common cause of flakiness. .  Retry in the same process.  This retry attempts to replicate the same conditions in which the test failed: same Ruby VM, same database, same host. If the test passes under the same conditions, it is likely caused by randomness in the code or a race condition. .  Retry in the same process, shifted into the future.  This retry attempts to replicate the same conditions with one exception: time. If the test passes when run in the future, as simulated by test helpers, it is likely caused by an incorrect assumption about time (e.g., “there are 28 days in February”). .  Retry on a different host.  This attempts to run the same code in a completely separate environment: different Ruby VM, different database, different host. If the test passes under these conditions but fails in the other two retries, it is likely caused by test order-dependence or some other shared state. . Using this approach,  we are able to automatically identify 90 percent of flaky failures.  . Further, because we kept a history of how a test would fail, we were also able to estimate the cause of flakiness: chance, time-based, or order-dependent. When it came time to fix a test, this gave the developer a headstart. . Once we could accurately detect flaky failures, we needed a way to quantify impact to automate prioritization. . To do this, we used information tracked with every test failure: build, branch, author, commit, and more. Using this information, a flaky test is given an impact score based on how many times it has failed as well as how many branches, developers, and deploys were affected by it. The higher the score, the more important the flaky test. . Once the score exceeds a certain threshold, an issue is automatically opened and assigned to the people who most recently modified either the test files or associated code prior to the test becoming flaky. To help jog the memory of those assigned, a link to the commit that may have introduced the problem is added to the issue. . Teams can also view flaky tests by impact, CODEOWNER, or suite, giving insight into the test suite and giving developers a TODO list for problem areas. . By reducing the number of flaky builds by 18x, the new system makes CI more trustworthy and red builds more meaningful. If your pull request has a failure, it’s a sign you need to change something, not a sign you should hit  Rebuild . When it comes time to deploy, you can be sure that your build won’t go red late in the day because a test doesn’t take into account daylight saving time. . This keeps us moving, even when we make mistakes. ", "date": "December 16, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tCommits are snapshots, not diffs\t\t", "author": ["\n\t\tDerrick Stolee\t"], "link": "https://github.blog/2020-12-17-commits-are-snapshots-not-diffs/", "abstract": " Git  has a reputation  for  being confusing . Users stumble over terminology and phrasing that misguides their expectations. This is most apparent in commands that “rewrite history” such as  git cherry-pick  or  git rebase . In my experience, the root cause of this confusion is an interpretation of commits as  diffs  that can be shuffled around. However,  commits are snapshots, not diffs!  . I believe that Git becomes understandable if we peel back the curtain and look at how Git stores your repository data. After we investigate this model, we’ll explore how this new perspective helps us understand commands like  git cherry-pick  and  git rebase . . If you want to go  really  deep, you should read  the Git Internals chapter of the Pro Git book . . I’ll be using the   git/git  repository  checked out at  v2.29.2  as an example. Follow along with my command-line examples for extra practice. . The most important part to know about Git objects is that Git references each by its  object ID  (OID for short), providing a unique name for the object. We will use  the  git rev-parse &lt;ref&gt;  command  to discover these OIDs. Each object is essentially a plain-text file and we can examine its contents using  the  git cat-file -p &lt;oid&gt;  command . . You might also be used to seeing OIDs given as a shorter hex string. This string is given as something long enough that only one object in the repository has an OID that matches that abbreviation. If we request the type of an object using an abbreviated OID that is too short, then we will see the list of OIDs that match . What are these types:  blob ,  tree , and  commit ? Let’s start at the bottom and work our way up. . At the bottom of the object model,  blobs  contain file contents. To discover the OID for a file at your current revision, run  git rev-parse HEAD:&lt;path&gt; . Then, use  git cat-file -p &lt;oid&gt;  to find its contents. . If I edit the  README.md  file on my disk, then  git status  notices that the file has a recent modified time and hashes the contents. If the contents don’t match the current OID at  HEAD:README.md , then  git status  reports the file as “modified on disk.” In this way, we can see if the file contents in the current working directory match the expected contents at  HEAD . . Note that blobs contain file  contents , but not the file  names ! The names come from Git’s representation of directories:  trees . A tree is an ordered list of path entries, paired with object types, file modes, and the OID for the object at that path. Subdirectories are also represented as trees, so trees can point to other trees! .   . We will use diagrams to visualize how these objects are related. We use boxes for blobs and triangles for trees. . Trees provide names for each sub-item. Trees also include information such as Unix file permissions, object type ( blob  or  tree ), and OIDs for each entry. We cut the output to the top 15 entries, but we can use  grep  to discover that this tree has a  README.md  entry that points to our earlier blob OID. . Trees can point to blobs and other trees using these path entries. Keep in mind that those relationships are paired with path names, but we will not always show those names in our diagrams. . The tree itself doesn’t know where it exists within the repository, that is the role of the objects pointing to the tree. The tree referenced by  &lt;ref&gt;^{tree}  is a special tree: the  root tree . This designation is based on a special link from your  commits . . A  commit  is a snapshot in time. Each commit contains a pointer to its root tree, representing the state of the working directory at that time. The commit has a list of  parent commits  corresponding to the previous snapshots. A commit with no parents is a  root commit  and a commit with multiple parents is a  merge commit . Commits also contain metadata describing the snapshot such as author and committer (including name, email address, and date) and a commit message. The commit message is an opportunity for the commit author to describe the purpose of that commit with respect to the parents. .   . For example, the commit at  v2.29.2  in the Git repository describes that release, and is authored and committed by the Git maintainer. . Looking a little farther in the history with  git log , we can see a more descriptive commit message talking about the change between that commit and its parent. . In our diagrams, we will use  circles  to represent commits. Notice the alliteration? Let’s review: . In Git, we move around the history and make changes without referring to OIDs most of the time. This is because  branches  provide pointers to the commits we care about. A branch with name  main  is actually a reference in Git called  refs/heads/main . These files literally contain hex strings referencing the OID of a commit. As you work, these references change their contents to point to other commits. . This means branches are significantly different from our previous Git objects. Commits, trees, and blobs are  immutable , meaning you can’t change their contents. If you change the contents, then you get a different hash and thus a new OID referring to the new object! Branches are named by users to provide meaning, such as  trunk  or  my-special-project . We use branches to track and share work. . The special reference  HEAD  points to the current branch. When we add a commit to  HEAD , it automatically updates that branch to the new commit. . We can create a new branch and update our  HEAD  using   git switch -c  : . Notice how creating  my-branch  created a file ( .git/refs/heads/my-branch ) containing the current commit OID and the  .git/HEAD  file was updated to point at this branch. Now, if we update  HEAD  by creating new commits, the branch  my-branch  will update to point to that new commit! . Let’s put all of these new terms into one giant picture. Branches point to commits, commits point to other commits and their root trees, trees point to blobs and other trees, and blobs don’t point to anything. Here is a diagram containing all of our objects all at once: .   . In this diagram, time moves from left to right. The arrows between a commit and its parents go from right to left. Each commit has a single root tree.  HEAD  points to the  main  branch here, and  main  points to the most-recent commit. The root tree at this commit is fully expanded underneath, while the rest of the trees have arrows pointing towards these objects. The reason for that is that the same objects are reachable from multiple root trees! Since these trees reference those objects by their OID (their  content ) these snapshots do not need multiple copies of the same data. In this way, Git’s object model forms a  Merkle tree . . When we view the object model in this way, we can see why commits are snapshots: they link directly to a full view of the expected working directory for that commit! . Even though commits are snapshots, we frequently look at a commit in a history view or  on GitHub as a diff . In fact, the commit message frequently refers to this diff. The diff is  dynamically generated  from the snapshot data by comparing the root trees of the commit and its parent. Git can compare any two snapshots in time, not just adjacent commits. . To compare two commits, start by looking at their root trees, which are almost always different. Then, perform a depth-first-search on the subtrees by following pairs when paths for the current tree have different OIDs. In the example below, the root trees have different values for the  docs , so we recurse into those two trees. Those trees have different values for  M.md , so those two blobs are compared line-by-line and that diff is shown. Still within  docs ,  N.md  is the same, so that is skipped and we pop back to the root tree. The root tree then sees that the  things  directories have equal OIDs as well as the  README.md  entries. .   . In the diagram above, we notice that the  things  tree is never visited, and so none of its reachable objects are visited. This way, the cost of computing a diff is relative to the number of paths with different content. . Now we have the understanding that  commits are snapshots  and we can dynamically compute a diff between any two commits. Then why isn’t this common knowledge? Why do new users stumble over this idea that a commit is a diff? . One of my favorite analogies is to think of commits as having  a wave/partical duality  where  sometimes  they are treated like snapshots and  other times  they are treated like diffs. The crux of the matter really goes into a different kind of data that’s not actually a Git object: patches. . A  patch  is a text document that describes how to alter an existing codebase. Patches are how extremely-distributed groups can share code without using Git commits directly. You can see these being shuffled around on  the Git mailing list . . A patch contains a description of the change and why it is valuable, followed by a diff. The idea is that someone could use that reasoning as a justification to  apply  that diff to their copy of the code. . Git can convert a commit into a patch using   git format-patch  . A patch can then be applied to a Git repository using   git apply  . This was the dominant way to share code in the early days of open source, but most projects have moved to sharing Git commits directly through pull requests. . The biggest issue with sharing patches is that the patch  loses the parent information  and the new commit has a parent equal to your existing  HEAD . Moreover, you get a different commit even if you use the same parent as before due to the commit time, but also the committer changes! This is the fundamental reason why Git has both “author” and “committer” details in the commit object. . The biggest problem with using patches is that it is hard to apply a patch when your working directory does not match the sender’s previous commit. Losing the commit history makes it difficult to resolve conflicts. . This idea of “moving patches around” has transferred into several Git commands as “moving commits around.” Instead, what actually happens is that commit diffs are  replayed , creating new commits. . The   git cherry-pick &lt;oid&gt;  command  creates a new commit with an identical diff to  &lt;oid&gt;  whose parent is the current commit. Git is essentially following these steps: . Compute the diff between the commit  &lt;oid&gt;  and its parent. . Apply that diff to the current  HEAD . . Create a new commit whose root tree matches the new working directory and whose parent is the commit at  HEAD . . Move the ref at  HEAD  to that new commit. .   . After Git creates the new commit, the output of  git log -1 -p HEAD  should match the output of  git log -1 -p &lt;oid&gt; . . It is important to recognize that we didn’t “move” the commit to be on top of our current  HEAD , we  created a new commit  whose diff matches the old commit. . The  git rebase  command presents itself as a way to move commits to have a new history. In its most basic form it is really just a series of  git cherry-pick  commands, replaying diffs on top of a different commit. . The most important thing is that  git rebase &lt;target&gt;  will discover the list of commits that are reachable from  HEAD  but not reachable from  &lt;target&gt; . You can show these yourself using  git log --oneline &lt;target&gt;..HEAD . . Then, the  rebase  command simply navigates to the  &lt;target&gt;  location and starts performing  git cherry-pick  commands on this commit range, starting from the oldest commits. At the end, we have a new set of commits  with different OIDs  but similar diffs to the original commit range. . For example, consider a sequence of three commits in the current  HEAD  since branching off of a  target  branch. When running  git rebase target , the common base  P  is computed to determine the commit list  A ,  B , and  C . These are then cherry-picked on top of  target  in order to construct new commits  A' ,  B' , and  C' . .   . The commits  A' ,  B' , and  C'  are brand new commits that share a lot of information with  A ,  B , and  C , but are distinct new objects. In fact, the old commits still exist in your repository until garbage collection runs. . We can even inspect how these two commit ranges are different using the  git range-diff  command! I’ll use some example commits in the Git repository to rebase onto the  v2.29.2  tag, then modify the tip commit slightly. . Notice that the resulting range-diff claims that commits  17e7dbbcbc  and  2aa8919906  are “equal”, which means they would generate the same patch. The second pair of commits are different, showing that the commit message changed and there is an edit to the  README.md  that was not in the original commit. . If you are following along, you can also see how the commit history still exists for these two commit sets. The new commits have the  v2.29.2  tag as the third commit in the history while the old commits have the (earlier)  v2.28.0  tag as the third commit. . If you were looking carefully at the object model, you might have noticed that Git never tracks changes between commits in the stored object data. You might have wondered “how does Git know a rename happened?” . Git doesn’t track renames. There is no data structure inside Git that stores a record that a rename happened between a commit and its parent. Instead, Git tries to  detect  renames during the dynamic diff calculation. There are two stages to this rename detection: exact renames and edit-renames. . After first computing a diff, Git inspects the internal model of that diff to discover which paths were  added  or  deleted . Naturally, a file that was moved from one location to another would appear as a deletion from the first location and an add in the second. Git attempts to  match  these adds and deletes to create a set of  inferred  renames. . The first stage of this matching algorithm looks at the OIDs of the paths that were added and deleted and see if any are exact matches. Such exact matches are paired together. . The second stage is the expensive part: how can we detect files that were renamed  and edited ? Git iterates through each added file and compares that file against each deleted file to compute a  similarity score  as a percentage of lines in common. By default, anything larger than 50% of lines in common counts as a potential edit-rename. The algorithm continues comparing these pairs until finding the maximum match. . Did you notice a problem? This algorithm runs  A * D  diffs, where  A  is the number of adds and  D  is the number of deletes. This is quadratic! To avoid extra-long rename computations, Git will skip this portion of detecting edit-renames if  A + D  is larger than an internal limit. You can modify this limit using the   diff.renameLimit  config option . You can also avoid the algorithm altogether by disabling the  diff.renames  config option. . I’ve used my awareness of the Git rename detection in my own projects. For example, I forked  VFS for Git  to create the  Scalar  project and wanted to re-use a lot of the code but also change the file structure significantly. I wanted to be able to follow the history of these files into the versions in the VFS for Git codebase, so I constructed my refactor in two steps: .  Rename all of the files without changing the blobs . .  Replace strings to modify the blobs without changing filenames . . These two steps ensured that I can quickly use  git log --follow -- &lt;path&gt;  to see the history of a file across this rename. . I abbreviated the output, but these last two commits don’t actually have a path corresponding to  Scalar/CommandLine/ScalarVerb.cs , but instead it is tracking the previous path  GVSF/GVFS/CommandLine/GVFSVerb.cs  because Git recognized the exact-content rename from the commit  fb3a2a36 [RENAME] Rename all files . . You now know that  commits are snapshots, not diffs!  This understanding will help you navigate your experience working with Git. . Now you are armed with deep knowledge of the Git object model. You can use this knowledge to expand your skills in using Git commands or deciding on workflows for your team. In a future blog post, we will use this knowledge to learn about different Git clone options and how to reduce the data you need to get things done! ", "date": "December 17, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we built the GitHub globe\t\t", "author": ["\n\t\tTobias Ahlin\t"], "link": "https://github.blog/2020-12-21-how-we-built-the-github-globe/", "abstract": " GitHub is where the world builds software. More than 56 million developers around the world build and work together on GitHub. With our new  homepage , we wanted to  show  how open source development transcends the borders we’re living in and to tell our product story through the lens of a developer’s journey. . Now that it’s live, we would love to share how we built the homepage-directly from the voices of our designers and developers. In this five-part series, we’ll discuss: .  How our globe is built  .  How we collect and use the data behind the globe  .  How we made the page fast and performant  .  How we illustrate at GitHub  .  How we designed the homepage and wrote the narrative  .   . At Satellite in 2019, our CEO  Nat  showed off  a visualization of open source activity  on GitHub over a 30-day span. The sheer volume and global reach was astonishing, and we knew we wanted to build on that story. .   .      The main goals we set out to achieve in the design and development of the globe were: . At the most fundamental level, the globe runs in a WebGL context powered by  three.js . We feed it data of recent pull requests that have been created and merged around the world through a JSON file. The scene is made up of five layers: a halo, a globe, the Earth’s regions, blue spikes for open pull requests, and pink arcs for merged pull requests. We don’t use any textures: we point four lights at a sphere, use about 12,000 five-sided  circles  to render the Earth’s regions, and draw a halo with a simple custom shader on the backside of a sphere. .   . To draw the Earth’s regions, we start by defining the desired density of circles (this will vary depending on the performance of your machine—more on that later), and loop through longitudes and latitudes in a nested for-loop. We start at the south pole and go upwards, calculate the circumference for each latitude, and distribute circles evenly along that line, wrapping around the sphere: . To determine if a circle should be visible or not (is it water or land?) we load a small PNG containing a map of the world, parse its image data through canvas’s  context.getImageData() , and map each circle to a pixel on the map through the visibilityForCoordinate(long, lat) method. If that pixel’s alpha is at least 90 (out of 255), we draw the circle; if not, we skip to the next one. . After collecting all the data we need to visualize the Earth’s regions through these small circles, we create an instance of  CircleBufferGeometry  and use an  InstancedMesh  to render all the geometry. . As you enter the new GitHub homepage, we want to make sure that you can see your own location as the globe appears, which means that we need to figure where on Earth that you are. We wanted to achieve this effect without delaying the first render behind an IP look-up, so we set the globe’s starting angle to center over Greenwich, look at  your device’s timezone offset , and convert that offset to a rotation around the globe’s own axis (in radians): . It’s not an  exact  measurement of your location, but it’s quick, and does the job. . The main act of the globe is, of course, visualizing all of the pull requests that are being opened and merged around the world. The data engineering that makes this possible is a different topic in and of itself, and we’ll be sharing how we make that happen in an  upcoming post . Here we want to give you an overview of how we’re visualizing all your pull requests. .   . Let’s focus on pull requests being merged (the pink arcs), as they are a bit more interesting. Every merged pull request entry comes with two locations: where it was opened, and where it was merged. We map these locations to our globe, and draw a bezier curve between these two locations: . We have three different orbits for these curves, and the longer the two points are apart, the further out we’ll pull out any specific arc into space. We then use instances of  TubeBufferGeometry  to generate geometry along these paths, so that we can use  setDrawRange()  to animate the lines as they appear and disappear. . As each line animates in and reaches its merge location, we generate and animate in one solid  circle  that stays put while the line is present, and one  ring  that scales up and immediately fades out. The ease out easings for these animations are created by multiplying a speed (here 0.06) with the difference between the target (1) and the current value (animated.dot.scale.x), and adding that to the existing scale value. In other words, for every frame we step 6% closer to the target, and as we’re coming closer to that target, the animation will naturally slow down. . The homepage and the globe needs to perform well on a variety of devices and platforms, which early on created some creative restrictions for us, and made us focus extensively on creating a well-optimized page. Although some modern computers and tablets could render the globe at 60 FPS with antialias turned on, that’s not the case for all devices, and we decided early on to leave antialias turned off and optimize for performance. This left us with a sharp and pixelated line running along the top left edge of the globe, as the globe’s highlighted edge met the darker color of the background: .   . This encouraged us to explore a halo effect that could hide that pixelated edge. We created one by using a custom shader to draw a gradient on the backside of a sphere that’s slightly larger than the globe, placed it behind the globe, and tilted it slightly on its side to emphasize the effect in the top left corner: .   . This smoothed out the sharp edge, while being a much more performant operation than turning on antialias. Unfortunately, leaving antialias off also produced a fairly prominent  moiré effect  as all the circles making up the world came closer and closer to each other as they neared the edges of the globe. We reduced this effect and simulated the look of a thicker atmosphere by using a  fragment shader  for the circles where each circle’s alpha is a function of its distance from the camera, fading out every individual circle as it moves further away: . We don’t know how quickly (or slowly) the globe is going to load on a particular device, but we wanted to make sure that the header composition on the homepage is always balanced, and that you got the impression that the globe loads quickly even if there’s a slight delay before we can render the first frame. . We created a bare version of the globe using only gradients in  Figma  and exported it as an SVG. Embedding this SVG in the HTML document adds little overhead, but makes sure that  something  is immediately visible as the page loads. As soon as we’re ready to render the first frame of the globe, we transition between the SVG and the canvas element by crossfading between and scaling up both elements using the  Web Animations API . Using the Web Animations API enables us to not touch the DOM at all during the transition, ensuring that it’s as stutter-free as possible. . We aim at maintaining 60 FPS while rendering an as beautiful globe as we can, but finding that balance is tricky—there are thousands of devices out there, all performing differently depending on the browser they’re running and their mood. We constantly monitor the achieved FPS, and if we fail to maintain 55.5 FPS over the last 50 frames we start to degrade the quality of the scene. .   . There are four quality tiers, and for every degradation we reduce the amount of expensive calculations. This includes reducing the pixel density, how often we raycast (figure out what your cursor is hovering in the scene), and the amount of geometry that’s drawn on screen—which brings us back to the circles that make up the Earth’s regions. As we traverse down the quality tiers, we reduce the desired circle density and rebuild the Earth’s regions, here going from the original ~12 000 circles to ~8 000: . These are some of the techniques that we use to render the globe, but the creation of the globe and the new homepage is part of a longer story, spanning multiple teams, disciplines, and departments, including design, brand, engineering, product, and communications. We’ll continue the deep-dive in this 5-part series, so come back soon or follow us on Twitter  @GitHub  for all the latest updates on this project and more. .  Next up:   how we collect and use the data behind the globe . . In the meantime, don’t miss out on the new GitHub globe wallpapers from the GitHub Illustration Team to enjoy the globe from your desktop or mobile device: .  Love the new GitHub homepage or any of the work you see here?    Join our team !   .  \t\t Tags:   \t\t Homepage design \t ", "date": "December 21, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tVisualizing GitHub’s global community\t\t", "author": ["\n\t\tTal Safran\t"], "link": "https://github.blog/2020-12-21-visualizing-githubs-global-community/", "abstract": " This is the second post in a series about how we built our new homepage. .  How our globe is built  .  How we collect and use the data behind the globe  .  How we made the page fast and performant  .  How we illustrate at GitHub  .  How we designed the homepage and wrote the narrative  . In the first post, my teammate  Tobias  shared  how we made the 3D globe come to life , with lots of nitty gritty details about Three.js, performance optimization, and delightful touches. . But there’s another side to the story—the data! We hope you enjoy the read. ✨ . When we kicked off the project, we knew that we didn’t want to make just another animated globe. We wanted the data to be interesting and engaging. We wanted it to be real, and most importantly, we wanted it to be live. . Luckily, the data was there. . The challenge then became designing a data service that addressed the following challenges: . How do we query our massive volume of data? . How do we show you the most interesting bits? . How do we geocode user locations in a way that respects privacy? . How do we expose the computed data back to the monolith? . How do we not break GitHub? 😊 . Let’s begin, shall we? . So, how hard could it be to show you some recent pull requests? It turns out it’s actually very simple: . Just kidding 😛 . Because of the volume of data generated on GitHub every day, the size of our databases, as well as the importance of keeping GitHub fast and reliable, we knew we couldn’t query our production databases directly. . Luckily, we have a data warehouse and a fantastic team that maintains it. Data from production is fetched, sanitized, and packaged nicely into the data warehouse on a regular schedule. The data can then be queried using  Presto , a flavor of SQL meant for querying large sets of data. . We also wanted the data to be as fresh as possible. So instead of querying snapshots of our MySQL tables that are only copied over once a day, we were able to query data coming from our  Apache Kafka  event stream that makes it into the data warehouse much more regularly. . As an example, we have an event that is reported every time a pull request is merged. The event is defined in a format called  protobuf , which stands for “protocol buffer.” . Here’s what the protobuf for a merged pull request event might look like: . Each row corresponds to an “entity,” each of which is defined in its own protobuf file. Here’s a snippet from the definition of a pull request entity: . Including an entity in an event will pass along all of the attributes defined for it. All of that data gets copied into our data warehouse for every pull request that is merged. . This means that a Presto query for pull requests merged in the past day could look like: . There are a few other queries we make to pull in all the data we need. But as you can see, this is pretty much standard SQL that pulls in merged pull requests from the last day in the event stream. . We wanted to make sure that whatever data we showed was interesting, engaging, and appropriate to be spotlighted on the GitHub homepage. If the data was good, visitors would be enticed to explore the vast ecosystem of open source being built on GitHub at that given moment. Maybe they’d even make a contribution! . So how do we find good data? . Luckily our data team came to the rescue yet again. A few years ago, the Data Science Team put together a model to rank the “health” of repositories based on 30-plus features weighted by importance. A healthy repository doesn’t necessarily mean having a lot of stars. It also takes into account how much current activity is happening and how easy it is to contribute to the project, to name a few. . The end result is a numerical health score that we can query against in the data warehouse. . Combining this query with the above, we can now pull in merged pull requests from repositories with health scores above a certain threshold: . We do some other things to ensure the data is good, like filtering out accounts with spammy behavior. But repository health scores are definitely a key ingredient. . Your GitHub profile has an optional free text field for providing your location. Some people fill it out with their actual location (mine says “San Francisco”), while others use fake or funny locations (42 users have “Middle Earth” listed as theirs). Many others choose to not list a location. In fact, two-thirds of users don’t enter anything and that’s perfectly fine with us. . For users that do enter something, we try to map the text to a real location. This is a little harder to do than using IP addresses as proxies for locations, but it was important to us to only include data that users felt comfortable making public in the first place. . In order to map the free text locations to latitude and longitude pairs, we use  Mapbox’s forward geocoding API  and their  Ruby SDK . Here’s an example of a forward geocoding of “New York City”: . There is a lot of data there, but let’s focus on  text ,  relevance , and  center  for now. Here are those fields for the “New York City”: . If you use “NYC” query string, you get the exact same result: . Notice that the  text  is still “New York City” in this second example? That is because Mapbox is normalizing the results. We use the normalized text on the globe so viewers get a consistent experience. This also takes care of capitalization and misspellings. . The  center  field is an array containing the longitude and latitude of the location. . And finally, the  relevance  score is an indicator of Mapox’s confidence in the results. A relevance score of 1 is the highest, but sometimes users enter locations that Mapbox is less sure about: . We discard anything with a score of less than 1, just to get confidence that the location we show feels correct. . Mapbox also provides a  batch geocoding  endpoint. This allows us to query multiple locations in one request: . After we’ve geocoded and normalized all of the results, we create a JSON representation of the pull request and its locations so our globe JavaScript client knows how to parse it. . Here’s a  pull request  we recently featured that was opened in San Francisco and merged in Tokyo: . We use short keys to shave off some bytes from the JSON we end up serving so the globe loads faster. . We run our data warehouse queries and geocoding throughout the day to ensure that the data on the homepage is always fresh. . For scheduling this work, we use another system from Apache called  Airflow . Airflow lets you run scheduled jobs that contain a sequence of tasks. Airflow calls these workflows Direct Acyclical Graphs (or DAGs for short), which is a  borrowed term  from graph theory in computer science. Basically this means that you schedule one task at a time, execute the task, and when the task is done, then the next task is scheduled and eventually executed. Tasks can pass along information to each other. . At a high level, our DAG executes the following tasks: . Query the data warehouse. . Geocode locations from the results. . Write the results to a file. . Expose the results to the GitHub Rails app. . We covered the first two steps earlier. For writing the file, we use  HDFS , which is a distributed file system that’s part of the Apache Hadoop project. The file is then uploaded to Munger, an internal service we use to expose results from the data science pipeline back to the GitHub Rails app that powers github.com. . Here’s what this might look like in the Airflow UI: .   . Each column in that screenshot represents a full DAG run of all of the tasks. The last column with the light green circle at the top indicates that the DAG is in the middle of a run. It’s completed the  build_home_page_globe_table  task (represented by a dark green box) and now has the next task  write_to_hdfs  scheduled (dark blue box). . Our Airflow instance runs more than just this one DAG throughout the day, so we may stay in this state for some time before the scheduler is ready to pick up the  write_to_hdfs  task. Eventually the remaining tasks should run. If everything ends up running smoothly, we should see all green: .   . Hope that gives you a glimpse into how we built this! . Again, thank you to all the teams that made the GitHub homepage and globe possible. This project would not have been possible without years of investment in our data infrastructure and data science capabilities, so a special shout out to  Kim ,  Jeff ,  Preston ,  Ike ,  Scott ,  Jamison ,  Rowan , and  Omoju . . More importantly, we could not have done it without  you , the GitHub community, and your daily contributions and projects that truly bring the globe to life. Stay tuned—we have even more in store for this project coming soon. . In the meantime, I hope to see you on the homepage soon. 😉 .  \t\t Tags:   \t\t Homepage design \t ", "date": "December 21, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGet up to speed with partial clone and shallow clone\t\t", "author": ["\n\t\tDerrick Stolee\t"], "link": "https://github.blog/2020-12-21-get-up-to-speed-with-partial-clone-and-shallow-clone/", "abstract": " As your Git repositories grow, it becomes harder and harder for new developers to clone and start working on them. Git is designed as a  distributed  version control system. This means that you can work on your machine without needing a connection to a central server that controls how you interact with the repository. This is only fully realizable if you have all reachable data in your local repository. . What if there was a better way? Could you get started working in the repository without downloading every version of every file in the entire Git history? Git’s  partial clone  and  shallow clone  features are options that can help here, but they come with their own tradeoffs. Each option breaks at least one expectation from the normal distributed nature of Git, and you might not be willing to make those tradeoffs. . If you are working with an extremely large monorepo, then these tradeoffs are more likely to be worthwhile or even  necessary  to interact with Git at that scale! . Before digging in on this topic, be sure you are familiar with how Git stores your data, including  commits, trees, and blob objects . I presented some of these ideas and other helpful tips at GitHub Universe in my talk,  Optimize your monorepo experience . .      . There are three ways to reduce clone sizes for repositories hosted by GitHub. . As we discuss the different clone types, we will use a common representation of Git objects: . We use arrows to represent a relationship between objects. Basically, if an OID  B  appears inside a commit or tree  A , then the object  A  has an arrow to the object  B . If we can follow a list of arrows from an object  A  to another object  C , then we say  C  is  reachable  from  A . The process of following these arrows is sometimes referred to as  walking objects . . We can now describe the data downloaded by a  git clone  command! The client asks the server for the latest commits, then the server provides those objects  and every other reachable object . This includes every tree and blob in the entire commit history! .   . In this diagram, time moves from left to right. The arrows between a commit and its parents therefore go from right to left. Each commit has a single root tree. The root tree at the  HEAD  commit is fully expanded underneath, while the rest of the trees have arrows pointing towards these objects. . This diagram is purposefully simple, but if your repository is very large you will have many commits, trees, and blobs in your history. Likely, the historical data forms a majority of your data. Do you actually need all of it? . These days, many developers always have a network connection available as they work, so asking the server for a little more data when necessary might be an acceptable trade-off. . This is the critical design change presented by partial clone. . Git’s partial clone feature is enabled by specifying  the  --filter  option in your  git clone  command . The full list of filter options exist in  the  git rev-list  documentation , since you can use  git rev-list --filter=&lt;filter&gt; --all  to see which objects in your repository match the filter. There are several filters available, but the server can choose to deny your filter and revert to a full clone. . On  github.com  and GitHub Enterprise Server 2.22+, there are two options available: .  Blobless clones:   git clone --filter=blob:none &lt;url&gt;  .  Treeless clones:   git clone --filter=tree:0 &lt;url&gt;  . Let’s investigate each of these options. . When using the  --filter=blob:none  option, the initial  git clone  will download all reachable commits and trees, and only download the blobs for commits when you do a  git checkout . This includes the first checkout inside the  git clone  operation. The resulting object model is shown here: .   . The important thing to notice is that we have a copy of every blob at  HEAD  but the blobs in the history are not present. If your repository has a deep history full of large blobs, then this option can significantly reduce your  git clone  times. The commit and tree data is still present, so any subsequent  git checkout  only needs to download the missing blobs. The Git client knows how to batch these requests to ask the server only for the missing blobs. . Further, when running  git fetch  in a blobless clone, the server only sends the new commits and trees. The new blobs are downloaded only after a  git checkout . Note that  git pull  runs  git fetch  and then  git merge , so it will download the necessary blobs during the  git merge  command. . When using a blobless clone, you will trigger a blob download whenever you need the  contents  of a file, but you will not need one if you only need the OID of a file. This means that  git log  can detect which commits changed a given path without needing to download extra data. . This means that blobless clones can perform commands like  git merge-base ,  git log , or even  git log -- &lt;path&gt;  with the same performance as a full clone. . Commands like  git diff  or  git blame &lt;path&gt;  require the contents of the paths to compute diffs, so these will trigger blob downloads the first time they are run. However, the good news is that after that you will have those blobs in your repository and do not need to download them a second time. Most developers only need to run  git blame  on a small number of files, so this tradeoff of a slightly slower  git blame  command is worth the faster clone and fetch times. . Blobless clones are the most widely-used partial clone option. I’ve been using them myself for months without issue. . In some repositories, the tree data might be a significant portion of the history. Using  --filter=tree:0 , a treeless clone downloads all reachable commits, then downloads trees and blobs on demand. The resulting object model is shown here: .   . Note that we have all of the data at  HEAD , but otherwise only have commit data. This means that the initial clone can be  much  faster in a treeless clone than in a blobless or full clone. Further, we can run  git fetch  to download only the latest commits. However, working in a treeless clone is more difficult because downloading a missing tree when needed is more expensive. . For example, a  git checkout  command changes the  HEAD  commit, usually to a commit where we do not have the root tree. The Git client then asks the server for that root tree by OID, but also for all reachable trees from that root tree. Currently, this request does not tell the server that the client already has some root trees, so the server might send many trees the client already has locally. After the trees are downloaded, the client can detect which blobs are missing and request those in a batch. . It is possible to work in a treeless clone without triggering too many requests for extra data, but it is much more restrictive than a blobless clone. . For example, history operations such as  git merge-base  or  git log  (without extra options) only use commit data. These will not trigger extra downloads. . However, if you run a file history request such as  git log -- &lt;path&gt; , then a treeless clone will start downloading root trees for almost every commit in the history! . We  strongly recommend  that developers do not use treeless clones for their daily work. Treeless clones are really only helpful for automated builds when you want to quickly clone, compile a project, then throw away the repository. In environments like GitHub Actions using public runners, you want to minimize your clone time so you can spend your machine time actually building your software! Treeless clones might be an excellent option for those environments. . ⚠️  Warning:  While writing this article, we were putting treeless clones to the test beyond the typical limits. We noticed that repositories that contain  submodules  behave very poorly with treeless clones. Specifically, if you run  git fetch  in a treeless clone, then the logic in Git that looks for changed submodules will trigger a tree request for every new commit! This behavior can be avoided by running  git config fetch.recurseSubmodules false  in your treeless clones. We are working on a more robust fix in the Git client. . Partial clones are relatively new to Git, but there is an older feature that does something very similar to a treeless clone: shallow clones. Shallow clones use  the  --depth=&lt;N&gt;  parameter in  git clone   to truncate the commit history. Typically,  --depth=1  signifies that we only care about the most recent commits. Shallow clones are best combined with the  --single-branch --branch=&lt;branch&gt;  options as well, to ensure we only download the data for the commit we plan to use immediately. . The object model for a shallow clone is shown in this diagram: .   . Here, the commit at  HEAD  exists, but its connection to its parents and the rest of the history is severed. The commits whose parents are removed are called  shallow commits  and together form the  shallow boundary . The commit objects themselves have not changed, but there is some metadata in the client repository directing the Git client to ignore those parent connections. All trees and blobs are downloaded for any commit that exists on the client. . Since the commit history is truncated, commands such as  git merge-base  or  git log  show different results than they would in a full clone! In general, you cannot count on them to work as expected. Recall that these commands work as expectedly in partial clones. Even in blobless clones, commands like  git blame -- &lt;path&gt;  will work correctly, if only a little slower than in full clones. Shallow clones don’t even make that a possibility! . The other major difference is how  git fetch  behaves in a shallow clone. When fetching new commits, the server must provide every tree and blob that is “new” to these commits, relative to the shallow commits. This computation can be more expensive than a typical fetch, partly because a well-maintained server can make use of  reachability bitmaps . Depending on how others are contributing to your remote repository, a  git fetch  operation in a shallow clone might end up downloading an almost-full commit history! . Here are some descriptions of things that can go wrong with shallow clones that negate the supposed values. For these reasons we  do not recommend shallow clones  except for builds that delete the repository immediately afterwards. Fetching from shallow clones can cause more harm than good! . Remember the “shallow boundary” mentioned earlier? The client sends that boundary to the server during a  git fetch  command, telling the server that it doesn’t have all of the reachable commits behind that boundary. The client then asks for the latest commits and everything reachable from those  until hitting a shallow commit in the boundary . If another user starts a topic branch below that boundary and then the shallow client fetches that topic (or worse, the topic is merged into the default branch), then the server needs to walk the full history and serve the client what amounts to almost a full clone! Further, the server needs to calculate that data without the advantage of performance features like reachability bitmaps. .   . Let’s recall each of our clone options. Instead of looking them at a pure object level, let’s explore each category of object. The figures below group the data that is downloaded by each repository type. In addition to the data downloaded at clone, let’s consider the situation where some time passes and then the client runs  git fetch  and then  git checkout  to move to a new commit. For each of these options, how much data is downloaded. . Full clones download all reachable objects. Typically, blobs are responsible for most of this data. .   . In a partial clone, some data is not served immediately and is delayed until the client needs it. Blobless clones skip blobs except those needed at checkout time. Treeless clones skip all trees in the history in favor of downloading a full copy of the trees needed for each checkout. . Fellow GitHub engineer   @solmazabbaspour   designed and ran an experiment to compare these different clone options on a variety of open source repositories. She will post a blog post tomorrow giving full details and data for the experiment, but I’ll share the executive summary here. Here are some common themes we identified that could help you choose the right scenario for your own usage: . There are many different types of clones beyond the default full clone. If you truly need to have a distributed workflow and want all of the data in your local repository, then you should continue using full clones. If you are a developer focused on a single repository and your repository is reasonably-sized, the best approach is to do a full clone. . You might switch to a blobless partial clone if your repository is very large due to many large blobs, as that clone will help you get started more quickly. The trade-off is that some commands such as  git checkout  or  git blame  will require downloading new blob data when necessary. . In general, calculating a shallow fetch is computationally more expensive compared to a full fetch. Always use a full fetch instead of a shallow fetch both in fully and shallow cloned repositories. . In workflows such as CI builds when there is a need to do a single clone and delete the repository immediately, shallow clones are a good option. Shallow clones are the fastest way to get a copy of the working directory at the tip commit with the additional cost that fetching from these repositories is much more expensive, so we do not recommend shallow clones for developers. If you need the commit history for your build, then a treeless partial clone might work better for you than a full clone. . In general,  your mileage may vary . Now that you are armed with these different options and the object model behind them, you can go and play with these kinds of clones. You should also be aware of some pitfalls of these non-full clone options: . Be sure to upgrade to  the latest Git version  so you have all the latest performance improvements! .  \t\t Tags:   \t\t monorepo \t ", "date": "December 21, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit clone: a data-driven study on cloning behaviors\t\t", "author": ["\n\t\tSolmaz Abbaspoursani\t"], "link": "https://github.blog/2020-12-22-git-clone-a-data-driven-study-on-cloning-behaviors/", "abstract": "  @derrickstolee  recently  discussed several different  git clone  options , but how do those options actually affect your Git performance? Which option is fastest for your client experience? Which option is fastest for your build machines? How can these options impact server performance? If you are a GitHub Enterprise Server administrator it’s important that you understand how the server responds to these options under the load of multiple simultaneous requests. . Here at GitHub, we use a data-driven approach to answer these questions. We ran an experiment to compare these different clone options and measured the client and server behavior. It is not enough to just compare  git clone  times, because that is only the start of your interaction with a Git repository. In particular, we wanted to determine how these clone options change the behavior of future Git operations such as  git fetch . . In this experiment, we aimed to answer the below questions: . How fast are the various  git clone  commands? . Once we have cloned a repository, what kind of impact do future  git fetch  commands have on the server and client? . What impact do full, shallow and partial clones have on a Git server? This is mostly important for our GitHub Enterprise Server Admins. . Will the repository shape and size make any difference in the overall performance? . It is worth special emphasis that these results come from simulations that we performed in our controlled environments and do not simulate complex workflows that might be used by many Git users. Depending on your workflows and repository characteristics these results may change. Perhaps this experiment provides a framework that you could follow to measure how your workflows are affected by these options. If you would like help analyzing your worksflows, feel free to engage with  GitHub’s Professional Services team . . For a summary of our findings, feel free to jump to  our conclusions and recommendations . . To maximize the repeatability of our experiment, we use open source repositories for our sample data. This way, you can compare your repository shape to the tested repositories to see which is most applicable to your scenario. . We chose to use the   jquery/jquery  ,   apple/swift   and   torvalds/linux   repositories. These three repositories vary in size and number of commits, blobs, and trees. . These repositories were mirrored to a GitHub Enterprise Server running version 2.22 on a 8-core cloud machine. We use an internal load testing tool based on  Gatling  to generate  git  requests against the test instance. We ran each test with  a specific number of users  across 5 different load generators for 30 minutes. All of our load generators use  git version 2.28.0  which by default is using protocol version 1. We would like to make a note that protocol version 2 only improves ref advertisement and therefore we don’t expect it to make a difference in our tests. . Once a test is complete, we use a combination of Gatling results,   ghe-governor   and server health metrics to analyze the test. . The  git-sizer  tool measures the size of Git repositories along many dimensions. In particular, we care about the total size on disk along with the count of each object type. The table below contains this information for our three test repositories. . The  jquery/jquery  repository is a fairly small repository with only 40MB of disk space.  apple/swift  is a medium-sized repository with around 130 thousand commits and 750MB on disk. The  torvalds/linux  repository is typically the gold standard for Git performance tests on open source repositories. It uses 4 gigabytes of disk space and has close to a million commits. . We care about the following clone options: . Full clones. . Shallow clones ( --depth=1 ). . Treeless clones ( --filter=tree:0 ). . Blobless clones ( --filter=blob:none ). . In addition to these options at clone time, we can also choose to fetch in a shallow way using  --depth=1 . Since treeless and blobless clones have their own way to reduce the objects downloaded during  git fetch , we only test shallow fetches on full and shallow clones. . We organized our test scenarios into the following ten categories, labeled T1 through T10. T1 to T4, simulate four different  git clone  types. T5 to T10 simulate various  git fetch  operations into these cloned repositories. . In partial clones, the new blobs at the new ref tip are not downloaded until we navigate to that position and populate our working directory with those blob contents. To be a fair comparison with the full and shallow clone cases, we also have our simulation run  git reset --hard origin/$branch  in all T5 to T10 tests. In T5 to T8 this extra step will not have a huge impact, but in T9 and T10 it will ensure the blob downloads are included in the cost. . In all the scenarios above, a single user was also set to repeatedly change 3 random files in the repository and push them to the same branch that the other users were cloning and fetching. This simulates repository growth so the  git fetch  commands actually have new data to download. . Let’s dig into the numbers to see what our experiment says. . The full numbers are provided in the tables below. . Unsurprisingly, shallow clone is the fastest clone for the client, followed by a treeless then blobless partial clones, and finally full clones. This performance is directly proportional to the amount of data required to satisfy the clone request. Recall that full clones need all reachable objects, blobless clones need all reachable commits and trees, treeless clones need all reachable commits. A shallow clone is the only clone type that does not grow at all along with the history of your repository. . The performance impact of these clone types grows in proportion to the repository size, especially the number of commits. For example, a shallow clone of  torvalds/linux  is four times faster than a full clone, while a treeless clone is only twice as fast and a blobless clone is only 1.5 times as fast. It is worth noting that the development culture of the Linux project promotes very small blobs that compress extremely well. We expect that the performance difference to be greater for most other projects with a higher blob count or size. . As for server performance, we see that the Git CPU time per clone is higher for the blobless partial clone (T4). Looking a bit closer with  ghe-governor , we observe that the higher Git CPU is mainly due to the higher amount of   pack-objects   operations in the partial clone scenarios (T3 and T4). In the  torvalds/linux  repository, the Git CPU time spent on  pack-objects  is four times more in a treeless partial clone (T3) compared to a full clone (T1). In contrast, in the smaller  jquery/jquery  repository, a full clone consumes more CPU per clone compared to the partial clones (T3 and T4). Shallow clone of all the three different repositories, consumes the lowest amount of total and Git CPU per clone. . If the full clone is sending more data in a full clone, then why is it spending more CPU on a partial clone? When Git sends all reachable objects to the client, it mostly transfers the data it has on disk without decompressing or converting the data. However, partial and shallow clones need to extract that subset of data and repackage it to send to the client.  We are investigating ways to reduce this CPU cost in partial clones.  . The  real  fun starts after cloning the repository and users start developing and pushing code back up to the server. In the next section we analyze scenarios T5 through T10, which focus on the  git fetch  and  git reset --hard origin/$branch  commands. . The full fetch performance numbers are provided in the tables below, but let’s first summarize our findings. . The biggest finding is that shallow fetches are the worst possible options, in particular from full clones. The technical reason is that the existence of a “shallow boundary” disables  an important performance optimization on the server . This causes the server to walk commits and trees to find what’s reachable from the client’s perspective. This is more expensive in the full clone case because there are more commits and trees on the client that the server is checking to not duplicate. Also, as more shallow commits are accumulated, the client needs to send more data to the server to describe those shallow boundaries. . The two partial clone options have drastically different behavior in the  git fetch  and  git reset --hard origin/$branch  sequence. Fetching from blobless partial clones increases the reset command by a small, measurable way, but not enough to make a huge difference from the user perspective. In contrast, fetching from a treeless partial clone causes significant more time because the server needs to send all trees and blobs reachable from a commit’s root tree in order to satisfy the  git reset --hard origin/$branch  command. . Due to these extra costs as a repository grows, we strongly recommend against shallow fetches and fetching from treeless partial clones. The only recommended scenario for a treeless partial clone is for quickly cloning on a build machine that needs access to the commit history, but will delete the repository at the end of the build. . Blobless partial clones do increase the Git CPU costs on the server somewhat, but the network data transfer is much less than a full clone or a full fetch from a shallow clone. The extra CPU cost is likely to become less important if your repository has larger blobs than our test repositories. In addition, you have access to the full commit history, which might be valuable to real users interacting with these repositories. . It is also worth noting that we noticed a surprising result during our testing. During T9 and T10 tests for the Linux repository, our load generators encountered memory issues as it seems that these scenarios with the heavy load that we were running, triggered more auto Garbage Collections (GC). GC in the Linux repository is expensive and involves a full repack of all Git data. Since we were testing on a Linux client, the GC processes were launched in the background to avoid blocking our foreground commands. However, as we kept fetching we ended up with several concurrent background processes; this is not a realistic scenario but a factor of our synthetic load testing. We ran  git config gc.auto false  to prevent this from affecting our test results. . It is worth noting that blobless partial clones might trigger automatic garbage collection more often than a full clone. This is a natural byproduct of splitting the data into a larger number of small requests. We have work in progress to make Git’s repository maintenance be more flexible, especially for large repositories where a full repack is too time-consuming. Look forward to more updates about that feature here on the GitHub blog. . Our experiment demonstrated some performance changes between these different clone and fetch options. Your mileage may vary! Our experimental load was synthetic, and your repository shape can differ greatly from these repositories. . Here are some common themes we identified that could help you choose the right scenario for your own usage: . If you are a developer focused on a single repository, the best approach is to do a full clone and then always perform a full fetch into that clone. You might deviate to a blobless partial clone if that repository is very large due to many large blobs, as that clone will help you get started more quickly. The trade-off is that some commands such as  git checkout  or  git blame  will require downloading new blob data when necessary. . In general, calculating a shallow fetch is computationally more expensive compared to a full fetch. Always use a full fetch instead of a shallow fetch both in fully and shallow cloned repositories. . In workflows such as CI builds when there is a need to do a single clone and delete the repository immediately, shallow clones are a good option. Shallow clones are the fastest way to get a copy of the working directory at the tip commit. If you need the commit history for your build, then a treeless partial clone might work better for you than a full clone. Bear in mind that in larger repositories such as the  torvalds/linux  repository, it will save time on the client but it’s a bit heavier on your git server when compared to a full clone. . Blobless partial clones are particularly effective if you are using  Git’s sparse-checkout feature  to reduce the size of your working directory. The combination greatly reduces the number of blobs you need to do your work. Sparse-checkout does not reduce the required data transfer for shallow clones. . Notably, we did not test repositories that are significantly larger than the  torvalds/linux  repository. Such repositories are not really available in the open, but are becoming increasingly common for private repositories. If you feel your repository is not represented by our test repositories, then we recommend trying to replicate our experiments yourself. . As can be observed, our test process is not simulating a real life situation where users have different workflows and work on different branches. Also the set of Git commands that have been analyzed in this study is a small set and is not a representative of a user’s daily Git usage. We are continuing to study these options to get a holistic view of how they change the user experience. . A special thanks to  Derrick Stolee  and our  Professional Services  team for their efforts and sponsorship of this study! .  \t\t Tags:   \t\t Git \t ", "date": "December 22, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tBuilding On-Call Culture at GitHub\t\t", "author": ["\n\t\tMary Moore-Simmons\t"], "link": "https://github.blog/2021-01-06-building-on-call-culture-at-github/", "abstract": " As GitHub grows in size and our product offerings grow in number and complexity, we need to constantly evolve our on-call strategy so we can continue to be the trusted home for all developers. Expanding upon our  Building GitHub  blog series, this post gives you a window into one of the major steps along our continuous journey for operational excellence at GitHub. . Most of the GitHub products you interact with are in a large Ruby on Rails monolith. Monolithic codebases are common for many high-growth startups, and it’s a difficult situation to detangle yourself from. One of the pain points we had was problems with the on-call system for our monolith. . The biggest pain points with our monolithic on-call structure were: . To address these pain points, we made large changes to our on-call structure. We split up the monolith on-call rotation so every engineering team was on-call for the code they maintain. We had a number of logistical difficulties to overcome, but most of the hurdles we faced were cultural and educational. . The GitHub monolith contains over 16,000 files, and file ownership was often unclear or undocumented. To solve this, we rolled out a new system that associates files to services, and then assigns services to teams, which made it much easier to change ownership as teams changed. Here’s a snippet of the file to service mappings code we use in the monolith: . You’ll see that each file in the monolith maps to a service, such as “apps” or “authzd”. We then have another file in the monolith that lists every service and maps them to teams. Here’s a snippet of that code for the apps service: . This information is automatically pulled into a service we developed in-house, called the Service Catalog, which then displays the information to GitHub employees. You can search for services in the Service Catalog, which allows GitHub engineers, support, and product to track down which engineering team owns which services. Here’s an example of what it looks like for the apps service: .   . We asked all teams to move to this new service ownership model, implemented a linter that doesn’t allow files in the monolith to be updated or added unless the ownership information was completed, and worked with engineering and leadership to assign ownership for major services that were still unowned. . Monitoring and alerting was set up for the monolith as a whole, so we asked teams to create monitoring specific to their areas of responsibility. Once this was mostly completed, a group of more senior engineers researched all remaining monolith-wide alerts and then split up alerts, assigned alerts out to teams, and decommissioned alerts that were no longer necessary. . There were over 50 engineering teams that needed to make changes to their processes in order to complete this effort. To coordinate this, we opened a GitHub issue for every team with clear checklists for the work needed. From there we did regular check-ins with teams and provided help and information so we didn’t leave anyone behind. . GitHub’s incident resolution time improved after the bulk of this initiative was completed, but our journey is never done. Organizations need to constantly improve their operational best practices or they’ll fall behind. . There are several long-term cultural changes discussed above that we must continue to promote for years at GitHub. We are conducting a retrospective in January to learn how we can improve rolling out large changes like these in the future, and how we can continue to improve the on-call experience for our engineers and GitHub’s stability for our customers. In addition, we are sending out regular surveys to engineers about their on-call experience and we continue to monitor GitHub’s uptime. We will continue to meet with engineering teams to discuss their on-call pain points and how to improve so that we can encourage a growth mindset in our engineering teams and help each other learn operational best practices. Everyone at GitHub is in this journey together, and we need to support each other in our drive for excellence so we can continue to be the trusted home for all developers. ", "date": "January 6, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: December 2020\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2021-01-06-github-availability-report-december-2020/", "abstract": " In December, we experienced no incidents resulting in service downtime. This month’s GitHub Availability Report will provide a summary and follow-up details on how we addressed an incident mentioned in November’s report. . Upon further investigation around one of the incidents mentioned in  November’s Availability Report , we discovered an edge case that triggered a large number of GitHub App token requests. This caused abnormal levels of replication lag within one of our MySQL clusters, specifically affecting the GitHub Actions service. This particular scenario resulted in amplified queries and increased the database lag, which impacted the database nodes that process GitHub App token requests. . When a GitHub Action is invoked, the Action is passed a GitHub App token to perform tasks on GitHub. In this case, the database lag resulted in the failure of some of those token requests because the database replicas did not have up to date information. . To help avoid this class of failure, we are updating the queries to prevent large quantities of token requests from overloading the database servers in the future. . Whether we’re  introducing a system to manage flaky tests  or  improving our CI workflow , we’ve continued to invest in our engineering systems and overall reliability. To learn more about what we’re working on, visit  GitHub’s engineering blog . .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "January 6, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe best of Changelog • 2020 Edition\t\t", "author": ["\n\t\tMichelle Mannering\t"], "link": "https://github.blog/2021-01-21-changelog-2020-edition/", "abstract": " If you haven’t seen it, the  GitHub Changelog  helps you keep up-to-date with all the latest features and updates to GitHub. We shipped a tonne of changes last year, and it’s impossible to blog about every feature. In fact, we merged over 90,000 pull requests into the GitHub codebase in the past 12 months! . Here’s a quick recap of the top changes made to GitHub in 2020. We hope these changes are helping you build cooler things better and faster. Let us know what your favourite feature of the past year has been. . While we haven’t exactly been travelling a lot recently, one of the things we love is the flexibility to work wherever we want, however we want. Whether you want to work on your couch, in the terminal, or check your notifications on the go, we’ve shipped some updates for you. .  Do you like to work in the command line? In September, we brought  GitHub to your terminal . Having GitHub available in the command line reduces the need to switch between applications or various windows and helps simplify a bunch of automation scenarios.  . The  GitHub CLI  allows you to run your entire GitHub workflow directly from the terminal. You can clone a repo, create, view and review PRs, open issues, assign tasks, and so much more. The CLI is available on Windows, iOS, and Linux. Best of all, the GitHub CLI is open source.  Download the CLI today , check out the  repo , and view the  Docs for a full list of the CLI commands . . GitHub CLI 1.0 is here 🎉 . Take GitHub to the command line and interact with repositories, issues, pull requests, releases, and more. . ✓ Free and open source ✓ Available for macOS, Windows, Linux ✓ GitHub Enterprise Server supported . Download it now:  https://t.co/sWwgXttURj   pic.twitter.com/JDTcsumffm  . — GitHub (@github)  September 17, 2020  .    . It doesn’t stop there.  Now you can also have  GitHub in your pocket with  GitHub for Mobile ! . This new native app makes it easy to  create, view, and comment on issues , check your notifications, merge a pull request, explore, organise your tasks, and  more . One of the most used features of  GitHub for Mobile  is push notification support. Mobile alerts means you’ll never miss a mention or review again and can help keep your team unblocked. . GitHub for Mobile is available on  iOS  and  Android . Download it today if you’re not already carrying the world’s development platform in your pocket. . Oh and did you know, GitHub for Mobile isn’t just in English? It’s also  available in  Brazilian Portuguese, Japanese, Simplified Chinese, and Spanish. . Did you know you can have GitHub in your pocket? . Carry the world's development platform wherever you go with GitHub Mobile. Check your notifications over a cup of coffee ☕  or merge pull requests whilst lounging on the couch.  pic.twitter.com/yCooFYMZjW  . — GitHub (@github)  November 13, 2020  .    .   . With the release of  GitHub Enterprise Server 2.21 in 2020 , there was a host of amazing new features. There are new features for PRs, a new notification experience, and changes to issues. These are all designed to make it easier to connect, communicate, and collaborate within your organisation. . And now we’ve made Enterprise Server even better with  GitHub Enterprise Server 3.0 RC . That means GitHub Actions, Packages, Code Scanning, Mobile Support, and Secret Scanning are now available in your Enterprise Server. This is the biggest release we’ve done of GitHub Enterprise Server in years, and you can install it now with full support. .   .  GitHub Actions  was launched at the end of 2019 and is already the most popular CI/CD service on GitHub. Our team has continued adding features and improving ways for you to automate common tasks in your repository. GitHub Actions is so much more than simply CI/CD. Our community has really stepped up to help you automate all the things with over 6,500 open source Actions available in the  GitHub Marketplace . . Some of the enhancements to GitHub Actions in 2020 include: . We made it easy for you to see what’s happening with your Actions automation. With Workflow visualisation, you can now see a  visual graph of your workflow . .   . This workflow visualisation allows you to easily view and understand your workflows no matter how complex they are. You can also track the progress of your workflow in real time and easily monitor what’s happening so you can access deployment targets. . On top of workflow visualisation, you can also  create workflow templates . This makes it easier to promote  best practices  and consistency across your organisation. It also cuts down time when using the same or similar workflows. You can even define rules for these templates that work across your repo. . Right at the end of 2019, we announced GitHub Actions supports  self-hosted runner groups . It offered developers maximum flexibility and control over their workflows. Last year, we made updates to self-hosted runners,  making self-hosted runners shareable  across some or all of your GitHub organisations. . In addition, you can separate your runners into groups, and add custom labels to the runners in your groups. Read more about these  Enterprise self-hosted runners  and  groups  over on our GitHub Docs. . Last year we added  environment protection rules and environment secrets  across our CD capabilities in GitHub Actions. This  new update  ensures there is separation between the concerns of deployment and concerns surrounding development to meet compliance and security requirements. . With Environments, we also added the ability to  pause a job  that’s trying to deploy to the protected environment and request manual approval before that job continues. This unleashes a whole new raft of continuous deployment workflows, and we are very excited to see how you make use of these new features. . New to Actions: the ability to require manual approvals for Action workflows, making it a full-featured continuous delivery engine. Control the full developer pipeline from commit to deploy.  #GitHubUniverse   #Keynote   https://t.co/9gQRFt3aqQ   pic.twitter.com/rgUnKyXhqY  . — GitHub (@github)  December 8, 2020  .    . Yes there’s all the big updates, and we’re committed to making small improvements too. Alongside other changes, we now have  better support for whatever default branch name you choose . We updated all our starter workflows to use a new  $default-branch macro . . We also added the ability to  re-run all jobs  after a successful run, as well as change the  retention days for artifacts and logs . Speaking of logs, we  updated how the logs are displayed . They are now much easier to read, have better searching, auto-scrolling, clickable URLs, support for more colours, and full screen mode. You can now  disable  or  delete workflow runs in the Actions tab  as well as manually trigger Actions runs with the  workflow_dispatch trigger . . While having access to all 6,500+ actions in the marketplace helps integrate with different tools, some enterprises want to limit which actions you can invoke to a limited trusted sub-set. You can now  fine-tune access to your external actions  by limiting control to GitHub-verified authors, and even limit access to specific Actions. . There were so many amazing changes and updates to GitHub Actions that we couldn’t possibly include them all here. Check out the  Changelog for all our GitHub Actions updates . . Keeping your code safe and secure is one of the most important things for us at GitHub. That’s why we made a number of improvements to  GitHub Advanced Security  for 2020. . You can read all about these improvements in the special  Security Highlights from 2020 . There are new features such as  code scanning ,  secret scanning ,  Dependabot updates ,  Dependency review , and  NPM advisory information . . If you missed the talk at GitHub Universe on the state of security in the software industry, don’t forget to check it out. Justin Hutchings, the Staff Product Manager for Security, walks through the latest trends in security and all things DevSecOps. It’s definitely worth carving out some time over the weekend to watch this: .      . GitHub is about building code together. That’s why we’re always making improvements to the way you work with your team and your community. . Issues are important for keeping track of your project, so we have been busy making issues work better and faster on GitHub. . You can now also link  issues and PRs  via the sidebar, and issues now have  list autocompletion . When you’re looking for an issue to reference, you can use  multiple words to search for that issue inline . .   . Sometimes when creating an issue, you might like to add a GIF or short video to demo a bug or new feature.  Now you can do it natively  by adding an *.mp4 or *.mov into your issue. .   . Issues are a great place to talk about feature updates and bug fixes, but what about when you want to have an open- ended conversation or have your community help answering common questions? .  GitHub Discussions  is a place for you and your community to come together and collaborate, chat, or discuss something in a separate space, away from your issues. Discussions allows you to have threaded conversations. You can even convert Issues to Discussions, mark questions as answered, categorise your topics, and pin your Discussions. These features help you provide a welcoming space to new people as well as quick access to the most common discussion points. . Available now, every open source community can try GitHub Discussions in the new public beta. Turn it on in your repo settings and you're ready to go!  #GitHubUniverse   #Keynote   https://t.co/9gQRFt3aqQ   pic.twitter.com/cWKCoSgmZ8  . — GitHub (@github)  December 8, 2020  .    . If you are an admin or maintainer of a public repo you can enable Discussions via repo settings today.  Check out our Docs  for more info. . Speaking of Docs, did you know we recently published all our documentation as an open source project? Check it out and  get involved today . .   . We  launched GitHub Sponsors in 2019 , and people have been loving this program. It’s a great way to contribute to open source projects. In 2020, we made  GitHub Sponsors available  in even more countries. Last year, GitHub Sponsors became available in  Mexico ,  Czech Republic ,  Malta , and  Cyprus . .   . We also added some other fancy features to GitHub Sponsors. This includes the ability to  export a list of your sponsors . You can also set up webhooks for events in your sponsored account and easily keep track of everything that’s happening via your activity feed. . At  GitHub Universe, we also announced   Sponsors for Companies . This means organisations can now invest in open source projects via their billing arrangement with GitHub. Now is a great time to consider supporting your company’s most critical open source dependencies. . We’re always finding ways to help developers. As Nat said in his  GitHub Universe keynote , the thing we care about the most is helping developers build amazing things. That’s why we’re always trying to make it quicker and easier to collaborate on code. . Draft pull requests are a great way to let your team know you are working on a feature. It helps start the conversation about how it should be built without worrying about someone thinking it’s ready to merge into main. We recently made it easy to  convert an existing PR into a draft anytime . .   . Not only can you do  multi-line comments , you can now suggest a  specific change to multiple lines of code  when you’re reviewing a pull request. Simply click and drag and then edit text within the suggestion block. .   . Alongside the entire Git community, we’ve been trying to make it easier for teams wanting to use more inclusive naming for their  default branch . This also gives teams much more flexibility around branch naming. We’ve added first-tier support for renaming branches in the GitHub UI. . This helps take care of retargeting pull requests and updating branch protection rules. Furthermore, it provides instructions to people who have forked or cloned your repo to make it easier for them to update to your new branch names. . We provided  re-directs so links to deleted branch names  now point to the new default branch. In addition, we updated GitHub Pages to allow it to publish from any branch. We also added a preference so you can set the default branch name for your organization. If you need to stay with ‘master’ for compatibility with your existing tooling and automation, or if you prefer to use a different default branch, such as ‘development,’ you can now set this in a single place. . For new organizations to GitHub, we also updated the  default to ‘main ’ to reflect the new consensus among the Git community. Existing repos are also not affected by any of these changes. Hopefully we’ve helped make it easier for the people who do want to move away from the old ‘master’ terminology in Git. . In mid 2020, we launched a fresh new look to the  GitHub UI . The way repos are shown on the homepage and the overall look and feel of GitHub is super sleek. There’s responsive layout, improved UX in the mobile web experience, and more. We also made lots of small improvements. For example, the way your commits are shown in the pull request timeline  has changed . PRs in the past were ordered by author date. Now they’ll show up according to their chronological order in the head branch. .   . If you’ve been following a lot of our socials, you’ll know we’ve also got a brand new look and feel to  GitHub.com . Check out these changes, and we hope it gives you fresh vibes for the future. . Speaking of fresh vibes, you’ve asked for it, and  now it’s here ! No longer will you be blinded by the light. Now you can go to the  dark side  with dark mode for the web. . echo \"hello, darkness\"  #GitHubUniverse   #Keynote   https://t.co/9gQRFt3aqQ   pic.twitter.com/HEotvXVJ7R  . — GitHub (@github)  December 8, 2020  .    . These are just some of the highlights for 2020. We’re all looking forward to bringing you more great updates in 2021. . Keep an eye on the  Changelog  to stay informed and ensure you don’t miss out on any cool updates. You can also follow our changes with  @GHChangelog on Twitter  and see what’s coming soon by checking out the  GitHub Roadmap .  Tweet us  your favourite changes for 2020, and tell us what you’re most excited to see in 2021. .  \t\t Tags:   \t\t CI/CD   GitHub Actions   GitHub for mobile \t ", "date": "January 21, 2021"},
{"website": "Github-Engineering", "title": "\n\t\t\tTesting cloud apps with GitHub Actions and cloud-native open source tools\t\t", "author": ["\n\t\tSarah Khalife\t"], "link": "https://github.blog/2020-10-09-devops-cloud-testing/", "abstract": "  See this post in action during  GitHub Demo Days  on October 16 . . What makes a project successful? For developers building cloud-native applications, successful projects thrive on transparent, consistent, and rigorous collaboration. That collaboration is one of the reasons that many  open source projects , like Docker containers and Kubernetes, grow to become standards for how we build, deliver, and operate software. Our  Open Source Guides  and  Introduction to innersourcing  are great first steps to setting up and encouraging these best practices in your own projects. . However, a common challenge that application developers face is manually testing against inconsistent environments. Accurately testing Kubernetes applications can differ from one developer’s environment to another, and implementing a rigorous and consistent environment for end-to-end testing isn’t easy. It can also be very time consuming to spin up and down Kubernetes clusters. The inconsistencies between environments and the time required to spin up new Kubernetes clusters can negatively impact the speed and quality of cloud-native applications. . On GitHub, integration and testing becomes a little easier by combining  GitHub Actions  with open source tools. You can treat Actions as the native continuous integration and continuous delivery (CI/CD) tool for your project, and customize your Actions workflow to include automation and validation as next steps. . Since Actions can be triggered based on nearly any  GitHub event , it’s also possible to build in accountability for updating tests and fixing bugs. For example, when a developer creates a pull request, Actions status checks can automatically block the merge if the test fails. . Here are a few more examples: .   .  Branch protection rules  in the repository help enforce certain workflows, such as requiring more than one pull request review or requiring certain status checks to pass before allowing a pull request to merge. .   . GitHub Actions are natively configured to act as  status checks  when they’re set up to trigger  `on: [pull_request]` . .  Continuous integration  (CI) is extremely valuable as it allows you to run tests before each pull request is merged into production code. In turn, this will reduce the number of bugs that are pushed into production and increases confidence that newly introduced changes will not break existing functionality. . But transparency remains key:  Requiring CI status checks  on  protected branches  provides a clearly-defined, transparent way to let code reviewers know if the commits meet the conditions set for the repository—right in the pull request view. . Now that we’ve thought through the simple CI policies, automated workflows are next. Think of an Actions workflow as a set of “plug and play” open sourced, automated steps contributed by the community. You can use them as they are, or customize and make them your own. Once you’ve found the right one, open sourced Actions can be plugged into your workflow with the `- uses: repo/action-name`  field. .   . You might ask, “So how do I find available Actions that suit my needs?” . The  GitHub Marketplace ! .   . As you’re building automation and CI pipelines, take advantage of Marketplace to find pre-built Actions provided by the community. Examples of pre-built Actions span from a  Docker publish  and the  kubectl CLI installation  to  container scans  and cloud deployments. When it comes to cloud-native Actions, the list keeps growing as container-based development continues to expand. . Testing is a critical part of any CI/CD pipeline, but running tests in Kubernetes can absorb the extra time that automation saves. Enter  kind . kind stands for “Kubernetes in Docker.” It’s an open source project from the Kubernetes special interest group (SIGs) community, and a tool for running local Kubernetes clusters using Docker container “nodes.” Creating a kind cluster is a simple way to run Kubernetes cluster and application testing—without having to spin up a complete Kubernetes environment. .   . As the number of Kubernetes users pushing critical applications to production grows, so does the need for a repeatable, reliable, and rigorous testing process. This can be accomplished by combining the creation of a homogenous Kubernetes testing environment with kind, the community-powered Marketplace, and the native and transparent Actions CI process. . {    “@context”: “http://schema.org/”,    “@type”: “VideoObject”,    “thumbnailurl”: “https://github.blog/wp-content/uploads/2020/10/DevOps-Header.png?w=2415”,    “name”: “GitHub Demo Days – Using GitHub Actions for testing cloud native applications”,   “description”: “A common challenge that cloud native application developers face is manually testing against inconsistent environments. GitHub Actions can be triggered based on nearly any GitHub event making it possible to build in accountability for updating tests and fixing bugs.”,     “uploadDate”: “2020-10-08T08:00:00+00:00”,    “embedUrl”: “https://www.youtube.com/embed/-sprH1_jWAY”,    “publication”: {      “@type”: “BroadcastEvent”,      “isLiveBroadcast”: true,      “startDate”: “2020-10-16T3:00:00+00:00”,      “endDate”: “2020-10-16T4:00:00+00:00”    }  } . Come see kind and Actions at work during our next  GitHub Demo Day live stream  on October 16, 2020 at 11am PT. I’ll walk you through how to easily set up automated and consistent tests per pull request, including how to use kind with Actions to automatically run end-to-end tests across a common Kubernetes environment. .  \t\t Tags:   \t\t CI/CD   DevOps   GitHub Actions \t ", "date": "October 9, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow to get your organization started with containerized deployments\t\t", "author": ["\n\t\tSarah Khalife\t"], "link": "https://github.blog/2020-10-15-how-to-get-your-organization-started-with-containerized-deployments/", "abstract": "  This is our second post on cloud deployment with containers. Looking for more? Join our  upcoming GitHub Actions webcast  with Sarah, Solutions Engineer Pavan Ravipati, and Senior Product Manager Kayla Ngan on October 22.  . In the past few years, businesses have moved towards cloud-native operating models to help streamline operations and move away from costly infrastructure. When running applications in dynamic environments with Docker, Kubernetes, and other tooling, a container becomes the tool of choice as a consistent, atomic unit of packaging, deployment, and application management. This sounds straightforward: build a new application, package it into containers, and scale elastically across the infrastructure of your choice. Then you can automatically update with new images as needed and focus more on solving problems for your end users and customers. . However, organizations don’t work in vacuums. They’re part of a larger ecosystem of customers, partners, and open source communities, with unique cultures, existing processes, applications, and tooling investments in place. This adds new challenges and complexity for adopting cloud native tools such as containers, Kubernetes, and other container schedulers. . At GitHub, we’re fortunate to work with many customers on their container and DevOps strategy. When it comes to adopting containers, there are a few consistent challenges we see across organizations. . Despite the few challenges of adopting containers and leveraging Kubernetes, more and more organizations continue to use them. Stepping over those hurdles allows enterprises to automate and streamline their operations, here with a few examples of how enterprises make it work successfully with support from package managers and CI/CD tools. At GitHub, we’ve introduced container support in  GitHub Packages , CI/CD through  GitHub Actions , and  partnered within the ecosystem  to simplify cloud-native workflows. Finding the right container tools should mean less work, not more—easily integrating alongside other tools, projects, and processes your organization already uses. .   . Want to simplify container deployments in your organization? Join me, Solutions Engineer Pavan Ravipati, and Senior Product Manager Kayla Ngan on October 22 to learn more about successfully adopting containers. We’ll walk through how to use them in the real world and demo best practices for deploying an application to Azure with GitHub Container Registry. .  When   October 22, 2020  11:00 am PT / 2:00 pm ET .  Watch the on-demand version of the webcast that previously aired. . .  \t\t Tags:   \t\t DevOps   GitHub Actions \t ", "date": "October 15, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tHighlights from Git 2.29\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2020-10-19-git-2-29-released/", "abstract": " The open source Git project  just released Git 2.29  with features and bug fixes from over 89 contributors, 24 of them new. Last time  we caught up  with you, Git 2.28 had just been released. One version later, let’s take a look at the most interesting features and changes that have happened since then. . Git 2.29 includes experimental support for writing your repository’s objects using a SHA-256 hash of their contents, instead of using SHA-1. . What does all of that mean? To explain, let’s start from the beginning. . When you add files to a repository, Git copies their contents into  blob  objects in its local database, and creates  tree  objects that refer to the blobs. Likewise, when you run  git commit , this creates a  commit  object that refers to the tree representing the committed state. How do these objects “refer” to each other, and how can you identify them when interacting with Git? The answer is that each object is given a unique name, called its  object id , based on a hash of its contents. Git uses SHA-1 as its hash algorithm of choice, and depends on the object ids of different objects to be unique. . Back in  this blog post , we estimated that even if you had five million programmers writing one commit every second, you would only have a 50% chance of accidentally generating a collision before the Sun engulfs the Earth.  Some   published   attacks  exist which use tricks that exploit weaknesses in SHA-1 that can reduce the effort required to generate a collision, but these attacks still cost tens of thousands of dollars to execute, and no known examples have been published which target Git. . Like we stated back in that earlier blog post, Git (and providers that use it, like GitHub) checks each object it hashes to see if there is evidence that that object is part of a colliding pair. This prevents GitHub from accepting both the benign and malicious halves of the pair, since the mathematical tricks required to generate a collision in any reasonable amount of time can be detected and rejected by Git. . Even so, any weaknesses in a cryptographic hash are a bad sign. Even though Git has implemented detections that prevent the known attacks from being carried out, there’s no guarantee that new attacks won’t be found and used in the future. So the Git project has been preparing a transition plan to begin using a new object format with no known attacks: SHA-256. . In Git 2.29, you can try out a SHA-256 enabled repository for yourself: . As of version 2.29, Git can operate in either a full SHA-1 or full SHA-256 mode. It is currently not possible for repositories using different object formats to interoperate with one another, but eventual support is planned. It is also important to note that there are no major providers (including GitHub) which support hosting SHA-256-enabled repositories at the time of writing. . In future releases, Git will support interoperating between repositories with different object formats by computing both a SHA-1 and SHA-256 hash of each object it writes, and storing a translation table between them. This will eventually allow repositories that store their objects using SHA-256 to interact with (sufficiently up-to-date) SHA-1 clients, and vice-versa. It will also allow converted SHA-256 repositories to have their references to older SHA-1 commits still function as normal (e.g., if I write a commit whose message references an earlier commit by its SHA-1 name, then Git will still be able to follow that reference even after the repository is converted to use SHA-256 by consulting the translation table). . For more about SHA-256 in Git, and what some of the future releases might look like, you can read  Git’s transition plan . . [ source ,  source ,  source ,  source , and  so ,  much ,  more ] . When you run  git fetch origin , all of the branches from the remote  origin  repository are fetched into your local  refs/remotes/origin/  hierarchy. How does Git know which branches to fetch, and where to put them? . The answer is that your configuration file contains one or more “refspecs” for each remote (remember that a “ref” is Git’s word for any named point in history: branches, tags, etc). When you run  git clone , it sets up a default refspec to be used when you fetch from your origin repository: . This refspec tells Git to fetch what’s on the left side of the colon (everything in  refs/heads/ ; i.e., all branches) and to write them into the hierarchy on the right-hand side. The  *  means “match everything” on the left-hand side and “replace with the matched part” on the right-hand side. . You can have multiple refspecs, and they can refer to individual refs. For example, this command instructs Git to additionally fetch any  git notes  from the remote (the  --add  is important so that we don’t overwrite the default refspec that fetches branches): . Refspecs are used by  git push , as well. Even if you type only  git push origin mybranch , that last  mybranch  is really a shorthand for  refs/heads/mybranch:refs/heads/mybranch . This allows you to express more complicated scenarios. Say you’re tagging and want to push all of the tags you have, but you’re not quite ready to share the tips of all of your branches. Here, you could write something like: . Prior to Git 2.29, refspecs could only be used to say which selection of reference(s) you want. So, if you wanted to fetch all branches except one, you’d have to list them out as arguments one by one. Of course, that assumes that you know the names of all the other references beforehand, so in practice this would look something like: . to get all refs in  refs/heads/*  except for  refs/heads/ref-to-exclude . Yeesh; there must be a better way. . In Git 2.29, there is: negative refspecs. Now, if a refspec begins with  ^  it indicates which references are to be excluded. So, instead of the above, you could write instead something like: . and achieve the same result. When a negative refspec is present, the server considers a reference worth sending if it matches at least one positive refspec and does not match any negative refspecs. Negative refspecs behave exactly as you expect, with a couple of caveats: . And of course those negative refspecs work equally well in configuration values. If you always want to fetch every branch except  foo , you can just add it to your config: . [ source ] . While you have almost certainly used (or heard of)  git log , the same might not be necessarily true of  git shortlog . For those who haven’t,  git shortlog  acts a lot like  git log , except instead of displaying commits in a sequence, it groups them by the author. . In fact, the Git release notes end with a shortlog of all of the patches in the release, broken out by their author, generated by  git shortlog  [ source ]. At the time of writing, they look something like this: . In older versions of Git,  git shortlog  could only group by commit author (the default behavior), and optionally by the committer identity (with  git shortlog -c ). This restricts who gets the credit for a commit by who that commit’s author/committer is. So, if your project uses  the ‘Co-authored-by’ trailer  (like  this  commit in  git/git  does), then your co-authors are out of luck: there is no way to tell  git shortlog  to group commits by co-authors. . …That is, until Git 2.29! In this release,  git shortlog  learned a new  --group  argument, to specify how commits are grouped and assigned credit. It takes  --group=author  (the default behavior from before) and  --group=committer  (equivalent to  git shortlog -c ), but it also accepts a  --group=trailer:&lt;field&gt;  argument. . Passing the latter allows us to group commits by their co-authors, and it also allows for more creative uses. If your project is using the  Reviewed-by  trailer, you can use  git shortlog  to see who is reviewing the most patches: .  git shortlog  also allows multiple  --group=&lt;type&gt;  arguments, in which case commits are counted once per each grouping. So, if you want to see who is  contributing  the most whether that individual is the primary author, or is listed as a co-author, then you can write: . …putting authors and co-authors on equal footing. Instead of counting, you can also use the  --format  option to find other fun ways to show the data. For example: . [ source ] .  git for-each-ref  learned a few new tricks in Git 2.29. Since there are a good handful of them, let’s start there: . Now with all of the  git for-each-ref  updates out of the way, let’s move on to all of the rest of the tidbits: . Can you tell what this message means? In versions of Git prior to 2.29, it was ambiguous: did Git remove stuff and rename files in  HEAD , or is “Removed unnecessary stuff” the name of a commit message? It turns out that it’s the latter, but you had no way of knowing that! . In 2.29, Git will now enclose the subject of a commit message in parenthesis, making much clearer what part of the conflict message came from a commit, and what part was generated by Git. . [ source ] . That’s just a sample of changes from the latest release. For more, check out  the release notes for 2.29 , or  any previous version  in  the Git repository . .  \t\t Tags:   \t\t Git \t ", "date": "October 19, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tBuilding GitHub: introduction\t\t", "author": ["\n\t\tKate Studwell\t"], "link": "https://github.blog/2020-10-29-building-github-introduction/", "abstract": " Here at GitHub, we pride ourselves on providing a first-class developer experience to you, our customers. We’re developers, too, and we love that the features that we build for GitHub.com make your day easier — and make ours easier, too. We also know that the more we invest in the infrastructure and tooling that powers GitHub, the faster we can deliver those features, and we’ll have a more delightful experience to boot. . In addition to investing in the infrastructure, we also want to shine a light on all the hard work we do behind the scenes to make GitHub better, specifically focusing on our internal development tooling and infrastructure. And, today, we’re excited to introduce the Building GitHub blog series, providing deep-dives on how teams across the engineering organization have been banding together to identify and address opportunities that would provide us an even smoother internal development experience, up our technical excellence, and improve system reliability in the process. From running the latest and greatest Ruby version, to dramatically decreasing our application boot time, to smoother and more reliable progressive deploys, these efforts paid off greatly and decreased our cycle times. . To help frame our efforts for potential investments, we revisited the Four Key Metrics of high performing software delivery, as our very own Dr. Nicole Forsgren found in her research and outlined by  DevOps Research and Assessment . These include: . Ideally, any investment we make in our development tooling would move the needle in at least one of these areas. We’ve had teams across the organization join together to tackle these, sometimes diving into areas of our internal systems that they weren’t previously familiar with. This approach provides the opportunity to explore new solutions, and collaborate cross-team and cross-discipline. The excitement of engineers involved in each of these efforts is palpable — not only are we thrilled when we notice a dramatic shift in boot time or introduce new tooling that makes monitoring and debugging even easier, but teams enjoy working more closely with engineers in other parts of the org. . Continue reading along with us in the Building GitHub blog series, where we’ll share specific goals and lessons, the impact of our work, and how we did it. To continue this journey, we’ll start the series with a  deep dive on faster CI  and we hope to share more soon. ", "date": "October 29, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tMaking GitHub CI workflow 3x faster\t\t", "author": ["\n\t\tKeerthana Kumar\t"], "link": "https://github.blog/2020-10-29-making-github-ci-workflow-3x-faster/", "abstract": " Welcome to the first deep dive of the   Building GitHub  blog series , providing a look at how teams across the GitHub engineering organization identify and address opportunities to improve our internal development tooling and infrastructure. . At GitHub, we use the  Four Key Metrics  of high performing software development to help frame our engineering fundamentals effort. As we measured Lead Time for Changes—the time it takes for code to be successfully running in production—we identified that developers waited an average of 45 minutes for a successful run of our continuous integration suite to complete before merging any change. This 45-minute lead time was repeated once more before deploying a merge branch. In a perfect scenario, a developer waited almost two hours after checking in code before the change went live on GitHub.com. This 45-minute CI now takes only 15 minutes to run! Here is a deep dive on how we made GitHub’s CI workflow 3x faster. . At this moment the monumental Ruby monolith that powers millions of developers on GitHub.com, has over 7,000 test suites and over 5,000 test files. Every commit to a pull request triggers 25 CI jobs and requires 15 of those CI jobs to complete before merging a pull request. This meant that a developer at GitHub spent approximately 45 minutes and 600 cores of computing resources for every commit. That’s a lot of developer-hours and machine-hours that could be spent creating value for our customers. . Analyzing the types of CI jobs, we identified four categories: unit testing, linting/performance, integration testing, builds/deployments. All jobs except two of the integration testing jobs took less than 13 minutes to run. The two integration testing jobs were the bottleneck in our Lead Time for Changes. As it is true for most DevOps cycles, several test suites were also flaky. Although this blog post isn’t going to share how we solved for the flakiness of our tests, spoiler alert, a future post in this series will explain that process. Apart from being flaky, the two integration testing jobs increased developer friction and reduced productivity at GitHub. . GitHub Enterprise Server, the on-premise offering of GitHub used by our enterprise customers, ships a new patch release every two weeks and a major release every quarter. The two long running test suites were added to the CI workflow to ensure a pull request did not break the GitHub experience for our Enterprise Server customers. It was also clear that these 45-minute test suites did not provide additional value blocking GitHub.com deployments that happen continuously throughout the day. Driven by customer obsession and developer satisfaction, we developed the deferred compliance tool. . The deferred compliance tool integrated along with our CI workflow system aims to strike a critical balance between improving Lead Time for Change in deploying GitHub.com and creating accountability for the quality of Enterprise Server. The long running CI jobs are no longer required to pass before a pull request is merged but the deferred compliance tool is monitoring for any test failure. .   . If a CI job fails, a GitHub issue with a  deferred compliance  label is created and the pull request author and code segment’s code owners are tagged. A warning message is sent on Slack to the developer and a 72-hour timer is kicked off. The developer now has 72 hours to fix the build, push a change or revert the pull request. A successful run of the CI job automatically closes the compliance issue and the 72-hour timer is turned off. If the CI job remains broken for more than 72 hours, all deployments to GitHub.com are halted, barring any exceptional situations, until the integration tests for Enterprise Server are fixed. This creates accountability and ownership for all our developers to build features that work flawlessly on GitHub.com and Enterprise Server. The 72-hour timer is customizable but our analysis showed that with a global team of developers, 72 hours reduced the possibility that a change merged by a developer in San Francisco on a Friday afternoon did not unintentionally block deployments for a developer in Sydney on Monday morning. Deferred compliance can be used for any long running CI run that does not need to block deployments while creating a call for action for CI run failures. . Overall, this project is a testimony that a simple solution can significantly improve developer productivity and that can have long-term positive implications to an engineering organization. And of course, since numbers matter, we made our CI 3x faster. ", "date": "October 29, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGetting started with DevOps automation\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-10-29-getting-started-with-devops-automation/", "abstract": "  This is the second post in our series on DevOps fundamentals. For a guide to what DevOps is and answers to common DevOps myths    check out part one   .   . First things first—automation is one of the key principles for accelerating with DevOps. As noted  in my last blog post , it enables consistency, reliability, and efficiency within the organization, making it easier for teams to discover and troubleshoot problems. . However, as we’ve worked with organizations, we’ve found not everyone knows where to get started, or which processes can and should be automated. In this post, we’ll discuss a few best practices and insights to get teams moving in the right direction. . The path to DevOps automation is continually evolving. Before we dive into best practices, there are a few common guidelines to keep in mind as you’re deciding what and how you automate. . Now that our guidelines are in place, we can evaluate which sets of processes we need to automate. We’ve broken some best practices for DevOps automation into four categories to help you get started. . We often think of the term “DevOps” as being synonymous with “CI/CD”. At GitHub we recognize that DevOps includes so much more, from enabling contributors to build and run code (or deploy configurations) to improving developer productivity. In turn, this shortens the time it takes to build and deliver applications, helping teams add value and learn faster. While CI/CD and DevOps aren’t precisely the same, CI/CD is still a core component of DevOps automation. . Together, continuous integration and continuous delivery (commonly referred to as CI/CD) create a collaborative process for people to work on projects through shared ownership. At the same time, teams can maintain quality control through automation and bring new features to users with continuous deployment. . Change management is often a critical part of business processes. Like the automation guidelines, there are some common principles and tooling that  development and operations  teams can use to create consistency. . By now, you also may have heard of “infrastructure as code,” “configuration as code,” “policy as code,” or some of the other “as code” models. These models provide a declarative framework for managing different aspects of your operating environments through high level abstractions. Stated another way, you provide variables to a tool and the output is consistently the same, allowing you to recreate your resources consistently.  DevOps  implements the “as code” principle with several goals, including: an auditable change trail for compliance, collaborative change process via version control, a consistent, testable and reliable way of deploying resources, and as a way to lower the learning curve for new team members. . Operational insights are an invaluable component of any production environment. In order to understand the behaviors of your software in production, you need to have information about how it operates. Continuous monitoring—the processes and technology that monitor performance and stability of applications and infrastructure throughout the software lifecycle—provides operations teams with data to help troubleshoot, and development teams the information needed to debug and patch. This also leads into an important aspect of security, where  DevSecOps  takes on these principles with a security focus. Choosing the right monitoring tools can be the difference between a slight service interruption and a major outage. When it comes to gaining operational insights, there are some important considerations: . At this point, we’ve talked much about automation in the DevOps space, so is DevOps all about automation? Put simply, no. Automation is an important means to accomplishing this work efficiently between teams. Whether you’re new to DevOps or migrating from another set of automation solutions, testing new tooling with a small project or process is a great place to start. It will lay the foundation for scaling and standardizing automation across your entire organization, including how to measure effectiveness and progression toward your goals. . Regardless of which toolset you choose to automate your DevOps workflow, evaluating your teams’ current workflows and the information you need to do your work will help guide you to your tool and platform selection, and set the stage for success. Here are a few more resources to help you along the way: .  Want to see what DevOps automation looks like in practice?  See how engineers at Wiley build faster and more securely with GitHub Actions .   .   {“@context”:”https://schema.org”,”@type”:”FAQPage”,”mainEntity”:[{“@type”:”Question”,”name”:”Is DevOps all about automation?”,”acceptedAnswer”:{“@type”:”Answer”,”text”:”Answered succinctly, no. But automation is an important means to accomplishing the work. Whether you’re new to DevOps or migrating from another set of automation solutions, adopting new tooling with a small with a trial project or process will give you the base set of learnings to help scale and standardize automation across your entire organization. With this automation in place you will lay the foundation for measuring effectiveness and further progress toward your goals.”}}]}   .  \t\t Tags:   \t\t Automation   best practices   DevOps \t ", "date": "October 29, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tNbdev: A literate programming environment that democratizes software engineering best practices\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2020-11-20-nbdev-a-literate-programming-environment-that-democratizes-software-engineering-best-practices/", "abstract": " At GitHub, we are deeply invested in democratizing software development. Part of this is accomplished by serving as the home of open source and providing  tools to educators  and  students . We are also building features that lower the barrier to entry for software development, such as  Codespaces . However, there is much work left to be done in order to make software development more approachable and to make it easier to employ best practices, such as  continuous integration ,  distribution , and documentation of software. . This is why we decided to assist  fastai  in their development of a new,  literate programming  environment for Python, called  nbdev . A discussion of the motivations behind nbdev as well as a primer on the history of literate programming can be found in this  blog post . For the uninitiated, literate programming, as described by Donald Knuth, it is: . …a move away from writing computer programs in the manner and order imposed by the computer, and instead enables programmers to develop programs in the order demanded by the logic and flow of their thoughts. . While a subset of ideas from literate programming have shown up in tools, such as  Swift Playgrounds ,  Jupyter , and  Mathematica , there has been a lack of tools that encompass the entire software development life cycle.  nbdev  builds on top of Jupyter notebooks to fill these gaps and provides the following features, many of which are integrated with GitHub: .  As a teaser, this is a preview of this literate programming environment in Codespaces, which includes a notebook, a docs site and an IDE:  .   . In addition to this GitHub integration, nbdev also offers the following features: .  nbdev  promotes software engineering best practices by allowing developers to write unit tests and documentation in the same context as source code, without having to learn special APIs or worry about web development. Similarly, GitHub Actions run unit tests automatically by default without requiring any prior experience with these tools. We believe removing friction from writing documentation and tests promotes higher quality software and makes software more inclusive. . Aside from using nbdev to create Python software, you can extend nbdev to build new types of tools. For example, we recently used nbdev to build  fastpages , an easy to use blogging platform that allows developers to create blog posts directly with Jupyter notebooks.  fastpages  uses GitHub Actions and GitHub Pages to automate the conversion of notebooks to blog posts and offers a variety of other features to Python developers that democratize the sharing of knowledge. We have also used nbdev and fastpages to create  covid19-dashboards , which demonstrates how to create interactive dashboards that automatically update with Jupyter notebooks. . We are excited about the potential of nbdev to make software engineering more inclusive, friendly, and robust.  We are also hopeful that tools like nbdev can inspire the next generation of literate programming tools. To learn more about nbdev, please see the following resources: . Finally, If you are building any projects with nbdev or would like to have further discussions, please feel free to reach out on  the nbdev forums  or on  GitHub . ", "date": "November 20, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: November 2020\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-12-02-availability-report-november-2020/", "abstract": " In November, we experienced two incidents resulting in significant impact and degraded state of availability for issues, pull requests, and GitHub Actions services. . The SSL certificate for *.githubassets.com expired, impacting web requests for GitHub.com UI and services. There was an auto-generated issue indicating the certificate was within 30 days of expiration, but it was not addressed in time. Impact was reported, and the on-call engineer remediated it promptly. . We are using this occurrence to evaluate our current processes, as well as our tooling and automation, within this area to reduce the likelihood of such instances in the future. . Our service monitors detected abnormal levels of replication lag within one of our MySQL clusters affecting the GitHub Actions service. . Due to the recency of this incident, we are still investigating the contributing factors and will provide a more detailed update in next month’s report. . We place great importance in the reliability of our services along with the trust that our users place in us every day. We’ll continue to keep you updated on the progress we’re making to ensure this. To learn more about what we’re working on, visit the  GitHub engineering blog . .  \t\t Tags:   \t\t GitHub Actions   GitHub Availability Report \t ", "date": "December 2, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tImproving the GHES release process: release candidates\t\t", "author": ["\n\t\tMaya Ross\t"], "link": "https://github.blog/2020-12-03-improving-the-ghes-release-process-release-candidates/", "abstract": " In our ongoing “ Building GitHub ” series, we talk about some of the projects we’re working on to improve  how efficiently we build GitHub , as well as increase GitHub’s availability, stability, and resilience. We know how important the stability of our platform is for developers and enterprises, and it continues to be a priority area of investment across GitHub. . In that spirit, we want to share a change in how we make new feature releases available to our GitHub Enterprise Server customers. This change will take effect with our next release, and we hope this increases our collaboration with our GHES customers and improves our release process. . Release candidates, or RCs, are builds that allow our GitHub Enterprise Server customers to try the latest release early. These RCs are a way for us to work with our customers on bugs and issues that will be used to improve the quality of every release. . Working in the open like this is the best way for us to collaborate with our customers to improve GitHub Enterprise Server and ensure that we are delivering a product that meets and (hopefully) exceeds expectations. . Customers can start testing an RC as soon as it’s available, and release notes will accompany each RC. We expect each feature release will have one or more RC versions (eg. 2.22.0.RC1, 2.22.0.RC2), with each new version adding bug fixes for issues found in prior versions. The number of RCs will be driven by customer feedback, and we’ll decide based on quality and customer feedback when to publish and make generally available a final production release. . RCs can be upgraded to and from GA releases. They should be deployed on test or staging environments. . Customers that test RCs can raise issues with GitHub Support. Each RC is supported while live, but is not included in long-term support. . With this new RC process, testing and feedback from our customers will be critical. We’re confident this will help us improve GitHub Enterprise Server, together. We’ll have more to share about upcoming RCs at  GitHub Universe  next week. Make sure you tune in! ", "date": "December 3, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe evolving role of operations in DevOps\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-12-03-the-evolving-role-of-operations-in-devops/", "abstract": "  This is the third blog post in our series of DevOps fundamentals. For a quick intro on what DevOps is, check out    part one   ; for a primer on automation in DevOps, visit    part two   .  . As  businesses reorganize for DevOps , the responsibilities of teams throughout the software lifecycle inevitably shift. Operations teams that traditionally measure themselves on uptime and stability—often working in silos separate from business and development teams—become collaborators with new stakeholders throughout the software lifecycle. Development and operations teams begin to work closely together to build and continually improve their delivery and management processes. In this blog post, we’ll share more on what these evolving roles and responsibilities look like for IT teams today, and how operations help drive consistency and success across the entire organization. . To better understand how DevOps changes the responsibilities of operations teams, it will help to recap the traditional, pre-DevOps role of operations. Let’s take a look at a typical organization’s software lifecycle: before DevOps, developers package an application with documentation, and then ship it to a QA team. The QA teams install and test the application, and then hand off to production operations teams. The operations teams are then responsible for deploying and managing the software with little-to-no direct interaction with the development teams. . These dev-to-ops handoffs are typically one-way, often limited to a few scheduled times in an application’s release cycle. Once in production, the operations team is then responsible for managing the service’s stability and uptime, as well as the infrastructure that hosts the code. If there are bugs in the code, the virtual assembly line of dev-to-qa-to-prod is revisited with a patch, with each team waiting on the other for next steps. This model typically requires pre-existing infrastructure that needs to be maintained, and comes with significant overhead. While many businesses continue to remain competitive with this model, the faster, more collaborative way of bridging the gap between development and operations is finding wide adoption in the form of DevOps. . Over the past decade, the maturation of the public cloud has added complexity to the responsibilities of operations teams. The ability to rent stable, secure infrastructure by the minute and provide everything as a service to customers has enabled organizations to deploy rapidly and frequently, often several times per day. Smaller, faster delivery cycles give organizations the critical capability of improving their customer experience through rapid feedback and automated deployments. Cloud technologies have made  development  velocity a fundamental part of delivering a competitive customer experience. . Cloud technologies have transformed how we deliver and operate software, impacting how we do DevOps  today. Developers now focus more on stability and uptime in addition to developer velocity, and operations teams now have a stake in developer velocity along with their traditional role of maintaining uptime. When it comes to the specific role of operations in DevOps, this often means:   . While it’s well understood that DevOps requires close collaboration between teams, we’re often asked “How are development and operations functions  really  coordinated in a DevOps model?” At GitHub, we’re fortunate to partner with thousands of businesses every year on improving their DevOps practices. Sometimes these organizations focus on the clearest target, asking developers and delivery teams to go to market faster while paying less attention to the post-deployment operations teams. . However, we find the best results come through improving the practices of  all  the teams involved in the software lifecycle, together. Operations teams aren’t simply infrastructure and process owners for the organizations, but are also a critical part of the feedback loop for development. Try it out for yourself—a small pilot project that includes developers, release engineering, operations, and even InfoSec can give more teams the momentum they need. It can give them confidence to continue their work, establish best practices, and even train others within your organization along the way. .  For a closer look at IT operations in DevOps, tune in to next week’s GitHub Universe session:    Continuous delivery with GitHub Actions   .   ", "date": "December 3, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we launched docs.github.com\t\t", "author": ["\n\t\tSarah Schneider\t"], "link": "https://github.blog/2020-07-02-how-we-launched-docs-github-com/", "abstract": " ICYMI:  docs.github.com is the new place to discover all of GitHub’s product documentation ! . We recently completed a major overhaul of GitHub’s documentation websites. When you visit docs.github.com today, you’ll see content from the former help.github.com and developer.github.com sites in a unified experience. . Our engineering goals were two-fold: 1) improve the reading and navigation experience for GitHub’s users; 2) improve the suite of tools that GitHub’s writers use to create and publish documentation. . Combining the content was the last of several complex projects we completed to reach these goals. Here’s the story behind this years-long effort, undertaken in collaboration with GitHub’s Product Documentation team and many other contributors. . Providing separate docs sites for different audiences was the right choice for us for many years. But our plans evolved along with GitHub’s products. Over time, we aspired to help an international audience use GitHub by: . We couldn’t do these things when we had two static sites, each with its own codebase, its own way of organizing content, and its own markup conventions. Efforts were made to streamline the tooling over the years, but they were limited by the nature of static builds. . To achieve our goals, we determined we needed to write a custom dynamic backend, and eventually, combine the content. . Our docs sites were  previously hosted on GitHub Pages  using  Jekyll   practices: a  content  directory full of Markdown files and a  data  directory full of YAML files. This is a great setup for simple sites, and it worked for us for a long time. Although we outgrew Jekyll tooling, the writing conventions based in Markdown and YAML worked well. So we kept them, and we built the dynamic site around them. . Keeping these conventions let us alleviate pain points in the tooling without introducing a new paradigm for technical writing and asking writers to learn it. It also meant that writers could continue publishing content that helps people use GitHub in the old system while we built the new one. . We outgrew static site tooling for a number of reasons. A big factor was the complexity of versioning content for our GitHub Enterprise Server product. . We release a new version of GitHub Enterprise Server every three months, and we support docs for each version for one year before we deprecate them. At any time, we provide docs for  four  versions of GitHub Enterprise Server. . We handle this complexity by  single-sourcing  our docs. This means we provide multiple versions of each article, and a dropdown on the site lets users switch between versions. Here’s how it looked in the old help.github.com: .   . Versioning can be hard. Some articles are available in all versions. Some are GitHub.com-only. Some are Enterprise Server-only. Some have lots of internal conditionals, where a single paragraph or even a word may be relevant in some versions but not others. We also have workflows and tools for releasing new versions and deprecating old versions. . What does single-sourcing look like under the hood? Writers use the Liquid templating language ( another Jekyll holdover ) to render version-specific content using  if  /  else  statements: . Statements like this are all over the  content  and  data  files. . Static site generators are designed to do  one  build. They don’t build multiple versions of pages. To support our single-source approach in the Jekyll days, we had to create a  backport  process, in which writers would build Enterprise Server versions separately from building GitHub.com docs. Backport pull requests had to be reviewed, deployed to staging, and published as a separate process. Over the years, as we released new Enterprise Server versions, the tooling started to fray around the edges. Backports took a long time to build, did weird things, or got forgotten entirely. Ultimately, backports became a liability. . When we set out to create a new dynamic site, we started with help.github.com. We built it over six months and carefully coordinated with the writers to swap out the backend, while mostly leaving the content alone. In February 2019, we launched the new Node.js site backed by Express. On the frontend, there is just vanilla JavaScript and CSS using  Primer . . It was a big improvement: . Within a few months, the dynamic backend allowed us to reach our next major milestone: internationalization. We launched the  Japanese and simplified Chinese versions of the site in June 2019  and added support for  Spanish  and  Portuguese  by the end of the year. (Look for a deep dive post into the internationalization process coming soon!) . This was progress. But developer.github.com was still running on the old static build, and parts of it were starting to break down. We needed to bring the developer content into the new codebase. . First, we needed to more robustly support the idea of products. . When we originally launched the new help site, the homepage did allow users to choose a product: .  But the  content  for these products was organized in wildly different ways. For example, a directory called  content/dotcom/articles  contained nearly a thousand Markdown files with no hierarchy. URLs looked like  help.github.com/articles/&lt;article&gt; , with no indication of which product they belonged to. Writers and contributors had a hard time navigating the repository. It all “worked,” but it wouldn’t scale. . So we created a new product-centric structure that would be consistent across the site:  content/&lt;product&gt;/&lt;category&gt;/&lt;article&gt; , with URLs that matched. To support this change, we developed a new TOC system and refactored product handling on the backend. Once again, we coordinated the changes with writers who were still actively writing, and once again, we left the core Jekyll conventions untouched. We also added support for redirects from the legacy article URLs to the new product-based ones. . In 2019, we released  GitHub Actions  as the first new product on the help site. . With a more scalable content organization in place, we were ready to  start  thinking about how to get developer content into the codebase. . Historically, developer.github.com hosted documentation for integrators, including docs for GitHub’s two APIs: REST and GraphQL. . From the time GitHub’s REST docs were first released roughly a decade ago, they were handwritten and updated by humans using unstructured formats. This workflow was sustainable at first, but as the API grew, it became a big drain on writers’ time. Updating REST input and response parameters manually was hard enough, but versioning them for GitHub.com and Enterprise Server was almost impossibly complex. Readers had long asked for code samples and other standard features of API documentation we were unable to provide. We’d dreamed of autogenerating REST docs from a structured schema, but this seemed unattainable for years. . With the new codebase in place, it opened the door for us to think about autogeneration for real. And we happened into some lucky timing. .  Octokit maintainer Gregor Martynus  had already started the process of generating an OpenAPI schema that described how GitHub’s API works. This schema happened to be exactly what we needed. Rather than reinventing the wheel, we invested in that existing schema effort and enlisted the services of  Redoc.ly , a small firm that specializes in OpenAPI schema design and implementation. We worked with them to get the work-in-progress OpenAPI over the finish line and ready for production use, and created a pipeline to consume and render the docs from OpenAPI. . Check out the new REST docs:  http://docs.github.com/rest/reference  . GraphQL is a different story from REST. Since GitHub first released its GraphQL API in 2017, we’ve had a pipeline for autogenerating docs from a schema using  https://github.com/gjtorikian/graphql-docs . . This tooling worked well, but it was written in Ruby, and with the new Node.js backend, we needed something more JavaScript-friendly. We looked for existing JavaScript GraphQL docs generators but didn’t find any that fit our specific needs. So we rolled our own. . We wrote a script that takes a GraphQL schema as input, does some sanitization, and outputs JSON files containing only the data needed for rendering documentation. Our HTML files loop over that JSON data and render it on page load. . The script runs via a  scheduled GitHub Actions workflow  and automatically opens and merges PRs with the updates. This means writers never have to touch GraphQL documentation; it publishes itself. . Check out the new GraphQL docs:  http://docs.github.com/graphql/reference   . In addition to API docs, developer.github.com contained content about GitHub and OAuth apps, GitHub Marketplace, and webhooks. The majority of this content is vanilla Markdown, so the project requirements were (finally) straightforward: import the files, process them, run tests. We wrote scripts to do this in a repeatable process. . The content strategist on the docs team created a comprehensive spreadsheet that mapped all the old developer content to their new product-based locations, with titles and intros for each. This formed the basis of our scripted efforts. We ran the scripts several times, doing reviews and making changes each time, before unveiling the final documentation. . Check out the new docs:  http://docs.github.com/developers  . Getting a 404 on a documentation site is an abrupt end to a learning experience. GitHub’s docs team makes a commitment to prevent 404s when content files get renamed or moved around. There are a number of ways we support 20,000+ redirects in the codebase, and they can get complex. . For example, if you go to an Enterprise URL without a version, such as https://docs.github.com/enterprise, the site redirects you to the latest version by injecting the number in the URL. But we have to be careful – what if the URL happens to include  enterprise  somewhere but is not a real Enterprise Server path? No number should be injected in those cases. . We also redirect any URL without a language code to use the  /en  prefix. And we have special link rewriting under the hood to make sure that if you are on a Japanese page, all links to other GitHub articles take you to  /ja  versions of those pages instead of  /en  versions. . Soon we will enable blanket redirects to point most  https://developer.github.com  links to   https://docs.github.com  . That step won’t be too hard, but in the course of the migration, we changed the names and locations of much of the developer content. For example,  /v3  became  /rest/reference ,  /apps  became  /developers/apps , and so on. To support all these redirects, we worked from a list of the top few hundred developer.github.com URLs from Google Analytics to whittle down dead links, path by path. . These redirects will help GitHub’s users arrive at the content they want when they navigate docs.github.com or follow legacy bookmarks or links. . This is an exciting time for content at GitHub. With the foundation we built for docs.github.com, we can’t wait to continue improving the experience for people creating and using GitHub’s content. Keep an eye out for more behind-the-scenes posts about docs.github.com! .  \t\t Tags:   \t\t GitHub Docs \t ", "date": "July 2, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing the GitHub Availability Report\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-07-08-introducing-the-github-availability-report/", "abstract": " Historically, GitHub has published post-incident reviews for major incidents that impact service availability. Whether we’re sharing new investments to infrastructure or detailing site downtimes, our belief is that we can collectively grow as an industry by learning from one another. This month, we’re excited to introduce the GitHub Availability Report. . On the first Wednesday of each month, we’ll publish a report describing GitHub’s availability, including a description of any incidents that may have occurred and update you on how we are evolving our engineering systems and practices in response. You should expect these updates to include a summary of what happened, as well as a technical explanation for incidents where we believe the occurrence was novel and contains information that helps engineers around the world learn how to improve product operations at scale. . Availability and performance are a core feature, including how GitHub responds to service disruptions. We strive to engineer systems that are highly available and fault-tolerant and we expect that most of these monthly updates will recap periods of time where GitHub was &gt;99% available. When things don’t go as planned, rather than waiting to share information about particularly interesting incidents, we want to describe all of the events that may impact you. Our hope is that by increasing our transparency and sharing what we’ve learned, rather than simply reporting minutes of downtime on a status page, everyone can learn from our experiences. At GitHub, we take the trust you place in us very seriously, and we hope this is a way for you to help hold us accountable for continuously improving our operational excellence as well as our product functionality. . In May and June, we experienced four distinct incidents resulting in a lack of availability or degraded service for GitHub.com. . During the incident, a shared database table’s auto-incrementing ID column exceeded the size that can be represented by the MySQL Integer type (Rails int(11)): 2147483647. When we attempted to insert larger integers into the column, the database rejected the value and Rails raised an ActiveModel::RangeError, which resulted in 500s from our API endpoint. . This impacted GitHub apps that rely on getting installation tokens. The top affected GitHub apps internally included Actions, Pages, and Dependabot. . GitHub’s monitoring systems currently alert when tables hit 70% of the primary key size used. We are now extending our test frameworks to include a linter in place for int / bigint foreign key mismatches. . During a planned maintenance operation (failing over a MySQL primary instance) we experienced a novel crash in the mysqld process on the newly promoted MySQL primary server. To mitigate the impact of the crash, we manually redirected traffic back to the original primary. However, the crashed MySQL primary had already served approximately six seconds of write traffic. At this point, a restore of replicas from the new primary was initiated which took approximately four hours with a further hour for cluster reconfiguration to re-enable full read capacity. For a period of approximately five hours, users may have observed delays before data written to the affected database cluster were visible in the web interface and API. . We’ve run multiple internal gameday exercises in response to ensure a higher degree of preparedness for similar topology inconsistencies and will continue to exercise our automated failover systems to reduce recovery time. . Changes to better instrument A/B experimentation for UI improvements introduced an unknown dependency on the presence of a specific, dynamically generated file that is served by a separate application. . During an application deployment, the file failed to be generated on a significant proportion of the application deployments due to a high retrieval rate being rate limited by the upstream application. This resulted in site-wide application errors for a percentage of users enrolled in the experiment. Upon detection, we were able to disable the requirement on this file which restored service to all users. . Going forward, configuration for A/B and multivariate experiments will be cached internally to ensure successful propagation of dependencies. . As part of maintenance, the database team rolled out an updated version of ProxySQL on Monday, June 22. A week later, the primary MySQL node on one of our main database clusters failed and was replaced automatically by a new host. Within seconds, the newly promoted primary crashed.  Orchestrator ’s anti-flapping mechanism prevented a subsequent automatic failover. After we recovered service manually, the new primary became CPU starved and crashed again. A new primary was promoted which also crashed shortly thereafter. To recover, we rolled back to the previous version of ProxySQL and disabled a change in our application that had required the new ProxySQL version. When this completed, we were able to allow writes on the primary node without it crashing. . We are analyzing application logs, MySQL core dumps, and our internal telemetry as part of continued investigation into the CPU starvation issue to avoid similar failure modes going forward. . As an organization we continue to invest heavily in reliability. We treat each incident discussed here as an invaluable opportunity from which to learn and grow. Our systems and processes continue to evolve based on these learnings and we look forward to sharing our progress in future updates. . Please follow our  status page  for real time updates and watch our blog for next month’s availability report. .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "July 8, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing GitHub’s OpenAPI Description\t\t", "author": ["\n\t\tMarc-Andre Giroux\t"], "link": "https://github.blog/2020-07-27-introducing-githubs-openapi-description/", "abstract": " The GitHub REST API has been through three major revisions since it was first released,  only a month after the site was launched . We often receive feedback that our REST API is an inspiration to many for design, and that it’s an industry reference for what an API should look like. Today, we’re excited to announce an improvement to how developers can interact with the API. GitHub has open sourced an  OpenAPI description  of the REST API. . The  OpenAPI specification  is a programming language agnostic standard that lets providers describe the interface of their HTTP APIs. This allows both humans and machines to discover the capabilities of an API without needing to first read documentation or understand the implementation. OpenAPI is a widely adopted industry standard and GitHub is proud to be part of the community and help push the standard forward. . The GitHub OpenAPI description contains more than 600 operations exposed in our API. For visual exploration of the API, you can load the description as a  Postman Collection . Programmatically, the description can be used to generate mock servers, test suites, and bindings for languages not supported by  Octokit . . The description is provided under two formats. The  bundled  version is preferred for most use cases as it makes use of  OpenAPI components  for reuse and readability. For tooling that has poor support for inline references to components, we also provide a fully  dereferenced  version. . The description is currently in  beta . Describing a 12-year-old API is no easy task. We’ve built this description using a mix of existing JSON schemas, documented examples, contract testing, and love. We expect to make the description even more complete and accurate as we go forward and as OpenAPI becomes central to our developer experience — internally and externally. . Quarterly releases of the description are available for GitHub Enterprise Server and GitHub Private Instances, with versions like  v2.21 . More frequent updates to the description will be available for GitHub.com. . We’re always looking to make our OpenAPI description more complete and accurate as well as making it easier to consume. If you’d like to help contribute to the description,  check out our contributing guide . If something is not working for you, please file an Issue on the repository. . Building a complete OpenAPI description for the GitHub API was no easy task and could not have been possible without a great team. Thanks to  Gregor Martynus for his initial work on describing the API , the  Docs Engineering team for their amazing work around OpenAPI and documentation ,  Will Roden  for his help validating the description with  octo-go , as well as the folks at  Redoc.ly  who helped along the way. . Learn more about our  REST API OpenAPI Description  .  *  The OpenAPI Initiative logo is a trademark of The Linux Foundation  ", "date": "July 27, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tHighlights from Git 2.28\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2020-07-27-highlights-from-git-2-28/", "abstract": " The open source Git project just  released Git 2.28  with features and bug fixes from over 58 contributors, 13 of them new. We last caught up with you on the latest in Git back  when 2.26 was released . Here’s a look at some of the most interesting features and changes introduced since then. . When you initialize a new Git repository from scratch with  git init , Git has always created an initial first branch with the name  master . In Git 2.28, a new configuration option,  init.defaultBranch  is being introduced to replace the hard-coded term. (For more background on this change,  this statement from the Software Freedom Conservancy  is an excellent place to look). . Starting in Git 2.28,  git init  will instead look to the value of  init.defaultBranch  when creating the first branch in a new repository. If that value is unset,  init.defaultBranch  defaults to  master . Here, it’s important to note that: . This configuration variable can be set by the user, and overriding the default value is as easy as:   $ git config --global init.defaultBranch main     . This configuration variable only affects new repositories, and does not cause branches in existing projects to be renamed.  git clone  will also continue to respect the  HEAD  of the repository you’re cloning from, so you won’t see a change in branch names until a maintainer initiates one. . This change supports the many communities, both on GitHub and in the wider Git community, who are considering renaming the default branch name of their repository from  master . . To learn more about the complementary changes GitHub is making, see  github/renaming .  GitLab  and  Bitbucket  are also making similar changes. . [ source ] . In Git 2.27, the  commit-graph  file format was extended to store changed-path Bloom filters. What does all of that mean? In a sense, this new information helps Git find points in history that touched a given path much more quickly (for example,  git log -- &lt;path&gt; , or  git blame ). Git 2.28 takes advantage of these optimizations to deliver a handful of sizeable performance improvements. . Before we get into all of that, it’s worth taking a refresher through commit graphs whether you’re new to the concept, or familiar with them. (If you are familiar, and want to take a deeper dive, check out  this blog post explaining all of the juicy technical details ).  In the very simplest terms, the   commit-graph  file  stores information about commits. In essence, the  commit-graph  acts like a cache for commonly-accessed information about commits: who their parent(s) are, what their root tree is, and things like that. It also stores computed information, too, like a commit’s  generation number , and changed-path Bloom filters (more on that in just a moment). . Why store all of this information? To understand the answer to this, it is helpful to have a cursory understanding of how Git stores objects. Git stores objects in one of two ways: either as a  loose object  (in which case the object’s contents are stored in a single file unique to that object on disk), or as a  packed object  (in which case the object is assembled from a compressed format in a  *.pack  file). No matter which way a commit is stored, we still have to parse and decompress it before its fields like “root tree” and “parents” can be accessed. . With a  commit-graph  file, all of that information is immediate: for a given commit  C , Git knows exactly where to look in a commit-graph file for all of those fields that we store, and can read them off immediately, no decompression or piecing together required. This can shave some time off your usual Git operations by itself, but where the  commit-graph  really shines is in the computed data it stores. . Generation numbers are a sort of reachability index that can help Git answer questions about things like reachability and topological ordering very quickly. Since generation numbers aren’t new in this release (and trying to explain them quickly would lose a lot of the benefit of a more careful exposition), I’ll refer you instead to  this blog post  by freshly-minted Hubber  Derrick Stolee  on the matter. . OK, if you’ve made it this far, you’ve got a pretty good handle on what commit graphs are, and what they’re useful for. Now, let’s get to the juicy details. In Git 2.27, the  commit-graph  file learned how to store changed-path Bloom filters. What are changed-path Bloom filters, you ask? A  Bloom filter  is a  probabilistic set ; that is it’s a set of items, but querying that set for the presence of some item  x  returns either “ x  is definitely not in this set” or “ x  might be in this set”, but never “ x  is definitely in this set”. The  commit-graph  stores one of these Bloom filters for commits that reside in the  commit-graph , and it populates that Bloom filter with a list of paths changed between that commit and its first parent. . These Bloom filters are a huge boon for performance in lots of Git commands. The general pattern is something like: if you have a Git command that computes diffs (which can sometimes be proportionally expensive), then having Bloom filters allows Git to compute far fewer diffs by skipping the computation for certain commits when their Bloom filters return “definitely not” for paths of interest. . Take  git log -- /path/to/file , for example. Prior to Git 2.27,  git log  would have to compute a diff over every revision in its walk before determining whether or not to show it (i.e., whether or not that diff has any entries for  /path/to/file ). In Git 2.27 and newer, Git can skip computing many of those diffs altogether by consulting each commit  C ‘s changed-path Bloom filter and querying it for  /path/to/file . Again: if querying returns “definitely not”, then Git knows that computing that diff is strictly uninteresting. . Because computing diffs between commits can be expensive (at least, relative to the complexity of the algorithm for which they are being generated), reducing the number of diffs computed overall can greatly improve performance. . To try this for yourself, you can run the command: . This generates a  commit-graph  file with changed path Bloom filters enabled. [ 1 ]  You should be able to see performance improvements in commands like  git log -- &lt;path&gt; ,  git log -L ,  git blame , and anything else that computes first-parent diffs against a given pathspec. . [ source ,  source ,  source ] . Now that we’ve talked about a few of the headlining changes from the past couple of releases, let’s look at a few more new features 🔎 . [ source ] . [ source ] . [ source ] . [ source ] . [ source ] . [ source ] . That’s just a sample of changes from the latest couple of releases. For more, check out the release notes for  2.27  and  2.28 , or  any previous version  in the Git repository. .  [1] : Note that since Bloom filters are not persisted automatically (that is, you have to pass  --changed-paths  explicitly on each subsequent write), it is a good idea to disable configuration that automatically generates  commit-graph s, like  fetch.writeCommitGraph  and  gc.writeCommitGraph . .  \t\t Tags:   \t\t Git \t ", "date": "July 27, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tCodeGen: Semantic’s improved language support system\t\t", "author": ["\n\t\tAyman Nadeem\t"], "link": "https://github.blog/2020-08-04-codegen-semantics-improved-language-support-system/", "abstract": " The Semantic Code team shipped a massive improvement to the language support system that powers  code navigation . Code navigation features only scratch the surface of possibilities that start to open up when we combine  Semantic ‘s program analysis potential with GitHub’s scale. . GitHub is home to over 50 million developers worldwide. Our team’s mission is to analyze code on our platform and surface insights that empower users to feel more productive. Ideally, this analysis should work for all users, regardless of which programming language they use. Until now, however, we’ve only been able to support a handful of languages due to the high cost of adding and maintaining them. Our new language support system,  CodeGen , cuts that cost dramatically by automating a significant portion of the pipeline, in addition to making it more resilient. The result is that it is now easier than ever to add new programming languages that get parsed by our library. . Before Semantic can compute diffs or perform abstract interpretation, we need to transform code into a representation that is amenable to our analysis goals. For this reason, a significant portion of our pipeline deals with processing source code from files on GitHub.com into an appropriate representation—we call this our “language support system”. .   . Zooming into the part of Semantic that achieves language support, we see that it involved several development phases, including two parsing steps that required writing and maintaining two separate grammars per language. .   . Reading the diagram from left to right, our historic language support pipeline: .  Parsed source code into ASTs.  A grammar is hand-written for a given language using   tree-sitter  , an incremental GLR parsing library for programming tools. .  Read  tree-sitter  ASTs into  Semantic .  Connecting Semantic to  tree-sitter ‘s C library requires providing an interface to the C source. We achieve this through our   haskell-tree-sitter   library, which has Haskell bindings to  tree-sitter . .  Parsed ASTs into a generalized representation of syntax.  For these ASTs to be consumable by our Haskell project, we had to translate the  tree-sitter  parse trees into an appropriate representation. This required:      À la carte syntax types:  generalization across programming languages  Many constructs, such as  If  statements, occur in several languages. Instead of having different representations of  If  statements for each language, could we reduce duplication by creating a generalized representation of syntax that could be shared across languages, such as a datatype modeling the semantics of conditional logic? This was the reasoning behind creating our hand-written, generalized à la carte syntaxes based on Wouter Swierstra’s  Data types à la carte  approach, allowing us to represent those shared semantics across languages. For example,  this file  captures a subset of à la carte datatypes that model expression syntaxes across languages.     Assignment:  turning  tree-sitter ‘s representation into a Haskell datatype representation  We had to translate  tree-sitter  AST nodes to be represented by the new generalized à la carte syntax. To do this, a second grammar was written in Haskell to  assign  the nodes of the  tree-sitter  ASTs onto a generalized representation of syntax modeled by the à la carte datatypes. As an example,  here  is the  Assignment  grammar written for Ruby.     .   À la carte syntax types:  generalization across programming languages  Many constructs, such as  If  statements, occur in several languages. Instead of having different representations of  If  statements for each language, could we reduce duplication by creating a generalized representation of syntax that could be shared across languages, such as a datatype modeling the semantics of conditional logic? This was the reasoning behind creating our hand-written, generalized à la carte syntaxes based on Wouter Swierstra’s  Data types à la carte  approach, allowing us to represent those shared semantics across languages. For example,  this file  captures a subset of à la carte datatypes that model expression syntaxes across languages. .   Assignment:  turning  tree-sitter ‘s representation into a Haskell datatype representation  We had to translate  tree-sitter  AST nodes to be represented by the new generalized à la carte syntax. To do this, a second grammar was written in Haskell to  assign  the nodes of the  tree-sitter  ASTs onto a generalized representation of syntax modeled by the à la carte datatypes. As an example,  here  is the  Assignment  grammar written for Ruby. .  Performed Evaluation.  Next, we captured what it meant to interpret the syntax datatypes. To do so, we wrote a polymorphic type class called  Evaluatable , which defined the necessary interface for a term to be evaluated.  Evaluatable  instances were added for each of the à la carte syntaxes. .  Handled Effects.  In addition to evaluation semantics, describing the control flow of a given program also necessitates modeling effects. This helps ensure we can represent things like the file system, state, non-determinism, and other effectful computations. .  Validated via tests.  Tests for diffing, tagging, graphing, and evaluating source code written in that language were added along the process. . The process described had several obstacles. Not only was it very technically involved, but it had additional limitations. .  The system was brittle.  Each language’s  Assignment  code was tightly coupled to the language’s  tree-sitter  grammar. This meant it could break at runtime if we changed the structure of the grammar, without any compile-time error. To prevent such errors required tracking ongoing changes in tree-sitter, which was also tedious, manual, and error-prone. Each time a grammar changed, assignment changes had to be made to accommodate new tree-structures, such as nodes that changed names or shifted positions. Because improvements to the underlying grammars required changes to  Assignment —which were costly in terms of time and risky in terms of the possibility of introducing bugs—, our system had inadvertently become incentivized  against  iterative improvement. .  There were no named child nodes.   tree-sitter ‘s syntax nodes didn’t provide us with named child nodes. Instead, child nodes were structured as ordered-lists, without any name indicating the role of each child. This didn’t match Semantic’s internal representation of syntax nodes, where each type of node has a specific set of named children. This meant more  Assignment  work was necessary to compensate for the discrepancy. One concern, for example, was about how we represented comments, which could be any arbitrary node attached to any part of the AST. But if we had named child nodes, this would allow us to associate comments relative to their parent nodes (like if a comment appeared in an  if  statement, it could be the first child for that  if  statement node). This would also apply to any other nodes that could appear anywhere within the tree, such as Ruby heredocs. .  Evaluation and à la carte sum types were sub-optimal.  Taking a step back to examine language support also gave us an opportunity to rethink our à la carte datatypes and the evaluation machinery. À la carte syntax types were motivated by a desire to better share tooling in evaluating common fragments of languages. However, the introduction of these syntax types (and the design of the  Evaluatable  typeclass) did not make our analysis sensitive to minor linguistic differences, or even to relate different pieces of syntax together. We could overcome this by adding language-specific syntax datatypes to be used with  Assignment , along with their accompanying  Evaluatable  instances—but this would defeat the purpose of a generalized representation. This is because à la carte syntax was essentially untyped; it enforced only a minimal structure on the tree. As a result, any given subterm could be any element of the syntax, and not some limited subset. This meant that many  Evaluatable  instances had to deal with error conditions that in practice can’t occur. To make this idea more concrete, consider examples showcasing a before and after syntax type transformation:   -- former system: à la carte syntax  data Op a = Op { ann :: a, left :: Expression a, right :: Expression a }     -- new system: auto-generated, precisely typed syntax  data Op a = Op { ann :: a, left :: Err (Expression a), right :: Err (Expression a) }    The shape of a syntax type in our à la carte paradigm has polymorphic children, compared with the monomorphic children of our new “precisely-typed” syntax, which offers better guarantees of what we could expect.  . The shape of a syntax type in our à la carte paradigm has polymorphic children, compared with the monomorphic children of our new “precisely-typed” syntax, which offers better guarantees of what we could expect. .  Infeasible time and effort was required.  A two-step parsing process required writing two separate language-specific grammars by hand. This was time-consuming, engineering-intensive, error-prone, and tedious. The  Assignment  grammar used parser combinators in Haskell mirroring the tree-sitter grammar specification, which felt like a lot of duplicated work. For a long time, this work’s mechanical nature begged the question of whether we could automate parts of it. While we’ve open-sourced Semantic, leveraging community support for adding languages has been difficult because, until recently, it was backed by such a grueling process. . To address challenges, we introduced a few changes: .  Add named child nodes.  To address the issue of not having named child nodes, we modified the  tree-sitter  library by adding a new function called  field  to the grammar API and resultantly updating every language grammar. When parsing, you can retrieve a nodes’ children based on their field name. Here is an example of what a Python  if_statement  looks like in the old and new  tree-sitter  grammar APIs:    .  Generate a Node Interface File.  Once a grammar has this way of associating child references, the parser generation code also produces a  node-types.json  file that indicates what kinds of children references you can expect for each node type. This JSON file provides static information about nodes’ fields based on the grammar. Using this JSON, applications like ours can use meta-programming to generate specific datatypes for each kind of node.  Here  is an example of the JSON generated from the grammar definition of an  if  statement. This file provided a schema for a language’s ASTs and introduced additional improvements, such as the way we specify highlighting. .  Auto-generate language-specific syntax datatypes.  Using the structure provided by the  node-types.json  file, we can auto-generate syntax types instead of writing them by hand. First, we deserialize the JSON file to capture the structure we want to represent in the desired shape of datatypes. Specifically, we have four distinct shapes that the nodes in our node-types JSON file take on:  sums ,  products ,  named leaves , and  anonymous leaves . We then use Template Haskell to generate syntax datatypes for each of the language constructs represented by the Node Interface File. This means that our hand-written à la carte syntax types get replaced with auto-generated language-specific types, saving all of the developer time historically spent writing them.  Here  is an example of an auto-generated datatype representing a Python  if  statement derived from the JSON snippet provided above, which is structurally a product type. .  Build ASTs generically.  Once we have an exhaustive set of language-specific datatypes, we need to have a mechanism that can map appropriate auto-generated datatypes onto the ASTs representing the source code being parsed. Historically, this was accomplished by manually writing an  Assignment  grammar. To obviate the need for a second grammar, we have created an API that uses Haskell’s  generic metaprogramming framework  to unmarshal tree-sitter’s parse trees automatically. We iterate over  tree-sitter ‘s parse trees using its  tree cursor API  and produce Haskell ASTs, where each node is represented by a Template Haskell generated datatype described by the previous step. This allows us to parse a particular set of nodes according to their structure, and return an AST with meta-data (such as range and span information).  Here is an example  of the AST generated if the Python source code is simply  1     . The final result is a set of language-specific, strongly-typed, TH-generated datatypes represented as the sum of syntax possible at a given point in the grammar. Strongly-typed trees give us the ability to indicate only the subset of the syntax that can occur at a given position. For example, a function’s name would be strongly typed as an  identifier ; a  switch  statement would contain  case  statements; and so on. This provides better guarantees about where syntax can occur, and strong compile-time guarantees about both correctness and completeness. . The new system bypasses a significant part of the engineering effort historically required; it cuts code from our pipeline in addition to addressing the technical limitations described above. The diagram below provides a visual “diff” of the old and new systems. .   . A big testament to our approach’s success was that we were able to  remove our à la carte syntaxes  completely. In addition, we were also able to ship two new languages,  Java  and  CodeQL , using precise ASTs generated by the new system. . To learn more about how you can help, check out our documentation  here . ", "date": "August 4, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: July 2020\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-08-05-github-availability-report-july-2020/", "abstract": "  In July we experienced one specific incident resulting in a degraded state of availability for GitHub.com. We’d like to share our learnings from this incident with the community in the spirit of being transparent about our service disruptions, and helping other services improve their own operations.  .  The incident started when our production Kubernetes Pods started getting marked as unavailable. This cascaded through our clusters resulting in a reduction in capacity, which ultimately brought down our services. Investigation into the Pods revealed that a single container within the Pod was exceeding its defined memory limits and being terminated. Even though that container is not required for production traffic to be processed, the nature of Kubernetes requires that all containers be healthy for a Pod to be marked as available.  .  Normally when a Pod runs into this failure mode, the cluster will recover within a minute or so. In this case, the container in the Pod was configured with an ImagePullPolicy of Always, which instructed Kubernetes to fetch a new container image every time. However, due to a routine DNS maintenance operation that had been completed earlier, our clusters were unable to successfully reach our registry resulting in Pods failing to start. This issue impact was increased when a redeploy was triggered in an attempt to mitigate  ,   and we saw the failure start to propagate across our production clusters. It wasn’t until we restarted the process with the cached DNS records that we were able to successfully fetch container images, redeploy, and recover our services.   .  Moving forward, we’ve identified a number of areas to address this quarter:   .  In parallel, we have an ongoing workstream to improve our approach to progressive deployments that will provide the ability to carefully evaluate the impact of deployments in a more incremental fashion. This is part of a broader engineering initiative focused on reliability that we will have more details on in the coming months.  .  We place great importance in the reliability of our service along with the trust that our users place in us every day. We look forward to continuing to share more details of our journey and hope you can learn from our experiences along the way.   .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "August 5, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tWhy Write ADRs\t\t", "author": ["\n\t\tEli Perkins\t"], "link": "https://github.blog/2020-08-13-why-write-adrs/", "abstract": " Architecture decision records, also known as ADRs, are a great way to document how and why a decision was reached within a codebase. We’ve started to adopt them within the mobile team here at GitHub, documenting decisions that affect the iOS codebase and Android codebase, as well as decisions that affect both mobile clients. . ADRs are not the most common within open source codebases, but have gained more popularity  since ~2017 within long-lived, “evolutionary” codebases  like those in more enterprise-y settings. . So why write an ADR? Why spend time documenting something when a decision has already been made? . ADRs are not meant to be a self-discovery, reflection process on what you decided. ADRs will help you 6-12 months from now recall what your mindset was when you decided upon that architecture. .  ADRs capture the decision at the time it’s being made.  ADRs are the culmination of all those minutes and hours you spent in meetings, on Zoom, in Slack, or jamming through various proof-of-concepts in Xcode. All of the context that’s in your head has a chance to get out into words so that when you’re revisiting the architecture down the road, you can put that context right back into your head. . The real bonus comes when someone  git blame s you some months from now and asks you how the  GitHubAPIClient  module works. Better than setting up a 30-minute pairing call to walk them through the code, you can now link to the ADR you wrote to explain some more about the decisions made while building the  GitHubAPIClient  module. . ADRs force you to write more than a one-liner “this ships the feature for #3128”. They are a longer form of prose to help your teammates understand why the feature is built the way it is, and not built some other way (see: “Alternatives Considered” and “Pros/Cons” within ADRs themselves). . Something that may be simple for you might be complicated for your teammates. Taking the time to write down what your thought process was as you made decisions gives your teammates the chance to get inside your head. Writing ADRs allows for  “decision socialization” , where your  team  comes to a decision that the  team  is responsible for maintaining, rather than decisions made in isolation. . By  expanding  upon what you write in your pull request titles and descriptions (and you are still writing quality pull request descriptions, right?),  you give your teammates more information about how a patch or diff works in a larger system.  . Better yet, by writing an ADR  ahead  of putting your pull requests up, you’ll get better pull request reviews from the team reviewing it. No longer do you need to explain how line 387 in  APIClient+Caching.swift  will affect data fetching and caching architecture, because your teammates already understand how you’re changing the system from the ADR you wrote about “Adding cache support to  E-Tag  entities”. . ADRs are not meant you to show off how smart you are or for folks to fawn over the architecture you built. ADRs  are  for helping onboard new teammates as they work to understand the codebase and how it has evolved over time. . As teams scale and grow, the number of lines of communication between teammates increases. A team of three individuals only has three lines of communication ( A &lt;&gt; B ,  A &lt;&gt; C ,  B &lt;&gt; C ). A team of four has six ( A &lt;&gt; B ,  A &lt;&gt; C ,  A &lt;&gt; D ,  B &lt;&gt; C ,  B &lt;&gt; D ,  C &lt;&gt; D ). Wanna do the math for a team of five or six? How about 14 engineers, two designers, two PMs, and three EMs? . Writing down decisions made helps communicate to your current teammates, but also those who join your team as it scales and grows.  By informing your team how and why a decision was made in an asynchronous fashion, you no longer need to “hop on a Zoom call” to onboard each new teammate on a per-architectural-decision basis.  . In the best-case scenario, you’ll have your teammates writing new ADRs for  you ,  superseding the decisions  past you made, so that you can learn from your teammates in the future. . I hope this has convinced you to document the decisions that you’re making as we build software that (hopefully) millions of folks use! As our team grows larger and our codebases grow more entangled and intertwined, architecture decision records are a great way to help future us, our current teammates, and future teammates. . 👋 Thanks for coming to my TED talk. ", "date": "August 13, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing the Rally + GitHub integration\t\t", "author": ["\n\t\tJared Murrell\t"], "link": "https://github.blog/2020-08-18-introducing-the-rally-github-integration/", "abstract": " GitHub’s Professional Services Engineering team has decided to open source another project:  Rally + GitHub . You may have seen our most recent open source project,  Super Linter . Well, the team has done it again, this time to help users ensure that Rally stays up to date with the latest development in GitHub! 🎉 . This project integrates GitHub Enterprise Server (and cloud, if you host it yourself) with Broadcom’s Rally project management. . Every time a pull request is created or updated,  Rally + GitHub  will check for the existence of a  Rally User Story  or  Defect  in the  title ,  body , or  commit messages , and then validate that they both  exist  and are in the correct  state  within  Rally . .   . GitHub Enterprise Server had a legacy Services integration with Rally. The deprecation of legacy Services for GitHub was  announced in 2018 , and the release of GitHub Enterprise Server 2.20  officially removed this functionality . As a result, many GitHub Enterprise users will be left without the ability to integrate the two platforms when upgrading to recent releases of GitHub Enterprise Server. . While Broadcom  created a new integration for github.com , this functionality does not extend to GitHub Enterprise Server environments. . We encourage you to check out this  project  and set it up with your existing Rally instance. A good place to start getting set up is the  Get Started  guide in the project’s  README.md  . We invite you to join us in developing this project! Come engage with us by opening up an  issue  even just to share your experience with the project. .   ", "date": "August 18, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tUpgrading GitHub to Ruby 2.7\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2020-08-25-upgrading-github-to-ruby-2-7/", "abstract": " After many months of work, we deployed GitHub to production using Ruby 2.7 in July. For those who aren’t familiar with GitHub’s stack, we’ve been running on Ruby since the beginning. Many years ago, we ran GitHub on a fork of Ruby (and Rails!) and while that hasn’t been the case for some time, that experience taught us how important it is to keep up with new releases. . Ruby 2.7 is a unique upgrade because the Ruby Core team has  deprecated how keyword arguments behave . With this release, future versions of Ruby will no longer accept passing an options hash when a method expects keyword arguments. At GitHub, we’re committed to running deprecation-free on both Ruby and Rails to prevent falling behind on future upgrades. It’s important to identify major changes early so we can evolve the application when necessary. . In order to run Ruby 2.7 deprecation-free, we had to fix over 11k warnings. Fixing that many warnings, some of which were coming from external libraries, takes a lot of coordination and teamwork. In order to be successful we needed a solid strategy for sharing the work. . Just like we did with our  Rails upgrade , we set up our application to be dual-bootable in both Ruby 2.6 and Ruby 2.7 by using an environment variable. This made it easy for us to make backwards compatible changes, merge those to the main branch, and avoid maintaining a long running branch for our upgrade. It also made it easier for other engineering teams who needed to make changes to get their system running with the new Ruby version. Due to how large our application is (over 400k lines!) and how many changes go in daily (100’s of PRs!), this drastically simplifies our upgrade process. . Once we had the build running, we weren’t quite yet ready to ask other teams to help fix warnings. Since Ruby warnings are simply strings in the test output we needed to capture the deprecations and turn them into lists for each team to fix. . To accomplish this we monkey patched the  Warning  module in Ruby. Here’s a simplified version of our monkey patch: . The patch stores the deprecation warning and the test path that caused the warning in a  WarningCollector  object which writes the warnings to a file and then processes them: . The  WarningCollector#process  method stores all the warnings in a file called  warnings.txt . We then parse warnings using   CODEOWNERS   and turn them into files that correspond to each owning team. . Once we had all the warnings processed, we opened issues for those teams with easy-to-follow directions for booting the application in the new Ruby version. Our warning reports included the file emitting the warning, the warning itself, and the test suites that triggered the warnings. They looked like this: . This process helped us avoid duplicating work across teams and made it simple to determine ownership and status of each warning. . We tracked warning counts in the Ruby 2.7 CI build to ensure that new code wasn’t introducing new warnings. After a few months, coordinating with 40 teams, 30+ gem upgrades, and 11k warnings our CI build was 100% warning-free. Gems that were unmaintained were replaced with maintained gems. Once we had fixed the warnings, we altered our monkey patch to raise errors in Ruby 2.7 which ensured that all new code going into the GitHub codebase was warning-free. . You may be reading this and wondering why it’s worth doing all this work and investing the engineering resources and time in the Ruby upgrade. If you’ve been writing Ruby for a while you’re likely aware of the difficulty with this particular upgrade. It’s been the topic of conversation in the Ruby community since before the release in December. Regardless of how hard this upgrade was, we saw an impressive improvement in performance. The Ruby Core team is well on their way to fulfilling the promise of  Ruby 3.0 being 3x faster . . First, we saw a drop in the amount of time it takes the application to boot in production mode. In production (this is when the entire application is eager loaded) we saw our boot time drop from an average of ~90 seconds to ~70 seconds. That’s a 20-second drop. Here’s a graph: .   . This faster boot time means faster deploys which means you get our features, bug fixes, and performance improvements faster as well! . In addition to an improvement in boot time, we saw a decrease in object allocations which went from ~780k allocations to ~668k allocations. Object allocations affect available memory so it’s important to lower these numbers whenever possible. .   . Aside from the performance benefits of upgrading, ensuring you stay on the most recent version of your languages and frameworks helps keep your application healthy. Through this process we found a lot of unowned code that was no longer used in the application and deleted it. We also took this opportunity to remove or replace unmaintained gems in our application. . For gems that were maintained we gave back to the community by sending patches for any gems that were emitting warnings in our application including Rails,  rails-controller-testing ,  capybara ,  factory_bot ,  view_component ,  posix-spawn ,  github-ds ,  ruby-kafka , and many others. GitHub believes strongly in supporting the open source community and upgrades are one of many ways that we do so directly. . There are risks to deploying any major upgrade, but at GitHub we’ve designed processes that reduce this risk drastically. . For Ruby and Rails upgrades, we run in dual-builds until we’re sure all the tests are passing and code is stable. In addition, we have all teams that work on the core product click test their area of the codebase in a staging environment to ensure there are no obvious issues with the new version. . Rolling out the upgrade is a big deal, so we do it carefully by increasing the percentage of traffic running on the new version and verifying each deployment is error-free in Sentry and regression-free in Datadog. For this deploy, we rolled out to 2% of traffic and quickly saw a new frozen string exception. Due to our process we were able to rollback quickly and less than 10 users saw an error in one endpoint. . Once we had a fix for the frozen string exception, we restarted the rollout process and again deployed to 2% of traffic. We let this one sit for 15 minutes before going to the next percentage: 30% of Kubernetes partitions. Again we waited about 15 minutes and after verifying there was no regression we deployed to another 30% to total 60% of Kubernetes partitions. . Finally, we deployed to 30% of our non-Kubernetes deployment partitions. These deploys take longer because they need to compile Ruby. It’s a bit nerve-wracking waiting 15 minutes for Ruby to compile, but everything went smoothly. From there we did a full-production deploy and merged the upgrade after 30 minutes. Overall the entire deploy took about 2 hours. . At GitHub, we’ve invested in building out processes for deploying Ruby and Rails upgrades so that we can be confident they are the lowest possible risk. We had no downtime while deploying the Ruby upgrade and our customer impact was almost zero. . For any companies that are wondering if this upgrade is worth it the answer is: 100%. Even without the performance improvements, falling behind on Ruby upgrades has drastic negative effects on the stability of your codebase. Upgrading Ruby supports your application health, improves performance, fixes language and framework bugs, and guides the future of the language! . At GitHub, not only do we believe in the open source community, we believe that a strong foundation is the first step to a stable, resilient, and functioning application. Running on the most recent version Ruby helps us do just that. We’re looking forward to Ruby 3.0 and beyond. Happy upgrading! ", "date": "August 25, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Availability Report: August 2020\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-09-02-github-availability-report-august-2020/", "abstract": "  In August, we experienced no incidents resulting in service downtime. This month’s GitHub Availability Report will dive into updates to the GitHub Status Page and provide follow-up details on how we’ve addressed the incident mentioned in July’s report.  .  At GitHub, we are always striving to be more transparent and clear with our users. We know our customers trust us to communicate the current availability of our services and we are now making that more clear for everyone to see. Previously, our status page shared a 90-day history of GitHub’s availability by service, but this history can distract users from what’s happening actively, which during an incident is the most important piece of information. Starting today, the status page will display our current availability and inform users of any degradation in service with real-time updates from the team. The history view will continue to be available and can be found under the “incident history” link. As a reminder, if you want to receive updates on any status changes, you can subscribe to get notifications whenever GitHub creates, updates, or resolves an incident.   .  Check out the  new GitHub Status Page   .  .   .  Since the incident mentioned in  July’s GitHub Availability Report   , we’ve worked on a number of improvements to both our deployment tooling and to the way we configure our Kubernetes deployments, with the goal of improving the reliability posture of our systems.  .  First, we audited all the Kubernetes deployments used in production to remove all usages of the ImagePullPolicy of Always configuration.  .  Our philosophy when dealing with Kubernetes configuration is to make it easy for internal users to ship their code to production while continuing to follow best practices. For this reason, we implemented a change that automatically replaces the ImagePullPolicy of Always setting in all our Kubernetes-deployed applications, while still allowing experienced users with particular needs to opt out of this automation.  .  Second, we implemented a mechanism equivalent to the one of Kubernetes mutating admission controllers that we use to inject the latest version of sidecar containers, identified by the SHA256 digest of the image.  .  These changes allowed us to remove the strong coupling between Kubernetes Pods and the availability of our Docker registry in case of container restarts. We have more improvements in the pipeline that will help further increase the resilience of our Kubernetes deployments and we plan to share more information about those in the future.  .  We’re excited to be able to share these updates with you and look forward to future updates as we continue our efforts in making GitHub more resilient every day.   .  \t\t Tags:   \t\t GitHub Availability Report \t ", "date": "September 2, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tFebruary service disruptions\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-02-28-february-service-disruptions/", "abstract": "  Recently, we’ve had multiple service interruptions on GitHub.com. We know how important reliability of our service is for your projects and teams. We take this responsibility very seriously and apologize for these disruptions.  .  These incidents were distinct events, but have a common theme of uncovering new challenges in scaling our database tier. Specifically, increased load on our largest database cluster contributed to degradations across multiple services.    .  In the coming weeks, we’ll follow up with a more in-depth and technical report of these events and the work we are doing to improve the scalability and performance of our backend systems.    .  We have several data partitioning initiatives already in progress, and we’ll be rolling out some of this work very soon. You can follow our status page for updates about the availability of our systems.   .   .   Check GitHub’s current status   ", "date": "February 28, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tCERT partners with GitHub Security Lab for automated remediation\t\t", "author": ["\n\t\tNico Waisman\t"], "link": "https://github.blog/2020-03-18-cert-partners-with-github-security-lab-for-automated-remediation/", "abstract": " As security researchers, the GitHub Security Lab team constantly embarks on an emotional journey with each new vulnerability challenge. The excitement of starting new research, the disappointment that comes with hitting a plateau, the energy it takes to stay focused and on target…and hopefully, the sheer joy of achieving a tangible result after weeks or months of working on a problem that seemed unsolvable. . Regardless of how proud you are of the results, do you ever get a nagging feeling that maybe you didn’t make enough of an impact? While single bug fixes are worthwhile in improving code, it’s not sufficient enough to improve the state of security of the open source software (OSS) ecosystem as a whole. This holds true especially when you consider that software is always growing and changing—and as vulnerabilities are fixed, new ones are introduced. . At GitHub, we host millions of OSS projects which puts us in a unique position to take a different approach with OSS security. We have the power and responsibility to make an impact beyond single bug fixes. This is why a big part of the GitHub Security Lab mission is to find ways to scale our vulnerability hunting efforts and empower others to do the same. . Our goal is to turn single vulnerabilities into hundreds, if not thousands, of bug fixes at a time. Enabled by the GitHub engineering teams, we aim to establish workflows that are open to the community that tackle vulnerabilities at scale on the GitHub platform. . Ultimately, we want to establish feedback loops with the developer and security communities, and act as security facilitators, all while working with the OSS community to secure the software we all depend upon. . We’re taking a deep-dive in the remediation of a security vulnerability with CERT. Learn more about how we found ways to scale our vulnerability hunting efforts and empower others to do the same. .  Continue reading on the Security Lab blog  ", "date": "March 18, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tSix years of the GitHub Security Bug Bounty program\t\t", "author": ["\n\t\tBrian Anglin\t"], "link": "https://github.blog/2020-03-25-six-years-of-the-github-security-bug-bounty-program/", "abstract": " Last month GitHub reached some big milestones for our  Security Bug Bounty program . As of February 2020, it’s been six years since we started accepting submissions. Over the years we’ve been able to invest in the bug bounty community through live events, private bug bounties, feature previews, and of course through cash bounties. . We’re excited to announce that we recently passed $1,000,000 in total payments to researchers since we moved our program to HackerOne in 2016. We paid out over half of our total awards in the last year alone, reaching almost $590,000 in total bounty rewards across our programs. We’ve also been able to provide quick response times to an increasingly large amount of submissions—maintaining an average response time of 17 hours. This is all while seeing a 40 percent increase in submissions since last year. We’re sharing some highlights from the past year along with our upcoming plans for the future. . One of my favorite parts of working on the bug bounty program is getting to see the amazing submissions we get from the community. Many of the best submissions show an understanding of GitHub and our technology that rivals that of our own engineering teams. We’ve offered very competitive bounties so we can attract those talented individuals and provide them an incentive to spend time digging deep into our codebase. The community in 2019 did not disappoint. .  @not-an-ardvark  has a lot of great submissions to our program but this was particularly impactful. He wrote a great  post  about it in detail that I’ll quickly recap. . GitHub provides a few ways for integrators to interact with our ecosystem. One of the ways integrators can use GitHub is via OAuth applications which allow the application to take actions on behalf of a GitHub user. Before allowing access to a user’s data, an OAuth application must redirect the user to GitHub.com, allowing them to review the requested permissions and explicitly authorize the application. @not-an-ardvark found a way to bypass our controls to authorize OAuth applications without any user interaction. Let’s get into how this happened. . When we process state changing requests on GitHub.com, such as authorizing an OAuth application, we rely on Ruby on Rails’ Cross Site Request Forgery (CSRF) protection. We inject a special token into the DOM of every form element that we validate when receiving POST requests. The OAuth application authorization flow uses POST requests which require a valid CSRF token. However, the OAuth controller incorrectly allowed both POST and HEAD requests to trigger the authorization logic. We skip CSRF validation when processing HEAD requests since they’re not typically state changing. This allowed a malicious site to automatically authorize an OAuth application without any user interaction. . Due to the severity of the vulnerability, we needed to patch it as quickly as possible. We worked closely with the engineering team and shipped a fix to GitHub users within three hours of receiving the submission. We also conducted a full investigation with SIRT engineers and confirmed that this vulnerability wasn’t exploited in the wild. Additionally,  we rolled out patches for GitHub Enterprise Server for all supported versions . We rewarded @not-an-aardvark with $25,000 for the severity of the vulnerability and their detailed writeup in their submission. . This bug demonstrates the important role that researchers play in our overall security. By identifying this issue via our bug bounty program, we were able to protect our users by patching the issue and validating that it wasn’t previously exploited. .  @ajxchapman  achieved remote code execution in GitHub.com by triggering command injection in our Mercurial import feature. The import logic didn’t correctly sanitize branch names which allowed a maliciously crafted branch name to execute code on our servers. Since the import feature is quite complicated, we’ve traditionally run the import code in a sandbox on dedicated servers isolated from our production network. This isolation limited the impact of the vulnerability, and we were able to quickly release a fix for GitHub.com and backported the fix for GitHub Enterprise Server customers. We also audited the import logic for similar issues and confirmed from our logging systems that this wasn’t exploited in the wild. . What makes this bug particularly interesting is the root cause: it was ultimately caused by an outdated dependency. The bug existed in a dependency that handles code imports and was previously fixed upstream. However, we failed to keep up with the latest version and were ultimately vulnerable to this issue. This issue highlights how critical dependency management is to the overall success of a security program. GitHub continues to invest in  dependency management tooling  to keep us and our customers secure. Find more of Alex’s work on his  personal blog . . GitHub released many new features in 2019 that were added to our Security Bug Bounty scope: . We’ve had several valuable submissions that influenced the development of these products significantly. We paid out over $20,000 in bounties for vulnerabilities affecting the products in this expanded scope, and we’re excited to continue expanding our Bug Bounty scope as GitHub grows. . In August 2019, we returned to Las Vegas to participate in our second  H1-702 event . This event invited the top hackers from HackerOne’s platform to join us along with two other companies for three nights of live hacking. We were excited to participate and wanted to give researchers every incentive to dig deep into our application. We even added a bunch of bonuses on top of our base payouts, including bonuses for  Best Proof of Concept ,  Longest Exploit Chain , and  RCE . We also set up a CTF on GitHub.com to direct researchers to some of our newest attack surfaces. Lastly, we hid flags in a  Maintainer Security Advisory  and  GitHub Package Registry  with bonuses for every flag. We received positive feedback from some of our researchers about our CTF and will continue to include a CTF component in future events. . Overall, we paid out over $155,000 to researchers in one night, with half those rewards for high or critical severity issues. We can’t express how important live-hacking events, like H1-702, are to our bug bounty program. We look forward to more live-hacking events in the future and other new and innovative ways to engage the community. . Beyond the wide scope of our public program, we conducted an invite-only program where we preview features to researchers before they’re launched to everyone. These private programs allow us to work closely with a small group, and give us the opportunity to find bugs before they can affect the majority of our users. We’ve paid out just over $37,000 via our private program this year, and many of these findings were fixed before new features reached a significant number of our customers. . Following the success of our first private bug bounty targeting GitHub Actions, we wanted to re-run the private program to target the most recent iteration of our GitHub Actions product. We used what we learned in our first bug bounty to secure the product against similar issues. The community accepted the challenge and found novel bugs in our second iteration. . Just like any combination of two complex systems, the acquisition of Dependabot presented a unique challenge for our security team in integrating these two separate architectures. We used the private bug bounty to supplement our own security review of these new services. The findings from the private bug bounty program greatly informed how we integrated Dependabot with GitHub.com. We were also able to surface a few issues before rolling it out. . Like Dependabot, pull reminders required the same care and attention to ensure a secure transition from an integration to a first-party GitHub product. Pull reminders also added more complexity through its connection to Slack. Our own Slack integration provided a foundation for this feature, but there was significant re-architecture and development to tie these two features together. Again, we turned to our bug bounty community to test our pull reminder integration before releasing the feature widely. . We have a lot of plans for 2020 and want to highlight some of our upcoming changes. . We launched the  GitHub Security Lab bounty program  to incentivize researchers to help us secure all open source software. The new program rewards community members who write CodeQL queries that detect entire vulnerability classes so that the rest of the community can run those queries against their own projects. This results in removing  vulnerabilities at scale . . Making a contribution to this program not only influences the global state of software security,  but also prevents similar vulnerabilities from being released in the future. This is an exciting twist on our traditional bug bounty program, and we’re excited to see researchers using our new CodeQL tooling. To date, we received 20 submissions and awarded almost $21,000, with hundreds of vulnerabilities fixed across the OSS ecosystem as a direct result. . This year, we’re  assigning CVEs  to bounty submissions which affect GitHub Enterprise Server. This is a big step forward in consistently communicating the state of our software to our customers, but also provides another accolade for our researchers who identify vulnerabilities in GitHub Enterprise Server. . Are you excited by the new additions to our program? Get involved! Visit the  GitHub Security Bug Bounty page  for details of our scope, rules, and rewards. We can’t wait to make GitHub better for everyone with the help of your submissions. .  Learn more about the GitHub Security Bug Bounty  ", "date": "March 25, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tFebruary service disruptions post-incident analysis\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-03-26-february-service-disruptions-post-incident-analysis/", "abstract": " In late February, GitHub experienced multiple  service interruptions  that resulted in degraded service for a total of eight hours and 14 minutes over four distinct events. Unexpected variations in database load, coupled with an unintended configuration issue introduced as a part of ongoing scaling improvements, led to resource contention in our  mysql1  database cluster. . We sincerely apologize for any negative impact these interruptions may have caused. We know you place trust in GitHub for your most important projects—and we make it our top priority to maintain that trust with a platform that is highly durable and available. We’re committed to applying what we’ve learned from these incidents to avoid disruptions moving forward. . Originally,  all our MySQL data lived in a single database cluster . Over time, as  mysql1  grew larger and busier, we split functionally grouped sets of tables into new clusters and created new clusters for new features. However, much of our core dataset still resides within that original cluster. .  We’re constantly scaling our databases to handle the additional load  driven by new users and new products. In this case, an unexpected variance in database load contributed to cluster degradations and unavailability. . At this time, an unexpectedly resource-intensive query began running against our  mysql1  database cluster. The intent was to run this load against our read replica pool at a much lower frequency, but we inadvertently sent this traffic to the master of the cluster, increasing the pressure on that host beyond surplus capacity. This pressure overloaded ProxySQL, which is responsible for connection pooling, resulting in an inability to consistently perform queries. . Two days later, as part of a planned master database promotion, we saw unexpectedly high load which triggered a similar ProxySQL failure. The intent of this maintenance was to give our teams visibility into issues they might experience when a master is momentarily read-only. . After an initial load spike, we were able to use the same remediation steps as in the previous incident to restore the system to a working state. We suspended further maintenance events of this type as we investigated how this system failed. . In this third incident involving ProxySQL, active database connections crossed a critical threshold that changed the behavior of this new infrastructure. Because connections remained above the critical threshold after remediation, the system fell back into a degraded state. During this time, GitHub.com services were affected by stalled writes on our  mysql1  database cluster. . As a result of our previous investigation, we understood that file descriptor limits on ProxySQL nodes were capped at levels significantly lower than intended and insufficient to maintain throughput at high load levels. Specifically, because of a system-level limit of  1048576 , our process manager silently reduced our  LimitNOFILE  setting from  1073741824  to  65536 . During remediation, we also encountered a race condition between our process manager and service configurations which slowed our ability to change our file limit to  1048576 . . Application logic changes to database query patterns rapidly increased load on the master of our  mysql1  database cluster. This spike slowed down the cluster enough to affect availability for all dependent services. . We’ve used these events to identify operational readiness and observability improvements around how we need to operate ProxySQL. We have made changes to allow us to more quickly detect and address issues like this in the future. Remediating these issues were straightforward once we tracked down interactions between systems. It’s clear to us that we need better system integration and performance testing at realistic load levels in some areas before fully deploying to production. . We’re also devoting more energy to understanding the performance characteristics of ProxySQL at scale and the trickle-down effect on other services before it affects our users. . We decided to freeze production deployments for three days to address short-term hotspotting as a result of the final incident on February 27. This helped us stabilize GitHub.com. Our increased confidence in service reliability gave us the breathing room to plan next steps thoughtfully and also helped identify long-term investments we can make to mitigate underlying scalability issues. . We shipped a sizable chunk of data partitioning efforts we’ve worked on for the past six months just days after these incidents for one of our more significant MySQL table domains, the “abilities” table. Given that every authentication request to GitHub uses this table domain in some way, we had to be very deliberate about how we performed this work to fulfill our requirement of zero downtime and minimal user-impact. To accomplish this, we: . Removed all  JOIN  queries between the  abilities  table and any other table in the database . Built a new cluster to move all of the table data into . Copied all data to the new cluster using  Vitess ‘s vertical replication feature, keeping the copied data up-to-date in real-time . Moved all reads to the new cluster . Moved all writes to the new cluster using Vitess’ proxy layer  vtgate  . Steps one and two took months’ worth of effort, while steps three through five were completed in four hours. . These changes reduced load on the  mysql1  cluster master by 20 percent, and queries per second by 15 percent. The following graph shows a snapshot of query performance on March 2nd, prior to partitioning, and on March 3rd, after partitioning. Queries per second peaked at 109.9k/second immediately prior to partitioning and decreased to a peak of 89.19k queries/second after. .   . We know you depend on our platform to be reliable. With the immediate changes we’ve made and long-term plans in progress, we’ll continue to use what we’ve learned to make GitHub better every day. ", "date": "March 26, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tFrom 48k lines of code to 10—the story of GitHub’s JavaScript SDK\t\t", "author": ["\n\t\tGregor Martynus\t"], "link": "https://github.blog/2020-04-09-from-48k-lines-of-code-to-10-the-story-of-githubs-javascript-sdk/", "abstract": "  Gregor is the maintainer of the JavaScript Octokit. He’s a seasoned open source maintainer with a passion for automating repetitive tasks and lowering the barrier for contributors of all kinds and backgrounds. Aside from Octokit, Gregor is a maintainer of    Probot   ,    nock   , and    semantic-release   . Outside of work, he spends time taking care of his triplets Nico, Ada, and Kian. You can read more from Gregor on the    DEV community    and on    Twitter   .  .  @octokit/rest  wasn’t originally created by GitHub. Instead,  @bkeepers  decided to adopt the package that was the most popular back in 2017:  github . In this post, we’ll cover the story of   @octokit/rest  —the official JavaScript SDK for  GitHub’s REST APIs . . Later renamed to  @octokit/rest , the github module was one of the oldest projects in the Node ecosystem, with  its first commit  from June 2010. That was the time of  Node v0.1  and package.json didn’t exist because the npm registry was still being made. . In 2017, GitHub hired me to turn github into the official JavaScript SDK for GitHub’s APIs, for both Node.js and browsers.  You can even see my first commit  from September 2017. At the time, the project had close to 16 thousand lines of code (LOC) spread across three JavaScript files, one huge JSON file, and two files for TypeScript/Flow type definitions. . The primary goal of the project was maintainability. The library’s core piece was a huge  routes.json file  with nearly eight thousand LOC, and it was supposed to define all of GitHub’s REST API endpoints. The file was maintained manually, and endpoints were only added or fixed once someone realized it was missing or incorrect ( check out an example ). . With this in mind, instead of manually maintaining the definitions in routes.json, I created a script to scrape GitHub’s REST API documentation nightly and turn it into a machine-readable JSON representation:   octokit/routes  . If the script found changes,  @octokit/rest  received a pull request to update the routes.json file. After merging, a new version was released automatically. Thanks to the automation, the routes.json file was now guaranteed to cover all of GitHub’s REST API endpoints, and it consists of  10,275 LOC . The accompanying TypeScript definitions grew to over  26,700 LOC . . Once the completeness and maintainability were taken care of, I focused on another project goal:  Decomposability . . The JavaScript Octokit is meant for all JavaScript runtime environments, some of which have strict constraints. For example, bundle size is significant for browser usage. Instead of a single, monolithic library which implements all REST API endpoints, authentication strategies, and recommended best practices (such as pagination), it’s important to give users the choice to use lower-level libraries. That way users can make their own choices about trade-offs between bundle size and features. . Here’s an  overview  of the architecture I came up with in January 2018: .   . The result of the internal refactoring into the new architecture looked like this:   Note that   this output was simplified for readability.  . Over the next six months, I refactored the code and started extracting some of the modules: . After using plugins internally for roughly six months, I announced the plugin API in November 2018 with  version 16  and moved most of the library’s module to  internal plugins  that I could extract in the future. . The new internal code architecture was now looking like this:   Note that this output was simplified for readability : . Later, I created   @octokit/core.js  : the new Octokit JavaScript core library that  @octokit/rest  and all other Octokit libraries will be based on moving forward. Most of its logic was extracted from  @octokit/rest , without all deprecated features. I didn’t use it right away within @octokit/rest in order to avoid breaking changes. . As  @octokit/core  was free of any legacy code and had a lower cost for breaking changes, I experimented with decomposing the different means of authentication, too. The results are separate packages for each authentication strategy—all listed in   @octokit/auth ‘s README . If you’d like to learn more about GitHub’s authentication strategies, check out my series on  GitHub API Authentication . .  @octokit/core  and the separate authentication libraries would replace all the code in  lib/* and plugins/authentication/* . All that was left were three plugins that I extracted into their own modules: . The validate plugin became obsolete thanks to TypeScript’s code validation at compile time. There was no longer a need for validating request parameters in the client. That resulted in a significant reduction of code and bundles size. For example, here’s the current definition for the  octokit.checks.create()  method: . Starting with v17, the definition looks like this 😎: . Finally, all I needed to do was to put the previously extracted code back together. As promised, 10 LOC: . Every single line of code was changed between version 16 and the upcoming version 17 of  @octokit/rest . The only way to be confident that no new bugs were introduced is to run extensive tests. . When adopting the module back in 2017, no tests existed for our use case, but there were  usage examples . The first thing I did was turn these usage examples into integration tests. And because the JavaScript Octokit SDK was meant to be the start of a suite of SDKs across all popular languages, I created   octokit/fixtures  —a language-agnostic, auto-updating set of  http  mocks for common use cases. . For the remaining logic specific to  @octokit/rest , I created integration tests until we reached 100% test coverage. The tests will fail today if coverage drops below that. . While working on the migration to version 17 with its 10 LOC, I continued to run the tests of version 16 against the new version, with the exception of tests for deprecated APIs. At the same time, too many tests aren’t helpful either. Once I got all tests to pass for version 17, I reviewed all existing tests and removed any that no longer belong in  @octokit/rest . Some of the tests were moved into the plugins,  @octokit/core  or  @octokit/request . Now, all that’s left are a few smoke tests and the scenario tests using  @octokit/fixtures . .  @octokit/rest  used to be the Node.js library for GitHub’s REST APIs. Starting with v17, it will be the JavaScript library implementing all best practices for the  @octokit/rest  APIs, including pagination, throttling and automated request retries. It will support all existing and future authentication strategies and even GraphQL requests, as that is part of  @octokit/core . . And finally, we’d like to say thanks to  Fabian Jakobs ,  Mike de Boer , and  Joe Gallo  who created and maintained the github module before it became  @octokit/rest . ", "date": "April 9, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tSawfish phishing campaign targets GitHub users\t\t", "author": ["\n\t\tGitHub SIRT\t"], "link": "https://github.blog/2020-04-14-sawfish-phishing-campaign-targets-github-users/", "abstract": " Over the last week, GitHub has received reports related to a phishing campaign targeting our customers. We’re publishing this blog to increase awareness of this ongoing threat. . The phishing message claims that a repository or setting in a GitHub user’s account has changed or that unauthorized activity has been detected. The message goes on to invite users to click on a malicious link to review the change. Specific details may vary since there are many different lure messages in use. Here’s a typical example: .   . Clicking the link takes the user to a phishing site mimicking the GitHub login page, which steals any credentials entered. For users with  TOTP-based two-factor authentication  enabled, the site also relays any TOTP codes to the attacker and GitHub in real-time, allowing the attacker to break into accounts protected by TOTP-based two-factor authentication. Accounts protected by  hardware security keys  are not vulnerable to this attack. . The attacker uses the following tactics, but not all tactics are used in every case: . GitHub Security is monitoring for new phishing sites while filing abuse reports and takedown requests. We’re committed to enabling users and organizations to better secure their accounts and data, and provide assistance securing accounts and investigating activity associated with compromised accounts. . GitHub is working tirelessly to make existing security features more accessible, as well as adding new features designed to make user accounts significantly harder to compromise. . If you believe you may have entered credentials on a phishing site: . In order to prevent phishing attacks (which collect two-factor codes) from succeeding, consider using  hardware security keys  or  WebAuthn two-factor authentication . Also consider using a browser-integrated password manager. Many commercial and open-source options exist including browser-based password management native to popular web browsers. These provide a degree of phishing protection by autofilling or otherwise recognizing only a legitimate domain for which you have previously saved a password. If your password manager doesn’t recognize the website you’re visiting, it might be a phishing site. . To verify that you’re not entering credentials in a phishing site, confirm that the URL in the address bar is  https://github.com/login  and that the site’s TLS certificate is issued to GitHub, Inc. .   .   . If you’ve received phishing emails related to this phishing campaign, please  contact GitHub Support  with details about the sender email address and URL of the malicious site to help us respond to this issue. . Currently, we’ve observed the following phishing domains used by the attacker. Most of these are already offline, but the attacker frequently creates new domains and will likely continue to do so: ", "date": "April 14, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tApril service disruptions analysis\t\t", "author": ["\n\t\tKeith Ballinger\t"], "link": "https://github.blog/2020-05-22-april-service-disruptions-analysis/", "abstract": "  In April, we experienced multiple service interruptions impacting GitHub.com. Three distinct incidents resulted in unavailability for all of GitHub.com and resulted in degraded services for a total of 5 hours and 36 minutes.   .  We know that any disruption to our service can impact your development workflow, and apologize for these interruptions. We investigated each of these incidents and want to share an update on the causes and remediations.   .  We use a traditional    primary-replicas configuration   , where a primary cluster accepts writes and are then asynchronously fanned out to a number of replica cluster nodes that service read traffic for the majority of the website.   .  We run the majority of our systems on our own bare metal infrastructure, with our networking infrastructure built around a    Clos network topology    with each network device sharing routes via Border Gateway Protocol (BGP). Our edge network devices have complete internet routing tables while our internal network devices have only internal routes which span the internal data centers. The    GitHub Load Balancer    (GLB) runs as the primary ingress point for customer traffic to our network, and also acts as an internal gateway between our applications and many of our internal services and databases they depend on.  .  A misconfiguration of our software load balancers disrupted internal routing of traffic between applications that serve GitHub.com and the internal services they depend on. The misconfiguration introduced TCP socket binds that went over a predefined limit. The load balancer canary deployments don’t include all subsystems, so this problem only occurred when deploying the load balancer to each site.   .  A misconfiguration of database connections, related to our ongoing data partitioning efforts, made it unexpectedly to production. This caused 40 percent of traffic to the main mysql1 database cluster powering GitHub.com to stop using replicas for reads and sent all traffic to the mysql1 primary node.  .  This pressure overloaded the primary node and caused a majority of write traffic to fail. Around 40 percent of requests to GitHub.com failed during a period of 50 minutes. Because of the central nature of the data on the main mysql1 database cluster, all services of GitHub.com and all users were affected.  .  A networking configuration was inadvertently applied to our production network, causing a failure of our networking switches for 31 minutes. The configuration intent was to pass a subset of routes to the downstream devices table. The router couldn’t interpret the policy mistake, and it reacted by propagating too many routes to every downstream switch in the region. This resulted in disruptions for all GitHub.com services for an additional two hours.  .  Following these three incidents, we were able to identify gaps between our staging/canary environments and production. We’re devoting more energy to close these gaps across engineering and more quickly detect and address issues like this in the future.   .  One significant investment we’ve made is building a hardware staging environment for network continuous integration. We now have a network staging environment in place, which mirrors our production networks to test changes via CI. This has been in flight for the past nine months and landed last week. We intend to have the first CI jobs for network configuration working in the coming weeks. We are prioritizing this work for our networking template engines in order to reach 100 percent software coverage in the next quarter.  .  Another area we’ve identified that presents gaps is our staging labs environment. This staging environment does not set up the databases and database connections the same way as the production environment. This can lead to limited testability of connection changes specific to the production environment. We will be addressing this issue in the coming months.  .  Developers use GitHub every day to create, collaborate, learn, and get their work done, but none of our work matters if we can’t promise our customers the reliability they need. We’re committed to addressing and learning from these issues to earn the trust you place in us and make GitHub better every day.  .  You can follow our status page for updates about the availability of our systems.   .   Check GitHub’s current status   ", "date": "May 22, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing GitHub Super Linter: one linter to rule them all\t\t", "author": ["\n\t\tLucas Gravley\t"], "link": "https://github.blog/2020-06-18-introducing-github-super-linter-one-linter-to-rule-them-all/", "abstract": " Setting up a new repository with all the right linters for the different types of code can be time consuming and tedious. So many tools and configurations to choose from and often more than one linter is needed to cover all the languages used. . The  GitHub Super Linter  was built out of necessity by the GitHub Services  DevOps  Engineering team to maintain consistency in our documentation and code while making communication and collaboration across the company a more productive experience. Now we are open sourcing that so everyone can use and improve it! . The  Super Linter  solves many of these requirements through automation. Some included features: . The Super Linter is a  source code repository  that is packaged into a Docker container and called by GitHub Actions. This allows for any repository on GitHub.com to call the Super Linter and start utilizing its benefits. . The Super Linter will currently support a lot of languages and more coming in the future. For details on languages, check out the   README.md  . . When you’ve set your repository to start running this action, any time you open a pull request, it will start linting the code case and return via the Status API. It will let you know if any of your code changes passed successfully, or if any errors were detected, where they are, and what they are. This then allows the developer to go back to their branch, fix any issues, and create a new push to the open pull request. At that point, the Super Linter will run again and validate the updated code and repeat the process. You can configure your branch protection rules to make sure all code must pass before being able to merge as an additional measure. . There’s a ton of customization with flags and templates that can help you customize the Super Linter to your individual repository. Just follow the detailed directions at the  Super Linter repository  and the  Super Linter wiki . . This tool can also be helpful for any repository where multiple types of code and/or documentation all live together (monorepo). .   . Standardizing a rule set across the Super Linter has been an interesting challenge as each developer is unique in how they code. This is why we allow users to use any rules for the linter as they see fit for their repository. But, if no ruleset is defined, we must default to a certain standard. . The rule set for Ruby and Rails are pulled from the Ruby gem:   rubocop-github   and follow the same rules and versioning we use on GitHub.com. . For other languages, we choose what is the default when installing the linter such as:   coffeelint   or   yamllint  . For others, we try to find a happy middle ground that lays the simple groundwork and helps establish some best practices like:   Markdownlint   or   pylint  . . The beauty of this is, out of the box you will start establishing the framework, and your team can decide at any point, if additional customization is needed, you have all the ability to do so. . Just navigate to the Super Linter and copy templates from the  TEMPLATES  folder to your local repository. . We encourage you to set up this action and start the process of cleaning up your codebase and building your team’s standards and best practices. . We’re always looking to update best practices, add additional languages, and make the tool easier for consumption. If you’d like to help contribute to this action, check out our  contributing guide . .  Learn more about our Super Linter  ", "date": "June 18, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tUsing GitHub Actions for MLOps & Data Science\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2020-06-17-using-github-actions-for-mlops-data-science/", "abstract": "  Machine Learning Operations  (or MLOps) enables Data Scientists to work in a more collaborative fashion, by providing testing, lineage, versioning, and historical information in an automated way.  Because the landscape of MLOps is nascent, data scientists are often forced to implement these tools from scratch. The closely related discipline of  DevOps  offers some help, however many  DevOps  tools are generic and require the implementation of “ML awareness” through custom code. Furthermore, these platforms often require disparate tools that are decoupled from your code leading to poor debugging and reproducibility. . To mitigate these concerns, we have created a series of GitHub Actions that integrate parts of the data science and machine learning workflow with a software development workflow. Furthermore, we provide components and examples that automate common tasks. . Consider the below example of how an experiment tracking system can be integrated with  GitHub Actions  to enable MLOps. In the below example, we demonstrate how you can orchestrate a machine learning pipeline to run on the infrastructure of your choice, collect metrics using an experiment tracking system, and report the results back to a pull request. .   . A screenshot of this  pull request . . For a live demonstration of the above example, please see  this talk . . MLOps is not limited to the above example. Due to the composability of GitHub Actions, you can stack workflows in many ways that can help data scientists. Below is a concrete example of a very simple workflow that adds links to  mybinder.org  on pull requests: . When the above YAML file is added to a repository’s  .github/workflow  directory, pull requests can be annotated with a useful link as illustrated below [1]: .   . There is a growing number of Actions available for machine learning ops and data science. Below are some concrete examples that are in use today, categorized by topic. .  Orchestrating Machine Learning Pipelines : .  Jupyter Notebooks : .  End-To-End Workflow Orchestration : .  Experiment Tracking  . This is by no means an exhaustive list of the things you might want to automate with GitHub Actions with respect to data science and machine learning.   You can follow our progress towards this goal  on our page , which contains links to  blog posts ,  GitHub Actions ,  talks , and  examples  that are relevant to this topic. . We invite the community to create other Actions that might be useful for the community. Some ideas for getting started include data and model versioning, model deployment, data validation, as well as expanding upon some of the areas mentioned above. A great place to start is the documentation for GitHub Actions, particularly on how  to build Actions for the community ! . Footnotes: . [1] This example workflow will not work on pull requests from forks.  To enable this, you have to trigger a PR comment to occur via a  different event.  .  \t\t Tags:   \t\t GitHub Actions \t ", "date": "June 17, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tAccelerating the GitHub Sponsors beta with Stripe Connect\t\t", "author": ["\n\t\tKatie Delfin\t"], "link": "https://github.blog/2019-09-10-accelerating-the-github-sponsors-beta/", "abstract": "  Update : GitHub Sponsors is available in every country where GitHub does business, not just the 30 countries supported by Stripe Connect. We’ll continue to use our existing manual payout system for anyone outside of Connect’s list of currently supported countries. .  Back in May,    we announced GitHub Sponsors   , a new way to support the developers who build and maintain the open source software you use every day. We launched in beta as early as possible so we could work closely with the community to address feedback and understand their needs before adding more developers to the program. Today, we’re taking the next step in accelerating the availability of GitHub Sponsors through a new streamlined onboarding and payment experience with    Stripe Connect   .  .  Within hours of announcing GitHub Sponsors, thousands of people signed up for the waitlist from all over the world. It was awesome to see so much excitement for the program, but we knew this meant we had a lot of work ahead of us.   .  We started the beta with a small group of sponsored developers, and every few weeks, we invited more into the program. Everything was a manual process—setting up account information, verifying identity, waiting for approval, running reports, and processing payouts across   multiple departments. Due to the manual nature of the process, we were limited to a small number of people in the program until we could automate and streamline our operations.  .  At GitHub, we do business with companies of all sizes, from Fortune 50 companies to early stage startups from all over the world. Collecting revenue globally is something we’ve done for years, but with GitHub Sponsors, it’s not just about receiving money—it’s also about paying it out. This means providing a seamless and secure experience for sponsored developers to verify their identities, enter banking information, receive funds, and manage payouts. We also know that, for many maintainers, this will be the first time they’ve been financially rewarded for their contributions to open source. We want to ensure the process is easy for developers to complete, so they can spend more time doing what they love, like contributing to open source.  .   Stripe Connect Express    offers a simple, low overhead onboarding experience that complies with web accessibility guidelines, coupled with the ability to localize the experience to simplify payouts in countries with unique tax laws and compliance regulations. And by not building our own onboarding solution, we’re able to save months of engineering and maintenance time, and start onboarding more sponsored developers today.   .  One of Stripe Connect’s features that’s helped us deliver a great experience to our users is their localization support. Stripe just released international support for 30 countries (with more to come) for Connect Express. With this expanded support, Stripe takes care of adjusting for localized rules and regulations for supported countries—no small feat. We couldn’t be more thrilled to be one of the first to offer support across all 30 countries to serve our global community.  .   .  Our maintainer onboarding process can now start to keep up with the growing demand of the program. With Stripe, users can onboard using Connect Express, reducing onboarding time from a week-long process to under five minutes.    .  And there’s much more to come. While we’re just under four months into the program, we’re focused on   making GitHub Sponsors available to even more developers around the world.  .   Sign up to join the beta   —you’ll hear from us soon.  .   Learn more about GitHub Sponsors   ", "date": "September 10, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tTrack your work easily with the latest changes to project boards\t\t", "author": ["\n\t\tLauren Brose\t"], "link": "https://github.blog/2019-09-25-project-board-improvements/", "abstract": "  We want to make it easy for you to track your work with GitHub Projects, but noticed a few issues get in the way of a seamless experience. Project automation helps keep your issues in sync with your development process but created a problem with your column prioritization by placing moved issues at the top of the list. We also updated the issue sidebar to help show you where an item was in your project board flow but required you to navigate away to make any changes.  .  The projects section of the issue sidebar was updated to better convey related project information. Closed projects are now collapsed in the sidebar by default so you can focus on the most relevant information.   Additionally, you can change the issue’s project column directly from the issue page without needing to navigate away.  .   .  Now, any issues moved via automation or the inline sidebar will move to the bottom of the destination column. However, any “Done” automation triggers will continue to move issues to the top of the column so that your team can always see what was recently finished.  .  We hope this change makes your workflow smoother, but we can always make changes to improve.    Let us know    how the issue sidebar changes are working for you, and if there’s anything we can do to help.  .   Learn more about project boards   ", "date": "September 25, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tThree bugs in the Go MySQL Driver\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2020-05-20-three-bugs-in-the-go-mysql-driver/", "abstract": " Although GitHub.com is still a Rails monolith, over the past few years we’ve begun the process of extracting critical functionality from our main application, by rewriting some of the code in Go—mostly addressing the pieces that need to run faster and more reliably than what we can accomplish with Ruby. Last year, we deployed a new authorizations service in production,  authzd , which powers the “fine-grained authorizations” feature that we announced in GitHub Satellite 2019. . This has been a significant milestone and an interesting challenge, because  authzd  is our first Go service that reads from the production MySQL databases as part of a web request. In the past, we’ve deployed other Go services in our infrastructure that read and write to MySQL, but they’re either internal management services (such as our Search cluster manager,  manticore ), or async batch jobs (such as  gitbackups , which takes care of scheduling Git repository backups).  authzd  is called several times during a normal request to our Rails monolith, meaning that its requirements for performance and reliability are much more stringent. . Adding to this challenge,  authzd  is deployed to our Kubernetes clusters, where we’ve been experiencing issues with high latencies when opening new TCP connections, something that particularly affects the pooling of connections in the Go MySQL driver. One of the most dangerous lies that programmers tell themselves is that the network is reliable, because, well, most of the time the network  is  reliable. But when it gets slow or spotty, that’s when things start breaking, and we get to find out the underlying issues in the libraries we take for granted. . Here’s what it took, from a MySQL point of view, to get  authzd  ready to serve all our production traffic while meeting our availability SLOs. . For a long time,  \"Invalid connection (unexpected EOF)\"  was the most elusive of all known Go MySQL bugs. It’s been over two years since  I initially diagnosed and reported the bug upstream : we first came across this problem when deploying the initial versions of our repository backup orchestrator ( gitbackups ) to production.  At the time, there was no consensus on how to fix it on the MySQL driver, so we fixed  gitbackups  with a workaround in the application’s code. Last year, when doing our first  authzd  deploys to production, the issue was back in full effect: hundreds of  Invalid connection  errors per minute were dramatically increasing  authzd ‘s error response rate. This forced me to address the issue again, and I finally landed a fix upstream that solves the issue without having to change any application code. Let’s dive into the issue and its fix. . The job of a Go SQL driver is to provide a simple primitive to the  database/sql  package that comes in the Go standard library, a “SQL connection”. The connections that a driver provides are stateful, and cannot be used concurrently, because this is the default behavior for the connections of all major SQL servers (including MySQL and Postgres). This is where the  database/sql  package comes in: it manages the access and lifetime of these connections. The  DB  struct in the package, which is the entry point for all operations you can perform on a SQL database, is an abstraction over a connection pool, full of individual connections provided by the driver. Hence, calling methods like  (*DB).Query  or  (*DB).Exec  involves quite a bit of complex logic: polling the connection pool for a connection that is active but idle, or calling the underlying Driver to create a new connection if all the existing ones are busy, and then executing the command or query and returning the connection back to the pool, or discarding it if the pool is full. . The “unexpected EOF” crash we experienced in both  gitbackups  and  authzd  always happens  when taking a connection out of the connection pool and trying to perform a query on it. The reason was obvious simply by reading the MySQL server logs. The server was closing connections that spent too long in Go’s  sql.(*DB)  pool because they reached their idle timeout. Keep in mind, we run our production MySQL clusters with a pretty aggressive idle timeout  (30s) to ensure we never have too many connections open at any given time. And yet, the fact that this invalid connection error was being reported to the app was puzzling (and a problem, because our service was continuously crashing). . It’s common for TCP connections to be interrupted. The  database/sql  package is designed to handle an interruption in any of the hundreds of simultaneous connections that sit in a  (*DB)  pool at any time, and the specific driver implementation is supposed to help with this. A driver connection can become unhealthy for a multitude of reasons, whether it’s because it’s been closed, it received a bad packet, or any of the many things that can go wrong with a SQL connection. Whenever a driver detects that a connection has become unhealthy, it’s supposed to return  driver.ErrBadConn  on the next call to any of the methods in the connection. .  ErrBadConn  is the magic word that signals the  database/sql  package that this stateful connection is no longer valid. If the connection was in the pool, it needs to be removed from the pool, or if the connection was just pulled from the pool to perform a query, it must be discarded and a new connection must be pulled from the pool (or created) so the query can be performed. With this logic in mind, it’s expected that calling  (*DB).Query  would never fail with an “invalid connection” error, because even if the library attempted to perform the query in an invalid connection,  driver.ErrBadConn  would be returned and cause the query to be retried in another connection, transparently, and without the user noticing. . So, what’s going on in  authzd  to cause these “invalid connection” errors? . There’s a nuanced mismatch between the TCP protocol and the MySQL wire protocol overlaid on top. A TCP connection is full-duplex, where data can be flowing in each direction independently of the other direction, while the overlaid MySQL protocol on top  becomes fully client-managed once it’s in its Command Phase. A MySQL server only sends a packet to a MySQL client in response to a packet from the client. This becomes an issue when the server closes an active connection in the Command Phase, because it reached its idle timeout, or because connections are actively being pruned by processes such as  pt-kill . Let’s look at a network diagram: .     .  Figure 1: A network flow diagram representing the packets sent between the Go MySQL driver and the MySQL server.  . In this reproduction of the bug, we can see a standard TCP handshake, followed by a request and response pair at the MySQL level. We then sleep for several seconds. After a while without sending requests to the MySQL server, we reach the server-side timeout and the MySQL server performs an active  close  to our socket. When a TCP server closes a socket, the server’s kernel sends a  [FIN, ACK]  pair to the client, which means that the server is finished sending data. The client’s kernel acknowledges the  [FIN, ACK]  with an  [ACK]  but it doesn’t close its side of the connection—this is what we mean by TCP being full-duplex. The write side, client -&gt; server,  of the connection is independent, and must be explicitly closed by calling  close  in the client. . In most network protocols on top of TCP, this isn’t an issue. The client is performing  read s from the server, and as soon as it receives a  [SYN, ACK] , the next read returns an  EOF  error, because the Kernel knows that the server won’t write more data to this connection. However, as discussed earlier, once a MySQL connection is in its Command Phase, the MySQL protocol is client-managed. The client only reads from the server  after  it sends a request, because the server only sends data in response to requests from the client. . The previous network diagram clearly shows the practical effects of this. After the sleeping period, we send a request to the server and this  write  succeeds because our side of the connection is still open. The server consequently answers our request with a  [RST]  packet because  it’s  actually closed—we just don’t know about it yet. And then, when our client attempts to read the response from the MySQL server, it belatedly finds out that the connection is invalid as it receives an  EOF  error. . This answers why our connection is crashing, but not why our application code is crashing with it. Our MySQL connection is no longer valid, and we found out about it (better late than never), so why doesn’t our MySQL driver return  driver.ErrBadConn  when this happens? And why doesn’t it allow  database/sql  to retry our query in a brand new connection? Sadly, because transparently retrying the query is not safe to do in the general case. . The sequence of events depicted in the previous network diagram is very frequent. In our production MySQL clusters, it happens thousands of times per minute, particularly during off-peak hours when our service is mostly idle and the timeouts are reached. But that’s far from the only thing that could cause a connection to be closed. . What would happen if we performed an  UPDATE  in a perfectly healthy connection, MySQL executed it, and then our network went down before it could reply to us? The Go MySQL driver would also receive an  EOF  after a valid write. But if it were to return  driver.ErrBadConn ,  database/sql  would run the  UPDATE  statement on a new connection. Disaster! Data corruption! And what a thoroughly annoying place to be in: we know that in the common case (when MySQL kills our connection), it’s safe to retry these queries. There was no error when writing the query to the network, but we know MySQL didn’t receive it — let alone executed it. But we must assume the worst, that the query  did  make it to the server. And hence, returning  driver.ErrBadConn  is not safe. . So, how do we go about fixing this? The most obvious solution, and the one we applied on  gitbackups  back in the day, is calling  (*DB).SetConnMaxLifetime  on our client to set the max lifetime of MySQL connections to a value lower than our MySQL cluster’s idle timeout. This is far from ideal. For starters,  SetConnMaxLifetime  sets the  maximum duration  of any connection on the client, not its  maximum idle duration . That means that  database/sql  will continuously kill connections when they’ve lived that long, even if the connections are being actively used and could not trigger the server’s idle timeout. The  database/sql  package does not provide a “maximum idle duration” API for databases, because not all SQL servers implement the concept of idle timeouts. Ah, the miseries of abstraction. In practice, this fix works OK except for the needless connection churn, and cases where the MySQL server is actively prunning connections, which this workaround cannot detect. . Clearly, the optimal solution would be checking for a connection’s health once it’s pulled from the connection pool, and before we attempt to  write  any requests to it. This was unfortunately not possible until the introduction of   SessionResetter   in Go 1.10: before that interface was available, there was no way to know when a connection was being returned to the pool, which is why the bug stalled for almost 2 years until we could come up with an adequate fix. . There are two ways to check the health of that connection: at the MySQL level, or at the TCP level. A MySQL health check involves sending a  PING  packet and waiting for its response. It’s always safe to return  driver.ErrBadConn  because the ping packet doesn’t perform write operations in MySQL (one would hope). However, there is the drawback of adding arbitrary latency to the first operation performed on a connection fresh off the pool. That’s a bit spooky, because connections are sent and returned to the pool often in a Go application. Consequently, we went with the cheaper and simpler fix which is to simply check whether the MySQL server has closed its side of the connection at the TCP level. . Performing this check is very inexpensive, all we have to do is a non-blocking read on our connection before we attempt any writes. If the server has closed its side of the connection, we’ll get an  EOF  right away. If the connection is still open, the read also returns immediately but with an  EWOULDBLOCK  error, signaling that no data exists to be read (yet). Now, the good news is that all sockets in a Go program are already set to non-blocking mode. Don’t be fooled by the elegant abstraction of Goroutines. Under the hood, the user-land Go scheduler uses an async event loop to put Goroutines to sleep and wake them once they have data to read (how quaint). The bad news is that we don’t get to perform non-blocking reads by calling methods such as  net.Conn.Read , because the scheduler will put us right to sleep (again, the elegant abstraction of Goroutines). The proper interface to perform this non-blocking read wasn’t introduced until Go 1.9:   (*TCPConn).SyscallConn   gives us access to the underlying file descriptor so we can use the  read  system call directly. . Armed with this new API,  I was able to implement a connection health check that solved the “stale connections” issue  with less than five microseconds of overhead. A non-blocking read is very quick because, huh, that’s the point of non-blocking reads—they don’t block. . After deploying the fix in production, all the “Invalid Connection” needles disappeared right away, resulting in our first “9” of availability for the service. .     .  Figure 2: Client-side needles and retries in  authzd  after a deploy  . If your MySQL server uses idle timeouts, or is actively pruning connections, you don’t need to call  (*DB).SetConnMaxLifetime  in your production services. It’s no longer needed as the driver can now gracefully detect and retry stale connections. Setting a max lifetime for connections simply causes unnecessary churn by killing and re-opening healthy connections. . A good pattern to manage high-throughput access to MySQL is configuring your  sql.(*DB)  pool ( (*DB).SetMaxIdleConns  and  (*DB).SetMaxOpenConns ) with values that support your peak-hour traffic for the service, and making sure that your MySQL server is actively pruning idle connections during off-hours. These pruned connections are detected by the MySQL driver and re-created when necessary. . When we put a service like  authzd  as a dependency in the standard request path for our monolith, we’ve fundamentally added its response latency to the response latency of all the requests of our app, or for the requests that perform some kind of authorization—which is a lot of them. It’s crucial to ensure that  authzd  requests never take more time than what we’ve allotted for them, or they could very easily lead to a site-global performance degradation. . From the Rails side, this means being very careful with how we perform RPC requests to the service and how we time them out. In the Go server side, it means one thing,  Context . The  context  package was introduced to the standard library back in Go 1.7, although its API was already available for years as a separate library. Its premise is simple, pass a  context.Context  to every single function that is involved in the life-cycle of your service’s requests to make your service cancellation-aware. Each incoming HTTP request provides a  Context  object, which becomes canceled if the client disconnects early, and on top of it you can extend your  Context  with a deadline. This allows us to manage early request cancellation and request timeouts using a single API. . The engineering team in charge of developing the new service did a stellar job of ensuring that all methods in  authzd  were passing around a  Context , including the methods that perform queries to MySQL. Passing a  Context  to these methods is  fundamental  because they’re the most expensive part of our request life-cycle, and we need to ensure the  database/sql  query methods are receiving our request’s  Context  so they can cancel our MySQL queries early if they’re taking too long. . In practice, however, it seemed like  authzd  wasn’t taking request cancellations or timeouts into account: .     .  Figure 3: In-service response times for an  authzd  deployment. The resolver’s timeout was set to 1000ms—you can tell from the helpful red line I’ve drawn in the graph, but definitely not from the many random spikes that climb all the way to 5000ms.  . Just from looking at these metrics, it’s quite clear that even though we’re successfully propagating our request’s  Context  throughout our application’s code, there’s a point during the lifetime of some requests where cancellation is simply ignored. Finding this spot through code review can be rather laborious, even when we strongly suspect that the source of the timeouts must be the Go MySQL driver, since it’s the only part of our service that performs external requests. In this case, I captured a stack trace from a production host to find out where the code was blocking. It took several tries to capture a stack trace that seemed relevant, but once I had one that was blocking on a  QueryContext  call, the issue became immediately obvious: . The issue is that we’re not propagating our request’s  Context  as deeply as we initially thought. Although we’re passing the  Context  to  QueryContext  when performing SQL queries, and this context is being used to actually perform and timeout the SQL queries, there’s a corner case in the  database/sql/driver  API we’re missing. When we’re trying to perform a query but no connections are available in our connection pool, we need to create a new SQL connection by calling   driver.Driver.Open()  , but this interface method doesn’t take a  context.Context ! . The previous stack trace clearly shows how in  (8)  we stop propagating our  Context  and simply call  (*MysqlDriver).Open()  with the DSN to our database. Opening a MySQL connection isn’t a cheap operation: it involves doing a TCP open, SSL negotiation (if we’re using SSL), performing the MySQL handshake and authentication, and setting any default connection options. In total, there are  at least  six network round trips which do not obey our request timeout, because they’re not  Context -aware. . What do we do about this? The first thing we tried is setting the  timeout  value on our connection’s DSN, which configures the TCP open timeout that  (*MySQLDriver).Open  uses. But this isn’t good enough, because TCP open is just the first step of initializing the connection. The remaining steps (MySQL handshake, etc) weren’t obeying any timeouts, so we still had requests going way past the global 1000ms timeout. . The proper fix involved refactoring a large chunk of the Go MySQL driver. The underlying issue was introduced back in the Go 1.8 release, which implemented the  QueryContext  and  ExecContext  APIs. Although these APIs could be used to cancel SQL queries because they’re  Context  aware, the  driver.Open  method wasn’t updated to actually take a  context.Context  argument. This new interface was only added two patches later, in Go 1.10: a new   Connector   interface was introduced, which had to be implemented  separately  from the driver itself. Hence, supporting both the old  driver.Driver  and  driver.Connector  interfaces required major changes in the structure of any SQL driver. Because of this complexity, and a lack of awareness that the old  Driver.Open  interface can easily lead to availability issues, the Go MySQL driver never got around to implementing the new interface. . Fortunately, I had the time and patience to undertake that refactoring. After  shipping the resulting Go MySQL driver with the new Connector  interface in  authzd , we could verify on stack traces that the context passed to  QueryContext  was being propagated when creating new MySQL connections: . Note how  (*DB).conn  now calls our driver’s  Connector  interface directly, passing the current  Context . Likewise, it was very clear that the global request timeout was being obeyed for  all  requests: .     .  Figure 4: Resolver response times with a timeout set at 90ms. You can tell that the  Context  is being respected because the 99th percentile looks more like a barcode than a graph  . You don’t need to change your  sql.(*DB)  initialization to use the new  sql.OpenDB  API. Even when calling  sql.Open  in Go 1.10, the  database/sql  package detects the  Connector  interface and new connections are created with  Context  awareness. . However, changing your application to use  sql.OpenDB  with a  mysql.NewConnector  has several benefits, most notably the fact that the connection options for your MySQL cluster can be configured out of a  mysql.Config  struct, without requiring to compose a DSN. . Don’t set a  ?timeout=  (or its  mysql.(Config).Timeout  equivalent) on your MySQL driver. Having a static value for a dial timeout is a bad idea, because it doesn’t take into account how much of your current request budget has been spent. . Instead, make sure that every SQL operation in your app is using the  QueryContext  /  ExecContext  interfaces, so they can be canceled based on your current request’s  Context  regardless of whether the connection is failing to dial or the query is slow to execute. . Last, here’s a very serious security issue caused from a data race, which is as devious as it is subtle. We’ve talked a lot about how  sql.(*DB)  is simply a wrapper around a pool of stateful connections, but there’s a detail we haven’t discussed. Performing a SQL query, either via  (*DB).Query  or  (*DB).QueryContext , which is one of the most common operations you can do on a database, actually  steals  a connection from the pool. . Unlike a simple call to  (*DB).Exec , which has no actual data returned from the database,  (*DB).Query  may return one or more rows, and in order to read these rows from our app code, we must take control of the stateful connection where those rows will be written to by the server. That’s the purpose of the  sql.(*Rows)  struct, which is returned from each call to  Query  and  QueryContext : it wraps a connection we’re borrowing from the pool so we can read the individual  Row s from it. And this is why it’s critical to call  (*Rows).Close  once we’re done processing the results from our query, otherwise the stolen connection never returns to the connection pool. . The flow of a normal SQL query with  databaseb/sql  has been identical since the very first stable release of Go. Something like: . That’s straightforward in practice:  (*DB).Query  returns a  sql.(*Rows) , which is stealing a connection from the SQL pool—the connection on which we’ve performed the query. Subsequent calls to  (*Rows).Next  will read a result row from the connection, whose contents we can extract by calling  (*Rows).Scan , until we finally call  (*Rows).Close  and then any further calls to  Next  will return  false  and any calls to  Scan  will return an error. . Under the hood, the implementation involving the underlying driver is more complex: the driver returns  its own Rows interface , which is wrapped by  sql.(*Rows) . However, as an optimization the driver’s  Rows  doesn’t have a  Scan  method, it has a  Next(dest []Value) error  iterator. The idea behind this iterator is that it returns  Value  objects for each column in the SQL query, so calling  sql.(*Rows).Next  simply calls  driver.Rows.Next  under the hood, keeping a pointer to the values returned by the driver. Then, when the user calls  sql.(*Rows).Scan , the standard library converts the driver’s  Value  objects into the target types that the user passed as arguments to  Scan . This means that the driver doesn’t need to perform argument conversion (the Go standard library takes care of it), and that the driver can return  borrowed memory  from its rows iterator instead of allocating new  Value s, since the  Scan  method converts the memory anyway. . As you may guess right away, the security issue we’re dealing with is caused by this  borrowed memory . Returning temporary memory is actually an important optimization, which is explicitly encouraged in the driver contract, and works well in practice for MySQL because it lets us return pointers to the connection buffer where we’re reading results directly from MySQL. The row data for a SQL query as it comes off the wire can be passed directly to the  Scan  method, which will then parse its textual representation into the types ( int ,  bool ,  string , etc) that the user is expecting. And this has always been safe to do, because we’re not overwriting our connection buffer until the user calls  sql.(*Rows).Next  again, so no data races are possible. . …This was the case, at least, until the introduction of the  QueryContext  APIs in Go 1.8. With these new interfaces, it’s now possible to call  db.QueryContext  with a  Context  object that will interrupt a query early—we’ve discussed this at length in this post. But the fact that a query can be interrupted or timed out while we’re scanning its resulting rows opens up a serious security vulnerability. Any time a SQL query is interrupted while inside a  sql.(*Rows).Scan  call, the driver overwrites the underlying MySQL connection buffer, and  Scan  returns corrupted data. . This may seem surprising, but it makes sense if we understand that canceling our  Context  means canceling the SQL query  in the client , not in the MySQL server. The MySQL server continues serving the results of the query through our active MySQL connection, so if we want to be able to reuse the connection where a query has been canceled, we must first “drain” all the result packets that the server sent. To drain and discard all the remaining rows in the query, we need to read these packets into our connection buffer, and since query cancellation can happen at the same time as a  Scan  call, we overwrite the memory behind the  Value s the user is scanning from. The result is, with an extremely high likelihood, corrupted data. . This race is certainly spooky as described so far, but here’s the scariest thing of it all: in practice, you won’t be able to tell if the race is happening or has happened in your app. In the previous example of  rows.Scan  usage, despite the fact that we’re performing error checking, there is no reliable way to tell if our SQL query was canceled because our  Context  expired. If context cancellation has happened  inside  the  rows.Scan  call (for example, where it needs to happen for this data race to trigger), the row values are scanned, potentially with corrupted data, but no error is returned because  rows.Scan  only checks if the rows are closed once  when entering the function . Hence, when the race triggers we’re getting back corrupted data without an error return. We won’t be able to tell that the query has been canceled until our  next  call to  rows.Scan , but that call never happens because  rows.Next()  returns false, again without reporting an error. . The only way to tell whether scanning the rows of our SQL query has finished cleanly or whether the query has been canceled early is checking the return value of  rows.Close . And we’re not doing that because the rows are being closed in a  defer  statement. Oof. A  quick Code Search for  rows.Close()   on GitHub shows that pretty much  nobody  is explicitly checking the return value of  rows.Close()  in their Go code. This was actually safe to do before the  QueryContext  APIs were introduced, because  rows.Scan  would always catch any errors, but since Go 1.8, this isn’t  correct. Even after this data race is fixed in the Go MySQL driver, you won’t be able to tell whether you’ve scanned all the rows of your SQL query unless you check the return value of  rows.Close() . So update your linters accordingly. . I can think of several ways to fix this race. The most obvious is to always clone memory when returning from  driver.Rows.Next , but this is also the slowest. The whole point of the driver contract is that the driver doesn’t allocate memory when calling  driver.Rows.Next , because the memory allocation is delayed until the user calls  sql.(*Rows).Scan . If we allocate in  Next , we’re allocating  twice  for each row, which means that this bug fix would cause a significant performance regression. The Go MySQL maintainers were not OK with this, and I wasn’t either. Other similar approaches such as  truncating the underlying connection buffer when calling  sql.(*Rows).Close   were rejected for the same reason—they potentially allocate memory on each  Scan . All these rejected fixes led to a stalemate between the maintainers which caused this critical bug to become stale for over six months. I personally find it terrifying that such a bug can go unfixed for so long, so I had to come up with a hot new take to fix the issue in our production hosts without causing a performance regression. . The first thing I attempted was “draining” the MySQL connection without using the underlying connection buffer. If we don’t write to the connection buffer, then the data race won’t occur. This quickly became a nightmare of spaghetti code, because MySQL can send lots of packets which we must drain, and these packets all have different sizes, defined in their headers. Partially parsing these headers without a buffer was not pretty. . After some more thought, I finally came up with a solution which was extremely simple and fast enough to be eventually merged upstream—double buffering. In ancient Computer Graphics stacks, one would write individual pixels directly on a frame buffer, and the monitor would simultaneously read the pixels at its given refresh rate. When the frame buffer was being written to while the screen was reading from it, a graphical glitch occurred—what we commonly call  flickering . Flickering is fundamentally a data race which you can see with your own eyes, and you’ll agree that’s a pretty cool concept. The most straightforward way to fix flickering in computer graphics is to allocate  two  frame buffers: one buffer that the screen will be reading from (the front buffer), and another buffer that the graphics processor will be writing to (the back buffer). When the graphics processor is done rendering a frame, we atomically flip the back buffer with the front buffer, so the screen never reads a frame that’s currently being composed. .     .  Figure 5: Double buffering in the Nintendo 64 graphics stack. If this is good enough for Mario, it’s good enough for the MySQL driver  . This situation sounds a lot like what’s happening with the connection buffer on the MySQL driver, so why not fix it in the same way? In our case, when our  driver.Rows  struct is being closed because a query was canceled early, we  swap  our connection buffer with a background buffer, so that the user can still call  sql.Rows.Scan  on the front buffer while we’re draining the MySQL connection into the back buffer. The next time a SQL query is performed on the same connection, we’re going to continue reading into the background buffer, until the  Rows  are closed again and we flip the buffers to their original positions. This implementation is trivial, and although it allocates memory, it only does so  once  for the lifetime of a MySQL connection, so the allocation is easily amortized. We further optimized this corner case by delaying the allocation of the back buffer until the first time we need to drain data into it. In the common case where no queries are canceled early in a MySQL connection, no extra allocations are performed. . After  carefully crafting some benchmarks  to ensure that the double buffering approach wasn’t causing performance regressions in any cases, I finally  landed the bug fix upstream . Unfortunately, we have no way to check how often we’ve performed corrupted reads in the past because of this bug, but it’s probably been  quite a few  because, despite the subtlety in its origin, it’s surprisingly easy to trigger. . Always do an explicit check in for the error return in  (*Rows).Close . It’s the only way to detect whether the SQL query has been interrupted during scanning. However, do not remove the  defer rows.Close()  call in your app, as it’s the only way the  Rows  get closed if a  panic  or early return happens during your scanning. Calling  (*Rows).Close  several times is always safe. . Never use  (*Rows).Scan  with a  sql.RawBytes  target. Even though the Go MySQL driver is now much more resilient, accessing  RawBytes  can and will fail with other SQL drivers if your query is cancelled early. You will, most likely, read invalid data if that happens. The only difference between scanning into  []byte  and  sql.RawBytes  is that the raw version won’t allocate extra memory. This tiny optimization isn’t worth the potential data race in a  Context -aware application. . Deploying code in a new programming language to production is always a challenge, particularly at the scale GitHub operates. The throughput and complexity of our MySQL usage patterns means we’ve spent many years tweaking our MySQL client for Ruby, and now we’re excited to do the same for the Go MySQL driver. The fixes we’ve upstreamed as part of our first Go deployment are now generally available as part of the  1.5.0 release  of the Go MySQL driver, and we will continue to contribute more fixes and functionality to the driver as we expand our production usage of Go. . Thanks to the Go MySQL maintainers,  @methane  and  @julienschmidt , for their help reviewing and landing these patches! ", "date": "May 20, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing the CodeSearchNet challenge\t\t", "author": ["\n\t\tHamel Husain\t"], "link": "https://github.blog/2019-09-26-introducing-the-codesearchnet-challenge/", "abstract": "  Searching for code to reuse, call into, or to see how others handle a problem is one of the most common tasks in a software developer’s day. However, search engines for code are often frustrating and never fully understand what we want, unlike regular web search engines. We started using modern machine learning techniques to improve code search but quickly realized that we were unable to measure our progress. Unlike natural language processing with    GLUE    benchmarks, there is no standard dataset suitable for code search evaluation.  .  With our partners from    Weights &amp; Biases   , today we’re announcing the    CodeSearchNet Challenge    evaluation environment and    leaderboard   . We’re also releasing a large dataset to help data scientists build models for this task, as well as several baseline models showing the current state of the art. Our    leaderboard    uses an annotated dataset of queries to evaluate the quality of code search tools.  .   Learn   more from our technical report      .  We collected a large dataset of functions with associated documentation written in Go, Java, JavaScript, PHP, Python, and Ruby from open source projects on GitHub. We used our    TreeSitter    infrastructure for this effort, and we’re also releasing our    data preprocessing pipeline    for others to use as a starting point in applying machine learning to code. While this data is not directly related to code search, its pairing of code with related natural language description is suitable to train models for this task. Its substantial size also makes it possible to apply high-capacity models based on modern    Transformer    architectures.  .  Our fully preprocessed CodeSearchNet Corpus is available for    download on Amazon S3   , including:  .  Building on our earlier efforts in    semantic code search   , we’re also releasing a collection of    baseline models    leveraging modern techniques in learning from sequences (including a    BERT   -like self-attentional model) to help data scientists get started on code search.   .  To evaluate code search models, we collected an initial set of code search queries and had programmers annotate the relevance of potential results. We started by collecting common search queries from Bing that had high click-through rates to code and combined these with queries from    StaQC   , yielding 99 queries for concepts related to code (i.e., we removed everything that was just an API documentation lookup).  .  We then used a standard    Elasticsearch    installation and our baseline models to obtain 10 likely results per query from our CodeSearchNet Corpus. Finally, we asked programmers, data scientists, and machine learning researchers to annotate the proposed results for relevance to the query on a scale from zero (“totally irrelevant”) to three (“exact match”). See our    technical report    for an in-depth explanation of the annotation process and data.  .  We want to expand our evaluation dataset to include more languages, queries, and annotations in the future. As we continue adding more over the next few months, we aim to include an extended dataset for the next version of CodeSearchNet Challenge in the future.  .  We anticipate other use cases for this dataset beyond code search and are presenting code search as one possible task that leverages learned representations of natural language and code. We’re excited to see what the community builds next.  .  The CodeSearchNet Challenge would not be possible without the Microsoft Research Team and core contributors from GitHub, including    Marc Brockschmidt   ,    Miltos Allamanis   ,    Ho-Hsiang Wu   ,    Hamel Husain,    and    Tiferet Gazit   .  .  We’re also thankful for all of the contributors from the community who helped put this project together:  .   @nbardy   ,    @raubitsj   ,    @staceysv   ,    @cvphelps   ,    @tejaskannan   ,    @s-zanella   ,    @AntonioND   ,    @goutham7r   ,    @campoy   ,    @cal58   ,    @febuiles   ,    @letmaik   ,    @sebastiandziadzio   ,    @panthap2   ,    @CoderPat   .  .   Learn more about the CodeSearchNet Challenge   .  \t\t Tags:   \t\t insights \t ", "date": "September 26, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tNew workflow editor for GitHub Actions\t\t", "author": ["\n\t\tChris Patterson\t"], "link": "https://github.blog/2019-10-01-new-workflow-editor-for-github-actions/", "abstract": "  It’s now even easier to create and edit a GitHub Actions workflow with the updated editor. We’ve provided inline auto-complete and lint as you type so you can say goodbye to YAML indentation issues and explore the full workflow syntax without going to the docs.  .  Auto-complete can be triggered with Ctrl+Space almost anywhere, and in some cases, automatically. It suggests keys or values depending on the current position of the cursor, and displays brief contextual documentation so you can discover and understand all the available options without losing the focus. Auto-complete works even inside    expressions   .  .   .  Additionally, the editor will insert a new line with the right indentation when you press enter. It will also either suggest the next key to insert or insert code snippets that you can navigate through using the tab key.  .   .  Snippets also work when using functions inside expressions, allowing you to easily write and navigate through the required arguments.  .  There are many updates to help you write workflow files, but if you make a mistake, we’ve got you covered too.  .   .  The editor now highlights structural errors in your file, unexpected values, or even conflicting values, such as an invalid shell value for the chosen operating system.  .  Finally, the editor will also help you edit your scheduled jobs by describing the cron expressions you have used with natural language:  .   .  These are just a few features we’ve made to help you edit workflows with fewer errors. We’d love to hear any feedback you have—share your questions and comments with us in the    GitHub Actions Community Forum   .  .   Learn more about GitHub Actions   ", "date": "October 1, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tGetting started with Git and GitHub is easier than ever with GitHub Desktop 2.2\t\t", "author": ["\n\t\tAmanda Pinsker\t"], "link": "https://github.blog/2019-10-02-get-started-easier-with-github-desktop-2-2/", "abstract": "  Anyone who uses Git knows that it has a steep learning curve. We’ve learned from developers that most people tend to learn from a buddy, whether that’s a coworker, a professor, a friend, or even a YouTube video. In GitHub Desktop 2.2, we’re releasing the first version of an interactive Git and GitHub tutorial that can be your buddy and help you get started. If you’re new to Desktop, you can download and try out the tutorial at    desktop.github.com   .  .  To get set up, we help you through two major pieces: creating a repository and connecting an editor. When you first open Desktop, a welcome page appears with a new option to “Create a Tutorial Repository”. Starting with this option creates a tutorial repository that guides you through the core concepts of working with Git using GitHub Desktop.  .   .  There are a lot of tools you need to get started with Git and GitHub. The most important of these is your code editor. In the first step of the tutorial, you’re prompted to install an editor if you don’t have one already.  .   .  Next, we guide you through how to use GitHub Desktop to make changes to code locally and get your work on GitHub. You’ll create a new branch, make a change to a file, commit it, push it to GitHub, and open your first pull request.  .  We’ve also heard that new users initially experience confusion between Git, GitHub, and GitHub Desktop. We cover these differences in the tutorial and make sure to reinforce the explanations.  .   .  In GitHub Desktop 1.6, we introduced    suggested next steps    based on the state of your repository. Now when you complete the tutorial, we similarly suggest next steps: exploring projects on GitHub that you might want to contribute to, creating a new project, or adding an existing project to Desktop. We always want GitHub Desktop to be the tool that makes your next steps clear, whether you’re in the flow of your work, or you’re a new developer just getting started.  .   .  With GitHub Desktop 2.2, we’re making the product our users love more approachable to newcomers. We’ll be iterating on the tutorial based on your feedback, and we’ll continue to build on the connection between GitHub and your local machine. If you want to start building something but don’t know how, think of GitHub Desktop as your buddy to help you get started.  .   Learn more about GitHub Desktop   .  \t\t Tags:   \t\t GitHub Desktop \t ", "date": "October 2, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tDebugging network stalls on Kubernetes\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2019-11-21-debugging-network-stalls-on-kubernetes/", "abstract": " We’ve talked about  Kubernetes  before, and over the last couple of years it’s become the standard deployment pattern at GitHub. We now run a large portion of both internal and public-facing services on Kubernetes. As our Kubernetes clusters have grown, and our targets on the latency of our services have become more stringent, we began to notice that certain services running on Kubernetes in our environment were experiencing sporadic latency that couldn’t be attributed to the performance characteristics of the application itself. . Essentially, applications running on our Kubernetes clusters would observe seemingly random latency of up to and over 100ms on connections, which would cause downstream timeouts or retries. Services were expected to be able to respond to requests in well under 100ms, which wasn’t feasible when the connection itself was taking so long. Separately, we also observed very fast MySQL queries, which we expected to take a matter of milliseconds and that MySQL observed taking only milliseconds, were being observed taking 100ms or more from the perspective of the querying application. . The problem was initially narrowed down to communications that involved a Kubernetes node, even if the other side of a connection was outside Kubernetes. The most simple reproduction we had was a  Vegeta  benchmark that could be run from any internal host, targeting a Kubernetes service running on a node port, and would observe the sporadically high latency. In this post, we’ll walk through how we tracked down the underlying issue. . Using an example reproduction, we wanted to narrow down the problem and remove layers of complexity. Initially, there were too many moving parts in the flow between Vegeta and pods running on Kubernetes to determine if this was a deeper network problem, so we needed to rule some out. .   . The client, Vegeta, creates a TCP connection to any kube-node in the cluster. Kubernetes runs in our data centers as an  overlay network  (a network that runs on top of our existing datacenter network) that uses  IPIP  (which encapsulates the overlay network’s IP packet inside the datacenter’s IP packet). When a connection is made to that first kube-node, it performs stateful  Network Address Translation  (NAT) to convert the kube-node’s IP and port to an IP and port on the overlay network (specifically, of the pod running the application). On return, it undoes each of these steps. This is a complex system with a lot of state, and a lot of moving parts that are constantly updating and changing as services deploy and move around. . As part of running a  tcpdump  on the original Vegeta benchmark, we observed the latency during a TCP handshake (between SYN and SYN-ACK). To simplify some of the complexity of HTTP and Vegeta, we can use  hping3  to just “ping” with a SYN packet and see if we observe the latency in the response packet—then throw away the connection. We can filter it to only include packets over 100ms and get a simpler reproduction case than a full Layer 7 Vegeta benchmark or attack against the service. The following “pings” a kube-node using TCP SYN/SYN-ACK on the “node port” for the service (30927) with an interval of 10ms, filtered for slow responses: . Our first new observation from the sequence numbers and timings is that this isn’t a one-off, but is often grouped, like a backlog that eventually gets processed. . Next up, we want to narrow down which component(s) were potentially at fault. Is it the kube-proxy iptables NAT rules that are hundreds of rules long? Is it the IPIP tunnel and something on the network handling them poorly? One way to validate this is to test each step of the system. What happens if we remove the NAT and firewall logic and only use the IPIP part: .   . Linux thankfully lets you just talk directly to an overlay IP when you’re on a machine that’s part of the same network, so that’s pretty easy to do: . Based on our results, the problem still remains! That rules out iptables and NAT. Is it TCP that’s the problem? Let’s see what happens when we perform a normal ICMP ping: . Our results show that the problem still exists. Is it the IPIP tunnel that’s causing the problem? Let’s simplify things further: .   . Is it possible that it’s every packet between these two hosts? . Behind the complexity, it’s as simple as two kube-node hosts sending any packet, even ICMP pings, to each other. They’ll still see the latency, if the target host is a “bad” one (some are worse than others). . Now there’s one last thing to question: we clearly don’t observe this everywhere, so why is it just on kube-node servers? And does it occur when the kube-node is the sender or the receiver? Luckily, this is also pretty easy to narrow down by using a host outside Kubernetes as a sender, but with the same “known bad” target host (from a staff shell host to the same kube-node). We can observe this is still an issue in that direction: . And then perform the same from the previous source kube-node to a staff shell host (which rules out the source host, since a ping has both an RX and TX component): . Looking into the packet captures of the latency we observed, we get some more information. Specifically, that the “sender” host (bottom) observes this timeout while the “receiver” host (top) does not—see the Delta column (in seconds): .   . Additionally, by looking at the difference between the ordering of the packets (based on the sequence numbers) on the receiver side of the TCP and ICMP results above, we can observe that ICMP packets always arrive in the same sequence they were sent, but with uneven timing, while TCP packets are sometimes interleaved, but a subset of them stall. Notably, we observe that if you count the ports of the SYN packets, the ports are not in order on the receiver side, while they’re in order on the sender side. . There is a subtle difference between how modern server  NICs —like we have in our data centers—handle packets containing TCP vs ICMP. When a packet arrives, the NIC hashes the packet “per connection” and tries to divvy up the connections across receive queues, each (approximately) delegated to a given CPU core. For TCP, this hash includes both source and destination IP and port. In other words, each connection is hashed (potentially) differently. For ICMP, just the IP source and destination are hashed, since there are no ports. . Another new observation is that we can tell that ICMP observes stalls on all communications between the two hosts during this period from the sequence numbers in ICMP vs TCP, while TCP does not. This tells us that the RX queue hashing is likely in play, almost certainly indicating the stall is in processing RX packets, not in sending responses. . This rules out kube-node transmits, so we now know that it’s a stall in processing packets, and that it’s on the receive side on some kube-node servers. . To understand why the problem could be on the receiving side on some kube-node servers, let’s take a look at how the Linux kernel processes packets. . Going back to the simplest traditional implementation, the network card receives a packet and sends an  interrupt  to the Linux kernel stating that there’s a packet that should be handled. The kernel stops other work, switches context to the interrupt handler, processes the packet, then switches back to what it was doing. .   . This context switching is slow, which may have been fine on a 10Mbit NIC in the 90s, but on modern servers where the NIC is 10G and at maximal line rate can bring in around 15 million packets per second, on a smaller server with eight cores that could mean the kernel is interrupted millions of times per second per core. . Instead of constantly handling interrupts, many years ago Linux added  NAPI , the networking API that modern drivers use for improved performance at high packet rates. At low rates, the kernel still accepts interrupts from the NIC in the method we mentioned. Once enough packets arrive and cross a threshold, it disables interrupts and instead begins polling the NIC and pulling off packets in batches. This processing is done in a “softirq”, or  software interrupt context . This happens at the end of syscalls and hardware interrupts, which are times that the kernel (as opposed to userspace) is already running. .   . This is much faster, but brings up another problem. What happens if we have so many packets to process that we spend all our time processing packets from the NIC, but we never have time to let the userspace processes actually drain those queues (read from TCP connections, etc.)? Eventually the queues would fill up, and we’d start dropping packets. To try and make this fair, the kernel limits the amount of packets processed in a given softirq context to a certain budget. Once this budget is exceeded, it wakes up a separate thread called  ksoftirqd  (you’ll see one of these in  ps  for each core) which processes these softirqs outside of the normal syscall/interrupt path. This thread is scheduled using the standard process scheduler, which already tries to be fair. .   . With an overview of the way the kernel is processing packets, we can see there is definitely opportunity for this processing to become stalled. If the time between softirq processing calls grows, packets could sit in the NIC RX queue for a while before being processed. This could be something deadlocking the CPU core, or it could be something slow preventing the kernel from running softirqs. . At this point, it makes sense that this could happen, and we know we’re observing something that looks a lot like it. The next step is to confirm this theory, and if we do, understand what’s causing it. . Let’s revisit the slow round trip packets we saw before: . As discussed previously, these ICMP packets are hashed to a single NIC RX queue and processed by a single CPU core. If we want to understand what the kernel is doing, it’s helpful to know where (cpu core) and how (softirq, ksoftirqd) it’s processing these packets so we can catch them in action. . Now it’s time to use the tools that allow live tracing of a running Linux kernel— bcc  is what was used here. This allows you to write small C programs that hook arbitrary functions in the kernel, and buffer events back to a userspace Python program which can summarize and return them to you. The “hook arbitrary functions in the kernel” is the difficult part, but it actually goes out of its way to be as safe as possible to use, because it’s designed for tracing exactly this type of production issue that you can’t simply reproduce in a testing or dev environment. . The plan here is simple: we know the kernel is processing those ICMP ping packets, so let’s hook the kernel function  icmp_echo  which takes an incoming ICMP “echo request” packet and initiates sending the ICMP “echo response” reply. We can identify the packet using the incrementing icmp_seq shown by  hping3  above. . The code for this  bcc script  looks complex, but breaking it down it’s not as scary as it sounds. The  icmp_echo  function is passed a  struct sk_buff *skb , which is the packet containing the ICMP echo request. We can delve into this live and pull out the  echo.sequence  (which maps to the  icmp_seq  shown by  hping3  above), and send that back to userspace. Conveniently, we can also grab the current process name/id as well. This gives us results like the following, live as the kernel processes these packets: . One thing to note about this process name is that in a post-syscall  softirq  context, you see the process that made the syscall show as the “process”, even though really it’s the kernel processing it safely within the kernel context. . With that running, we can now correlate back from the stalled packets observed with  hping3  to the process that’s handling it. A simple  grep  on that capture for the  icmp_seq  values with some context shows what happened before these packets were processed. The packets that line up with the above  hping3  icmp_seq values have been marked along with the rtt’s we observed above (and what we’d have expected if &lt;50ms rtt’s weren’t filtered out): . The results tells us a few things. First, these packets are being processed by  ksoftirqd/11  which conveniently tells us this particular pair of machines have their ICMP packets hashed to core 11 on the receiving side. We can also see that every time we see a stall, we always see some packets processed in  cadvisor ’s syscall softirq context, followed by  ksoftirqd  taking over and processing the backlog, exactly the number we’d expect to work through the backlog. . The fact that  cadvisor  is always running just prior to this immediately also implicates it in the problem. Ironically,  cadvisor  “analyzes resource usage and performance characteristics of running containers”, yet it’s triggering this performance problem. As with many things related to containers, it’s all relatively bleeding-edge tooling which can result in some somewhat expected corner cases of bad performance. . With the understanding of how the stall can happen, the process causing it, and the CPU core it’s happening on, we now have a pretty good idea of what this looks like. For the kernel to hard block and not schedule  ksoftirqd  earlier, and given we see packets processed under  cadvisor ’s softirq context, it’s likely that  cadvisor  is running a slow syscall which ends with the rest of the packets being processed: .   . That’s a theory but how do we validate this is actually happening? One thing we can do is trace what’s running on the CPU core throughout this process, catch the point where the packets are overflowing budget and processed by ksoftirqd, then look back a bit to see what was running on the CPU core. Think of it like taking an x-ray of the CPU every few milliseconds. It would look something like this: .   . Conveniently, this is something that’s already mostly supported. The   perf record   tool samples a given CPU core at a certain frequency and can generate a call graph of the live system, including both userspace and the kernel. Taking that recording and manipulating it using a quick fork of a tool from  Brendan Gregg’s FlameGraph  that retained stack trace ordering, we can get a one-line stack trace for each 1ms sample, then get a sample of the 100ms before  ksoftirqd  is in the trace: . This results in the following: . There’s a lot there, but looking through it you can see it’s the cadvisor-then-ksoftirqd pattern we saw from the ICMP tracer above. What does it mean? . Each line is a trace of the CPU at a point in time. Each call down the stack is separated by  ;  on that line. Looking at the middle of the lines we can see the syscall being called is  read(): .... ;do_syscall_64;sys_read; ...  So cadvisor is spending a lot of time in a  read()  syscall relating to  mem_cgroup_*  functions (the top of the call stack / end of line). . The call stack trace isn’t convenient to see what’s being read, so let’s use  strace  to see what cadvisor is doing and find 100ms-or-slower syscalls: . Sure enough, we see the slow  read()  calls. From the content being read and  mem_cgroup  context above, these  read()  calls are to a  memory.stat  file which shows the memory usage and limits of a cgroup (the resource isolation technology used by Docker). cadvisor is polling this file to get resource utilization details for the containers. Let’s see if it’s the kernel or cadvisor that’s doing something unexpected by attempting the read ourselves: . Since we can reproduce it, this indicates that it’s the kernel hitting a pathologically bad case. . At this point it’s much more simple to find similar issues reported by others. As it turns out, this has been reported to cadvisor as an  excessive CPU usage problem , it just hadn’t been observed that latency was also being introduced to the network stack randomly as well. In fact, some folks internally had noticed cadvisor was consuming more CPU than expected, but it didn’t seem to be causing an issue since our servers had plenty of CPU capacity, and so the CPU usage hadn’t yet been investigated. . The overview of the issue is that the memory cgroup is accounting for memory usage inside a namespace (container). When all processes in that cgroup exit, the memory cgroup is released by Docker. However, “memory” isn’t just process memory, and although processes memory usage itself is gone, it turns out the kernel also assigns cached content like dentries and inodes (directory and file metadata) that are cached to the memory cgroup. From that issue: . “zombie” cgroups: cgroups that have no processes and have been deleted but still have memory charged to them (in my case, from the dentry cache, but it could also be from page cache or tmpfs). . Rather than the kernel iterating over every page in the cache at cgroup release time, which could be very slow, they choose to wait for those pages to be reclaimed and then finally clean up the cgroup once all are reclaimed when memory is needed, lazily. In the meantime, the cgroup still needs to be counted during stats collection. . From a performance perspective, they are trading off time on a slow process by amortizing it over the reclamation of each page, opting to make the initial cleanup fast in return for leaving some cached memory around. That’s fine, when the kernel reclaims the last of the cached memory, the cgroup eventually gets cleaned up, so it’s not really a “leak”. Unfortunately the search that  memory.stat  performs, the way it’s implemented on the kernel version (4.9) we’re running on some servers, combined with the huge amount of memory on our servers, means it can take a significantly long time for the last of the cached data to be reclaimed and for the zombie cgroup to be cleaned up. . It turns out we had nodes that had such a large number of zombie cgroups that some had reads/stalls of over a second. . The workaround on that cadvisor issue, to immediately free the dentries/inodes cache systemwide, immediately stopped the read latency, and also the network latency stalls on the host, since the dropping of the cache included the cached pages in the “zombie” cgroups and so they were also freed. This isn’t a solution, but it does validate the cause of the issue. . As it turns out newer kernel releases (4.19+) have improved the performance of the  memory.stat  call and so this is no longer a problem after moving to that kernel. In the interim, we had existing tooling that was able to detect problems with nodes in our Kubernetes clusters and gracefully drain and reboot them, which we used to detect the cases of high enough latency that would cause issues, and treat them with a graceful reboot. This gave us breathing room while OS and kernel upgrades were rolled out to the remainder of the fleet. . Since this problem manifested as NIC RX queues not being processed for hundreds of milliseconds, it was responsible for both high latency on short connections and latency observed mid-connection such as between MySQL query and response packets. Understanding and maintaining performance of our most foundational systems like Kubernetes is critical to the reliability and speed of all services that build on top of them. As we invest in and improve on this performance, every system we run benefits from those improvements. ", "date": "November 21, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tBehind the scenes: GitHub security alerts\t\t", "author": ["\n\t\tJustin Hutchings\t"], "link": "https://github.blog/2019-12-11-behind-the-scenes-github-vulnerability-alerts/", "abstract": "  If you have code on GitHub, chances are that you’ve had a security vulnerability alert at some point. Since the feature launched, GitHub has sent more than 62 million security alerts for vulnerable dependencies.   .  Vulnerability alerts rely on two pieces of data: an inventory of all the software that your code depends on, and a curated list of known vulnerabilities in open-source code.   .  Any time you push a change to a dependency manifest file, GitHub has a job that parses those manifest files, and stores your dependency on those packages in the dependency graph. If you’re dependent on something that hasn’t been seen before, a background task runs to get more information about the package from the package registries themselves and adds it. We use the information from the package registries to establish the canonical repository that the package came from, and to help populate metadata like readmes, known versions, and the published licenses.   .  On GitHub Enterprise Server, this process works identically, except we don’t get any information from the public package registries in order to protect the privacy of the server and its code.   .  The dependency graph supports manifests for JavaScript (npm, Yarn), .NET (Nuget), Java (Maven), PHP (Composer), Python (PyPI), and Ruby (Rubygems). This data powers our vulnerability alerts, but also dependency insights, the used by badge, and the community contributors experiences.   .  Beyond the dependency graph, we aggregate data from a number of sources and curate those to bring you actionable security alerts. GitHub brings in security vulnerability data from a number of sources, including the    National Vulnerability Database    (a service of the United States National Institute of Standards and Technology), maintainer security advisories from open-source maintainers, community datasources, and our partner    WhiteSource   .   .  Once we learn about a vulnerability, it passes through an advanced machine learning model that’s trained to recognize vulnerabilities which impact developers. This model rejects anything that isn’t related to an open-source toolchain. If the model accepts the vulnerability, a bot creates a pull request in a GitHub private repository for our  team of curation experts to manually review.  .   .  GitHub curates vulnerabilities because CVEs (Common Vulnerability Entries) are often ambiguous about which open-source projects are impacted. This can be particularly challenging when multiple libraries with similar names exist, or when they’re a part of a larger toolkit. Depending on the kind of vulnerability, our curation team may follow-up with outside security researchers or maintainers about the impact assessment. This follow-up helps to confirm that an alert is warranted and to identify the exact packages that are impacted.   .  Once the curation team completes the mappings, we merge the pull request and it starts a background job that notifies users about any affected repositories. Depending on the vulnerability, this can cause a lot of alerts. In a recent incident, more than two million repositories were alerted about a vulnerable version of lodash, a popular JavaScript utility library.  .  GitHub Enterprise Server customers get a slightly different experience. If an admin has enabled security vulnerability alerts through GitHub Connect, the server will download the latest curated list of vulnerabilities from GitHub.com over the private GitHub Connect channel on its next scheduled sync (about once per hour). If a new vulnerability exists, the server determines the impacted users and repositories before generating alerts directly.   .  Security vulnerabilities are a matter of public good. High-profile breaches impact the trustworthiness of the entire tech industry, so we publish a curated set of vulnerabilities on our    GraphQL APIs    for community projects and enterprise tools to use in custom workflows as necessary. Users can also browse the known vulnerabilities from public sources on the    GitHub Advisory Database   .  .  Despite advanced technology, security alerting is a human process driven by dedicated GitHubbers. Meet Rob (   @rschultheis   )  , one of the core members of our security team, and learn about his experiences at GitHub through a friendly Q&amp;A:  .  How long have you been with GitHub?   .  Two years  .  How did you get into software security?   .  I’ve worked with open source software for most of my 20 year career in tech, and honestly for much of that time I didn’t pay much attention to security. When I started at GitHub I was given the opportunity to work on the first iteration of security alerts. It quickly became clear that having a high quality, open dataset was going to be a critical factor in the success of the feature. I dove into the task of curating that advisory dataset and found a whole side to the industry that was open for exploration, and I’ve stayed with it ever since!  .  What are the trickiest parts of vulnerability curation?   .  The hardest problem is probably confirming that our advisory data correctly identifies which version(s) of a package are vulnerable to a given advisory, and which version(s) first address it.  .  What was the most difficult security vulnerability you’ve had to publish?   .  One memorable vulnerability was    CVE-2015-9284   . This one was tough in several ways because it was a part of a popular library, it was also unpatched when it became fully public, and finally, it was published four years after the initial disclosure to maintainers. Even worse, all attempts to fix it had stalled.  .  We ended up proceeding to publish it and the community quickly responded and finally got the security issue patched.  .  What’s your favorite feel-good moment working in security?   .  Seeing tweets and other feedback thanking us is always wonderful. We do read them! And that goes the same for those critical of the feature or the way certain advisories were disclosed or published. Please keep them coming—they’re really valuable to us as we keep evolving our security offerings.  .  Since you work at home, can you introduce us to your furry officemate?   .  I live with a seven month old shepherd named Humphrey Dogart. His primary responsibilities are making sure I don’t spend all day on the computer, and he does a great job of that. I think we make a great team!  .  Learn more about GitHub security alerts  ", "date": "December 11, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we built the good first issues feature\t\t", "author": ["\n\t\tTiferet Gazit\t"], "link": "https://github.blog/2020-01-22-how-we-built-good-first-issues/", "abstract": "  GitHub is leveraging machine learning (ML) to help more people contribute to open source. We’ve launched the good first issues feature, powered by deep learning, to help new contributors find easy issues they can tackle in projects that fit their interests.  .  If you want to start contributing, or to attract new contributors to a project you maintain, get started with our  overview of the good first issues feature . Read on to learn about how we leverage machine learning to detect easy issues.  .   .  In May 2019, our initial launch surfaced recommendations based on labels that were applied to issues by project maintainers. Some analysis of our data, together with manual curation, led to a list of about 300 label names used by popular open source repositories—all synonyms for either “good first issue” or “documentation”. Some examples include “beginner friendly”, “easy bug fix”, and “low-hanging-fruit”. We include documentation issues because they’re often a good way for new people to begin contributing, but rank them lower relative to issues specifically tagged as easy for beginners.  .  Relying on these labels, however, means that only about 40 percent of the repositories we recommend have easy issues we can surface. Moreover, it leaves maintainers with the burden of triaging and labeling issues. Instead of relying on maintainers to manually label their issues, we wanted to use machine learning to broaden the set of issues we could surface.  .  Last month, we shipped an updated version, that includes both label-based and ML-based issue recommendations. With this new version, we’re able to surface issues in about 70 percent of repositories we recommend to users. There is a tradeoff between coverage and accuracy, which is the typical precision and recall tradeoff found in any ML product. To prevent the feed from being swamped with false positive detections, we aim for extremely high precision at the cost of recall. This is necessary because only a tiny minority of all issues are good first issues. The exact numbers are constantly evolving, both as we improve our training data and modeling, and as we adjust the precision and recall tradeoff based on feedback and user behavior.  .  As with any supervised machine learning project, the first challenge is building a labeled training set. In this case, manual labeling is a difficult task that requires domain-area expertise in a range of topics, projects, and programming languages. Instead, we’ve opted for a weakly-supervised approach, inferring labels for hundreds of thousands of candidate samples automatically.  .  To detect positive training samples, we begin with issues that have any of the roughly-300 labels in our curated list, but this training set is not sufficiently large or diverse for our needs. We supplement it with a few sets of issues that are also likely to be beginner-friendly. This includes issues that were closed by a pull request from a user who had never previously contributed to the repository, and issues that were closed by a pull request that touched only a few lines in a single file. Because we prefer to miss some good issues than to swamp the feed with false positives, we define all issues that were not explicitly detected as positive samples to be negative samples in our training set. Naturally, this gives us a hugely imbalanced set, so we subsample the negative set in addition to weighting the loss function. We detect and remove near-duplicate issues, and separate the training, validation, and test sets across repositories to prevent data leakage from similar content.  .  In order for our classifiers to detect good issues as soon as they’re opened, we train them using only issue titles and bodies, and avoid relying on additional signals such as conversations. The titles and bodies undergo some preprocessing and denoising, such as removing parts of the body that are likely to come from an issue template, since they carry no signal for our problem. We’ve experimented with a range of model variants and architectures, including both classical ML methods such as    random forests    using    tf-idf    input vectors and neural network models such as 1D    convolutional neural networks    and    recurrent neural networks   . The deep learning models are implemented in TensorFlow, and use one-hot encodings of issue titles and bodies, fed into trainable embedding layers, as separate inputs to networks, with features from the two inputs concatenated towards the top of the network. Unsurprisingly, these networks generally outperform the classical methods using tf-idf, because they can use the full information contained in word ordering, context, and sentence structure, rather than just information on the unordered relative count of words in the vocabulary. Because both the training and inference happen offline, the added cost of deep learning methods is not prohibitive in this case. However, given the limited size of the positive training set, we’ve found various textual data augmentation techniques to be crucial to the success of these networks, in addition to regularization and early stopping.   .  To surface issue recommendations given a trained classifier, we run inference on all qualifying open issues from non-archived public repositories. Each issue for which the classifier predicts a probability above the required threshold is slated for recommendation, with a confidence score equal to its predicted probability. We also detect all open issues from non-archived public repositories that have at least one of the labels from the curated label list. These issues are given a confidence score based on the relevance of their labels, with synonyms of “good first issue” given higher confidence than synonyms of “documentation”. In general, label-based detections are given higher confidence than ML-based detections. Within each repository, all detected issues are then ranked primarily based on their confidence score, along with a penalty on issue age.  .  Our data acquisition, training, and inference pipelines run daily, using scheduled    Argo    workflows, to ensure our results remain fresh and relevant. As the first deep-learning-enabled product to launch on Github.com, this feature required careful design to ensure that the infrastructure would generalize to future projects.  .  We continue to iterate on the training data, training pipeline, and classifier models to improve the surfaced issue recommendations. In parallel, we’re adding better signals to our repository recommendations to help users find and get involved with the best projects related to their interests. We also plan to add a mechanism for maintainers and triagers to approve or remove ML-based recommendations in their repositories. Finally, we plan on extending issue recommendations to offer personalized suggestions on next issues to tackle for anyone who has already made contributions to a project.  .  We hope you find these issue recommendations useful.  .   Learn more about good first issues   ", "date": "January 22, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tSupercharge your command line experience: GitHub CLI is now in beta\t\t", "author": ["\n\t\tBilly Griffin\t"], "link": "https://github.blog/2020-02-12-supercharge-your-command-line-experience-github-cli-is-now-in-beta/", "abstract": " We’re introducing an easier and more seamless way to work with GitHub from the command line— GitHub CLI , now in beta. Millions of developers rely on GitHub to make building software more fun and collaborative, and  gh  brings the GitHub experience right to your terminal. . You can install GitHub CLI today on  macOS, Windows, and Linux , and there’s more to come as we iterate on your feedback from the beta. It’s available today for GitHub Team and Enterprise Cloud, but not yet available for GitHub Enterprise Server. We’ll be exploring support for Enterprise Server when it’s out of beta. . We started with issues and pull requests because many developers use them every day. Check out a few examples of how  gh  can improve your experience when contributing to an open source project and learn more from the  manual . . Find an open source project you want to contribute to and clone the repository. And then, to see where maintainers want community contributions, use  gh  to filter the issues to only show those with help wanted labels. .   . Find an issue describing a bug that seems like something you can fix, and use  gh  to quickly open it in the browser to get all the details you need to get started. .   . Create a branch, make several commits to fix the bug described in the issue, and use  gh  to create a pull request to share your contribution. .   . By using GitHub CLI to create pull requests, it also automatically creates a fork when you don’t already have one, and it pushes your branch and creates your pull request to get your change merged. . Get a quick snapshot the next morning of what happened since you created your pull request.  gh  shows the review and check status of your pull requests. .   . One of the maintainers reviewed your pull request and requested changes. You probably switched branches since then, so use  gh  to checkout the pull request branch. We never remember the right commands either! .   . Make the changes, push them, and soon enough the pull request is merged—congratulations! . We hope you’ll love the foundation we’ve built with pull requests and issues. And we’re even more excited about the future as we explore what it looks like to build a truly delightful experience with GitHub on the command line. As GitHub CLI continues to make it even more seamless to contribute to projects on GitHub, the  sky’s the limit  on what we can achieve together. . We can’t wait to hear about your experience with GitHub CLI, and we’d love your feedback. Create an issue in our  open source repository  or provide feedback in our  google form . What commands feel like you can’t live without them? What’s clunky or missing? Let us know so we can make GitHub CLI even better. .  Learn more about GitHub CLI beta  ", "date": "February 12, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tAutomating MySQL schema migrations with GitHub Actions and more\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2020-02-14-automating-mysql-schema-migrations-with-github-actions-and-more/", "abstract": " In the past year, GitHub engineers shipped GitHub Packages, Actions, Sponsors, Mobile, security advisories and updates, notifications, code navigation, and more. Needless to say, the development pace at GitHub is accelerated. . With MySQL serving our backends, updating code requires changes to the underlying database schema. New features may require new tables, columns, changes to existing columns or indexes, dropping unused tables, and so on. On average, we have two schema migrations running daily on our production servers. Some days we have a half dozen migrations to run. We’ll cover how this amounted to a significant toil on the database infrastructure team, and how we searched for a solution to automate the manual parts of the process. . At first glance, migrating appears to be no more difficult than adding a  CREATE, ALTER  or  DROP TABLE  statement. At a closer look, the process is far more complex, and involves multiple owners, platforms, environments, and transitions between those pieces. Here’s the flow as we experience it at GitHub: . It begins with a developer who identifies the need for a schema change. Maybe they need a new table, or a new column in an existing table. The developer has a local testing environment where they can experiment however they like, until they’re satisfied and wish to apply changes to production. . The developer doesn’t just apply their changes online. First, they seek review and discussion with their peers. Depending on the change, they may ask for a review from a group of schema reviewers (at GitHub, this is a volunteer group experienced with database design). Then, they seek the agreement of the database infrastructure team, who owns the production databases. The database infrastructure team reviews the changes, looking for performance concerns, among other potential issues. Assuming all reviews are favorable, it’s on the database infrastructure engineer to deploy the change to production. . At this point, we need to determine  where  the change is taking place since we have multiple clusters. Some of them are sharded, so we have to ask: Where do the affected tables exist in our clusters or schemas? Next, we need to know  what  to run. The developer presented the schema they want to see in production, but how do we transition the existing production schema into the one requested? What’s the formal  CREATE, ALTER  or  DROP  statement? Following what to run, we need to know  how  we should run the migration. Do we run the query directly? Or is it a blocking operation and we need an online schema change tool? And finally, we need to know  when  to execute the migration. Perhaps now is not a good time if there’s already a migration running on the cluster. . At long last, we’re ready to run the migration. Some of our larger tables may take hours and even days to migrate, especially since the site needs to be up and running. We want to track status. And we want to see what impact the migration may have on production, or, preferably, to ensure it  does not  have an impact. . Even as the migration completes there are further steps to take. There’s some cleanup process, and we want to unblock the next migration, if any currently exists. The database infrastructure team wishes to advertise to the developer that the changes have taken place, and the developer will have their own followup to address. . Throughout that flow, there’s a lot of potential for friction: . The database infrastructure engineer needs to either create or review the migration statement, double-check their logic, ensure they can begin the migration, follow up, unblock other migrations as needed, advertise progress to the developer, and so on. . With our volume of daily migrations, this flow sometimes consumed hours of work of a database infrastructure engineer per day, and—in the best-case scenario—at least several hours of work per week. They would frequently multitask between two or three migrations and keep mental notes for next steps. Developers would ping us to ask what the status was, and their work was sometimes blocked until the migration was complete. . GitHub was originally created as a Ruby on Rails (RoR) app. Like other frameworks, and in particular, those using Active Record, RoR has a built-in mechanism to generate database schema from code, as well as programmatically express  migrations . RoR tooling can analyze code changes and create and run the SQL statements to change the database schema. . We use the GitHub flow to manage our own development: when suggesting a change, we create a branch, commit, push, and open a pull request. We use the declarative approach to schema definition: our RoR GitHub repository contains the full schema definition, such as the  CREATE TABLE  statements that generate the complete schema. This way, we know exactly what schema is associated with each commit or branch. Counter that with the programmatic approach, where your commits contain migration statements, and where to deduce a schema you need to start at some baseline and run through all statements sequentially. . The database infrastructure and the application teams collaborated to create a set of chatops tooling. We ran a chatops command to list pull requests with schema changes, and then another command to generate the  CREATE/ALTER/DROP  statement for a given pull request. For this, we used RoR’s  rake  command. Our wrapper scripts then added meta information, like which cluster is involved, and generated a script used to run the migration. . The generated statements and script were mostly fine, with occasional SQL syntax errors. We’d review the output and fix it manually as needed. . A few years ago we developed  gh-ost , an online table migration solution, which added even more visibility and control through our chatops. We’d check progress, change runtime configuration, and cut-over the migration through chat. While simple, these were still manual steps. . The heart of GitHub’s app remains with the same RoR, but we’ve expanded far beyond it. We created more repositories and some also use RoR, while others are in other programming languages such as Go. However, we didn’t use Object Relational Mapping practice with the new repositories. . As GitHub expanded, the more toil the database infrastructure team had. We’d review pull requests, compare schemas, generate migration statements manually, and verify on a local machine. Other than the git log, no formal tracking for schema migrations existed. We’d check in chat, issues, and pull requests to see what was done and what wasn’t. We’d keep track of ongoing migrations in our heads, context switch between the migrations throughout the day, and how often we’d get interrupted by notifications. And we did this while taking each migration through the next step, keeping mental notes, and communicating the progress to our peers. . With these steps in mind, we wanted a solution to automate the process. We came up with various ideas, and in 2019 GitHub Actions was released. This was our solution: multiple loosely coupled components, each owning a specific aspect of the flow, all orchestrated by a controller service. The next section covers the breakdown of our solution. . Our basic premise is that schema design should be treated as code. We want the schema to be versioned, and we want to know what schema is associated and with what version of our code. . To illustrate, GitHub provides not only github.com, but also GitHub Enterprise, an on-premise solution. On github.com we run continuous deployments. With GitHub Enterprise, we make periodic releases, and our customers can upgrade in-house. This means we need to be able to reproduce any schema changes we make to github.com on a customer’s Enterprise server. . Therefore we must keep our schema design coupled with the code in the same git repository. For a developer to design a schema change, they need to follow our normal development flow: create a branch, commit, push, and open a pull request. The pull request is where code is reviewed and discussion takes place for any changes. It’s where continuous integration and testing run. Our solution revolves around the pull request, and this is standardized across all our repositories. . Once a pull request is opened, we need to be able to identify what changes we’d like to make. Typically, when we review code changes, we look at the  diff . And it might be tempting to expect that  git diff  can help us formalize the schema change. Unfortunately, this is not the case, and  git diff  is poor at identifying these changes. For example, consider this simplified table definition: . Suppose we decide to add a new column and drop the index on hostname. The new schema becomes: . Running  git diff  on the two schemas yields the following: . The pull request’s “Files changed” tab shows the same: .   . See how the  PRIMARY KEY  line goes into the diff because of the trailing comma. This diff does not capture the schema change well, and while RoR provides tooling for that,  we’ve still had to carefully review them. Fortunately, there’s a good MySQL-oriented tool to do the task. .  skeema  is an open source schema management utility developed by  Evan Elias . It expects the declarative approach, and looks for a schema definition on your file system (hopefully as part of your repository). The file system layout should include a directory per schema/database, a file per table, and then some special configuration files telling skeema the identities of, and the credentials for, MySQL servers in various environments. Skeema is able to run useful tasks, such as: . skeema can do much more, including the ability to invoke online schema change tools—but that’s outside this post’s scope. .  Git  users will feel comfortable with skeema. Indeed, skeema works very well with git-versioned schemas. For us, the most valuable asset is its  diff  output: a well formed, reliable set of statements to show the SQL transition from one schema to another. For example,  skeema diff  output for the above schema change is: . Note that the above is not only  correct , but also  formal . It reproduces correctly whether our code uses lower/upper case, includes/omits default value, etc. . We wanted to use skeema to tell us what statements we needed to run to get from our existing state into the state defined in the pull request. Assuming the  master  branch reflects our current production schema, this now becomes a matter of  diffing  the schemas between  master  and the pull request’s branch. . Skeema wasn’t without its challenges, and we had to figure out where to place skeema from a design perspective. Do the developers own it? Does every repository own it? Is there a central service to own it? Each presented its own problems, from false ownership to excessive responsibilities and access. . Enter  GitHub Actions . With Actions, you’re able to run code as a response to events taking place in your repository. A new pull request, review, comment, issue, and quite a few others, are such events. The code (the  action ) is arbitrary, and GitHub spawns a container on its own infrastructure, where your code will run. What makes this extra interesting is that the container can get access to your repository. GitHub Actions implicitly receives an API token to interact with the repository. . The container comes with popular software packages pre-installed, such as a MySQL server. . Perhaps the most classic use of Actions is  CI/CD .  When  a  pull_request  event occurs (a new pull request and any subsequent commit) run some code to build, test, lint, or validate the change. We took this approach to run skeema as part of a  pull_request  action flow, called  skeema-diff . . Here’s a simplified breakdown of the action: . Fetch skeema binary . Checkout master branch . Run  skeema push  to populate the container’s MySQL server with the schema as defined by the master branch . Checkout pull request’s branch . Run  skeema diff  to generate the statements that take the schema from the one in MySQL (remember, this is the master schema) to the one in the pull request’s branch . Add the diff as a comment in the pull request . Add a special label to indicate this pull request has a schema change .   . The code is more complex than what we’ve shown. We actually use  base  and  head  instead of  master  and  branch , and there’s some logic to formalize, edit and validate the diff, to handle commits that further change the schema, among other processes. . By now, we have a partial flow, which works entirely on GitHub’s platform: . Up to this point, everything is constrained to the repository. The repository itself doesn’t have information about where the schema gets deployed in production. This information is something that’s outside the repository’s scope, and it’s owned by the database infrastructure team rather than the repository’s developers. Neither the repository nor any action running on that repository has access to production, nor should they, as that would be a breach of domains. . Before we describe how the schema gets to production, let’s jump ahead and discuss the schema migration itself. . Even the simplest schema migration isn’t simple. We are concerned with three types of table migrations: . In order to run a migration, we must first determine the strategy for that migration (Is it direct query, gh-ost, or a manual?). We need to be able to tell where it can run,  how to go about the process if the cluster is sharded, as well as When to schedule it. While migrations can wait in queue while others are running, we want to be able to prioritize migrations, in case the queue is large. . We created  skeefree  as the glue, which means it’s an orchestrating service that’s aware of our repositories, can communicate with our pull requests, knows about production (or, can get information about production) and which invokes the migrations. We run skeefree as a stateless kubernetes service, backed by a MySQL database that holds the state. Note that skeefree’s own schema is managed by skeefree. . skeefree uses GitHub’s API to interact with pull requests, GitHub’s internal inventory and discovery services, to locate clusters in production, and gh-ost to run migrations. Skeefree is best described by following a schema migration flow: . A developer wishes to change the schema, so they open a pull request. . skeema-diff Action springs to life and seeks a schema change. If a schema change isn’t found in the pull request, nothing happens. If there is a schema change, the Action, computes the change via skeema, adds a well-formed comment to the pull request indicating the change, and adds a  migration:skeema:diff  label to the pull request. This is done via the GitHub API. . A developer looks into the change, and seeks review from a team member. At this time they may communicate to team members without actually going to production. Finally, they add the label  migration:for:review . .  skeefree  is aware of the developer’s repository and uses the GitHub API to periodically look for open pull requests, which are labeled by both  migration:skeema:diff  and  migration:for:review , and have been approved by at least one developer. . Once detected, skeefree investigates the pull request, and reads the schema change comment, generated by the Action. It maps the schema/repository to the schema/production cluster, and uses our inventory and discovery services to know if the cluster is sharded. Then, it finds the location and name of the cluster. . skeefree then adds this to its backend database, and advertises its analysis on the pull request with another comment. This comment generally means “here’s what I will do if you approve”. And it proceeds to get a review from an authority.   . For most repositories, the authority is the database-infrastructure team. On our original RoR repository, we also seek review from a cross-functional team, known as the db-schema-reviewers, who are familiar with the general application and database design throughout the years and who have more context to offer. skeefree automatically knows which teams should be notified on which repositories. . The relevant teams review and hopefully approve, and skeefree detects the approval, before choosing the proper strategy (direct query for  CREATE  and  DROP , or  RENAME ), and gh-ost for  ALTER . It then queues the migration(s). . skeefree’s scheduler periodically checks what next can be executed. Remember we only run a single  ALTER  migration on a given cluster at a time, but we also have a limited number of runner hosts. If there’s a free runner host and the cluster is not running any migration, skeefree then proceeds to kick off a migration. Skeefree advertises this fact as a pull request comment to notify the developer that the migration started. . Once the migration is complete, skeefree announces it in a pull request comment. The same applies should the migration fail. . The pull request may also have more than one migration. Perhaps the cluster is sharded, or there may be multiple tables changed in the pull request. Once all migrations are successfully completed, skeefree advertises this in a pull request comment. The developer is notified that all migrations are done, and they’re encouraged to proceed with their standard deploy/merge flow. .   . There are a few nuances here that make a good experience to everyone involved: . skeefree and the skeema-diff Action were authored internally at GitHub to solve a specific problem. skeefree uses our internal inventory and discovery services, it works with our chatops and uses some internal libraries. . Our experience in releasing open source software is that no one’s use case is exactly the same as ours. Our perception of an automated migrations flow may be very different from another organization’s perception. We still want to share more than just our words, so we’ve open sourced the code. . It’s a bit of a peculiar OSS release: . Note that the code is available, but not open for issues and pull requests. We hope the community finds it useful. .  Get the code  .  \t\t Tags:   \t\t insights \t ", "date": "February 14, 2020"},
{"website": "Github-Engineering", "title": "\n\t\t\tOctober 21 post-incident analysis\t\t", "author": ["\n\t\tJason Warner\t"], "link": "https://github.blog/2018-10-30-oct21-post-incident-analysis/", "abstract": " Last week, GitHub experienced  an incident  that resulted in degraded service for 24 hours and 11 minutes. While portions of our platform were not affected by this incident, multiple internal systems were affected which resulted in our displaying of information that was out of date and inconsistent. Ultimately, no user data was lost; however manual reconciliation for a few seconds of database writes is still in progress. For the majority of the incident, GitHub was also unable to serve webhook events or build and publish GitHub Pages sites. . All of us at GitHub would like to sincerely apologize for the impact this caused to each and every one of you. We’re aware of the trust you place in GitHub and take pride in building resilient systems that enable our platform to remain highly available. With this incident, we failed you, and we are deeply sorry. While we cannot undo the problems that were created by GitHub’s platform being unusable for an extended period of time, we can explain the events that led to this incident, the lessons we’ve learned, and the steps we’re taking as a company to better ensure this doesn’t happen again. . The majority of user-facing GitHub services are run within our own  data center facilities . The data center topology is designed to provide a robust and expandable edge network that operates in front of several regional data centers that power our compute and storage workloads. Despite the layers of redundancy built into the physical and logical components in this design, it is still possible that sites will be unable to communicate with each other for some amount of time. . At 22:52 UTC on October 21, routine maintenance work to replace failing 100G optical equipment resulted in the loss of connectivity between our US East Coast network hub and our primary US East Coast data center. Connectivity between these locations was restored in 43 seconds, but this brief outage triggered a chain of events that led to 24 hours and 11 minutes of service degradation. .   . In the past, we’ve discussed how we use  MySQL to store GitHub metadata  as well as our approach to  MySQL High Availability . GitHub operates multiple MySQL clusters varying in size from hundreds of gigabytes to nearly five terabytes, each with up to dozens of read replicas per cluster to store non-Git metadata, so our applications can provide pull requests and issues, manage authentication, coordinate background processing, and serve additional functionality beyond raw Git object storage. Different data across different parts of the application is stored on various clusters through functional sharding. . To improve performance at scale, our applications will direct writes to the relevant primary for each cluster, but delegate read requests to a subset of replica servers in the vast majority of cases. We use  Orchestrator  to manage our MySQL cluster topologies and handle automated failover. Orchestrator considers a number of variables during this process and is built on top of  Raft  for consensus. It’s possible for Orchestrator to implement topologies that applications are unable to support, therefore care must be taken to align Orchestrator’s configuration with application-level expectations. .   . During the network partition described above, Orchestrator, which had been active in our primary data center, began a process of leadership deselection, according to Raft consensus. The US West Coast data center and US East Coast public cloud Orchestrator nodes were able to establish a quorum and start failing over clusters to direct writes to the US West Coast data center. Orchestrator proceeded to organize the US West Coast database cluster topologies. When connectivity was restored, our application tier immediately began directing write traffic to the new primaries in the West Coast site. . The database servers in the US East Coast data center contained a brief period of writes that had not been replicated to the US West Coast facility. Because the database clusters in both data centers now contained writes that were not present in the other data center, we were unable to fail the primary back over to the US East Coast data center safely.  . Our internal monitoring systems began generating alerts indicating that our systems were experiencing numerous faults. At this time there were several engineers responding and working to triage the incoming notifications. By 23:02 UTC, engineers in our first responder team had determined that topologies for numerous database clusters were in an unexpected state. Querying the Orchestrator API displayed a database replication topology that only included servers from our US West Coast data center. . By this point the responding team decided to manually lock our internal deployment tooling to prevent any additional changes from being introduced. At 23:09 UTC, the responding team placed the site into  yellow status . This action automatically escalated the situation into an active incident and sent an alert to the incident coordinator. At 23:11 UTC the incident coordinator joined and two minutes later made the decision change to  status red . . It was understood at this time that the problem affected multiple database clusters. Additional engineers from GitHub’s database engineering team were paged. They began investigating the current state in order to determine what actions needed to be taken to manually configure a US East Coast database as the primary for each cluster and rebuild the replication topology. This effort was challenging because by this point the West Coast database cluster had ingested writes from our application tier for nearly 40 minutes. Additionally, there were the several seconds of writes that existed in the East Coast cluster that had not been replicated to the West Coast and prevented replication of new writes back to the East Coast. . Guarding the confidentiality and integrity of user data is GitHub’s highest priority. In an effort to preserve this data, we decided that the 30+ minutes of data written to the US West Coast data center prevented us from considering options other than failing-forward in order to keep user data safe. However, applications running in the East Coast that depend on writing information to a West Coast MySQL cluster are currently unable to cope with the additional latency introduced by a cross-country round trip for the majority of their database calls. This decision would result in our service being unusable for many users. We believe that the extended degradation of service was worth ensuring the consistency of our users’ data. .   . It was clear through querying the state of the database clusters that we needed to stop running jobs that write metadata about things like pushes. We made an explicit choice to partially degrade site usability by pausing webhook delivery and GitHub Pages builds instead of jeopardizing data we had already received from users. In other words, our strategy was to prioritize data integrity over site usability and time to recovery. . Engineers involved in the incident response team began developing a plan to resolve data inconsistencies and implement our failover procedures for MySQL. Our plan was to restore from backups, synchronize the replicas in both sites, fall back to a stable serving topology, and then resume processing queued jobs. We  updated our status  to inform users that we were going to be executing a controlled failover of an internal data storage system. .   . While MySQL data backups occur every four hours and are retained for many years, the backups are stored remotely in a public cloud blob storage service. The time required to restore multiple terabytes of backup data caused the process to take hours. A significant portion of the time was consumed transferring the data from the remote backup service. The process to decompress, checksum, prepare, and load large backup files onto newly provisioned MySQL servers took the majority of time. This procedure is tested daily at minimum, so the recovery time frame was well understood, however until this incident we have never needed to fully rebuild an entire cluster from backup and had instead been able to rely on other strategies such as delayed replicas. . A backup process for all affected MySQL clusters had been initiated by this time and engineers were monitoring progress. Concurrently, multiple teams of engineers were investigating ways to speed up the transfer and recovery time without further degrading site usability or risking data corruption. . Several clusters had completed restoration from backups in our US East Coast data center and begun replicating new data from the West Coast. This resulted in slow site load times for pages that had to execute a write operation over a cross-country link, but pages reading from those database clusters would return up-to-date results if the read request landed on the newly restored replica. Other larger database clusters were still restoring. . Our teams had identified ways to restore directly from the West Coast to overcome throughput restrictions caused by downloading from off-site storage and were increasingly confident that restoration was imminent, and the time left to establishing a healthy replication topology was dependent on how long it would take replication to catch up. This estimate was linearly interpolated from the replication telemetry we had available and the status page was  updated  to set an expectation of two hours as our estimated time of recovery. . GitHub published a  blog post  to provide more context. We use GitHub Pages internally and all builds had been paused several hours earlier, so publishing this took additional effort. We apologize for the delay. We intended to send this communication out much sooner and will be ensuring we can publish updates in the future under these constraints. . All database primaries established in US East Coast again. This resulted in the site becoming far more responsive as writes were now directed to a database server that was co-located in the same physical data center as our application tier. While this improved performance substantially, there were still dozens of database read replicas that were multiple hours delayed behind the primary. These delayed replicas resulted in users seeing inconsistent data as they interacted with our services. We spread the read load across a large pool of read replicas and each request to our services had a good chance of hitting a read replica that was multiple hours delayed. . In reality, the time required for replication to catch up had adhered to a power decay function instead of a linear trajectory. Due to increased write load on our database clusters as users woke up and began their workday in Europe and the US, the recovery process took longer than originally estimated. . By now, we were approaching peak traffic load on GitHub.com. A discussion was had by the incident response team on how to proceed. It was clear that replication delays were increasing instead of decreasing towards a consistent state. We’d begun provisioning additional MySQL read replicas in the US East Coast public cloud earlier in the incident. Once these became available it became easier to spread read request volume across more servers. Reducing the utilization in aggregate across the read replicas allowed replication to catch up. . Once the replicas were in sync, we conducted a failover to the original topology, addressing the immediate latency/availability concerns. As part of a conscious decision to prioritize data integrity over a shorter incident window, we kept the service  status red  while we began processing the backlog of data we had accumulated. . During this phase of the recovery, we had to balance the increased load represented by the backlog, potentially overloading our ecosystem partners with notifications, and getting our services back to 100% as quickly as possible. There were over five million hook events and 80 thousand Pages builds queued. . As we re-enabled processing of this data, we processed ~200,000 webhook payloads that had outlived an internal TTL and were dropped. Upon discovering this, we paused that processing and pushed a change to increase that TTL for the time being. . To avoid further eroding the reliability of our status updates, we remained in degraded status until we had completed processing the entire backlog of data and ensured that our services had clearly settled back into normal performance levels. . All pending webhooks and Pages builds had been processed and the integrity and proper operation of all systems had been confirmed. The site status was  updated to green . . During our recovery, we captured the MySQL binary logs containing the writes we took in our primary site that were not replicated to our West Coast site from each affected cluster. The total number of writes that were not replicated to the West Coast was relatively small. For example, one of our busiest clusters had 954 writes in the affected window. We are currently performing an analysis on these logs and determining which writes can be automatically reconciled and which will require outreach to users. We have multiple teams engaged in this effort, and our analysis has already determined a category of writes that have since been repeated by the user and successfully persisted. As stated in this analysis, our primary goal is preserving the integrity and accuracy of the data you store on GitHub.  . In our desire to communicate meaningful information to you during the incident, we made several public estimates on time to repair based on the rate of processing of the backlog of data. In retrospect, our estimates did not factor in all variables. We are sorry for the confusion this caused and will strive to provide more accurate information in the future. . There are a number of technical initiatives that have been identified during this analysis. As we continue to work through an extensive post-incident analysis process internally, we expect to identify even more work that needs to happen. .   Adjust the configuration of Orchestrator to prevent the promotion of database primaries across regional boundaries. Orchestrator’s actions behaved as configured, despite our application tier being unable to support this topology change. Leader-election within a region is generally safe, but the sudden introduction of cross-country latency was a major contributing factor during this incident. This was emergent behavior of the system given that we hadn’t previously seen an internal network partition of this magnitude.   . Adjust the configuration of Orchestrator to prevent the promotion of database primaries across regional boundaries. Orchestrator’s actions behaved as configured, despite our application tier being unable to support this topology change. Leader-election within a region is generally safe, but the sudden introduction of cross-country latency was a major contributing factor during this incident. This was emergent behavior of the system given that we hadn’t previously seen an internal network partition of this magnitude. .   We have accelerated our migration to a new status reporting mechanism that will provide a richer forum for us to talk about active incidents in crisper and clearer language. While many portions of GitHub were available throughout the incident, we were only able to set our status to green, yellow, and red. We recognize that this doesn’t give you an accurate picture of what is working and what is not, and in the future will be displaying the different components of the platform so you know the status of each service.   . We have accelerated our migration to a new status reporting mechanism that will provide a richer forum for us to talk about active incidents in crisper and clearer language. While many portions of GitHub were available throughout the incident, we were only able to set our status to green, yellow, and red. We recognize that this doesn’t give you an accurate picture of what is working and what is not, and in the future will be displaying the different components of the platform so you know the status of each service. .   In the weeks prior to this incident, we had started a company-wide engineering initiative to support serving GitHub traffic from multiple data centers in an active/active/active design. This project has the goal of supporting N+1 redundancy at the facility level. The goal of that work is to tolerate the full failure of a single data center failure without user impact. This is a major effort and will take some time, but we believe that multiple well-connected sites in a geography provides a good set of trade-offs. This incident has added urgency to the initiative.   . In the weeks prior to this incident, we had started a company-wide engineering initiative to support serving GitHub traffic from multiple data centers in an active/active/active design. This project has the goal of supporting N+1 redundancy at the facility level. The goal of that work is to tolerate the full failure of a single data center failure without user impact. This is a major effort and will take some time, but we believe that multiple well-connected sites in a geography provides a good set of trade-offs. This incident has added urgency to the initiative. .   We will take a more proactive stance in testing our assumptions. GitHub is a fast growing company and has built up its fair share of complexity over the last decade. As we continue to grow, it becomes increasingly difficult to capture and transfer the historical context of trade-offs and decisions made to newer generations of Hubbers.   . We will take a more proactive stance in testing our assumptions. GitHub is a fast growing company and has built up its fair share of complexity over the last decade. As we continue to grow, it becomes increasingly difficult to capture and transfer the historical context of trade-offs and decisions made to newer generations of Hubbers. . This incident has led to a shift in our mindset around site reliability. We have learned that tighter operational controls or improved response times are insufficient safeguards for site reliability within a system of services as complicated as ours. To bolster those efforts, we will also begin a systemic practice of validating failure scenarios before they have a chance to affect you. This work will involve future investment in fault injection and chaos engineering tooling at GitHub. . We know how much you rely on GitHub for your projects and businesses to succeed. No one is more passionate about the availability of our services and the correctness of your data. We will continue to analyze this event for opportunities to serve you better and earn the trust you place in us. ", "date": "October 30, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tAtom understands your code better than ever before\t\t", "author": ["\n\t\tMax Brunsfeld\t"], "link": "https://github.blog/2018-10-31-atoms-new-parsing-system/", "abstract": " Text editors like Atom have many features that make code easier to read and write— syntax highlighting  and  code folding  are two of the most important examples. For decades, all major text editors have implemented these kinds of features based on a very crude understanding of code, obtained by searching for simple,  regular expression  patterns. This approach has severely limited how helpful text editors can be. . At GitHub, we want to explore new ways of making programming intuitive and delightful, so we’ve developed a parsing system called  Tree-sitter  that will serve as a new foundation for code analysis in Atom. Tree-sitter makes it possible for Atom to parse your code while you type—maintaining a  syntax tree  at all times that precisely describes the structure of your code. We’ve enabled the new system by default in Atom, bringing a number of improvements. . Atom’s syntax highlighting is now based on the syntax trees provided by Tree-sitter. This lets us use color to outline your code’s structure more clearly than before. Notice the consistency with which  fields ,  functions ,  keywords ,  types , and  variables  are highlighted across a variety of languages: .   . In most text editors, code folding is based on indentation: lines with greater indentation are considered to be nested more deeply than lines with less indentation. But this doesn’t always match the structure of our code and can make code folding useless in some files. With Tree-sitter, Atom folds code based on its syntax, which allows folding to work as expected, even for code like this: .   . Atom also uses syntax trees as the basis for two new editing commands:  Select Larger Syntax Node  and  Select Smaller Syntax Node , bound to  Alt+Up  and  Alt+Down . These commands can make many editing tasks more efficient and fun, especially when used in combination with multiple cursors. .   . Parsing an entire source file can be time-consuming. This is why most IDEs wait to parse your code until you stop typing for a moment, and there is often a delay before syntax highlighting updates. We want to avoid these delays, so we designed Tree-sitter to parse your code incrementally: it keeps the syntax tree up to date as you edit your code without ever having to re-parse the entire file from scratch. .   . Currently, we use Tree-sitter to parse 11 languages: Bash, C, C++, ERB, EJS, Go, HTML, JavaScript, Python, Ruby, and TypeScript. And we’ve  added Rust support  on  our Beta channel . If you’d like to help us bring the power of Tree-sitter to more languages, check out the  Tree-sitter documentation  and the  grammar page  of the Atom Flight Manual. . Want to know more about Tree-sitter? Check out  this talk  from this year’s  StrangeLoop  conference. . If you write code and you’re interested in trying our new system, give the new version of Atom a try. We’d love to hear your feedback—tweet us at  @AtomEditor  or if you’ve run into a bug,  open an issue . ", "date": "October 31, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tAn open source parser for GitHub Actions\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2019-02-07-an-open-source-parser-for-github-actions/", "abstract": "  Update:  This blog post is no longer relevant with the update to GitHub Actions in August 2019. See the  GitHub Actions documentation  for more information. . Since the beta  release of GitHub Actions  last October, thousands of users have added workflow files to their repositories. But until now, those files only work with the tools GitHub provided: the Actions editor, the Actions execution platform, and the syntax highlighting built into pull requests. To expand that universe, we need to release the  parser  and the  specification  for the Actions workflow language as open source. Today, we’re doing that. . We believe that tools beyond GitHub should be able to run workflows. We believe there should be programs to check, format, compose, and visualize workflow files. We believe that text editors can provide syntax highlighting and autocompletion for Actions workflows. And we believe all that can only happen if the Actions community is empowered to build these tools along with us. That can happen better and faster if there is a single language  specification  and a free parser implementation. . The first project to use the open source parser will be   act  , which is   @nektos  ‘s tool for running Actions workflows in a local development environment. . The parser and language specification are both in   actions/workflow-parser  , which we’re sharing under an MIT license. As of today, there is a  Go implementation , which is the same code that powers both the Actions UI and the Actions execution platform. The repository also contains a  Javascript parser in development , along with syntax-highlighting configurations for Atom and Vim. .  GitHub Actions  is platform for automating software development workflows, from idea to production. Developers add a simple text file to their repository,  .github/main.workflow , to describe automation. The workflow file describes how events like pushing code or opening and closing issues map to automation actions, implemented in any Docker container. Those automation actions have whatever powers you grant them: pushing commits to the repository, cutting a new release, building it through continuous integration, deploying it to staging in the cloud, testing the deployment, flipping it to production, and announcing it to the world — and any others you can build. . Every workflow begins with an event and runs through a set of actions to reach some target or goal. Those events and actions are described in a  main.workflow  file, which  you can create and edit  with the visual editor or any text editor you like. Here is a simple example: . Whenever I push to a branch that contains that file, the  when I push  workflow executes. It resolves the target action  ci , which runs  ./script/cibuild  in a  golang:latest  Docker container. I can add more  workflow  blocks to harness more events, and I can add more  action  blocks to run after the  ci  action or in parallel with it. . All  main.workflow  files are written in the Actions workflow language, which is a subset of Hashicorp’s HCL. In fact, our parser builds on top of the open source   hashicorp/hcl   parser. . All Actions workflow files are valid HCL, but not all HCL files are valid workflows. The Actions workflow parser is stricter, allowing only a specific set of keywords and prohibiting nested objects, among other restrictions. The reason for that is a long-standing goal of Actions: making the  Actions editor  and the text representation of workflows equivalent and interchangeable. Any file you write in the graphical editor can be expressed in a  main.workflow  file, of course, but also: any  main.workflow  can be fully displayed and edited in the graphical editor. There is one exception to this: the graphical editor does not display comments. But it preserves them: changes you make in the graphical editor do not disturb comments you have added to your  main.workflow  file. . We have two reasons for releasing the parser. First, we want to encourage the Actions community to build tools that generate and manipulate workflow files. The language  specification  should help developers understand the language, and the parser should save developers the trouble of writing their own. . Second, we welcome your contributions. If you find bugs, please open an issue or send a pull request. If you want to add a syntax highlighter for a new editor or implement the parser in another language, we welcome that. The only real limitation is on features that go beyond the parser to the rest of Actions. For broader questions and suggestions about the rest of Actions, reach out through support;  we’re listening . ", "date": "February 7, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tFive years of the GitHub Bug Bounty program\t\t", "author": ["\n\t\tPhil Turnbull\t"], "link": "https://github.blog/2019-02-19-five-years-of-the-github-bug-bounty-program/", "abstract": " GitHub launched our  Security Bug Bounty program  in 2014, allowing us to reward independent security researchers for their help in keeping GitHub users secure. Over the past five years, we have been continuously impressed by the hard work and ingenuity of our researchers. Last year was no different and we were glad to pay out $165,000 to researchers from our public bug bounty program in 2018. . We’ve  previously talked  about our other initiatives to engage with researchers. In 2018, our  researcher grants ,  private bug bounty programs , and a live-hacking event allowed us to reach even more independent security talent. These different ways of working with the community helped GitHub reach a huge milestone in 2018: $250,000 paid out to researchers in a single year. . We’re happy to share some of our highlights from the past year and introduce some big changes for the coming year: full legal protection for researchers, more GitHub properties eligible for rewards, and increased reward amounts. . Since the launch of our researcher grants program in 2017 we’ve been on the lookout for bug bounty researchers who show a specialty in particular features of our products. In mid-2018  @kamilhism  submitted a series of vulnerabilities to the public bounty program showing his expertise in the authorization logic of our REST and GraphQL APIs. To support their future research, we provided Kamil with a fixed grant payment to perform a systematic audit of our API authorization logic. Kamil’s audit was excellent, uncovering and allowing us to fix an additional seven authorization flaws in our API. . In August, GitHub took part in HackerOne’s  H1-702  live-hacking event in Las Vegas. This brought together over 75 of the top researchers from HackerOne to focus on GitHub’s products for one evening of live-hacking. The event didn’t disappoint—GitHub’s security improved and nearly $75,000 was paid out for 43 vulnerabilities. This included one critical-severity vulnerability in GitHub Enterprise Server. We also met with our researchers in-person and received great feedback on how we could improve our bug bounty program. . In October, GitHub launched a limited public beta of  GitHub Actions . As part of the limited beta, we also ran a private bug bounty program to complement our extensive internal security assessments. We sent out over 150 invitations to researchers from last year’s private program, all H1-702 participants, and invited a number of the best researchers that have worked with our public program. The private bounty program allowed us to uncover a number of vulnerabilities in GitHub Actions. . We also held an office-hours event so that the GitHub security team and researchers could meet. We took the opportunity to meet face-to-face with other researchers because it’s a great way to build a community and learn from each other. Two of our researchers,  @not-an-aardvark  and  @ngaloggc , gave an overview of their submissions and shared details of how they approached the target with everyone. . We’ve been making refinements to our internal  bug bounty workflow  since we last announced it back in 2017.  Our ChatOps-based tools have continued to evolve over the past year as we find more ways to streamline the process. These aren’t just technical changes—each day we’ve had individual on-call first responders who were responsible for handling incoming bounty submissions. We’ve also added a weekly status meeting to review current submissions with all members of the Application Security team. These meetings allow the team to ensure that submissions are not stalled, work is correctly prioritized by engineering teams based on severity, and researchers are getting timely updates on their submissions. . A key success metric for our program is how much time it takes to validate a submission and triage that information to the relevant engineering team so remediation work can begin. Our workflow improvements have paid off and we’ve significantly reduced the average time to triage from four days in 2017 down to 19 hours. Likewise, we’ve reduced our average time to resolution from 16 days to six days. Keep in mind: for us to consider a submission as resolved, the issue has to either be fixed or properly prioritized and tracked, by the responsible engineering team. . We’ve continued to reach our target of replying to researchers in less than 24 hours on average. Most importantly for our researchers, we’ve also dropped our average time for rewarding a submission from 17 days in 2017 down to 11 days. We’re grateful for the effort that researchers invest in our program and we aim to reduce these times further over the next year. . Although our program has been running successfully for the past five years, we know that we can always improve. We’ve taken feedback from our researchers and are happy to announce three major changes to our program for 2019: . Keeping bounty program participants safe from the legal risks of security research is a high priority for GitHub. To make sure researchers are as safe as possible, we’ve added a robust set of  Legal Safe Harbor terms  to our site policy. Our new policies are based on  CC0-licensed templates  by GitHub’s Associate Corporate Counsel,  @F-Jennings . These templates are a fork of  EdOverflow ’s  Legal Bug Bounty  repo, with extensive modifications based on broad discussions with security researchers and  Amit Elazari ’s  general research  in this field. The templates are also inspired by other best-practice safe harbor examples including Bugcrowd’s  disclose.io  project and Dropbox’s  updated vulnerability disclosure policy . . Our new Legal Safe Harbor terms cover three main sources of legal risk: . Other organizations can look to these terms as an industry standard for safe harbor best practices—and we encourage others to freely adopt, use, and modify them to fit their own bounty programs. In creating these terms, we aim to go beyond the current standards for safe harbor programs and provide researchers with the best protection from criminal, civil, and third-party legal risks. The terms have been reviewed by expert security researchers, and are the product of many months of legal research and review of other legal safe harbor programs. Special thanks to  MG ,  Mugwumpjones , and several other researchers for providing input on early drafts of  @F-Jennings ’ templates. . Over the past five years, we’ve been steadily expanding the list of GitHub products and services that are eligible for reward. We’re excited to share that we are now increasing our bounty scope to reward vulnerabilities in all first party services hosted under our github.com domain. This includes  GitHub Education ,  GitHub Learning Lab ,  GitHub Jobs , and our  GitHub Desktop  application. While GitHub Enterprise Server has been in scope since 2016, to further increase the security of our enterprise customers we are now expanding the scope to include  Enterprise Cloud . . It’s not just about our user-facing systems. The security of our users’ data also depends on the security of our employees and our internal systems. That’s why we’re also including all first-party services under our employee-facing githubapp.com and github.net domains. . We regularly assess our reward amounts against our industry peers. We also recognize that finding higher-severity vulnerabilities in GitHub’s products is becoming increasingly difficult for researchers and they should be rewarded for their efforts. That’s why we’ve increased our reward amounts at all levels: . Our broad ranges have served us well, but we’ve been consistently impressed by the ingenuity of researchers. To recognize that, we no longer have a maximum reward amount for critical vulnerabilities. Although we’ve listed $30,000 as a guideline amount for critical vulnerabilities, we’re reserving the right to reward significantly more for truly cutting-edge research. . The bounty program remains a core part of GitHub’s security process and we’re learning a lot from our researchers. With our new initiatives, now is the perfect time to get involved. Details about our safe harbor, expanded scope, and increased awards are available on the  GitHub Bug Bounty site . . Working with the community has been a great experience—we’re looking forward to triaging your submissions in the future! ", "date": "February 19, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tHighlights from Git 2.21\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2019-02-24-highlights-from-git-2-21/", "abstract": " The open source Git project  just released Git 2.21  with features and bug fixes from over 60 contributors. We last caught up with you on the latest Git  releases    when 2.19 was released  . Here’s a look at some of the most interesting features and changes introduced since then. . As part of its output,  git log  displays the date each commit was authored. Without making an alternate selection, timestamps will display in Git’s “default” format (for example, “Tue Feb 12 09:00:33 2019 -0800”). . That’s very precise, but a lot of those details are things that you might already know, or don’t care about. For example, if the commit happened today, you already know what year it is. Likewise, if a commit happened seven years ago, you don’t care which second it was authored. So what do you do? You could use  --date=relative , which would give you output like  6 days ago , but often, you want to relate “six days ago” to a specific event, like “my meeting last Wednesday.” So, was six days ago Tuesday, or was it Wednesday that you were interested in? . Git 2.21 introduces a new date format that tells you exactly when something occurred with just the right amount of detail:  --date=human . Here’s how  git log  looks with the new format: .   . That’s both more accurate than  --date=relative  and easier to consume than the full weight of  --date=default . . But what about when you’re scripting? Here, you might want to frequently switch between the human and machine-readable formats while putting together a pipeline. Git 2.21 has an option suitable for this setting, too:  --date=auto:human . When printing output to a pager, if Git is given this option it will act as if it had been passed  --date=human . When otherwise printing output to a non-pager, Git will act as if no format had been given at all. If  human  isn’t quite your speed, you can combine  auto  with any other format of your choosing, like  --date=auto:relative . .  [ source ] . One commonly asked Git question is, “After cloning a repository, why does  git status  report some of the files as modified?” Quite often, the answer is that the repository contains a tree which cannot be represented on your file system. For instance, if it contains both  file  as well as  FILE  and your file system is case-insensitive, Git can only checkout one of those files. Worse, Git doesn’t actually detect this case during the clone; it simply writes out each path, unaware that the file system considers them to be the same file. You only find out something has gone wrong when you see a mystery modification. . The exact rules for when this occurs will vary from system to system. In addition to “folding” what we normally consider upper and lowercase characters in English, you may also see this from language-specific conversions, non-printing characters, or Unicode normalization. . In Git 2.20,  git clone  now detects and reports colliding groups during the initial checkout, which should remove some of the confusion. Unfortunately, Git can’t actually fix the problem for you. What the original committer put in the repository can’t be checked out as-is on your file system. So if you’re thinking about putting files into a multi-platform project that differ only in case, the best advice is still: don’t. .    [ source ] . Behind the scenes, a lot has changed over the last couple of Git releases, too. We’re dedicating this section to overview a few of these changes. Not all of them will impact your Git usage day-to-day, but some will, and all of the changes are especially important for server administrators. . Git stores objects (e.g., representations of the files, directories, and more that make up your Git repository) in both the “loose” and “packed” formats. A “loose” object is a compressed encoding of an object, stored in a file. A “packed” object is stored in a packfile, which is a collection of objects, written in terms of deltas of one another. . Because it can be costly to rewrite these packs every time a new object is added to the repository, repositories tend to accumulate many loose objects or individual packs over time. Eventually, these are reconciled during a “repack” operation. However, this reconciliation is not possible for larger repositories, like the   Windows repository  . . Instead of repacking, Git can now create a multi-pack index file, which is a listing of objects residing in multiple packs, removing the need to perform expensive repacks (in many cases). . [ source ] . An important optimization for Git servers is that the format for transmitted objects is the same as the heavily-compressed on-disk packfiles. That means that in many cases, Git can serve repositories to clients by simply copying bytes off disk without having to inflate individual objects. . But sometimes this assumption breaks down. Objects on disk may be stored as “deltas” against one another. When two versions of a file have similar content, we might store the full contents of one version (the “base”), but only the differences against the base for the other version. This creates a complication when serving a fetch. If object  A  is stored as a delta against object  B , we can only send the client our on-disk version of  A  if we are also sending them  B  (or if we know they already have  B ). Otherwise, we have to reconstruct the full contents of  A  and re-compress it. . This happens rarely in many repositories where clients clone all of the objects stored by the server. But it can be quite common when multiple distinct but overlapping sets of objects are stored in the same packfile (for example, due to repository forks or unmerged pull requests). Git may store a delta between objects found only in two different forks. When someone clones one of the forks, they want only one of the objects, and we have to discard the delta. . Git 2.20 solves this by introducing the concept  of “   delta islands   “ . Repository administrators can partition the ref namespace into distinct “islands”, and Git will avoid making deltas between islands. The end result is a repository which is slightly larger on disk but is still able to serve client fetches much more cheaply. . [ source 1 ,  source 2 ] . We already discussed the importance of reusing on-disk deltas when serving fetches, but how do we know when the other side has the base object they’ need to use the delta we send them? If we’re sending them the base, too, then the answer is easy. But if we’re not, how do we know if they have it? . That answer is deceptively simple: the client will have already told us which commits it has (so that we don’t bother sending them again). If they claim to have a commit which contains the base object, then we can re-use the delta. But there’s one hitch: we not only need to know about the commit they mentioned, but also the entire object graph. The base may have been part of a commit hundreds or thousands of commits deep in the history of the project. . Git doesn’t traverse the entire object graph to check for possible bases because it’s too expensive to do so. For instance, walking the entire graph of a Linux kernel takes roughly 30 seconds. . Fortunately, there’s already a solution within Git:   reachability bitmaps   . Git has an optional on-disk data structure to record the sets of objects “reachable” from each commit. When this data is available, we can query it to quickly determine whether the client has a base object. This results in the server generating    smaller packs that are produced more quickly   for an overall faster fetch experience. . [ source ] . Repository alternates are a tool that server administrators have at their disposal to reduce redundant information. When two repositories are known to share objects (like a fork and its parent), the fork can list the parent as an “alternate”, and any objects the fork doesn’t have itself, it can look for in its parent. This is helpful since we can avoid storing twice the vast number of objects shared between the fork and parent. . Likewise, a repository with alternates advertises “tips” it has when receiving a push. In other words, before writing from your computer to a remote, that remote will tell you what the tips of its branches are, so you can determine information that is already known by the remote, and therefore use less bandwidth. When a repository has alternates, the tips advertisement is the union of all local and alternate branch tips. . But what happens when computing the tips of an alternate is more expensive than a client sending redundant data? It makes the push so slow that we have disabled this feature for years at GitHub. In Git 2.20, repositories can hook into the way that they enumerate alternate tips, and make the corresponding transaction much faster. . [ source ] . Now that we’ve highlighted a handful of the changes in the past two releases, we want to share a summary of a few other interesting changes. As always, you can learn more by clicking the “source” link, or reading  the    documentation    or    release notes  . . Veteran Git  users    from our last post   might recall that  git branch -l  establishes a reflog for a newly created branch, instead of listing all branches. Now, instead of doing something you almost certainly didn’t mean,  git branch -l  will list all of your repository’s branches, keeping in line with other commands that accept  -l . [ source ] . If you’ve ever been stuck or forgotten what a certain command or flag does, you might have run  git --help  (or  git -h ) to learn more. In Git 2.21, this invocation now follows aliases, and shows the aliased command’s helptext. [ source ] . In repositories with large on-disk checkouts,  git status  can take a long time to complete. In order to indicate that it’s making progress, the status command now displays a progress bar. [ source ] . Many parts of Git have historically been implemented as shell scripts, calling into tools written in C to do the heavy lifting. While this allowed rapid prototyping, the resulting tools could often be slow due to the overhead of running many separate programs. There are continuing efforts to move these scripts into C, affecting   git submodule  ,   git bisect  , and  git rebase . You may notice  rebase  in particular being much faster, due to the hard work of the Summer of Code students,  Pratik Karki  and  Alban Gruin . . The   -G  option  tells  git log  to only show commits whose diffs match a particular pattern. But until Git 2.21, it was searching binary files as if they were text, which made things slower and often produced confusing results. [ source ] . We went through a few of the changes that have happened over the last couple of versions, but there’s a lot more to discover.  Read the    release notes for 2.21   , or review the    release notes for previous versions    in the Git  repository. .  \t\t Tags:   \t\t Git \t ", "date": "February 24, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tVulcanizer: a library for operating Elasticsearch\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2019-03-05-vulcanizer-a-library-for-operating-elasticsearch/", "abstract": " At GitHub, we use  Elasticsearch  as the main technology backing our search services. In order to administer our clusters, we use ChatOps via  Hubot . As of 2017, those commands were a collection of Bash and Ruby-based scripts. . Although this served our needs for a time, it was becoming increasingly apparent that these scripts lacked composability and reusability. It was also difficult to contribute back to the community by open sourcing any of these scripts due to the fact they are specific to bespoke GitHub infrastructure. . There are plenty of excellent Elasticsearch libraries, both official and community driven. For Ruby, GitHub has already released the  Elastomer  library and for Go we make use of the  Elastic library by user olivere . However, these libraries focus primarily on indexing and querying data. This is exactly what an application needs to use Elasticsearch, but it’s not the same set of tools that operators of an Elasticsearch cluster need. We wanted a high-level API that corresponded to the common operations we took on a cluster, such as disabling allocation or draining the shards from a node. Our goal was a library that focused on these administrative operations and that our existing tooling could easily use. . We started looking into Go and were inspired by GitHub’s success with   freno   and   orchestrator  . . Go’s structure encourages the construction of composable (self-contained, stateless, components that can be selected and assembled) software, and we saw it as a good fit for this application. . We initially scoped the project out to be a packaged chat app and planned to open source only what we were using internally. During implementation, however, we ran into a few problems: . Based on these factors we decided to break out the core of our library into a separate package that we could open source. This would decouple the package from our internal libraries, Consul, and ChatOps RPC. . The package would only have a few goals: . This module could then be open sourced without being tied to our internal infrastructure, so that anyone could use it with the ChatOps infrastructure, service discovery, or tooling they choose. . To that end, we wrote  vulcanizer.  . Vulcanizer is a Go library for interacting with an Elasticsearch cluster. It is not meant to be a full-fledged Elasticsearch client. Its goal is to provide a high-level API to help with common tasks that are associated with operating an Elasticsearch cluster such as querying health status of the cluster, migrating data off of nodes, updating cluster settings, and more. . Elasticsearch is great in that almost all things you’d want to accomplish can be done via its HTTP interface, but you don’t want to write JSON by hand, especially during an incident. Below are a few examples of how we use Vulcanizer for common tasks and the equivalent curl commands. The Go examples are simplified and don’t show error handling. . You’ll often want to list the nodes in your cluster to pick out a specific node or to see how many nodes of each type you have in the cluster. . Vulcanizer exposes typed structs for these types of objects. . The  index recovery speed  is a common setting to update when you want balance time to recovery and I/O pressure across your cluster. The curl version has a lot of JSON to write. . The Vulcanizer API is fairly simple and will also retrieve and return any existing setting for that key so that you can record the previous value. . To safely update a node, you can  set allocation rules  so that data is migrated off a specific node. In the Elasticsearch settings, this is a comma-separated list of node names, so you’ll need to be careful not to overwrite an existing value when updating it. . The Vulcanizer API will safely add or remove nodes from the exclude settings so that shards won’t allocate on to a node unexpectedly. . Included is a small CLI application that leverages the library: . ChatOps is important for GitHub and our geographically distributed workforce. Vulcanizer enables us to build ChatOps tooling around Elasticsearch quickly and easily for common tasks: . We stumbled a bit when we first started down this path, but the end result is best for everyone: . Visit  the Vulcanizer repository  to clone or contribute to the project. We have ideas for future development in the  Vulcanizer roadmap . ", "date": "March 5, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tDirect instruction marking in Ruby 2.6\t\t", "author": ["\n\t\tAaron Patterson\t"], "link": "https://github.blog/2019-06-04-direct-instruction-marking-in-ruby-2-6/", "abstract": "  We recently upgraded GitHub to use the latest version of Ruby 2.6. Ruby 2.6 contains an optimization for reducing memory usage. We’ve found it to reduce the “post-boot live heap” by about 3 percent. The “post-boot live heap” are the objects still referenced and not garbage collected after booting our Rails application, but before accepting any requests.  .  MRI (Matz’s Ruby Implementation) uses a stack-based virtual machine.  Each instruction manipulates a stack, and that stack can be thought of as a sort of a “scratch space”. For example, the program  3 + 5  could be represented with the instruction sequences:    .  As the virtual machine executes, each instruction manipulates the stack:  .   .  Before the virtual machine has something to execute, the code must go through a few different processing phases. The code is tokenized, parsed, turned in to an AST (or Abstract Syntax Tree), and finally the AST is converted to byte code. The byte code is what the virtual machine will eventually execute.  .  Let’s look at an example Ruby program:  .  This code is turned into an AST, which is a tree data structure. Each node in the tree is represented internally by an object called a  T_NODE  object. The tree for this code will look like this:  .   .  Some of the  T_NODE  objects reference literals. In this case some  T_NODE  objects reference the string literals “hello” and “world”. Those string literals are allocated via Ruby’s Garbage Collector. They are just like any other string in Ruby, except that they happen to be allocated as the code is being parsed.  .  Instructions and their operands are represented as integers and can only be represented as integers. Instruction sequences for a program are just a list of integers, and it’s up to the virtual machine to interpret the meaning of those integers.  .  The compilation process produces the list of integers that the virtual machine will interpret. To compile a program, we simply walk the tree translating nodes and their operands to integers.  The program  \"hello\" + \"world\"  will result in three instructions: two  push  operations and one  add  operation. The  push  instructions have  \"hello\"  and  \"world\"  as operands.  .  There are a fixed number of instructions, and we can represent each instruction with an integer. In this case, let’s use the number  7  for  push  and the number  9  for  add . But how can we convert the strings “hello” and “world” to integers?  .   .  In order to represent these strings in the instruction sequences, the compiler will add    the address    of the Ruby object that represents each string. The virtual machine knows that the operand to the  push  instruction is actually the address of a Ruby object, and will act appropriately. This means that the final instructions will look like this:  .   .  After the AST is fully processed, it is thrown away and only the instruction   sequences remain:  .   .   .  Instruction Sequences are Ruby objects and are managed via Ruby’s garbage collector. As mentioned earlier, string literals are also Ruby objects and are managed by the garbage collector. If the string literals are not marked, they could be collected, and the instruction sequences would point to an invalid address.  .  To prevent these literal objects from being collected, Ruby 2.5 would maintain a “mark array”. The mark array is simply a Ruby array that contains references to all literals referenced for that set of instruction sequences:  .   .  Both the mark array and the instructions contain references to the string literals found in the code. But the instruction sequences depend on the mark array to keep the literals from being collected.  .  Ruby 2.6    introduced a patch that eliminates this mark array   . When instruction sequences are marked, rather than marking an array, it disassembles the instructions and marks instruction operands that were allocated via the garbage collector. This disassembly process means that the mark array can be completely eliminated:  .   .  We found that this reduced the number of live objects in our heap by three percent after the application starts.  .  Of course, disassembling instructions is more expensive than iterating an array. We found that only 30 percent of instruction sequences actually contain references to objects that need marking. In order to prevent needless disassembly, instructions that contain objects allocated from Ruby’s garbage collector are flagged at compile time, and only those instruction sequences are disassembled during mark time. On top of this, instruction sequence objects typically become “old”. This means that thanks to Ruby’s generational garbage collector, they are examined very infrequently. As a result, we observed memory reduction with zero cost to throughput.  .  Have a good day!  ", "date": "June 4, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tAtom editor is now faster\t\t", "author": ["\n\t\tRafael Oleza\t"], "link": "https://github.blog/2019-06-12-atom-editor-is-now-faster/", "abstract": "  The Atom Team improved a few of Atom’s most common features—they’re now dramatically faster and ready to help you be even more productive.  .  The fuzzy finder is one of the most popular features in Atom, and it’s used by nearly every user to quickly open files by name. We wanted to make it even better by speeding up the process.    Atom version 1.38    introduces an experimental fast mode that brings drastic speed improvements to any project. With this mode, indexing a medium or large project is roughly six times faster than when using the standard mode.  .   .  This change also made the process to display filtered results in Atom about 11 times faster. You’ll notice the difference in speed as soon as you start searching for a query.  .   .  Our goal is to make the new experimental mode the default option on Atom version 1.39 when it’s released next month. Until then, switch to this mode by clicking the   try experimental fast mode   option in the fuzzy finder.  .  The next Atom version 1.39 will also introduce performance improvements to the find and replace package. We’re adding a new mode to make searching for files between 10 and 20 times faster than the current mode.  .  We’d like to thank the open source community since it has allowed us to build and improve Atom over the years. More specifically, we’ve made these performance improvements by building on the shoulders of two big giants:   .   Try out Atom.io   ", "date": "June 12, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tC# or Java? TypeScript or JavaScript? Machine learning based classification of programming languages\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2019-07-02-c-or-java-typescript-or-javascript-machine-learning-based-classification-of-programming-languages/", "abstract": "  GitHub hosts over 300 programming languages—from commonly used languages such as Python, Java, and Javascript to esoteric languages such as    Befunge   , only known to very small communities.  .  One of the necessary challenges that GitHub faces is to be able to recognize these different languages. When some code is pushed to a repository, it’s important to recognize the type of code that was added for the purposes of search, security vulnerability alerting, and syntax highlighting—and to show the repository’s content distribution to users.  .  Despite the appearance, language recognition isn’t a trivial task. File names and extensions, while providing a good indication of what the coding language is likely to be, do not offer the full picture. In fact, many extensions are associated with the same language (e.g., “.pl”, “.pm”, “.t”, “.pod” are all associated with Perl), while others are ambiguous and used almost interchangeably across languages (e.g., “.h” is commonly used to indicate many languages of the “C” family, including C, C++, and Objective-C). In other cases, files are simply provided with no extension (especially for executable scripts) or with the incorrect extension (either on purpose or accidentally).  .   Linguist    is the tool we currently use to detect coding languages at GitHub. Linguist a Ruby-based application that uses various strategies for language detection, leveraging naming conventions and file extensions and also taking into account Vim or Emacs modelines, as well as the content at the top of the file (shebang). Linguist handles language disambiguation via heuristics and, failing that, via a Naive Bayes classifier trained on a small sample of data.   .  Although Linguist does a good job making file-level language predictions (84% accuracy), its performance declines considerably when files use unexpected naming conventions and, crucially, when a file extension is not provided. This renders Linguist unsuitable for content such as GitHub Gists or code snippets within README’s, issues, and pull requests.  .  In order to make language detection more robust and maintainable in the long run, we developed a machine learning classifier named OctoLingua based on an Artificial Neural Network (ANN) architecture which can handle language predictions in tricky scenarios. The current version of the model is able to make predictions for the top 50 languages hosted by GitHub and surpasses Linguist in accuracy and performance.   .  OctoLingua was built from scratch using Python, Keras with TensorFlow backend—and is built to be accurate, robust, and easy to maintain. In this section, we describe our data sources, model architecture, and performance benchmark for OctoLingua. We also describe what it takes to add support for a new language.   .  The current version of OctoLingua was trained on files retrieved from    Rosetta Code    and from a set of quality repositories internally crowdsourced. We limited our language set to the top 50 languages hosted on GitHub.  .  Rosetta Code was an excellent starter dataset as it contained source code for the same task expressed in different programming languages. For example, the task of generating a    Fibonacci sequence    is expressed in C, C++, CoffeeScript, D, Java, Julia, and more. However, the coverage across languages was not uniform where some languages only have a handful of files and some files were just too sparsely populated. Augmenting our training set with some additional sources was therefore necessary and substantially improved language coverage and performance.  .  Our process for adding a new language is now fully automated. We programmatically collect source code from public repositories on GitHub. We choose repositories that meet a minimum qualifying criteria such as having a minimum number of forks, covering the target language and covering specific file extensions. For this stage of data collection, we determine the primary language of a repository using the classification from Linguist.   .  Traditionally, for text classification problems with Neural Networks, memory-based architectures such as Recurrent Neural Networks (RNN) and Long Short Term Memory Networks (LSTM) are often employed. However, given that programming languages have differences in vocabulary, commenting style, file extensions, structure, libraries import style and other minor differences, we opted for a simpler approach that leverages all this information by extracting some relevant features in tabular form to be fed to our classifier. The features currently extracted are as follows:  .  Top five special characters per file  .  Top 20 tokens per file  .  File extension  .  Presence of certain special characters commonly used in source code files such as colons, curly braces, and semicolons  .  We use the above features as input to a two-layer Artificial Neural Network built using Keras with Tensorflow backend.   .  The diagram below shows that the feature extraction step produces an n-dimensional tabular input for our classifier. As the information moves along the layers of our network, it is regularized by dropout and ultimately produces a 51-dimensional output which represents the predicted probability that the given code is written in each of the top 50 GitHub languages plus the probability that it is not written in any of those.  .  We used 90% of our dataset for training over approximately eight epochs. Additionally, we removed a percentage of file extensions from our training data at the training step, to encourage the model to learn from the vocabulary of the files, and not overfit on the file extension feature, which is highly predictive.  .  In Figure 3, we show the    F1 Score    (harmonic mean between precision and recall) of OctoLingua and Linguist calculated on the same test set (10% from our initial data source).   .  Here we show three tests. The first test is with the test set untouched in any way. The second test uses the same set of test files with file extension information removed and the third test also uses the same set of files but this time with file extensions scrambled so as to confuse the classifiers (e.g., a Java file may have a “.txt” extension and a Python file may have a “.java”) extension.   .  The intuition behind scrambling or removing the file extensions in our test set is to assess the robustness of OctoLingua in classifying files when a key feature is removed or is misleading. A classifier that does not rely heavily on extension would be extremely useful to classify gists and snippets, since in those cases it is common for people not to provide accurate extension information (e.g., many code-related gists have a .txt extension).  .  The table below shows how OctoLingua maintains a good performance under various conditions, suggesting that the model learns primarily from the vocabulary of the code, rather than from meta information (i.e. file extension), whereas Linguist fails as soon as the information on file extensions is altered.  .   .  As mentioned earlier, during training time we removed a percentage of file extensions from our training data to encourage the model to learn from the vocabulary of the files. The table   below shows the performance of our model with different fractions of file extensions removed during training time.   .  Notice that with no file extension removed during training time, the performance of OctoLingua on test files with no extensions and randomized extensions decreases considerably from that on the regular test data. On the other hand, when the model is trained on a dataset where some file extensions are removed, the model performance does not decline much on the modified test set. This confirms that removing the file extension from a fraction of files at training time induces our classifier to learn more from the vocabulary. It also shows that the file extension feature, while highly predictive, had a tendency to dominate and prevented more weights from being assigned to the content features.   .  Adding a new language in OctoLingua is fairly straightforward. It starts with obtaining a bulk of files in the new language (we can do this programmatically as described in data sources). These files are split into a training and a test set and then run through our preprocessor and feature extractor. This new train and test set is added to our existing pool of training and testing data. The new testing set allows us to verify that the accuracy of our model remains acceptable.  .  As of now, OctoLingua is at the “advanced prototyping stage”. Our language classification engine is already robust and reliable, but does not yet support all coding languages on our platform. Aside from broadening language support—which would be rather straightforward—we aim to enable language detection at various levels of granularity. Our current implementation already allows us, with a small modification to our machine learning engine, to classify code snippets. It wouldn’t be too far fetched to take the model to the stage where it can reliably detect and classify embedded languages.   .  We are also contemplating the possibility of open sourcing our model and would love to hear from the community if you’re interested.  .  With OctoLingua, our goal is to provide a service that enables robust and reliable source code language detection at multiple levels of granularity, from file level or snippet level to potentially line-level language detection and classification. Eventually, this service can support, among others, code searchability, code sharing, language highlighting, and diff rendering—all of this aimed at supporting developers in their day to day development work in addition to helping them write quality code.  If you are interested in leveraging or contributing to our work, please feel free to get in touch on Twitter  @github !  .  \t\t Tags:   \t\t insights \t ", "date": "July 2, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tRunning GitHub on Rails 6.0\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2019-09-09-running-github-on-rails-6-0/", "abstract": " On August 26, 2019, the GitHub application was deployed to production with 100 percent of traffic on the newest Rails version: 6.0. This change came just 1.5 weeks after  the final release of Rails 6.0 . Rails upgrades aren’t always something companies announce, but looking back at GitHub’s history of being on a custom fork of Rails 3.2, this upgrade is a big deal. It represents how far we’ve come in the last few years, and the hard work and dedication from our upgrade team made it smoother, easier, and faster than any of our previous upgrades. . At GitHub, we have a lot to celebrate with the release of Rails 6.0 and the subsequent production deploy. First, we were more involved in this release than we have been in any previous release of Rails. GitHub engineers sent over 100 pull requests to Rails 6.0 to improve documentation, fix bugs, add features, and speed up performance. For many GitHub contributors, this was the first time sending changes to the Rails framework, demonstrating that upgrading Rails not only helps GitHub internally, but also improves our developer community as well. . Second, we deployed Rails 6.0 to production without any negative impact to customers—we had only one Rails 6.0 exception occur during testing, and it was hit by a bot! We were able to achieve this level of stability for the upgrade because we were heavily involved with its development. As soon as we finished the Rails 5.2 upgrade last year, we started upgrading our application to Rails 6.0. . Instead of waiting for the final release, we’d upgrade every week by pulling in the latest changes from Rails master and run all of our tests against that new version. This allowed us to find regressions quickly and early—often finding regressions in Rails master just hours after they were introduced. Upgrading weekly made it easy to find where these regressions were introduced since we were bisecting Rails with only a week’s worth of commits instead of more than a year of commits. Once our build for Rails 6.0 was green, we’d merge the pull request to master, and all new code that went into GitHub would need to pass in Rails 5.2 and the newest master build of Rails. Upgrading every week worked so well that we’ll continue using this process for upgrading from 6.0 to 6.1. . In addition to ensuring that Rails 6.0 was stable, we also contributed to the new features of the framework like parallel testing and multiple databases. The code for these tools is used in our production application every day—it’s well-tested, GitHub-approved code in a public, open source framework. By upstreaming this tooling, we’re able to reduce complexity in our code base and set a standard where so many companies once had to implement this functionality on their own. . There are so many wins to staying upgraded that go beyond more security, faster performance, and new features. By staying current with Rails master, we’re influencing the future of the framework to meet our needs and giving back to the open source community in big ways. This process means that the GitHub code base evolves alongside Rails instead of in response to Rails. Investing in our application by staying up to date with the Rails framework has had a tremendous positive effect on our code base and engineering teams. Staying current allows us to invest in our community, invest in our tools for the long term, and improve the experience of working with the GitHub code base for our engineers. . Keep a look out for all the great stuff we’ll be contributing to Rails 6.1 and beyond—this is just the beginning. ", "date": "September 9, 2019"},
{"website": "Github-Engineering", "title": "\n\t\t\tRemoving jQuery from GitHub.com frontend\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2018-09-06-removing-jquery-from-github-frontend/", "abstract": " We have recently completed a milestone where we were able to drop jQuery as a dependency of the frontend code for GitHub.com. This marks the end of a gradual, years-long transition of increasingly decoupling from jQuery until we were able to completely remove the library. In this post, we will explain a bit of history of how we started depending on jQuery in the first place, how we realized when it was no longer needed, and point out that—instead of replacing it with another library or framework—we were able to achieve everything that we needed using standard browser APIs. . GitHub.com pulled in  jQuery 1.2.1  as a dependency in late 2007. For a bit of context, that was a year before Google released the first version of their Chrome browser. There was no standard way to query DOM elements by a CSS selector, no standard way to animate visual styles of an element, and the  XMLHttpRequest interface  pioneered by Internet Explorer was, like many other APIs, inconsistent between browsers. . jQuery made it simple to manipulate the DOM, define animations, and make “AJAX” requests— basically, it enabled web developers to create more modern, dynamic experiences that stood out from the rest. Most importantly of all, the JavaScript features built in one browser with jQuery would generally work in other browsers, too. In those early days of GitHub when most of its features were still getting fleshed out, this allowed the small development team to prototype rapidly and get new features out the door without having to adjust code specifically for each web browser. . The simple interface of jQuery also served as a blueprint to craft extension libraries that would later serve as building blocks for the rest of GitHub.com frontend:  pjax  and  facebox . . We will always be thankful to John Resig and the jQuery contributors for creating and maintaining such a useful and, for the time,  essential  library. . Over the years, GitHub grew into a company with hundreds of engineers and a dedicated team gradually formed to take responsibility for the size and quality of JavaScript code that we serve to web browsers. One of the things that we’re constantly on the lookout for is technical debt, and sometimes technical debt grows around dependenices that once provided value, but whose value dropped over time. . When it came to jQuery, we compared it against the rapid evolution of supported web standard in modern browsers and realized: . Furthermore, the chaining syntax didn’t satisfy how we wanted to write code going forward. For example: . This syntax is simple to write, but to our standards, doesn’t communicate intent really well. Did the author expect one or more  js-widget  elements on this page? Also, if we update our page markup and accidentally leave out the  js-widget  classname, will an exception in the browser inform us that something went wrong? By default, jQuery silently skips the whole expresion when nothing matched the initial selector; but to us, such behavior was a bug rather than a feature. . Finally, we wanted to start annotating types with  Flow  to perform static type checking at build time, and we concluded that the chaining syntax doesn’t lend itself well to static analysis, since almost every result of a jQuery method call is of the same type. We chose Flow over alternatives because, at the time, features such as  @flow weak  mode allowed us to progressively and efficiently start applying types to a codebase which was largely untyped. . All in all, decoupling from jQuery would mean that we could rely on web standards more, have  MDN web docs  be de-facto default documentation for our frontend developers, maintain more resilient code in the future, and eventually drop a 30 kB dependency from our packaged bundles, speeding up page load times and JavaScript execution times. . Even with an end goal in sight, we knew that it wouldn’t be feasible to just allocate all resources we had to rewriting everything from jQuery to vanilla JS. If anything, such a rushed endeavor would likely lead to many regressions in site functionality that we would later have to weed out. Instead, we: . Instead of having to rewrite all of those call sites at once to the new approach, we’ve opted to trigger  fake   ajax*  lifecycle events and keep these forms submitting their contents asynchronously as before; only this time  fetch()  was used internally. . With these and similar efforts combined over the years, we were able gradually reduce our dependence on jQuery until there was not a single line of code referencing it anymore. . One technology that has been making waves in the recent years is  Custom Elements : a component library native to the browser, which means that there are no additional bytes of a framework for the user to download, parse and compile. . We had created a few Custom Elements based on the v0 specification since 2014. However, as standards were still in flux back then, we did not invest as much. It was not until 2017 when the Web Components v1 spec was released and implemented in both Chrome and Safari that  we began to adopt Custom Elements  on a wider scale. . During the jQuery migration, we looked for patterns that would be suitable for extraction as custom elements. For example, we converted our facebox usage for displaying modal dialogs to the   &lt;details-dialog&gt;  element . . Our general philosophy of striving for progressive enhancement extends to custom elements as well. This means that we keep as much of the content in markup as possible and only add behaviors on top of that. For example,  &lt;local-time&gt;  shows the raw timestamp by default and gets upgraded to translate the time to the local timezone, while  &lt;details-dialog&gt; , when nested in the  &lt;details&gt;  element, is interactive even without JavaScript, but gets upgraded with accessibility enhancements. . Here is an example of how a  &lt;local-time&gt;  custom element could be implemented: . One aspect of Web Components that we’re looking forward to adopting is  Shadow DOM . The powerful nature of Shadow DOM has the potential to unlock a lot of possibilities for the web, but that also makes it harder to polyfill. Because polyfilling it today incurs a performance penalty even for code that manipulates parts of the DOM  unrelated  to web components, it is unfeasible for us to start using it in production. . These are the polyfills that helped us transition to using standard browser features. We try to serve most of these polyfills only when absolutely necessary, i.e. to outdated browsers as part of a separate “compatibility” JavaScript bundle. .     .     .     .     ", "date": "September 6, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tTowards Natural Language Semantic Code Search\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2018-09-18-towards-natural-language-semantic-code-search/", "abstract": "   . Searching code on GitHub is currently limited to keyword search. This assumes either the user knows the syntax, or can anticipate what keywords might be in comments surrounding the code they are looking for. Our machine learning scientists have been researching ways to enable the  semantic search  of code. . To fully grasp the concept of semantic search, consider the below search query,  “ping REST api and return results” : .   . Note that the demonstrated semantic search returns reasonable results even though there are no keywords in common between the search query and the text (the code &amp; comments found do not contain the words “Ping”, “REST” or “api”)! The implications of augmenting keyword search with semantic search are profound. For example, such a capability would expedite the process of on-boarding new software engineers onto projects and bolster the discoverability of code in general. . In this post, we want to share how we are leveraging deep learning to make progress towards this goal.  We also share an open source example with code and data that you can use to reproduce these results!  . One of the key areas of machine learning research underway at GitHub is representation learning of entities, such as repos, code, issues, profiles and users. We have made significant progress towards enabling semantic search by learning representations of code that share a common vector space as text. For example, consider the below diagram: .   . In the above example, Text 2 (blue) is a reasonable description of the code, whereas Text 1 (red) is not related to the code at all. Our goal is to learn representations where (text, code) pairs that describe the same concept are close neighbors, whereas unrelated (text, code) pairs are further apart. By representing text and code in the same vector space, we can vectorize a user’s search query and lookup the nearest neighbor that represents code. Below is a four-part description of the approach we are currently using to accomplish this task: . In order to learn a representation of code, we train a  sequence-to-sequence model  that learns to summarize code. A way to accomplish this for Python is to supply (code, docstring) pairs where the docstring is the target variable the model is trying to predict. One active area of research for us is incorporating domain specific optimizations like  tree-based LSTMs ,  gated-graph networks  and syntax-aware tokenization. Below is a screenshot that showcases the code summarizer model at work. In this example, there are two python functions supplied as input, and in both cases the model produces a reasonable summary of the code as output: .   . It should be noted that in the above examples, the model produces the summary by using the entire code blob, not merely the function name. . Building a code summarizer is a very exciting project on its own, however, we can utilize the encoder from this model as a general purpose feature extractor for code. After extracting the encoder from this model, we can  fine-tune  it for the task of mapping code to the vector space of natural language. . We can evaluate this model objectively using the  BLEU score . Currently we have been able to achieve a BLEU score of 13.5 on a holdout set of python code, using the  fairseq-py library  for sequence to sequence models. . In addition to learning a representation for code, we needed to find a suitable representation for short phrases (like sentences found in Python docstrings). Initially, we experimented with the  Universal Sentence Encoder , a pre-trained encoder for text that is  available on TensorFlow Hub . While the embeddings from worked reasonably well, we found that it was advantageous to learn embeddings that were specific to the vocabulary and semantics of software development. One area of ongoing research involves evaluating different domain-specific corpuses for training our own model, ranging from GitHub issues to third party datasets. . To learn this representation of phrases, we trained a  neural language model  by leveraging the  fast.ai  library. This library gave us easy access to state of the art architectures such as  AWD LSTMs , and to techniques such as  cyclical learning rates  with random restarts. We extracted representations of phrases from this model by summarizing the hidden states using the concat pooling approach found in  this paper . . One of the most challenging aspects of this exercise was to evaluate the quality of these embeddings. We are currently building a variety of downstream supervised tasks similar to those  outlined here  that will aid us in evaluating the quality of these embeddings objectively. In the meantime, we sanity check our embeddings by manually examining the similarity between similar phrases. The below screenshot illustrates examples where we search the vectorized docstrings for similarity against user-supplied phrases: .   . Next, we map the code representations we learned from the code summarization model (part 1) to the vector space of text. We accomplish this by fine-tuning the encoder of this model. The inputs to this model are still code blobs, however the target variable the model is now the vectorized version of docstrings. These docstrings are vectorized using the approach discussed in the previous section. . Concretely, we perform multi-dimensional regression with  cosine proximity loss  to bring the hidden state of the encoder into the same vector-space as text. . We are actively researching alternate approaches that directly learn a joint vector space of code and natural language, borrowing from some ideas outlined  here . . Finally, after successfully creating a model that can vectorize code into the same vector-space as text, we can create a semantic search mechanism. In its most simple form, we can store the vectorized version of all code in a database, and perform nearest neighbor lookups to a vectorized search query. . Another active area of our research is determining the best way to augment existing keyword search with semantic results and how to incorporate additional information such as context and relevance.  Furthermore, we are actively exploring ways to evaluate the quality of search results that will allow us to iterate quickly on this problem. We leave these topics for discussion in a future blog post. . The below diagram summarizes all the steps in our current semantic-search workflow: .   . We are exploring ways to improve almost every component of this approach, including data preparation, model architecture, evaluation procedures, and overall system design. What is described in this blog post is only a minimal example that scratches the surface. . Our  open-source end-to-end tutorial  contains a detailed walkthrough of the approach outlined in this blog, along with code and data you can use to reproduce the results. . This open source example (with some modifications) is also used as a tutorial for the  kubeflow  project, which is implemented  here . . We believe that semantic code search will be most useful for targeted searches of code within specific entities such as repos, organizations or users as opposed to general purpose “how to” queries. The  live demonstration  of semantic code search hosted on our  recently announced Experiments site  does not allow users to perform targeted searches of repos. Instead, this demonstration is designed to share a taste of what might be possible and searches only a limited, static set of python code. . Furthermore, like all machine learning techniques, the efficacy of this approach is limited by the training data used. For example, the data used to train these models are (code, docstring) pairs. Therefore, search queries that closely resemble a docstring have the greatest chance of success. On the other hand, queries that do not resemble a docstring or contain concepts for which there is little data may not yield sensible results. Therefore, it is not difficult to challenge our  live demonstration  and discover the limitations of this approach. Nevertheless, our initial results indicate that this is an extremely fruitful area of research that we are excited to share with you. . There are many more use cases for semantic code search. For example, we could extend the ideas presented here to allow users to search for code using the language of their choice (French, Mandarin, Arabic, etc.) across many different programming languages simultaneously. . This is an exciting time for the machine learning research team at GitHub and we are looking to expand. If our work interests you, please  get in touch ! ", "date": "September 18, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tUpgrading GitHub from Rails 3.2 to 5.2\t\t", "author": ["\n\t\tEileen M. Uchitelle\t"], "link": "https://github.blog/2018-09-28-upgrading-github-from-rails-3-2-to-5-2/", "abstract": " On August 15th GitHub celebrated a major milestone: our main application is now running on the latest version of Rails: 5.2.1! 🎉 . In total the project took a year and a half to upgrade from Rails 3.2 to Rails 5.2. Along the way we took time to clean up technical debt and improve the overall codebase while doing the upgrade. Below we’ll talk about how we upgraded Rails, lessons we learned and whether we’d do it again. . Upgrading Rails on an application as large and as trafficked as GitHub is no small task. It takes careful planning, good organization, and patience. The upgrade started out as kind of a hobby; engineers would work on it when they had free time. There was no dedicated team. As we made progress and gained traction it became not only something we hoped we could do, but a priority. . Since GitHub is so important to our community, we can’t stop feature development or bug fixes in order to upgrade Rails. . Instead of using a long-running branch to upgrade Rails we added the ability to dual boot the application in multiple versions of Rails. We created two Gemfile.lock’s: one for the current version  Gemfile.lock  and one for the future version  Gemfile_next.lock . The dual booting allows us to regularly deploy changes for the next version to GitHub without requiring long running branches or altering how production works. We do this by conditionally loading the code. . Each time we got a minor version of Rails green we’d make the CI job required for all pushes to the GitHub application and start work on the next version. While we worked on Rails 4.1 a CI job would run on every push for 3.2 and 4.0. When 4.1 was green we’d swap out 4.1 for 4.0 and get to work on 4.2. This allowed us to prevent regressions once a version of Rails was green, and time for engineers to get used to writing code that worked in multiple versions of Rails. . The two versions that we deployed were 4.2 and 5.2. We deployed 4.2 because it was a huge milestone and was the first version of Rails that hadn’t been EOL’d yet (as an aside: we’d been backporting security fixes to 3.2 but not to 4.0+ so we couldn’t deploy 4.0 or 4.1. Rest assured your security is our top priority). . To roll out the Rails upgrade we created a careful and iterative process. We’d first deploy to our testing environment and requested volunteers from each team to click test their area of the codebase to find any regressions the test suite missed. . After fixing those regressions, we deployed in off-hours to a percentage of production servers. During each deploy we’d collect data about exceptions and performance of the site. With that information we’d fix bugs that came up and repeat those steps until the error rate was low enough be considered equal to the previous version. We merged the upgrade once we could deploy to full production for 30 minutes at peak traffic with no visible impact. . This process allowed us to deploy 4.2 and 5.2 with minimal customer impact and no down time. . The Rails upgrade took a year and a half. This was for a few reasons. First, Rails upgrades weren’t always smooth and some versions had major breaking changes. Rails improved the upgrade process for the 5 series so this meant that while 3.2 to 4.2 took 1 year, 4.2 to 5.2 only took 5 months. . Another reason is the GitHub codebase is 10 years old. Over the years technical debt builds and there’s bound to be gremlins lurking in the codebase. If you’re on an old version of Rails, your engineers will need to add more monkey patches or implement features that exist upstream. . Lastly, when we started it wasn’t clear what resources were needed to support the upgrade and since most of us had never done a Rails upgrade before, we were learning as we went. The project originally began with 1 full-time engineer and a small army of volunteers. We grew that team to 4 full-time engineers plus the volunteers. Each version bump meant we learned more and the next version went even faster. . Through this work we learned some important lessons that we hope make your next upgrade easier: . Absolutely. . Upgrading Rails has allowed us to address technical debt in our application. Our test suite is now closer to vanilla Rails, we were able to remove StateMachine in favor of Active Record enums, and start replacing our job runner with Active Job. And that’s just the beginning. . Rails upgrades are a lot of hard work and can be time-consuming, but they also open up a ton possibilities. We can push more of our tooling upstream to Rails, address areas of technical debt, and be one of the largest codebases running on the most recent version of Rails. Not only does this benefit us at GitHub, it benefits our customers and the open source community. ", "date": "September 28, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tApplying machine intelligence to GitHub security alerts\t\t", "author": ["\n\t\tBen Thompson\t"], "link": "https://github.blog/2018-10-09-applying-machine-intelligence-to-security-alerts/", "abstract": " Last year, we released  security alerts  that track security vulnerabilities in Ruby and JavaScript packages. Since then, we’ve identified  more than four million of these vulnerabilities  and added support for Python.  In our launch post , we mentioned that all vulnerabilities with CVE IDs are included in security alerts, but sometimes there are vulnerabilities that are not disclosed in the  National Vulnerability Database . Fortunately, our collection of security alerts can be supplemented with vulnerabilities detected from activity within our developer community. . There are many places a project can publicize security fixes within a new version: the CVE feed, various mailing lists, and open source groups, or even within its release notes or changelog. Regardless of how projects share this information, some developers within the GitHub community will see the advisory and immediately bump their required versions of the dependency to a known safe version. If detected, we can use the information in these commits to generate security alerts for vulnerabilities which may not have been published in the CVE feed. . On an average day, the dependency graph can track around 10,000 commits to dependency files for any of our supported languages. We can’t manually process this many commits. Instead, we depend on machine intelligence to sift through them and extract those that might be related to a security release. . For this purpose, we created a machine learning model that scans text associated with public commits (the commit message and linked issues or pull requests) to filter out those related to possible security upgrades. With this smaller batch of commits, the model uses the diff to understand how required version ranges have changed. Then it aggregates across a specific timeframe to get a holistic view of all dependencies that a security release might affect. Finally, the model outputs a list of packages and version ranges it thinks require an alert and currently aren’t covered by any known CVE in our system. . No machine learning model is perfect. While machine intelligence can sift through thousands of commits in an instant, this anomaly-detection algorithm will still generate false positives for packages where no security patch was released. Security alert quality is a focus for us, so we review all model output before the community receives an alert. . Interested in learning more? Join us at  GitHub Universe  next week to explore the connections that push technology forward and keep projects secure through talks, trainings, and workshops. Tune in to the blog October 16-17 for more updates and announcements. ", "date": "October 9, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tBehind the scenes of GitHub Token Scanning\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-10-17-behind-the-scenes-of-github-token-scanning/", "abstract": " Several years ago we  started scanning  all pushes to public repositories for  GitHub OAuth tokens  and  personal access tokens . Now we’re extending this capability to include tokens from cloud service providers and additional credentials, such as unencrypted SSH private keys associated with a user’s GitHub account. . We live in amazing times for software development. Capabilities that were once only available to large technology companies are now accessible to the smallest of startups. Developers can leverage cloud services to quickly perform continuous integration testing, deploy their code to fully scalable infrastructure, accept credit card payments from customers, and nearly anything else you can imagine. . Composing cloud services like this is the norm going forward, but it comes with inherent security complexities. Each cloud service a developer typically uses requires one or more credentials, often in the form of API tokens. In the wrong hands, they can be used to access sensitive customer data—or vast computing resources for mining cryptocurrency, presenting significant risks to both users and cloud service providers. .   Fig 1:  Your master branch might look innocent, but exposed credentials in your project’s history could prove costly.  . GitHub, and our users, aren’t immune to this problem. That is why we started scanning for GitHub OAuth tokens in public repositories several years ago. But, our users are not just GitHub customers. They are also customers of cloud infrastructure providers, cloud payment processing providers, and other cloud service providers that have become commonplace in modern development. . Our existing solution focused on GitHub OAuth tokens exclusively and was never designed for extensibility. The existing code leveraged hand-tuned assembly that was extremely fast at finding 40-hex character strings (the format of GitHub OAuth tokens). This bit of code was patched into Git and run inline whenever code was pushed to GitHub. It was an amazing piece of work, but could not support multiple credential formats. Our vision was to support all of the popular cloud service providers. . So began our journey into the next generation of GitHub Token Scanning. The obvious path to a more extensible scanner is some form of regular expression support. However, scanning for credentials using typical regular expression libraries doesn’t scale from a performance perspective, as they optimize for a slightly different problem than the one we have. . The vast majority of regular expression libraries are designed to return the first match in a set of patterns. Given we have at least one pattern for each cloud service provider, we require all matches are returned and not just the first. The only way to ensure this with traditional libraries is to scan a given input once for each pattern. However, this increases the scan time dramatically for large repositories or large sets of patterns. Fortunately, scanning Git data for credentials is just a specific case of a general problem. For example, high-performance application-level firewalls similarly need to scan high-volume network traffic for sets of patterns to identify known viruses or malware. If you squint, scanning high-volume Git push data for credentials is a very similar problem. . Our research eventually lead us to a GitHub repository hosting the amazing  Hyperscan library  by Intel. This library is incredibly performant and provides exactly what we need. We will explore the technical details in more depth in a follow-up engineering post. But, in short, Hyperscan let us replace all of the assembly code patches to Git with a new standalone scanner, written in Go, that has scaled nicely. . In parallel with working on the implementation, we reached out to several cloud service providers we thought would be interested in testing out Token Scanning in a private beta. They were all enthusiastic to participate, as many of them had contacted us in the past looking for a solution to this widespread problem. . Since April, we’ve worked with cloud service providers in private beta to scan all changes to public repositories and public Gists for credentials (GitHub doesn’t scan private code). Each candidate credential is sent to the provider, including some basic metadata such as the repository name and the commit that introduced the credential. The provider can then validate the credential and decide if the credential should be revoked depending on the associated risks to the user or provider. Either way, the provider typically contacts the owner of the credential, letting them know what occurred and what action was taken. . We have received amazing feedback from both providers and users during the private beta. Cloud service providers have told us that GitHub Token Scanning has been tremendously effective in helping them identify credentials before malicious users. And, while GitHub users were not aware that Token Scanning was in beta, we did take notice of a tweet from a GitHub user that included a number of enthusiastic exclamation marks and 🎉 emojis. This user was extremely grateful for having received a notification from a participating cloud service provider less than a minute after they had accidentally pushed a highly sensitive credential to a public repository. . During the beta we have scanned millions of public repository changes and identified millions of candidate credentials. As announced yesterday at GitHub Universe, Token Scanning is now in public beta, and supports an increasing number of cloud providers. We’re excited by the impact of Token Scanning today and have lots of ideas about how to make it even more powerful in the future. Dealing with credentials is an unavoidable part of modern development. With GitHub by your side, we hope to minimize the security impact of such accidents. ", "date": "October 17, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tFour years of the GitHub Security Bug Bounty\t\t", "author": ["\n\t\tGreg Ose\t"], "link": "https://github.blog/2018-03-14-four-years-of-bug-bounty/", "abstract": " Last month GitHub celebrated the fourth year of our Security Bug Bounty program. As we’ve done in the past, we’re sharing some details and highlights from 2017 and looking ahead to where we see the program going in 2018. . Last year was our biggest year yet as our Bug Bounty program continued to grow in participation by researchers, program initiatives, and the rewards paid out. . Diving straight into the numbers, we can review the details of this growth. In 2017, we reviewed and triaged a total of 840 submissions to the program. Of these submissions, we resolved and rewarded a total of 121 reports with an average payout of $1,376 (and swag!). Compared to our  previous statistics for 2016 , this was a significant increase from 48 out of 795 reports being resolved. In 2017, our rate of valid reports increased from 6% to almost 15%.   . Our total payouts also saw a significant increase from $95,300 in 2016 to $166,495 in 2017. We attribute this to the increased number of valid reports and in October we took time to re-evaluate our payout structure. Corresponding with  HackerOne’s Hack the World competition , we  doubled  our payout amounts across the board, bringing our minimum and maximum payouts to $555 and $20,000, bringing our bug bounty in line with the industry’s top programs. . To accelerate our program’s growth in 2017, we launched a number of initiatives to help engage researchers. Among the changes to the program was the introduction of GitHub Enterprise to the scope of the Bug Bounty program, which allowed researchers to focus on areas of our applications that may not be exposed on GitHub.com or are specific to certain enterprise deployments. In the beginning of 2017, a number of reports impacting our enterprise authentication methods prompted us to not only focus on this internally, but also identify how we could engage researchers to focus on this functionality. To promote a more targeted review of these critical code paths we kicked off two new initiatives beyond our public Bug Bounty program. . Providing researcher grants is something that has been on our radar since Google launched  their Vulnerability Research Grants  in 2015. The basic premise is that we pay a fixed amount to a researcher to dig into a specific feature or area of the application. In addition to the fixed payment for the grant, any vulnerabilities identified would also be paid out through the Bug Bounty program. During the beginning of the year, we identified a researcher with specialty in assessing troublesome enterprise authentication methods. We reached out and launched our first researcher grant. We couldn’t have been happier with the results. It provided a depth of expertise and review that was well worth the extra monetary incentive. . In March 2017 we  launched GitHub for Business , bringing  enterprise authentication to organizations on GitHub.com. We used this feature launch as an opportunity to roll out a new part of the Bug Bounty program: private bug bounties. Through a private program on HackerOne, we reached out to all researchers who had previously participated in our program and allowed them access to this functionality before its public launch. This added to our internal pre-ship security assessments with review by external researchers and helped us identify and remediate issues before general exposure. With the extra review, we were able to limit the impact of vulnerabilities in production while also providing fresh code and functionality for researchers to look into. . Internal improvements to the program have helped us more efficiently triage and remediate submissions from researchers. ChatOps and GitHub-based workflows are core to how we deal with incoming submissions. As soon as new ones arrive, we receive alerts in Slack using  HackerOne’s Slack integration . From there, we can triage issues directly from chat, letting the team know which issues are critical and which can wait until later. At the end of our triage workflow, we use ChatOps to issue rewards through HackerOne, so we can close the loop and pay researchers as quickly as possible. . To support these workflows, we’ve continued to build on our  Ruby on Rails HackerOne API client  and extensively use these and GitHub APIs in our internal processes. . So far, these improvements have made us significantly more efficient. Our average response time in 2017 was 10 hours, valid issues were triaged to developers on average within two days, and bounties were rewarded on average in 17 days. Given the time and effort that researchers dedicate to participating in our program, we feel great about these improvements. And in 2018, we’ll continue to refine our process. We’re always looking for ways to make sure our researchers receive a prompt and satisfactory response to their submissions. . Also in 2018, we’re planning to expand the initiatives that proved so successful last year. We’ll be launching more private bounties and research grants to gain focus on specific features both before and after they publicly launch. Later in the year, we’ll announce additional promotions to continue to keep researchers interested and excited to participate. . Given the program’s success, we’re also looking to see how we can expand its scope to help secure our production services and protect GitHub’s ecosystem. We’re excited for what’s next and look forward to  triaging and fixing your submissions  this year! ", "date": "March 14, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tUsing Figma designs to build the Octicons icon library\t\t", "author": ["\n\t\tJon Rohan\t"], "link": "https://github.blog/2018-04-12-driving-changes-from-designs/", "abstract": " Recently our friends at Figma announced their  new Figma platform , and we’ve been really excited about its potential. We’ve immediately put the platform to use with  Octicons , our SVG icon library. . Previously, we checked our asset files into the GitHub repository. This workflow was restrictive and confusing for contributors who might want to iterate on or update an Octicon. We wanted anyone to be able to make contributions, but they needed all of these things to work in order to contribute. . To support your project’s contributors it’s important to make the contributing experience as frictionless as possible. Migrating our Octicons to Figma let us cut out painful steps in our previous workflow. Having their API available for automating the work has allowed contributors to contribute using powerful platform-agnostic design tools without any overly complex setup. . Robots are great for doing repeatable tasks, and handing that work off to automated systems frees us up to think about the big picture. We lean on continuous integration to build, export, and distribute the icons. . On every pull request we use CI to export our icons from the file and distribute alpha versions of the libraries. .   . We also take advantage of  Probot , an out-of-the-box robot that makes automating GitHub tasks easy. . Probot has the ability to check our pull requests on Octicons and look for changes in the Figma source URL. When this occurs,  our Probot app  will query Figma’s platform and look for changes to any of the icons. When it finds those changes, it will comment on the pull request with  before and after images . This makes the process easier for both contributors and maintainers. .   . The API’s potential is the most exciting part, and we can’t wait to see how it improves our workflow. The Design Systems Team at GitHub are designers with an engineering focus. We want to keep our components in code, then make them available for our designers to prototype. . The  upcoming Figma write API  will allow us to maintain our component library in code and export those as Figma components. Using a team library we can publish updates and make them available to the GitHub Product Design Team to use in their design mockups and prototypes. ", "date": "April 12, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tMySQL High Availability at GitHub\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2018-06-20-mysql-high-availability-at-github/", "abstract": " GitHub uses MySQL as its main datastore for all things non- git , and its availability is critical to GitHub’s operation. The site itself, GitHub’s API, authentication and more, all require database access. We run multiple MySQL clusters serving our different services and tasks. Our clusters use classic primary-replicas setup, where a single node in a cluster (the  primary ) is able to accept writes. The rest of the cluster nodes (the  replicas ) asynchronously replay changes from the primary and serve our read traffic. . The availability of primary nodes is particularly critical. With no primary, a cluster cannot accept writes: any writes that need to be persisted cannot be persisted. Any incoming changes such as commits, issues, user creation, reviews, new repositories, etc., would fail. . To support writes we clearly need to have an available writer node, a primary of a cluster. But just as important, we need to be able to identify, or  discover , that node. . On a failure, say a primary box crash scenario, we must ensure the existence of a new primary, as well as be able to quickly advertise its identity. The time it takes to detect a failure, run the failover and advertise the new primary’s identity makes up the total outage time. . This post illustrates GitHub’s MySQL high availability and primary service discovery solution, which allows us to reliably run a cross-data-center operation, be tolerant of data center isolation, and achieve short outage times on a failure. . The solution described in this post iterates on, and improves, previous high availability (HA) solutions implemented at GitHub. As we scale, our MySQL HA strategy must adapt to changes. We wish to have similar HA strategies for our MySQL and for other services within GitHub. . When considering high availability and service discovery, some questions can guide your path into an appropriate solution. An incomplete list may include: . To illustrate some of the above, let’s first consider our previous HA iteration, and why we changed it. . In our previous iteration, we used: . In that iteration, clients discovered the writer node by using a name, e.g.  mysql-writer-1.github.net . The name resolved to a Virtual IP address (VIP) which the primary host would acquire. . Thus, on a normal day, clients would just resolve the name, connect to the resolved IP, and find the primary listening on the other side. . Consider this replication topology, spanning three different data centers: . In the event of a primary failure, a new server, one of the replicas, must be promoted in its place. .  orchestrator  will detect a failure, promote a new primary, and then act to reassign the name/VIP. Clients don’t actually know the identity of the primary: all they have is a  name , and that name must now resolve to the new primary. However, consider: . VIPs are cooperative: they are claimed and owned by the database servers themselves. To acquire or release a VIP, a server must send an ARP request. The server owning the VIP must first release it before the newly promoted primary acquires it. This has some undesired effects: . In parts of our setup VIPs are bound by physical location. They are owned by a switch or a router. Thus, we can only reassign the VIPs onto co-located servers. In particular, in some cases we cannot assign the VIP to a server promoted in a different data center, and must make a DNS change. . These limitations alone were enough to push us in search of a new solution, but for even more consideration were: . These extra steps were a contributing factor to the total outage time and introduced their own failures and friction. . The solution worked, and GitHub has had successful MySQL failovers that went well under the radar, but we wanted our HA to improve on the following: . Our new strategy, along with collateral improvements, solves or mitigates much of the concerns above. In today’s HA setup, we have: . The new setup removes VIP and DNS changes altogether. And while we introduce more components, we are able to decouple the components and simplify the task, as well as be able to utilize solid and stable solutions. A breakdown follows. . On a normal day the apps connect to the write nodes through GLB/HAProxy. . The apps are never aware of the primary’s identity. As before, they use a name. For example, the primary for  cluster1  would be  mysql-writer-1.github.net . In our current setup, however, this name gets resolved to an  anycast  IP. . With  anycast , the name resolves to the same IP everywhere, but traffic is routed differently based on a client’s location. In particular, in each of our data centers we have GLB, our highly available load balancer, deployed on multiple boxes. Traffic to  mysql-writer-1.github.net  always routes to the local data center’s GLB cluster. Thus, all clients are served by local proxies. . We run GLB on top of  HAProxy . Our HAProxy has  writer pools : one pool per MySQL cluster, where each pool has exactly one backend server: the cluster’s  primary . All GLB/HAProxy boxes in all DCs have the exact same pools, and they all indicate the exact same backend servers in these pools. Thus, if an app wishes to write to  mysql-writer-1.github.net , it matters not which GLB server it connects to. It will always get routed to the actual  cluster1  primary node. . As far as the apps are concerned, discovery ends at GLB, and there is never a need for re-discovery. It’s all on GLB to route the traffic to the correct destination. . How does GLB know which servers to list as backends, and how do we propagate changes to GLB? . Consul is well known as a service discovery solution, and also offers DNS services. In our solution, however, we utilize it as a highly available key-value (KV) store. . Within Consul’s KV store we write the identities of cluster primarys. For each cluster, there’s a set of KV entries indicating the cluster’s primary  fqdn , port, ipv4, ipv6. . Each GLB/HAProxy node runs  consul-template : a service that listens on changes to Consul data (in our case: changes to clusters primarys data).  consul-template  produces a valid config file and is able to reload HAProxy upon changes to the config. . Thus, a change in Consul to a primary’s identity is observed by each GLB/HAProxy box, which then reconfigures itself, sets the new primary as the single entity in a cluster’s backend pool, and reloads to reflect those changes. . At GitHub we have a Consul setup in each data center, and each setup is highly available. However, these setups are independent of each other. They do not replicate between each other and do not share any data. . How does Consul get told of changes, and how is the information distributed cross-DC? . We run an  orchestrator/raft  setup:  orchestrator  nodes communicate to each other via  raft  consensus. We have one or two  orchestrator  nodes per data center. .  orchestrator  is charged with failure detection, with MySQL failover, and with communicating the change of primary to Consul. Failover is operated by the single  orchestrator/raft  leader node, but the  change , the news that a cluster now has a new primary, is propagated to all  orchestrator  nodes through the  raft  mechanism. . As  orchestrator  nodes receive the news of a primary change, they each communicate to their local Consul setups: they each invoke a KV write. DCs with more than one  orchestrator  representative will have multiple (identical) writes to Consul. . In a primary crash scenario: . There is a clear ownership of responsibilities for each component, and the entire design is both decoupled as well as simplified.  orchestrator  doesn’t know about the load balancers. Consul doesn’t need to know where the information came from. Proxies only care about Consul. Clients only care about the proxy. . Furthermore: . To further secure the flow, we also have the following: . We further tackle concerns and pursue HA objectives in the following sections. .  orchestrator  uses a  holistic approach  to detecting failure, and as such it is very reliable. We do not observe false positives: we do not have premature failovers, and thus do not suffer unnecessary outage time. .  orchestrator/raft  further tackles the case for a complete DC network isolation (aka DC fencing). A DC network isolation can cause confusion: servers within that DC can talk to each other. Is it  they  that are network isolated from other DCs, or is it  other DCs  that are being network isolated? . In an  orchestrator/raft  setup, the  raft  leader node is the one to run the failovers. A leader is a node that gets the support of the majority of the group (quorum). Our  orchestrator  node deployment is such that no single data center makes a majority, and any  n-1  DCs do. . In the event of a complete DC network isolation, the  orchestrator  nodes in that DC get disconnected from their peers in other DCs. As a result, the  orchestrator  nodes in the isolated DC cannot be the leaders of the  raft  cluster. If any such node did happen to be the leader, it steps down. A new leader will be assigned from any of the other DCs. That leader will have the support of all the other DCs, which are capable of communicating between themselves. . Thus, the  orchestrator  node that calls the shots will be one that is outside the network isolated data center. Should there be a primary in an isolated DC,  orchestrator  will initiate the failover to replace it with a server in one of the available DCs. We mitigate DC isolation by delegating the decision making to the quorum in the non-isolated DCs. . Total outage time can further be reduced by advertising the primary change sooner. How can that be achieved? . When  orchestrator  begins a failover, it observes the fleet of servers available to be promoted. Understanding replication rules and abiding by hints and limitations, it is able to make an educated decision on the best course of action. . It may recognize that a server available for promotion is also an  ideal candidate , such that: . In such a case  orchestrator  proceeds to first set the server as writable, and immediately advertises the promotion of the server (writes to Consul KV in our case), even while asynchronously beginning to fix the replication tree, an operation that will typically take a few more seconds. . It is likely that by the time our GLB servers have been fully reloaded, the replication tree is already intact, but it is not strictly required. The server is good to receive writes! . In MySQL’s  semi-synchronous replication  a primary does not acknowledge a transaction commit until the change is known to have shipped to one or more replicas. It provides a way to achieve lossless failovers: any change applied on the primary is either applied or waiting to be applied on one of the replicas. . Consistency comes with a cost: a risk to availability. Should no replica acknowledge receipt of changes, the primary will block and writes will stall. Fortunately, there is a timeout configuration, after which the primary can revert back to asynchronous replication mode, making writes available again. . We have set our timeout at a reasonably low value:  500ms . It is more than enough to ship changes from the primary to local DC replicas, and typically also to remote DCs. With this timeout we observe perfect semi-sync behavior (no fallback to asynchronous replication), as well as feel comfortable with a very short blocking period in case of acknowledgement failure. . We enable semi-sync on local DC replicas, and in the event of a primary’s death, we expect (though do not strictly enforce) a lossless failover. Lossless failover on a complete DC failure is costly and we do not expect it. . While experimenting with semi-sync timeout, we also observed a behavior that plays to our advantage: we are able to influence the identity of the  ideal candidate  in the event of a primary failure. By enabling semi-sync on designated servers, and by marking them as  candidates , we are able to reduce total outage time by  affecting  the outcome of a failure. In our  experiments  we observe that we typically end up with the  ideal candidates , and hence run quick advertisements. . Instead of managing the startup/shutdown of the  pt-heartbeat  service on promoted/demoted primarys, we opted to run it everywhere at all times. This required some  patching  so as to make  pt-heartbeat  comfortable with servers either changing their  read_only  state back and forth or completely crashing. . In our current setup  pt-heartbeat  services run on primarys and on replicas. On primarys, they generate the heartbeat events. On replicas, they identify that the servers are  read-only  and routinely recheck their status. As soon as a server is promoted as primary,  pt-heartbeat  on that server identifies the server as writable and begins injecting heartbeat events. . We further delegated to orchestrator: . On all things new-primary, this reduces friction. A primary that is just being promoted is clearly expected to be alive and accessible, or else we would not promote it. It makes sense, then, to let  orchestrator  apply changes directly to the promoted primary. . The proxy layer makes the apps unaware of the primary’s identity, but it also masks the apps’ identities from the primary. All the primary sees are connections coming from the proxy layer, and we lose information about the actual source of the connection. . As distributed systems go, we are still left with unhandled scenarios. . Notably, on a data center isolation scenario, and assuming a primary is in the isolated DC, apps in that DC are still able to write to the primary. This may result in state inconsistency once network is brought back up. We are working to mitigate this split-brain by implementing a reliable  STONITH  from within the very isolated DC. As before,  some time  will pass before bringing down the primary, and there could be a short period of split-brain. The operational cost of avoiding split-brains altogether is very high. . More scenarios exist: the outage of Consul at the time of the failover; partial DC isolation; others. We understand that with distributed systems of this nature it is impossible to close all of the loopholes, so we focus on the most important cases. . Our orchestrator/GLB/Consul setup provides us with: . The orchestration/proxy/service-discovery paradigm uses well known and trusted components in a decoupled architecture, which makes it easier to deploy, operate and observe, and where each component can independently scale up or down. We continue to seek improvements as we continuously test our setup. ", "date": "June 20, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tPerformance Impact of Removing OOBGC\t\t", "author": ["\n\t\tAaron Patterson\t"], "link": "https://github.blog/2018-05-18-removing-oobgc/", "abstract": " Until last week, GitHub used an Out of Band Garbage Collector (OOBGC) in production.  Since removing it, we decreased CPU time across our production machines by 10%.  Let’s talk about what an OOBGC is, when to use it, and when not to use it.  Then follow up with some statistics about the impact of removing it from GitHub’s stack. . An OOBGC is not really a Garbage Collector, but more of a technique to use when deciding  when  to collect garbage in your program.  Instead of allowing the GC to run normally, the GC is stopped before processing a web request, then restarted after the response has been sent to the client.  Meaning that garbage collection occurs “out of band” of request and response processing. . Ruby’s GC is a “stop the world, mark and sweep” collector.  Which means that when the GC runs, your program pauses, and when the GC finishes your program resumes.  The time your program is paused is called “pause time”, and while your program is paused it can’t do anything.  Historically, Ruby’s GC would pause the program for long periods of time.  We would rather clients don’t wait around for the GC to run, so only executing GC after each request made sense. . In the past years, Ruby’s Garbage Collector has undergone many performance improvements.  These changes include: becoming a generational collector, incremental marking, and lazy sweeping.  A generational collector reduces the overall amount of work the GC needs to do.  Incremental marking and lazy sweeping mean that the GC can execute concurrently with your program.  What these techniques add up to is less time spent in GC, and higher throughput of your program. . Since the OOBGC runs the GC after the response is finished, it can cause the web worker to take longer in order to be ready to process the next incoming request.  This means that clients can suffer from latency due to queuing wait times. . If a particular request doesn’t allocate enough garbage to warrant a GC execution under normal conditions, then the OOBGC could cause the process to do more work than it would have without the OOBGC. . Finally, the OOBGC can cause full collections (examining old and new objects) which defeats the generational GC optimizations. . GitHub has been using Ruby in production for a long time, and at the time adding an OOBGC made sense and worked well.  However, it is always good to question assumptions, especially after technological advancements such as the improvements made in Ruby’s GC.  We wanted to see if running an OOBGC was still necessary for our application after upgrading to Ruby 2.4, so we decided to remove it and observe the impact. . After removing the OOBGC, we saw a 10% drop in Kubernetes cluster CPU utilization: .   . This graph compares cluster CPU utilization from the current day, previous day, and previous week: .   . The blue line is CPU utilization for the day the patch went out.  You can see a great drop around 15:20. . This graph shows the difference in core utilization before and after OOBGC removal.  In other words “number of cores used yesterday” minus “number of cores used today”: .   . We saw a savings of between 400 and around 1000 cores depending on usage at that point in the day. . Finally, removing OOBGC reduced average response times by about 25% (the gray line is with OOBGC, the blue line is without): .   . Of course, removing OOBGC was not an all around win.  Incremental marking and lazy sweeping amortize the cost of memory collection over time.  This means that memory usage will increase on average, and that is what we observed in production: .   . For our application, the CPU savings far outdid the price we had to pay in average memory usage.  Removing the OOBGC from our system resulted in a great savings for our systems.  Taking measurements, acting on data, and questioning assumptions is one of the most difficult and fun parts of being an engineer.  This time it paid off for us, and hopefully this post can help you too! ", "date": "May 18, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tGLB: GitHub’s open source load balancer\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2018-08-08-glb-director-open-source-load-balancer/", "abstract": " At GitHub, we serve tens of thousands of requests every second out of our network edge, operating on  GitHub’s metal cloud . We’ve previously  introduced GLB , our scalable load balancing solution for bare metal datacenters, which powers the majority of GitHub’s public web and git traffic, as well as fronting some of our most critical internal systems such as  highly available MySQL clusters . Today we’re excited to share more details about our load balancer’s design, as well as release the GLB Director as open source. . GLB Director is a  Layer 4  load balancer which scales a single IP address across a large number of physical machines while attempting to minimise connection disruption during any change in servers. GLB Director does not replace services like haproxy and nginx, but rather is a layer in front of these services (or any TCP service) that allows them to scale across multiple physical machines without requiring each machine to have unique IP addresses. . The basic property of a Layer 4 load balancer is the ability to take a single IP address and spread inbound connections across multiple servers. To scale a single IP address to handle more traffic than any single machine can process, we need to not only split amongst backend servers, but also need to be able to scale up the servers that handle the load balancing themselves. This is essentially another layer of load balancing. . Typically we think of an IP address as referencing a single physical machine, and routers as moving a packet to the next closest router to that machine. In the simplest case where there’s always a single best next hop, routers pick that hop and forward all packets there until the destination is reached. . In reality, most networks are far more complicated. There is often more than a single path available between two machines, for example where multiple ISPs are available or even when two routers are joined together with more than one physical cable to increase capacity and provide redundancy. This is where  Equal-Cost Multi-Path (ECMP) routing  comes in to play – rather than routers picking a single best next hop, where they have multiple hops with the same cost (usually defined as the number of  ASes  to the destination), they instead hash traffic so that connections are balanced across all available paths of equal cost. . ECMP is implemented by hashing each packet to determine a relatively consistent selection of one of the available paths. The hash function used here varies by device, but typically it’s a  consistent hash  based on the source and destination IP address as well as the source and destination port for TCP traffic. This means that multiple packets for the same ongoing TCP connection will typically traverse the same path, meaning that packets will arrive in the same order even when paths have different latencies. Notably in this case, the paths can change without any disruption to connections because they will always end up at the same destination server, and at that point the path it took is mostly irrelevant. . An alternative use of ECMP can come in to play when we want to shard traffic across multiple  servers  rather than to the same server over multiple  paths . Each server can announce the same IP address with  BGP  or another similar network protocol, causing connections to be sharded across those servers, with the routers blissfully unaware that the connections are being handled in different places, not all ending on the same machine as would traditionally be the case. . While this shards traffic as we had hoped, it has one huge drawback: when the set of servers that are announcing the same IP change (or any path or router along the way changes), connections must rebalance to maintain an equal balance of connections on each server. Routers are typically stateless devices, simply making the best decision for each packet without consideration to the connection it is a part of, which means some connections will break in this scenario. . In the above example on the left, we can imagine that each colour represents an active connection. A new proxy server is added to announce the same IP. The router diligently adjusts the consistent hash to move 1/3 connections to the new server while keeping 2/3 connections where they were. Unfortunately for those 1/3 connections that were already in progress, the packets are now arriving on a server that doesn’t know about the connection, and so they fail. . The issue with the previous ECMP-only solution is that it isn’t aware of the full context for a given packet, nor is it able to store data for each packet/connection. As it turns out, there are commonly used patterns to help out with this situation by implementing some stateful tracking in software, typically using a tool like  Linux Virtual Server (LVS) . We create a new tier of “director” servers that take packets from the router via ECMP, but rather than relying on the router’s ECMP hashing to choose the backend proxy server, we instead control the hashing and store state (which backend was chosen) for all in-progress connections. When we change the set of proxy tier servers, the director tier hopefully hasn’t changed, and our connection will continue. . Although this works well in many cases, it does have some drawbacks. In the above example, we add both a LVS director and backend proxy server at the same time. The new director receives some set of packets, but doesn’t have any state yet (or has delayed state), so hashes it as a new connection and may get it wrong (and cause the connection to fail). A typical workaround with LVS is to use  multicast connection syncing  to keep the connection state shared amongst all LVS director servers. This still requires connection state to propagate, and also still requires duplicate state – not only does each proxy need state for each connection in the Linux kernel network stack, but  every  LVS director also needs to store a mapping of connection to backend proxy server. . When we were designing GLB, we decided we wanted to improve on this situation and not duplicate state at all. GLB takes a different approach to that described above, by using the flow state already stored in the proxy servers as part of maintaining established Linux TCP connections from clients. . For each incoming connection, we pick a primary and secondary server that could handle that connection. When a packet arrives on the primary server and isn’t valid, it is forwarded to the secondary server. The hashing to choose the primary/secondary server is done once, up front, and is stored in a lookup table, and so doesn’t need to be recalculated on a per-flow or per-packet basis. When a new proxy server is added, for 1/N connections it becomes the new primary, and the old primary becomes the secondary. This allows existing flows to complete, because the proxy server can make the  decisions with its local state , the single source of truth. Essentially this gives packets a “second chance” at arriving at the expected server that holds their state. . Even though the director will still send connections to the wrong server, that server will then know how to forward on the packet to the correct server. The GLB director tier is completely stateless in terms of TCP flows: director servers can come and go at any time, and will always pick the same primary/secondary server providing their forwarding tables match (but they rarely change). To change proxies, some care needs to be taken, which we describe below. . The core of the GLB Director design comes down to picking that primary and secondary server consistently, and to allow the proxy tier servers to drain and fill as needed. We consider each proxy server to have a state, and carefully adjust the state as a way of adding and removing servers. . We create a static binary forwarding table, which is generated identically on each director server, to map incoming flows to a given primary and secondary server. Rather than having complex logic to pick from all available servers at packet processing time, we instead use some indirection by creating a table (65k rows), with each row containing a primary and secondary server IP address. This is stored in memory as flat array of binary data, taking about 512kb per table. When a packet arrives, we consistently hash it (based on packet data alone) to the same row in that table (using the hash as an index into the array), which provides a consistent primary and secondary server pair. . We want each server to appear approximately equally in both the primary and secondary fields, and to never appear in both in the same row. When we add a new server, we desire some rows to have their primary become secondary, and the new server become primary. Similarly, we desire the new server to become secondary in some rows. When we remove a server, in any rows where it was primary, we want the secondary to become primary, and another server to pick up secondary. . This sounds complex, but can be summarised succinctly with a couple of  invariants : . Reading the problem that way,  Rendezvous hashing  is an ideal choice, since it can trivially satisfy these invariants. Each server (in our case, the IP) is hashed along with the row number, the servers are sorted by that hash (which is just a number), and we get a unique order for servers for that given row. We take the first two as the primary and secondary respectively. . Relative order will be maintained because the hash for each server will be the same regardless of which other servers are included. The only information required to generate the table is the IPs of the servers. Since we’re just sorting a set of servers, the servers only appear once. Finally, if we use a good hash function that is pseudo-random, the ordering will be pseudo-random, and so the distribution will be even as we expect. . Adding or removing proxy servers require some care in our design. This is because a forwarding table entry only defines a primary/secondary proxy, so the draining/failover only works with at most a single proxy host in draining. We define the following valid states and state transitions for a proxy server: . When a proxy server is  active ,  draining  or  filling , it is included in the forwarding table entries. In a stable state, all proxy servers are  active , and the rendezvous hashing described above will have an approximately even and random distribution of each proxy server in both the  primary  and  secondary  columns. . As a proxy server transitions to  draining , we adjust the entries in the forwarding table by swapping the  primary  and  secondary  entries we would have otherwise included: . This has the effect of sending packets to the server that was previously  secondary  first. Since it receives the packets first, it will accept SYN packets and therefore take any new connections. For any packet it doesn’t understand as relating to a local flow, it forwards it to the other server (the previous  primary ), which allows existing connections to complete. . This has the effect of draining the desired server of connections gracefully, after which point it can be removed completely, and proxies can shuffle in to fill the empty  secondary  slots: . A node in  filling  looks just like  active , since the table inherently allows a second chance: . This implementation requires that no more than one proxy server at a time is in any state other than  active , which in practise has worked well at GitHub. The state changes to proxy servers can happen as quickly as the longest connection duration that needs to be maintained. We’re working on extensions to the design that support more than just a primary and secondary, and some components (like the header listed below) already include initial support for arbitrary server lists. . We now have an algorithm to consistently pick backend proxy servers and operate on them, but how do we actually move packets around the datacenter? How do we encode the secondary server inside the packet so the primary can forward a packet it doesn’t understand? . Traditionally in the LVS setup, an  IP over IP (IPIP)  tunnel is used. The client IP packet is encapsulated inside an internal datacenter IP packet and forwarded on to the proxy server, which decapsulates it. We found that it was difficult to encode the additional server metadata inside IPIP packets, as the only standard space available was the  IP Options , and our datacenter routers passed packets with unknown IP options to software for processing (which they called “Layer 2 slow path”), taking speeds from millions to thousands of packets per second. . To avoid this, we needed to hide the data inside a different packet format that the router wouldn’t try to understand. We initially adopted raw  Foo-over-UDP (FOU)  with a custom  Generic Route Encapsulation (GRE)  payload, essentially encapsulating everything inside a UDP packet. We recently transitioned to  Generic UDP Encapsulation (GUE) , which is a layer on top FOU which provides a standard for encapsulating IP protocols inside a UDP packet. We place our secondary server’s IP inside the private data of the GUE header. From a router’s perspective, these packets are all internal datacenter UDP packets between two normal servers. . Another benefit to using UDP is that the source port can be filled in with a per-connection hash so that they are flow within the datacenter over different paths (where ECMP is used within the datacenter), and received on different RX queues on the proxy server’s NIC (which similarly use a hash of TCP/IP header fields). This is not possible with IPIP because most commodity datacenter NICs are only able to understand plain IP, TCP/IP and UDP/IP (and a few others). Notably, the NICs we use cannot look inside IP/IP packets. . When the proxy server wants to send a packet back to the client, it doesn’t need to be encapsulated or travel back through our director tier, it can be sent directly to the client (often called “Direct Server Return”). This is typical of this sort of load balancer design and is especially useful for content providers where the majority of traffic flows  outbound  with a relatively small amount of traffic  inbound . . This leaves us with a packet flow that looks like the following: . Since we first  publicly discussed our initial design , we’ve completely rewritten  glb-director  to use  DPDK , an open source project that allows  very  fast packet processing from userland by bypassing the Linux kernel. This has allowed us to achieve NIC line rate processing on commodity NICs with commodity CPUs, and allows us to trivially scale our director tier to handle as much inbound traffic as our public connectivity requires. This is particularly important during DDoS attacks, where we do not want our load balancer to be a bottleneck. . One of our initial goals with GLB was that our load balancer could run on commodity datacenter hardware without any server-specific physical configuration. Both GLB director and proxy servers are provisioned like normal servers in our datacenter. Each server has a  bonded pair of network interfaces , and those interfaces are shared between DPDK and Linux on GLB director servers. . Modern NICs support  SR-IOV , a technology that enables a single NIC to act like multiple NICs from the perspective of the operating system. This is typically used by virtual machine hypervisors to ask the real NIC (“Physical Function”) to create multiple pretend NICs for each VM (“Virtual Functions”). To enable DPDK and the Linux kernel to share NICs, we use  flow bifurcation , which sends specific traffic (destined to GLB-run IP addresses) to our DPDK process on a Virtual Function while leaving the rest of the packets with the Linux kernel’s networking stack on the Physical Function. . We’ve found that the packet processing rates of DPDK on a Virtual Function are acceptable for our requirements. GLB Director uses a  DPDK Packet Distributor  pattern to spread the work of encapsulating packets across any number of CPU cores on the machine, and since it is stateless this can be highly parallelised. . GLB Director supports matching and forwarding inbound IPv4 and IPv6 packets containing TCP payloads, as well as inbound ICMP Fragmentation Required messages used as part of  Path MTU Discovery , by peeking into the inner layers of the packet during matching. . One problem that typically arises in creating (or using) technologies that operate at high speeds due to using low-level primitives (like communicating with the NIC directly) is that they become significantly more difficult to test. As part of creating the GLB Director, we also created a test environment that supports simple end-to-end packet flow testing of our DPDK application, by leveraging the way DPDK provides an  Environment Abstraction Layer (EAL)  that allows a physical NIC and a libpcap-based local interface to appear the same from the view of the application. . This allowed us to write tests in  Scapy , a wonderfully simple Python library for reading, manipulating and writing packet data. By creating a Linux  Virtual Ethernet Device , with Scapy on one side and DPDK on the other, we were able to pass in custom crafted packets and validate what our software would provide on the other side, being a fully GUE-encapsulated packet directed to the expected backend proxy server. . This allows us to test more complex behaviours such as traversing layers of ICMPv4/ICMPv6 headers to retrieve the original IPs and TCP ports for correct forwarding of ICMP messages from external routers. . Part of the design of GLB is to handle server failure gracefully. The current design of having a designated primary/secondary for a given forwarding table entry / client means that we can work around single-server failure by running health checks from the perspective of each director. We run a service called  glb-healthcheck  which continually validates each backend server’s GUE tunnel and arbitrary HTTP port. . When a server fails, we swap the primary/secondary entries anywhere that server is primary. This performs a “soft drain” of the server, which provides the best chance for connections to gracefully fail over. If the healthcheck failure is a false positive, connections won’t be disrupted, they will just traverse a slightly different path. . The final component that makes up GLB is a  Netfilter  module and  iptables  target that runs on every proxy server and allows the “second chance” design to function. . This module provides a simple task deciding whether the inner TCP/IP packet inside every GUE packet is valid locally according to the Linux kernel TCP stack, and if it isn’t, forwards it to the next proxy server (the secondary) rather than decapsulating it locally. . In the case where a packet is a SYN (new connection) or is valid locally for an established connection, it simply accepts it locally. We then use the Linux kernel 4.x GUE support provided as part of the  fou  module to receive the GUE packet and process it locally. . When we started down the path of writing a better datacenter load balancer, we decided that we wanted to release it open source so that others could benefit from and share in our work. We’re excited to be releasing all the components discussed here as open source at  github/glb-director . We hope this will allow others to reuse our work and contribute to a common standard software load balancing solution that runs on commodity hardware in physical datacenter environments. . GLB and the GLB Director has been an ongoing project designed, authored, reviewed and supported by various members of GitHub’s Production Engineering organisation, including  @joewilliams ,  @nautalice ,  @ross ,  @theojulienne  and many others. If you’re interested in joining us in building great infrastructure projects like GLB, our Data Center team is hiring production engineers specialising in  Traffic Systems ,  Network  and  Facilities . ", "date": "August 8, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tTransit and Peering: How your requests reach GitHub\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-10-12-transit-and-peering-how-your-requests-reach-github/", "abstract": " GitHub is at a scale that provides exposure to interesting aspects of running a major site and are working to mature and level-up many parts of our infrastructure as we grow. One of the areas where this is evident is in how your requests find their destination using DNS and make their way into our sites over transit and peering. Many organizations are either too small to need to tackle these sorts of problems or so large they have groups to maintain existing solutions for each portion of them. It is really compelling to be able to directly work on such projects and closely with great engineers who are solving others still. . The foundation is built with links to Internet providers referred to as  transit . Conceptually they’re similar to the connection you have at home, a link through which you can reach any other point on the Internet. The biggest difference is that IP blocks can be announced as reachable through the link via  BGP . Like your home connection the link will have an IP address, but in contrast that IP address is an implementation detail and not directly used by us to serve  github.com . We instead announce blocks of IP addresses, for example  192.30.253.0/24 . When an ISP sees packets with a destination address in that range, they will forward them towards the best source of the announcement. Another difference is how transit is priced, typically billed by bandwidth in Mbps, megabit per second, at the  95th percentile . . We have multiple providers in each of our regions with diverse links spread across routers. These links are commonly referred to as  cross-connects  and are physical strands of fiber optic cable that run from our device to theirs within the same facility. . In early 2015, 100% of our requests arrived over transit links directly into our IAD data center. In the past two and a half years we’ve grown that substantially, now landing over double that capacity at each our IAD and SEA edge sites. As part of that move we’ve gained access to high quality  peering . Broadly speaking peering is a connection established directly between two networks. The main difference between peering and transit is that peering only allows you to reach things on the peer’s network. You generally can’t “transit” them to reach others. . We have two types of peering currently in use. The first is private network interconnect (PNI), a direct connection between their routers and ours. This is generally used when a sufficiently large volume of traffic regularly travels between the two networks. For us it can consist of a single pair of 10Gbps links or many pairs as required by the bandwidth and availability needs. The other type of peering we do is over  Internet exchanges . In the case of exchange based peering instead of direct connections between the two networks’ routers, a connection is made through an intermediate network specifically for connecting peers. Through the exchange hundreds of other networks can get access to us and vice versa. Since there’s no per-peer physical resources being used beyond the initial exchange connection it’s effectively free to peer in this way and in fact exchange based peering is commonly settlement free, meaning no money changes hands. This is often the case for PNI as well. . While peering can cut costs, that’s often a secondary benefit. The real upsides are in performance and availability. Having a direct connection means that traffic between the two networks is 100% in the control of two parties who are able to communicate with each other. It also means fewer hops between the source and destination which can improve latency considerably and avoid congestion. . The increases in our transit capacity, addition of a second region, and the work on  GLB , our load-balancing tier, has allowed us to handle volumetric DDoS attacks that in the past would have impacted our users. We’re very proud of that progress and always working to improve it further. . The first step in the process of contacting GitHub is to direct things to the closest region. We use  DNS , specifically GeoDNS to return a record based on the geographic location of the IP address doing the lookup. We conduct network level measurements using tools like  RIPE Atlas  and instrument our applications to collect similar information on real user requests. This data allows us to make choices on which locations should be serviced in SEA and which are best sent to IAD. We configure our  split authority DNS providers  with the results. . Now that your request has a destination, network routing takes over. There are dozens of last hops your requests can take into our sites and nearly boundless options for getting to that point. To take a seemingly intractable problem and solve it millions of times per second at each point in the network, things have to be simplified. This is accomplished by localizing decisions. . Each hop along the path between yours and our network makes its own choice using an algorithm like  BGP route selection . This allows an extremely quick next hop decision to be made for a packet. There are a number of inputs into the process, but conceptually each device has its own idea of the best link to send a packet down in order to get it closer to its ultimate destination. We have the ability to influence the decisions by withdrawing or modifying our announcements, but ultimately control rests with the network devices and operators who set local preferences based on costs, capacity, health, and other factors. The adjustments we make are most often in response to large incidents with our direct or indirect providers, and aim to shift the traffic away from a specific trouble spot or temporarily remove a provider altogether. . It’s important to note that at this level there’s no fundamental difference between transit and peering. Both use the same mechanisms to make next hop decisions, it’s that the best next hop when peered directly with someone is almost certainly over that connection when things are operating normally. Take away that link and the algorithm would fall back to its next best option, likely a transit link. . To recap, we have two major types of connectivity – transit and peering. Within peering, we have two sub-types, PNI and IX. We’ve talked previously about  our internal network metrics  and we have the same types of data for our points of ingress/egress. . The image above shows our ingress and egress traffic for the past week across transit, PNI, and Exchange links. Especially interesting is that peering accounts for 60-70% of our traffic. That’s a huge deal, it means that three quarters of our bandwidth has an optimal path between its origin and destination, providing best case performance and cost savings for both parties. If you happen to have an exchange connected network check out  our PeeringDB page  and reach out if you’re interested in connecting. . If you’re excited about network and traffic engineering, we’re looking to add Site Reliability  Engineers  and an  Engineering Manager  to the team. ", "date": "October 12, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tEvolution of GitHub’s data centers\t\t", "author": ["\n\t\tSam Kottler\t"], "link": "https://github.blog/2017-10-12-evolution-of-our-data-centers/", "abstract": " Over the past 18 months we’ve made a significant investment in GitHub’s physical infrastructure. The goal of this work is to improve the redundancy and global availability of our system. In doing so we’ve solidified the foundation upon which we will expand our compute and storage footprint in support of our growing user base. . We’ve got four facilities, two of which are transit hotels which we call points of presence (POPs) and two of which are data centers. To give an idea of our scale, we’ve got petabytes of Git data stored for users of GitHub.com and do around 100Gb/s across transit, internet exchanges, and private network interfaces in order to serve thousands of requests per second. Our network and facilities are built using a hub and spoke design. We operate our own backbone between Seattle and northern Virginia POPs which provides more consistent latencies and throughput via protected fiber. . The POPs are a few cabinets primarily composed of networking equipment. They’re placed in facilities with a high concentration of transit providers and access to regional internet exchanges. Those facilities don’t store customer data, rather they’re focused on internet and backbone connectivity as well as direct connect and private network interfaces to Amazon Web Services. Today we have a POP in northern Virginia and one in the Seattle metro area, each of which are independently connected to transit providers and the data center in their respective geographic region. . Those well connected POPs have some downsides though, and those led us to choose to have separate data centers for storing customer data and serving requests. For one, it’s more difficult to get space in the facilities we use as POPs because of the demand for space with a variety of transit options. Because of that demand for space there is often less power available which prevents us from being able to deploy the high-density storage and compute cabinets we favor. Therefore the data centers are in standalone cages in less well connected facilities and we use metro waves between the POPs and data centers along with passive dense wavelength division multiplexing (DWDM) to connect them. Today the POPs are one-to-one with data centers, so we have a data center on each coast of the continental United States. . Although the way cages in POPs and data centers are laid out and deployed is somewhat different, there is still a set of shared principles about how we build our facilities. We aim to have cabinet types be as homogenous as possible to increase repeatability. There is one type of cabinet in the POPs which includes management and production border routers, optical networking gear, and enough compute hardware to run core services.  Our data centers have three different types of cabinets — networking, compute, and storage. The majority of the cabinets are for compute and storage hardware, each of which are in specific sets of cabinets rather than mixing them to allow the tiers to be grown independently. The other thing we focus on across both the POPs and data centers is having a clear separation between our infrastructure and things like cross connects run by data center providers. In the POPs we use pre-labeled and cabled points of demarcation to make adding new transit, transport, or direct connect providers a matter of interacting with a single side of one patch panel. . The data centers are built to simplify adding new capacity through the use of structured cabling. When the west coast data center was built the structured cabling was deployed for the whole space, including cabinet positions not yet in use. Each storage and compute cabinet is connected to the structured cabling via an overhead patch panel. Each cabinet-side patch panel is connected to a pair of matching panels in a set of two-post racks that reside between a pair of networking cabinets. Connecting a cabinet to the spine and aggregation switches in the networking cabinets is a matter of patching a set of links from leaf switches and the console device to the overhead panel then patching from the two-post racks to networking cabinets. The two-post to networking patching is done using pre-labeled and tapered fiber looms, which help keep the network cabinets tidy despite the high density of fiber that lands in them. . We work closely with a cabinet integrator to handle rack and stack, cabling, validation, and logistics. When a project to expand compute and/or storage capacity in one of our data centers gets kicked off, the first step is to either use an existing cabinet configuration we’ve built with them or design a new one. Unless we are deploying a new type of cabinet or making changes to configuration of chassis and sleds in an existing one this process involves getting quotes and lead times from vendors, ensuring the cabinet layouts match what we expect, and then placing an order. Once all the components are sourced, the rack integrators build the cabinets and ultimately pack them into a custom shipping crate suitable for freight shipment. . Upon arrival at one of our facilities the cabinet gets landed in its target location in the data center. Network configurations are pre-generated and are already on the switches by the time the cabinets leave the build site, so all that’s required to turn up a new set of cabinets is patching each one, landing power circuits, and powering up the hardware inside them. Data about the cabinet layout from the integrator is imported into our hardware management system,  gPanel , to map serial numbers to rack units and enclosures. gPanel provides hardware discovery via a stateless image that’s run across all chassis that aren’t currently installed. On first boot each chassis enters an  unknown  state and then moves through a state machine to configure devices like the iDRAC, make sure the BMC, BIOS, and drive firmwares match the versions we run in production elsewhere, and then validate the health of the machine. Once each of these steps have been completed the box ends up in the  ready  state and is made available to be requested via gPanel. . The deployment of new storage and compute cabinets is highly reproducible using this approach, allowing us to confidently expand our physical footprint inside the data centers. In its current form the process allows us to make hardware available for engineers across the company to be provisioned the same day the cabinets are patched and powered up. . Here’s a compute cabinet we recently added to our Seattle data center: . The improvements we’ve made to our data centers and network have enabled us to continue to confidently expand our physical infrastructure. Introducing a repeatable process to add new capacity and decoupling the regional network edge from our compute and storage systems has enabled us to enter the next stage of our infrastructure’s growth. . If data center and network engineering excites you,  we’re looking to add more SREs to the team . ", "date": "October 12, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tStretching Spokes\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2017-10-13-stretching-spokes/", "abstract": " GitHub’s Spokes system stores multiple distributed copies of Git repositories. This article discusses how we got Spokes replication to span widely separated datacenters. . GitHub developed a system called  Spokes  to store multiple replicas of our users’ Git repositories and to keep the replicas in sync. Spokes uses several strategies to ensure that every Git update is safely replicated to all of the replicas in most cases and to at least a quorum of replicas in every case. Spokes does replication at the Git application level, replacing an older system that did replication at the filesystem block level. . Every push to a Git repository goes through a proxy, which transparently replicates it to multiple fileservers. Early versions of Spokes required low-latency communication between the proxy and all of the replicas to sustain high update rates. Therefore, the replicas had to be located relatively close to each other. . But there are well-known advantages to being able to place data replicas distant from each other: . This article explains why latency was a problem in the first place, how we overcame the problems to allow continent-wide distributed storage of Git data, and what improvements this has brought our users. . Before Spokes, we used  DRBD  filesystem block-level replication to keep repositories duplicated. This system was very sensitive to latency, so we were forced to keep fileserver replicas right next to each other. This was obviously not ideal, and getting off of it was the motivation that drove the initial development of Spokes. . As soon as we had Spokes running, we started pushing the limits of how distant Spokes replicas could be from each other. The farther apart the replicas, the longer the latency between them. Higher latency limits the maximum rate of Git reference updates that Spokes can sustain for each repository. . It might seem surprising that we are concerned about this. A single repository doesn’t get pushed to  that  often, does it? . Well,  most  users don’t push often at all. But if you host nearly 70 million repositories, you will find that  some projects use workflows that you never would have foreseen . We work very hard to make GitHub Just Work for all but the most ludicrous use cases. . Also, GitHub generates a surprisingly large number of reference updates for its own internal bookkeeping purposes. For example, every time a user pushes to a pull request branch, we have to record the push itself, we might need to sync that branch to the target repository, and we compute a test merge and test rebase of the pull request, creating references for each of those. And if a user pushes to a project’s  master  branch, we compute a test merge and a test rebase for every active pull request whose target is  master . In some repositories, this can trigger updates of more than a hundred references. . It is crucial to the workability of Spokes with distant replicas that reference updates be sufficiently quick even with high latency. Specifically, we’d like to support at least a few updates per second in a single repository. This translates to a budget of a few hundred milliseconds per update. And remember that whatever we do to optimize writes mustn’t slow down reads, which outnumber writes by about 100:1. . Due to the speed of light and other annoying facts, every round-trip communication to a distant replica takes time. For example, a network round-trip across the continental US takes something like 60-80 milliseconds. It wouldn’t take very many round trips to use up our time budget. . We use the  three-phase commit protocol  to update the replicas and additionally use the replicas as a distributed lock to ensure that the database is updated in the correct order. All in all, this costs four round-trips to the distant replicas; expensive, but not prohibitive. (We have plans to reduce the number of round trips through the use of a more advanced consensus algorithm.) . As much as possible, we also make use of the time spent waiting on the network to get other work done. For example, while one replica is acquiring its lock, another replica might be computing a checksum and the coordinator might be reading from the database. . Three-phase commit is a core part of keeping the replicas in sync. Implementing it requires each of the replicas to be able to answer the question “can you carry out the following reference updates?”, and then to either commit or roll back the transaction based on the coordinator’s instructions. To make this possible, we  implemented transactions for Git reference updates  in the open source Git project (these are exposed, for example, via  git update-ref --stdin ), and we  did  a  lot  of  work  to  make   sure   that  the  results  of a  transaction  are  deterministic  across replicas. First Git acquires all necessary local reference locks, then it verifies that the old values were as expected and that the new values make sense. If everything is OK, the tentative transaction is committed; otherwise, it is rolled back in its entirety. . Aside from the network latency, we also have to consider the time that it takes to update a Git reference on a single replica. Therefore, we have  also   implemented  a  number  of  speedups  to reference-related operations. These changes have also been contributed back to the open source Git project. . To summarize the state of a replica, we use a checksum over the list of all of its references and their values, plus a few other things. We call this the “Spokes checksum”. If two replicas have the same Spokes checksums, then they definitely hold the same logical contents. We compute the Spokes checksum for every replica after every update as an extra check that they are all in sync. . In a busy repository with a lot of references, computing the Spokes checksum from scratch is relatively expensive, and would limit the maximum rate of reference updates. Therefore, we compute the Spokes checksum incrementally whenever possible. We define it to be the exclusive-or of the hash of each  (refname, value)  pair. So when a reference is updated, we can update this part of the checksum via . This makes it cheap to compute the new Spokes checksum if we know the old one. . Even with all of these optimizations, a reference update still costs about a third of a second. This is fine in most situations. But in the case that we mentioned earlier, where an update to  master  might cause a cascade of more than a hundred bookkeeping reference updates, processing the updates could keep the repository busy for something like 30 seconds. If these updates would block user-initiated reference updates for such a long time, user requests would be highly delayed or even time out. . We’ve gotten around this problem by coalescing some of the bookkeeping updates into fewer transactions, and by giving user-initiated updates priority over bookkeeping updates (which aren’t time-critical). . The most tangible benefit that Spokes brings to GitHub users is that Git reads (fetches and clones) can often be served by a nearby Spokes replica. Since Spokes can quickly figure out which replicas are up to date, it routes reads to the closest replica that is in sync. This aspect of Spokes is already speeding up transfers for many users of GitHub.com, and it will only improve with time as we add replicas in more geographical areas. . GitHub Enterprise, the on-premises version of GitHub, now also supports  Geo-replication , using the same underlying Spokes technology. Users close to such a replica can enjoy faster Git transfers, even if they are distant from the main GHE host(s). These replicas are configured to be non-voting, so that Git pushes to the main GHE hosts continue to work even if the Geo-replicated hosts become temporarily unreachable. . Spokes’s design, plus careful optimization of the performance of distributed reference updates, now allows Spokes to replicate Git repositories over long distances. This improves robustness, speed, and flexibility for both GitHub.com and GitHub Enterprise. ", "date": "October 13, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tMitigating replication lag and reducing read load with freno\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-10-13-mitigating-replication-lag-and-reducing-read-load-with-freno/", "abstract": " At GitHub, we use MySQL as the main database technology backing our services. We run classic MySQL master-replica setups, where writes go to the master, and replicas replay master’s changes asynchronously. To be able to serve our traffic we read data from the MySQL replicas. To scale our traffic we may add more replica servers. Reading from the master does not scale and we prefer to minimize master reads. . With asynchronous replication, changes we make to the master are not immediately reflected on replicas. Each replica pulls changes from its master and replays them as fast as it can. There is a nonzero delay between the point in time where changes are made visible on a master and the time where those changes are visible on some replica or on all replicas. This delay is the replication lag. . The higher the replication lag on a host, the more  stale  its data becomes. Serving traffic off of a lagging replica leads to poor user experience, as someone may make a change and then not see it reflected. Our automation  removes lagging replicas  from the serving pool after a few seconds, but even those few seconds matter: we commonly expect sub-second replication lag. . Maintaining low replication lag is challenging. An occasional  INSERT  or  UPDATE  is nothing, but we routinely run massive updates to our databases. These could be batched jobs, cleanup tasks, schema changes, schema change followups or otherwise operations that affect large datasets. Such large operations may easily introduce replication lag: while a replica is busy applying a change to some  100,000  rows, its data quickly becomes stale, and by the time it completes processing it’s already lagging and requires even more time to catch up. . To mitigate replication lag for large operations we use batching. We never apply a change to  100,000  rows all at once. Any big update is broken into small segments, subtasks, of some  50  or  100  rows each. . As an example, say our app needs to purge some rows that satisfy a condition from a very large table. Instead of running a single  DELETE FROM my_table WHERE some_condition = 1  we break the query to smaller subqueries, each operating on a different slice of the table. In its purest form, we would get a sequence of queries such as: . These smaller queries can each be processed by a replica very quickly, making it available to process more events, some of which may be the normal site’s update traffic or the next segments. . However, the numbers still add up. On a busy hour a heavily loaded replica may still find it too difficult to manage both read traffic and massive changes coming from the replication stream. . We recognize that  most  of the large volume operations come from background jobs, such as archiving or schema migrations. There is no particular user of API requests waiting on those operations to complete. It is  fine  for these operations to take a little while longer to complete. . In order to apply these large operations, we break them into smaller segments and throttle in between applying those segments. Each segment is small enough and safe to send down the replication stream, but a bunch of those segments can be too much for the replicas to handle. In between each segment we pause and ask:  is replication happy and in good shape?  . There is no direct mechanism in MySQL to do that. Closest would be  semisynchronous replication , but even that doesn’t guarantee replication lag to be caught up nor be within reasonable margins. . It is up to us to be able to identify our relevant production traffic serving replicas and ask: “What is your current lag?” . If lag on all relevant replicas is within good margins (we expect subsecond), we proceed to run the next segment, then ask again. If lag is higher than desired, we throttle: we stall the operation, and keep polling lag until we’re satisfied it is low enough for us. . This flow suggests: . Our site runs Ruby on Rails. Over time, we built into our Rails app a series of abstractions to identify the set of replicas that were active in a cluster, ask them their current replication delay value, and determine whether that value was  low enough to continue writing on the master. In its simplest form, the api looks like this: . We would routinely inspect HAProxy for the list relevant replicas and populate a special table with that list. The table was made available to the throttler.  When  .throttle  was called, the throttler pulled the list of replicas and polled replication delay metrics from the MySQL servers. . A common tool part of  Percona Toolkit  called   pt-heartbeat   inserts a timestamp each 100ms in the master. That timestamp is replicated along with the rest of the information to the replicas, and as a consequence, the following query returns the replication delay in seconds from each of the replicas. . The aggregated value of replication delay was the maximum of the different metrics polled from each replica. . The throttler also had local configuration to determine when that aggregated value was low-enough. If it wasn’t, the code block above would sleep for a second and check the replication delay again; if it instead was good, it would run the block, thus writing the subset. . As we’ve grown, we’ve introduced write workloads outside of our main Rails application, and they’ve needed to be throttled as well. . We routinely archive or purge old data via   pt-archiver  . This is a Perl script, and fortunately comes with its own implementation for  replication lag  based throttling. The tool crawls down the topology to find replicating servers, then periodically checks their replication lag. . Last year we introduced  gh-ost ,  our schema migration tool .  gh-ost , by definition, runs our most massive operations: it literally rebuilds and rewrites entire tables. Even if not throttled, some of our tables could take hours or days to rebuild.  gh-ost  is written in Go, and could not use the Ruby throttler implementation nor the Perl implementation. Nor did we wish for it to depend on either, as we created it as a general purpose, standalone solution to be used by the community.  gh-ost  runs its own throttling mechanism, checking first and foremost the very replica on which it operates, but then also the list of  --throttle-control-replicas .  gh-ost ‘s  interactive commands  allow us to change the list of throttle control replicas during runtime. We would compute the list dynamically when spawning gh-ost, and update, if needed, during migration. . Then  Spokes  brought more massive writes. As our infrastructure grew, more and more external Ruby and non-Ruby services began running massive operations on our database. . What used to work well when we were running exclusively Ruby on Rails code and in smaller scale didn’t work so well as we grew. We increasingly ran into operational issues with our throttling mechanisms. . We were running more and more throttling tasks, many in parallel. We were also provisioning, decommissioning, and refactoring our MySQL fleets. Our traffic grew substantially. We realized our throttling setups had limitations. . Different apps were getting the list of relevant replicas in different ways. While the Ruby throttler always kept an updated list, we’d need to educate  pt-archiver  and  gh-ost  about an initial list, duplicating the logic the Ruby throttler would use. And while the Ruby throttler found out in real time about list changes (provisioning, decommissioning servers in production),  gh-ost  had to be  told  about such changes, and  pt-archiver ‘s list was immutable; we’d need to kill it and restart the operation for it to consider a different list. Other apps were mostly trying to operate similarly to the Ruby throttler, but never exactly. . As result, different apps would react differently to ongoing changes. One app would be able to gain the upper hand on another, running its own massive operation while starving the other. . The databases team members would have more complicated playbooks, and would need to run manual commands when changing our rosters. More importantly, the database team had no direct control of the apps. We would be able to cheat the apps into throttling if we wanted to, but it was all-or-nothing: either we would throttle  everything  or we would throttle  nothing . Occasionally we would like to prioritize one operation over another, but had no way to do that. . The Ruby throttler provided great metric collection, but the other tools did not; or didn’t integrate well with the GitHub infrastructure, and we didn’t have visibility into what was being throttled and why. . We were wasting resources. The Ruby throttler would probe the MySQL replicas synchronously and sequentially per server host. Each throttle check by the app would introduce a latency to the operation by merely iterating the MySQL fleet, a wasted effort if no actual lag was found. It invoked the check for each request, which implied dozens of calls per second. That many duplicate calls were wasteful. As result we would see dozens or even hundreds of stale connections on our replica servers made by the throttler from various endpoints, either querying for lag or sleeping. . We built  freno , GitHub’s central throttling service, to replace all our existing throttling mechanisms and solve the operational issues and limitations described above. .  freno  is Spanish for  brake , as in  car brakes . The name  throttled  was already taken and  freno  was just the next most sensible name. . In its very basic essence,  freno  runs as a standalone service that understands replication lag status and can make recommendations to inquiring apps. However, let’s consider some of its key design and operational principles: .  freno  continuously probes and queries the MySQL replicas. It does so independently of any app wanting to issue writes. It does so  asynchronously  to any app. .  freno  continuously updates the list of servers per cluster. Within a  10  second timeframe  freno  will recognize that servers were removed or added. In our infrastructure,  freno  polls our  GLB  servers (seen as HAProxy) to get the roster of production replicas. . An app doesn’t need to (and actually just  doesn’t ) know the identity of backend servers, their count or location. It just needs to know the  cluster  it wants to write to. . Multiple  freno  services form a  raft  cluster. Depending on cluster size some boxes can go down and still the  freno  service would be up, with a leader to serve traffic. Our proxies direct traffic to the  freno  leader (we have highly available proxies setup, but one may also detect the leader’s identity directly). .  freno  use is voluntary. It is not a proxy to the MySQL servers, and it has no power over the applications. It provides recommendations to applications that are interested in replication lag status. Those applications are expected to cooperate with  freno ‘s recommendations. . An app issues a  HEAD  request to  freno , and gets a  200 OK  when it is clear to write, or a  different code  when it should throttle. . We are able to throttle specific apps, for a predefined duration and to a predefined degree. For example, an engineer can issue the following in chat: . We may altogether refuse an app’s requests or only let it operate in low volumes. .  freno  records all requests by all apps. We are able to see which app requested writes to which cluster and when. We know when it was granted access and when it was throttled. . We are able to query  freno  and tell which metric hasn’t been healthy in the past  10  minutes, or who requested writes to a particular cluster, or what is being forcefully throttled, or what would be the response  right now  for a  pt-archiver  request on a cluster. .  freno  brought a unified, auditable and controllable method for throttling our app writes. There is now a single, highly available, entity querying our MySQL servers. . Querying  freno  is as simple as running: .  $ curl -s -I http://my.freno.com:9777/check/myscript/mysql/clusterB  . where the  myscript  app requests access to the  clusterB  cluster. This makes it easy to use by any client. .  freno  has been open source since early in its development and is  available  under the MIT license. . While the simplest client may just run a  HTTP GET  request, we’ve also made available more elaborate clients: .  freno  was built for monitoring  MySQL  replication, but may be extended to collect and serve other metrics. . We wanted  freno  to serve those massive write operation jobs, typically initiated by background processes. However, we realized we could also use it to reduce read load from our masters. . Reads from the master should generally be avoided, as they don’t scale well. There is only one master and it can only serve so many reads. Reads from the master are typically due to the consistent-read problem: a change has been made, and needs to be immediately visible in the next read. If the read goes to a replica, it may hit the replica too soon, before the change was replayed there. . There are various ways to solve the consistent-read problem, that include blocking on writes or blocking on reads. At GitHub, we have a peculiar common flow that can take advantage of  freno . . Before  freno , web and API  GET  requests were routed to the replicas only if the last write happened more than five seconds ago. This pseudo-arbitrary number had some sense: If the write was five seconds ago or more, we can safely read from the replica, because if the replica is lagging above five seconds, it means that there are worse problems to handle than a read inconsistency, like database availability being at risk. . When we introduced  freno , we started using the information it knows about replication lag across the cluster to address this. Now, upon a  GET  request after a write, the app asks  freno  for  the maximum replication lag across the cluster , and if the reported value is below the elapsed time since the last write (up to some granularity), the read is known to be safe and can be routed to a replica. . By applying that strategy, we managed to route to the replicas ~30% of the requests that before were routed to the master. As a consequence, the number of selects and threads connected on the master was reduced considerably, leaving the master with more free capacity: .   . We applied similar strategies to other parts of the system: Another example is search indexing jobs. . We index in  Elasticsearch  when a certain domain object changes. A change is a write operation, and as indexing happens asynchronously, we sometimes need to re-hydrate data from the database. . The time it takes to process a given job since the last write is in the order of a hundred milliseconds. As replication lag is usually above that value, we were also reading from the master within indexing jobs to have a consistent view of the data that was written. This job was responsible for another 11% of the reads happening on the master. . To reduce reads on the master from indexing jobs, we used the replication delay reported by freno to delay the execution of each indexing job until the data has been replicated. To do it, we store in the job payload the timestamp at which the write operation that triggered the job occurred, and based on the replication delay reported by freno, we wait until we are sure the data was replicated. This happens in less than 600ms 95% of the time. . The above two scenarios account for ~800 rps to freno, but replication delay cannot grow faster than the clock, and we used this fact to optimize access and let freno scale to growing usage demands. We implemented client side caching over memcache, using the same replication delay values for 20ms, and adding a cushioning time to compensate both freno sampling rates and the caching TTL. This way, we capped to 50rps to freno from the main application. . Seeing that  freno  now serves production traffic, not just backend processes, we are looking into its serving capacity. At this time a single  freno  process is well capable of serving what requests we’re sending its way, with much room to spare. If we need to scale it, we can bootstrap and run multiple  freno  clusters: either used by different apps and over different clusters (aka sharding), or just for sharing read load, much like we add replicas to share query read load. The replicas themselves can tolerate many  freno  clients, if needed. Recall that we went from dozens/hundreds of throttler connections to a single  freno  connection; there’s room to grow. . Motivated by the client-side caching applied in the web application, we now can tell freno to write results to  memcached , which can be used to decouple freno’s probing load from the app’s requests load. .  freno  &amp;  freno-client  make it easy for apps to add throttling to massive operations. However, the engineer still needs to be aware that their operation should use throttling, and it takes a programmatic change to the code to call upon throttling. We’re looking into intelligently identifying queries: the engineer would run “normal” query code, and an adapter or proxy would identify whether the query requires throttling, and how so. .     .  Miguel Fernández   Senior Platform Engineer .     .  Shlomi Noach   Senior Infrastructure Engineer ", "date": "October 13, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tDoubling Bug Bounty rewards\t\t", "author": ["\n\t\tBrent Johnson\t"], "link": "https://github.blog/2017-10-18-doubling-bug-bounty-rewards/", "abstract": " We’re coming up on four years since the Bug Bounty program was  first announced . A lot has changed in that time, and we constantly try to keep our reward structure inline with top security bug bounty programs. We’re excited to announce that starting today we’re doubling our payout amounts, bringing the minimum and maximum payouts to  $555  and  $20,000 , respectively. This means that any report eligible for a bounty will be met with at least a $555 reward. This doesn’t mean we’re raising the bar for what is considered a valid report, we’re simply raising the payouts. . This bump to our payouts aligns with  Hack the World , an annual hacking competition by HackerOne, which kicked off this morning and runs until November 18th. During this time participants compete against each other to find the most security vulnerabilities across all sites on HackerOne’s platform. We’re one of the sponsors, which means hackers will be rewarded with twice the reputation points on HackerOne when finding bugs on GitHub over the next month! As an additional incentive, we will also be rewarding  all  valid submissions with  free unlimited private repositories for life . The increased bounty payouts are here to stay, but unlimited private repositories will only be rewarded on reports submitted on or before November 18th! . Ready to compete? Submit all reports to our  Bug Bounty program . For more details on the competition, please visit the  Hack the World website . ", "date": "October 18, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tWeak cryptographic standards removal notice\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-02-01-crypto-removal-notice/", "abstract": "  Last year  we announced the deprecation of several weak cryptographic standards. Then we provided  a status update  toward the end of last year outlining some changes we’d made to make the transition easier for clients. We quickly approached the February 1, 2018 cutoff date we mentioned in previous posts and, as a result, pushed back our schedule by one week. On  February 8, 2018  we’ll start disabling the following: . We’ll disable the algorithms in two stages: . While only a small fraction of traffic currently makes use of the deprecated algorithms, and many clients will automatically transition and start using the new algorithms, there is invariably going to be a small fraction of clients that will be impacted. We expect most of these are older systems that are no longer maintained, but continue to access Git/the GitHub API using the deprecated algorithms. To help mitigate this, we will temporarily disable support for the deprecated algorithms for one hour on  February 8, 2018 19::00 UTC . By disabling support for the deprecated algorithms for a small window, these systems will temporarily fail to connect to GitHub. We will then restore support for the deprecated algorithms and provide a two week grace period for these systems to upgrade their libraries before we disable support for the deprecated algorithms  permanently on February 22, 2018 . . As noted above, the vast majority of traffic should be unaffected by this change. However, there are a few remaining clients that we anticipate will be affected. Fortunately, the majority of clients can be updated to work with  TLSv1.2 . . Git-Credential-Manager-for-Windows &lt; v1.14.0 does not support  TLSv1.2 . This can be addressed by  updating to v1.14.0 . . Red Hat 5,  6 , and  7  shipped with Git clients that did not support  TLSv1.2 . This can be addressed by updating to versions 6.8 and 7.2 (or greater) respectively. Unfortunately, Red Hat 5 does not have a point release that supports  TLSv1.2 . We advise that users of Red Hat 5 upgrade to a newer version of the operating system. . As noted in  this blog post by Oracle ,  TLSv1  was used by default for JDK releases prior to JDK 8. JDK 8 changed this behavior and defaults to  TLSv1.2 . Any client (ex. JGit is one such popular client) that runs on older versions of the JDK is affected. This can be addressed by updating to JDK &gt;= 8 or explicitly opting in to  TLSv1.2  in JDK 7 (look at the  https.protocols  JSSE tuning parameter). Unfortunately, versions of the JDK &lt;= 6 do not support  TLSv1.2 . We advise users of JDK &lt;= 6 to upgrade to a newer version of the JDK. . Visual Studio ships with specific versions of Git for Windows and the Git Credential Manager for Windows (GCM). Microsoft has updated the latest versions of Visual Studio 2017 to work with  TLSv1.2  Git servers. We advise users of Visual Studio to upgrade to the latest release by clicking on the in-product notification flag or by checking for an update directly from the IDE. Microsoft has provided additional guidance on the  Visual Studio developer community support forum . . As always, if you have any questions or concerns related to this announcement, please don’t hesitate to  contact us . ", "date": "February 1, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tWeak cryptographic standards removed\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2018-02-23-weak-cryptographic-standards-removed/", "abstract": " Earlier today we permanently removed support for the following weak cryptographic standards on github.com and api.github.com: . This change was originally announced  last year , with the final timeline for the removal posted  three weeks ago . If you run into any issues or have any questions, please don’t hesitate to let us know. ", "date": "February 23, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tFebruary 28th DDoS Incident Report\t\t", "author": ["\n\t\tSam Kottler\t"], "link": "https://github.blog/2018-03-01-ddos-incident-report/", "abstract": " On Wednesday, February 28, 2018 GitHub.com was unavailable from 17:21 to 17:26 UTC and intermittently unavailable from 17:26 to 17:30 UTC due to a distributed denial-of-service (DDoS) attack. We understand how much you rely on GitHub and we know the availability of our service is of critical importance to our users. To note, at no point was the confidentiality or integrity of your data at risk. We are sorry for the impact of this incident and would like to describe the event, the efforts we’ve taken to drive availability, and how we aim to improve response and mitigation moving forward. . Cloudflare described an amplification vector using memcached over UDP in their blog post this week,  “Memcrashed – Major amplification attacks from UDP port 11211” . The attack works by abusing memcached instances that are inadvertently accessible on the public internet with UDP support enabled. Spoofing of IP addresses allows memcached’s responses to be targeted against another address, like ones used to serve GitHub.com, and send more data toward the target than needs to be sent by the unspoofed source. The vulnerability via misconfiguration described in the post is somewhat unique amongst that class of attacks because the amplification factor is up to 51,000, meaning that for each byte sent by the attacker, up to 51KB is sent toward the target. . Over the past year we have deployed additional transit to our facilities. We’ve more than doubled our transit capacity during that time, which has allowed us to withstand certain volumetric attacks without impact to users. We’re continuing to deploy additional transit capacity and  develop robust peering relationships across a diverse set of exchanges . Even still, attacks like this sometimes require the help of partners with larger transit networks to provide blocking and filtering. . Between 17:21 and 17:30 UTC on February 28th we identified and mitigated a significant volumetric DDoS attack. The attack originated from over a thousand different autonomous systems (ASNs) across tens of thousands of unique endpoints. It was an amplification attack using the memcached-based approach described above that peaked at 1.35Tbps via 126.9 million packets per second. . At 17:21 UTC our network monitoring system detected an anomaly in the ratio of ingress to egress traffic and notified the on-call engineer and others in our chat system. This graph shows inbound versus outbound throughput over transit links: .   . Given the increase in inbound transit bandwidth to over 100Gbps in one of our facilities, the decision was made to move traffic to Akamai, who could help provide additional edge network capacity. At 17:26 UTC the command was initiated via our ChatOps tooling to withdraw BGP announcements over transit providers and announce  AS36459  exclusively over our links to Akamai. Routes reconverged in the next few minutes and access control lists mitigated the attack at their border. Monitoring of transit bandwidth levels and load balancer response codes indicated a full recovery at 17:30 UTC. At 17:34 UTC routes to internet exchanges were withdrawn as a follow-up to shift an additional 40Gbps away from our edge. .   . The first portion of the attack peaked at 1.35Tbps and there was a second 400Gbps spike a little after 18:00 UTC. This graph provided by Akamai shows inbound traffic in bits per second that reached their edge: .   . Making GitHub’s edge infrastructure more resilient to current and future conditions of the internet and less dependent upon human involvement requires better automated intervention. We’re investigating the use of our monitoring infrastructure to automate enabling DDoS mitigation providers and will continue to measure our response times to incidents like this with a goal of reducing mean time to recovery (MTTR). . We’re going to continue to expand our edge network and strive to identify and mitigate new attack vectors before they affect your workflow on GitHub.com. . We know how much you rely on GitHub for your projects and businesses to succeed. We will continue to analyze this and other events that impact our availability, build better detection systems, and streamline response. ", "date": "March 1, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tMeasuring the many sizes of a Git repository\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2018-03-05-measuring-the-many-sizes-of-a-git-repository/", "abstract": " Is your Git repository bursting at the seams?  git-sizer  is a new open source tool that can tell you when your repo is getting too big.  git-sizer  computes various Git repository size metrics and alerts you to any that might cause problems or inconvenience. . When people talk about the size of a Git repository, they often talk about the total size needed by Git to store the project’s history in its internal, highly-compressed format—basically, the amount of disk space used by the  .git  directory. This number is easy to measure. It’s also useful, because it indicates how long it takes to clone the repository and how much disk space it will use. . At GitHub we host over 78 million Git repositories, so  we’ve seen it all . What we find is that many of the repositories that tax our servers the most are  not  unusually big. The most challenging repositories to host are often those that have an unusual internal layout that Git is not optimized for. . Many properties aside from overall size can make a Git repository unwieldy. For example: . It could contain an astronomical number of Git objects (which are used to store the repository’s history) . The total size of the Git objects could be huge when uncompressed (even though their size is reasonable when compressed) . When the repository is checked out, the size of the working copy might be gigantic . The repository could have an unreasonable number of commits in its history . It could include enormous individual files or directories . It could contain large files/directories that have been modified very many times . It could contain too many references (branches, tags, etc) . Any of these properties, if taken to an extreme, can cause certain Git operations to perform poorly. And surprisingly, a repository can be grossly oversized in almost any of these ways without using a worrying amount of disk space. . It also makes sense to consider whether the size of your repository is commensurate with the type and scope of your project. The Linux kernel has been developed over 25 years by thousands of contributors, so it is not at all alarming that it has grown to 1.5 GB. But if your weekend class assignment is already 1.5 GB, that’s probably a strong hint that you could be using Git more effectively! . You can use  git-sizer  to measure many size-related properties of your repository, including all of those listed above. To do so, you’ll need a local clone of the repository and a copy of  the Git command-line client  installed and in your execution  PATH . Then: .  Install  git-sizer   . Change to the directory containing your repository . Run  git-sizer . You can learn about its command-line options by running  git-sizer --help , but no options are required .  git-sizer  will gather statistics about all of the references and reachable Git objects in your repository and output a report. For example, here is the verbose output for the Linux kernel repository: . The   git-sizer  project page  explains the output in detail. The most interesting thing to look at is the “level of concern” column, which gives a rough indication of which parameters are high compared with a typical, modest-sized Git repository. A lot of asterisks would suggest that your repository is stretching Git beyond its sweet spot, and that some Git operations might be noticeably slower than usual. If you see exclamation marks instead of asterisks in this column, then you likely have a problem that needs addressing. . As you can see from the output, even though the Linux kernel is a big project by most standards, it is fairly well-balanced and none of its parameters have extreme values. Some Git operations will certainly take longer than they would in a small repository, but not unreasonably, and not out of proportion to the scope of the project. The kernel project is comfortably manageable in Git. . If the  git-sizer  analysis flags up any problems in your repository, we suggest referring again to the   git-sizer  project page , where you will find many suggestions and resources for improving the structure of your Git repository. Please note that by far the easiest time to improve your repository structure is when you are just beginning to use Git, for example when migrating a repository from another version control system,  before  a lot of developers have started cloning and contributing to the repository. And keep in mind that repositories only grow over time, so it is preferable to establish good practices early. . Git is famous for its speed and ability to deal with even quite large development projects. But every system has its limits, and if you push its limits too hard, your experience might suffer.  git-sizer  can help you evaluate whether your Git repository will live happily within Git, or whether it would be advisable to slim it down to make your Git experience as delightful as it can be. .  Getting involved:   git-sizer  is open source! If you’d like to report bugs or contribute new features, head over to  the project page . ", "date": "March 5, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tImproving your OSS dependency workflow with Licensed\t\t", "author": ["\n\t\tJon Ruskin\t"], "link": "https://github.blog/2018-03-07-improving-your-oss-dependency-workflow-with-licensed/", "abstract": " GitHub recently open sourced  Licensed  in the hopes that it is as helpful to the OSS community as it has been to us. . Before we go any further, let’s review a few terms that will be repeated throughout this article . Dependency: An external software package used in an application    i.e. packages that are  require d or  import ed like Octokit, ActiveRecord, React     . i.e. packages that are  require d or  import ed like Octokit, ActiveRecord, React . Dependency source: A class that can enumerate dependencies for an application    i.e. by invoking a package management tool such as bundler, npm, bower, or cabal.     . i.e. by invoking a package management tool such as bundler, npm, bower, or cabal. . Licensed helps GitHub engineers make efficient use of OSS by surfacing potential problems with a dependency’s license early in our development cycle, ensuring we maintain dependency license documentation throughout our development cycle. . In practice, enumerating dependencies can be difficult.  In the easiest scenario a package manager provides a full listing of project dependencies in a parseable file.  More difficult scenarios require detailed knowledge of CLI tools, such as using  go list  for a general purpose  Golang  solution or  ghc-pkg  for  Haskell  package managers. . Licensed works in any Git repository to find, cache and check license metadata for dependencies.  It can detect dependencies from multiple language types and package managers across multiple projects in a single repository.  This flexibility allows Licensed to work equally well for a monolith repository as it would for a repository containing a single project. . Licensed uses a configuration file to determine how and where to enumerate dependencies for a repository. Configuration files specify one or more Licensed applications, where an application describes a location to enumerate dependencies and a directory to store metadata. For more information on configuration files and Licensed applications, see the Licensed  documentation . . Licensed enumerates dependencies for each application’s source path found in the configuration.  For each dependency found, Licensed finds the dependency source location in the local environment and extracts their basic metadata (e.g.  name ,  version ,  homepage  and  summary ). . Licensed uses  Licensee  to determine each dependency’s license(s) and find it’s license text (e.g.  LICENSE ) from the local dependency source location. . Once Licensed has the dependency’s metadata, it caches the metadata and license information for the project at the cache path(s) specified in the Licensed configuration file. . Storing the dependency data in a source control repository enables checking dependency data as part of the development workflow.  Requiring updates to license data whenever dependencies change forces the license data to stay up to date and relevant. . Keeping the cached data in a source control repository also means you automatically get a history of every dependency change in a single location.  Tracking down when a specific dependency changed becomes easier when there is a common location and fewer commits to look through. . Many dependencies’ licenses require distributing a copy of the licenses when used in downstream projects.  Licensed makes it easy to automate the build and distribution of these licenses, and collectively an open source bill of materials for your project, along with the project source. . Lastly, Licensed is used to report any dependencies needing review.  When checking dependency licenses, Licensed performs the following verifications: . GitHub engineers have a shared responsibility to ensure that their projects stay compliant with our OSS license requirements. . As the first line of defense in ensuring that dependencies meet our OSS license requirements, each repository has a CI job that checks dependency licenses. This process generally has little impact on developers, and only requires additional effort when a change might not meet our requirements. . When a license needs to be updated, it’s easy to do: . A developer opens a pull request that includes changes to the project dependencies . The repository CI job shows dependency license(s) need review, providing feedback on next steps to resolving the errors . The developer caches license data for the updated dependencies, including the metadata files in the pull request . The repository  CODEOWNERS  file requests a review from subject matter experts . The subject matter expert reviews the changes and provides guidance to resolve any remaining questions. . This process works very well at GitHub.  Involving subject matter experts early in the process reduces friction on the developer and prevents the developer from adding dependencies into the product under license terms that don’t meet our requirements. . Whenever a new project is started, we always try to use the best tool for the job.  In many cases this means a new language or framework that isn’t supported by Licensed.  To handle these cases, we’ve made adding a new dependency source to Licensed as easy as possible. . Creating new dependency sources in Licensed is easy.  Here is a simple example: . Future development for Licensed will focus on . Reducing friction when using Licensed in developer workflows . Reducing friction when adding new dependency sources . Adding new dependency sources 🙂 . Licensed isn’t just about open source, it is open source itself. Interested in adapting the tool to your team’s workflow or adding support for your favorite package manager?  We’d love your help . ", "date": "March 7, 2018"},
{"website": "Github-Engineering", "title": "\n\t\t\tSoft U2F\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2017-07-20-soft-u2f/", "abstract": " In an effort to increase the adoption of  FIDO U2F  second factor authentication, we’re releasing  Soft U2F : a software-based U2F authenticator for macOS. .   . We’ve long been interested in promoting better user security through two-factor authentication on GitHub.com. Initially, we  added support  for  TOTP -based 2FA. A few years later, we  added support  for FIDO U2F. U2F provides a better user experience, while overcoming several security shortcomings of TOTP. Unfortunately, U2F adoption has been low, presumably due to the need to purchase a physical device. . In order to lower the barrier to using U2F, we’ve developed a software-based U2F authenticator for macOS:  Soft U2F . Authenticators are normally USB devices that communicate over the HID protocol. By emulating a HID device, Soft U2F is able to communicate with your U2F-enabled browser, and by extension, any websites implementing U2F. . The Soft U2F installer can be downloaded  here  and the source code can be found  here . Contributions to the project are welcome. . A USB authenticator stores key material in hardware, whereas Soft U2F stores its keys in the macOS Keychain. There is an argument to be made that it is more secure to store keys in hardware since malware running on your computer can access the contents of your Keychain but cannot export the contents of a hardware authenticator. On the other hand, malware can also access your browser’s cookies and has full access to all authenticated website sessions, regardless of where U2F keys are stored. . In the case of malware installed on your computer, one meaningful difference between hardware and software key storage for U2F is the duration of the compromise. With hardware key storage, you are only compromised while the malware is running on your computer. With software key storage, you could continue to be compromised, even after the malware has been removed. . Some people may decide the attack scenario above is worth the usability tradeoff of hardware key storage. But, for many, the security of software-based U2F is sufficient and helps to mitigate against many common attacks such as password dumps, brute force attacks, and phishing related exploits. ", "date": "July 20, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tMySQL infrastructure testing automation at GitHub\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-07-06-mysql-testing-automation-at-github/", "abstract": " Our MySQL infrastructure is a critical component to GitHub. MySQL serves GitHub.com, GitHub’s API, authentication and more. Every  git  request touches MySQL in some way. We are tasked with keeping the data available, and maintaining its integrity. Even while our MySQL clusters serve traffic, we need to be able to perform tasks such as heavy duty cleanups, ad-hoc updates, online schema migrations, cluster topology refactoring, pooling and load balancing and more. We have the infrastructure to automate away such operations; in this post we share a few examples of how we build trust in our infrastructure through continuous testing. It is essentially how we sleep well at night. . It is incredibly important to take backups of your data. If you are not taking backups of your database, it is likely a matter of time before this will become an issue. Percona  Xtrabackup  is the tool we have been using for issuing full backups for our MySQL databases. If there is data that we need to be certain is saved, we have a server that is backing up the data. . In addition to the full binary backups, we run logical backups several times a day. These backups allow our engineers to get a copy of recent data. There are times that they would like a complete set of data from a table so they can test an index change on a production sized table or see data from a certain point of time. Hubot allows us to restore a backed up table and will ping us when the table is ready to use. . The data is loaded onto a non-production database which is accessible to the engineer requesting the restore. . The last way we keep a “backup” of data around is we use  delayed replicas . This is less of a backup and more of a safeguard. For each production cluster we have a host that has replication delayed by 4 hours. If a query is run that shouldn’t have, we can run  mysql panic  in chatops. This will cause all of our delayed replicas to stop replication immediately. This will also page the on-call DBA. From there we can use delayed replica to verify there is an issue, and then fast forward the binary logs to the point right before the error. We can then restore this data to the master, thus recovering data to that point. . Backups are great, however they are worthless if some unknown or uncaught error occurs corrupting the backup. A benefit of having a script to restore backups is it allows us to automate the verification of backups via cron. We have set up a dedicated host for each cluster that runs a restore of the latest backup. This ensures that the backup ran correctly and that we are able to retrieve the data from the backup. . Depending on dataset size, we run several restores per day. Restored servers are expected to join the replication stream and to be able to catch up with replication. This tests not only that we took a restorable backup, but also that we correctly identified the point in time at which it was taken and can further apply changes from that point in time. We are alerted if anything goes wrong in the restore process. . We furthermore track the time the restore takes, so we have a good idea of how long it will take to build a new replica or restore in cases of emergency. . The following is an output from an automated restore process, written by Hubot in our robots chat room. . gh-mysql-backup-restore: db-mysql-0752: restore_log.id = 4447 . gh-mysql-backup-restore: db-mysql-0752: Determining backup to restore for cluster ‘prodcluster’. . gh-mysql-backup-restore: db-mysql-0752: Enabling maintenance mode . gh-mysql-backup-restore: db-mysql-0752: Setting orchestrator downtime . gh-mysql-backup-restore: db-mysql-0752: Disabling Puppet . gh-mysql-backup-restore: db-mysql-0752: Stopping MySQL . gh-mysql-backup-restore: db-mysql-0752: Removing MySQL files . gh-mysql-backup-restore: db-mysql-0752: Running gh-xtrabackup-restore . gh-mysql-backup-restore: db-mysql-0752: Restore file: xtrabackup-notify-2017-07-02_0000.xbstream . gh-mysql-backup-restore: db-mysql-0752: Running gh-xtrabackup-prepare . gh-mysql-backup-restore: db-mysql-0752: Starting MySQL . gh-mysql-backup-restore: db-mysql-0752: Update file ownership . gh-mysql-backup-restore: db-mysql-0752: Upgrade MySQL . gh-mysql-backup-restore: db-mysql-0752: Stopping MySQL . gh-mysql-backup-restore: db-mysql-0752: Starting MySQL . gh-mysql-backup-restore: db-mysql-0752: Backup Host: db-mysql-0034 . gh-mysql-backup-restore: db-mysql-0752: Setting up replication . gh-mysql-backup-restore: db-mysql-0752: Starting replication . gh-mysql-backup-restore: db-mysql-0752: Replication catch-up . gh-mysql-backup-restore: db-mysql-0752: Restore complete (replication running) . gh-mysql-backup-restore: db-mysql-0752: Enabling Puppet . gh-mysql-backup-restore: db-mysql-0752: Disabling maintenance mode . gh-mysql-backup-restore: db-mysql-0752: Setting orchestrator downtime . gh-mysql-backup-restore: db-mysql-0752: Restore process complete. . One thing we use backups for is adding a new replica to an existing set of MySQL servers. We will initiate the build of a new server, and once we are notified it is ready, we can start a restore of the latest backup for that particular cluster. We have a script in place that runs all of the restore commands that we would otherwise have to do by hand. Our automated restore system essentially uses the same script. This simplifies the system build process and allows us to have a host up and running with a handful of chat commands opposed to dozens of manual processes. Shown below is a restore kicked manually in chat: . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Determining backup to restore for cluster ‘mycluster’. . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: restore_log.id = 4449 . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Enabling maintenance mode . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Setting orchestrator downtime . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Disabling Puppet . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Stopping MySQL . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Removing MySQL files . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Running gh-xtrabackup-restore . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Restore file: xtrabackup-mycluster-2017-07-02_0015.xbstream . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Running gh-xtrabackup-prepare . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Update file ownership . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting MySQL . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Upgrade MySQL . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Stopping MySQL . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting MySQL . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Setting up replication . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Starting replication . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Backup Host: db-mysql-0201 . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Replication catch-up . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Replication behind by 4589 seconds, waiting 1800 seconds before next check. . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Restore complete (replication running) . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Enabling puppet . @jessbreckenridge gh-mysql-backup-restore: db-mysql-0007: Disabling maintenance mode .  We use orchestrator  to perform automated failovers for masters and intermediate masters. We expect  orchestrator  to correctly detect master failure, designate a replica for promotion, heal the topology under said designated replica, make the promotion. We expect VIPs to change, pools to change, clients to reconnect,  puppet  to run essential components on promoted master, and more. A failover is a complex task that touches many aspects of our infrastructure. . To build trust in our failovers we set up a  production-like , test cluster, and we continuously crash it to observe failovers. . The  production-like  cluster is a replication setup that is identical in all aspects to our production clusters: types of hardware, operating systems, MySQL versions, network environments, VIP,  puppet  configurations,  haproxy setup , etc. The only thing different to this cluster is that it doesn’t send/receive production traffic. . We emulate a write load on the test cluster, while avoiding replication lag. The write load is not too heavy, but has queries that are intentionally contending to write on same datasets. This isn’t too interesting in normal times, but proves to be useful upon failovers, as we will shortly describe. . Our test cluster has representative servers from three data centers. We would  like  the failover to promote a replacement replica from within the same data center. We would  like  to be able to salvage as many replicas as possible under such constraint. We  require  that both apply whenever possible.  orchestrator  has no prior assumption on the topology; it must react on whatever the state was at time of the crash. . We, however, are interested in creating complex and varying scenarios for failovers. Our failover testing script prepares the grounds for the failover: . The script proceeds to crash the master by chosen method, and waits for  orchestrator  to reliably detect the crash and to perform failover. While we expect detection and promotion to both complete within  30  seconds, the script relaxes this expectation a bit, and sleeps for a designated time before looking into failover results. It will then: . These tests confirm that the failover was successful, not only MySQL-wise but also on our larger infrastructure scope. A VIP has been assumed; specific services have been started; information got to where it was supposed to go. . The script further proceeds to restore the failed server: . Consider the following visualization of a scheduled failover test: from having a well-running cluster, to seeing problems on some replicas, to diagnosing the master ( 7136 ) is dead, to choosing a server to promote ( a79d ), refactoring the topology below that server, to promoting it (failover successful), to restoring the dead master and placing it back into the cluster. .   . Our testing script uses a stop-the-world approach. A single failure in any of the failover components fails the entire test, disabling any future automated tests until a human resolves the matter. We get alerted and proceed to check the status and logs. . The script would fail on an unacceptable detection or failover time; on backup/restore issues; on losing too many servers; on unexpected configuration following the failover; etc. . We need to be certain  orchestrator  connects the servers correctly. This is where the contending write load comes useful: if set up incorrectly, replication is easily susceptible to break. We would get  DUPLICATE KEY  or other errors to suggest something went wrong. . This is particularly important as we make improvements and introduce new behavior to  orchestrator , and allows us to test such changes in a safe environment. . The testing procedure illustrated above will catch (and has caught) problems on many parts of our infrastructure. Is it enough? . In a production environment there’s always something else. Something about the particular test method that won’t apply to our production clusters. They don’t share the same traffic and traffic manipulation, nor the exact same set of servers. The types of failure can vary. . We are designing chaos testing for our production clusters. Chaos testing would literally destroy pieces in our production, but on expected schedule and under sufficiently controlled manner. Chaos testing introduces a higher level of trust in the recovery mechanism and affects (thus tests) larger parts of our infrastructure and application. . This is delicate work: while we acknowledge the need for chaos testing, we also wish to avoid unnecessary impact to our service. Different tests will differ in risk level and impact, and we will work to ensure availability of our service. .  We use gh-ost  to run live schema migrations.  gh-ost  is stable, but also under active developments, with major new features being added or planned. .  gh-ost  migrates tables by copying data onto a  ghost  table, applying ongoing changes intercepted by the binary logs onto the  ghost  table, even as the original table is being written to. It then swaps the  ghost  table in place of the original table. At migration completion GitHub proceeds to work with a table generated and populated by  gh-ost . . At this time almost all of GitHub’s MySQL data has been recreated by  gh-ost , and most of it multiple times. We must have high trust in  gh-ost  to let it tamper with our data over and over again, even in face of active development. Here’s how we gain this trust. .  gh-ost  provides a testing-in-production capability. It supports running a migration on a replica, in much the same way as it would run on the master:  gh-ost  would connect to the replica and treat it as if it were the master. It would parse its binary logs the same way it would for a real master migration. However it would copy rows and apply binlog events to the replica, and avoid making writes onto the master. . We run  gh-ost -dedicated replicas in production. These replicas do not serve production traffic. Each such replica retrieves the current list of production tables and iterates them in random order. One by one it picks a table and performs a replica-migration on that table. The migration doesn’t actually modify table structure, but instead runs a trivial  ENGINE=InnoDB . The test runs the migration even as the table is being used in production, thus copying real production data and applying true production traffic off the binary logs. . These migrations can be audited. Here’s how we can inspect status of running tests from chat: . When a test migration completes copying of table data it stops replication and performs the cut-over, replacing the original table with the  ghost  table, and then swaps back. We’re not interested in actually replacing the data. Instead we are left with both the original table and the  ghost  table, which should both be identical. We verify that by checksumming the entire table data for both tables. . A test can complete with: . Test results are audited, sent to robot chatrooms, sent as events to our metrics systems. Each vertical line in the following graph represents a successful migration test: .   . These tests run continuously. We are notified by alerts in case of failures. And of course we can always visit the robots chatroom to know what’s going on. . We continuously improve  gh-ost . Our development flow is based on  git  branches, which we then offer to merge via  pull requests . . A submitted  gh-ost  pull request goes through Continuous Integration (CI) which runs basic compilation and unit tests. Once past this, the PR is technically eligible for merging, but even more interestingly it is  eligible for deployment via Heaven . Being the sensitive component in our infrastructure that it is, we take care to deploy  gh-ost  branches for intensive testing before merging into  master . . @shlomi-noach is deploying gh-ost/fix-reappearing-throttled-reasons (baee4f6) to production (ghost-db-mysql-0007). . @shlomi-noach’s production deployment of gh-ost/fix-reappearing-throttled-reasons (baee4f6) is done! (2s) . @shlomi-noach, make sure you watch for exceptions in haystack . @jonahberquist is deploying gh-ost/interactive-command-question (be1ab17) to production (ghost-db-mysql-0012). . @jonahberquist’s production deployment of gh-ost/interactive-command-question (be1ab17) is done! (2s) . @jonahberquist, make sure you watch for exceptions in haystack . shlomi-noach testing fix-reappearing-throttled-reasons 41 seconds ago: ghost-db-mysql-0007 . jonahberquist testing interactive-command-question 7 seconds ago: ghost-db-mysql-0012 . Nobody is in the queue. . Some PRs are small and do not affect the data itself. Changes to status messages, interactive commands etc. are of lesser impact to the  gh-ost  app. Others pose significant changes to the migration logic and operation. We would tests these rigorously, running through our production tables fleet until satisfied these changes do not pose data corruption threat. . Throughout testing we build trust in our systems. By automating these tests, in production, we get repetitive confirmation that everything is working as expected. As we continue to develop our infrastructure we also follow up by adapting tests to cover the newest changes. . Production always surprises with scenarios not covered by tests. The more we test on production environment, the more input we get on our app’s expectations and our infrastructure’s capabilities. .     .  Tom Krouper   Staff Software Engineer .     .  Shlomi Noach   Senior Infrastructure Engineer ", "date": "July 6, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing Soft U2F, a software U2F authenticator for macOS\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2017-07-24-introducing-soft-u2f-a-software-u2f-authenticator-for-macos/", "abstract": " In an effort to increase the adoption of  FIDO U2F second factor authentication , we’re releasing  Soft U2F —a software-based U2F authenticator for macOS. . Soft U2F currently works with Google Chrome and Opera’s built-in U2F implementations, as well as with the U2F extensions for Safari and Firefox. . When a site loaded in a U2F-compatible browser attempts to register or authenticate with the software token, you’ll see a notification asking you to accept or reject the request. You can experiment on  Yubico’s U2F demo site : .   . And as of today, you can  download the Soft U2F installer  and  configure it for use with your GitHub account . . Interested in learning more? Head over to our Engineering Blog and read more about the  motivations for the project plus the security considerations of hardware vs. software key storage . .   ", "date": "July 24, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tTopic Suggestions for Millions of Repositories\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2017-07-31-topics/", "abstract": " We recently launched  Topics , a new feature that lets you tag your repositories with descriptive words or phrases, making it easy to discover projects and explore GitHub.com. Topic suggestions on public repositories, provides a quick way to add tags to repositories. .   . These suggestions are the result of recent data science work at GitHub. We applied concepts from text mining, natural language processing (NLP), and machine learning to build a topic extraction framework. . Because Topics is a brand new concept at GitHub, we started with no cues from users on what defined a topic and what type of topics they would typically add to their repositories. Given our focus on improving discoverability, internally we defined Topics as any “ word or phrase that roughly describes the purpose of a repository and the type of content it encapsulates “. These can be words such as “data science”, “nlp”, “scikit-learn”, “clustering algorithm”, “jekyll plugin”, “css template”, or “python”. . While no tag or label-type feature existed prior to the start of this project, we did have a rich set of textual information to start from. At its core, GitHub is a platform for sharing software with other people, and some of the data typically found in a repository provides information to humans rather than instructions for computers. Repository names, descriptions, and READMEs are text that communicate functionality, use case, and features to human readers. That’s where we started. . We developed a topic extraction framework, called  repo-topix , to learn from the human-readable text that users provide in repo names, descriptions, and READMEs written by developers about their projects by incorporating methods from text mining, NLP, and supervised machine learning. At a high level, repo-topix does three things: . Generates candidate topics from natural language text by incorporating data from millions of other repositories . Selects the best topics from the set of candidates . Finds similarities and relationships in topics to facilitate discoverability . Below, we describe each step of the repo-topix framework in greater technical detail. .   . While README files within GitHub.com tend to be formatted using Markdown and reStructuredText with fairly lightweight formatting, there are certain sections such as code blocks, tables, and image links that are not useful for topic suggestions. For example, month names from within a table would not be useful to a user. . To extract text sections of interest, we developed a heuristics-based README tagger that marks sections in the README file as relevant or non-relevant. This simple tagger uses common formatting cues such as indentation, spacing, and use of backticks to determine “noise sections” and “valid text sections”. The use of a grammar-based parser was unnecessary as we only care about useful text sections and regard everything else in a README as noise. . Once we extract text sections of interest, we perform basic cleanup to remove file extensions, HTML tags, paths, and hosts from further processing, as these are more distracting than useful. Finally, the remaining text gets segmented into coarse-grained units using punctuation marks as well as README section markers such as contiguous hash symbols. . We use the cleaned text from the previous step to generate candidate topics by eliminating low-information words and breaking the remaining text into strings of one or multiple consecutive words, called n-grams. Like any text, our sources contain many words that are so common that they do not contain distinguishing information. Called stop words, these typically include determiners like “is”, “the”, “are”, conjunctions like “and,”, “but”, and “yet”, and so on. Given our specialized domain, we created a custom stop word list that included words that are practically ubiquitous in our source text; for example, “push”, “pull”, “software”, “tool”, “var”, “val”, and “package.” The custom stop word list provides an efficient way to finding potential topics, as we simply take the resulting phrases left between eliminated words. For example,  “this open source software is used for web scraping and search”  produces three candidate topics: 1. “open source,” 2. “web scraping,” 3. “search”. This process eliminates the need for brute-force n-gram generation which could end up producing a large number of n-grams depending the length of the README files being processed. After testing internally among GitHub staff, we found that n-grams made up of many words tended to be too specific (e.g. “machine-learning-tutorial-part-1-intro”), so we limit candidate topics to n-grams of size 4 or less. . While some of the generated word units in the previous step would be meaningful as topics, some could also be plain noise. We have a few strategies for pruning noise and unpromising candidate topics. The first is simply to eliminate phrases with low frequency counts. For example, if we had the following candidates with their corresponding counts, we could eliminate some of the low frequency topics: . From the above, we could easily eliminate topics that don’t satisfy a minimum frequency count threshold. However, this method doesn’t prune out topics with unwanted grammatical structure or word composition. For example, words and phrases like “great”, “cool”, “running slowly”, “performing operations”, “install database” and “ @kavgan ” (a GitHub handle) are not great topics for a repository. To aggressively prune out these keywords, we developed a supervised logistic regression model, trained to classify a topic as “good” (positive) or “bad” (negative). We call this the keyword filtering model.  We manually gathered about 300 training examples balanced across the positive (good topics) and negative (bad topics) categories. Because of this manual process with no input from users, our training data is actually fairly small. While it’s possible to learn from the actual words that make up a topic when you have a very large training set, with limited training data we used features that draw on the meta-information of the training examples so that our model does not just memorize specific words. For instance, one of the features we used was the  part-of-speech  usage within topics. If the model learns that single word  verbs  are often considered bad topics, the next time it sees such an occurrence, it would help eliminate such words from further consideration. Other features we used were occurrence of user names, n-gram size of a phrase, length of a phrase, and numeric content within a phrase. Our classifier is tuned for high recall in order to keep as many phrases as possible and prune obviously incorrect ones. . With time, we plan to include feedback from users to update the keyword filtering model. For example, highly accepted topics can serve as positive training examples and highly rejected topics can either be used as stop words or used as negative examples in our model. We believe that this incremental update would help weed out uninteresting topics from the suggestions list. . Instead of treating all remaining candidate topics with equal importance, we rank the candidates by score to return only the top-N promising topics instead of a large list of arbitrary topics. We experimented with several scoring schemes. The first scoring approach measures the average strength of association of words in a phrase using  pointwise mutual information  (PMI) weighted by the frequency count of the phrases. The second approach we tried uses the average  tf-idf  scores of individual words in a phrase weighted by the phrase frequency (if it’s more than one word long) and n-gram size. . We found that the first scoring strategy favored topics that were unique in nature because of the way PMI works when data is fairly sparse: unique phrases tend to get very high scores. While some highly unique phrases can be interesting, some unique phrases can just be typos or even code snippets that were not formatted as code. The second approach favored phrases that were less unique and relatively frequent. We ended up using the tf-idf based scoring as it gave us a good balance between uniqueness of a topic and relevance of a topic to a repository. While our tf (term frequency) scoring is based on local counts, our idf (inverse document frequency) weighting is based on a large dictionary of idf scores built using the unstructured content from millions of public READMEs. The idf weights essentially tell us how common or unique a term is globally. The intuition is that the more common a term, the less information it carries and should thus have a lower weight. For example, in the GitHub domain, the term “application” is much more common than terms such as “machine”, “learning”, or “assignment” and this is clearly reflected by their idf weights as shown below: . If a phrase has many words with low idf weighting, then its overall score should be lower compared to a phrase with more significant words – this is the intuition behind our tf-idf scoring strategy. As an example, assuming that the normalized tf of each word above is 0.5, the average tf-idf score for “machine-learning-application” would be  3.21  and the average tf-idf score for “machine-learning-assignment” would be  3.91 . The former has a lower score because the term “application” is more ubiquitous and has a lower idf score than the term “assignment”. . In addition to the base tf-idf scoring, we are also experimenting with some additional ideas such as boosting scores of those phrases that tend to occur earlier in a document and aren’t unique to a few repositories. These minor tweaks are subject to change based on our internal evaluation. . Because different users can express similar phrases in different ways, the generated topics can also vary from repository to repository. For example, we have commonly seen these variation of topics with different repositories: . To keep topic suggestions fairly consistent, we use a dictionary to canonicalize suggested topics. Instead of suggesting the original topics discovered, we suggest a canonicalized version of the topic if present in our dictionary. This in-house dictionary was built using all non-canonicalized topics across public repositories. The non-canonicalized topics give us cues as to which topics are most commonly used and which ones can be grouped together as being equivalent. We currently use a combination of  edit-distance , stemming, and word-level  Jaccard similarity  to group similar topics together. Jaccard similarity in our case estimates how similar two phrases are by comparing members of two sets to see which members are shared and which are distinct. With this, phrases that share many words can be grouped together. . While it’s possible to suggest all top-scoring topics, some topics may be fairly repetitive, and the set of topics returned may not provide enough variety for labeling a repository. For example, the following top-scoring topics (from an actual repository), while valid and meaningful, are not interesting and lack variety as it captures different granularity of similar topics: . We use a greedy topic selection strategy that starts with the highest-scoring topic. If the topic is similar to other lower-scoring topics, the lower-scoring topics are dropped from consideration. We repeat this process iteratively using the next highest-scoring topic until all candidate topics have been accounted for. For the example above, the final set of topics returned to the user would be as follows: . We use word-level Jaccard similarity when computing similarity between phrases, because it’s known to work well for short phrases. It also produces a score between 0-1, making it easy to set thresholds. . As topic labels were not available during the development of repo-topix, we needed to get a rough approximation of how well the suggested topics describe a repository. For this rough approximation, we used the description text for repositories since descriptions often provide insights into the function of a repository. If indeed the auto-suggested topics are not completely arbitrary, there should be some amount of overlap between suggested topics and the description field. For this evaluation, we computed  ROUGE-1  precision and recall. ROUGE is an n-gram overlap metric that counts the number of overlapping units between a system summary (suggested topics in our case) and a gold standard summary (description in our case). We performed this evaluation on roughly 127,000 public repositories with fairly long descriptions. These are our most recent results: . The ROUGE recall above tells us quantitatively how much of the description is being captured by topic suggestions and precision tells us what proportion of the suggestion words are words that are also in the description. Based on the results we see that there is some overlap as expected. We’re not looking for perfect overlap, but some level of overlap after disregarding all stop words. . Our topics extraction framework is capable of discovering promising topics for any public repository on GitHub.com. Instead of applying heavy NLP and complex parsing algorithms within our framework (e.g. grammar-based markdown parsing, dependency parsing, chunking, lemmatization), we focused on using lightweight methods that would easily scale as GitHub.com’s repository base grows over the years. For many of our tasks, we leverage the volume in available data to build out reusable dictionaries such as the IDF dictionary, which was built using all public README files, a custom stop-word list, and a canonicalization dictionary for topics. While we currently depend on the presence of README files to generate suggestions, in the future we hope to make suggestions by looking at any available content within a repository. Most of the core topics extraction code was developed using Java and Python within the  Spark  framework. . Our plan for the near future is to evaluate the usage of suggested topics as well as manually created topics to continuously improve suggestions shown to users. Some of the rejected topics could feed into our topics extraction framework as stop words or as negative examples to our keyword filtering model. Highly accepted topics could add positive examples to our keyword filtering model and also provide lessons on the type of topics that users care about. This would provide cues as to what type of “meta-information” users add to their repositories in addition to the descriptive terms found within README files. We also plan to explore topic suggestions on private repositories and with GitHub Enterprise in a way that fully respects privacy concerns and eliminates certain data dependencies. . Beyond these near term goals, our vision for using topics is to build an ever-evolving GitHub knowledge graph containing concepts and mapping how they relate to each other and to the code, people, and projects on GitHub. . These are references to some of the libraries that we used: . Want to work on interesting problems like code analysis, social network analysis, recommendations engine and improving search relevance? Apply  here ! ", "date": "July 31, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tKubernetes at GitHub\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2017-08-16-kubernetes-at-github/", "abstract": " Over the last year, GitHub has gradually evolved the infrastructure that runs the Ruby on Rails application responsible for  github.com  and  api.github.com . We reached a big milestone recently: all web and API requests are served by containers running in  Kubernetes  clusters deployed on our  metal cloud . Moving a critical application to Kubernetes was a fun challenge, and we’re excited to share some of what we’ve learned with you today. . Before this move, our main Ruby on Rails application (we call it  github/github ) was configured a lot like it was eight years ago:  Unicorn  processes managed by a Ruby process manager called  God  running on Puppet-managed servers. Similarly, our  chatops deployment  worked a lot like it did when it was first introduced: Capistrano established SSH connections to each frontend server, then  updated the code in place  and restarted application processes. When peak request load exceeded available frontend CPU capacity, GitHub Site Reliability Engineers would  provision additional capacity  and add it to the pool of active frontend servers. . While our basic production approach didn’t change much in those years, GitHub itself changed a lot: new features, larger software communities, more GitHubbers on staff, and way more requests per second. As we grew, this approach began to exhibit new problems. Many teams wanted to extract the functionality they were responsible for from this large application into a smaller service that could run and be deployed independently. As the number of services we ran increased, the SRE team began supporting similar configurations for dozens of other applications, increasing the percentage of our time we spent on server maintenance, provisioning, and other work not directly related to improving the overall GitHub experience. New services took days, weeks, or months to deploy depending on their complexity and the SRE team’s availability. Over time, it became clear that this approach did not provide our engineers the flexibility they needed to continue building a world-class service. Our engineers needed a self-service platform they could use to experiment, deploy, and scale new services. We also needed that same platform to fit the needs of our core Ruby on Rails application so that engineers and/or robots could respond to changes in demand by allocating additional compute resources in seconds instead of hours, days, or longer. . In response to those needs, the SRE, Platform, and Developer Experience teams began a joint project that led us from an initial evaluation of container orchestration platforms to where we are today: deploying the code that powers  github.com  and  api.github.com  to Kubernetes clusters dozens of times per day. This post aims to provide a high-level overview of the work involved in that journey. . As a part of evaluating the existing landscape of “platform as a service” tools, we took a closer look at Kubernetes, a project from Google that described itself at the time as  an open-source system for automating deployment, scaling, and management of containerized applications . Several qualities of Kubernetes stood out from the other platforms we evaluated: the vibrant open source community supporting the project, the first run experience (which allowed us to deploy a small cluster and an application in the first few hours of our initial experiment), and a wealth of information available about the  experience  that motivated its design. . These experiments quickly grew in scope: a small project was assembled to build a Kubernetes cluster and deployment tooling in support of an upcoming hack week to gain some practical experience with the platform. Our experience with this project as well as the feedback from engineers who used it was overwhelmingly positive. It was time to expand our experiments, so we started planning a larger rollout. . At the earliest stages of this project, we made a deliberate decision to target the migration of a critical workload:  github/github . Many factors contributed to this decision, but a few stood out: . Given the critical nature of the workload we chose to migrate, we needed to build a high level of operational confidence before serving any production traffic. . As a part of this migration, we designed, prototyped, and validated a replacement for the service currently provided by our frontend servers using Kubernetes primitives like Pods, Deployments, and Services. Some validation of this new design could be performed by running  github/github ‘s existing test suites in a container rather than on a server configured similarly to frontend servers, but we also needed to observe how this container behaved as a part of a larger set of Kubernetes resources. It quickly became clear that an environment that supported exploratory testing of the combination of Kubernetes and the services we intended to run would be necessary during the validation phase. . Around the same time, we observed that our existing patterns for exploratory testing of  github/github  pull requests had begun to show signs of growing pains. As the rate of deploys increased along with the number of engineers working on the project, so did the utilization of the several  additional deploy environments  used as a part of the process of validating a pull request to  github/github . The small number of fully-featured deploy environments were usually booked solid during peak working hours, which slowed the process of deploying a pull request. Engineers frequently requested the ability to test more of the various production subsystems on “branch lab.” While branch lab allowed concurrent deployment from many engineers, it only started a single Unicorn process for each, which meant it was only useful when testing API and UI changes. These needs overlapped substantially enough for us to combine the projects and start work on a new Kubernetes-powered deployment environment for  github/github  called “review lab.” . In the process of building review lab, we shipped a handful of sub-projects, each of which could likely be covered in their own blog post. Along the way, we shipped: . The end result is a chat-based interface for creating an isolated deployment of GitHub for any pull request. Once a pull request passed all required CI jobs, a user can deploy their pull request to review lab like so: . Like branch lab before it, labs are cleaned up one day after their last deploy. As each lab is created in its own Kubernetes namespace, cleanup is as simple as deleting the namespace, which our deployment system performs automatically when necessary. . Review lab was a successful project with a number of positive outcomes. Before making this environment generally available to engineers, it served as an essential proving ground and prototyping environment for our Kubernetes cluster design as well as the design and configuration of the Kubernetes resources that now describe the  github/github  Unicorn workload. After release, it exposed a large number of engineers to a new style of deployment, helping us build confidence via feedback from interested engineers as well as continued use from engineers who didn’t notice any change. And just recently, we observed some engineers on our High Availability team use review lab to experiment with the interaction between Unicorn and the behavior of a new experimental subsystem by deploying it to a shared lab. We’re extremely pleased with the way that this environment empowers engineers to experiment and solve problems in a self-service manner. . With review lab shipped, our attention shifted to  github.com . To satisfy the performance and reliability requirements of our flagship service – which depends on low-latency access to other data services – we needed to build out Kubernetes infrastructure that supported the  metal cloud  we run in our physical data centers and POPs. Again, nearly a dozen subprojects were involved in this effort: . The combination of all of this hard work resulted in a cluster that passed our internal acceptance tests. Given that, we were fairly confident that the same set of inputs (the Kubernetes resources in use by review lab), the same set of data (the network services review lab connected to over a VPN), and same tools would create a similar result. In less than a week’s time – much of which was spent on internal communication and sequencing in the event the migration had significant impact – we were able to migrate this entire workload from a Kubernetes cluster running on AWS to one running inside one of our data centers. . With a successful and repeatable pattern for assembling Kubernetes clusters on our metal cloud, it was time to build confidence in the ability of our Unicorn deployment to replace the pool of current frontend servers. At GitHub, it is common practice for engineers and their teams to validate new functionality by creating a  Flipper  feature and then opting into it as soon as it is viable to do so. After enhancing our deployment system to deploy a new set of Kubernetes resources  to a  github-production  namespace in parallel with our existing production servers and enhancing GLB to support routing staff requests to a different backend based on a Flipper-influenced cookie, we allowed staff to opt-in to the experimental Kubernetes backend with a button in our  mission control bar : . The load from internal users helped us find problems, fix bugs, and start getting comfortable with Kubernetes in production. During this period, we worked to increase our confidence by simulating procedures we anticipated performing in the future, writing runbooks, and performing failure tests. We also routed small amounts of production traffic to this cluster to confirm our assumptions about performance and reliability under load, starting with 100 requests per second and expanding later to 10% of the requests to  github.com  and  api.github.com . With several of these simulations under our belt, we paused briefly to re-evaluate the risk of a full migration. . Several of our failure tests produced results we didn’t expect. Particularly, a test that simulated the failure of a single apiserver node disrupted the cluster in a way that negatively impacted the availability of running workloads. Investigations into the results of these tests did not produce conclusive results, but helped us identify that the disruption was likely related to an interaction between the various clients  that connect to the Kubernetes apiserver (like  calico-agent ,  kubelet ,  kube-proxy , and  kube-controller-manager ) and our internal load balancer’s behavior during an apiserver node failure. Given that we had observed a Kubernetes cluster degrade in a way that might disrupt service, we started looking at running our flagship application on multiple clusters in each site and automating the process of diverting requests away from a unhealthy cluster to the other healthy ones. . Similar work was already on our roadmap to support deploying this application into multiple independently-operated sites, and other positive trade-offs of this approach – including presenting a viable story for low-disruption cluster upgrades and associating clusters with existing failure domains like shared network and power devices – influenced us to go down this route. We eventually settled on a design that uses our deployment system’s support for deploying to multiple “partitions” and enhanced it to support cluster-specific configuration via a custom Kubernetes resource annotation, forgoing the existing federation solutions for an approach that allowed us to use the business logic already present in our deployment system. . With Cluster Groups in place, we gradually converted frontend servers into Kubernetes nodes and increased the percentage of traffic routed to Kubernetes. Alongside a number of other responsible engineering groups, we completed the frontend transition in just over a month while keeping performance and error rates within our targets. . During this migration, we encountered an issue that persists to this day: during times of high load and/or high rates of container churn, some of our Kubernetes nodes will kernel panic and reboot. While we’re not satisfied with this situation and are continuing to investigate it with high priority, we’re happy that Kubernetes is able to route around these failures automatically and continue serving traffic within our target error bounds. We’ve performed a handful of failure tests that simulated kernel panics with  echo c &gt; /proc/sysrq-trigger  and have found this to be a useful addition to our failure testing patterns. . We’re inspired by our experience migrating this application to Kubernetes, and are looking forward to migrating more soon. While scope of our first migration was intentionally limited to stateless workloads, we’re excited about  experimenting with patterns for running stateful services on Kubernetes. . During the last phase of this project, we also shipped a workflow for deploying new applications and services into a similar group of Kubernetes clusters. Over the last several months, engineers have already deployed dozens of applications to this cluster. Each of these applications would have previously required configuration management and provisioning support from SREs. With a self-service application provisioning workflow in place, SRE can devote more of our time to delivering infrastructure products to the rest of the engineering organization in support of our best practices, building toward a faster and more resilient GitHub experience for everyone. . We’d like to extend our deep thanks to the entire Kubernetes team for their software, words, and guidance along the way. I’d also like to thank the following GitHubbers for their incredible work on this project: @samlambert, @jssjr, @keithduncan, @jbarnette, @sophaskins, @aaronbbrown, @rhettg, @bbasata, and @gamefiend. . Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us. Apply  here ! ", "date": "August 16, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tKeeping an eye on our network\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-09-05-keeping-an-eye-on-our-network/", "abstract": " Visibility is essential to effectively operating complex systems. As our network has grown, we’ve had to improve the the way we collect data about it to keep up. Key to these improvements has been the ability to tag metrics. Tagging has allowed us to build dashboards that start at a high level and facilitate drilling down into interesting or problematic areas by filtering and aggregating on multiple dimensions. A simple example below shows data flowing across a set of our spine switches during a rolling update: . Starting just prior to midnight UTC the first switch was drained and returned to service around 30 minutes later. From there, the other three spines were updated in sequence. We kept a close eye on this and related graphs during the process to make sure that traffic volumes were not impacted and that things left and returned to the devices when expected. It’s tough to fully demonstrate the flexibility we have in a handful of static graphs, but for example, if we wanted to dig in deeper to see what neighboring devices were sending traffic to the first spine we drained, we could flip around and look by neighbor: . Again traffic drains from and returns to the device during the maintenance. The interesting bit here is that we can now see what neighbors decided to use spine 1 due to BPG best path selection when spine 3 was out of consideration. Picking one of the leaf switches that shifted during the period and flipping things yet again we can plot what neighbors it was sending traffic to, this time including individual hosts given that it’s a leaf. The cutover from spine 3 to spine 1 and back is visible: . We have this instrumentation on all of our network devices with tagging suited to the role. One of the more interesting areas is the border routers, the devices that connect our data centers to the outside world via transit and peering. Some of the things we have the ability to filter and aggregate by are site, device, interface, provider/peer. Below is a graph of transit bandwidth across all devices in a point of presence grouped by provider over a one week period, and the same graph for a one hour span during a recent DDoS event: . This information is critical to detecting and defending against attacks, and essential to keeping an eye on capacity utilization and determining our future needs. When combined with host-level stats, flow data, and external route monitoring, it equips us with the data we need to effectively operate GitHub. . Network devices expose data about health and performance over  Simple Network Management Protocol  (SNMP.) While the protocol itself is in theory simple, understanding the schemas that lay out where to find the desired data is not. It takes spending some time with them before they start to make sense. . For our purposes there were two types of information: stats on the network interfaces and data about the performance and health of the devices themselves. Things like temperature, load, and memory usage are collected from device specific scalar values and stored for graphing and monitoring. Common interface stats are available via  IF-MIB::ifTable ,  IF-MIB::ifXTable , and in some cases additional vendor specific tables. Each row includes an index that allows grouping values for an interface. For example a subset of the fields available for index  528  on one of our border devices: . In the snippet above we can see that  987,957,376  octets have been received and  40,255,192,735  transmitted over a link to “A Provider” since the last time the counters rolled over. We currently collect around 20 symbols from these tables for all of our interfaces including in and out octets, drops, errors, and laser levels. We also collect interface speeds so that we can monitor and alert on interface utilization and spot problems before they have impact. . The neighbor functionality powering some of the graphs in the first section is build using data from the  Link Layer Discovery Protocol  available in  LLDP-MIB::lldpRemTable . With some devices that’s as simple as grabbing the value from a corresponding index e.g.  528 , but other devices require some intermediate mapping. . There are a pair of nodes in each region charged with collecting data from the network gear there. They run an agent that cycles every 10s. During each run the host checks to see if it’s the leader and if so, polls its pool of devices and submits the results. If not it goes back to sleep and tries again next time. This allows us to continue to get data in the event that one of the hosts fails. Thus far we’ve found two to be sufficiently robust, but the design allows for any number of hosts if we learn otherwise. . We’ve been working with the basics of this system for around nine months and evolving and extending it as we go. The LLDP neighbor functionality for instance is only a couple months old. Unfortunately, the nature and current state of our solution doesn’t lend itself to open sourcing. It has dependencies on and assumptions about the specifics of environment and hardware. . If network monitoring and telemetric excites you,  we’re looking to add more SREs to the team . ", "date": "September 5, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe data science behind topic suggestions\t\t", "author": ["\n\t\tKavita Ganesan\t"], "link": "https://github.blog/2017-09-14-the-data-science-behind-topic-suggestions/", "abstract": " Earlier this year, we launched  topics , a new feature that lets you tag repositories with descriptive words or phrases. Topics help you create connections between similar GitHub projects and explore them by type, technology, and other characteristics they have in common. .   . All public repositories show topic suggestions, so you can quickly tag repositories with relevant words and phrases. These suggestions are the result of some exciting data science work—in particular, a topic extraction framework based on text mining, natural language processing, and machine learning called repo-topix. .  Learn more about repo-topix from the Engineering Blog  .   . Now when you add or reject topics, you’re doing more than keeping projects organized. Every topic will contribute to surfacing connections and inspiring discovery across GitHub. Repository names, descriptions, and READMEs from millions of public projects serve as the very start of an ever-evolving knowledge graph of concepts. Eventually, the graph will map how these concepts relate to each other and to the code, people, and projects on GitHub. . Topics is part of a greater effort to use our public data to make meaningful improvements to how people discover, interact, and build on GitHub. We’ll be sharing more ways that data can improve the way you work at Universe—our flagship product and community conference. .  Get tickets to GitHub Universe  .  \t\t Tags:   \t\t insights \t ", "date": "September 14, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit LFS 2.3.0 released\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-09-14-git-lfs-2-3-0-released/", "abstract": " Git LFS v2.3.0 is now available with performance improvements to  git lfs migrate  and  git clone , new features, bug fixes, and more. .   Download Git LFS v2.3.0   . With our latest release,  git lfs migrate  ships with a native implementation for reading packed objects: an important next step to making Git LFS’s migrator performance significantly faster. Git LFS also learned how to avoid saving unchanged objects, making it 52% faster [ 1 ]  to examine your repository for large objects than in previous releases. . The  git clone  command is now 170% faster on repositories using Git LFS than in previous releases. [ 2 ]  That means the native  git clone  command is as fast as the (now deprecated)  git lfs clone  wrapper. With simultaneous object batching and transferring, you can expect dramatic performance improvements for tools that shell out to  git clone  or  git checkout . . You’ll also find support for new transfer agents, release targets, documentation, and more—all of which are thanks to gracious contributions from the Git LFS open source community. . For more information about the Git LFS v2.3.0 release, check out the  release notes . .  \t\t Tags:   \t\t Git \t ", "date": "September 14, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tWeak cryptographic standards deprecation update\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-09-18-crypto-deprecation-notice-update-1/", "abstract": "  Earlier this year , we announced the deprecation of several weak cryptographic standards. As noted during our initial announcement, the vast majority of HTTPS clients connect to GitHub using  TLSv1.2  and won’t be affected by our disabling of  TLSv1 / TLSv1.1 . Since the announcement, we have been focusing on the impact of disabling the  diffie-hellman-group1-sha1  and  diffie-hellman-group14-sha1  key exchanges for SSH. As of last week, we have enabled  diffie-hellman-group-exchange-sha256 . This key exchange method is widely supported and will allow most legacy clients to seamlessly transition away from  diffie-hellman-group1-sha1  and  diffie-hellman-group14-sha1 . . Since enabling  diffie-hellman-group-exchange-sha256 , we have seen some traffic automatically transition and start using the new key exchange algorithm. However, we still see a small percentage of traffic continue to use the older key exchange algorithms. This occurs for two reasons: . The most common reason a client might prefer an older algorithm is because it is an older client, and support for a more modern algorithm was new and not yet made the default. The majority of traffic currently preferring the older algorithms does support  diffie-hellman-group-exchange-sha256  and will transition to using it when we disable the older algorithms. The remaining traffic are clients that don’t support the newer key exchange algorithm and will be unable to connect to GitHub when we disable support for the older algorithms. This is a very small percentage of traffic, but we would like to see if we can reduce the incompatible traffic percentage even further before disabling support for the older key exchange algorithms on  February 1, 2018 . . We performed a deeper analysis of the “banner” sent to us from incompatible clients during connection setup and found the vast majority of the traffic is from various older versions of a  popular Java library  that implements the SSH protocol. Our logs show that JSch started supporting  diffie-hellman-group-exchange-sha256  in version 0.1.51 (released in 2014), but there are clients using older releases of the library. We are continuing to analyze our log data to try to identify projects that are using older versions of JSch, or any other incompatible library, so we can reach out to them directly. . As noted in the original announcement,  we plan to disable  TLSv1 / TLSv1.1 ,  diffie-hellman-group1-sha1 , and  diffie-hellman-group14-sha1  on February 1, 2018 . Given that support for  diffie-hellman-group-exchange-sha256  is deployed, this provides approximately five months for us (and external developers) to identify projects that are using outdated libraries, such as JSch, and upgrade to a more recent release. As always, if you have any questions or concerns related to this announcement, please don’t hesitate to  contact us . ", "date": "September 18, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Debug\t\t", "author": ["\n\t\tAlice Goldfuss\t"], "link": "https://github.blog/2017-09-19-github-debug/", "abstract": " GitHub is proud to handle thousands of requests per second from our millions of users. The Internet, however, can be a fickle beast of cables and sparks, and sometimes those requests don’t happen very fast (or at all). While we’re happy to help you troubleshoot connection issues to us, we also know our users like swift answers and a hands-on approach. . Today, we’re introducing  GitHub Debug . . This debugging tool mimics  github.com  as much as possible, including using GeoDNS and the same certificate authority. Using your public IP, it applies a set of standard networking tools to collect relevant information about your connection to GitHub. This information, including data on download speed, packet loss, and routing, can be used by you or us to troubleshoot connection issues between you and GitHub’s servers. . Visit  github-debug.com  when you’re having issues connecting to  github.com , whether on the command line or in your browser. . Have questions? Copy and paste your  github-debug.com  output into a ticket to our  Support Team  and we can start digging into the issue. . We’re excited to provide this tool for your use. Happy building! ", "date": "September 19, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tBug Bounty third anniversary wrap-up\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-03-14-bug-bounty-third-anniversary-wrap-up/", "abstract": "   . In honor of our Bug Bounty Program’s third birthday, we kicked off a  promotional bounty period  in January and February. In addition to bonus payouts, the scope of the bug bounty was expanded to include GitHub Enterprise. It may come as no surprise that including a new scope meant that the most severe bugs were all related to the newly included target. . There was no shortage of high-quality reports. Picking winners is always tough, but below are the intrepid researchers receiving extra bounties. . The first prize bonus of $12,000 goes to  @jkakavas  for their GitHub Enterprise  SAML authentication bypass  report which allowed an attacker to construct a SAML response that could arbitrarily set the authenticated user account. You can read more about the story on  their blog . . The second prize bonus of $8,000 goes to  @iblue  for their  remote code execution bug  found in the GitHub Enterprise management console. This was due to a static secret mistakenly being used to cryptographically sign the session cookie. The static secret was intended to only be used for testing and development. However, an unrelated change of file permissions prevented the intended (and randomly generated) session secret from being used. By knowing this secret, an attacker could forge a cookie that is deserialized by  Marshal.load , leading to remote code execution. . The third prize bonus of $5,000 goes to  @soby  for their report of another GitHub Enterprise  SAML authentication bypass . This attack showed it was possible to replay a SAML response to have our SAML implementation use unsigned data in determining which user account was authenticated. . The best report bonus of $5,000 goes to  @orangetw  for their  report of Server-Side Request Forgery . This report involved chaining together four vulnerabilities to deliver requests to internal services that end up executing attacker-controlled code. The reporter supplied a clear explanation of the problem through each step and included proof-of-concept scripts for the entire journey. While this report did not earn a prize for being the most severe, it is exactly the type of report we want to encourage reporters to submit. . We received a wonderful Christmas gift from  @orangetw  with a  SQL Injection bug on GitHub Enterprise . You can read more about how they  learned Rails in three days  before finding the bug. . A theme we’ve seen continue over the years is paying out for bugs that aren’t in our own code but in browsers. 2016 was no exception with  @filedescriptor ‘s report of  funky Internet Explorer behavior  detailing how a triple-encoded host value in a URL is handled in redirects. . Lastly, Unicode gonna Unicode.  @jagracy  found  a way to exploit the way Unicode was normalized in our code and in our database engine  to deliver password reset emails to entirely different addresses than what were intended. .  Our “Two Years of Bounties” post  has detailed stats for our submissions during the first two years of the program. These years saw a total payout of $95,300 across 102 submissions out of a total of 7,050 submissions (1.4% validity rate). So far in the third year of the program, we have paid out for 73 submissions for a total of $81,700. Many of these reports fall into our “$200 thank you” bucket. These are issues that we do not consider severe enough for an immediate fix, but that we still want to reward our researchers for. Forty-eight of these issues were deemed high enough risk to warrant a write-up on  https://bounty.github.com . We saw a total of 48 out of 795 valid reports, bringing our validity rate to 6%. . In 2016, we saw a slight decrease in the number of reports compared to the average of the previous two years. While we can’t know for certain, we suspect this is due to the ever-decreasing presence of “low hanging fruit.” Unfortunately, we saw an increase in the number of “critical” reports. All other categories saw a decrease in the number of reports. .   . Out of the accepted reports, we saw some notable changes in the type of vulnerability reports submitted. Many categories such as XSS, Injection, and CSRF saw a decrease in reports. Notably, the number of valid CSRF reports for 2016 was zero. At the same time, we saw an increase in session handling bugs, sensitive data exposure, and missing function level access controls. .   . In April of 2016, we transitioned to the HackerOne platform. The graphs will not include all data for the year, unfortunately, but the data is still interesting. Notably, even though we had run a public bounty for almost 2.4 years, we still experienced a large spike upon announcing the transition, just like many programs’ initial public launch. .   . We’ve also been able to track new data via the platform. For example, our average response was about 16 hours and our time to resolution was about 28 days. Tracking this data and keeping it within acceptable bounds will help ensure our program continues to run smoothly and efficiently. . We continue to see rewards being donated to charities. We absolutely love donating bounties, and we match all contributions. This year saw donations to  Doctors without Borders  and the  Electronic Frontier Foundation (EFF) . . We also sponsored an event that was aimed at helping people from under-represented backgrounds participate in bug bounties. The  h1-415 event  saw attendance from groups like  Hack the Hood ,  Women in Security and Privacy (WISP) ,  FemHacks ,  Lairon College Cyber Patriots , and  /dev/color . Hack the Hood was nice enough to make  a video from the event . . In 2016, we’ve learned a lot about running a bug bounty program and we’ve continued to uncover sharp edges of our services and codebase. Our program will continue to evolve to engage and support the community of bounty hunters that have made this program successful. We look forward to your submission in our fourth year of the program! . Best regards and happy hacking, .  @GitHubSecurity  ", "date": "March 14, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tSHA-1 collision detection on GitHub.com\t\t", "author": ["\n\t\tJeff King\t"], "link": "https://github.blog/2017-03-20-sha-1-collision-detection-on-github-com/", "abstract": " A few weeks ago, researchers announced  SHAttered , the first collision of the SHA-1 hash function. Starting today, all SHA-1 computations on GitHub.com will detect and reject any Git content that shows evidence of being part of a collision attack. This ensures that GitHub cannot be used as a platform for performing collision attacks against our users. . This fix will also be included in the next patch releases for the supported versions of GitHub Enterprise. . Git stores all data in “objects.” Each object is named after the SHA-1 hash of its contents, and objects refer to each other by their SHA-1 hashes. If two distinct objects have the same hash, this is known as a collision. Git can only store one half of the colliding pair, and when following a link from one object to the colliding hash name, it can’t know which object the name was meant to point to. . Two objects colliding accidentally is exceedingly unlikely. If you had five million programmers each generating one commit per second, your chances of generating a single accidental collision before the Sun turns into a red giant and engulfs the Earth is about 50%. . If a Git fetch or push tries to send a colliding object to a repository that already contains the other half of the collision, the receiver can compare the bytes of each object, notice the problem, and reject the new object. Git has implemented this detection since its inception. . However, SHA-1 names can be assigned trust through various mechanisms. For instance, Git allows you to cryptographically sign a commit or tag. Doing so signs only the commit or tag object itself, which in turn points to other objects containing the actual file data by using their SHA-1 names. A collision in those objects could produce a signature which appears valid, but which points to different data than the signer intended. In such an attack the signer only sees one half of the collision, and the victim sees the other half. . The recent attack cannot generate a collision against an existing object. It can only generate a colliding pair from scratch, where the two halves of the pair are similar but contain a small section of carefully-selected random data that differs. . An attack therefore would look something like this: .   Generate a colliding pair, where one half looks innocent and the other does something malicious. This is best done with binary files where humans are unlikely to notice the difference between the two halves (the recent attack used PDFs for this purpose).   . Generate a colliding pair, where one half looks innocent and the other does something malicious. This is best done with binary files where humans are unlikely to notice the difference between the two halves (the recent attack used PDFs for this purpose). .   Convince a project to accept your innocent half, and wait for them to sign a tag or commit that contains it.   . Convince a project to accept your innocent half, and wait for them to sign a tag or commit that contains it. .   Distribute a copy of the repository with the malicious half (either by breaking into a hosting server and replacing the innocent object on disk, or hosting it elsewhere and asking people to verify its integrity based on the signatures).  Anybody verifying the signature will think the contents match what the project owners signed.   . Distribute a copy of the repository with the malicious half (either by breaking into a hosting server and replacing the innocent object on disk, or hosting it elsewhere and asking people to verify its integrity based on the signatures).  Anybody verifying the signature will think the contents match what the project owners signed. . Generating a collision via brute-force is computationally too expensive, and will remain so for the foreseeable future.  The recent attack uses special techniques to exploit weaknesses in the SHA-1 algorithm that find a collision in much less time. These techniques leave a pattern in the bytes which can be detected when computing the SHA-1 of either half of a colliding pair. . GitHub.com now performs this detection for each SHA-1 it computes, and aborts the operation if there is evidence that the object is half of a colliding pair. That prevents attackers from using GitHub to convince a project to accept the “innocent” half of their collision, as well as preventing them from hosting the malicious half. . The actual  detection code  is open-source and was written by Marc Stevens (whose work is the basis of the SHAttered attack) and Dan Shumow. We are grateful for their work on that project. . Not yet. Git’s object names take into account not only the raw bytes of the files, but also some Git-specific header information. The PDFs provided by the SHAttered researchers collide in their raw bytes, but not when added to a Git repository. The same technique could be used to generate a Git object collision, but like the generation of the original SHAttered PDFs, it would require spending hundreds of thousands of dollars in computation. . Blocking collisions that pass through GitHub is only the first step. We’ve already been working with the Git project to include the collision detection library upstream. Future versions of Git will be able to detect and reject colliding halves no matter how they reach the developer: fetching from other hosting sites, applying patches, or generating objects from local data. . The Git project is also developing a plan to transition away from SHA-1 to another, more secure hash algorithm, while minimizing the disruption to existing repository data. As that work matures, we plan to support it on GitHub. ", "date": "March 20, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tOpen sourcing our Delegated Account Recovery implementation\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-04-14-open-sourcing-our-delegated-recovery-implementation/", "abstract": " In February, we shipped the “ Recover Accounts Elsewhere ” feature to help people regain access to their accounts if they lose access to their two-factor device or token. It is an implementation of the  Delegated Account Recovery specification  published by Facebook. .  GitHub will be open sourcing a Ruby library soon so that the complicated and dangerous code paths will be abstracted away and we will provide a Ruby reference implementation as a guide.  . We’ve released a Ruby library:  Darrrr  (say it with your best Hollywood pirate accent). The library is not coupled with any frameworks and can be used for Rails and non-Rails applications alike. Along with the library, you’ll find a Sinatra application that demonstrates the entire flow. . Additionally, Facebook has released  SDKs for Java and JavaScript , including demo applications. They’re also inviting developers to  join a beta program  for becoming an integrator. You can read more about Facebook’s release on their  Protect the Graph page . . Providing open source implementations for common languages was always part of the plan to help ease adoption of the recovery protocol. We hope these libraries and sample implementations lead to more integrations! ", "date": "April 14, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tEnabling DNS split authority with OctoDNS\t\t", "author": ["\n\t\tRoss McFarland\t"], "link": "https://github.blog/2017-04-27-enabling-split-authority-dns-with-octodns/", "abstract": " Building robust systems involves designing for failure. As Site Reliability Engineers at GitHub, we’re always on the lookout for places where redundancy can help to mitigate problems, and today we’ll be talking about steps we’ve recently taken to shore up how you locate our servers via DNS. . Large  DNS  providers have many levels of redundancy built into their services, but issues will arise causing outages and there are steps that can be taken to lessen their impact. One of the best options available is to split authority for your zones across multiple providers. Enabling split authority is straightforward, you just configure two or more sets of  name servers  for your zones in the registrar and DNS requests will be split across the full list. However, the catch is that you now have to keep the records for those zones in sync across multiple providers and depending on the details, that can either be complex to set up or a completely manual process. . The above query is asking a  TLD name server  for  github.com.   NS  records. It returns the values configured in our registrar, in this case four each of two providers. If one of those providers was to experience an outage, the other would hopefully still be available to service requests. We keep our records in sync in further places and can safely change over to them without having to worry about stale or incorrect state. . The last piece of fully configuring split authority is to add all of the name servers as apex  NS  records, the root of the zone, in both providers. . At GitHub we have dozens of zones and thousands of records, and while the majority of those aren’t critical enough to require redundancy, we have a fair number that do. We wanted a solution that was able to keep these records in sync in multiple providers and more generally manage all of our DNS records, both internal and external. So today we’re announcing  OctoDNS . .   . OctoDNS has allowed us to revamp our DNS workflow. Our zones and records are laid out in config files stored in a Git repo. Changes now use the  GitHub Flow  and are  branch deployed just like the site . We can even do “noop” deploys to preview what records will be modified by a change. The config files are yaml dictionaries, one per zone, where the top-level keys are record names and the values lay out the ttl, type, and type-specific data. For example, the following config will create the  A  record  octodns.github.com.  when included in the zone file  github.com.yaml . . The second piece of configuration maps sources of record data to providers. The snippet below tells OctoDNS to load the zone  github.com  from the  config  provider and to sync the results to  dyn  and  route53 . . Once our configuration is in place OctoDNS can evaluate the current state and build a plan listing the set of changes it would need to match the targets’ state to the source’s. In the example below,  octodns.github.com  is a new record so the required action is to create the record in both. . By default  octodns-sync  is in dry-run mode, so no action was taken. Once we’ve reviewed the changes and we’re happy with them, we can run the command again and add the  --doit  flag. OctoDNS will run through it’s process and this time continue on to make the necessary changes in Route53 and Dynect so that the new record exists. . At this point we have consistent record data stored in both providers and can comfortably split our DNS requests across them knowing they’ll be providing the accurate results. While we’re running OctoDNS commands directly above, our internal workflow relies on deploy scripts and chatops. You can find more about that in the  workflow section of the README . . Split authority is something we feel most sites can benefit from and the hope is that with  OctoDNS , one of the biggest obstacles to enabling it has been removed. Even if split authority isn’t of interest, OctoDNS may still be worth a look as it brings the benefits of  Infrastructure as Code  to DNS. . Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us.  Apply Here  ", "date": "April 27, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit LFS 2.1.0 released\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-04-28-git-lfs-2-1-0-released/", "abstract": " Today we’re announcing the next major release of  Git LFS : v2.1.0, including new features, performance improvements, and more. . With Git LFS 2.1.0, get a more comprehensive look at which files are marked as modified by running the  git lfs status  command. . Git LFS will now tell you if your large files are being tracked by LFS, stored by Git, or a combination of both. For instance, if LFS sees a file that is stored as a large object in Git, it will convert it to an LFS pointer on checkout which will mark the file as modified. To diagnose this, try  git lfs status  for a look at what’s going on: . Git LFS 2.1.0 introduces support for  URL-style configuration  via your  .gitconfig  or  .lfsconfig . For settings that apply to URLs, like  http.sslCert  or  lfs.locksverify , you can now scope them to a top-level domain, a root path, or just about anything else. . To better understand and debug network requests made by Git LFS, version 2.1.0 introduces a detailed view via the  GIT_LOG_STATS=1  environment variable: . The Git LFS API has long supported an  expires_at  property in both  SSH authenticate  as well as  Batch API responses . This introduced a number of issues where an out-of-sync system clock would cause LFS to think that objects were expired when they were still valid. Git LFS 2.1.0 now supports an  expires_in  property to specify a duration relative to your computer’s time to expire the object. . The LFS team is working on a migration tool to easily migrate your existing Git repositories with large objects into LFS without the need to write a  git filter-branch  command. We’re also still inviting your feedback on our  File Locking feature . . In addition, our  roadmap  is public: comments, questions, and pull requests are welcomed. To learn more about Git LFS, visit the  Git LFS website . . That was a quick overview of some of the larger changes included in this release. To get a more detailed look, check out the  release notes . .  \t\t Tags:   \t\t Git \t ", "date": "April 28, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow Four Native Developers Wrote An Electron App\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-05-16-how-four-native-developers-wrote-an-electron-app/", "abstract": " Today we released the new  GitHub Desktop Beta , rewritten on  Electron . . Electron is a well-known on-ramp for web developers to build desktop apps using familiar web technologies: HTML, CSS, and JavaScript. Our situation was different. Everyone on the GitHub Desktop team is a native developer by trade—three from the .NET world and one from Cocoa. We knew how to make native apps, so how and why did we end up here? . First, the elephant in the room: why? Rewrites are rarely a good idea. Why did we decide to walk away from two codebases and rewrite? . From the start, GitHub Desktop for macOS and Windows were two distinct products, each with their own team. We worked in two separate tech stacks using two different skill sets. To maintain parity across the codebases, we had to implement and design the same features twice. If we ever wanted to add Linux support, we’d have to do it all a third time. All this meant we had twice the work, twice the bugs, and far less time to build new features. . As it turns out, building native apps for multiple platforms doesn’t scale. . This isn’t a new or unique problem. Over the years we explored various ways for evolving the existing applications towards a shared codebase, including  Electron ,  Xamarin , a shared C++, or in our wildest dreams, Haskell. . We’d already  experimented with web technologies  to share work. The gravitational pull of the web was too strong to resist. . Beyond our own experience, we had to acknowledge the critical mass accumulating around web technologies. Companies like Google, Microsoft, and Facebook, not to mention GitHub, are investing  incredible  amounts of time, money, and engineering effort in the web as a platform. With Electron, we leverage that investment. . The web isn’t a perfect platform, but native apps aren’t built on perfect platforms either. Rewriting on Electron does mean swapping one set of tradeoffs for another. . Now we can share our logic and UI across all platforms, which is fantastic. But Windows and macOS are different and their users have different expectations. We want to meet those expectations as much as possible, while still sharing the same UI. This manifests in a number of ways, some big and some small. . In the small, some button behaviors vary depending on your platform. On macOS, buttons are  Title Case . On Windows they are  Sentence case . Or on Windows, the default button in dialogs is on the left, where on macOS it’s the right. We enforce the latter convention both  at runtime  and with a  custom lint rule . . On macOS, Electron gives us access to the standard app menu bar, but on Windows, the menu support is less than ideal. The menu is shown in the window frame, which doesn’t work with our frameless window design. The built-in menu’s usability is also rough around the edges and doesn’t support the keyboard accessibility Windows users expect. . We  worked hard  to recreate a Windows-appropriate menu in web technologies, complete with access keys and appropriate focus states. . There were times when the web platform or Electron didn’t provide us with the APIs we needed. But in contrast to building a web app, building on Electron meant that we weren’t stuck. We could do something about it. . We  added   support  in Electron for importing certificates into the user’s certificate store, using the appropriate platform-specific APIs. . The web’s internationalization support doesn’t provide fine-grained localization information. For example, we can’t format numbers in a  locale-aware manner , distinct from the  preferred language  defined by the user. Similar to adding the import certificate support, we plan to add better localization support to Electron. . If you’re making a native app, your tech stack choices are pretty limited. On the macOS side, you’ll use Xcode, Swift, and AppKit. On Windows, you’ll use Visual Studio, C#, and WPF or UWP. But in the web world, choices abound. React? Angular? CSS? SASS? CSS in JS? Some compile-to-JavaScript language? Browserify? Webpack? Gulp? Grunt? The ecosystem is enormous. The relative openness of the web platform also means developers are able to experiment and innovate, independent from the constraints of any single vendor. This cuts both ways: the array of choices can be  overwhelming , but it also means you’re more able to pick the right tool for the job. . We were coming from C#, Objective-C, and Swift where static type systems let the compiler watch our back and help us along the way. For us, the question wasn’t  if  we’d choose a compile-to-Javascript language but rather which one. . At the same time, one of the big benefits to writing an Electron app is JavaScript itself. It’s the lingua franca of programming. This lowers the barrier of entry for an  open source project like ours . So while languages like  Elm  and  PureScript  are interesting and would scratch our static types itch, they were too far outside the mainstream for us to consider. . Our best two options were  Flow  and  TypeScript . We landed on TypeScript. At the time we started the project, Flow’s Windows support lagged far behind macOS. For a team like ours where more than half of the engineers live on Windows, this was an instant deal-breaker. Thankfully, TypeScript has been fantastic. Its type system is incredibly expressive, the team at Microsoft moves fast and is responsive to the community, and the community grows every day. . We learned to take the long feedback cycles of native development as a given. Change the code, compile, wait, launch the app, wait, see the change. This doesn’t seem like much, but it adds up. But every minute spent waiting for the compiler to compile or the app to launch is waste. It is time where we could lose our focus, fall out of the flow, and get distracted. . Using web technologies tightens up our feedback cycle. We can tweak designs live, in the app, as it shows real data. Code changes reload in place. Our feedback cycle went from minutes to seconds. It keeps us motivated! . We’ve been working on GitHub Desktop Beta for  just over a year . We’re happy with how far we’ve been able to come in that time, but it’s far from done.  Check it out ,  leave us your feedback , and  get involved ! .  William Shepherd   Application Engineer   GitHub Profile  .  Josh Abernathy   Engineering Manager   GitHub Profile |   Twitter Profile  .  Markus Olsson   Application Engineer   GitHub Profile |   Twitter Profile  .     .  Brendan Forster   Application Engineer   GitHub Profile |   Twitter Profile  ", "date": "May 16, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit 2.13 has been released\t\t", "author": ["\n\t\tJeff King\t"], "link": "https://github.blog/2017-05-10-git-2-13-has-been-released/", "abstract": " The open source Git project has just released Git 2.13.0, with features and bugfixes from over  65 contributors . Before we dig into the new features, we have a brief security announcement. . For those running their own Git hosting server, Git 2.13 fixes a vulnerability in the  git shell  program in which an untrusted Git user can potentially run shell commands on a remote host.  This only affects you if you’re running a hosting server and have specifically configured  git shell . If none of that makes sense to you, you’re probably fine. See  this announcement  for more details. As neither GitHub.com nor GitHub Enterprise uses  git shell , both are unaffected. . Phew. With that out of the way, let’s get on to the fun stuff. . Did I say fun? Oops, we’re not there yet. . You may have heard that researchers recently found the  first collision  in SHA-1, the hash function Git uses to identify objects. Their techniques may eventually be used to conduct collision-based attacks against Git users. Fortunately those same researchers also provided a way to detect content that is trying to exploit this technique to create collisions. In March, GitHub.com  began using that implementation  to prevent it being used as a potential platform for conducting collision attacks. . Git 2.13 ships with similar changes, and will detect and reject any objects that show signs of being part of a collision attack. The collision-detecting SHA-1 implementation is now the default. The code is included with Git, so there’s no need to install any additional dependencies.  Note that this implementation is slower than the alternatives, but in practice this has a negligible effect on the overall time of most Git operations (because Git spends only a small portion of its time computing SHA-1 hashes in the first place). . In other collision detection news, efforts have continued to develop a transition plan and to prepare the code base for handling new hash functions, which will eventually allow the use of stronger hash algorithms in Git. . [ collision detection ,  SHA-1 transition 1 ,  SHA-1 transition 2 ] . You’ve probably passed path arguments to Git before, like: . But you may not have known that to Git, the  foo.c  and  program.rb  arguments are actually   pathspecs  , a Git-specific pattern for matching paths. Pathspecs can be literal paths, prefixes, or wildcards: . But they also have a powerful extension syntax. Pathspecs starting with  :(magic)  enable special matching features. The complete list can be found in the   pathspec  section of  git help glossary  , but let’s look at a few here. . For instance, you may want to exclude some files from a grep, which you can do with the  :(exclude)  directive: . There are a few things to note in that example. The first is that we had to put our pathspec after a  --  (double-dash) separator. This is necessary because most Git commands actually take a combination of revisions and pathspecs. The full syntax is  [&lt;revisions&gt;] -- [&lt;pathspecs&gt;] . If you omit the double-dash, Git will check each argument to see if it’s either a valid object name or a file in the filesystem. But since our exclude pattern is neither, without the double-dash Git would give up and complain (this may change in a future version of Git; wildcards like  *.c  used to have the same problem, but the rules were recently loosened to resolve them as pathspecs). More information is available via   git help cli  . . The second thing to note is that typing  :(exclude)  is a pain, and we have to quote it from the shell. But there’s a solution for that: short form pathspec magic. The short form for  exclude  is  !  (exclamation point). This is easy to remember, since it matches the syntax in other parts of Git, like  .gitignore  files. . That’s shorter than  exclude , but we still have to quote, since the exclamation point triggers history expansion in most shells. Git 2.13 adds  ^  (caret) as a synonym for the exclamation point, letting you do the same thing without any shell quoting: . Ah, much better. Technically we would need to also quote the  *.c  wildcard from the shell, but in practice it works out. Unless you have a file that starts with  :^  and ends in  .c , the shell will realize that the wildcard matches nothing and pass it through to Git verbatim. . But wait, there’s more! Git 2.13 also adds the  attr  token, which lets you select files based on their gitattributes values. For instance, if you use Git LFS, you may want to get a list of files which have been configured to use it: . You can even define your own attributes in order to group files. Let’s say you frequently want to  grep  a certain set of files. You can define an attribute, and then select those files using that attribute: . And if you want to get really fancy, you can combine the  attr  and  exclude  tokens: . Note that the  attr  token is not yet supported in all parts of the code. Some commands may report that it cannot be used with them, but this is likely to be expanded in future versions of Git. . [ negative pathspecs ,  attribute pathspecs ] . Git’s configuration system has several levels of priority: you can specify options at the system level, the user level, the repository level, or for an individual command invocation (using   git -c  ). In general, an option found in a more specific location overrides the same option found in a less specific one. Setting  user.email  in a repository’s  .git/config  file will override the user-level version you may have set in  ~/.gitconfig . . But what if you need to set an option to one value for a group of repositories, and to another value for a different group? For example, you may use one name and email address when making commits for your day job and another when working on open source. You can set the open source identity in the user-level config in your home directory and then override it in the work repositories. But that’s tedious to keep up to date, and if you ever forget to configure a new work repository, you’ll accidentally make commits with the wrong identity! . Git 2.13 introduces  conditional configuration includes . For now, the only supported condition is matching the filesystem path of the repository, but that’s exactly what we need in this case. You can configure two conditional includes in your home directory’s  ~/.gitconfig  file: . Now you can put whatever options you want into those files: . The appropriate config options will be applied automatically whenever you’re in a repository that’s inside your  work  or  play  directories. . [ conditional includes ] .  --decorate=auto  is now the default for  git log . When output is sent to the user’s terminal, commits that are pointed to directly by a branch or tag will be “decorated” with the name of the branch. [ source ] .  git branch ‘s output routines have been ported to the  ref-filter  system shared by  git for-each-ref  and  git tag . This means you can now use  git branch --format=  to get custom output. See   git help for-each-ref   for the list of substitutions. As a side note, these patches are from Karthik Nayak, Git’s Google Summer of Code student from 2015. Though his GSoC project to introduce  ref-filter  was completed almost two years ago, he’s continued contributing to the project. Great work! [ source ] .  git branch ,  git tag , and  git for-each-ref  all learned the  --no-contains  option to match their existing  --contains  option.  This can let you ask which tags or branches  don’t  have a particular bug (or bugfix).  [ source ] .  git stash  now accepts pathspecs. You can use this to create a stash of part of your working tree, which is handy when picking apart changes to turn into clean commits.  [ source ] . The special branch names  @{upstream} ,  @{u} , and  @{push}  are now case-insensitive. This is especially convenient as both  @  and  {  require holding down the shift key on most keyboards, making it easy to accidentally type a capital  U . Now you can hold that shift key AS LONG AS YOU WANT. [ source ] . More commands have learned to recurse into submodules in the past few versions of Git, including  checkout ,  grep , and  ls-files .   git status --short  also now reports more information about submodules.  [ source ,  source ,  source ,  source ] . The last few versions of Git have cleaned up many corner cases around repository discovery and initialization. As a final step in that work, Git 2.13 introduced a new assertion to catch any cases that were missed. After being tested for months in development versions, this shouldn’t trigger. But it’s possible that you may see  BUG: setup_git_env called without repository . If you do, please consider making a  bug report .  [ source ] . That’s just a sampling of the changes in Git 2.13, which contains over 700 commits. Check out the  the full release notes  for the complete list. ", "date": "May 10, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntegrating Git in Atom\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-05-16-integrating-git-in-atom/", "abstract": " The Atom team has been working to bring the power of Git and GitHub as close to your cursor as possible. With  today’s release of the GitHub package for Atom , you can now perform common Git operations without leaving the editor: stage changes, make commits, create and switch branches, resolve merge conflicts, and more. . In this post, we’ll look at the evolution of how the  Atom GitHub package  interacts with the  .git  folder in your project. .     . GitHub is a core contributor to a library called  libgit2 , which is a reentrant C implementation of Git’s core methods and is used to power the backend of GitHub.com via Ruby bindings. Our initial approach to the development of this new Atom package used  Nodegit , a Node module that provides native bindings to libgit2. . Months into development we started to question whether this was the optimal approach for our Atom integration. libgit2 is a powerful library that implements the core data structures and algorithms of the Git version control system, but it intentionally implements only a subset of the system. While it is  very effective  as a technology that powers the backend of GitHub.com, our use case is sufficiently different and more akin to the Git command-line experience. . Compare what we had to do with Nodegit/libgit2 versus shelling out: .  Nodegit/libgit2  . Read the current index file . Update the files that have changed . Create a tree with this state . Write the updated index back to disk . Manually run pre-commit hooks . Create the new commit with the tree . Manually sign the commit if necessary . Update the currently active branch to point to the commit . Manually run post-commit hook .  Shelling out  . Run  git commit , the command-line tool made for our exact use case . Shelling out to Git simplifies development, gives us access to the full set of commands, options, and formatting that Git core provides, and enables us to use all of the latest Git features without having to reimplement custom logic or wait for support in libgit2. For these reasons and more, we made the switch. . We bundled a minimal version of Git for Mac, Windows, and Linux into a package called  dugite-native  and created a lightweight library called  dugite  for making Node  execFile  calls. Bundling Git makes package installation easier for the user and gives us full control over the Git API we are interacting with. . As much as possible, we keep your Git data in Atom in sync with the actual state of your local repo to allow for maximal flexibility. You can partially stage a file in Atom, switch to the command line and find the state of your repo exactly as you’d expect. Additionally, any changes you make outside of Atom will be detected by a file watcher and the Git data in your editor will be refreshed automatically. . Overall, the transition from Nodegit to shelling out went pretty well. However, there were noticeable performance tradeoffs and overhead costs associated with spawning a new process every time we asked for Git data. . Recent Atom releases  have   delivered   numerous   performance   improvements , and we wanted this new package to demonstrate our continued focus on responsiveness. After core functionality was in place, we introduced a series of optimizations. To inform and measure progress on this front, we created a  custom waterfall view  to visualize the time spent shelling out to Git; the red section shows the time an operation spent waiting in the queue for its turn to run, while the yellow and green represent the time the operation took to actually execute. . Here’s what it looked like before and after we  parallelized read operations  based on the number of cores on a user’s computer: .     . We also noticed that for larger repos we would get file-watching update events in several batches, each causing a model update to be scheduled. A merge with conflicts in  github/github , the GitHub.com codebase, would queue up 12 updates. To address this we redesigned our   ModelObserver   to never schedule more than a single pending fetch if new update requests come in while a fetch is in progress,  preventing ModelObserver update backlogs . .     . Aggressive  caching  and selectively invalidating cached repository state reduced the number of times we shell out to Git so that we avoid the performance penalty of launching a new process: .     . Even though we spawn subprocesses asynchronously, there is still a small synchronous overhead to shelling out to Git. Normally, this is no more than a couple milliseconds. On rare occasions, however, the application would get into a strange state, and this time would begin to grow; this overhead is represented in the waterfall views above by the yellow sections. The additional time spent in synchronous code would block the UI thread long enough to degrade the user experience. The issue would persist until the Atom window was refreshed. .     . After investigating the root cause of this issue, we realized that a proper fix for it would have involved changing Node or libuv. Since our launch date was looming on the horizon, we needed a more immediate solution and made the decision to work around the problem by making Git calls in a separate process. This would keep the main thread free and prevent locking the UI when this issue arises. . Our first approach used forked Node processes, but benchmarking revealed that IPC time grows quadratically relative to message size, which could become an issue when reading large diffs from stdout. This issue seems to be  fixed  in future versions of Node, but again, time was of the essence and we couldn’t afford to wait. Thankfully, IPC times using Electron renderer processes were much more reasonable, so our short term solution involved using a  dedicated renderer process  to run Git commands. . We introduced a   WorkerManager   which creates a   Worker   that wraps a   RendererProcess   which shells out to Git and sends results back over IPC. If the renderer process is not yet ready, we fall back to shelling out in process. We track a running average of the time it takes to make a spawn call and if this exceeds a specified threshold, the  WorkerManager  creates a new  Worker  and routes all new Git data requests to it. With this approach, if the long spawn call issue manifests, users will experience no freezing due to a blocked main thread. At worst, they may experience slower UI updates, but once a new renderer process is up the spawn times should drop back down and normal responsiveness should be restored. . As with most decisions, there are tradeoffs. Here we prevent indefinite locking of the UI, but there is now extra time spent in IPC and overhead costs associated with creating new Electron renderer processes. In the timeline below, the pink represents the IPC time associated with each Git command. .     . Once we  upgrade Atom to Electron v1.6.x  in the next release cycle, we’ll be able to re-implement this system using Chromium Web Workers with Node integration. Using the  SharedArrayBuffer  object, we can read shared memory and bypass IPC, cutting down overall operation time. And using native Web Workers rather than Electron Renderer Processes will reduce the overhead associated with these side processes and save on computing resources for shelling out to Git. . In addition to more performance improvements, you can look forward to more Git features, UI/UX improvements, and more comprehensive and in-depth GitHub integration. . As developers, much of our work is powered by a few key tools that enable us to write software and collaborate. We spend most of our days in our editors, periodically pausing to take version control snapshots of our code, and soliciting input and feedback from our colleagues. It’s every developer’s dream to be able to do all of these things with minimal friction and maximal ease. With these new integrations the Atom team is working to make those dreams more of a reality. .  Want to help the Atom team make developers’ lives easier?  We’d love for you to join us. Keep an eye out for a  job posting  coming soon! .  Katrina Uychaco   Application Engineer, Atom   GitHub Profile |   Twitter Profile  .  Michelle Tilley   Application Engineer, Atom   GitHub Profile |   Twitter Profile  .  Ash Wilson   Application Engineer, Atom   GitHub Profile  |  Twitter Profile  .   ", "date": "May 16, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tDNS Infrastructure at GitHub\t\t", "author": ["\n\t\tJoe Williams\t"], "link": "https://github.blog/2017-05-31-dns-infrastructure-at-github/", "abstract": " At GitHub we recently revamped how we do DNS from the ground up. This included both how we  interact with external DNS providers  and how we serve records internally to our hosts. To do this, we had to design and build a new DNS infrastructure that could scale with GitHub’s growth and across many data centers. . Previously GitHub’s DNS infrastructure was fairly simple and straightforward. It included a local, forwarding only DNS cache on every server and a pair of hosts that acted as both caches and authorities used by all these hosts. These hosts were available both on the internal network as well as public internet. We configured zone stubs in the caching daemon to direct queries locally rather than recurse on the internet. We also had NS records set up at our DNS providers that pointed specific internal zones to the public IPs of this pair of hosts for queries external to our network. . This configuration worked for many years but was not without its downsides. Many applications are highly sensitive to resolving DNS queries and any performance or availability issues we ran into would cause queuing and degraded performance at best and customer impacting outages at worst. Configuration and code changes can cause large unexpected changes in query rates. As such scaling beyond these two hosts became an issue. Due to the network configuration of these hosts we would just need to keep adding IPs and hosts which has its own problems. While attempting to fire fight and remediate these issues, the old system made it difficult to identify causes due to a lack of metrics and visibility. In many cases we resorted to  tcpdump  to identify traffic and queries in question. Another issue was running on public DNS servers we run the risk of leaking internal network information. As a result we decided to build something better and began to identify our requirements for the new system. . We set out to design a new DNS infrastructure that would improve the aforementioned operational issues including scaling and visibility, as well as introducing some additional requirements. We wanted to continue to run our public DNS zones via external DNS providers so whatever system we build needed to be vendor agnostic. Additionally, we wanted this system to be capable of serving both our internal and external zones, meaning internal zones were only available on our internal network unless specifically configured otherwise and external zones are resolvable without leaving our internal network. We wanted the new DNS architecture to allow both a  deploy-based workflow for making changes  as well as API access to our records for automated changes via our inventory and provisioning systems. The new system could not have any external dependencies; too much relies on DNS functioning for it to get caught in a cascading failure. This includes connectivity to other data centers and DNS services that may reside there. Our old system mixed the use of caches and authorities on the same host; we wanted to move to a tiered design with isolated roles. Lastly, we wanted a system that could support many data center environments whether it be EC2 or bare metal. .   . To build this system we identified three classes of hosts: caches, edges, and authorities. Caches serve as recursive resolvers and DNS “routers” caching responses from the edge tier. The edge tier, running a DNS authority daemon, responds to queries from the caching tier for zones it is configured to zone transfer from the authority tier. The authority tier serve as hidden DNS masters as our canonical source for DNS data, servicing zone transfers from the edge hosts as well as providing an HTTP API for creating, modifying or deleting records. . In our new configuration, caches live in each data center meaning application hosts don’t need to traverse a data center boundary to retrieve a record. The caches are configured to map zones to the edge hosts within their region in order to route our internal zones to our own hosts. Any zone that is not explicitly configured will recurse on the internet to resolve an answer. . The edge hosts are regional hosts, living in our network edge PoPs (Point of Presence). Our PoPs have one or more data centers that rely on them for external connectivity, without the PoP the data center can’t get to the internet and the internet can’t get to them. The edges perform zone transfers with all authorities regardless of what region or location they exist in and store those zones locally on their disk. . Our authorities are also regional hosts, only containing zones applicable to the region it is contained in. Our inventory and provisioning systems determine which regional authority a zone lives in and will create and delete records via an HTTP API as servers come and go. OctoDNS maps zones to regional authorities and uses the same API to create static records and to ensure dynamic sources are in sync. We have an additional separate authority for external domains, such as github.com, to allow us to query our external domains during a disruption to connectivity. All records are stored in MySQL. .   . One huge benefit of moving to a more modern DNS infrastructure is observability. Our old DNS system had little to no metrics and limited logging. A large factor in deciding which DNS servers to use was the breadth and depth of metrics they produce. We finalized on  Unbound  for the caches,  NSD  for the edge hosts and  PowerDNS  for the authorities, all of which have been proven in DNS infrastructures much larger than at GitHub. . When running in our bare metal data centers, caches are accessed via a private  anycast  IP resulting in it reaching the nearest available cache host. The caches have been deployed in a rack aware manner that provides some level of balanced load between them and isolation against some power and network failure modes. When a cache host fails, servers that would normally use it for lookups will now automatically be routed to the next closest cache, keeping latency low as well as providing tolerance to some failure modes. Anycast allows us to scale the number of caches behind a single IP address unlike our previous configuration, giving us the ability to run as many caching hosts as DNS demand requires. . Edge hosts perform zone transfers with the authority tier, regardless of region or location. Our zones are not large enough that keeping a copy of all of them in every region is a problem. This means for every zone, all caches will have access to a local edge server with a local copy of all zones even when a region is offline or upstream providers are having connectivity issues. This change alone has proven to be quite resilient in the face of connectivity issues and has helped keep GitHub available during failures that not long ago would have caused customer facing outages. . These zone transfers include both our internal and external zones from their respective authorities. As you might guess zones like github.com are external and zones like github.net are generally internal. The difference between them is only the types of use and data stored in them. Knowing which zones are internal and external gives us some flexibility in our configuration. . Public zones are  sync’d  to external DNS providers and are records GitHub users use everyday. Addtionally, public zones are completely resolvable within our network without needing to communicate with our external providers. This means any service that needs to look up  api.github.com  can do so without needing to rely on external network connectivity. We also use the stub-first configuration option of Unbound which gives a lookup a second chance if our internal DNS service is down for some reason by looking it up externally when it fails. . Most of the  github.net  zone is completely private, inaccessible from the internet and only contains  RFC 1918  IP addresses. Private zones are split up per region and site. Each region and/or site has a set of sub-zones applicable to that location, sub-zones for management network, service discovery, specific service records and yet to be provisioned hosts that are in our inventory. Private zones also include reverse lookup zones for PTRs. . Replacing an old system with a new one that is ready to serve millions of customers is never easy. Using a pragmatic, requirements based approach to designing and implementing our new DNS system resulted in a DNS infrastructure that was able to hit the ground running and will hopefully grow with GitHub into the future. . Want to help the GitHub SRE team solve interesting problems like this? We’d love for you to join us.  Apply Here  ", "date": "May 31, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tOrchestrator at GitHub\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-12-08-orchestrator-github/", "abstract": " GitHub uses MySQL to store its metadata: Issues, Pull Requests, comments, organizations, notifications and so forth. While  git  repository data does not need MySQL to exist and persist, GitHub’s service does. Authentication, API, and the website itself all require the availability of our MySQL fleet. . Our replication topologies span multiple data centers and this poses a challenge not only for availability but also for manageability and operations. . We use a classic MySQL master-replicas setup, where the master is the single writer, and replicas are mainly used for read traffic. We expect our MySQL fleet to be available for writes. Placing a review, creating a new repository, adding a collaborator, all require write access to our backend database. We require the master to be available. . To that effect we employ automated master failovers. The time it would take a human to wake up &amp; fix a failed master is beyond our expectancy of availability, and operating such a failover is sometimes non-trivial. We expect master failures to be automatically detected and recovered within 30 seconds or less, and we expect failover to result with minimal loss of available hosts. . We also expect to avoid false positives and false negatives. Failing over when there’s no failure is wasteful and should be avoided. Not failing over when failover should take place means an outage. Flapping is unacceptable. And so there must be a reliable detection mechanism that makes the right choice and takes a predictable course of action. . We employ  Orchestrator  to manage our MySQL failovers.  orchestrator  is an open source MySQL replication management and high availability solution. It observes MySQL replication topologies, auto-detects topology layout and changes, understands replication rules across configurations and versions, detects failure scenarios and recovers from master and intermediate master failures.    .  orchestrator  takes a different approach to failure detection than the common monitoring tools. The common way to detect master failure is by observing the master: via ping, via simple port scan, via simple  SELECT  query. These tests all suffer from the same problem:  What if there’s an error?  . Network glitches can happen; the monitoring tool itself may be network partitioned. The naive solutions are along the lines of “try several times at fixed intervals, and on the n-th successive failure, assume master is failed”. While repeated polling works, they tend to lead to false positives and to increased outages: the smaller  n  is (or the smaller the interval is), the more potential there is for a false positive: short network glitches will cause for unjustified failovers. However larger  n  values (or longer poll intervals) will delay a true failure case. . A better approach employs multiple observers, all of whom, or the majority of whom must agree that the master has failed. This reduces the danger of a single observer suffering from network partitioning. .  orchestrator  uses a holistic approach, utilizing the replication cluster itself. The master is not an isolated entity. It has replicas. These replicas continuously poll the master for incoming changes, copy those changes and replay them. They have their own retry count/interval setup. When  orchestrator  looks for a failure scenario, it looks at the master  and  at all of its replicas. It knows what replicas to expect because it continuously observes the topology, and has a clear picture of how it looked like the moment before failure. .  orchestrator  seeks agreement between itself and the replicas: if  orchestrator  cannot reach the master, but all replicas are happily replicating and making progress, there is no failure scenario. But if the master is unreachable to  orchestrator  and all replicas say: “Hey! Replication is broken, we cannot reach the master”, our conclusion becomes very powerful: we haven’t just gathered input from multiple hosts. We have identified that the replication cluster is broken  de-facto . The master may be alive, it may be dead, may be network partitioned; it does not matter: the cluster does not receive updates and for all practical purposes does not function. This situation is depicted in the image below:     Masters are not the only subject of failure detection:  orchestrator  employs similar logic to intermediate masters: replicas which happen to have further replicas of their own. . Furthermore,  orchestrator  also considers more complex cases as having unreachable replicas or other scenarios where decision making turns more fuzzy. In some such cases, it is still confident to proceed to failover. In others, it suffices with detection notification only. . We observe that  orchestrator ‘s detection algorithm is very accurate. We spent a few months in testing its decision making before switching on auto-recovery. . Once the decision to failover has been made, the next step is to choose where to failover to. That decision, too, is non trivial. . In semi-sync replication environments, which  orchestrator  supports, one or more designated replicas are guaranteed to be most up-to-date. This allows one to guarantee one or more servers that would be ideal to be promoted. Enabling semi-sync is on our roadmap and we use asynchronous replication at this time. Some updates made to the master may never make it to any replicas, and there is no guarantee as for which replica will get the most recent updates. Choosing the most up-to-date replica means you lose the least data. However in the world of operations not all replicas are created equal: at any given time we may be experimenting with a recent MySQL release, that we’re not ready yet to put to production; or may be transitioning from  STATEMENT  based replication to  ROW  based; or have servers in a remote data center that preferably wouldn’t take writes. Or you may have a designated server of stronger hardware that you’d like to promote no matter what. .  orchestrator  understands all replication rules and picks a replica that makes most sense to promote based on a set of rules and the set of available servers, their configuration, their physical location and more. Depending on servers’ configuration, it is able to do a two-step promotion by first healing the topology in whatever setup is easiest, then promoting a designated or otherwise best server as master. . We build trust in the failover procedure by continuously testing failovers. We intend to write more on this in a later post. . Flapping is strictly unacceptable. To that effect  orchestrator  is configured to only perform one automated failover for any given cluster in a preconfigured time period. Once a failover takes place, the failed cluster is marked as “blocked” from further failovers. This mark is cleared after, say,  30  minutes, or until a human says otherwise. . To clarify, an automated master failover in the middle of the night does not mean stakeholders get to sleep it over. Pages will arrive, even as failover takes place. A human will observe the state, and may or may not acknowledge the failover as justified. Once acknowledged,  orchestrator  forgets about that failover and is free to proceed with further failovers on that cluster should the case arise. . There’s more than failovers to  orchestrator . It allows for simplified topology management and visualization. . We have multiple clusters of differing size, that span multiple datacenters (DCs). Consider the following:     The different colors indicate different data centers, and the above topology spans three DCs. Cross-DC network has higher latency and network calls are more expensive than within the intra-DC network, and so we typically group DC servers under a designated  intermediate master , aka  local DC master , and reduce cross-DC network traffic. In the above  instance-64bb  (blue, 2nd from bottom on the right) could replicate from  instance-6b44  (blue, bottom, middle) and free up some cross-DC traffic. . This design leads to more complex topologies: replication trees that go deeper than one or two levels. There are more use cases to having such topologies: . Deep nested replication topologies introduce management complexity: .  orchestrator  allows for easy and safe refactoring and management of such complex topologies: .  orchestrator  also serves as the de-facto topology state/inventory indicator. It complements  puppet  or service discoveries configuration which imply  desired  state, by actually observing the  existing  state. State is queryable at various levels, and we employ  orchestrator  at some of our automation tasks. . We love our chatops as they make our operations visible and accessible to our greater group of engineers.  While the orchestrator service provides a web interface, we rarely use it; one’s browser is her own private command center, with no visibility to others and no history. . We rely on chatops for most operations. As a quick example of visibility we get by chatops, let’s examine a cluster: . Say we wanted to upgrade  instance-fadf  to  5.6.31-77.0-log . It has two replicas attached, that I don’t want to be affected. We can: . To the effect of: . The instance is now free to be taken  out of the pool . . Other actions are available to us via chatops. We can force a failover, acknowledge recoveries, query topology structure etc.  orchestrator  further communicates with us on chat, and notifies us in the event of a failure/recovery. .  orchestrator  also runs as a command-line tool, and the  orchestrator  service supports web API, and so can easily participate in automated tasks. . GitHub has adopted  orchestrator , and will continue to improve and maintain it. The  github repo  will serve as the new upstream and will accept issues and pull requests from the community. .  orchestrator  continues to be free and open source, and is released under the  Apache License 2.0 . . Migrating the project to the  GitHub repo  had the unfortunate result of diverging from the original  Outbrain repo , due to the way import paths are coupled with repo URI in  golang . The two diverged repositories will not be kept in sync; and we took the opportunity to make some further diverging changes, though made sure to keep API &amp; command line spec compatible. We’ll keep an eye for incoming Issues on the Outbrain repo. . It is our pleasure to acknowledge  Outbrain  as the original author of  orchestrator . The project originated at Outbrain while seeking to manage a growing fleet of servers in three data centers. It began as a means to visualize the existing topologies, with minimal support for refactoring, and came at a time where massive hardware upgrades and datacenter changes were taking place.  orchestrator  was used as the tool for refactoring and for ensuring topology setups went as planned and without interruption to service, even as servers were being provisioned or retired. . Later on Pseudo-GTID was introduced to overcome the problems of unreachable/crashing/lagging intermediate masters, and shortly afterwards recoveries came into being.  orchestrator  was put to production in very early stages and worked on busy and sensitive systems. . Outbrain was happy to develop  orchestrator  as a public open source project and provided the resources to allow its development, not only to the specific benefits of the company, but also to the wider community. Outbrain authors many more open source projects, which can be found on their GitHub’s  Outbrain engineering  page. . We’d like to thank Outbrain for their contributions to  orchestrator , as well as for their openness to having us adopt the project. .  orchestrator  was later developed at  Booking.com . It was brought in to improve on the existing high availability scheme.  orchestrator ‘s flexibility allowed for simpler hardware setup and faster failovers. It was fortunate to enjoy the large MySQL setup Booking.com employs, managing various MySQL vendors, versions, configurations, running on clusters ranging from a single master to many hundreds of MySQL servers and Binlog Servers on multiple data centers. Booking.com continuously contributes to  orchestrator . . We’d like to further acknowledge major community contributions made by Google/ Vitess  ( orchestrator  is the  failover mechanism  used by Vitess), and by  Square, Inc . . We’ve released a public  Puppet module for orchestrator , authored by @tomkrouper. This module sets up the  orchestrator  service, config files, logging etc. We use this module within our own  puppet  setup, and actively maintain it. . Chef users, please consider this  Chef cookbook  by  @silviabotros . ", "date": "December 8, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tBug Bounty anniversary promotion: bigger bounties in January and February\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-01-09-bug-bounty-anniversary-promotion-bigger-bounties-in-january-and-february/", "abstract": "   . The GitHub  Bug Bounty Program  is turning three years old. To celebrate, we’re offering bigger bounties for the most severe bugs found in January and February. . The process is the same as always: hackers and security researchers find and  report vulnerabilities through our responsible disclosure process . To recognize the effort these researchers put forth, we reward them with actual money. Standard bounties range between $500 and $10,000 USD and are determined at our discretion, based on overall severity. In January and February we’re throwing in bonus rewards for standout individual reports in addition to the usual payouts. .   . In addition to cash prizes, we’ve also made limited edition t-shirts to thank you for helping us hunt down GitHub bugs. We don’t have enough for everyone—just for the 15 submitters with the most severe bugs. . GitHub Enterprise is now included in the bounty program. So go ahead and find some Enterprise bugs. If they’re big enough you’ll be eligible for the promotional bounty. Otherwise, rewards are the same as GitHub.com ($200 to $10,000 USD). For more details,  visit our bounty site . . Giving winners some extra cash doesn’t mean anyone has to lose. If you find a bug, you’ll still receive the standard bounties. . Happy hunting! ", "date": "January 9, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit LFS 2.2.0 released\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-06-27-git-lfs-2-2-0-released/", "abstract": "  Git LFS  v2.2.0 is now available with the all-new  git-lfs-migrate  command, making it easier than ever to start using Git LFS in your repository. . For example, if you’ve tried to push a large file to GitHub without LFS, you might have seen the following error: . You can use the  git lfs migrate info  command to see which files are causing the push failure: . Using the information above, you can determine which files to pluck out of your history and store in LFS: . You can also configure the ‘import’ command to migrate specific filetypes, branches, and more. For a detailed overview, take a look at the  man page . . This was a quick look at the  migrate  command available today in Git LFS v2.2.0. For more on the full release, check out the  release notes . .  \t\t Tags:   \t\t Git \t ", "date": "June 27, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub’s post-CSP journey\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-01-19-githubs-post-csp-journey/", "abstract": " Last year we shared some details on  GitHub’s CSP journey . A journey was a good way to describe it, as our usage of  Content Security Policy  (CSP) significantly changed from our initial release nearly  four years ago  to where we ended up last year. It wasn’t until then that we felt our policy was relatively stable and, while we were not foolish enough to call it “done,” we found the policy was refined enough to focus on protections beyond what CSP offered. . With a solid foundation provided by CSP, we opened up an internal issue titled “Defending against post-CSP exploitation” to continue the journey. The goal of this issue was twofold. First, to research bypasses of our existing CSP policy and secondly, to brainstorm additional mitigations for attacks not prevented by CSP. We quickly identified a few ideas for additional defense-in-depth protections, but we were curious what gaps we hadn’t considered. To help us investigate this, we enlisted the expertise of  Cure53 . In a relatively unique project, we asked Cure53 to assess what an attacker could do, assuming a content injection bug in GitHub.com. As expected and hoped for, Cure53 identified a handful of interesting bypasses that helped evolve our mitigations. In this post, we wanted to share some of the mitigations that resulted from these efforts. . Locking down  img-src  isn’t at the top of the list when most people start working on CSP. But, as noted in last year’s writeup, once you have used CSP to address the low-hanging fruit related to actual JavaScript execution, you start to focus on how other tags can be used to exfiltrate sensitive information to a third-party. Images are a prime candidate, since many CSP policies are relatively permissive with what sources can be used. Using the same example as last year, consider the following injected content: . A tag with an unclosed quote will capture all output up to the next matching quote. This could include security sensitive content on the pages such as: . The resulting image element will send a request to  https://some_evilsite.com/log_csrf?html=...some_csrf_token_value... . As a result, an attacker can leverage this dangling markup attack to exfiltrate CSRF tokens to a site of their choosing. We were aware of this vector, and had purposefully narrowed our  img-src  list quite a bit. Our policy had evolved from an  img-src *  to a more prescriptive list: . We felt pretty good about this list, though we always had some misgivings about including third-party sites given our lack of control over their content. Specifically, we wanted to remove  www.google-analytics.com ,  *.gravatar.com , and  *.wp.com . The first was used for Google Analytics (surprising I know) and the latter two were needed to support  Gravatars  for default user profile images. During the assessment, Cure53 identified ways to use both Google Analytics and Gravatar to exfiltrate sensitive information from a page given a content injection vulnerability. As a result, we decided to see how difficult it would be to rid ourselves of all third-party image sources. Let’s first take a look at Google Analytics. Imagine a content injection that looks something like: . Cure53 noted that the Google Analytics’  ea  parameter is used to log event actions and can contain arbitrary strings. An attacker could setup a Google Analytics account and then inject an image referencing their account: . This results in the following request, logging “secret” to their account: . The Gravatar bypass was even more nuanced, and made use of a feature we were unaware of in the service. Gravatar lets users associate a “globally recognized avatar” image with an email address (the MD5 of the email address specifically). A site that supports Gravatar can retrieve the avatar image for a user by making a request to a URL that contains the MD5 of a user’s email address. For example, GitHub.com might embed a Gravatar image similar to: . Cure53 noted that Gravatar supports a “fallback image” feature if you pass an additional  d  parameter. This fallback image is used in the case where Gravatar has no avatar associated with the MD5 passed in the URL. So, an attacker could inject: . The dangling markup causes “secret” to be captured and included as part of the value associated with the  d  parameter sent to Gravatar, resulting in the following request: . The passed in MD5 is invalid, so Gravatar will fallback to whatever was passed in the  d  parameter. This is implemented as a redirect to a proxy hosted by Gravatar. The redirect URL is: . While somewhat mangled, “secret” ends up getting leaked to  https://some-evil-site.comhttps://githubengineering.com/images/avatar.jpg/psecret/p  through the proxy on  https://i1.wp.com . . In both the Google Analytics and Gravatar cases, potentially sensitive HTML content could be exfiltrated to an attacker. In response, we decided to see how difficult it would be to remove these hosts from our  img-src  list. Google Analytics was particularly easy. We moved from using their image-based system to their  XHR approach . This largely involved switching a few pieces of configuration data from  image  to  xhr  in our Google Analytics code. We were already sending some XHR requests to Google Analytics, so their host was already on our  connect-src  list. This simple change allowed us to remove Google Analytics from our  img-src  all together. . Gravatar was slightly more complex, as we still needed to effectively place Gravatar URLs inside of  img  tags. However, we observed that every time we generated such a URL, it was known to us server-side before we rendered the page. So, instead of using raw Gravatar URLs inside of image tags, we decided to proxy the Gravatar images through our own existing image proxy (called Camo). We already use Camo when a user references an image to a third-party site anywhere we accept Markdown (ex. pull request and issue comments), so using it for Gravatar was a pretty natural extension. Unlike the Gravatar image proxy described above, our image proxy is more strict and will only fetch URLs that the server explicitly authenticates. For example, if we want to proxy an image for the URL  https://www.gravatar.com/avatar/c491ecde69687affd1256d4cb19cab00 , our image proxy library code will generate a URL like the following: . The above URL has two path components: . The second path parameter is simply the original URL hex encoded: . The first path component is a HMAC of the original URL. The proxy will only fetch the image if the HMAC is valid for the URL. In the case of a content injection attack, it would be impossible to inject a proxied Camo URL unless you know the entire URL before it was injected. Looking at all of the examples above, image injections leverage unclosed attributes to capture unknown surrounding markup that includes various secrets. So, by definition, the attacker doesn’t know the full URL and an attacker would be unable to formulate a Camo proxy image URL with a valid HMAC. We now proxy all Gravatar URLs and were able to completely remove all Gravatar related sources from our  img-src  list. . A consistent goal in mitigating dangling markup attacks is to protect secrets contained within the page. CSRF tokens are a natural starting point when analyzing these attacks. In parallel with adding some of the other mitigations discussed later in this article, we also considered the problem purely from the perspective of blocking dangling markup in the first place. Our CSRF tokens are primarily written to a form input tag like the following: . In the vast majority of cases, we know that our CSRF token is a child node of a form tag. Our form tags are generally rendered using Rails’ form helper methods and, given our consistent use of these helpers, we investigated what we could add before the generated forms to attempt to close dangling markup. In theory, closing dangling markup before the form tag would prevent the CSRF token from being exfiltrated. We weren’t foolish enough to think we could come up with a 100% effective solution, as there are many obscure parsing behaviors implemented by browsers. However, as a starting point and to further our own understanding of the edge cases with dangling markup, we decided to experiment. Our first iteration on this protection was the following: . This attempts to close tags and quotes that we know have been used in dangling markup attacks. During our collaboration with Cure53, they noted a few gaps in our first attempt. Imagine an injection like: . By combining a  &lt;textarea&gt;  tag and an unclosed attribute, our dangling markup protection is bypassed. The unclosed quote will consume the  &lt;/textarea&gt; . Because forms can’t be nested (as mandated by the  content model  for forms), the injected top level form’s  action  will take precedence. Upon clicking the injected button, the following URL is requested: . Two issues were surfaced here. First, we had the order backwards on our prefixed markup. Since attributes occur within tags, we should have closed dangling attributes first and then closed tags. Second, nested forms are also problematic and are something we should try to prevent. This resulted in an updated dangling markup mitigation prefix: . Notice that we are forced to place the closing  &lt;/form&gt;  outside of a comment.  &lt;textarea&gt;  contents are CDATA and as a result, the parser ignores all HTML markup within the tags. The parser simply searches for a matching  &lt;/textarea&gt;  regardless of where it is found (even if inside of a HTML comment). However, form contents are not CDATA and the matching closing tag must be found in a non-comment. . With our updated prefix, the original bypass was fixed. Let’s take a look at how: . With the updated order, the dangling  name  attribute is now closed. With that attribute closed, the  &lt;/textarea&gt;  now correctly closes the injected  &lt;textarea&gt;  tag. Finally, with the newly added  &lt;/form&gt; , the injected form is also closed. The end result is an injected form, but one that doesn’t capture or override any part of the form, or secrets, that follow it. . Unfortunately, our conquering of dangling markup attacks was short lived. Cure53 had a few other findings from their analysis to be solved. In our prefix, we didn’t account for all the tags that could be used to capture content. Cure53 called out the following additional tags: . The first two are simple enough.  &lt;xmp&gt;  (don’t feel bad… I hadn’t  heard of it  either) is similar to  &lt;textarea&gt;  in that the contents of the node are treated like CDATA. So, we simply added  &lt;/xmp&gt;  to our CDATA comment.  &lt;option&gt;  is similar to  &lt;form&gt; , and we could just add a closing  &lt;/option&gt;  tag. With these changes, the resulting dangling markup mitigation string looks like this: . That just leaves the  &lt;plaintext&gt;  tag. Sadly, with this seemingly innocuous tag, we finally hit a brick wall. The parsing rules for  &lt;plaintext&gt;  are “unique” to say the least. Quoting a description of the tag from  Mozilla : . The HTML Plaintext Element ( &lt;plaintext&gt; ) renders everything following the start tag as raw text, without interpreting any HTML. There is no closing tag, since everything after it is considered raw text. . Unlike  &lt;textarea&gt;  and  &lt;option&gt; , there is, by definition, no way to close the tag. It turns out, different browsers act differently with  &lt;plaintext&gt;  tags, particularly with respect to how they are treated within forms. Chrome allows a  &lt;plaintext&gt;  tag as a child of an  &lt;option&gt;  tag. For example, given the following injection: .  &lt;plaintext&gt;  can’t be closed. All of our dangling markup is consumed and clicking the injected button results in the following request: . Sadly, there is nothing we can do; the parsing rules in Chrome make it impossible to prevent. I had found the following on  Mozilla’s site  regarding the  &lt;plaintext&gt;  tag: . This feature is obsolete. Although it may still work in some browsers, its use is discouraged since it could be removed at any time. Try to avoid using it. . Given this statement, I reached out to  @mikewest , and asked if a deprecation or removal of the  &lt;plaintext&gt;  tag from Chrome would be an option. While outright removal proved impractical, it turned out Chrome is the only browser that is exploitable. So,  the current plan  is to update Chrome to follow the behavior of other browsers and ignore the  &lt;plaintext&gt;  tag inside of a  &lt;select&gt;  tag. That is not to say this fix will be sufficient, as there may be other edge cases that need to be considered. But, it is an incremental step towards more secure behavior and encouraging to see effort put toward making  &lt;plaintext&gt;  more sensible. . In the end, our dangling markup mitigation has a known bypass in the most popular browser used by GitHub users. We still feel the effort was worthwhile, as it has sparked discussion around the attack surface of dangling markup. Dangling markup is a non-trivial problem to solve and without efforts such as these, the problem sits stagnant. We are thrilled to have stirred up discussion with folks in a position to help mitigate the issue at the browser level. . While the content exfiltration attacks described above can potentially be used to disclose arbitrary secrets, the most common secrets to go after are CSRF tokens. CSRF tokens occur on nearly every page and the exfiltration of one can be easily leveraged to escalate privileges. Our existing CSP is already robust against exfiltration of CSRF tokens for the following reasons: . However, given our  form-action  list includes  self  and  gist.github.com , we were concerned that there could be scenario where a CSRF token could be disclosed by triggering a form submission to one of these allowed hosts. For example, imagine a scenario where an attacker injects a form that leaks a CSRF token by causing dangling markup contents to be stored in a new Gist that is accessible to an attacker. To mitigate this, we thought about the idea of per-form CSRF tokens, such that the utility of any single CSRF token is generally negligible. . GitHub.com is heavily based on Ruby on Rails and uses ERB templates to generate much of the HTML we render. One nice thing about using a framework consistently is that we have a chokepoint where CSRF tokens are generated. For example, when you use the  form_for  Rails helper, Rails will automatically generate a hidden CSRF token input for your form. Traditionally, this CSRF token is unique to the user’s session, but is consistent across all forms within an application. In other words, if you successfully exfiltrate a CSRF token associated with one form, it will work across any CSRF protected endpoint in the application. But, what if that weren’t the case? What if CSRF tokens were only valid for the form it was generated for? Part of the information Rails uses when generating a form using  form_for  is the path for the form’s  action  as well as the form’s  method . So, what if we bound the form’s CSRF token to that form’s  action / method  tuple? This is exactly what my teammate  @mastahyeti  implemented and contributed to Rails in  this pull request . Once that was merged, we backported the implementation to work with the version of Rails currently deployed on GitHub.com. . There were definitely hurdles when backporting per-form CSRF tokens into GitHub.com, as not every CSRF protected request was implemented using  form_for . For example, we had a number of client-side XHR requests made using JavaScript. Surprisingly, many of these were compatible, as the developers were extracting a CSRF token for the XHR by storing the CSRF token generated by  form_for  in a data attribute or a hidden  input  tag. However, there were quite a few callsites that were not compatible. The most common pattern we saw for these looked something like this: . In other words, the JavaScript would scan across the DOM looking for any CSRF token to use. These callsites had to be fixed up to use a CSRF token specific to their use. While not trivial, there weren’t as many of these cases as we feared, and were able to fix up all incompatible callsites with about a week of effort. . We fully shipped per-form CSRF tokens just before the holidays and are pleased with the safety it provides. Exfiltration of a CSRF token associated with staring a repository can no longer be used to do something more sensitive, such as disabling two-factor authentication. Moreover, since the CSRF tokens are associated with a given form’s  action  (i.e. the full path to where the form will  POST ), the protection is extremely granular. For example, the path for staring Rails  /rails/rails/star  is distinct from staring Django  /django/django/star . As a result, a CSRF token for staring Rails can’t even be used to star Django. .  Same-site cookies  is a relatively recent specification spearheaded by  @mikewest  and implemented in recent Chrome releases. The general idea is to add an additional cookie attribute that lets you control when a cookie is sent in a cross-origin request. Traditionally, a cookie set on a given domain will be sent along with any request to that domain (XHR requests requiring a CORS preflight notwithstanding). This is just “how the web works,” since cookies are a form of “ambient authority.” In many cases, this ambient authority is not desirable or intended when requests are made between origins, and this is the crux of CSRF attacks in applications. For example, imagine the following form is embedded on  https://some-evil-site.com : . By default, the ambient authority of cookies would let  https://some-evil-site.com  submit that form when a GitHub user is visiting their site. Without protection against CSRF, this would disable the user’s two factor authentication. This is possible because the user’s session cookies will be sent along with the form submission. CSRF tokens are the obvious and go-to solution for mitigating the issue. But, CSRF tokens must be implemented (correctly) by the application. Same-site cookies are a browser-focused mechanism for helping to mitigate CSRF attacks, even in the scenario where CSRF tokens themselves may not be implemented correctly. . In short, a cookie can be made same-site by setting it’s  SameSite  attribute to either the  Lax  or  Strict . For example: . The above sets a  Strict  same-site cookie that will be transmitted along with a request only when the request originates from the site itself. The previous cross-origin form submission would not send a  Strict  cookie. However, simply switching our primary  user_session  cookie to be  Strict  would have broken most users’ expected experience. . A  Strict  cookie is strict in every sense of the word. For example, a  Strict  cookie won’t be transmitted even during top-level navigation. If a user is visiting  https://stackoverflow.com  and clicked a link to  https://github.com/ , a  Strict  cookie wouldn’t be transmitted, and the request would look like one from a logged out user. That leads to a pretty jarring user experience. This is exactly what the  SameSite=Lax  cookie attribute value was created for. It has similar restrictions as a  Strict  cookie, but allows a cookie to be sent during “safe” top-level navigations (i.e. link clicks but not cross-origin POST requests). Given the trade-offs between  Lax  and  Strict , we decided to approach the problem by using a combination of cookies. . To strike this balance, we deployed same-site cookies by creating a  Strict  copy of our session cookie and now effectively have two session cookies. The primary session cookie,  user_session , is the same as before, without the  SameSite  attribute. This allowed us to minimize changes to a few endpoints that expect cross-origin  POST  requests. The second cookie, beautifully named  __Host-user_session_same_site  (the cookie name leverages the  cookie prefixes specification ), is set with  SameSite=Strict . We then added validation logic to check this new cookie: . This extra validation gets checked inline with our existing CSRF token. One caveat in coupling these checks is that we are restricting the same-site check to endpoints that already perform CSRF protection. Any endpoint mistakenly missing CSRF protection, such as an action incorrectly implemented as a  GET  based endpoint, would also bypass the new check. However, this helps protect against leaked CSRF tokens or regressions in our token-based checks. The additional validation provided by same-site cookies is a nice belt and suspender protection that required minimal changes in our code. . The four mitigations discussed are the largest of the efforts completed as a result of our “Defending against post-CSP exploitation” brainstorming. However, they aren’t the only things we worked on this journey. There were other ideas investigated and edge cases researched. Here is a quick hit list of some of our other changes: . To further combat injected forms, we experimented with adding a nonce in a data attribute for all  &lt;form&gt;  tags. Upon page load, we would execute JavaScript to enumerate all forms on the page and remove any form that didn’t contain the nonce. We ended up reverting this experiment since we found edge cases related to HTML caching. Luckily, our previously discussed mitigations provide a better overall solution. . GitHub uses  Facebox  to render many of our modal dialogs. In most cases, the HTML for the dialog is statically defined and referenced by a hidden  &lt;div&gt;  in the DOM. However, by default, Facebox supports fetching HTML partials from arbitrary hosts based on a data attribute. This could be leveraged in content injection attacks that control or inject attributes to render HTML from an arbitrary Amazon S3 bucket on GitHub.com. There were a handful of cases where we relied on this XHR functionality. So, we refactored these cases to reference a static hidden  &lt;div&gt;  and removed the XHR functionality from Facebox. . Part of what made the Facebox XHR functionality easy to exploit was the way we used S3. Early on, we had added  s3.amazonaws.com  to our  connect-src  since we had uses that were making requests to  https://s3.amazonaws.com/github-cloud . However, this effectively opened up our  connect-src  to any Amazon S3 bucket. We refactored our URL generation and switched all call sites and our  connect-src  to use  https://github-cloud.s3.amazonaws.com  to reference our bucket. . As always, there is more to be done. While we are relatively satisfied with the mitigations put in place, we are excited to continue pushing our protections to the next level. We are particularly looking forward to exploring  suborigins  once they ship in Chrome. There are some obvious basic uses, such as user-content sandboxing, but we would also like to investigate how to fully integrate and utilize the restrictions it provides. . In addition, there has been a recent up-tick in discussion around bypasses for nonce-based CSP sources. While GitHub does not make use of nonce-based sources, the ongoing research has many parallels with protecting against dangling markup attacks. Nonce-based sources have gained popularity with the recent introduction of   strict-dynamic  . In short,  strict-dynamic  makes secure deployment of CSP easier in many environments. However, because  strict-dynamic  relies on using nonce-based sources, leaking of CSP nonces becomes a valid avenue of attack. This research began in earnest only recently, and you can follow along by watching  this page  as new bypasses are discovered. This research has already lead to a recent Chrome “intent to implement” from  @mikewest  to  protect against dangling markup attacks . We will continue to follow along, and contribute to the conversation, in hopes that dangling markup attacks can be mitigated (or largely mitigated) with some sensible browser changes. . Finally, to encourage additional research into bypasses in our existing CSP policy, we are opening up a new bug bounty for GitHub.com. Even if you don’t identify a content injection bug on GitHub.com, we are still interested in novel approaches that can be used to bypass our CSP policy to leak sensitive content. Bounty payouts range between $500 and $5000, and will be based on factors such as novelty and practicality. The full details can be found on  our bug bounty site . Good luck! ", "date": "January 19, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tMoving persistent data out of Redis\t\t", "author": ["\n\t\tBryana Knight\t"], "link": "https://github.blog/2017-01-10-moving-persistent-data-out-of-redis/", "abstract": " Historically, we have used Redis in two ways at GitHub: . We used it as an  LRU cache  to conveniently store the results of expensive computations over data originally persisted in Git repositories or MySQL. We call this  transient Redis . . We also enabled  persistence , which gave us durability guarantees over data that was not stored anywhere else. We used it to store a wide range of values: from sparse data with high read/write ratios, like configuration settings, counters, or quality metrics, to very dynamic information powering core features like spam analysis. We call this  persistent Redis . . Recently we made the decision to disable persistence in Redis and stop using it as a source of truth for our data. The main motivations behind this choice were to: . Transitioning all that information transparently involved planning and coordination. For each problem domain using persistent Redis, we considered the volume of operations, the structure of the data, and the different access patterns to predict the impact on our current MySQL capacity, and the need for provisioning new hardware. . For the majority of callsites, we replaced persistent Redis with  GitHub::KV , a MySQL key/value store of our own built atop InnoDB, with features like key expiration. We were able to use  GitHub::KV  almost identically as we used Redis: from trending repositories and users for the explore page, to rate limiting to spammy user detection. . We have lots of “events” at GitHub. Starring a repository, closing an issue and pushing commits are all events that we display on our activity feeds, like the one found on  your GitHub homepage . . We used Redis as a secondary indexer for the MySQL table that stores all our events. Previously, when an event happened, we “dispatched” the event identifier to Redis keys corresponding to each user’s feed that should display the event. That’s a lot of write operations and a lot of Redis keys and no single table would be able to handle that fanout. We weren’t going to be able to simply replace Redis with  GitHub::KV  everywhere in this code path and call it a day. . Our first step was to gather some metrics and let them tell us what to do. We pulled numbers for the different types of feeds we had and calculated the writes and reads per second for each timeline type (e.g.,  issue events in a repository ,  public events performed by a user , etc.). One timeline wasn’t ever read, so we were able to axe it right away and immediately knock one off the list. Of the remaining timelines, two were so write-heavy that we knew we couldn’t port them to MySQL as is. So that’s where we began. . Let’s walk through how we handled one of the two problematic timelines. The “organization timeline” that you can see if you toggle the event feed on your home page to one of the organizations you belong to, accounted for 67% of the more than 350 million total writes per day to Redis for these timelines. Remember when I said we “dispatched” event IDs to Redis for every user that should see them? Long story short – we were pushing event IDs to separate Redis keys for every event and every user within an org. So for an active organization that produces, say, 100 events per day and has 1000 members, that would potentially be 100,000 writes to Redis for only 100 events. Not good, not efficient, and would require far more MySQL capacity than what we are willing to accept. . We changed up how writing to and reading from Redis keys worked for this timeline before even thinking about MySQL. We’d write every event happening to one key for the org, and then on retrieval, we’d reject those events that the requesting user shouldn’t see. Instead of doing the filtering each time the event is fanned out, we’d do it on reads. . This resulted in a dramatic 65% reduction of the write operations in for this feature, getting us closer to the point were we could move the activity feeds to MySQL entirely. .   . Although the single goal in mind was to stop using Redis as a persistent datastore, we thought that, given this was a legacy piece of code that evolved organically over the years, there would be some room for improving its efficiency as well. Reads were fast because the data was properly indexed and compact. Knowing that, we decided to stop writing separately to certain timelines that we could compose from the events contained in others, and therefore reduce the remaining writes another 30% (~11% overall). We got to a point that we were writing less than 1500 keys per second 98% of the time, with spikes below 2100 keys written per second. This was a volume of operations we thought we could handle with our current MySQL infrastructure without adding any new servers. . While we prepared to migrate the activity feeds to MySQL, we experimented with different schema designs, tried out one-record-per-event normalization and fixed-size feed subsets per record, and we even experimented with  MySQL 5.7 JSON data type  for modeling the list of event IDs. However we finally went with a schema similar to that of  GitHub::KV , just without some of the features we didn’t need, like the record’s last updated at and expiration timestamps. . On top of that schema, and inspired by  Redis pipelining , we created a small library for batching and throttling writes of the same event that were dispatched to different feeds. . With all that in place, we began migrating each type of feed we had, starting with the least “risky”. We measured risk of migrating any given type based on its number of write operations, as reads were not really the bottleneck. . After we migrated each feed type, we checked cluster capacity, contention and replication delay. We had feature flags in place that enabled writes to MySQL, while still writing to persistent Redis, so that we wouldn’t disrupt user experience if we had to roll back. Once we were sure writes were performing well, and that all the events in Redis were copied to MySQL, we flipped another feature flag to read from the new data store, again measured capacity, and then proceeded with the next activity feed type. . When we were sure everything was migrated and performing properly we deployed a new pull request removing all callsites to persistent Redis. These are the resulting performance figures as of today: .      . We can see how at the store level, writes ( mset ) are below 270wps at peak, with reads ( mget ) below 460ps. These values are way lower than the number of events being written thanks to the way events are batched before writes. .   . Replication delay is below 180 milliseconds at peak. The blue line, correlated with the number of write operations, shows how delay is checked before any batch is written to prevent replicas from getting out of sync. . At the end of the day we just grew out of Redis as a persistent datastore for some of our use cases. We needed something that would work for both github.com and GitHub Enterprise, so we decided to lean on our operational experience with MySQL. However, clearly MySQL isn’t a one-size-fits-all solution and we had to rely on data and metrics to guide us in our usage of it for our event feeds at GitHub. Our first priority was moving off of persistent Redis, and our data-driven approach enabled us to optimize and improve performance along the way. . Thank you to everybody on the Platform and Infrastucture teams who contributed to this project. If you would like to work on problems that help scale GitHub out, we are looking for an engineer to join us. The Platform team is responsible for building a resilient, highly available platform for internal engineers and external integrators to add value to our users. . We would love you to join us.  Apply here!  .  Bryana Knight   Platform Engineer   GitHub Profile |   Twitter Profile  .     .  Miguel Fernández   Senior Platform Engineer   GitHub Profile |   Twitter Profile  ", "date": "January 10, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tNew and improved two-factor lockout recovery process\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2017-01-30-recover-accounts-elsewhere/", "abstract": " The Recover Accounts Elsewhere feature lets you associate your GitHub account with your Facebook account. This will help us recover your account for certain two-factor authentication lockout scenarios. For example, you may become locked out of your GitHub account because you have lost your phone or U2F key, changed phones without re-enrolling, or have otherwise lost the ability to use your phone or token without a usable backup. . Currently, losing the ability to authenticate with your phone or token requires that you prove account ownership before we can disable two-factor authentication. Proving ownership requires access to a confirmed email address and a valid SSH private key for a given account. This new recovery feature will provide an alternative proof of account ownership that can be used along with these other methods. . Starting on Tuesday January 31st 2017, you can start using this feature but the tl;dr is: . At this point, an audit log entry is added signifying a successful recovery. GitHub Support can then use this information as part of a risk-based analysis to decide if proof of account ownership has been established in order to disable two-factor authentication. . The underlying “Delegated Account Recovery”  specification  was written by  Brad Hill . GitHub will act as the first account provider while Facebook will be the first recovery provider. The general recovery flow between Facebook and GitHub is as follows: . GitHub only stores the token ID, user ID, and token state. Facebook only stores a token with an encrypted secret that is associated with a Facebook account and does not become valid until it’s used in a recovery. This process helps limit the impact of database dumps and SQL injection vulnerabilities without an additional compromise of the encryption and signing keys. . At no point does GitHub exchange any personally identifiable information with Facebook. Likewise, Facebook does not exchange any personally identifiable data with us. . The astute reader will note that the above flow could likely be implemented using an existing standard such as OAuth. OAuth is “the devil we know” and with just a few small tweaks and additional OAuth scopes, a similar process could be implemented. . So why not OAuth? OAuth’s flexibility can be unnecessary and dangerous. OAuth scopes with bearer tokens immediately grant access to functionality. With the new spec, there are no scopes. No different kinds of tokens. No ambiguities. Additionally, tokens are not immediately valid on their own. We aim to do one thing: help people recover accounts in a safer way. . GitHub will be open sourcing a Ruby library soon so that the complicated and dangerous code paths will be abstracted away and we will provide a Ruby reference implementation as a guide. Follow us on Twitter @GitHubSecurity to keep informed as to when we announce it! . While it is true this spec was developed in private with limited exposure, it was done by someone well versed in this area and has been reviewed by numerous experts in the field. To promote public review, GitHub and Facebook are including the specification as part of a joint bug bounty program. Naturally, our implementation of the specification is automatically in scope. Additionally, we will all pay out for bugs in the specification including features we haven’t implemented yet. You can report these bugs directly to the  Facebook Bug Bounty . . We are taking a very careful approach to rolling this out. GitHub values the security of our users’ accounts and we believe using this for two-factor authentication recovery is the best first step. In the future, it may replace or complement password resets. Potentially, you may be able to store encrypted secrets with Facebook such as TOTP seeds or two-factor authentication recovery codes. We’re also planning to support reciprocal Facebook account recovery in the near future. At that point, you’ll be able to use your GitHub account to recover your Facebook account, too. . We think this is a great step towards a better account recovery process. Traditional email-based recovery has flaws, transmitting secrets that grant immediate account access through many hops and clients. With this feature we can ease the pain associated with a locked out account in a way that is well defined and protects the security and privacy of our users. ", "date": "January 30, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tAdding Community & Safety checks to new features\t\t", "author": ["\n\t\tDanielle Leong\t"], "link": "https://github.blog/2017-01-31-community-and-safety-feature-reviews/", "abstract": " With the continuous shipping nature at GitHub, it’s easy for the most well-intentioned feature to accidentally become the vector of abuse and harassment. The Community &amp; Safety engineering team focuses on building  community management  tools and maintaining  user safety , but we also review new features our colleagues have written to ensure there are no accidental abuse vectors. Similar to  Application Security  reviews, these Community &amp; Safety reviews hopefully catch any potential problems  before  they go out, in order to minimize impact on marginalized folks, reduce spam, and encourage healthy communities. . But manually reviewing every pull request doesn’t scale, so we’ve created a handy checklist for folks who haven’t had the privilege of being harassed on the internet for things to look out for. . Our approach focuses on three main areas: ensuring explicit consent, keeping an audit log trail, and minimizing abuse. . On the Community &amp; Safety team, we believe in  explicit consent  in our daily lives as well as when we build software. Many abuse vectors can be avoided by simply asking:  Are all parties involved aware and consenting to this interaction?  If everyone is aware and on board with what’s going on, we reduce the number of unpleasant surprises, lower support ticket volume, and increase user trust. A great example of explicit consent is the  Repository Invitations  feature by @CoralineAda. . Any time you have two or more users interacting, there’s potential for harassment and abuse. Let’s say that Alice has been contacted by Bob using your new feature (i.e. direct messages). . Some example questions we ask to ensure explicit consent include: . Support folks are the unsung heroes of all matters related to Community &amp; Safety. They are often dropped into a battlefield with very little context of what’s going on. Make it easy for your support folks to help your users quickly and with minimal digging by ensuring there’s a clear audit log trail available.  Audit logs  keep track of your activity, and any organization you own, and can be very helpful to provide context and accountability in the event that something goes wrong. You can read more about audit logs  in the documentation . . Some example questions we ask to ensure proper audit trail logs include: . Many sites are optimized for easy account creation, but this often leads to spam or sock puppet (throwaway) accounts that are handy harassment tools. Limiting the amount of features 0-day accounts can access on high-risk features can help limit abuse. . Some example questions we ask to ensure minimal abuse vectors include: . These are just some things to think about that can help your teams curb abuse vectors on new features before they go out. We hope that this checklist will help you build safer products and lead to happier users. ", "date": "January 31, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tA glimpse into GitHub’s Bug Bounty workflow\t\t", "author": ["\n\t\tGreg Ose\t"], "link": "https://github.blog/2017-02-22-githubs-bug-bounty-workflow/", "abstract": " Last month, we announced the  third anniversary of our Bug Bounty Program . While there’s still time to disclose your findings through the  program , we wanted to pull back the curtain and give you a glimpse into how GitHub’s Application Security team triages and runs it. . Over the three years of our program, it has evolved to streamline our internal processes and to resolve (and pay out!) the submitted issues as quickly as possible. As with most processes, we have iteratively refined and formalized the steps we take for every bounty submission received through the program. Ideally, the details of our process in this post will help other security teams looking to launch or improve their bounty process, as well as offer transparency to our researchers. . Additionally, we have released a  HackerOne API client library  developed for our workflow. We hope other bug bounty teams utilizing HackerOne can leverage this to add or improve the automation within their program. . When the GitHub Application Security Team  launched the program in 2014 , we had several key goals in mind. One particular goal was to ensure that the people taking the time to research and find vulnerabilities in our products were treated and communicated to in a way that respected the time and effort they put into the program. We have strived to maintain a knowledgable and appreciative first response to every submission received. . As the Application Security team has grown in responsibility and duties, it has been hard to manage the effort required for sufficient review and communication amongst other daily tasks. To help maintain sufficient attention to the Bounty program, each member of the Application Security team rotates daily through our on-call, First Responder, schedule. One major task during this assigned day each week is to handle incoming Bug Bounty triage. . The volume of submissions to the program has increased significantly each year ( 2014 ,  2015 ). Given this growth, we identified that there are major advantages of having one person handle the Bug Bounty triage per day. This allows the First Responder to focus exclusively on the Bug Bounty program, without switching between other work. If it is a particularly slow day, they can spend the time catching up on triage backlog or pushing along previous issues that may have stalled on development or other tasks. The First Responder is then the owner of the issue until it is closed out or resolved. This helps to evenly distribute the load over time, reduce duplication of effort in getting up to speed on a submission, and ensure researchers have consistent communication throughout the process. . Having a daily First Responder also helps reduce the mental duplication of work. Even if another team member isn’t on call, it’s tempting to check the incoming reports to see if there are any urgent tasks to attend to. By having one person responsible per day, this allows other members to completely check out of the Bug Bounty and focus on other work, knowing that any high-risk issues will be handled immediately. In reality, specific members may be pulled in as an subject matter expert in an area related to a submission. . Finally, by setting a schedule and committing to consistent triage, it helps us keep the inbox from growing without bounds. It has also helped us avoid periods of Bug Bounty neglect when other team priorities, such as internal code review for new feature launches, demand our full attention. . To ensure consistency throughout the team and create an easily flowing process with researchers, we have created some guidelines for the First Responder’s handling of initial triage. This is not a strict set of rules set upon the First Responder, but a workflow that we have found to be useful to best triage the submissions received. Additionally, to streamline the process for the First Responder, we utilize canned responses for a number of common submissions that typically do not make it to the further stages of triage. . The general steps taken during the initial handling of a submission are: .  Respond and close out the submission if the submission falls outside of the  scope of the Bug Bounty program . By having a set scope of the Bounty program, it helps us to strategically release new targets in scope and to focus researchers in areas that we think are interesting to researchers and important to our core business.  .  Respond and close out the submission if the issue has been previously identified, is an issue we are aware of and consider low risk, or is one of our  commonly received and ineligible submissions . Sometimes we receive reports that are obviously invalid or very low risk. We close out those reports during this intial stage as well.  .  If the submission looks valid, risky, and new, we respond to the researcher letting them know that validation is underway and that we will be in touch once we have an update. Responding before validation allows us to quickly follow up with researchers to let them know we are taking their report seriously, while still being able to take our time to sufficiently vet and understand the issue at hand.  .  Validation can typically be performed directly by the Application Security team member performing the triage. All members of the team have a very strong understanding of our products, access to testing and development environments, logs, as well as source code. This allows us to perform the bulk of validation before escalating to the development teams. At this point we can either move the submission forward with triaging or respond back to the submitter asking for more information to help us reproduce the issue.  .  If we have validated the issue, or have reached the limits of our initial validation and need some expertise from the engineering team, we open an internal tracking issue under the relevant source repository on GitHub. In this issue we provide the full details from the researcher, any details from our initial validation, and typically a set of questions we need clarified by the feature’s or product’s engineers. We then use this issue to work through the root cause of the submission and the best methods for remediation. We communicate with the engineering team to help derive an initial risk of the issue, using critical, high, medium, or low severity buckets and attaching it to the issue using a label. This helps the engineering team to appropriately prioritize any resulting engineering effort. If the issue was not previously validated by the First Responder, they contact the researcher after the engineers have helped us to determine the submission’s validity.  . When we communicate our decision on the validity of the issue to a researcher, we also detail next steps in the process. These steps include the engineering team or Application Security team developing a fix based on the discussed remediation and the Application Security team determining the finalized risk of the issue. . To determine the risk used for our internal prioritization and as a mapping to our payout structure for bounty submissions, we group issues into fairly broad buckets: . We use this same rating to determine the payout of a vulnerability, as well as to express prioritization to the engineering teams. The Application Security team maps each of these risk buckets to also determine a recommended target time to fix, the urgency in which we should escalate the issue, and – if it affects GitHub Enterprise – what should the patch release cycle look like. . All vulnerabilities identified, either internally or externally through the Bug Bounty program, are handled within GitHub’s Engineering teams the same as any other bug would be. Application Security offers our recommendations around remediation and prioritization based on the determined risk. For some issues, depending on the root cause, a member of the Application Security team will fix the issue and request a review by the responsible engineering team. In other cases, such as a larger, more impactful change, the engineering team will take the lead, consulting with the Application Security team for validation of the issue. In all cases, the specific vulnerability is not only fixed, but investigation and analysis is performed to see if other similar code paths could have similar vulnerabilities. . In addition to code fixes, we strongly believe that, like with all bugs, test cases should have caught the issue before it was shipped. We work with the engineering teams to ensure proper test coverage is also included as part of the remediation work, specifically reviewing that we have negative test cases. Similarly, we have internal tooling in place to perform static analysis during the development lifecycle. We use vulnerabilities as a chance to refine and improve the analysis performed during development to catch similar issues in future development. . Depending on the timeline for the corresponding engineering work, a fix may or may not be shipped by the time we get to the fun part: rewarding our intrepid researchers for their hard work. Using the determined risk bucket, we derive a dollar amount to pay for the submission. Over the past three months, we have paid bounty hunters over $80,000 in rewards, with an average award of $1,200 per payout. . After the payout has been determined and communicated, we use HackerOne to issue the payout amount and send some GitHub Security Swag to the researcher. We then close out the report on HackerOne. . In addition to a cash payout, there are few other perks we award to our researchers. As an added bonus we apply a coupon to the researcher’s GitHub account providing 100% off unlimited private repositories for a year. For repeat researchers we extend a lifetime coupon. . We will also add researchers to the  @GitHubBounty organization . If they accept the team invitation, a  Security Bug Bounty Hunter  badge is added to their public GitHub profile. We use this organization to enable soon-to-be-released features to give researchers a head start on finding vulnerabilities. . When we started the program in 2014, we wanted a way to be transparent about the submissions we received and fixed. We determined that the best way to do so would be to build and maintain a  GitHub Pages  and  Jekyll -based site at  https://bounty.github.com . We use this to give an extra shoutout to our researchers and publish a quick writeup of the submissions we have paid out. We feel that this helps our users know what we are doing to fix and secure our products, gives our researchers credit for their awesome work, and hopefully helps other application security teams learn from our experiences with the program. . In running the program, we noticed that the final two steps, adding coupons and teams to a researcher’s GitHub account and writing up posts to the bounty site, were consuming a fair amount of our time. These tasks usually occurred after a fix happened and carried less urgency than the rest of the process, sometimes getting stale and forgotten due to the manual steps required. . With  HackerOne’s release of an API , we took the opportunity to automate these final steps. By issuing a command in our chat system, we can open a PR to our GitHub Pages repo for the bounty site as well as apply coupons and team membership to the researcher’s GitHub account. The bounty site post is templated with the the majority of the required metadata, allowing us to get to the meaty part of the writeup without unnecessary copy and pasting. The   hackerone-client   library was developed to interface this internal tooling with the HackerOne API. . GitHub’s Bug Bounty program has been evolving for the past three years and we’ve learned from the peaks and valleys it has experienced. We have seen moments of overwhelming participation that tax our resources, as well as moments of neglect as our team has shifted priorities at times. Because of these experiences, we’ve been able to create a process that allows our team to work smartly and efficiently. . As we expand the program in the future, we will continue to adapt our tools and processes to fit our needs. We would love feedback from both bug bounty researchers as well as other bug bounty teams. Send us a message on Twitter at  @GitHubSecurity . . GitHub’s Application Security team is also looking to expand. If you are interested working alongside us, check out our  Application Security Engineer job listing . ", "date": "February 22, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit LFS 2.0.0 released\t\t", "author": ["\n\t\tTaylor Blau\t"], "link": "https://github.blog/2017-03-02-git-lfs-2-0-0-released/", "abstract": "    Today we’re announcing the next major release of  Git LFS : v2.0.0. . The official  release notes  have the complete list of all the new features, performance improvements, and more. In the meantime, here’s our look at a few of our newest features: . With Git LFS 2.0.0 you can now lock files that you’re actively working on, preventing others from pushing to the Git LFS server until you unlock the files again. . This will prevent merge conflicts as well as lost work on non-mergeable files at the filesystem level. While it may seem to contradict the distributed and parallel nature of Git, file locking is an important part of many software development workflows—particularly for larger teams working with binary assets. . Git LFS v2.0.0 also comes with a host of other great features, bug fixes, and other changes. . Our transfer queue, the mechanism responsible for uploading and downloading files, is faster, more efficient, and more resilient to failure. . To dive in, visit our  release notes  to learn more. . Git LFS has tremendously improved internals, particularly in Git and filesystem operations.  push  and  pull  operations have been optimized to run concurrently with the underlying tree scans necessary to detect LFS objects. Repositories with large trees can begin the  push  or  pull  operation immediately, while the tree scan takes place, greatly reducing the amount of time it takes to complete these operations. . The mechanism that scans for files tracked by LFS has been enhanced to ignore directories included in your repository’s  .gitignore , improving the efficiency of these operations. . In Git LFS v1.5.0, we introduced the  process  filter (along with  changes in Git v2.11 ) to  dramatically improve performance across multiple platforms , thanks to contributions from  @larsxschneider . . Since its release, Git LFS has benefited from the contributions of  81 members of the open source community . There have been 1,008 pull-requests, 851 of which have been merged into an official release. Git LFS would not be possible without the gracious efforts of our wonderful contributors. Special thanks to  @sinbad  who contributed to our work on file locking. . File locking is an early release, so we’re eager to hear your feedback and thoughts on how the feature should work. . In addition, our  roadmap  is public: comments, questions (and pull requests) are welcomed. To learn more about Git LFS, visit the  Git LFS website . . Psst! We also just announced the  GitHub plugin for Unity , which brings the GitHub workflow to Unity, including support for Git LFS and file locking.  Sign up for early access now . .  \t\t Tags:   \t\t Git \t ", "date": "March 2, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tA formal spec for GitHub Flavored Markdown\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2017-03-14-a-formal-spec-for-github-markdown/", "abstract": " We are glad we chose Markdown as the markup language for user content at GitHub. It provides a powerful yet straightforward way for users (both technical and non-technical) to write plain text documents that can be rendered richly as HTML. . Its main limitation, however, is the lack of standardization on the most ambiguous details of the language. Things like how many spaces are needed to indent a line, how many empty lines you need to break between different elements, and a plethora of other trivial corner cases change between implementations: very similar looking Markdown documents can be rendered as wildly different outputs depending on your Markdown parser of choice. . Five years ago, we started building GitHub’s custom version of Markdown, GFM (GitHub Flavored Markdown) on top of  Sundown , a parser which we specifically developed to solve some of the shortcomings of the existing Markdown parsers at the time. . Today we’re hoping to improve on this situation by releasing a formal specification of the syntax for GitHub Flavored Markdown, and its corresponding reference implementation. . This formal specification is based on  CommonMark , an ambitious project to formally specify the Markdown syntax used by many websites on the internet in a way that reflects its real world usage. CommonMark allows people to continue using Markdown the same way they always have, while offering developers a comprehensive specification and reference implementations to interoperate and display Markdown in a consistent way between platforms. . Taking the CommonMark spec and re-engineering our current user content stack around it is not a trivial endeavour. The main issue we struggled with is that the spec (and hence its reference implementations) focuses strictly on the common subset of Markdown that is supported by the original Perl implementation. This does not include some of the extended features that have been always available on GitHub. Most notably, support for  tables, strikethrough, autolinks and task lists  are missing. . In order to fully specify the version of Markdown we use at GitHub (known as GFM), we had to formally define the syntax and semantics of these features, something which we had never done before. We did this on top of the existing CommonMark spec, taking special care to ensure that our extensions are a strict and optional superset of the original specification. . When reviewing  the GFM spec , you can clearly tell which parts are GFM-specific additions because they’re highlighted as such. You can also tell that no parts of the original spec have been modified and therefore should remain fully compliant with all other implementations. . To ensure that the rendered Markdown in our website is fully compliant with the CommonMark spec, the new backend implementation for GFM parsing on GitHub is based on  cmark , the reference implementation for CommonMark developed by  John MacFarlane  and many other  fantastic contributors . . Just like the spec itself,  cmark  focuses on parsing a strict subset of Markdown, so we had to also implement support for parsing GitHub’s custom  extensions on top of the existing parser. You can find these changes on our  fork of  cmark  ; in order to track the always-improving upstream project, we continuously rebase our patches on top of the upstream master. Our hope is that once a formal specification for these extensions is settled, this patchset can be used as a base to upstream the changes in the original project. . Besides implementing the GFM-specific features in our fork of  cmark , we’ve also contributed many changes of general interest to the upstream. The vast majority of these contributions are focused around performance and security. Our backend renders a massive volume of Markdown documents every day, so our main concern lies in ensuring we’re doing these operations as efficiently as possible, and making sure that it’s not possible to abuse malicious Markdown documents to attack our servers. . The first Markdown parsers in C had a terrible security history: it was feasible to cause stack overflows (and sometimes even arbitrary code execution) simply by nesting particular Markdown elements sufficiently deep. The  cmark  implementation, just like our earlier parser Sundown, has been designed from scratch to be resistant to these attacks. The parsing algorithms and its AST-based output are thought out to gracefully handle deep recursion and other malicious document formatting. . The performance side of  cmark  is a tad more rough: we’ve contributed many optimizations upstream based on performance tricks we learnt while implementing Sundown, but despite all these changes, the current version of  cmark  is still not faster than Sundown itself: Our benchmarks show it to be between 20% to 30% slower on most documents. . The old optimization adage that  “the fastest code is the code that doesn’t run”  applies here: the fact is that  cmark  just does  more things  than Sundown ever did. Amongst other functionality,  cmark  is UTF8 aware, has better support for references, cleaner interfaces for extension, and most importantly: it doesn’t  translate  Markdown into HTML, like Sundown did. It actually generates an AST (Abstract Syntax Tree) out of the source Markdown, which we can transform and eventually render into HTML. . If you consider the amount of HTML parsing that we had to do with Sundown’s original implementation (particularly regarding finding user mentions and issue references in the documents, inserting task lists, etc),  cmark ‘s AST-based approach saves us a tremendous amount of time  and  complexity in our user content stack. The Markdown AST is an incredibly powerful tool, and well worth the performance cost that  cmark  pays to generate it. . Changing our user content stack to be CommonMark compliant is not as simple as switching the library we use to parse Markdown: the fundamental roadblock we encountered here is that the corner cases that CommonMark specifies (and that the original Markdown documentation left ambiguous) could cause some old Markdown content to render in unexpected ways. . Through synthetic analysis of GitHub’s massive Markdown corpus, we determined that less than 1% of the existing user content would be affected by the new implementation: we gathered these stats by rendering a large set of Markdown documents with both the old (Sundown) and the new ( cmark , CommonMark compliant) libraries, normalizing the resulting HTML, and diffing their trees. . 1% of documents with minor rendering issues seems like a reasonable tradeoff to swap in a new implementation and reap its benefits, but at GitHub’s scale, 1% is a lot of content, and a lot of affected users. We really don’t want anybody to check back on an old issue and see that a table that was previously rendering as HTML now shows as ASCII — that is bad user experience, even though obviously none of the original content was lost. . Because of this, we came up with ways to soften the transition. The first thing we did was gathering separate statistics on the two different kinds of Markdown user content we host on the website: comments by the users (such as in Gists, issues, Pull Requests, etc), and Markdown documents inside the Git repositories. . There is a fundamental difference between these two kinds of content: the user comments are stored in our databases, which means their Markdown syntax can be normalized (e.g. by adding or removing whitespace, fixing the indentation, or inserting missing Markdown specifiers until they render properly). The Markdown documents stored in Git repositories, however, cannot be touched  at all , as their contents are hashed as part of Git’s storage model. . Fortunately, we discovered that the vast majority of user content that was using complex Markdown features were user comments (particularly Issue bodies and Pull Request bodies), while the documents stored in Git repositories were rendering properly with both the old and the new renderer in the overwhelming majority of cases. . With this in mind, we proceeded to normalize the syntax of the existing user comments, as to make them render identically in both the old and the new implementations. . Our approach to translation was rather pragmatic: Our old Markdown parser, Sundown, has always acted as a translator more than a parser. Markdown content is fed in, and a set of semantic callbacks convert the original Markdown document into the corresponding markup for the target language (in our use case, this was always HTML5). Based on this design approach, we decided to use the semantic callbacks to make Sundown translate from Markdown to CommonMark-compliant Markdown, instead of HTML. . More than translation, this was effectively a normalization pass, which we had high confidence in because it was performed by the same parser we’ve been using for the past 5 years, and hence all the existing documents should be parsed cleanly while keeping their original semantic meaning. . Once we updated Sundown to normalize input documents and sufficiently tested it, we were ready to start the transition process. The first step of the process was flipping the switch on the new  cmark  implementation for all new user content, as to ensure that we had a finite cut-off point to finish the transition at. We actually enabled CommonMark for all  new  user comments in the website several months ago, with barely anybody noticing — this is a testament to the CommonMark team’s fantastic job at formally specifying the Markdown language in a way that is representative of its real world usage. . In the background, we started a MySQL transition to update in-place the contents of all Markdown user content. After running each comment through the normalization process, and before writing it back to the database, we’d render it with the new implementation and compare the tree to the previous implementation, as to ensure that the resulting HTML output was visually identical and that user data was never destroyed in any circumstances. All in all, less than 1% of the input documents were modified by the normalization process, matching our expectations and again proving that the CommonMark spec really represents the real-world usage of the language. . The whole process took several days, and the end result was that all the Markdown user content on the website was updated to conform to the new Markdown standard while ensuring that the final rendered output was visually identical to our users. . Starting today, we’ve also enabled CommonMark rendering for all the Markdown content stored in Git repositories. As explained earlier, no normalization has been performed on the existing documents, as we expect the overwhelming majority of them to render just fine. . We are really excited to have all the Markdown content in GitHub conform to a live and pragmatic standard, and to be able to provide our users with a  clear and authoritative reference  on how GFM is parsed and rendered. . We also remain committed to following the CommonMark specification as it irons out any last bugs before a final point release. We hope GitHub.com will be fully conformant to the 1.0 spec as soon as it is released. . To wrap up, here are some useful links for those willing to learn more about CommonMark or implement it on their own applications: .  Vicent Martí   Systems Engineer   GitHub Profile |   Twitter Profile  .     .  Ashe Connor   Senior Systems Engineer   GitHub Profile |   Twitter Profile  ", "date": "March 14, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tDiscontinue support for weak cryptographic standards\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2017-02-27-crypto-deprecation-notice/", "abstract": " Cryptographic standards are ever evolving. It is the canonical game of security cat and mouse, with attacks rendering older standards ill-suited, and driving the community to develop newer and stronger standards to take their place. There have been a number of cryptographic attacks over the past of couple of years. These include, but are not limited to, attacks such as  POODLE  and  Logjam  . And, while there have been workarounds for some of these attacks, they demonstrated that several cryptographic standards in wide deployment are showing their age and should be retired. As a result, GitHub is announcing the immediate deprecation, and eventual disablement, of our use of the following cryptographic standards: . All of the above will be disabled on  February 1, 2018 . . In order to minimize the number of users affected by this change we intend do the following before disabling support: . The vast majority of HTTPS connections (approximately 95%) made to  https://github.com  and  https://api.github.com  use TLS 1.2 and will not be affected. This includes every currently shipping browser used by GitHub users. The vast majority of connections made to GitHub services using TLS 1/TLS 1.1 are clients built using older SSL/TLS libraries that do not support TLS 1.2. Mostly commonly, this includes clients built using older versions of the Java JDK as well clients built on operating systems bundled with an older version of OpenSSL. . The Java JDK did not use TLS 1.2 by default until JDK 8 was released in 2014. While JDK 7 supported TLS 1.2, it was not enabled by default for compatibility reasons. Likewise, OpenSSL did not support TLS 1.2  until version 1.0.1  was released in 2012. As a result, several popular older operating systems, such as Red Hat 5, continue to rely on older versions of OpenSSL. We appreciate the difficulty associated with upgrading systems that rely on these older libraries, but feel the security gained for all GitHub users make it a worthwhile trade-off. . GitHub supports both HTTPS as well as SSH based connections when performing Git operations. When a SSH connection is made to github.com, the client and server must determine a mutually agreeable set of cryptographic algorithms to use for the connection. One such algorithm is the key exchange algorithm. The key exchange algorithm is used to securely exchange a strong cryptographic key to protect future messages in the protocol. Without a secure key exchange algorithm, all future messages exchanged between the client and server can’t be trusted. . The  Logjam Attack  research released in 2015 noted some key exchange algorithms were subject to an attack and should be disabled. In particular, they encouraged all system administrators to  disable support  for the  diffie-hellman-group1-sha1  key exchange algorithm. While their analysis further clarified that  diffie-hellman-group14-sha1  should be secure for the foreseeable future, GitHub is choosing to pro-actively discontinue support for this algorithm as well. SSH supports a number of more contemporary algorithms that are not subject, even theoretically, to the precompuation attacks described in the Logjam research. . The majority of SSH connections (approximately 75% ) made to GitHub.com are compatible with more contemporary SSH key exchange algorithms and will not be affected by the removal of  diffie-hellman-group1-sha1  and  diffie-hellman-group14-sha1 . However, that leaves a minority, but still substantial, set of clients that are currently only compatible with one of the legacy key exchange algorithms. Fortunately, the vast majority of these clients do support some newer algorithms, but none that currently overlap with those supported by GitHub. As a result, GitHub will add support for  diffie-hellman-group-exchange-sha256  before we remove support for  diffie-hellman-group1-sha1  and  diffie-hellman-group14-sha1 . By adding support for  diffie-hellman-group-exchange-sha256  we estimate that 5% of current clients would be affected. . We understand that this will incur additional burden for a small set of developers and users. It is for that reason that we are announcing this deprecation now. We hope that, given approximately a year to prepare, developers and users are able to upgrade their operating systems, libraries, and client software to be compatible with these changes. If you have any questions or concerns related to this announcement, please don’t hesitate to  contact us . ", "date": "February 27, 2017"},
{"website": "Github-Engineering", "title": "\n\t\t\tSYN Flood Mitigation with synsanity\t\t", "author": ["\n\t\tTheo Julienne\t"], "link": "https://github.blog/2016-07-12-syn-flood-mitigation-with-synsanity/", "abstract": " GitHub hosts a wide range of user content, and like all large websites this often causes us to become a target of denial of service attacks. Around a year ago, GitHub was on the receiving end of a large, unusual and very well publicised attack involving both application level and volumetric attacks against our infrastructure. . Our users rely on us to be highly available and we take this seriously. Although the attackers are doing the wrong thing, there’s no use blaming the attacker for their attacks being successful. Our commitment is to own our own availability, and that we have a responsibility to mitigate these sorts of attacks to the maximum extent technically possible. . In an effort to reduce the impact of these attacks, we began work on a series of additional mitigation strategies and systems to better prepare us for a future attack of a similar nature. Today we’re sharing our mitigation for one of the attacks we received: synsanity, a SYN flood DDoS mitigation module for Linux 3.x. . SYN floods are one of the oldest and most common attacks, so common that the Linux kernel includes some built in support for mitigating them. When a client connects to a server using TCP, it uses the  three-way handshake  to synchronise: . A SYN packet is essentially the client telling the server “I’d like to connect”. During this handshake, both client and server generate random Initial Sequence Numbers (ISNs), which are used to synchronise the TCP connection between the two parties. These sequence numbers let TCP keep track of which messages have been sent and acknowledged by the other party. . A SYN flood abuses this handshake by only going part way through the handshake. Rather than progressing through the normal sequence, an attacker floods the target server with as many SYN packets as they can muster, from as many different hosts as they can, and spoofing the origin IP as much as they can. . The host receiving the SYN flood must respond to each and every packet with a SYN-ACK, but unfortunately the source IP was likely spoofed, so they go nowhere (or worse, come back as rejected). These packets are almost indistinguishable from real SYN packets from real clients, which means it’s hard or impossible to filter out the bad ones on the server. Even external DDoS scrubbing services can only guess whether a packet is legitimate or part of a flood, making it difficult to mitigate an attack without impacting legitimate traffic. . To make matters worse, when the server is handling normal connections and receives the ACK from a real client, it still needs to know that it came from a SYN packet it sent, so it must also keep a list of connections (in state  SYN_RECV ) for which a SYN has been received and an ACK has not yet been received. . During a SYN flood, this behaviour is undesirable. If the queue of connections in  SYN_RECV  has no size limit, memory will get exhausted pretty quickly. If it does have a size limit, as is the case in Linux, then there’s no more space to store state and the connections will simply fail as the packets are dropped. .  SYN cookies  are a clever way of avoiding the storage of TCP connection state during the initial handshake, deferring that storage until a valid ACK has been received. It works by crafting the Initial Sequence Number (ISN) in the SYN-ACK packet sent by the server in such a way that it cryptographically hashes details about the initial SYN packet and its TCP options, so that when the ACK is received (with a sequence number 1 larger than the ISN), the server can validate that it generated the SYN-ACK packet for which an ACK is now being received. The server stores no state for the connection until the ACK (containing the validated SYN cookie) is received, and only at that point is state regenerated and stored. . Since this hash is calculated with a secret that only the server knows, it doesn’t significantly weaken the sequence number selection and it’s still difficult for someone to forge an ACK (or other packet) for a different connection without having seen the SYN-ACK from the real server. . SYN cookies have been around for a while, and they have fairly minimal impact on the reliability and spoof-protection of TCP. Rather than enabling them constantly, the Linux kernel by default automatically enables SYN cookies only when the SYN receive queue is full. This means that under normal circumstances when no SYN flood is occurring, you get no impact at all, but during a SYN flood, you accept the minimal impact of SYN cookies (in return for not dropping connections). The extra CPU cost of creating SYN cookies is offset by the fact that you no longer have a limited resource, and in practise this is an excellent trade-off. . In Linux 3.x, SYN cookies are generated inside a machine-wide lock on the LISTEN socket that the packet was destined for. This implementation causes all SYN cookies to be generated serially across all cores, defeating the benefits of a multi-processor system. To make matters worse, all cores spin waiting for the lock to become available. This was fine back in the days when an average attacker could only send a few MBits of SYN packets your way, mostly thanks to the networks being much slower. These days however, with servers attached to transit providers with multiple 10GB+ links the whole way down the line, it’s now possible to completely saturate CPU resources. . While Linux 4.x has a patch to send SYN cookies under a per-CPU-core socket lock, which does fix the problem, we wanted a solution that allowed us to use an existing, maintained kernel with upstream security patches. We didn’t want to roll and maintain an entire custom kernel and all related future security patches just to mitigate this form of attack. Patching Linux 3.x to backport the socket lock change was also a similar maintenance burden we wanted to avoid. . One solution to get the best of both worlds was the SYNPROXY iptables module. It sits in  netfilter  in the kernel, before the Linux TCP stack, and as the name suggests, proxies all connections while generating SYN cookies. When a SYN packet comes in, it responds with a SYN-ACK and throws away all state. On receipt of a valid ACK packet matching the SYN cookie, it then sends a SYN downstream and completes the usual TCP handshake. For every subsequent packet in each direction, it modifies the sequence numbers so that it is transparent to both sides. . This is quite an intrusive way of solving the problem since it touches every packet during the entire connection, but it does successfully mitigate SYN floods. Unfortunately we found that in practise under our load and with the amount of malformed packets we receive, it quickly broke down and caused a kernel panic. Additionally, it had to be enabled all the time, since there was no simple way to activate it only when under attack. This meant that we would have to accept the minimal impact of SYN cookies constantly, and at our scale this still would likely cause issues for some of our users. . We decided that it was more complicated than it needed to be for our use case, and we wanted a simpler solution that would only touch the packets that needed to be touched to mitigate a SYN flood. We also decided that a mitigation should only cause potential (even if minimal) impact during mitigation, and not under normal operation. . Enter synsanity, our solution to mitigate SYN floods on Linux 3.x. synsanity is inspired by SYNPROXY, in that it is an iptables module that sits inside iptables between the Linux TCP stack and the network card. The major difference is that rather than touch all packets, synsanity simply generates a SYN cookie identically to the way the Linux kernel would generate one if the SYN queue was full, and once it validates the ACK packet, it allows it through to the standard Linux SYN cookie code, which creates and completes the connection. After this point, synsanity doesn’t touch any further packet in the TCP connection. . Similar to the way that Linux only enables SYN cookies when the SYN queue overflows, we only enable synsanity when the SYN queue overflows as well. We match the core Linux code exactly, except that we do it in an iptables module, outside the LISTEN lock. Since an iptables module can be compiled and maintained outside the Linux kernel source tree itself, we don’t need to use a custom Linux kernel, and can instead just maintain and deploy a single module to our servers. . synsanity has allowed us to mitigate multiple attacks that would have previously caused a partial or complete service outage, both long running attacks and large volume attacks. . We believe that if you need to hide your mitigation to keep it secure, it’s not designed well enough. The best and most secure tools are shared, open and subject to community scrutiny, so today we’re open sourcing  synsanity  so that everyone can benefit from this work. ", "date": "July 12, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tgh-ost: GitHub’s online schema migration tool for MySQL\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-08-01-gh-ost-github-s-online-migration-tool-for-mysql/", "abstract": " Today we are announcing the open source release of  gh-ost : GitHub’s triggerless online schema migration tool for MySQL. .  gh-ost  has been developed at GitHub in recent months to answer a problem we faced with ongoing, continuous production changes requiring modifications to MySQL tables.  gh-ost  changes the existing online table migration paradigm by providing a low impact, controllable, auditable, operations friendly solution. . MySQL table migration is a well known problem, and has been addressed by online schema change tools since 2009. Growing, fast-paced products often require changes to database structure. Adding/changing/removing columns and indexes etc., are blocking operations with the default MySQL behavior. We conduct such schema changes multiple times per day and wish to minimize user facing impact. . Before illustrating  gh-ost , let’s address the existing solutions and the reasoning for embarking on a new tool. . Today, online schema changes are made possible via these three main options: . Other options include Rolling Schema Upgrade with Galera Cluster, and otherwise non-InnoDB storage engines. At GitHub we use the common master-replicas architecture and utilize the reliable InnoDB engine. . Why have we decided to embark on a new solution rather than use either of the above? The existing solutions are all limited in their own ways, and the below is a very brief and generalized breakdown of some of their shortcomings. We will drill down more in-depth about the shortcomings of the trigger-based online schema change tools. . All online-schema-change tools operate in similar manner: they create a  ghost  table, in the likeness of your original table, migrate that table while empty, slowly and incrementally copy data from your original table to the  ghost  table, meanwhile propagating ongoing changes (any  INSERT ,  DELETE ,  UPDATE  applied to your table) to the  ghost  table. When the tool is satisfied the tables are in sync, it replaces your original table with the  ghost  table. . Tools like  pt-online-schema-change ,  LHM  and  oak-online-alter-table  use a synchronous approach, where each change to your table translates immediately, utilizing same transaction space, to a mirrored change on the  ghost  table. The Facebook tool uses an asynchronous approach of writing changes to a changelog table, then iterating that and applying changes onto the  ghost  table. All of these tools use triggers to identify those ongoing changes to your table. . Triggers are stored routines which are invoked on a per-row operation upon  INSERT ,  DELETE ,  UPDATE  on a table. A trigger may contain a set of queries, and these queries run in the same transaction space as the query that manipulates the table. This makes for an atomicy of both the original operation on the table and the trigger-invoked operations. . Trigger usage in general, and trigger-based migrations in particular, suffer from the following: .  gh-ost  stands for GitHub’s Online Schema Transmogrifier/Transfigurator/Transformer/Thingy .  gh-ost  is: .  gh-ost  does not use triggers. It intercepts changes to table data by tailing the binary logs. It therefore works in an asynchronous approach, applying the changes to the  ghost  table some time after they’ve been committed. .  gh-ost  expects binary logs in RBR (Row Based Replication) format; however that does not mean you cannot use it to migrate a master running with SBR (Statement Based Replication). In fact, we do just that.  gh-ost  is happy to read binary logs from a replica that translates SBR to RBR, and it is happy to reconfigure the replica to do that. . By not using triggers,  gh-ost  decouples the migration workload from the general master workload. It does not regard the concurrency and contention of queries running on the migrated table. Changes applied by such queries are streamlined and serialized in the binary log, where  gh-ost  picks them up to apply on the  gh-ost  table. In fact,  gh-ost  also serializes the row-copy writes along with the binary log event writes. Thus, the master only observes a single connection that is sequentially writing to the  ghost  table. This is not very different from ETLs. . Since all writes are controlled by  gh-ost , and since reading the binary logs is an asynchronous operation in the first place,  gh-ost  is able to suspend all writes to the master when throttling. Throttling implies no row-copy on the master  and  no row updates.  gh-ost  does create an internal tracking table and keeps writing heartbeat events to that table even when throttled, in negligible volumes. .  gh-ost  takes throttling one step further and offers multiple controls over throttling: . All the above metrics can be  dynamically changed  even while the migration is executing.  . With existing tools, when a migration generates a high load, the DBA would reconfigure, say, a smaller  chunk-size , terminate and re-run the migration from start. We find this wasteful. .  gh-ost  listens to requests via unix socket file and (configurable) via TCP. You may give  gh-ost  instructions even while migration is running. You may, for example: . Likewise, the same interface can be used to ask  gh-ost  of the  status .  gh-ost  is happy to report current progress, major configuration params, identity of servers involved and more. As this information is accessible via network, it gives great visibility into the ongoing operation, that you would otherwise find today only by using a shared screen or tailing log files. . Because the binary log content is decoupled from the master’s workload, applying a migration on a replica is more  similar to a true master migration (though still not completely, and more work is on the roadmap). .  gh-ost  comes with built-in support for testing via  --test-on-replica : it allows you to run a migration on a replica, such that at the end of the migration  gh-ost  would stop the replica, swap tables, reverse the swap, and leave you with both tables in place and in sync, replication stopped. This allows you to examine and compare the two tables at your leisure. . This is how we test  gh-ost  in production at GitHub: we have multiple designated production replicas; they are not serving traffic but instead running continuous covering migration test on all tables. Each of our production tables, as small as empty and as large as many hundreds of GB, is being migrated via a trivial statement that does not really modify its structure ( engine=innodb ). Each such migration ends with stopped replication. We take complete checksum of entire table data from both the original table and  ghost  table and expect them to be identical. We then resume replication and proceed to next table. Every single one of our production tables is  known  to have passed multiple successful migrations via  gh-ost , on replica. . All the above, and more, are made to build trust with  gh-ost ‘s operation. After all, it is a new tool in a landscape that has used the same tool for years. .  gh-ost  operates by connecting to potentially multiple servers, as well as connecting itself as a replica in order to stream binary log events directly from one of those servers. There are various operation modes, which depend on your setup, configuration, and where you want to run the migration. . This is the mode  gh-ost  expects by default.  gh-ost  will investigate the replica, crawl up to find the topology’s master, and connect to it as well. Migration will: . If your master works with SBR, this is the mode to work with. The replica must be configured with binary logs enabled ( log_bin ,  log_slave_updates ) and should have  binlog_format=ROW  ( gh-ost  can apply the latter for you). . However even with RBR we suggest this is the least master-intrusive operation mode. . If you don’t have replicas, or do not wish to use them, you are still able to operate directly on the master.  gh-ost  will do all operations directly on the master. You may still ask it to be considerate of replication lag. . This will perform a migration on the replica.  gh-ost  will briefly connect to the master but will thereafter perform all operations on the replica without modifying anything on the master.  Throughout the operation,  gh-ost  will throttle such that the replica is up to date. .  gh-ost  is now powering all of our production migrations. We’re running it daily, as engineering requests come, sometimes multiple times a day. With its auditing and control capabilities, we will be integrating it into our chatops. Our engineers will have clear insight into migration progress and will be able to control its behavior. Metrics and events are being collected and will provide with clear visibility into migration operations in production. .  gh-ost  is  released  with    to the open source community  under the  MIT  license . . While we find it to be stable, we have improvements we want to make. We release it at this time as we wish to welcome community participation and contributions. From time to time we may publish suggestions for community contributions. .  gh-ost  is actively maintained. We encourage you to try it out, test it; we’ve made great efforts to make it trustworthy. .  gh-ost  is designed, developed, reviewed and tested by the database infrastructure engineering team at GitHub: .  @jonahberquist ,  @ggunson ,  @tomkrouper ,  @shlomi-noach  . We would like to acknowledge the engineers at GitHub who have provided valuable information and advice. Thank you to our friends from the MySQL community who have reviewed and commented on this project during its pre-production stages. ", "date": "August 1, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tContext aware MySQL pools via HAProxy\t\t", "author": ["\n\t\tShlomi Noach\t"], "link": "https://github.blog/2016-08-17-context-aware-mysql-pools-via-haproxy/", "abstract": " At GitHub we use MySQL as our main datastore. While repository data lies in  git , metadata is stored in MySQL. This includes Issues, Pull Requests, Comments etc. We also auth against MySQL via a custom git proxy ( babeld ). To be able to serve under the high load GitHub operates at, we use MySQL replication to scale out read load. . We have different clusters to provide with different types of services, but the single-writer-multiple-readers design applies to them all. Depending on growth of traffic, on application demand, on operational tasks or other constraints, we take replicas in or out of our pools. Depending on workloads some replicas may lag more than others. . Displaying up-to-date data is important. We have tooling that helps us ensure we keep replication lag at a minimum, and typically it doesn’t exceed  1  second. However sometimes lags do happen, and when they do, we want to put aside those lagging replicas, let them catch their breath, and avoid sending traffic their way until they are caught up. . We set out to create a self-managing topology that will exclude lagging replicas automatically, handle disasters gracefully, and yet allow for complete human control and visibility. . We use HAProxy for various tasks at GitHub. Among others, we use it to load balance our MySQL replicas. Our applications connect to HAProxy servers at  :3306  and are routed to replicas that can serve read requests. Exactly what makes a replica able to “serve read requests” is the topic of this post. . MySQL load balancing via HAProxy is commonly used, but we wanted to tackle a few operational and availability concerns: . With this criteria in mind, the standard  mysql-check  commonly used in HAProxy-MySQL load balancing will not suffice. . This simple check merely tests whether a MySQL server is live and doesn’t gain additional insight as for its internal replication state (lag/broken) or for its operational state (maintenance/ETL/backup jobs etc.). . Instead, we make our HAProxy pools context aware. We let the backend MySQL hosts make an informed decision: “should I be included in a pool or should I not?” . In its very simplistic form, context awareness begins with asking the MySQL backend replica: “are you lagging?” We will reach far beyond that, but let’s begin by describing this commonly used setup. . In this situation, HAProxy no longer uses a  mysql-check  but rather an  http-check . The MySQL backend server provides an  HTTP  interface, responding with  HTTP 200  or  HTTP 503  depending on replication lag. HAProxy will interpret these as “good” ( UP ) or “bad” ( DOWN ), respectively. On the HAProxy side, it looks like this: . The backend servers need to provide an HTTP service on  :9876 . That service would connect to MySQL, check for replication lag, and return with  200  (say,  lag &lt;= 5s ) or  503  ( lag &gt; 5s  or replication is broken). . This commonly used setup automatically excludes or includes backend servers based on replication status. If the server is lagging, the specialized HTTP service will report  503 , which HAProxy will interpret as  DOWN , and the server will not serve traffic until it recovers. . But, what happens when two, three, or four replicas are lagging? We are left with less and less serving capacity. The remaining replicas are receiving two or three times more traffic than they’re used to receiving. If this happens, the replicas might succumb to the load and lag as well, and the solution above might not be able to handle an entire fleet of lagging replicas. . What’s more, some of our replicas have special roles. Each cluster has a node running continuous logical or physical backups. For example, other nodes might be serving a purely analytical workload or be partially weighted to verify a newer MySQL version. . In the past, we would update the HAProxy config file with the list of servers as they came and went. As we grew in volume and in number of servers this became an operational overhead. We’d rather take a more dynamic approach that provides increased flexibility. . We may operate a MySQL master failover. This may be a planned operation (e.g. upgrading to latest release) or an unplanned one (e.g. automated failover on hardware failure). The new master must be excluded from the read-pool. The old master, if available, may now serve reads. Again, we wish to avoid the need to update HAProxy’s configuration with these changes. . In our current setup the HAProxy configuration does not regularly change. It may change when we introduce new hardwares now and then, but otherwise it is static, and HAProxy reacts to ongoing instructions by the backend servers telling it: . The HAProxy config file lists each and every known server. The list includes the backup server. It includes the analytics server. It even includes the master. And the backend servers tell HAProxy whether they wish to participate in taking read traffic or not. . The HAProxy config file lists each and every known server. The list includes the backup server, the analytics server, and even the master. The backend servers themselves tell HAProxy whether they wish to participate in taking read traffic or not. . Before showing you how to implement this, let’s consider availability. . HAProxy supports multiple backend pools per frontend, and provides with Access Control Lists ( ACLs ). ACLs often use incoming connection data (headers, cookies etc.) but are also able to observe backend status. . The scheme is to define two (or more) backend pools: . We use an  acl  that observes the number of available servers in our  main  backend. We then set a rule to use the  backup  pool if that  acl  applies: . See  code sample  . In the example above we choose to switch to the  mysql_ro_backup  pool when left with less than three active hosts in our  mysql_ro_main  pool. We’d rather serve stale data than stop serving altogether. Of course, by this time our alerting system will have alerted us to the situation and we will already be looking into the source of the problem. . Remember that it’s not HAProxy that makes the decision “who’s in and who’s out” but the backend server itself. To that effect, HAProxy sends a check  hint  to the server. We choose to send the hint in the form of a URI, as this makes for a readable, clear code: . See  code sample  . Both backend pools list the exact same servers. The major difference between the pools is the  check  URI: . vs. . As the URI suggests, the first,  main  pool is looking for backup servers that do not lag (and we wish to also exclude the master, the backup server, etc.). The  backup  pool is happy to take servers that actually do lag. But still, it wishes to exclude the master and other special servers. . HAProxy’s behavior is to use the  main  pool for as long as at least three replicas are happy to serve data. If only two replicas or less are in good shape, HAProxy switches to the  backup  pool, where we re-introduce the lagging replicas; serving more stale data, but still serving. . Also noteworthy in the above is  http-check disable-on-404 , which puts a  HTTP 404  server in a  NOLB  state. We will discuss this in more detail soon. . Any  HTTP  service implementation will do. At GitHub, we commonly use  shell  and  Ruby  scripts, that integrate well with our  ChatOps . We have many reliable  shell  building blocks, and our current solution is a  shell  oriented service, in the form of  xinetd . .  xinetd  makes it easy to “speak HTTP” via  shell . A simplified setup looks like this: . In the above, we’re in particular interested that  xinetd  serves on  :9876  and calls upon  /path/to/scipts/xinetd-mysql  to respond to HAPRoxy’s  check  requests. . The  xinetd-mysql  script routes the request to an appropriate handler. Recall that we asked HAProxy to  hint  per  check . The hint URI, such as  /check-lag , is intercepted by  xinetd-mysql  which further invokes a dedicated handler for this check. Thus, we have different handlers for  /check-lag ,  /ignore-lag ,  /ignore-lag-and-yes-please-allow-backup-servers-as-well  etc. . The real magic happens when running  this handler script . This is where the server makes the decision: “Should I be included in the read-pool or not?” The script bases its decision on the following factors: . This  xinetd / shell  implementation suggests we do not use persistent MySQL connections; each  check  generates a new connection on the backend server. While this seems wasteful, the rate of incoming check requests is not high, and negligible in the scale of our busy servers. But, furthermore, this better serves our trust in the system: a hogged server may be able to serve existing connections but refuse new ones; we’re happy to catch this scenario. . Servers that just don’t want to participate send a  404 , causing them to go  NOLB . Lagging, broken or dead replicas send a  503 . This makes it easier on our alerting system and makes it clearer when we have a problem. . One outstanding issue is that HAProxy never transitions from  DOWN  to  NOLB . The automaton requires first going  UP . This is not an integrity problem but causes more alerting. We work around this by cross checking servers and refreshing if need be. This is a rare situation for us and thus of no significant concern. . This small building blocks design permits us to do simple unit testing. Control and visibility are easily gained: disabling and enabling servers is a matter of creating a file. Whether forced to exist by a human or implied by server role. . These scripts integrate well within our chatops. We are able to see the exact response HAProxy sees via simple chatops commands: . Or we can interfere and force backends in/out the pools: . We have specialized monitoring for these HAProxy boxes, but we don’t wish to be notified if a single replica starts to lag. Rather, we’re interested in the bigger picture: a summary of the total found errors in the pools. This means there’s a difference between a half empty  main  pool and a completely empty one. In the event of problems, we get a single alert that summarizes the status across the cluster’s pools. As always, we can also check from chatops: . We’ve stripped our script and config files to decouple them from GitHub’s specific setup and flow. We’ve also  open sourced them  in the hope that you’ll  find them useful, and that they’ll help you implement your own solution with context-aware MySQL replica pools. ", "date": "August 17, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tBuilding resilience in Spokes\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2016-09-07-building-resilience-in-spokes/", "abstract": "  Spokes  is the replication system for the file servers where we store over 38 million Git repositories and over 36 million gists.It keeps at least three copies of every repository and every gist so that we can provide durable, highly available access to content even when servers and networks fail. Spokes uses a combination of Git and rsync to replicate, repair, and rebalance repositories. . Before we get into the topic at hand—building resilience—we have a new name to announce: DGit is now Spokes. . Earlier this year, we  announced  “DGit” or “Distributed Git,” our application-level replication system for Git. We got feedback that the name “DGit” wasn’t very distinct and could cause confusion with the Git project itself. So we have decided to rename the system  Spokes . . In any system or service, there are two key ways to measure resilience: availability and durability. A system’s availability is the fraction of the time it can provide the service it was designed to provide. Can it serve content? Can it accept writes? Availability can be partial, complete, or degraded: is every repository available? Are some repositories—or whole servers—slow? . A system’s durability is its resistance to permanent data loss. Once the system has accepted a write—a push, a merge, an edit through the website, new-repository creation, etc.—it should never corrupt or revert that content. The key here is the moment that the system accepts the write: how many copies are stored, and where? Enough copies must be stored for us to believe with some very high probability that the write will not be lost. . A system can be durable but not available. For example, if a system can’t make the minimum required number of copies of an incoming write, it might refuse to accept writes. Such a system would be temporarily unavailable for writing, while maintaining the promise not to lose data. Of course, it is also possible for a system to be available without being durable, for example, by accepting writes whether or not they can be committed safely. . Readers may recognize this as related to the  CAP Theorem . In short, a system can satisfy at most two of these three properties: . Spokes puts the highest priority on consistency and partition tolerance. In worst-case failure scenarios, it will refuse to accept writes that it cannot commit, synchronously, to at least two replicas. . Spokes’s availability is a function of the availability of underlying servers and networks, and of our ability to detect and route around server and network problems. . Individual servers become unavailable pretty frequently. Since rolling out Spokes this past spring, we have had individual servers crash due to a kernel deadlock and faulty RAM chips. Sometimes servers provide degraded service due to lesser hardware faults or high system load. In all cases, Spokes must detect the problem quickly and route around it. Each repository is replicated on three servers, so there’s almost always an up-to-date, available replica to route to even if one server is offline. Spokes is more than the sum of its individually-failure-prone parts. . Detecting problems quickly is the first step. Spokes uses a combination of heartbeats and real application traffic to determine when a file server is down. Using real application traffic is key for two reasons. First, heartbeats learn and react slowly. Each of our file servers handles a hundred or more incoming requests per second. A heartbeat that happens once per second would learn about a failure only after a hundred requests had already failed. Second, heartbeats test only a subset of the server’s functionality: for example, whether or not the server can accept a TCP connection and respond to a no-op request. But what if the failure mode is more subtle? What if the Git binary is corrupt? What if disk accesses have stalled? What if all authenticated operations are failing? No-ops can often succeed when real traffic will fail. . So Spokes watches for failures during the processing of real application traffic, and it marks a node as offline if too many requests fail. Of course, real requests do fail sometimes. Someone can try to read a branch that has already been deleted, or try to push to a branch they don’t have access to, for example. So Spokes only marks the node offline if three requests fail in a row. That sometimes marks perfectly healthy nodes offline—three requests can fail in a row just by random chance—but not often, and the penalty for it is not large. . Spokes uses heartbeats, too, but not as the primary failure-detection mechanism. Instead, heartbeats have two purposes: polling system load and providing the all-clear signal after a node has been marked as offline. As soon as a heartbeat succeeds, the node is marked as online again. If the heartbeat succeeds despite ongoing server problems (retrieving system load is almost a no-op), the node will get marked offline again after three more failed requests. . So Spokes detects that a node is down within about three failed operations. That’s still three failed operations too many! For clean failures—connections refused or timeouts—all operations know how to try the next host. Remember, Spokes has three or more copies of every repository. A routing query for a repository returns not one server, but a list of three (or so) up-to-date replicas, sorted in preference order. If an operation attempted on the first-choice replica fails, there are usually two other replicas to try. . A graph of operations (here, remote procedure calls, or RPCs) failed over from one server to another clearly shows when a server is offline. In this graph, a single server is unavailable for about 1.5 hours; during this time, many thousands of RPC operations are redirected to other servers. This graph is the single best detector the Spokes team has for discovering misbehaving servers. . Spokes’s node-offline detection is only advisory—i.e., only an optimization. A node that has had three failures in a row just gets moved to the end of the preference order for all read operations, rather than removed from the list of replicas. It’s better for Spokes to try a probably-offline replica last, than to not try it at all. . This failure detector works well for server failures: when a server is overloaded or offline, operations to it will fail. Spokes detects those failures and temporarily stops directing traffic to the failed server until a heartbeat succeeds. However, failures of networks and application (Rails) servers are much messier. A given file server can appear to be offline to just a subset of the application servers, or one bad application server can spuriously determine that every file server is offline. So Spokes’s failure detection is actually MxN: each application server keeps its own list of which file servers are offline, or not. If we see many application servers marking a single file server as offline, then it probably is. If we see a single application server marking many file servers offline, then we’ve learned about a fault on that application server, instead. . The figure below illustrates the MxN nature of failure detection and shows in red which failure detectors are true if a single file server,  dfs4 , is offline. . In one recent incident, a single front-end application server in a staging environment lost its ability to resolve the DNS names of the file servers. Because it couldn’t reach the file servers to send them RPC operations or heartbeats, it concluded that every file server was offline. But that incorrect determination was limited to that one application server; all other application servers worked normally. So the flaky application server was immediately obvious in the RPC-failover graphs, and no production traffic was affected. . Sometimes, servers fail. Disks can fail; RAID controllers can fail; even entire servers or entire racks can fail. Spokes provides durability for repository data even in the face of such adversity. . The basic building block of durability, like availability, is replication. Spokes keeps at least three copies of every repository, wiki, and gist, and those copies are in different racks. No updates to a repository—pushes, renames, edits to a wiki, etc.—are accepted unless a strict majority of the replicas can apply the change and get the same result. . Spokes needs just one extra copy to survive a single-node failure. So why a majority? It’s possible, even common, for a repository to get multiple writes at roughly the same time. Those writes might conflict: one user might delete a branch while another user pushes new commits to that same branch, for example. Conflicting writes must be serialized—that is, they have to be applied (or rejected) in the same order on every replica, so every replica gets the same result. The way Spokes serializes writes is by ensuring that every write acquires an exclusive lock on a majority of replicas. It’s impossible for two writes to acquire a majority at the same time, so Spokes eliminates conflicts by eliminating concurrent writes entirely. . If a repository exists on exactly three replicas, then a successful write on two replicas constitutes both a durable set, and a majority. If a repository has four or five replicas, then three are required for a majority. . In contrast, many other replication and consensus protocols have a single primary copy at any moment. The order that writes arrive at the primary copy is the official order, and all other replicas must apply writes in that order. The primary is generally designated manually, or automatically using an election protocol. Spokes simply skips that step and treats every write as an election—selecting a winning order and outcome directly, rather than a winning server that dictates the write order. . Any write in Spokes that can’t be applied identically at a majority of replicas gets reverted from any replica where it was applied. In essence, every write operation goes through a voting protocol, and any replicas on the losing side of the vote are marked as unhealthy—unavailable for reads or writes—until they can be repaired. Repairs are automatic and quick. Because a majority agreed either to accept or to roll back the update, there are still at least two replicas available to continue accepting both reads and writes while the unhealthy replica is  repaired. . To be clear, disagreements and repairs are exceptional cases. GitHub accepts many millions of repository writes each day. On a typical day, a few dozen writes will result in non-unanimous votes, generally because one replica was particularly busy, the connection to it timed out, and the other replicas voted to move on without it. The lagging replica almost always recovers within a minute or two, and there is no user-visible impact on the repository’s availability. . Rarer still are whole-disk and whole-server failures, but they do happen. When we have to remove an entire server, there are suddenly hundreds of thousands of repositories with only two copies, instead of three. This, too, is a repairable condition. Spokes checks periodically to see if every repository has the desired number of replicas; if not, more replicas are created. New replicas can be created anywhere, and they can be copied from wherever the surviving two copies of each repository are. Hence, repairs after a server failure are N-to-N. The larger the file server cluster, the faster it can recover from a single-node failure. . As described above, Spokes can deal quickly and transparently with a server going offline or even failing permanently. So, can we use that for planned maintenance, when we need to reboot or retire a server? Yes and no. . Strictly speaking, we can reboot a server with  sudo reboot , and we can retire it just by unplugging it. But there are subtle disadvantages to doing so, so we have more careful mechanisms, reusing a lot of the same logic that would respond to a crash or a failure. . Simply rebooting a server does not affect future read and write operations, which will be transparently directed to other replicas. It doesn’t affect in-progress write operations, either, as those are happening on all replicas, and the other two replicas can easily vote to proceed without the server we’re rebooting. But a reboot does break in-progress read operations. Most of those reads—e.g., fetching a README to display on a repository’s home page—are quick and will complete while the server shuts down gracefully. But some reads, particularly clones of large repositories, take minutes or hours to complete, depending on the speed of the end user’s network. Breaking these is, well, rude. They can be restarted on another replica, but all progress up to that point would be lost. . Hence, rebooting a server intentionally in Spokes begins with a quiescing period. While a server is quiescing, it is marked as offline for the purposes of new read operations, but existing read operations, including clones, are allowed to finish. Quiescing can take anywhere from a few seconds to many hours, depending on which read operations are active on the server that is getting rebooted. . Perhaps surprisingly, write operations are sent to servers as usual, even while they quiesce. That’s because write operations run on all replicas, so one replica can drop out at any time without user-visible impact. Also, that replica would fall arbitrarily far behind if it didn’t receive writes while quiescing, creating a lot of catch-up load when it is finally brought fully back online. . We don’t perform “ chaos monkey ” testing on the Spokes file servers, for the same reasons we prefer to quiesce them before rebooting them: to avoid interrupting long-running reads. That is, we do not reboot them randomly just to confirm that sudden, single-node failures are still (mostly) harmless. . Instead of “chaos monkey” testing, we perform rolling reboots as needed, which accomplish roughly the same testing goals. When we need to make some change that requires a reboot—e.g., changing kernel or filesystem parameters, or changing BIOS settings—we quiesce and reboot each server. Racks serve as availability zones  [1]  , so we quiesce entire racks at a time. As servers in a given rack finish quiescing—i.e., complete all outstanding read operations—we reboot up to five of them at a time. When a whole rack is finished, we move on to the next rack. . Below is a graph showing RPC operations failed over during a rolling reboot. Each server gets a different color. Values are stacked, so the tallest spike shows a moment where eight servers were rebooting at once. The large block of light red shows where one server did not reboot cleanly and was offline for over two hours. . Retiring a server by simply unplugging it has the same disadvantages as unplanned reboots, and more. In addition to disrupting any in-progress read operations, it creates several hours of additional risk for all the repositories that used to be hosted on the server. When a server disappears suddenly, all of the repositories formerly on it are now down to two copies. Two copies are enough to perform any read or write operation, but two copies aren’t enough to tolerate an additional failure. In other words, removing a server without warning increases the probability of rejecting write operations later that same day. We’re in the business of keeping that probability to a minimum. . So instead, we prepare a server for retirement by removing it from the count of active replicas for any repository. Spokes can still use that server for both read and write operations. But when it asks if all repositories have enough replicas, suddenly some of them—the ones on the retiring server—will say no, and more replicas will be created. These repairs proceed exactly as if the server had just disappeared, except that now the server remains available in case some other server fails. . Availability is important, and durability is more important still. Availability is a measure of what fraction of the time a service responds to requests. Durability is a measure of what fraction of committed data a service can faithfully store. . Spokes keeps at least three replicas of every repository, to provide both availability and durability. Three replicas means that one server can fail with no user-visible effect. If two servers fail, Spokes can provide full access for most repositories and read-only access to repositories that had two of their replicas on the two failing servers. . Spokes does not accept writes to a repository unless a majority of replicas—and always at least two—can commit the write and produce the same resulting repository state. That requirement provides consistency by ensuring the same write ordering on all replicas. It also provides durability in the face of single-server failures by storing every committed write in at least two places. . Spokes has a failure detector, based on monitoring live application traffic, that determines when a server is offline and routes around the problem. Finally, Spokes has automated repairs for recovering quickly when a disk or server fails permanently. .   1. Treating racks as availability zones means we place repository replicas so that no repository has two replicas within the same rack. Hence, we can lose an entire rack of servers and not affect the availability or durability of any of the repositories hosted on them. We chose racks as availability zones because several important failure modes, especially related to power and networking, can affect entire racks of servers at a time. ", "date": "September 7, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe GitHub GraphQL API\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-09-14-the-github-graphql-api/", "abstract": " GitHub announced a public API  one month after the site launched . We’ve evolved this platform through three versions, adhering to RFC standards and embracing new design patterns to provide a clear and consistent interface. We’ve often heard that our REST API was an inspiration for other companies; countless tutorials refer to our endpoints. Today, we’re excited to announce our biggest change to the API since we snubbed XML in favor of JSON: we’re making the GitHub API available through GraphQL. . GraphQL is, at its core, a specification for a data querying language. We’d like to talk a bit about GraphQL, including the problems we believe it solves and the opportunities it provides to integrators. . You may be wondering why we chose to start supporting GraphQL. Our API was designed to be RESTful and hypermedia-driven. We’re fortunate to have  dozens of different open-source clients  written in a plethora of languages. Businesses grew around these endpoints. . Like most technology, REST is not perfect and has some drawbacks. Our ambition to change our API focused on solving two problems. . The first was scalability. The REST API is responsible for over 60% of the requests made to our database tier. This is partly because, by its nature, hypermedia navigation requires a client to repeatedly communicate with a server so that it can get all the information it needs. Our responses were bloated and filled with all sorts of  *_url  hints in the JSON responses to help people continue to navigate through the API to get what they needed. Despite all the information we provided, we heard from integrators that our REST API also wasn’t very flexible. It sometimes required two or three separate calls to assemble a complete view of a resource. It seemed like our responses simultaneously sent too much data  and  didn’t include data that consumers needed. . As we began to audit our endpoints in preparation for an APIv4, we encountered our second problem. We wanted to collect some meta-information about our endpoints. For example, we wanted to identify the OAuth scopes required for each endpoint. We wanted to be smarter about how our resources were paginated. We wanted assurances of type-safety for user-supplied parameters. We wanted to generate documentation from our code. We wanted to generate clients instead of manually supplying patches to  our Octokit suite . We studied a variety of API specifications built to make some of this easier, but we found that none of the standards totally matched our requirements. . And then we learned about GraphQL. .  GraphQL  is a querying language developed by Facebook over the course of several years. In essence, you construct your request by defining the resources you want. You send this via a  POST  to a server, and the response matches the format of your request. . For example, say you wanted to fetch just a few attributes off of a user. Your GraphQL query might look like this: . And the response back might look like this: . You can see that the keys and values in the JSON response match right up with the terms in the query string. . What if you wanted something more complicated? Let’s say you wanted to know how many repositories you’ve starred. You also want to get the names of your first three repositories, as well as their total number of stars, total number of forks, total number of watchers, and total number of open issues. That query might look like this: . The response from our API might be: . You just made  one  request to fetch all the data you wanted. . This type of design enables clients where smaller payload sizes are essential. For example, a mobile app could simplify its requests by only asking for the data it needs. This enables new possibilities and workflows that are freed from the limitations of downloading and parsing massive JSON blobs. . Query analysis is something that we’re also exploring with. Based on the resources that are requested, we can start providing more intelligent information to clients. For example, say you’ve made the following request: . Before executing the request, the GraphQL server notes that you’re trying to get the  email  field. If your client is misconfigured, a response back from our server might look like this: . This could be beneficial for users concerned about the OAuth scopes required by integrators. Insight into the scopes required could ensure that only the appropriate types are being requested. . There are several other features of GraphQL that we hope to make available to clients, such as: . In order to determine if GraphQL really was a technology we wanted to embrace, we formed a small team within the broader Platform organization and went looking for a feature on the site we wanted to build using GraphQL. We decided that implementing  emoji reactions on comments  was concise enough to try and port to GraphQL. Choosing a subset of the site to power with GraphQL required us to model a complete workflow and focus on building the new objects and types that defined our GraphQL schema. For example, we started by constructing a user in our schema, moved on to a repository, and then expanded to issues within a repository. Over time, we grew the schema to encapsulate all the actions necessary for modeling reactions. . We found implementing a GraphQL server to be very straightforward.  The Spec  is clearly written and succinctly describes the behaviors of various parts of a schema. GraphQL has a type system that forces the server to be unambiguous about requests it receives and responses it produces. You define a schema, describing the objects that represent your resources, fields on those objects, and the connections between various objects. For example, a  Repository  object has a non-null  String  field for the  name . A repository also has  watchers , which is a connection to another non-nullable object,  User . . Although the initial team exploring GraphQL worked mostly on the backend, we had several allies on the frontend who were also interested in GraphQL, and, specifically, moving parts of GitHub to use  Relay . They too were seeking better ways to access user data and present it more efficiently on the website. We began to work together to continue finding portions of the site that would be easy to communicate with via our nascent GraphQL schema. We decided to begin transforming some of our social features, such as the profile page, the stars counter, and the ability to watch repositories. These initial explorations paved the way to placing GraphQL in production. (That’s right! We’ve been running GraphQL in production for some time now.) As time went on, we began to get a bit more ambitious: we ported over some of the Git commit history pages to GraphQL and used  Scientist  to identify any potential discrepancies. . Drawing off our experiences in supporting the REST API, we worked quickly to implement our existing services to work with GraphQL. This included setting up logging requests and reporting exceptions, OAuth and AuthZ access, rate limiting, and providing helpful error responses. We tested our schema to ensure that every part of was documented and we wrote linters to ensure that our naming structure was standardized. . We work primarily in Ruby, and we were grateful for the existing gems supporting GraphQL. We used the  rmosolgo/graphql-ruby  gem to implement  the entirety  of our schema. We also incorporated the  Shopify/graphql-batch  gem to ensure that multiple records and relationships were fetched efficiently. . Our frontend and backend engineers were also able to contribute to these gems as we experimented with them. We’re thankful to the maintainers for their very quick work in accepting our patches. To that end, we’d like to humbly offer a couple of our own open source projects: . We’re going to continue to extract more parts of our system that we’ve developed internally and release them as open source software, such as our loaders that efficiently batch ActiveRecord requests. . The move to GraphQL marks a larger shift in our Platform strategy to be more transparent and more flexible. Over the next year, we’re going to keep iterating on our schema to bring it out of Early Access and into a wider production readiness. . Since our application engineers are using the same GraphQL platform that we’re making available to our integrators, this provides us with the opportunity to ship UI features  in conjunction with  API access. Our new Projects feature is a good example of this: the UI on the site is powered by GraphQL, and you can already use the feature programmatically. Using GraphQL on the frontend and backend eliminates the gap between what we release and what you can consume. We really look forward to making more of these simultaneous releases. . GraphQL represents a massive leap forward for API development. Type safety, introspection, generated documentation, and predictable responses benefit both the maintainers and consumers of our platform. We’re looking forward to our new era of a GraphQL-backed platform, and we hope that you do, too! . If you’d like to get started with GraphQL—including our new GraphQL Explorer that lets you make :sparkles:live queries:sparkles:, check out  our developer documentation ! .     .  gjtorikian   Platform Engineer   GitHub Profile |   Twitter Profile  .     .     .     .     .     .  Kyle Daigle   Platform Engineering Manager   GitHub Profile |   Twitter Profile  ", "date": "September 14, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\toctocatalog-diff: GitHub’s Puppet development and testing tool\t\t", "author": ["\n\t\tKevin Paulisse\t"], "link": "https://github.blog/2016-10-20-octocatalog-diff-github-s-puppet-development-and-testing-tool/", "abstract": " Today we are announcing the open source release of  octocatalog-diff : GitHub’s Puppet development and testing tool. . GitHub uses  Puppet  to configure the infrastructure that powers GitHub.com, comprised of hundreds of roles deployed on thousands of nodes. Each change to Puppet code must be validated to ensure not only that it serves the intended purpose for the role at hand, but also to avoid causing unexpected side effects on other roles. GitHub employs automated Continuous Integration testing and manual deployment testing for Puppet code changes, but it can be time-consuming to complete the manual deployment testing across hundreds of roles. . Recently, GitHub has been using an internally-developed tool called  octocatalog-diff  to help reduce the time required for these testing cycles. With this tool, developers are able to preview the effects of their change across all roles via a distributed “catalog difference” test that takes less than three minutes to run. Because of reduced testing cycles and increased confidence in their deployments, developers can iterate much faster on their Puppet code changes. . Before demonstrating  octocatalog-diff , let’s address the existing solutions and the reasoning for creating a new tool. . There are three main strategies for Puppet code testing in wide use, and GitHub uses all of them: .  Deployment testing — actually running the Puppet agent (possibly with  --noop  to preview actions without actually making changes) allows the developer to review log files or examine the system to see if the results are as intended.  .  Automated testing — this may include unit tests with   rspec-puppet  , acceptance tests with   beaker  , syntax checking  puppet parser validate  or linting with   puppet-lint  . These types of tests often run in a Continuous Integration environment to verify that the code meets a set of specified criteria.  .  Catalog testing —  octocatalog-diff  and Puppet’s   catalog_preview   module both allow comparison of catalogs produced by two different Puppet versions or between two environments.  . GitHub needed a catalog testing approach that could run from a development or CI environment, because for security reasons only a small number of engineers have direct access to the Puppet master. Because  catalog_preview  is designed to be fully integrated into the Puppet master, it would be inaccessible for a large portion of GitHub’s Puppet contributors, and as such it was not the right fit. Therefore, we embarked upon our own development of a tool that could run independently of a Puppet installation, and produced  octocatalog-diff . . This screen shot shows  octocatalog-diff  in action, as run from a developer’s workstation: . In this example, the developer is comparing the Puppet catalog changes between the master branch and the Puppet code in the current working directory. Two resources are being created (an Exec resource to create the mount point, and a Filesystem resource to format  /dev/xvdf ). Two resources are being removed (the old Exec resource to change permissions on the work directory, and the old Filesystem on  /dev/xvdb ). And one resource is being changed (several parameters of the mount point are being updated). . The output was generated in under 15 seconds, obviating a traditional workflow of committing code, waiting for CI jobs to pass, deploying code to a node, and reviewing the results. The process that generated this output did not require access to, or put any load on, either the Puppet master or the node whose catalog was computed. . The next graphic shows output from  octocatalog-diff  when run via a distributed CI job, to preview the effects of a code change on nodes across the fleet: . In this example, the developer wishes to see which systems will be affected by a particular change to the Puppet code. The output from  octocatalog-diff  reveals that the changes affect certain GitHub API nodes. The developer can use this information to test deployment on just those six representative systems instead of hundreds or thousands of nodes. This cuts down on unnecessary testing and provides confidence that there will not be unexpected side effects, allowing the developer to complete the work more efficiently and with less risk. .  octocatalog-diff  has several useful features: .  octocatalog-diff  is able to compare catalogs obtained in the following ways: .  octocatalog-diff  is being used in “catalog only” mode as a Continuous Integration (CI) job in GitHub’s Puppet repository. Upon every push,  octocatalog-diff  compiles the catalogs for over 50 critical roles, using real node names and facts, to ensure that changes to one role do not unexpectedly break Puppet catalogs for other roles. In addition, developers use  octocatalog-diff  in “difference” mode to preview their changes across the fleet, which has enabled them to perform major refactoring with minimal risk. . Over the past year, GitHub has successfully upgraded from Puppet 3.4 to 4.5, migrated hard-coded parameters from thousands of manifests into the  hiera  hierarchical data store, transitioned node classification from hostname regular expressions to application and roles, expanded roles to run in different environments and in containers, and upgraded roles to run under new operating systems. Using  octocatalog-diff  to predict changes across the fleet, a relatively small number of developers accomplished these substantial initiatives quickly and without their Puppet changes causing outages. .  octocatalog-diff  is  released  to the open source community  under the MIT license . . While we find  octocatalog-diff  to be reliable for our needs, there are undoubtedly configurations or customizations within others’ Puppet code bases that we have not anticipated. We welcome community participation and contributions, and look forward to enhancing the compatibility and functionality of the tool. . We acknowledge and thank the Site Reliability Engineering team at GitHub for their suggestions and code reviews, and the other engineers at GitHub who worked patiently with us to diagnose problems and test improvements during the pre-production stages. ", "date": "October 20, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing the GitHub Load Balancer\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-09-22-introducing-glb/", "abstract": " At GitHub we serve billions of HTTP, Git and SSH connections each day. To get the best performance we run on  bare metal hardware . Historically one of the more complex components has been our load balancing tier. Traditionally we scaled this vertically, running a small set of very large machines running  haproxy , and using a very specific hardware configuration allowing dedicated 10G link failover. Eventually we needed a solution that was scalable and we set out to create a load balancer solution that would run on commodity hardware in our typical data center configuration. . Over the last year we’ve developed our new load balancer, called GLB (GitHub Load Balancer). Today, and over the next few weeks, we will be sharing the design and releasing its components as open source software. . GitHub is growing and our monolithic, vertically scaled load balancer tier had met its match and a new approach was required. Our original design was based around a small number of large machines each with dedicated links to our network spine. This design tied networking gear, the load balancing hosts and load balancer configuration together in such a way that scaling horizontally was deemed too difficult. We set out to find a better way. . We first identified the goals of the new system, design pitfalls of the existing system and prior art that we could draw  experience  and  inspiration  from. After some time we determined that the following would produce a successful load balancing tier that we could maintain into the future: . To achieve these goals we needed to rethink the relationship between IP addresses and hosts, the constituent layers of our load balancing tier and how connections are routed, controlled and terminated. . In a typical setup, you assign a single public facing IP address to a single physical machine. DNS can then be used to split traffic over multiple IPs, letting you shard traffic across multiple servers. Unfortunately, DNS entries are cached fairly aggressively (often ignoring the TTL), and some of our users may specifically whitelist or hardcode IP addresses. Additionally, we offer a certain set of IPs for our Pages service which customers can use directly for their apex domain. Rather than relying on adding additional IPs to increase capacity, and having an IP address fail when the single server failed, we wanted a solution that would allow a single IP address to be served by multiple physical machines. . Routers have a feature called Equal-Cost Multi-Path (ECMP) routing, which is designed to split traffic destined for a single IP across multiple links of equal cost. ECMP works by hashing certain components of an incoming packet such as the source and destination IP addresses and ports. By using a consistent hash for this, subsequent packets that are part of the same TCP flow will hash to the same path, avoiding out of order packets and maintaining session affinity. . This works great for routing packets across multiple paths to the same physical destination server. Where it gets interesting is when you use ECMP to split traffic destined for a single IP across multiple physical servers, each of which terminate TCP connections but share no state, like in a load balancer. When one of these servers fails or is taken out of rotation and is removed from the ECMP server set a  rehash event occurs . 1/N connections will get reassigned to the remaining servers. Since these servers don’t share connection state these connections get terminated. Unfortunately, these connections may not be the same 1/N connections that were mapped to the failing server. Additionally, there is no way to gracefully remove a server for maintenance without also disrupting 1/N active connections. . A pattern that has been used by other projects is to split the load balancers into a L4 and L7 tier. At the L4 tier, the routers use ECMP to shard traffic using consistent hashing to a set of L4 load balancers – typically using software like  ipvs/LVS . LVS keeps connection state, and optionally syncs connection state with multicast to other L4 nodes, and forwards traffic to the L7 tier which runs software such as haproxy. We call the L4 tier “director” hosts since they direct traffic flow, and the L7 tier “proxy” hosts, since they proxy connections to backend servers. . This L4/L7 split has an interesting benefit: the proxy tier nodes can now be removed from rotation by gracefully draining existing connections, since the connection state on the director nodes will keep existing connections mapped to their existing proxy server, even after they are removed from rotation for new connections. Additionally, the proxy tier tends to be the one that requires more upkeep due to frequent configuration changes, upgrades and scaling so this works to our advantage. . If the multicast connection syncing is used, then the L4 load balancer nodes handle failure slightly more gracefully, since once a connection has been synced to the other L4 nodes, the connection will no longer be disrupted. Without connection syncing, providing the director nodes hash connections the same way and have the same backend set, connections may successfully continue over a director node failure. In practise, most installations of this tiered design just accept connection disruption under node failure or node maintenance. . Unfortunately, using LVS for the director tier has some significant drawbacks. Firstly, multicast was not something we wanted to support, so we would be relying on the nodes having the same view of the world, and having consistent hashing to the backend nodes. Without connection syncing, certain events, including planned maintenance of nodes, could cause connection disruption. Connection disruption is something we wanted to avoid due to how git cannot retry or resume if the connection is severed mid-flight. Finally, the fact that the director tier requires connection state at all adds an extra complexity to DDoS mitigation such as  synsanity  – to avoid resource exhaustion, syncookies would now need to be generated on the director nodes, despite the fact that the connections themselves are terminated on the proxy nodes. . We decided early on in the design of our load balancer that we wanted to improve on the common pattern for the director tier. We set out to design a new director tier that was stateless and allowed both director and proxy nodes to be gracefully removed from rotation without disruption to users wherever possible. Users live in countries with less than ideal internet connectivity, and it was important to us that long running clones of reasonably sized repositories would not fail during planned maintenance within a reasonable time limit. . The design we settled on, and now use in production, is a variant of  Rendezvous hashing  that supports constant time lookups. We start by storing each proxy host and assign a state. These states handle the connection draining aspect of our design goals and will be discussed further in a future post. We then generate a single, fixed-size forwarding table and fill each row with a set of proxy servers using the ordering component of Rendezvous hashing. This table, along with the proxy states, are sent to all director servers and kept in sync as proxies come and go. When a TCP packet arrives on the director, we hash the source IP to generate consistent index into the forwarding table. We then encapsulate the packet inside another IP packet (actually  Foo-over-UDP ) destined to the internal IP of the proxy server, and send it over the network. The proxy server receives the encapsulated packet, decapsulates it, and processes the original packet locally. Any outgoing packets use Direct Server Return, meaning packets destined to the client egress directly to the client, completely bypassing the director tier. . Now that you have a taste of the system that processed and routed the request to this blog post we hope you stay tuned for future posts describing our director design in depth, improving haproxy hot configuration reloads and how we managed to migrate to the new system without anyone noticing. .  Joe Williams   Senior Infrastructure Engineer   GitHub Profile |   Twitter Profile |   Blog  .     .  Theo Julienne   Senior Production Engineer   GitHub Profile  ", "date": "September 22, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tIncident Report: Inadvertent Private Repository Disclosure\t\t", "author": ["\n\t\tTodd Berman\t"], "link": "https://github.blog/2016-10-28-incident-report-inadvertent-private-repository-disclosure/", "abstract": " On Thursday, October 20th, a bug in GitHub’s system exposed a small amount of user data via Git pulls and clones. In total, 156 private repositories of GitHub.com users were affected (including one of GitHub’s).  We have notified everyone affected by this private repository disclosure, so if you have not heard from us, your repositories were not impacted and there is no ongoing risk to your information.  . This was not an attack, and no one was able to retrieve vulnerable data intentionally. There was no outsider involved in exposing this data; this was a programming error that resulted in a small number of Git requests retrieving data from the wrong repositories. . Regardless of whether or not this incident impacted you specifically, we want to sincerely apologize. It’s our responsibility not only to keep your information safe but also to protect the trust you have placed in us. GitHub would not exist without your trust, and we are deeply sorry that this incident occurred. . Below is the technical analysis of our investigation, including a high-level overview of the incident, how we mitigated it, and the specific measures we are taking to safeguard against incidents like this from happening in the future. . In order to speed up unicorn worker boot times, and simplify the post-fork boot code, we applied the following buggy patch: .   . The database connections in our rails application are split into three pools: a read-only group, a group used by Spokes (our distributed Git back-end), and the normal Active Record connection pool. The read-only group and the Spokes group are managed manually, by our own connection handling code. This meant the pool was shared between all child processes of the rails application when running using the change. The new line of code disconnected only  ConnectionPool  objects that are managed by Active Record, whereas the previous snippet would disconnect all  ConnectionPool  objects held in memory. . The impact of this bug for most queries was a malformed response, which errored and caused a near immediate rollback. However, a very small percentage of the queries responses were interpreted as legitimate data in the form of the file server and disk path where repository data was stored. Some repository requests were routed to the location of another repository. The application could not differentiate these incorrect query results from legitimate ones, and as a result, users received data that they were not meant to receive. . When properly functioning, the system works as sketched out roughly below. However, during this failure window, the MySQL response in step 4 was returning malformed data that would end up causing the git proxy to return data from the wrong file server and path. .   . Our analysis of the ten-minute window in question uncovered: . After establishing the effects of the bug, we set out to determine which requests were affected in this way for the duration of the deploy. Normally, this would be an easy task, as we have an in-house monitor for Git that logs every repository access. However, those logs contained some of the same faulty data that led to the misrouted requests in the first place. Without accurate usernames or repository names in our primary Git logs, we had to turn to data that our git proxy and git-daemon processes sent to syslog. In short, the goal was to join records from the proxy, to git-daemon, to our primary Git logging, drawing whatever data was accurate from each source. Correlating records across servers and data sources is a challenge because the timestamps differ depending on load, latency, and clock skew. In addition, a given Git request may be rejected at the proxy or by git-daemon before it reaches Git, leaving records in the proxy logs that don’t correlate with any records in the git-daemon or Git logs. . Ultimately, we joined the data from the proxy to our Git logging system using timestamps, client IPs, and the number of bytes transferred and then to git-daemon logs using only timestamps. In cases where a record in one log could join several records in another log, we considered all and took the worst-case choice. We were able to identify cases where the repository a user requested, which was recorded correctly at our git proxy, did not match the repository actually sent, which was recorded correctly by git-daemon. . We further examined the number of bytes sent for a given request. In many cases where incorrect data was sent, the number of bytes was far larger than the on-disk size of the repository that was requested but instead closely matched the size of the repository that was sent. This gave us further confidence that indeed some repositories were disclosed in full to the wrong users. . Although we saw over 100 misrouted fetches and clones, we saw no misrouted pushes, signaling that the integrity of the data was unaffected. This is because a Git push operation takes place in two steps: first, the user uploads a pack file containing files and commits. Then we update the repository’s refs (branch tips) to point to commits in the uploaded pack file. These steps look like a single operation from the user’s point of view, but within our infrastructure, they are distinct. To corrupt a Git push, we would have to misroute both steps to the same place. If only the pack file is misrouted, then no refs will point to it, and git fetch operations will not fetch it. If only the refs update is misrouted, it won’t have any pack file to point to and will fail. In fact, we saw two pack files misrouted during the incident. They were written to a temporary directory in the wrong repositories. However, because the refs-update step wasn’t routed to the same incorrect repository, the stray pack files were never visible to the user and were cleaned up (i.e., deleted) automatically the next time those repositories performed a “git gc” garbage-collection operation. So no permanent or user-visible effect arose from any misrouted push. . A misrouted Git pull or clone operation consists of several steps. First, the user connects to one of our Git proxies, via either SSH or HTTPS (we also support git-protocol connections, but no private data was disclosed that way). The user’s Git client requests a specific repository and provides credentials, an SSH key or an account password, to the Git proxy. The Git proxy checks the user’s credentials and confirms that the user has the ability to read the repository he or she has requested. At this point, if the Git proxy gets an unexpected response from its MySQL connection, the authentication (which user is it?) or authorization (what can they access?) check will simply fail and return an error. Many users were told during the incident that their repository access “was disabled due to excessive resource use.” . In the operations that disclosed repository data, the authentication and authorization step succeeded. Next, the Git proxy performs a routing query to see which file server the requested repository is on, and what its file system path on that server will be. This is the step where incorrect results from MySQL led to repository disclosures. In a small fraction of cases, two or more routing queries ran on the same Git proxy at the same time and received incorrect results. When that happened, the Git proxy got a file server and path intended for another request coming through that same proxy. The request ended up routed to an intact location for the wrong repository.  Further, the information that was logged on the repository access was a mix of information from the repository the user requested and the repository the user actually got. These corrupted logs significantly hampered efforts to discover the extent of the disclosures. . Once the Git proxy got the wrong route, it forwarded the user’s request to git-daemon and ultimately Git, running in the directory for someone else’s repository. If the user was retrieving a specific branch, it generally did not exist, and the pull failed. But if the user was pulling or cloning all branches, that is what they received: all the commits and file objects reachable from all branches in the wrong repository. The user (or more often, their build server) might have been expecting to download one day’s commits and instead received some other repository’s entire history. . Users who inadvertently fetched the entire history of some other repository, surprisingly, may not even have noticed. A subsequent “git pull” would almost certainly have been routed to the right place and would have corrected any overwritten branches in the user’s working copy of their Git repository. The unwanted remote references and tags are still there, though. Such a user can delete the remote references, run “git remote prune origin,” and manually delete all the unwanted tags. As a possibly simpler alternative, a user with unwanted repository data can delete that whole copy of the repository and “git clone” it again. . To prevent this from happening again, we will modify the database driver to detect and only interpret responses that match the packet IDs sent by the database. On the application side, we will consolidate the connection pool management so that Active Record’s connection pooling will manage all connections. We are following this up by upgrading the application to a newer version of Rails that doesn’t suffer from the “connection reuse” problem. . We will continue to analyze the events surrounding this incident and use our investigation to improve the systems and processes that power GitHub. We consider the unauthorized exposure of even a single private repository to be a serious failure, and we sincerely apologize that this incident occurred. ", "date": "October 28, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tGLB part 2: HAProxy zero-downtime, zero-delay reloads with multibinder\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-12-01-glb-part-2-haproxy-zero-downtime-zero-delay-reloads-with-multibinder/", "abstract": " Recently we  introduced GLB , the GitHub Load Balancer that powers GitHub.com. The GLB proxy tier, which handles TCP connection and TLS termination is powered by  HAProxy , a reliable and high performance TCP and HTTP proxy daemon. As part of the design of GLB, we set out to solve a few of the common issues found when using HAProxy at scale. . Prior to GLB, each host ran a single monolithic instance of HAProxy for all our public services, with frontends for each external IP set, and backends for each backing service. With the number of services we run, this became unwieldy, our configuration was over one thousand lines long with many interdependent ACLs and no modularization. Migrating to GLB we decided to split the configuration per-service and support running multiple isolated load balancer instances on a single machine. Additionally, we wanted to be able to update a single HAProxy configuration easily without any downtime, additional latency on connections or disrupting any other HAProxy instance on the host. Today we are releasing our solution to this problem,  multibinder . . HAProxy uses the SO_REUSEPORT socket option, which allows multiple processes to create LISTEN sockets on the same IP/port combination. The Linux kernel then balances new connections between all available LISTEN sockets. In this diagram, we see the initial stage of an HAProxy reload starting with a single process (left) and then causing a second process to start (right) which binds to the same IP and port, but with a different socket: . This works great so far, until the original process terminates. HAProxy sends a signal to the original process stating that the new process is now  accept() ing and handling connections (left), which causes it to stop accepting new connections and close its own socket before eventually exiting once all connections complete (right): . Unfortunately there’s a small period between when this process last calls  accept()  and when it calls  close()  where the kernel will still route some new connections to the original socket. The code then blindly continues to close the socket, and all connections that were queued up in that LISTEN socket get discarded (because  accept()  is never called for them): . For small scale sites, the chance of a new connection arriving in the few microseconds between these calls is very low. Unfortunately at the scale we run HAProxy, a customer impacting number of connections would hit this issue each and every time we reload HAProxy. Previously we used the official solution offered by HAProxy, dropping SYN packets during this small window, causing the client to retry the SYN packet shortly afterwards. Other  potential solutions  to the same problem include using  tc qdisc  to stall the SYN packets as they come in, and then un-stall the queue once the reload is complete. During development of GLB, we weren’t satisfied with either solution and sought out one without any queue delays and sharing of the same LISTEN socket. . The way other services typically support zero-downtime reloads is to share a LISTEN socket, usually by having a parent process that holds the socket open and  fork() s the service when it needs to reload, leaving the socket open for the new process to consume. This creates a slightly different situation, where the kernel has a single LISTEN socket and clients are queued for  accept()  by either process. The file descriptors in each process may be different, but they will point to the same in-kernel socket structure. . In this scenario, a new process would be started that inherits the same LISTEN socket (left), and when the original pid stops calling  accept() , connections remain queued for the new process to process because the kernel LISTEN socket and queue are shared (right): . Unfortunately, HAProxy doesn’t support this method directly. We considered patching HAProxy to add built-in support but found that the architecture of HAProxy favours process isolation and non-dynamic configuration, making it a non-trivial architectural change. Instead, we created  multibinder  to solve this problem generically for any daemon that needs zero-downtime reload capabilities, and integrated it with HAProxy by using a few tricks with existing HAProxy configuration directives to get the same result. . Multibinder is similar to other file-descriptor sharing services such as  einhorn , except that it runs as an isolated service and process tree on the system, managed by your usual process manager. The actual service, in this case HAProxy, runs separately as another service, rather than as a child process. When HAProxy is started, a small wrapper script calls out to multibinder and requests the existing LISTEN socket to be sent using  Ancillary Data  over an UNIX Domain Socket. The flow looks something like the following: . Once the socket is provided to the HAProxy wrapper, it leaves the LISTEN socket in the file descriptor table and writes out the HAProxy configuration file from an ERB template, injecting the file descriptors using  file descriptor binds  like  fd@N  (where N is the file descriptor received from multibinder), then calls  exec()  to launch HAProxy which uses the provided file descriptor rather than creating a new socket, thus inheriting the same LISTEN socket. From here, we get the ideal setup where the original HAProxy process can stop calling  accept()  and connections simply queue up for the new process to handle. . Along with the release of multibinder, we’re also providing examples of  running multiple HAProxy instances with multibinder  leveraging systemd service templates. Following these instructions you can launch a set of HAProxy servers using separate configuration files, each using the same system-wide multibinder instance to request their binds and having true zero-downtime, zero-delay reloads. .     .  Joe Williams   Senior Infrastructure Engineer   GitHub Profile |   Twitter Profile |   Blog  .     .  Theo Julienne   Senior Production Engineer   GitHub Profile  ", "date": "December 1, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we made diff pages three times faster\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-12-06-how-we-made-diff-pages-3x-faster/", "abstract": " We serve a lot of diffs here at GitHub. Because it is computationally expensive to generate and display a diff, we’ve traditionally had to apply  some very conservative limits on what gets loaded. We knew we could do better, and we set out to do so. . Before this change, we fetched diffs by asking Git for the diff between two commit objects. We would then parse the output, checking it against the various limits we had in place. At the time they were as follows: . These limits were in place to both prevent excessive load on the file servers, as well as prevent the browser’s DOM from growing too large and making the web page less responsive. . In practice, our limits did a pretty good job of protecting our servers and users’ web browsers from being overloaded. But because these limits were applied in the order Git handed us back the diff text, it was possible for a diff to be truncated before we reached the interesting parts. Unfortunately, users had to fall back to command-line tools to see their changes in these cases. . Finally, we had timeouts happening far more frequently than we liked. Regardless of the size of the requested diff, we shouldn’t force the user to wait up to eight seconds before responding, and even then occasionally with an error message. . Our main goal was to improve the user experience around (re)viewing diffs on GitHub: . To achieve the aforementioned goals, we had to come up with a new and better approach to handling large diffs. We wanted a solution that would allow us to get a high-level overview of all changes in a diff, and then load the patch texts for the individual changed files “progressively”. These discrete sections could later be assembled by the user’s browser. . But to achieve this without disrupting the user experience, our new solution also needed to be flexible enough to load and display diffs identically to how we were doing it in production to date. We wanted to verify accuracy and monitor any performance impact by running the old and new diff loading strategies in production, side-by-side, before changing to the new progressive loading strategy. . Lucky for us, Git provides an excellent plumbing command called  git-diff-tree . .  git-diff-tree  is a low-level (plumbing)  git  command that can be used to compare the contents of two tree objects and output the comparison result in different ways. . The default output format is  --raw , which prints a list of changed files: . Using  git-diff-tree --raw  we could determine what changed at a high level very quickly, without the overhead of generating patch text. We could then later paginate through this list of changes, or “deltas”, and load the exact patch data for each “page” by specifying a subset of the deltas’ paths to  git-diff-tree --patch . . To better understand the obvious performance overhead of calling two git commands instead of one, and to ensure that we wouldn’t cause any regressions in the returned data, we initially focused on generating the same output as a plain call to  git-diff-tree --patch , by calling  git-diff-tree --raw  and then feeding all returned paths back into  git-diff-tree --patch . . We started a  Scientist  experiment which ran both algorithms in parallel, comparing accuracy and timing. This gave us detailed information on cases where results were not as expected, and allowed us to keep an eye on performance. . As expected, our new algorithm, which was replacing something that hadn’t been materially refactored in years, had many mismatches and performance was worse than before. . Most of the issues that we found were simply unexpected behaviors of the old code under certain conditions. We meticulously emulated these corner cases, until we were left only with mismatches related to rename detection in  git diff . . Loading the patch text from a set of deltas sounds like it should have been a pretty straightforward operation. We had the list of paths that changed, and just needed to look up the patch texts for these paths. What could possibly go wrong? . In our first attempt we loaded the diffs by passing the first 300 paths from our deltas to  git-diff-tree --patch . This emulated our existing behaviour, – and we unexpectedly ran into rare mismatches. Curiously, these mismatches were all related to renames, but only when multiple files containing the same or very similar contents got renamed in the same diff. . This happened because rename detection in git is based on the contents of the tree that it is operating on, and by looking at only a subset of the original tree, git’s rename detection was failing to match renames as expected. . To preserve the rename associations from the initial  git-diff-tree --raw  run,  @peff  added a  git-diff-pairs  command to our fork of  Git. Provided a set of blob object IDs (provided by the deltas) it returns the corresponding diff text, exactly what we needed. . On a high level, the process for generating a diff in Git is as follows: . Do a tree-wide diff, generating modified pairs, or added/deleted paths (which  are just considered pairs with a null before/after state). . Run various algorithms on the whole set of pairs, like rename detection. This  is just linking up adds and deletes of similar content. . For each pair, output it in the appropriate format (we’re interested in   --patch , obviously). .  git-diff-pairs  lets you take the output from step 2, and feed it individually into step 3. . With this new function in place, we were finally able to get our performance and accuracy to a point where we could transparently switch to this new diff method without negative user impact. . If you’re interested in viewing or contributing to the source for  git-diff-pairs  we submitted it upstream  here . . GitHub displays line change statistics for both the entire diff and each delta. Generating the line change statistics for a diff can be a very  costly operation, depending on the size and contents of the diff. However, it is very useful to have summary statistics on a diff at a glance so that the user can have a good overview of the changes involved. . Historically we counted the changes in the patch text as we processed it so that only one diff operation would need to run to display a diff. This operation and its results were cached so performance was optimal. However, in the case of truncated diffs there were changes that were never seen and therefore not included in these statistics. This was done to give us better performance at the cost of slightly inaccurate total counts for large diffs. . With our move to progressive diffs, it would become increasingly likely that we would only ever be looking at a part of the diff at any one time so the counts would be inaccurate most of the time instead of rarely. . To address this problem we decided to collect the statistics for the entire diff using  git-diff-tree --numstat --shortstat . This would not only solve the problem of dealing with partial diffs, but also make the counts accurate in cases where they would have been incorrect before. . The downside of this change is that Git was now potentially running the entire diff twice. We determined this was acceptable, however as the remaining diff processing for presentation was far more resource intensive. Also, with progressive diffs, it was entirely probable that many larger diffs would never have the second pass since those deltas might never be loaded anyway. . Due to the nature of how  git-diff-tree  works, we were even able to combine the call for these statistics with the call for deltas into a single command, to further improve performance. This is because Git already needed to perform a full diff in order to determine what the statistics were, so having it also print the tree diff information is essentially free. . For the initial request of a page containing a diff, we first fetched the deltas along with the diff statistics. Next we fetched as much diff text as we could, but with significantly reduced limits compared to before. . To determine optimal limits, we turned to some of our copious internal metrics. We wanted results as quickly as possible, but we also wanted a solution which would display the full diff in “most” cases. Some of the information our metrics revealed was: . From these, it was clear a great number of diffs only involved a handful of changes. If we set our new limits with these metrics in mind, we could continue to be very fast in most cases while significantly improving performance in previously slow or inaccessible diffs. . In the end, we settled on the following for the initial request for a diff page: . This allowed the initial request for a large diff to be  much  faster, and the rest of the diff to automatically load after the first batch of patches was already rendered. . After one of the limits on patch text was reached during asynchronous batch loading, we simply render the deltas without their diff text and a “load diff” button to retrieve the patch as needed. . Overall, the effective limits we enforce for the  entire  diff became: . With these changes, you got more of the diff you needed in less time than ever before. Of course, viewing a 60,000,000 line diff would require the user to press the “load diff” button more than a couple thousand times. . The benefits to this approach were a clear win. The number of diff timeouts dropped almost immediately. . Additionally, the higher percentile performance of our main diffs pages improved by nearly 3x! . Our diff pages pages were traditionally among our worst performing, so the performance win was even noticeable on our high percentile graph for overall requests’ performance  across the entire site , shaving off around 3.5s from the 99.9th percentile: . This new approach opens the door to new types of optimizations and interface ideas that weren’t possible before. We’ll be continuing to improve how we fetch and render diffs, making them more useful and responsive. .     .  Brian Lopez   Application Engineering Manager   GitHub Profile |   Twitter Profile  .     .     .     .  Arthur Schreiber   Application Engineer   GitHub Profile |   Twitter Profile  ", "date": "December 6, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub’s Metal Cloud\t\t", "author": ["\n\t\tLee Reilly\t"], "link": "https://github.blog/2015-12-01-githubs-metal-cloud/", "abstract": " At GitHub we place an emphasis on stability, availability, and performance. A large component of ensuring we excel in these areas is deploying services on bare-metal hardware. This allows us to tailor hardware configurations to our specific needs, guarantee a certain performance profile, and own the availability of our systems from end to end. . Of course, operating our own data centers and managing the hardware that’s deployed there introduces its own set of complications. We’re now tasked with tracking, managing, and provisioning physical pieces of hardware — work that is completely eliminated in a cloud computing environment. We also need to retain the benefits that we’ve all come to know and love in cloud environments: on-demand compute resources that are a single API call away. . Enter gPanel, our physical infrastructure management application. .   . gPanel is a Ruby on Rails application that we started developing over three years ago as we were transitioning from a managed environment to our own data center space. It was identified early on that we’d need the ability to track physical components of our new space; cabinets, PDUs, chassis, switches, and loose pieces of hardware. With this in mind, we set out building the application. . As we started transitioning hosts and services to our own data center, we quickly realized we’d also need an efficient process for installing and configuring operating systems on this new hardware. This process should be completely automated, allowing us to make it accessible to the entire company. Without this, specific knowledge about our new environment would be required to spin up new hosts, which leaves the very large task of a complete data center migration exclusively in the hands of our small Operations team. . Since we’d already elected to have gPanel act as the source of truth for our data center, we determined it should be responsible for server provisioning as well. . The system we ended up with is overall pretty straight-forward and simple — goals for any of our new systems or software. We utilize a few key pieces to drive the entire process. . Our hardware vendor configures machines to PXE boot from the network before they arrive at our data center. Machines are racked, connected to our network, and powered on. From there, our DHCP/PXE server tells the machines to  chainload iPXE and then contact gPanel for further instructions . gPanel can identify the server (or determine that it’s brand new) with the serial number that’s passed as a parameter in the iPXE request. . gPanel defines a number of states that chassis are in. This state is passed to our Ubuntu PXE image via kernel parameters so it can determine which action to take. These actions are driven by a simple set of bash scripts that we include in our Ubuntu image. . The initial state is  unknown  where we simply collect data about the machine and record it in gPanel. This is accomplished using Facter for gathering system information, exporting it as JSON, and then POSTing it to gPanel’s API. gPanel has a number of jobs that process this JSON and create the appropriate records. We try to model as much as possible in the application; CPUs, DIMMs, RAID cards, drives, NICs, and more are all separate records in the database. This allows us to track parts as they’re replaced, moved to a different machine, or removed entirely. . Once we’ve gathered all the information we need about the machine, we enter  configuring , where we assign a static IP address to the IPMI interface and tweak our BIOS settings. From there we move to  firmware_upgrade  where we update FCB, BMC, BIOS, RAID, and any other firmware we’d like to manage on the system. . At this point we consider the initial hardware configuration complete and will begin the burn-in process. Our burn-in process consists of two states in gPanel;  breakin  and  memtesting .  breakin  uses a suite from  Advanced Clustering  to exercise the hardware and detect any problems. We’ve added a script that POSTs updates to gPanel throughout this process so it can determine whether we have failures or not. If a failure is detected, the chassis is moved to our  failed  state where it sits until we have a chance to review the logs and replace the bad component. If the chassis passes  breakin , we’ll move on to  memtesting . . In  memtesting  we boot a custom  MemTest86  image and monitor it while it completes a full pass. Our custom version of MemTest86 changes the color of the failure message to red which allows us to detect trouble. We’ve hacked together a Ruby script that retrieves a console screenshot via IPMI and checks the color in the image to determine if we’ve hit a failure or not. Again, if a failure is detected, we’ll transition the chassis to  failed , otherwise it moves on to  ready . . The  ready  state is where our available pool of machines will sit until someone comes along and brings it into production. . Once machines have completed the burn-in process and deemed ready for production service, a user can instruct gPanel to install an operating system. Like the majority of our tooling, this is driven via  Hubot, our programmable chat bot . . First, the user will need to determine which chassis they’d like to perform the installation on. . Once the chassis is selected, you can initiate the installation. . If the user needs a different RAID configuration, or to have the host brought up on a different Puppet branch, they can specify those with the install command as well. . If we’re looking to spin up a number of hosts to expand capacity for a certain service tier, we can instruct gPanel to do this with our  bulk-install  command. This command takes  app ,  role ,  chassis_type , and  count  parameters, selects the appropriate hardware from our  ready  pool, and initiates the installations. . At this point gPanel will transition the chassis to our  installing  state and reboot the machine via IPMI. In this state we PXE boot the Ubuntu installer and retrieve a  preseed configuration  from gPanel. This configuration is rendered dynamically based on the hardware configuration and the options the user provided in their install command. Once the installation is complete, we move to the  installed  state where gPanel will instruct machines to boot from their local disk. . When we’re ready to decommission a host we simply tell Hubot, who will ask for confirmation in the form of a “magic word”. . gPanel transitions the chassis back to our  ready  state and makes it available again for future installations. . We’ve been pleased with the ease at which we’re able to bring new hardware into the data center and make it available to the rest of the company. We continue to find room for improvement and are constantly working to further automate the procurement and provisioning process. ", "date": "December 1, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tUpdate on 1/28 service outage\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2016-01-29-update-on-1-28-service-outage/", "abstract": " On Thursday, January 28, 2016 at 00:23am UTC, we experienced a severe service outage that impacted GitHub.com. We know that any disruption in our service can impact your development workflow, and are truly sorry for the outage. While our engineers are investigating the full scope of the incident, I wanted to quickly share an update on the situation with you. . A brief power disruption at our primary data center caused a cascading failure that impacted several services critical to GitHub.com’s operation. While we worked to recover service, GitHub.com was unavailable for two hours and six minutes. Service was fully restored at 02:29am UTC. Last night we completed the final procedure to fully restore our power infrastructure. . Millions of people and businesses depend on GitHub. We know that our community feels the effects of our site going down deeply. We’re actively taking measures to improve our resilience and response time, and will share details from these investigations. ", "date": "January 29, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tMove Fast and Fix Things\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2015-12-15-move-fast/", "abstract": " Anyone who has worked on a large enough codebase knows that technical debt is an inescapable reality: The more rapidly an application grows in size and complexity, the more technical debt is accrued. With GitHub’s growth over the last 7 years, we have found plenty of nooks and crannies in our codebase that are inevitably below our very best engineering standards. But we’ve also found effective and efficient ways of paying down that technical debt, even in the most active parts of our systems. . At GitHub we try not to brag about the “shortcuts” we’ve taken over the years to scale our web application to more than 12 million users. In fact, we do quite the opposite: we make a conscious effort to study our codebase looking for systems that can be rewritten to be cleaner, simpler and more efficient, and we develop tools and workflows that allow us to perform these rewrites efficiently and reliably. . As an example, two weeks ago we replaced one of the most critical code paths in our infrastructure: the code that performs merges when you press the Merge Button in a Pull Request. Although we routinely perform these kind of refactorings throughout our web app, the importance of the merge code makes it  an interesting story to demonstrate our workflow. . We’ve  talked at length in the past  about the storage model that GitHub uses for repositories in our platform and our Enterprise offerings. There are many implementation details that make this model efficient in both performance and disk usage, but the most relevant one here is the fact that repositories are always stored  “bare” . . This means that the actual files in the repository (the ones that you would see on your working directory when you clone the repository) are not actually available on disk in our infrastructure: they are compressed and delta’ed inside  packfiles . . Because of this, performing a merge in a production environment is a nontrivial endeavour. Git knows several  merge strategies , but the recursive merge strategy that you’d get by default when using  git merge  to merge two branches in a local repository assumes the existence of a working tree for the repository, with all the files checked out on it. . The workaround we developed in the early days of GitHub for this limitation is effective, but not particularly elegant: instead of using the default   git-merge-recursive   strategy, we wrote our own merge strategy based on the original one that Git used back in the day:   git-merge-resolve  . With some tweaking, the old strategy can be adapted to not require an actual checkout of the files on disk. . To accomplish this, we wrote a shell script that sets up a  temporary working directory , in which the merge engine performs content-level merges. Once these merges are complete, the files are written back into the original repository, together with the resulting trees. . The core of this merge helper looks like this: . This merges two trees in a  bare  repository rather effectively, but it has several shortcomings: . It creates temporary directories on disk; hence, it needs to clean up after  itself. . It creates a temporary index on disk, which also requires cleanup. . It is not particularly fast, despite Git’s highly optimized merge engine:  the old  git-merge-one-file  script spawns several processes for each file  that needs to be merged. The need to use disk as temporary scratch space  also acts as a bottleneck. . It doesn’t have the exact same behavior as  git merge  in a Git  client, because we’re using an outdated merge strategy (as opposed to the  recursive merge strategy that  git merge  now performs by default). .  libgit2  is the sharpest weapon we own to fight any Git-related technical debt in our platform. Building a web service around Git is notoriously hard, because the tooling of the Core Git project has been designed around local command-line usage, and less thought has been put on the use case of running Git operations on the server-side. . With these limitations in mind, five years ago we began the development of libgit2, a re-implementation of most of Git’s low level APIs as a library that could be linked and used directly within a server-side process. I personally led the development of libgit2 for the first 3 years of its existence, although right now the library is in the very capable hands of  Carlos Martín  and  Ed Thomson . It has also gained a lot of traction, with a solid stream of external contributions from other companies that also use it to build  Git infrastructure — particularly our friends at Microsoft, who use it to implement all the Git functionality in Visual Studio. . Despite being a C library, libgit2 contains many powerful abstractions to accomplish complex tasks that Git simply cannot do. One of these features are indexes that exist solely in memory and allow work-tree related operations to be performed without an actual working directory. Based on this abstraction, Ed Thomson  implemented an incredibly elegant merge engine  as one of his first contributions to the library. . With the in-memory index, libgit2 is capable of merging two trees in a repository without having to check out any of their files on disk. On paper, this implementation is  ideal  for our use case: it is faster, simpler and more efficient than what we’re currently doing with Git. But it is also a  serious  amount of new code that ought to replace some of the most critical functionality of our website. . In this specific case, although I had thoroughly reviewed the merge implementation as libgit2’s maintainer, it would be reckless to assume it to be production ready. In fact, it would be even more reckless if I had written the whole implementation myself. The merge process is incredibly complex, in both the old and the new implementations, and GitHub operates at a scale where seemingly obscure “corner cases”  will always happen  by default. When it comes to user’s data, ignoring corner cases is not an option. . To make matters worse, this is not a new feature; this is a replacement for an existing feature which worked with no major issues (besides the technical debt and poor performance characteristics). There is no room for bugs or performance regressions. The switch-over needs to be flawless and —just as importantly— it needs to be efficient. . Efficiency is fundamental in these kind of projects because even with the performance improvements they entail, it becomes hard to justify the time investment if we cannot wrap up the refactoring in a tight timeframe. . Without a clear deadline and a well defined workflow, it’s easy to waste weeks of work rewriting code that will end up being buggier and less reliable than the old implementation. To prevent this, and to make these rewrites sustainable, we need to be able to perform them methodically in a virtually flawless fashion. This is a hard problem to tackle. . Fortunately, this challenge is not unique to the systems code in GitHub. All of our engineering teams are just as concerned with code quality issues as we are, and for rewrites that interface with our main Rails app (like in this specific example, the PR merge functionality), our Core Application team has built extensive tooling that makes this extremely complicated process tenable. . The first step of the rollout process of the new implementation was to implement the RPC calls for the new functionality. All the Git-related operations in our platform happen through a service called GitRPC, which intelligently routes the request from the frontend servers and performs the operation on the fileserver where the corresponding repository is stored. . For the old implementation, we were simply using a generic  git_spawn  RPC call, which runs the corresponding Git command (in this case, the script for our custom merge strategy) and returns the  stdout ,  stderr  and exit code to the frontend. The fact that we were using a generic spawn, instead of a specialized RPC call that performed the whole operation on the fileserver and returned the result, was another sign of technical debt: our main application required its own logic to parse and understand the results of a merge. . Hence, for our new implementation, we wrote a specific  create_merge_commit  call, which would perform the merge operation in-memory and in-process on the fileserver side. The RPC service running on our fileservers is written in Ruby, just like our main Rails application, so all the libgit2 operations are actually performed using  Rugged , the Ruby bindings for libgit2 which we develop in house. . Thanks to the design of Rugged, which turns libgit2’s low level APIs into usable interfaces in Ruby-land, writing the merge implementation is straightforward: . Although GitRPC runs as a separate service on our fileservers, its source code is part of our main repository, and it is usually deployed in lockstep with the main Rails application in the frontends. Before we could start switching over the implementations, we merged and deployed the new RPC call to production, even though it was not being used anywhere yet. . We refactored the main path for merge commit creation in the Rails app to extract the Git-specific functionality into its own method. Then we implemented a second method with the same signature as the Git-based code, but this one performing the merge commit through Rugged/libgit2. . Since deploying to production at GitHub is  extremely straightforward  (we perform about 60 deploys of our main application every day), I deployed the initial refactoring right away. That way we needn’t worry about a potential gap in our extensive test suite: if the trivial refactoring introduced any issues in the existing behavior, they would be quickly spotted by deploying to a small percentage of the machines serving production traffic. Once I deemed the refactoring safe, it was fully deployed to all the frontend machines and merged into the mainline. . Once we have the two code paths ready in our main application and in the RPC server, we can let the magic begin: we will be testing  and  benchmarking the two implementations, in production but without affecting in any way our existing users, thanks to the power of Scientist. .  Scientist  is a library for testing refactorings, rewrites and performance improvements in Ruby. It was originally part of our Rails application, but we’ve extracted and  open-sourced it . To start our testing with Scientist, we declare an experiment and set the old code path as a control sample and the new code path as a candidate. . Once this trivial change is in place, we can safely deploy it to production machines. Since the experiment has not been enabled yet, the code will work just as it did before: the control (old code path) will be executed and its result returned directly to the user. But with the experiment code deployed in production, we’re one button click away from making SCIENCE! happen. . The first thing we do when starting an experiment is enable it for a tiny fraction (1%) of all the requests. When an experiment “runs” in a request, Scientist does many things behind the scenes: it runs the control and the candidate (randomizing the order in which they run to prevent masking bugs or performance regressions), stores the result of the control  and  returns it to the user, stores the result of the candidate and swallows any possible exceptions (to ensure that bugs or crashes in the new code cannot possibly impact the user), and compares the result of the control against the candidate, logging exceptions or mismatches in our Scientist web UI. . The core concept of Scientist experiments is that even if the  new  code path crashes, or gives a bad result, this will not affect users, because they always receive the result of the  old  code. . With a fraction of all requests running the experiment, we start getting valuable data. Running an experiment at 1% is useful to catch the obvious bugs and crashes in the new code: we can visualize the performance graph to see if we’re heading in a good direction, perf-wise, and we can eyeball the mismatches graph to see if the new code is more-or-less doing the right thing. Once these problems are solved, we increase the frequency of experiments to start catching the actual corner cases. .   . Shortly after the deploy, the accuracy graph shows that experiments are  being run at the right frequency and that mismatches are very few. .   . Graphing the errors/mismatches on their own shows their frequency in more  detail. Our tooling captures metadata all these mismatches so they can be  analyzed later. .   . Although it’s too early to tell, shortly after the initial deploy the  performance characteristics look extremely promising. The vertical axis is  time, in milliseonds. Percentiles for the old and new code are shown in blue  and green, respectively. . The main thing we noticed looking at experiment results is that a majority of mismatches came from the old code timing out whilst the new code succeeded in time. This is great news — it means we’re solving a real performance issue with user facing effect, not only making a graph look prettier. . After ignoring all experiments where the control was timing out, the remaining mismatches were caused by differing timestamps in the generated commits: our new RPC API was missing a “time” argument (the time at which the merge commit was created), and was using the local time in the fileserver instead. Since the experiment is perfomed sequentially, it was very easy to end up with mismatched timestamps between the control and the candidate, particularly for merges that take more than 1s. We fixed this by adding an explicit  time  argument to the RPC call, and after deploying, all the trivial mismatches were gone from the graph. . Once we had some basic level of trust in the new implementation, it was time to increase the percentage of requests for which the experiment runs. More requests leads to more accurate performance stats and more corner cases where the new implementation was giving incorrect results. .   . Simply looking at the performance graph makes it obvious that, even though for the average case the new code is  significantly  faster than the old implementation, there are performance spikes that need to be handled. . Finding the cause of these spikes is trivial thanks to our tooling: we can configure Scientist to log any experiments where the candidate takes longer than Xms to run (in our case we aimed at 5000ms), and let it run overnight capturing this data. . We did that for 4 days, every morning waking up to a tidy list of cases where the result of the new code was different from the old code, and cases where the new code was just giving good results but running too slowly. . Consequently, our workflow is straightforward: we fix the mismatches and the performance issues, we deploy again to production, and we continue running the experiments, until there are no mismatches and no slow runs. It is worth noting that we fix both performance problems and mismatches with no priority for either: the old adage of “make it work and then make it fast” is pointless here — and in all of GitHub’s systems. If it’s not fast, it is  not  working, at least not properly. . As a sample, here are a few of the cases we fixed over these 4 days: .  The exit value of this program is negative on error, and the number of  conflicts otherwise. If the merge was clean, the exit value is 0.  . Given that shells only use the lowest 8 bits of a program’s exit code, it’s obvious why Git could merge this file: the 768 conflicts were being reported as 0 by the shell, because 768 is a multiple of 256! . This bug was intense. But one more time, libgit2 was doing the right thing, so again we  fixed the issue upstream in Git  and continued with the experiments. . After 4 days of iterating on this process, we managed to get the experiment running for 100% of the requests, for 24 hours, and with no mismatches nor slow cases during the whole run. In total, Scientist verified tens of millions of merge commits to be virtually identical in the old and new paths. Our experiment was a success. . The last thing to do was the most exciting one: remove the Scientist code and switch all the frontend machines to run the new code by default. In a matter of hours, the new code was deployed and running for 100% of the production traffic in GitHub. Finally, we removed the old implementation — which frankly is the most gratifying part of this whole process. . To sum it up: In roughly 5 days of part-time work, we’ve replaced one of GitHub’s more critical code paths with no user-visible effects. As part of this process, we’ve fixed 2 serious merge-related bugs in the original Git implementation which had gone undetected for years, and 3 major performance issues in the new implementation. The 99th percentile of the new implementation is roughly equivalent to the 95th of the old implementation. . This kind of aggressive technical debt cleanup and optimization work can only happen as a by-product of our engineering ecosystem. Without being able to deploy the main application more than 60 times in a day, and without the tooling to automatically test and benchmark two wildly different implementations in production, this process would have taken months of work and there would be no realistic way to ensure that there were no performance or behavior regressions. ", "date": "December 15, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tScientist: Measure Twice, Cut Once\t\t", "author": ["\n\t\tJesse Toth\t"], "link": "https://github.blog/2016-02-03-scientist/", "abstract": " Today we’re releasing  Scientist  1.0 to help you rewrite critical code with confidence. . As codebases mature and requirements change, it is inevitable that you will need to replace or rewrite a part of your system. At GitHub, we’ve been lucky to have many systems that have scaled far beyond their original design, but eventually there comes a point when performance or extensibility break down and we have to rewrite or replace a large component of our application. . A few years ago when we were faced with the task of rewriting one of the most critical systems in our application — the permissions code that controls access and membership to repositories, teams, and organizations — we began looking for a way to make such a large change and have confidence in its correctness. . There is a fairly common architectural pattern for making large-scale changes known as  Branch by Abstraction . It works by inserting an abstraction layer around the code you plan to change. The abstraction simply delegates to the existing code to begin with. Once you have the new code in place, you can flip a switch in the abstraction to begin substituting the new code for the old. . Using abstractions in this way is a great way to create a chokepoint for calls to a particular code path, making it easy to switch over to the new code when the time comes, but it doesn’t really ensure that the behavior of the new system will match the old system — just that the new system will be called in all places where the old system was called. For such a critical piece of our system architecture, this pattern only fulfilled half of the requirements. We needed to ensure not only that the new system would be used in all places that the old system was, but also that its behavior would be correct and match what the old system did. . If you want to test correctness, you just write some tests for your new system, right? Well, not quite. Tests are a good place to start verifying the correctness of a new system as you write it, but  they aren’t enough . For sufficiently complicated systems, it is unlikely you will be able to cover all possible cases in your test suite. If you do, it will be a large, slow test suite that slows down development considerably. . There’s also a more concerning reason not to rely solely on tests to verify correctness: Since software has bugs, given enough time and volume, your data will have bugs, too.  Data quality  is the measure of how buggy your data is. Data quality problems may cause your system to behave in unexpected ways that are not tested or explicitly part of the specifications. Your users will encounter this bad data, and whatever behavior they see will be what they come to rely on and consider correct. If you don’t know how your system works when it encounters this sort of bad data, it’s unlikely that you will design and test the new system to behave in the way that matches the legacy behavior. So, while test coverage of a rewritten system is hugely important, how the system behaves with production data as the input is the only true test of its correctness compared to the legacy system’s behavior. . We built Scientist to fill in that missing piece and help test the production data and behavior to ensure correctness. It works by creating a lightweight abstraction called an  experiment  around the code that is to be replaced. The original code — the control — is delegated to by the  experiment  abstraction, and its result is returned by the experiment. The rewritten code is added as a candidate to be tried by the experiment at execution time. When the experiment is called at runtime, both code paths are run (with the order randomized to avoid ordering issues). The results of both the control and candidate are compared and, if there are any differences in that comparison, those are recorded. The duration of execution for both code blocks is also recorded. Then the result of the control code is returned from the experiment. . From the caller’s perspective, nothing has changed. But by running and comparing both systems and recording the behavior mismatches and performance differences between the legacy system and the new one, you can use that data as a feedback loop to modify the new system (or sometimes the old!) to fix the errors, measure, and repeat until there are no differences between the two systems. You can even start using Scientist before you’ve fully implemented the rewritten system by telling it to ignore experiments that mismatch due to a known difference in behavior. . The diagram below shows the happy path that experiments follow: .   . Happy paths are only part of a system’s behavior, though, so Scientist can also handle exceptions. Any exceptions encountered in either the control or candidate blocks will be recorded in the experiments observations. An exception in the control will be re-raised at the end of the experiment since this is the “return value” of that block; exceptions in candidate blocks will not be raised since that would create an unexpected side-effect of the experiment. If the candidate and control blocks raise the same exception, this is considered a match since both systems are behaving the same way. . Let’s say we have a method to determine whether a repository can be pulled by a particular user: . But the  is_collaborator?  method is very inefficient and does not perform well, so you have written a new method to replace it: . To declare an experiment, wrap it in a  science  block and name your experiment: . Declare the original body of the method to be the control branch — the branch to be returned by the entire science block once it finishes running: . Then specify the candidate branch to be tried by the experiment: . You may also want to add some context to the experiment to help debug potential mismatches: . By default, all experiments are enabled all of the time. Depending on where you are using Scientist and the performance characteristics of your application, this may not be safe. To change this default behavior and have more control over when experiments run, you’ll need to create your own experiment class and override the  enabled?  method. The code sample below shows how to override  enabled?  to enable each experiment a percentage of the time: . You’ll also need to override the  new  method to tell Scientist create new experiments with your class rather than the default experiment implementation: . Scientist is not opinionated about what you should do with the data it produces; it simply makes the metrics and results available and leaves it up to you to decide how and whether to store it. Implement the  publish  method in your experiment class to record metrics and store mismatches. Scientist passes an experiment’s result to this method. A  Scientist::Result  contains lots of useful information about the experiment such as: . At GitHub, we use  Brubeck  and Graphite to record metrics. Most experiments use Redis to store mismatch data and additional context. Below is an example of how we publish results: . By publishing this data, we get graphs that look like this: .      . And mismatch data like: . Once you have some mismatch data, you can begin investigating individual mismatches to see why the control and candidate aren’t behaving the same way. Usually you’ll find that the new code has a bug or is missing a part of the behavior of the legacy code, but sometimes you’ll find that the bug is actually in the legacy code or in your data. After the source of the error has been corrected, you can start the experiment again and repeat this process until there are no more mismatches between the two code paths. . Once you are able to conclude with reasonable confidence that the control and candidate are behaving the same way, it’s time to wrap up your experiment! Ending an experiment is as simple as disabling it, removing the science code and control implementation, and replacing it with the candidate implementation. . There are a few cases where Scientist is not an appropriate tool to use. The most important caveat is that Scientist is not meant to be used for any code that has side-effects. A candidate code path that writes to the same database as the control, invalidates a cache, or otherwise modifies data that affects the original, production behavior is dangerous and incorrect. For this reason, we only use Scientist on read operations. . You should also be mindful that you take a performance hit using Scientist in production. New experiments should be introduced slowly and carefully and their impact on production performance should be closely monitored. They should run for just as long as is necessary to gain confidence rather than being left to run indefinitely, especially for expensive operations. . We make liberal use of Scientist for a multitude of problems at GitHub. This development pattern can be used for something as small as a single method or something as large as an external system. The  Move Fast and Fix Things  post is a great example of a short rewrite made easier with Scientist. Over the last few years we’ve also used Scientist for projects such as: . If you’re about to make a risky change to your Ruby codebase, give the  Scientist gem  a try and see if it can help make your work easier. Even if Ruby isn’t your language of choice, we’d still encourage you to apply Scientist’s experiment pattern to your system. And of course we would love to hear about any open source libraries you build to accomplish this! ", "date": "February 3, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tJanuary 28th Incident Report\t\t", "author": ["\n\t\tScott Sanders\t"], "link": "https://github.blog/2016-02-03-january-28th-incident-report/", "abstract": " Last week GitHub  was unavailable  for two hours and six minutes. We understand how much you rely on GitHub and consider the availability of our service one of the core features we offer. Over the last eight years we have made considerable progress towards ensuring that you can depend on GitHub to be there for you and for developers worldwide, but a week ago we failed to maintain the level of uptime you rightfully expect. We are deeply sorry for this, and would like to share with you the events that took place and the steps we’re taking to ensure you’re able to access GitHub. . At 00:23am UTC on Thursday, January 28th, 2016 (4:23pm PST, Wednesday, January 27th) our primary data center experienced a brief disruption in the systems that supply power to our servers and equipment. Slightly over 25% of our servers and several network devices rebooted as a result. This left our infrastructure in a partially operational state and generated alerts to multiple on-call engineers. Our load balancing equipment and a large number of our frontend applications servers were unaffected, but the systems they depend on to service your requests were unavailable. In response, our application began to deliver HTTP 503 response codes, which carry the unicorn image you see on our error page. . Our early response to the event was complicated by the fact that many of our ChatOps systems were on servers that had rebooted. We do have redundancy built into our ChatOps systems, but this failure still caused some amount of confusion and delay at the very beginning of our response. One of the biggest customer-facing effects of this delay was that  status.github.com  wasn’t set to status red until 00:32am UTC, eight minutes after the site became inaccessible. We consider this to be an unacceptably long delay, and will ensure faster communication to our users in the future. . Initial notifications for unreachable servers and a spike in exceptions related to Redis connectivity directed our team to investigate a possible outage in our internal network. We also saw an increase in connection attempts that pointed to network problems. While later investigation revealed that a DDoS attack was not the underlying problem, we spent time early on bringing up DDoS defenses and investigating network health. Because we have experience mitigating DDoS attacks, our response procedure is now habit and we are pleased we could act quickly and confidently without distracting other efforts to resolve the incident. . With our DDoS shields up, the response team began to methodically inspect our infrastructure and correlate these findings back to the initial outage alerts. The inability to reach all members of several Redis clusters led us to investigate uptime for devices across the facility. We discovered that some servers were reporting uptime of several minutes, but our network equipment was reporting uptimes that revealed they had not rebooted. Using this, we determined that all of the offline servers shared the same hardware class, and the ones that rebooted without issue were a different hardware class. The affected servers spanned many racks and rows in our data center, which resulted in several clusters experiencing reboots of all of their member servers, despite the clusters’ members being distributed across different racks. . As the minutes ticked by, we noticed that our application processes were not starting up as expected. Engineers began taking a look at the process table and logs on our application servers. These explained that the lack of backend capacity was a result of processes failing to start due to our Redis clusters being offline. We had inadvertently added a hard dependency on our Redis cluster being available within the boot path of our application code. . By this point, we had a fairly clear picture of what was required to restore service and began working towards that end. We needed to repair our servers that were not booting, and we needed to get our Redis clusters back up to allow our application processes to restart. Remote access console screenshots from the failed hardware showed boot failures because the physical drives were no longer recognized. One group of engineers split off to work with the on-site facilities technicians to bring these servers back online by draining the flea power to bring them up from a cold state so the disks would be visible. Another group began rebuilding the affected Redis clusters on alternate hardware. These efforts were complicated by a number of crucial internal systems residing on the offline hardware. This made provisioning new servers more difficult. . Once the Redis cluster data was restored onto standby equipment, we were able to bring the Redis-server processes back online. Internal checks showed application processes recovering, and a healthy response from the application servers allowed our HAProxy load balancers to return these servers to the backend server pool. After verifying site operation, the maintenance page was removed and we moved to status yellow. This occurred two hours and six minutes after the initial outage. . The following hours were spent confirming that all systems were performing normally, and verifying there was no data loss from this incident. We are grateful that much of the disaster mitigation work put in place by our engineers was successful in guaranteeing that all of your code, issues, pull requests, and other critical data remained safe and secure. . Complex systems are defined by the interaction of many discrete components working together to achieve an end result. Understanding the dependencies for each component in a complex system is important, but unless these dependencies are rigorously tested it is possible for systems to fail in unique and novel ways. Over the past week, we have devoted significant time and effort towards understanding the nature of the cascading failure which led to GitHub being unavailable for over two hours. We don’t believe it is possible to fully prevent the events that resulted in a large part of our infrastructure losing power, but we can take steps to ensure recovery occurs in a fast and reliable manner. We can also take steps to mitigate the negative impact of these events on our users. . We identified the hardware issue resulting in servers being unable to view their own drives after power-cycling as a known firmware issue that we are updating across our fleet. Updating our tooling to automatically open issues for the team when new firmware updates are available will force us to review the changelogs against our environment. . We will be updating our application’s test suite to explicitly ensure that our application processes start even when certain external systems are unavailable and we are improving our circuit breakers so we can gracefully degrade functionality when these backend services are down. Obviously there are limits to this approach and there exists a minimum set of requirements needed to serve requests, but we can be more aggressive in paring down the list of these dependencies. . We are reviewing the availability requirements of our internal systems that are responsible for crucial operations tasks such as provisioning new servers so that they are on-par with our user facing systems. Ultimately, if these systems are required to recover from an unexpected outage situation, they must be as reliable as the system being recovered. . A number of less technical improvements are also being implemented. Strengthening our cross-team communications would have shaved minutes off the recovery time. Predefining escalation strategies during situations that require all hands on deck would have enabled our incident coordinators to spend more time managing recovery efforts and less time navigating documentation. Improving our messaging to you during this event would have helped you better understand what was happening and set expectations about when you could expect future updates. . We realize how important GitHub is to the workflows that enable your projects and businesses to succeed. All of us at GitHub would like to apologize for the impact of this outage. We will continue to analyze the events leading up to this incident and the steps we took to restore service. This work will guide us as we improve the systems and processes that power GitHub. ", "date": "February 3, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tTwo years of bounties\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2016-02-04-two-years-of-bounties/", "abstract": " Despite the best efforts of its writers, software has vulnerabilities, and GitHub is no exception. Finding, fixing, and learning from past bugs is a critical part of keeping our users and their data safe on the Internet. Two years ago,  we launched  the  GitHub Security Bug Bounty  and it’s been an incredible success. By rewarding the talented and dedicated researchers in the security industry, we discover and fix security vulnerabilities before they can be exploited. .   . Of  7,050  submissions in the past two years,  1,772  warranted further review, helping us to identify and fix vulnerabilities spanning all of the OWASP top 10 vulnerability classifications.  58  unique researchers earned a cumulative  $95,300  for the  102  medium to high risk vulnerabilities they reported. This chart shows the breakdown of payouts by severity and OWASP classification: . We love it when a reported vulnerability ends up not being our fault.  @kelunik  and  @bwoebi  reported a browser vulnerability, causing GitHub’s cookies to be sent to other domains.  @ealf  reported a browser bug, bypassing our JavaScript same-origin policy checks. We were able to protect our users from these vulnerabilities months before the browser vendors released patches. . Another surprising bug was reported by  @cryptosense , who found that some RSA key generators were creating SSH keys that were trivially factorable. We ended up finding and revoking 309 weak RSA keys and now have validations checking if keys are factorable by the first 10,000 primes. . In the first year of the bounty program, we saw reports mostly about our web services. In 2015, we received a number of reports for vulnerabilities in our desktop apps.  @tunz  reported a clever exploit against  GitHub for Mac , allowing remote code execution. Shortly thereafter,  @joernchen  reported a similar bug in  GitHub for Windows , following up a few months later with a separate client-side remote code execution vulnerability in  Git Large File Storage (LFS) . . In 2015 we saw an amazing increase in the number of bounties donated to a good cause. GitHub matches bounties donated to 501(c)(3) organizations, and with the help of our researchers we contributed to  the EFF ,  Médecins Sans Frontières ,  the Ada Initiative ,  the Washington State Burn Foundation , and  the Tor Project . A big thanks to  @ealf ,  @LukasReschke ,  @arirubinstein ,  @cryptosense ,  @bureado ,  @vito , and  @s-rah  for their generosity. . In the first two years of the program, we paid researchers nearly $100,000. That’s a great start, but we hope to further increase participation in the program. So, fire up your favorite proxy and start poking at GitHub.com. When you find a vulnerability,  report it  and join the ranks of our  leaderboard . Happy hacking! ", "date": "February 4, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tDelivering Octicons with SVG\t\t", "author": ["\n\t\tAaron Shekey\t"], "link": "https://github.blog/2016-02-22-delivering-octicons-with-svg/", "abstract": " GitHub.com no longer delivers its icons via icon font. Instead, we’ve replaced all the  Octicons  throughout our codebase with SVG alternatives. While the changes are mostly under-the-hood, you’ll immediately feel the benefits of the SVG icons. .   . Switching to SVG renders our icons as images instead of text, locking nicely to whole pixel values at any resolution. Compare the zoomed-in icon font version on the left with the crisp SVG version on the right. . Icon fonts have always been a hack. We originally used a custom font with our icons as unicode symbols. This allowed us to include our icon font in our CSS bundle. Simply adding a class to any element would make our icons appear. We could then change the size and color on the fly using only CSS. . Unfortunately, even though these icons were vector shapes, they’d often render poorly on 1x displays. In Webkit-based browsers, you’d get blurry icons depending on the browser’s window width. Since our icons were delivered as text, sub-pixel rendering meant to improve text legibility actually made our icons look much worse. . Since our SVG is injected directly into the markup (more on why we used this approach in a bit), we no longer see a flash of unstyled content as the icon font is downloaded, cached, and rendered. .   . As laid out in   Death to Icon Fonts  , some users override GitHub’s fonts. For dyslexics, certain typefaces can be more readable. To those changing their fonts, our font-based icons were rendered as empty squares. This messed up GitHub’s page layouts and didn’t provide any meaning. SVGs will display regardless of font overrides. For screen readers, SVG provides us the ability to add pronouncable  alt  attributes, or leave them off entirely. . For each icon, we currently serve a single glyph at all sizes. Since the loading of our site is dependent on the download of our icon font, we were forced to limit the icon set to just the essential 16px shapes. This led to some concessions on the visuals of each symbol since we’d optimized for the 16px grid. When scaling our icons up in blankslates or marketing pages, we’re still showing the 16px version of the icon. With SVGs, we can easily fork the entire icon set and offer more appropriate glyphs at any size we specify. We could have done this with our icon fonts, but then our users would need to download twice as much data. Possibly more. . Building custom fonts is hard. A few web apps have popped up to solve this pain. Internally, we’d built our own. With SVG, adding a new icon could be as trivial as dragging another SVG file into a directory. . We’re not saying we should, but we could, though SVG animation does have some practical applications— preloader animations , for example. . Our Octicons appear nearly 2500 times throughout GitHub’s codebase. Prior to SVG, Octicons were included as simple spans  &lt;span class=\"octicon octicon-alert\"&gt;&lt;/span&gt; . To switch to SVG, we first added a Rails helper for injecting SVG paths directly into to our markup. Relying on the helper allowed us to test various methods of delivering SVG to our staff before enabling it for the public. Should a better alternative to SVG come along, or if we need to revert back to icon fonts for any reason, we’d only have to change the output of the helper. . Input   &lt;%= octicon(:symbol =&gt; \"plus\") %&gt;  . Output . You can see we’ve landed on directly injecting the SVGs directly in our page markup. This allows us the flexibility to change the color of the icons with CSS using the  fill:  declaration on the fly. . Instead of an icon font, we now have a directory of SVG shapes whose paths are directly injected into the markup by our helper based on which  symbol  we choose. For example, if we want an  alert  icon, we call the helper  &lt;%= octicon(:symbol =&gt; \"alert\") %&gt; . It looks for the icon of the same file name and injects the SVG. . We tried a number of approaches when adding SVG icons to our pages. Given the constraints of GitHub’s production environment, some were dead-ends. . External .svg — We first attempted to serve a single external “svgstore��. We’d include individual sprites using the  &lt;use&gt;  element. With our current cross-domain security policy and asset pipeline, we found it difficult to serve the SVG sprites externally. . SVG background images — This wouldn’t let us color our icons on the fly. . SVGs linked via  &lt;img&gt;  and the  src  attribute — This wouldn’t let us color our icons on the fly. . Embedding the entire “svgstore” in every view and using  &lt;use&gt;  — It just didn’t feel quite right to embed every SVG shape we have on every single page throughout GitHub.com especially if a page didn’t include a single icon. . We’ve found there were no adverse effects on  pageload or performance  when switching to SVG. We’d hoped for a more dramatic drop in rendering times, but often performance has more to do with perception. Since SVG icons are being rendered like images in the page with defined widths and heights, the page doesn’t have nearly as much  jank . . We were also able to  kill a bit of bloat from our CSS bundles  since we’re no longer serving the font CSS. . By switching from icon fonts, we can serve our icons more easily, more quickly, and more accessibly. And they look better. Enjoy. ", "date": "February 22, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tRevamping GitHub’s Subversion Bridge\t\t", "author": ["\n\t\tGitHub Engineering\t"], "link": "https://github.blog/2016-03-08-revamping-githubs-subversion-bridge/", "abstract": " One of GitHub’s niche features is the ability to access a Git repository on GitHub using Subversion clients. Last year we re-architected a large portion of the Subversion bridge to work with our changing infrastructure. . A key part of the Subversion bridge is a mapping between Git commits and Subversion revision numbers. The mapping is persisted so that we can produce a consistent view of the repository. A bit of the mapping is exposed via a custom SVN property,  git-commit . For example, you can see that revision 2504 of phantomjs is an SVN representation of Git commit   2837f28  . . During the initial development of the Subversion bridge, we chose to store the mapping data in the repository directory as a  serialized Ruby data structure . This worked well as it co-located the mapping information with the target of the mapping. . Storing the mapping this way has some disadvantages. The mapping file required special treatment in the tools that manage our infrastructure. For example, backup scripts couldn’t simply  git clone  a repository. As our infrastructure was evolving, these types of special cases made it impractical to store ad-hoc files in Git repositories. . So, in 2015, we undertook an effort to move the Subversion mapping into the Git repository’s object database. This keeps the mapping data co-located with the repository and it also means that there are  no longer   different  special cases related to handling the mapping data. The result of this effort is that the mapping data is now just an ordinary Git commit: . In addition to moving the mapping data to Git our goals were to maintain feature parity and to not negatively impact performance for our end users. . In order to provide a seamless rollover from the old mapping to the new mapping we used our  Scientist library  to run both mappings in parallel. . The Scientist library helps you take two or more implementations, run the same inputs through them, and then compare the output of each in production. This helps build confidence that the new implementation is equivalent to the old. Testing accomplishes some of this. But in complex systems, real use provides a scope of testing that is simply not possible in a reasonable amount of engineering time. . The first step of this project was to extract a  MsgpackMapping  class. The new class encapsulated all of the storage needs of the SVN bridge. It was an interface that we could re-implement in terms of the new Git-backed mapping. . The  MsgpackMapping  class has a fairly wide interface (30 methods!) and the old and new mapping implementations are different enough that replacing one method at a time wasn’t possible. With Scientist we were able to incrementally implement and refine the new implementation. The new implementation could run alongside the old implementation. We could compare the accuracy and performance of each new method as we implemented it. . Next we created a new  GitMapping  class with the empty methods that matched the methods in  MsgpackMapping . . Then we created the  ScientificMapping  class that uses Scientist to run the experiments. This class let us enable and disable experiments for each method as we implemented it. . Finally we created a class to represent experiments. This class controls how often experiments are run. It records the results of the experiments. . One thing to note in the code above is how we stored results. We counted results with Statsd. We stored details about experiments in log files at first, and later switched to storing them in our exception reporting system. None of these were new to the Subversion bridge. They’ve all been in use for a long time, and we have good tooling for querying them. This highlights one of the boons of using Scientist: it makes no assumptions about how you want to store your results. For example, other apps at GitHub use Redis and/or MySQL to store Scientist’s results. . With Scientist configured and our new mapping classes in place we started the process of implementing each mapping method. . Our process for implementing the  GitMapping  class did not change much over the course of the project. However, we frequently made small changes that shortened the feedback loop for each step. The core of our process looked something like this: . Write a naive implementation that satisfies the existing unit and integration tests. . Enable the experiment in the  ScientificMapping  class. . Deploy to production and watch our graphs and error reporting system for mismatches. . Try to replicate a mismatch in development and add a new unit or integration test to cover the scenario. . Make the new test pass. . Repeat steps 3-5 until there are no mismatches in production. . We relied heavily on graphs, logs, and scripts for identifying mismatches, measuring performance, and tightening our feedback loop. . At the beginning we had a dashboard that summarized experiment mismatches, performance, and response times to ensure our experiments weren’t negatively impacting customers. .   . With a general sense of how things were going, we dug into specific mismatches with our raw log and error reporting systems. . This is an expanded view of a mismatch in our error reporting system. .   . Further into the project as mismatches became less of an issue and maintaining performance became more of a concern we added a new dashboard that split out each method into its own graph and gave us a quick visual of how the candidate was performing against the control. The new graphs enabled us to track down some significant performance regressions. .   . We used a handful of caching strategies to fix performance regressions in addition to implementation changes. . The process for replicating issues in development quickly became a bottleneck. Each repository is a special snowflake, made up of a set of commits from a variety of committers with a variety of native languages and a variety of Git versions. Each repository has a mapping file that was built up by many versions of the Subversion bridge. It was hard to reproduce bugs in development with contrived repositories. So we added  script/clone . This script cloned open source repositories and pulled a copy of the msgpack mapping file. This allowed us to reproduce, test, and debug problems locally. . The next script we wrote opened a console for our app with instances of the  MsgpackMapping ,  GitMapping , and  ScientificMapping  classes already initialized. It’s not like doing this manually took a long time. But we were doing it so often that scripting it saved time in the long run. It also made construction of the objects more consistent. . As performance tuning became more of a concern we added  script/benchmark  to help us quickly iterate on a single repository in development without having to deploy and then wait for performance data to be collected in production. . In the end we were able to swap out the msgpack based mapping for the new Git-backed mapping in production for thousands of customers. Our Git infrastructure team was able to continue making improvements without the Subversion mapping file in the way. . To learn more about how we use Scientist read  Scientist: Measure Twice, Cut Over Once  by  @jesseplusplus  and  Move Fast and Fix Things  by  @vmg . .     .  Matt Burke   Engineer   GitHub Profile |   Website |   Twitter Profile  .     .  Jonathan Hoyt   Engineer   GitHub Profile |   Twitter  | Profile  Blog  ", "date": "March 8, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing DGit\t\t", "author": ["\n\t\tPatrick Reynolds\t"], "link": "https://github.blog/2016-04-05-introducing-dgit/", "abstract": " Edit:  DGit is now called Spokes  . GitHub hosts over 35 million repositories and over 30 million Gists on  hundreds of servers.  Over the past year, we’ve built DGit, a new  distributed storage system that dramatically improves the availability,  reliability, and performance of serving and storing Git content. . DGit is short for “Distributed Git.” As many readers already know, Git  itself is distributed—any copy of a Git repository contains every file,  branch, and commit in the project’s entire history. DGit uses  this property of Git to keep three copies of every repository,  on three different servers. The design of DGit keeps repositories fully  available without interruption even if one of those servers goes down. Even  in the extreme case that two copies of a repository become unavailable at  the same time, the repository remains readable; i.e., fetches, clones, and  most of the web UI continue to work. . DGit performs replication at the application layer, rather than at the disk  layer. Think of the replicas as three loosely-coupled Git repositories kept  in sync via Git protocols, rather than identical disk images full of  repositories. This design gives us great flexibility to decide where to  store the replicas of a repository and which replica to use for read  operations. . If a file server needs to be taken offline, DGit automatically determines  which repositories are left with fewer than three replicas and creates new  replicas of those repositories on other file servers. This “healing” process  uses all remaining servers as both sources and destinations. Since healing  throughput is N-by-N, it is quite fast. And all this happens  without any downtime. . Most end users store their Git repositories as objects, pack files, and  references in a single  .git  directory. They access the repository using  the Git command-line client or using graphical clients like GitHub Desktop  or the built-in support for Git in IDEs like Visual Studio. Perhaps it’s  surprising that GitHub’s repository-storage tier, DGit, is built using the same  technologies. Why not a SAN? A distributed file system? Some other magical  cloud technology that abstracts away the problem of storing bits durably? . The answer is simple: it’s fast and it’s robust. . Git is very sensitive to latency. A simple  git log  or  git blame  might  require thousands of Git objects to be loaded and traversed sequentially. If there’s any  latency in these low-level disk accesses, performance suffers dramatically.  Thus, storing the repository in a distributed file system is not viable. Git  is optimized to be fast when accessing fast disks, so the DGit file servers  store repositories on fast, local SSDs. . At a higher level, Git is also optimized to exchange updates between Git  repositories (e.g., pushes and fetches) over efficient protocols.  So we use these protocols to keep the DGit replicas in sync. . Git is a mature and well-tested technology. Why reinvent the wheel  when there is a Formula One racing car already available? . It has always been GitHub’s philosophy to use Git on our servers in a manner  that is as close as possible to how Git is used by our users. DGit continues  this tradition. If we find performance bottlenecks or other problems, we  have several core Git and  libgit2  contributors  on staff who fix the problems and contribute the fixes to the open-source  project for everybody to use. Our level of experience and expertise with Git  helped make it the obvious choice to use for DGit replication. . Until recently, we kept copies of repository data using off-the-shelf,  disk-layer replication technologies—namely, RAID and   DRBD .  We organized our file servers in  pairs. Each active file server had a dedicated, online spare  connected by a cross-over cable.  Each disk had four copies: two  copies on the main file server, using RAID, and another two copies on that  file server’s hot spare, using DRBD.  If anything went wrong with a file  server—e.g., hardware failure, software crash, or an overload situation—a  human would confirm the fault and order the spare to take over. Thus, there  was a good level of redundancy, but the failover process required manual  intervention and inevitably caused a little bit of downtime for the  repositories on the failed server. To make such incidents as rare as  possible, we have always stored repositories on specialized, highly reliable  servers. . Now with DGit, each repository is stored on three servers chosen  independently in locations distributed around our large pool of  file servers. DGit automatically selects the servers to host each repository,  keeps those replicas in sync, and picks the best server to handle each  incoming read request. Writes are synchronously streamed to all three  replicas and are only committed if at least two replicas confirm success. . GitHub now stores repositories in a cluster called  github-dfs — dfs  is short  for “DGit file server.”  The repositories are stored on  local disks on these file servers and are served up by Git and  libgit2 .  The clients of this cluster include the web front end and the proxies that  speak to users’ Git clients. . DGit delivers many advantages both to GitHub users and to the  internal GitHub infrastructure team. It is also a key foundation that will  enable more upcoming innovations. . DGit is a big change, and so we’ve been rolling it out gradually. The most  complicated aspect of DGit is that replication is no longer transparent:  every repository is now stored explicitly on three servers,  rather than on one server with an automatically synchronized hot spare.  Thus, DGit must implement its own  serializability, locking, failure detection, and resynchronization,  rather then relying on DRBD and the RAID controller to keep the copies in  sync. Those are rich topics that we’ll explore in later posts; suffice it to  say, we wanted to test them all thoroughly before relying on DGit to store  customer data. Our deployment progressed over many steps: . During the rollout phase we routinely powered down servers, sometimes  several at once, while they were serving live production traffic. User  operations were not affected. . As of this writing, 58% of repositories and 96% of Gists, representing 67% of  Git operations, are in DGit.  We are moving the rest as quickly as we  can turn pre-DGit file server pairs into DGit servers. . GitHub always strives to make fetching, pushing, and viewing repositories quick  and reliable.  DGit is how our repository-storage tier will meet those  goals for years to come while allowing us to scale horizontally and increase fault tolerance. . Over the next month we will be following up with in-depth posts on the technology behind DGit. ", "date": "April 5, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub’s CSP journey\t\t", "author": ["\n\t\tPatrick Toomey\t"], "link": "https://github.blog/2016-04-12-githubs-csp-journey/", "abstract": " We shipped  subresource integrity  a few months back to reduce the risk of a compromised CDN serving malicious JavaScript. That is a big win, but does not address related content injection issues that may exist on GitHub.com itself. We have been tackling this side of the problem over the past few years and thought it would be fun, and hopefully useful, to share what we have been up to. . Just to get everyone on the same page, when talking about “content injection” we are talking about: . GitHub uses auto-escaping templates, code review, and static analysis to try to prevent these kinds of bugs from getting introduced in the first place, but history shows they are unavoidable. Any strategy that relies on preventing any and all content injection bugs is bound for failure and will leave your engineers, and security team, constantly fighting fires. We decided that the only practical approach is to pair prevention and detection with additional defenses that make content injection bugs much more difficult for attackers to exploit. As with most problems, there is no single magical fix, and therefore we have employed multiple techniques to help with mitigation. In this post we will focus on our ever evolving use of  Content Security Policy  (CSP), as it is our single most effective mitigation. We can’t wait to follow up on this blog to additionally review some of the “non-traditional” approaches we have taken to further mitigate content injection. .  Content Security Policy  is an HTTP header that enables a site to use a declarative policy to set restrictions for web resources (JavaScript, CSS, form submissions, etc). CSP is incredibly useful for leveling up the security of your site and is particularly suited for mitigating content injection bugs. GitHub was an early adopter of CSP, having shipped our  initial implementation  approximately three years ago. CSP was in its infancy then and our initial policy reflected this: . The policy was relatively simple, but substantially reduced the risk of XSS on GitHub.com. After the initial ship we knew there was quite a bit more we could do to tighten things up. During our initial ship we were forced to trust a number of domains to maintain backward compatibility. The above policy did nothing to help with HTML injection that could be used to exfiltrate sensitive information (demonstrated below). However, that was almost three years ago, and a lot has changed since then. We have refactored the vast majority of our third-party script dependencies and CSP itself has also added a number of new directives to further help mitigate content injection bugs and strengthen our policy. . Our current CSP policy looks like this: . While some of the above directives don’t directly relate to content injection, many of them do. So, let’s take a walk through the more important CSP directives that GitHub uses. Along the way we will discuss what our current policy is, how that policy prevents specific attack scenarios, and share some  bounty submissions  that helped us shape our current policy. . Unlike our original policy, we now only source JavaScript from our CDN. As was noted at the beginning of this post, we use subresource integrity to reduce our risk of sourcing malicious externally sourced JavaScript. The net result is that we have a very high assurance that the only JavaScript we are sourcing is what is checked into Git and then uploaded to our CDN. Of particular note is our lack of  self  in our source list. While sourcing JavaScript from  self  seems relatively safe (and extremely common), it should be avoided when possible. . There are edge cases that any developer must concern themselves with when allowing  self  as a source for scripts. There may be a forgotten JSONP endpoint that  doesn’t sanitize the callback function name . Or, another endpoint that serves user-influenced content with a  content-type  that could be  sniffed by browsers  as JavaScript. GitHub has several such endpoints. For example, we return  commit diffs  as  text/plain . By eliminating  self  from our policy, we don’t need to be concerned that a CSP supporting browser will ever use this as source of JavaScript. We also mitigate this by using the  X-Content-Type-Options: nosniff  header, but CSP provides extremely strong assurances, even if there were a bug that let an attacker control  content-type . . We previously allowed  self  for object and embed tags. The sole reason for this was a legacy reliance of sourcing  ZeroClipboard  from GitHub.com. We had since moved that asset to our CDN and the  self  source was no longer needed. However, this legacy directive wasn’t removed from our policy and, as with all things related to “legacy” and “security,” a bounty hunter found  a way to exploit it . This was a super interesting Bug Bounty submission that left us scratching our heads for a few minutes. The attack leveraged a content injection bug as well as a browser bug in Chrome to bypass CSP and gain JavaScript execution. The attack worked like this: . First, an attacker creates a Wiki entry with the following content: . One of the core features on GitHub is rendering user-supplied HTML (often via Markdown) in various locations (Issues, Pull Requests, Comments). All of these locations sanitize the resulting HTML to a safe subset to protect against arbitrary HTML injection. However, we had an oversight in our Wiki HTML sanitization filter that allowed setting an arbitrary  class  attribute. The combination of setting the class to   choose_plan  and  js-domain  triggered some automatic behavior in our JavaScript to fetch the  href  associated with the element and insert the response into the DOM. The resulting HTML was still subject to CSP and would not allow executing arbitrary JavaScript. It did however allow an attacker to insert arbitrary HTML into the DOM. The injected content in the proof of concept was the following: . The sourced URL corresponds to a “raw request” for a file in a user’s repository. A raw request for a non-binary file returns the file with a  content-type  of  text/plain , and is displayed in the user’s browser. As was hinted at previously, user-controlled content in combination with content sniffing often leads to unexpected behavior. We were well aware that serving user-controlled content on a GitHub.com domain would increase the chances of script execution on that domain. For that very reason, we serve all responses to raw requests on their own domain. A request to  https://github.com/test-user/test-repo/raw/master/script.png  will result in a redirect to  https://raw.githubusercontent.com/test-user/test-repo/master/script.png . And,  raw.githubusercontent.com  wasn’t on our  object-src  list. So, how was the proof of concept able to get Flash to load and execute? . After rereading the submission, doing a bit of researching, and brewing some extra coffee, we came across  this WebKit bug . Browsers are required to verify that all requests, including those resulting from redirects, are allowed by the CSP policy for the page. However, some browsers were only checking the domain from the first request against the source list in our CSP policy. Since we had  self  in our source list, the embed was allowed. Combining the Flash execution with the injected HTML (specifically the  allowscriptaccess=always  attribute) resulted in a full CSP bypass. The submission earned  @adob  a gold star and further cemented his placement at the  top of the leaderboard . We now restrict object embeds to our CDN, and hope to block all object embeds once more broad support for the  clipboard API  is in place. . Note: The file that that was fetched in the above bounty submission was returned with a  content-type  of  image/png . Unfortunately, Flash has a bad habit of desperately wanting to execute things and will gleefully execute if the response vaguely looks and quacks like a Flash file :rage  . . Unlike the directives we have talked about so far,  img-src  doesn’t often come to mind when talking about security. By restricting where we source images, we limit one avenue of sensitive data exfiltration. For example, what if an attacker were able to inject an  img  tag like this? . A tag with an unclosed quote will capture all output up to the next matching quote. This could include security sensitive content on the pages such as: . The resulting image element will send a request to  http://some_evilsite.com/log_csrf?html=...some_csrf_token_value... . An attacker can leverage this “dangling markup” attack to exfiltrate CSRF tokens to a site of their choosing. There are a number of types of dangling markup which could lead to the similar exfiltration of sensitive information, but CSP’s restrictions helps to reduce the tags and attributes that can be targeted. . As was noted above, GitHub has JavaScript that performs DOM modification by automatically fetching a URL from tags with a specific CSS class. We never intended to insert content sourced from anywhere besides GitHub.com, but until we added support for the  connect-src  directive, nothing was restricting the origin of the rendered response. Our current policy dramatically reduces the attack surface by limiting JavaScript connections to a small set of domains. We have recently further locked down our  connect-src  policy by adding support for dynamic policy additions. Historically, it has been relatively tedious to make dynamic changes to our policy per endpoint (i.e. we didn’t do it). But, with some  recent development  by  @oreoshake  to the  Secure Headers library , it is now much easier for us going forward. For example, connections to  api.braintreegateway.com  only occur on payment related pages. We can now enforce a unique exception to our policy, appending the third-party host only on pages that need to connect to the payment endpoint. Over time we hope to lock down other unique connection endpoints using dynamic CSP policies. . By limiting where forms can be submitted we help mitigate the risk associated with injected  form  tags. Unlike the “dangling markup” attack described above for image tags, forms are even more nuanced. Imagine an attacker is able to inject the following into a page: . Sitting below the injection location is a form like: . Since the injected form has no closing  &lt;/form&gt;  tag we have a situation where the original form is nested inside of the injected form. Nested forms are not allowed and browsers will prefer the topmost form tag. So, when a user submits the form they will export their CSRF token to an attacker, subsequently allowing an attacker to perform a CSRF attack against the user. . Similarly, there happens to be a relatively obscure feature of  button  elements: . By limiting  form-action  to a known set of domains we don’t have to think nearly as hard about all the possible ways form submissions might exfiltrate sensitive information. Support for  form-action  is probably one of the most effective recent additions to our policy, though adding it was not without challenges. . When we considered what might break in adding support for  form-action , we thought it would roll out cleanly. There were no forms identified that we submitted to an off-site domain. But, as soon as we deployed the “preview policy” (visible only to employees) we found an edge case we hadn’t anticipated. When users authorize an OAuth application they visit a URL like  https://github.com/login/oauth/authorize?client_id=b6a3dd26bac171548204 . If the user has previously authorized the application they are immediately redirected to the OAuth application’s site. If they have not authorized the application they are presented a screen to grant access. This confirmation screen results in a form  POST  to GitHub.com that does a 302 redirect to the OAuth application’s site. In this case, the form submission is to GitHub.com, but the request results in a redirect to a third-party site. CSP considers the full request flow when enforcing  form-action . Because the form submission results in navigation to a site that is not in our  form-action  source list, the redirect is denied. . Recall that we have relied (until recently) on a static policy enforced on every page on GitHub.com. There was no easy way for us to modify the policy dynamically based on the OAuth authorization submission. At first we thought this was a deal breaker and would require us to remove support for  form-action  until we had better support for a dynamic policy. Luckily, we found a work around by using a “meta refresh” redirect. We refactored our OAuth endpoint to redirect to the OAuth application’s site using a meta refresh tag (we have since optimized this to use a faster JS redirect that falls back to the meta refresh if necessary). By avoiding a 302 redirect, CSP only considers the initial form submission and not the subsequent redirect. We are effectively cheating by decoupling the form submission from the redirection. We would eventually like to add support for a dynamic source for our  form-action , but the meta refresh and JavaScript redirection hack allowed us to move forward with our deployment of  form-action . The benefits of this change overwhelmingly outweighed the downsides and we deployed the solution to production last May. . Inline frames (iframes) are a strong security boundary. Each frame enforces same-origin restrictions just as if the framed content were opened in a unique window or tab. However, there are still some small security benefits in restricting which pages we allow to be framed. For example, consider an attacker injecting a frame on GitHub.com. The frame would load an arbitrary website which could subsequently request HTTP Authentication using an HTTP 401 response code. Browsers don’t handle nested contexts and browser dialogs very well. Security savvy users may instantly recognize that GitHub doesn’t use basic authentication or JavaScript  prompt  dialogs, but many users wouldn’t understand the nuance and may be socially engineered into providing their GitHub credentials. Firefox has support for some frame sandbox directives that try to prevent this behavior, such as  allow-modals , but these directives only apply to explicitly sandboxed frames. There is no similar CSP directive that restricts what an arbitrary frame can do regarding modal dialogs. The only current mitigation is to limit the domains that can be framed. . Our current policy globally allows our render domain (used for rendering things such as  STL files ,  image diffs , and  PDFs ). Not long ago we also allowed  self . However,  self  was only used on a single page to preview GitHub Pages sites generated using our automatic generator. Using our recent support for dynamic policy additions, we now limit the  self  source to the GitHub Pages preview page. After some additional testing, we may be able to use a similar dynamic policy for rendering in the future. . This directive effectively replaces the  X-FRAME-OPTIONS  header and mitigates clickjacking and other attacks related to framing GitHub.com. Since this directive does not yet have broad browser support, we currently set both the  frame-ancestors  directive and the  X-FRAME-OPTIONS  header in all responses. Our default policy prevents any framing of content on GitHub.com. Similar to our  frame-src , we use a dynamic policy to allow  self  for previewing generated GitHub Pages sites. We also allow framing of an endpoint used to share Gists via iframes. . Though not incredibly common, if an attacker can inject a  base  tag into the head of a page, they can change what domain all relative URLs use. By restricting this to  self , we can ensure that an attacker cannot modify all relative URLs and force form submissions (including their CSRF tokens) to a malicious site. . Many browser plugins have a less than stellar security record. By restricting plugins to those we actually use on GitHub.com, we reduce the potential impact of an injected  object  or  embed  tag. The  plugin-types  directive is related to the  object-src  directive. As was noted above, once more broad support for the  clipboard API  is in place, we intend to block  object  and  embed  tags. At that point, we will be able to set our  object-src  source list to  none  and remove  application/x-shockwave-flash  from  plugin-types . . We are thrilled with the progress we have made with our CSP implementation and the security protections it provides to our users. Incremental progress has been key to getting our policy, and the underlying browser features, to the maturity it is today. We will continue to expand our use of dynamic CSP policies, as they let us work toward a “least privilege” policy for each endpoint on GitHub.com. Furthermore, we will keep our eyes on  w3c/webappsec  for the next browser feature enabling us to lock things down even more. . No matter how restrictive our policy, we remain humble. We know there will always be a content injection attack vector that CSP does not prevent. We have started to implement mitigations for the gaps we know of, but, it is a work in progress as we look to current research and constant brainstorming to identify loopholes. We would love to write about our work mitigating some of these “post-CSP” edge cases. Once a few more pull requests are merged, we will be back to share some details. Until then, good luck on your own CSP journey. ", "date": "April 12, 2016"},
{"website": "Github-Engineering", "title": "\n\t\t\tRead-only deploy keys\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-06-16-read-only-deploy-keys/", "abstract": " You can now create deploy keys with read-only access. A deploy key is an SSH key that is stored on your server and grants access to a single GitHub repository. They are often used to clone repositories during deploys or continuous integration runs. Deploys sometimes involve merging branches and pushing code, so deploy keys have always allowed both read and write access. Because write access is undesirable in many cases, you now have the ability to create deploy keys with read-only access. .   . New deploy keys created through GitHub.com will be read-only by default and can be given write access by selecting “Allow write access” during creation. Access level can be specified when  creating deploy keys from the API  as well. ", "date": "June 16, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tScripts to Rule Them All\t\t", "author": ["\n\t\tJon Maddox\t"], "link": "https://github.blog/2015-06-30-scripts-to-rule-them-all/", "abstract": " At GitHub we have a lot of software running our product and company. We also have a lot of potential contributing members. Being able to get from  git clone  to an up-and-running project in a development environment is imperative for fast, reliable contributions. A consistent bootstrapping experience across all our projects reduces friction and encourages contribution. . With practically every software project, developers need to perform the following tasks: . At GitHub, we have normalized on a set of script names for all of our projects that individual contributors will be familiar with the second after they clone a project. We call them “Scripts to Rule Them All”. . Here’s a quick mapping of what our scripts are named and what they’re responsible for doing: . Each of these scripts is responsible for a unit of work. This makes them composable, easy to call from other scripts, and easy to understand. . For example,  script/bootstrap  is only responsible for dependency management.  script/setup  initially calls  script/bootstrap  and then has its own code that will set a project to an initial state.  script/test  can be called on its own in a development environment to run tests, but is also called by  script/cibuild  by our CI server.  script/console  will load a development console on its own, but if appended with an environment name, will load the console for that environment. . Another advantage of consistent script names is language agnosticism. This means the scripts themselves can be written in whichever language is appropriate for the maintainers or the project. It also means the conventions can work for projects of varying languages and frameworks. This ensures an individual contributor can do things like bootstrap or run tests without knowing how to do them for a wide range of project types. . For example,  script/bootstrap  might call  bundle install ,  npm install ,  carthage bootstrap , or  git submodule update . . Normalizing on script names not only minimizes duplicated effort, it means contributors can do the things they need to do without having an extensive fundamental knowledge of how the project works. Lowering friction like this is key to faster and happier contributions. . We’ve created  github/scripts-to-rule-them-all  as a home for this pattern. In this repository, you’ll find working examples of scripts that make use of this technique as well as more detailed responsibilities of each script. ", "date": "June 30, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tBenchmarking GitHub Enterprise\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2015-07-22-benchmarking-github-enterprise/", "abstract": " The release of  GitHub Enterprise  2.0 brought more than just new features and support for deployment on Amazon Web Services. It also included a rework of our virtual machine architecture to improve performance and reliability. In making these changes, we used straightforward benchmarking each step of the way. With this data and our trusty set of Unix tools, we were able to discover, debug, and eventually solve several interesting performance issues for our Enterprise customers. . We considered a few benchmarking solutions to test the performance of the all-important  git clone  operation. In the end, we opted for a simple yet effective solution: a script that could run a series of simultaneous  git clone  commands against an Enterprise server. . This approach gave us a good starting point to see how Enterprise might perform under very heavy use. . We ran our script against GitHub.com first to get some initial numbers to compare against the Enterprise virtual machine. . The benchmark script cloned data over HTTPS or SSH and against the  github/gitignore  repository. We picked this repository because it is pretty small yet still has some realistic activity. In the benchmark we ramped up the number of concurrent clones over time in order to see how the VM handled heavier and heavier load. This also served as a way to determine the amount of load the benchmark script itself could generate running from a single client. . After each clone, our script would output a timestamp with the start time, duration of the operation, and exit status of the clone. This was then graphed with  matplotlib  to generate a graph like this: .     .     . On the left are the clone times in milliseconds, plotted on a logarithmic scale. The green boxplot and green marks show the collected samples of clone times.The line inside the green box indicates the median. The top of the box represents the 75th percentile, while the bottom line of the box represents the 25th. The “whiskers” indicate the  interquartile range . Measurements outside that range are plotted separately. . On the right it shows the number of git operations handled per second. This number is identified by the blue line in the graph. Any error in a benchmarking run is indicated as a red cross in the graph. . Benchmarking against GitHub.com provided a baseline to determine how much load our client could generate. Now it was time to gather data against our existing Enterprise 11.10.x VM. We considered this the very minimum baseline performance we would need to maintain in any upgrade (although, of course, our intention was significant improvement over the 11.10.x VM). . For this straightforward benchmarking exercise we set up two identical machines for each Enterprise version. Here is the data for HTTPS and SSH clones against Enterprise 11.10.x: .          . It was now time for our very first benchmark against Enterprise 2 (again, HTTPS first and then SSH). .          . Not good at all! Very high error rates and slow performance. . We hypothesized that the performance problems were largely attributable to those errors, as we observed a significant number of requests timing out after one minute without ever successfully completing. It was time to dig in and see what was going on. . We logged into the Enterprise 2 test instance, fired up  strace  and immediately  discovered many Ruby processes loaded on each clone as part of our post-receive hooks.  @simonsj  recognized that on GitHub.com we were already using a faster hook setup, so we ported that over to Enterprise 2. Although this change helped remove a lot of performance overhead, we were still seeing plenty of errors. Time for more digging. . We went looking through the log files for the components involved in this clone path. On GitHub.com and now on Enterprise 2 (but  not  on Enterprise 11.10.x) we run an internal C daemon called  babeld  to route and serve all our Git traffic across different protocols. So we figured the logs for  babeld  and  HAProxy  (which sits in front of  babeld ) were a good starting point: . The  log_level  attribute in these log lines indicates the type of problem.  ERROR  indicates a problem, the  msg  attribute shows the actual error. Normally it shows the HTTP status that is returned, but a status of  -1  indicates it is not even able to receive a response from upstream. The HTTP call here is hitting a service called  gitauth  (which determines the access levels the user has for a Git operation). Perhaps our benchmark was exhausting  gitauth  in such a way it wasn’t able to reply quickly to our authentication requests. .  babeld  has an internal queue length set for how many outstanding requests it will send along to  gitauth . This limit was set to 16, based on how many  gitauth  workers we were running on GitHub.com at this time. On Enterprise 2, however, we only have two to four of these workers, so a pending queue of 16 was able to overload this backend.  @simonsj  opened a pull request that allowed us to configure the  gitauth  settings as necessary. . We solved the problem during the authentication phase, but we still saw Git clients hang. This meant we had to look at the next step in the process where  babeld  connects to  git-daemon  to run the actual Git command. We noticed timeouts connecting to  git-daemon , indicating that the daemon was not accepting connections. This problem manifested itself in our testing as Git clients hanging indefinitely. And so the debugging continued. . Next we looked at  dmesg  on the test machine. We saw entries like this in the log there, triggered by the firewall: . These appeared to be blocking port resets being sent back to the clients, which could plausibly explain why they were hanging. So we fixed the firewall issue, but yet that  still  didn’t seem to solve all the client hangs. . Next up was  tcpdump , a classic Unix tool which we often use at GitHub to debug network problems. .     . This screenshot from  Wireshark  shows that although we are sending an  HTTP/1.1 200 OK  response, we never actually send any data packets, and a Git client will hang without data that it can properly deserialize. . We realized that we had missed an important clue back in that  dmesg  output. From time to time we would see this entry: . Port 9000 is where  git-daemon  runs, which matched our guess that  git-daemon  was a factor here. More Unix tools were in order and we looked at the output of  netstat -s  to see if we could glean more information. And  netstat  did indeed show us two very useful messages: . Aha! This indicated that we might not actually be accepting connections for  git-daemon  here, which could very well result in those weird errors and hangs all the way back in the client. . At this point  @scottjg  remembered a pull request he wrote months ago for a behavior change for  git-daemon . The problem there seemed to be similar where  babeld  wasn’t able to talk to  git-daemon . This never turned out to be a real issue in production, but definitely seemed related to what we were seeing here. . To confirm this issue, we took a look at  strace  to observe the  listen  call that  git-daemon  was doing. Our hypothesis was that we had a misconfiguration leading to a very small connection backlog. The output below shows the part where  git-daemon  sets up its socket to listen to incoming clients. . And indeed, the  listen(3, 5)  syscall means we listen to file descriptor  3 , but we only allow for a backlog of  5  connections. With such a small backlog setting, if  git-daemon  was even slightly loaded, we would drop incoming connections, which resulted in the dropped  SYN  packets we observed. . After all that debugging and all those fixes — firewall improvements, hook performance improvement, and of course the  git-daemon  backlog changes — we ended up with some new benchmark numbers. .          . Much better. We managed to almost double the throughput for  HTTPS  and tripled it  for  SSH . We also wiped out the errors at the levels of concurrency we benchmarked against. . We’re very happy with the results of these benchmarking tests, and of course even happier we were able to successfully debug the issues we saw. Most importantly, our customers have seen great results.  Spotify  sent us a graph of aggregate system  load  after upgrading to Enterprise2 on AWS: .   ", "date": "July 22, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tCross-platform UI in GitHub Desktop\t\t", "author": ["\n\t\tRob Rix\t"], "link": "https://github.blog/2015-08-19-cross-platform-ui-in-github-desktop/", "abstract": " The comparison graph is the centerpiece of  GitHub Desktop . It both drives the interaction with your branch and shows you the effects of your changes relative to a base branch. . It’s easily the most sophisticated piece of user interface in the app, with explanatory animations revealing the effects of commits, syncs, and merges.     With separate code bases for OS X &amp; Windows converging on a single design, we knew that sharing code would be essential going forward. Sophisticated as the graph is, implementing it twice would have been a significant burden. . Cross-platform code isn’t entirely new to us: both code bases use  git  &amp;  libgit2 , for example. Sharing UI in a way that respects the conventions of each platform is a different matter, however. . Fortunately, the comparison graph had been prototyped as a reusable web component within an  Electron  app. We knew that we could host that implementation within a native web view on OS X, and experiments in Windows proved promising as well. . Here’s how we did it. . Both of the GitHub Desktop implementations reference the comparison graph as a  submodule . This brings in the full HTML/CoffeeScript/SASS sources for the comparison graph and the tutorial, as well as the tests and build scripts. . Much of that is unnecessary to running the app, so a build script compiles the sources down to distribution-ready HTML/JavaScript/CSS files which we embed in the apps. . On OS X, we use Apple’s  WebView  in the UI to integrate the comparison graph. On Windows, our initial explorations with the .NET  WebBrowser  control found it to be unworkable, so we embed  CefSharp  and use its  ChromiumWebBrowser . While this adds to our download size, both  WebView  and  ChromiumWebBrowser  are derived from WebKit, which makes it vastly simpler for us to develop and test changes to the comparison graph. In general someone working on the comparison graph on OS X doesn’t have to worry about whether it will work on Windows, and vice versa. . We  do  have to consider the comparison graph’s API on both platforms, however, since breaking changes there will need to be integrated in both places. Since breaking changes are a result of prior discussion, this is a matter of calling them out in the pull request and making sure that someone familiar with the other platform has had a chance to comment. . We’ve also managed this sort of churn by making a conscious effort to keep the comparison graph API narrow and focused. The general flow is something like this: . The app fetches the current &amp; base branches. . The app asks the comparison graph to draw a comparison. . The comparison graph asks its branch delegate (the app) for a batch of commits along each branch. . The app fetches these commits and sends them off to the comparison graph. . Repeat steps 3 &amp; 4 until the comparison graph has either filled the width of the view (plus a little padding) or reached the branch point (where the branches converge), indicated by the app sending zero commits. . If the comparison graph hasn’t already found the branch point, repeat step 3 through 5 when scrolling or resizing leftwards exposes more of the graph. . The comparison graph can also draw a single branch, or a pull request. These differ somewhat in effect, but the overall flow is very similar. . There are a few additional interactions which the apps need to support: . Since both apps have to implement their side of this flow, they also share the rough architecture surrounding it. Broadly, this involves: . Computing a value representing the state of the graph when something has changed (e.g. the person has made a commit). . Diffing successive states. . Interpreting the differences by asking the comparison graph to perform some action(s). For example, if two states involve different branches, the apps ask the comparison graph to draw a new comparison (or branch, or pull request). If they involve the same branches, but with changes to the commits, the apps instead ask the comparison graph to insert the new commits incrementally. . Although the overall flow and architecture are shared, the implementations are often quite different. For example, although both platforms use WebKit, the bridging APIs are very different. . On OS X, the JavaScriptCore APIs allow us to pass arbitrary objects, functions, and arrays between Objective-C/Swift and JavaScript. Native code can call methods on JavaScript objects, and JavaScript code can call methods on native objects, without any special handling. We can also define protocols to specify which properties of our objects should be bridged. . CefSharp, on the other hand, does not bridge arrays or functions. JavaScript objects can call methods on native objects, but not on the properties of native objects. Native objects cannot call methods on JavaScript objects; you ask the  ChromiumWebBrowser  to evaluate a string of JavaScript source instead. Since the comparison graph is largely asynchronous, requiring both callbacks and arrays, this motivates a set of JavaScript shims baked into the comparison graph at build time. . On load, the shims set themselves up as the comparison graph’s delegate. When the comparison graph asks for more commits, it passes a callback which the commits should be passed to. The shims add this callback to a private table under a computed key, and call into the app, passing that key. Once the app has loaded the commits, it calls the callback, passing the same key and an array of commits. . Since CefSharp doesn’t bridge arrays, this involves serializing the app’s model objects representing the commits into the JSON representation which the comparison graph expects. Compared to the process on OS X (where arrays of commit objects can be bridged directly), this is slightly less convenient, but has the advantage that the comparison graph and the app are mutually insulated from any changes to state that the other might wish to make. . Finally, the app calls into the shims, passing the serialized array of commits, and the shims forward the commits to the comparison graph. . We also employ CSS shims to adjust the comparison graph’s appearance to suit the look and feel of the host platform. For example, on Windows, the buttons have a flat appearance, while on OS X they have a slight gradient. Likewise, text is in Segoe UI on Windows, but Helvetica Neue on OS X. This helps us to ensure that sharing effort between the platform doesn’t result in a lowest common denominator experience.    . Beyond the comparison graph, the tutorial, and some shared architecture, most of the implementation remains distinct on both platforms. Perhaps the most significant benefit of the approach we took was that it enabled us to converge GitHub for Mac and GitHub for Windows into GitHub Desktop  iteratively , rather than as a ground-up rewrite. That in turn enabled us to keep shipping updates while working on GitHub Desktop without having to divide our attention overmuch. . Having iterated our way here, we’re going to iterate onwards. There’s lots more in store for the comparison graph, and lots more that we’d like to share between the platforms. Here’s to 1.1! ", "date": "August 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub implements Subresource Integrity\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-09-18-github-implements-subresource-integrity/", "abstract": " With Subresource Integrity (SRI), using GitHub is safer than ever. SRI tells your browser to double check that our Content Delivery Network (CDN) is sending the right JavaScript and CSS to your browser. Without SRI, an attacker who is able to compromise our CDN could send malicious JavaScript to your browser. To get the benefits of SRI, make sure you’re using a modern browser like Google Chrome. . New browser security features like SRI are making the web a safer place. They don’t do much good if websites don’t implement them though. We’re playing our role, and encourage you to consider doing the same. . You can read more about Subresource Integrity and why we implemented it on the  GitHub Engineering blog . ", "date": "September 18, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tSubresource Integrity\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-09-19-subresource-integrity/", "abstract": " Like many sites, GitHub uses a content delivery network (CDN) to serve static assets such as JavaScript, CSS, and images to our users. The CDN makes web browsing faster by delivering assets from data centers that are geographically close to the end user and by using hardware and software that is optimized for quickly serving static assets. . For companies that go to great lengths to protect their users from Cross-Site Scripting (XSS) vulnerabilities, relying on a CDN to serve JavaScript means relinquishing control over the content of JavaScript and fully trusting the CDN. While it is not often discussed, and while many CDNs have a great security track record, the compromise of a major CDN could be devastating to the security of the hundreds of thousands of sites that depend on it. If our CDN were to be compromised, it could be used to serve malicious JavaScript to all our users, rendering our many XSS mitigations and transport security useless. Content Security Policy is invaluable for protecting against traditional XSS attacks, but it provides no defense against an attacker who can control assets served from whitelisted sources. . A new browser technology called  Subresource Integrity  gives websites more control over the assets their pages fetch from CDNs or other third parties. The website author includes an  integrity  attribute on JavaScript and CSS tags, specifying the cryptographic digest of the resource being loaded from the third party. When the browser fetches the resource, it computes the file’s digest and compares it with the value from the  integrity  attribute. If the values match, the resource is loaded. Otherwise, the browser refuses to load the resource. . Ruby on Rails apps can use   sprockets-rails  support  for computing and adding the  integrity  attribute: . For sites using Subresource Integrity, a compromised CDN is eliminated as a XSS vector. While for many sites this might not seem like the most plausible attack, third party analytics scripts have been hijacked in the past to inject malicious JavaScript. Widespread adoption of Subresource Integrity could have largely prevented the  Great Cannon  attack earlier this year. . GitHub has always been vigilant about XSS and other vulnerabilities, finding and fixing them internally as well as through our  Bug Bounty Program . We were also early adopters of another XSS mitigation, Content Security Policy, and are always working to harden that policy. Now, Subresource Integrity adds another layer of mitigation, further raising the bar for attackers. . New browser security features like Subresource Integrity are making the web a safer place. They don’t do much good if websites don’t implement them though. We’re playing our role, and encourage you to consider doing the same. . Browser support at the time of this article’s publishing: ", "date": "September 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tCounting Objects\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2015-09-22-counting-objects/", "abstract": " The Systems Team at GitHub works to solve complex bugs and performance  bottlenecks at the lowest levels of our infrastructure. Over the past two years  we’ve undertaken a major project to improve the performance of Git network  operations (like clones or fetches) for the repositories we host. . Two years ago, if you cloned a large repository from any Git host, you probably  found yourself waiting several minutes before data started being sent to your  machine, while the server proudly announced that it was  “counting objects” . . However, if you try to clone the equivalent repository hosted on GitHub.com  today, or on a GitHub Enterprise instance, you’ll notice that data starts being  sent immediately and the transfer is only limited by your available bandwidth to  the server. . Inquiring minds may wonder: What are these  objects , why did you have to count  them  every single time , and why are you not counting them anymore? . All data in a Git repository is stored in the shape of a  Directed Acyclic  Graph .  The history of the repository is ordered throughout time by the  way commits are linked to each other. Each commit has a link to its parent, the  commit that came right before it; some commits have two parents, the result of  merging two branches together. At the same time, each commit has a link to a  tree. The tree is a snapshot of the contents of the working directory in the  moment where the commit was created. It contains links to all the files in the  root of your repository, and links to other subtrees which are the folders and  recursively link to more files and subtrees. .   . The result is a highly dense forest of interlinked nodes stored in the shape of  a graph, but essentially  accessed as a key-value store  (each  object in the database is only indexed by the SHA1 of its contents). . When you connect to a Git daemon to perform a fetch, the client and the server  perform a  negotiation . The server shows the tips of all the branches it has,  and the client replies by comparing those against the tips of its own branches.  “I’ve already got all these objects. I want those! And those!” .   . There’s a very specific case of a fetch operation: a fresh clone, where little  negotiation is necessary. The server offers the tips of its branches, and the  client wants  all  of them, because it doesn’t have any. . This is when this whole ordeal starts becoming expensive, because the server  has little information on what to actually send. Git doesn’t keep a definite  list of all objects reachable from the graph, and it cannot send every single  object in its database as a whole, because it could very well be that some of  those objects are not reachable at all in the repository and should be thrown  away instead of sent to the client. The only thing Git knows are the tips of  all branches, so its only option is to walk down the graph, all the way to the  beginning of the history, listing every single object that needs to be sent. . Each commit in the history is loaded and added to the list of objects to send,  and from each commit its tree is also expanded, and blobs and subtrees are also  added to the list. This process scales with the size of the history of the  repository (i.e. the amount of commits) and the actual size of the repository.  The more files that exist on the repository, the longer it takes to iterate  through the trees of each commit to find the few blobs that haven’t been listed  yet. . This is the “Counting Objects” phase, and as you may gather, some repositories  have a pretty big graph, with lots of objects to count. . At GitHub we serve some of the highest-traffic Git repositories on the internet,  repositories which generally are also very large. A clone of the Linux kernel  can take up to 8 minutes of CPU time in our infrastucture. Besides the poor user  experience of having to wait several minutes before you can start receiving data  in a clone, a thundering herd of these operations could significantly impact the  availability of our platform.  This is why the Systems team made it a priority to  start researching workarounds for this issue. . Our initial attempts involved caching the result of every clone in order to  replay it to other clients asking for the same data. We quickly found out that  this approach was not elegant and definitely not scalable. . When it comes to caching clones, the repositories that matter the most are the  most active ones. This makes caching the full result of a clone pointless,  since the state of the repository changes constantly. On top of that, the  initial caching still takes the same unreasonable amount of CPU time, and the  size of the cache is about the same as that of the repository on disk. Each  cached response roughly doubles the amount of disk space needed by the  repository. . Because of these reasons, we decided against pursuing the caching approach.  Generally speaking, caching specific results to queries is a weak approach to  performance in complex systems. What you want to do instead is caching  intermediate steps of the computation, to be able to efficiently answer any kind  of query. In this case, we were looking for a system that would not only allow  us to serve clones efficiently, but also complex fetches. Caching responses  cannot accomplish this. . In 2013 we organized and obviously attended Git Merge in Berlin, the yearly  conference (previously known as GitTogether) where developers of all the  different Git implementations meet, mingle and attempt to cope with the fact  that it’s 2015 and we’re writing version control systems for a living. . There, we were delighted to learn about the “bitmap index” patchset that  Google’s  JGit  engineers had written for their open-source Java  implementation of Git.  Back then the patchset was still experimental, but the  concept was sound and the performance implications were very promising, so we  decided to tread down that path and attempt to port the patchset to Git, with  the hopes of being able to run it in our infrastucture and push it upstream for  the whole community to benefit. . The idea behind bitmap indexes is not particularly novel: it is the same method  that databases (especially relational ones) have used since the 1970s to speed up  their more complex queries. . In Git’s case, the queries we’re trying to perform are  reachability queries :  given a set of commits, what are all of the objects in the graph that can be  reached from those commits?  This is the operation that the “Counting Objects”  phase of a fetch performs, but we want to find the set of reachable objects  without having to traverse every single object on the way. . To do so, we create indexes (stored as bitmaps) that contain the information  required to answer these queries. For any given commit, its bitmap index marks  all the objects that can be reached from it. To find the objects that can be  reached from a commit, we simply look up its bitmap and check the marked bits  on it; the graph doesn’t need to be traversed anymore, and an operation that  used to take several minutes of CPU time (loading and traversing every single  object in the graph) now takes less than 3ms. .   . Of course, a practial implementation of these indexes has many details to work  around: . First and foremost, we cannot create an index for every single commit in the  repository. That would consume too much storage! Instead, we use heuristics to  decide the set of commits that will receive bitmaps. We’re interested in  indexing the most recent commits: the tips of all branches  need  an index,  because those are the reachability computations that we will perform with a  fresh clone. Besides that, we also add indexes to recent commits; it is likely  that their reachability will be computed when people fetch into a clone that is  slightly out of date. As we progress deeper into the commit graph, we decrease  the frequency at which we pick commits to give indexes to. Towards the end of  the graph, we pick one of every 3000 commits. . Last, but not least, we minimize the amount of disk used by the indexes by  compressing the bitmaps. JGit’s implementation originally used Daniel Lemire’s   EWAH Bitmaps , so we ported his C++ implementation to C to be backwards  compatible   [1]  . EWAH Bitmaps offer a very reasonable  amount of compression while still being extremely fast to decompress and work  with. The end result is an index that takes between 5 and 10% of the original  size of the repository — significantly less than caching the result of  fetches, whilst allowing us to speed up  any  fetch negotiation, not only those  of fresh clones. . Once we started working on the Git implementation, we found that JGit’s design  was so dramatically different from Git’s that it was simply unfeasible to port or  translate the patchset. JGit is after all a Java library, while Git is a  collection of small binaries written in C. . What we did instead was take the idea of bitmap indexes and re-implement it  from scratch, using only the on-disk format of JGit’s bitmap indexes as  reference (as to ensure interoperability between the two implementations). . The first iteration of the patchset took about 2 months of work, and was  drastically different from the original JGit implementation. But it seemed to  be compatible. We could open and query bitmap indexes generated by JGit. We did  several rounds of synthetic benchmarks, which showed that our implementation  was slightly faster than JGit’s (mostly because of performance differences  between the languages, not between the different designs). . So we started testing cloning real production repositories in a staging  environment. The benchmarks showed an exciting improvement of 40%… In the  wrong direction. The bitmap-based code was about 40% slower to generate the  packfile for a repository. .   . These results raised many questions: how can a clone operation be significantly  slower than in vanilla Git, when we’ve already tested that the bitmap indexes  are being queried instantly?  Did we really spend 3 months writing 5000 lines of  C that make clones slower instead of faster? Should we have majored in Math  instead of Computer Science? . Our existential crisis vanished swiftly with some in-depth profiling. If you  break down the graph by the amount of time spent during each phase of the  cloning operation you can see that, indeed, we’ve managed to reduce an operation  that took minutes to mere miliseconds, but unfortunately we’ve made another  operation — seemingly unrelated — three times slower. .   . The job of a Version Control System is tracking changes on a set of files over  time; hence, it needs to keep a snapshot of every single file in the repository  at every changeset: thousands of versions of thousands of different files which,  if stored individually, would take up an incredible amount of space on disk. . Because of this, all version control systems use smarter techniques to store  all these different versions of files. Usually, files are stored as the delta  (i.e.  difference) between the previous version of a file and its current one.  This delta is then compressed to further reduce the amount of size it takes on  disk. . In Git’s case, every snapshot of a file (which we call a blob) is stored  together with all the other objects (the tags, commits and trees that link them  together) in what we call a “packfile”. . When trying to minimize the size of a packfile, however, Git mostly disregards  history and considers blobs in isolation. To find a delta base for a given  blob, Git blindly scans for blobs that look similar to it. Sometimes it will  choose the previous version of the same file, but very often it’s a blob from  an unrelated part of the history. . In general, Git’s aggressive approach of delta-ing blobs against  anything  is  very efficient and will often be able to find smaller deltas. Hence the size of  a Git repository on-disk will often be smaller than its Subversion or Mercurial  equivalent. . This same process is also performed when serving a fetch negotiation. Once Git  has the full list of objects it needs to send, it needs to compress them into a  single packfile (Git always sends packfiles over the network). If the packfile  on disk has a given object X stored as a delta against an object Y, it could  very well be the case that object X needs to be sent, but object Y doesn’t.  In  this case, object X needs to be decompressed and delta’ed against an object  that  does  need to be sent in the resulting packfile. . This is a pretty frequent ocurrence when negotiating a small fetch, because  we’re sending few objects, all from the tip of the repository, and these objects  will usually be delta’ed against older objects that won’t be sent. Therefore,  Git tries to find new delta bases for these objects. . The issue here is that we were actually serving  full clones , and our  benchmarks were showing that we were recompressing  almost every single object  in the clone , which really makes no sense. The point of a clone is sending all  the objects in a repository! When we send  all  objects, we shouldn’t need to  find new delta bases for them, because their existing delta bases will be sent  too! . And yet the profiling did not lie. There was something fishy going on the  compression phase. . Very early on we figured out that actually  forking  people’s repositories was  not sustainable. For instance, there are almost 11,000 forks of  Rails   hosted on GitHub: if each one of them were its own copy of the repository, that  would imply an incredible amount of redundant disk space, requiring several  times more fileservers than the ones we have in our infrastructure. . That’s why we decided to use a feature of Git called  alternates .  When you fork a repository on GitHub, we create a shallow copy of it. This copy  has no objects of its own, but it has access to all the objects of an alternate,  a root repository we call  network.git  and which contains the objects for all  the forks in the network. When you push to your repository, eventually we move  over the objects you pushed to the  network.git  root, so they’re already  available in case you decide to create a Pull Request against the original  repository. . With all the repositories in the network sharing the same pool of objects, we  can keep all the objects inside of a single packfile, and hence minimize the  on-disk size of the repository network by not storing all the duplicated  objects between the forks. . Here’s the issue though: when we  delta  two objects inside this packfile (a  packfile that is shared between all the forks of a repository), Git uses the  straightforward, aggressive heuristic of “finding an object that looks alike”.  As stated earlier, this is very effective for minimizing the size of a packfile  on disk, but like all simple things in life, this comes back to bite us. . What happens when you delta an object of a fork against an object of another  fork? Well, when you clone that fork, that object cannot be sent as a delta,  because  its base  is not going to be sent as part of the clone. The object  needs to be blown up (recomposed from its original delta), and then delta’ed on  the fly against an object that is actually going to be sent on the clone. . This is the reason why the “Compressing Objects” phase of our fetches was  recompressing all the objects and taking so long to run: yes, we were sending a  whole repository in the clone, but the objects in that repository were delta’ed  against objects of a different fork altogether. They  did  need to be  recompressed on the fly. Ouch! . We had a solid explanation of why we were seeing all these objects being  recompressed, but something was still off. The “compressing objects” phase is  completely independent of the “counting objects” phase, and our bitmap changes  didn’t affect it at all. . The behavior of this compression phase is dictated by the way we store forks  jointly on disk, so it made no sense that it would take twice as long after our  patch, again considering these code paths were not touched. Why was object  compression suddenly so slow? . Here’s what happens: when traversing the graph during the “Counting Objects”  phase, traditional Git collected metadata about blobs, like their filenames,  sizes and paths. During the compression phase, it used this metadata as part of  its heuristics for finding good delta bases. . However, after three months of work in our bitmap index changes, we could now  find the list of all the objects to send through the bitmap index, without  having to actually load a single one of them. This was great for removing the  intense CPU requirements of the “counting objects” phase, but it had unintended  side effects when it was time to to compress the objects. . When Git noticed that none of the objects on the list could be sent because they  were delta’ed against other objects that were  not  in the list, it was forced  to re-delta these objects. But without any metadata to hint at the size or  contents of the objects, it had to randomly compare them against each other  (obviously by loading them and forfeiting all the benefits that the bitmap  index had in the first place) until it was able to find similar objects to  delta against and piece together a packfile that wasn’t outrageously large. . This is how we were losing all the benefits of our optimization, and in fact  making the process of generating a packfile 40% slower than it was before,  despite the fact that the most expensive phase of the process was essentially  optimized away. . Our dooming performance issue was being caused, again, because we were merging  several forks of a repository in the same packfile, but unfortunately, splitting  these forks into individual packs was not an option because it would multiply up  the storage costs of repositories several times. . After some thought, we agreed that the bitmap index approach was too promising  to give up on, despite the fact it could not run properly with our current  infrastucture. Therefore, we started looking for a workaround in the way we  store the different forks of repositories on disk. . The idea of “forks” is foreign to Git’s storage layer: when creating the  packfile for a network of forks, Git only sees hundreds of thousands of objects  and is not aware of the fact that they actually come from different forks of the  same repository. . However,  we  are aware of the source of the objects as we add them to the  packfile for delta’ing, and we can influence the way that Git will delta these  packfiles. . What we did was “paint” the graph of all forks, starting from the roots, with a  different color for each fork. The “painting” was performed by marking a bit on  each object for each fork that contained the object. .   . With this, it’s trivial to propagate the markings to the descendant objects, and  change the delta-base heuristic to only consider a parent object as a base if  its marking were a strict superset of the markings of the child object (i.e.  if  sending the child object would always imply sending also the parent, for all  forks). . As the markings trickle down, the final packfile will only have objects that can  be sent as-is, because when cloning any of the forks in the network, every  object will only be delta’ed against another object that  will also  be sent in  the clone. . With this new heuristic, we repacked the original repository for our benchmarks  (which in our case meant repacking the whole network of forks that contained the  repository), and ran the packfile generation test again. .   . The first thing we verified is that our heuristics worked, and that we had to  re-compress exactly 0 objects during the “Compressing Objects” phase. That was  the case. As for the performance of the pack generation phase… Well, let’s  just say we hit our performance target. . It took another couple months to work around the kinks in our implementation,  but we eventually deployed the patchset to production, where the average CPU  time spent on Git network operations was reduced by more than 90% — an order of  magnitude faster than the old implementation. . Shortly after our initial deploy, we also started the process of upstreaming  the changes to Git so the whole community could benefit from them. The  bitmap-index patchset   [2]   finally shipped in the much  anticipated  Git 2.0 release , and now Git network operations from most  major Git hosts are enjoying the overhaul. . Since the initial deployment of our patchset more than a year ago, it has  already saved our users roughly a  century  of waiting for their fetches to  complete (and the equivalent amount of CPU time in our fileservers). . But this is only the beginning: now that bitmap indexes are widely available,  they can be used to accelerate many other Git operations. We’ve implemented a  number of such improvements which we already run on production, and we are  preparing them for submission to the open source Git project. . The story behind these second generation optimizations, however, will have to  wait until another day and another blog  post.  .  1.  Daniel Lemire was extremely helpful assisting us with  questions regarding his EWAH compression algorithm, and allowed us to relicense  our C port under the GPLv2 to make it compatible with Git. His state-of-the-art   research on bitmap and array  compression  is used in a lot of open-source and commercial software. .  2.  The final series that made it upstream was  composed of 14 commits. They include thorough documentation of the whole  implementation in their commit messages. The key parts of the design are  explained in    compressed bitmap implementation  ,    add documentation for the bitmap format  ,    add support for bitmap indexes  ,    use bitmaps when packing objects   and    implement bitmap writing  . ", "date": "September 22, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit Concurrency in GitHub Desktop\t\t", "author": ["\n\t\tAmy Palamountain\t"], "link": "https://github.blog/2015-10-20-git-concurrency-in-github-desktop/", "abstract": " Careful use of concurrency is particularly important when writing responsive  desktop applications. Typically, complex operations are executed on background  threads. This results in an app that remains responsive to user input, while  still performing complex tasks. . In  GitHub Desktop , many background threads will  read or write to the same Git repository, at the same time. . However, git is typically not used in a concurrent fashion. When using git via  the command line, operations are executed in a sequential manner. Read or write  operations are performed against git, independently of each other.     During the build of GitHub Desktop, we discovered executing git commands serially  was a one-way ticket to an unresponsive app. For example, waiting  to load diffs until after we’ve counted the number of commits in the history  would result in a slow and unresponsive application. . To maintain correctness and a responsive user interface, we needed a  solution to concurrency control. . GitHub Desktop has two methods of interacting with a git repository. . We would like to use libgit2 for all of our git operations because it is faster and easier to program with. Unfortunately it is not yet a complete implementation, so we use the CLI to fill in the missing functionality.  This poses an interesting problem, in that both  git  and libgit2 have different  approaches to concurrency control. . Git implements a pessimistic approach to concurrency control. Lock files are used to prevent concurrent access to the underlying git objects on disk. When performing an operation against a git object, git will create a  *.lock   file inside the  .git  directory. This signals that the  *  object is locked for  use. Further operations are prevented until the lock is released and the  *.lock  file is deleted. . By contrast libgit2 cannot guarantee objects can safely be shared between threads. Mutable data structures in libgit2 are not thread safe, and operations must be performed carefully. The libgit2 API allows you to compose granular operations together, and granular locking would come at a performance cost. Libgit2 data structures are rarely used in isolation, and concurrency control should be implemented at the level over a collection of fine grained operations or a single unit of work. . GitHub Desktop ships as a native application on both Mac and Windows. The Mac app is implemented in Objective-C, while the Windows app is implemented in C#. Both platforms are implemented in a reactive style, using   Microsoft’s Reactive Extensions (Rx)   and  our own ReactiveCocoa (RAC) .  This allows the composition of background tasks, such as executing git operations.  All git operations are executed asynchronously and across thread boundaries. . To ensure GitHub Desktop executed git operations in a safe, and yet performant manner, we needed a new concurrency model that enabled us to: . Each high level operation GitHub Desktop performs can be thought of as a unit of work.  A single unit of work can be made up of many fine-grained operations. Our units  of work can be categorized as either: . Concurrent and exclusive operations don’t always have  a 1:1 relationship with reading and writing to the underlying repository.  For example, it is safe to write  Git refs  concurrently with other work,  because a ref update is atomic. On the other hand, some read operations may  update caches in an unsafe way, and so those need to be performed exclusively. . GitHub Desktop uses an  AsyncReaderWriterLock  as a queue, upon  which concurrent operations can either be run exclusively or in parallel. Exclusive operations behave  like a barrier, waiting for previously-enqueued work to complete before beginning,  and themselves finishing before any further work starts. . To execute Git operations, the appropriate lock must first be acquired. . Inside GitHub Desktop we define two interfaces. In C# these are  IExclusiveRepositoryConnection.cs  and  IConcurrentRepositoryConnection.cs . While in Objective-C they are defined by  GHExclusiveGitConnection.h  and  GHGitConnection.h . Each of these implementations only allow for git operations which make sense for that lock type. . The  ExclusiveRepositoryConnection  will only define operations which must be performed with exclusive access to the underlying repository object. The same is true of  ConcurrentRepositoryConnections . This means it is impossible to execute exclusive operations concurrently and concurrent operations exclusively. In this way we are able to prevent possible data corruption without a performance trade off. . For concurrent operations, we define a similar class. . Below is an example of how we might execute a fetch, calculate a merge base and create a commit. Each operation is executed asynchronously using Reactive Extensions, and inside either a concurrent or an exclusive lock. In this example, each operation is queued according to the kind of lock requested. Both  Fetch  and  FindMergeBase  will execute concurrently with respect to each other. However,  Commit  will be queued until all currently executing operations have completed. No subsequent operation will execute until the  Commit  has completed. . Unlike a queue of synchronous work as you might find in  Apple’s Grand Central  Dispatch   or  Clojure’s core.async , we treat  our asynchronous and thread-hopping operations as atomic units of work.  This means that even if we relinquish all threads while waiting for some data,  our queue doesn’t actually move onto the next thing until the operation says it’s well and truly completed. . Before these changes, GitHub Desktop suffered from race conditions as units  of work would become interleaved in error. . Since implementing the concurrent/exclusive locks we have seen an improvement  in stability and performance. We now have a way to talk about concurrency control  at a higher level. At the level of a single unit of work. . By carefully managing git concurrency, GitHub Desktop protects your repositories  from possible corruption. The end result is an app that remains responsive,  while putting the integrity of your repository first. ", "date": "October 20, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tLIKE injection\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-11-03-like-injection/", "abstract": " Looking through our exception tracker the other day, I ran across a notice from our slow-query logger that caught my eye. I saw a  SELECT … WHERE … LIKE  query with lots of percent signs in the  LIKE  clause. It was pretty obvious that this term was user-provided and my first thought was SQL injection. . Looking at the code, it turned out that we were using a user-provided term directly in the  LIKE  clause without any checks for metacharacters that are interpreted in this context ( % ,  _ ,   ). . While this isn’t full-blown SQL injection, it got me thinking about the impact of this kind of injection. This kind of pathological query clearly has some performance impact because we logged a slow query. The question is  how much?  . I asked our database experts and was told that it depends on where the wildcard is in the query. With a  %  in the middle of a query, the database can still check the index for the beginning characters of the term. With a  %  at the start of the query, indices may not get used at all. This bit of insight led me to run several queries with varied  %  placement against a test database. . It seems that unsanitized user-provided  LIKE  clauses do have a potential performance impact, but how do we address this in a Ruby on Rails application? Searching the web, I couldn’t find any great suggestions. There are no Rails helpers for escaping  LIKE  metacharacters, so we wrote some. . We then went through and audited all of our  LIKE  queries, fixing eleven such cases. The risk of these queries turned out to be relatively low. A user could subvert the intention of the query, though not in any meaningful way. For us, this was simply a Denial of Service (DoS) vector. It’s nothing revolutionary and it is  not  a new vulnerability class, but it’s something to keep an eye out for. Three second queries can be a significant performance hit and application-level DoS vulnerabilities need to be mitigated. .  Update:  A number of people pointed out that in Rails 4.2, ActiveRecord includes a   sanitize_sql_like   helper for sanitizing LIKE clauses. ", "date": "November 3, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tRunnable Documentation: Code for Humans\t\t", "author": ["\n\t\tMike McQuaid\t"], "link": "https://github.blog/2015-10-06-runnable-documentation/", "abstract": " On GitHub Enterprise we’ve moved our release process to using what we like to call “Runnable Documentation”: a step-by-step series of instructions that can be run by any person without requiring special domain knowledge. When creating and optimizing Runnable Documentation you should apply code refactoring principles to make it better. . Having only a single person with knowledge of how to do something is never a good idea. It provides a single point of failure, puts undue pressure on that person and the knowledge can be lost if something happens to the person. Unfortunately due to specialization and ownership in organizations these single points of failure can creep up on you without realizing. With the GitHub Enterprise release process we’ve made improvements over the last year to ensure that knowledge is distributed, documented and, where possible, automated. . Initially we did have most of the knowledge around the release process in the head of a single person and knew this had to change. After some synchronous conversations we were able to get the first version of their knowledge committed to a repository as step-by-step documentation. You can think of this like the  Initial commit  of some legacy code that lived outside of version control. . At this point it was tempting to change elements of the process after writing it down; “these steps could be reordered”, “we could write a script for that”, “this could be automated”. These were good instincts to have but it was important to not change the process just yet. Like when dealing with an untested legacy codebase you need to focus on testing before refactoring to provide you with three things: . an understanding of how and why the code/process works how it does . validation that the code/process does work how it claims to . a safety-net to ensure that the code/process remains the same after refactoring . It’s probably not useful or practical to write automated unit tests to try and spot errors in documentation but doing some trial runs of the process is the closest human equivalent. . We now had enough documentation in place to walk through a trial run of a GitHub Enterprise patch release. Like running new code for the first time not everything behaved quite as expected. There were edge-cases that weren’t handled, environmental conditions that were assumed and some steps that were unnecessary. . All software has a “minimal optimum version” of the code hiding out of sight. By “minimal optimum” we mean it does everything it needs to consistently with as little code as possible. The same instinct engineers have to try and pare down to this minimal optimum code is the same instinct we took with Runnable Documentation. For example, after a few trial runs through the documentation we were able to cut out a bunch of things, clarify others and work out where it was important to focus time and what could be skimmed over. We resisted the temptation to automate at this time; it’s important to ensure the existing process is 100% understood before trying to automate it. . By this stage we’ve had a working version of the process documented as some Runnable Documentation from a few trial runs. It’s an entirely manual process but we understand the problem sufficiently now to focus on optimization. Let’s think about optimizing code first. The following code isn’t great: . Computers are good at things like running the same command multiple times with varying input data so it’d be better to do something like: . Similarly this documentation isn’t great: . Why are we asking the user to run multiple scripts with the same input each time? A better idea would be to combine them into a single script (or create a new script that calls all three). . Our goal with the Runnable Documentation was to optimize for as little documentation as possible. Why ask the reader to call three scripts if they can call one? Why ask the reader to manually check things if they can be automated? . If you’re really lucky you may be able to replace all your Runnable Documentation with a single script (e.g.  script/release ) or an automated task. We prefer code over documentation (where possible) because it’s much easier to automatically verify that code is working and engineers tend to be more vigilant in fixing broken code than outdated documentation. . GitHub’s Enterprise release process was too complicated for a single script and instead requires multiple, time-separated steps with some human interaction and judgement between them. For example, deciding whether we want to build another prerelease is dependent on judgement on the results of manual QA, the importance of new additions and the risk of destabilizing by changing things too late in the release cycle. . GitHub makes heavy use of Hubot ChatOps internally (see  the previous post on this blog  for more examples) for semi-automated processes where we move from following documentation and manual commands run on developers’ machines to commands run in a chat application. This provides a few benefits: . For example, in GitHub Enterprise (and most release processes) we needed to know what’s changed between the release we’re releasing and the previous one. Knowing these changes allows you to write release notes and ensure QA can focus on relevant areas. While you could have the release person work this out by checking Git repositories this could be better handled by ChatOps that takes the current and previous release versions as arguments and summarizes the pull requests that have gone into this release. . GitHub also provides webhooks. These allow calling external HTTP endpoints on repository events. For GitHub Enterprise’s release process we wrote a small web application called  HookHand  to provide a service for calling arbitrary scripts from webhooks. A particularly useful script uses the GitHub API to automatically update a release dashboard to display all the pull requests that have been merged to the current release’s branch. . This is a small section of runnable documentation from GitHub Enterprise’s release process documentation: . The three commands here were separated as they require a time-gap with human discretion between each. Each command performs multiple tasks which could be done manually but automation makes it less time-consuming and error-prone for the person running the release . We’ve found that having Runnable Documentation has improved the quality, speed and accessibility of the GitHub Enterprise release process. Now anyone can jump in and help out with a release, see the current state on a dashboard and analyze differences between releases without needing to be walked through it by other developers or perform any setup on their local machine. ", "date": "October 6, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tLarge Scale DDoS Attack on github.com\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2015-03-27-large-scale-ddos-attack-on-github-com/", "abstract": " We are currently experiencing the largest DDoS ( distributed denial of service ) attack in github.com’s history. The attack began around 2AM UTC on Thursday, March 26, and involves a wide combination of attack vectors. These include every vector we’ve seen in previous attacks as well as some sophisticated new techniques that use the web browsers of unsuspecting, uninvolved people to flood github.com with high levels of traffic. Based on reports we’ve received, we believe the intent of this attack is to convince us to remove a specific class of content. . We are completely focused on mitigating this attack. Our top priority is making sure github.com is available to all our users while deflecting malicious traffic. Please watch  our status site  or follow  @githubstatus  on Twitter for real-time updates. ", "date": "March 27, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tEight lessons learned hacking on GitHub Pages for six months\t\t", "author": ["\n\t\tBen Balter\t"], "link": "https://github.blog/2015-04-27-eight-lessons-learned-hacking-on-github-pages-for-six-months/", "abstract": " Believe it or not, just over a year ago, GitHub Pages, the documentation hosting service that powers nearly three-quarters of a million sites, was little more than a 100-line shell script. Today, it’s a fully independent, feature-rich OAuth application that effortlessly handles well over a quarter million requests per minute. We wanted to take a look back at what we learned from leveling up the service over a six month period. .  GitHub Pages  is GitHub’s static-site hosting service. It’s used by government agencies like the White House to  publish policy , by big companies like  Microsoft ,  IBM , and  Netflix  to showcase their open source efforts, and by popular projects like  Bootstrap ,  D3 , and  Leaflet  to host their software documentation. Whenever you push to a specially named branch of your repository, the content is run through  the Jekyll static site generator , and served via its own domain. . At GitHub, we’re a big fan of eating our own ice cream (some call it  dogfooding ).  Many of   us have   our own ,  personal sites   hosted on   GitHub Pages , and many GitHub-maintained projects like  Hubot  and  Electron , along with sites like  help.github.com , take advantage of the service as well. This means that when the product slips below our own heightened expectations, we’re the first to notice. . We like to say that there’s a Venn diagram of things that each of us are passionate about, and things that are important to GitHub. Whenever there’s significant overlap, it’s win-win, and GitHubbers are encouraged to find time to pursue their passions. The recent improvements to GitHub Pages, a six-month sprint by a handful of Hubbers, was one such project. Here’s a quick look back at eight lessons we learned: . Before touching a single line of code, the first thing we did was create integration tests to mimic and validate the functionality experienced by users. This included things you might expect, like making sure a user’s site built without throwing an error, but also specific features like supporting different flavors of Markdown rendering or syntax highlighting. . This meant that as we made radical changes to the code base, like replacing the shell script with a fully-fledged Ruby app, we could move quickly with confidence that everyday users wouldn’t notice the change. And as we added new features, we continued to do the same thing, relying heavily on unit and integration tests, backed by real-world examples (fixtures) to validate each iteration. Like the rest of GitHub, nothing got deployed unless all tests were green. . One of our goals was to push the Pages infrastructure outside the GitHub firewall, such that it could function like any third-party service. Today, if you view  your OAuth application settings  you’ll notice an entry for GitHub Pages. Internally, we use the same public-facing Git clone endpoints to grab your site’s content that you use to push it, and the same public-facing repository API endpoints to grab repository metadata that you might use to build locally. . For us, that meant adding a few public APIs, like the inbound  Pages API  and outbound   PageBuildEvent  webhook . There’s a few reasons why we chose to use exclusively public APIs and to deny ourselves access to “the secret sauce”. For one, security and simplicity. Hitting public facing endpoints with untrusted user content meant all page build requests were routed through existing permission mechanisms. When you trigger a page build, we build the site as you, not as GitHub. Second, if we want to encourage a strong ecosystem of tools and services, we need to ensure the integration points are sufficient to do just that, and there’s no better way to do that than to put your code where your mouth is. . Developing a service is vastly different than developing an open source project. When you’re developing a software project, you have the luxury of semantic versioning and can implement radical, breaking changes without regret, as users can upgrade to the next major version at their convenience (and thus ensure their own implementation doesn’t break before doing so). With services, that’s not the case. If we implement a change that’s not backwards compatible, hundreds of thousands of sites will fail to build on their next push. . We made several breaking changes. For one, the Jekyll 2.x upgrade switched the default Markdown engine, meaning if users didn’t specify a preference, we chose one for them, and that choice had to change. In order to minimize this burden, we decided it was best for the user, not GitHub, to make the breaking change. After all, there’s nothing more frustrating than somebody else “messing with your stuff”. . For months leading up to the Jekyll 2.x upgrade users who didn’t specify a Markdown processor would get an email on each push, letting them know that Maruku was going the way of the dodo, and that they should  upgrade to Kramdown , the new default, at their convenience. There were some pain points, to be sure, but it’s preferable to set an hour aside to perform the switch and verify the output locally, rather than pushing a minor change, only to find your entire site won’t publish and hours of frustration as you try to diagnose the issue. . We made a big push to improve the way we communicated with GitHub Pages users. First, we began pushing  descriptive error messages when users’ builds failed , rather than an unhelpful “page build failed” error, which would require the user to either build the site locally or email GitHub support for additional context. Each error message let you know exactly what happened, and exactly what you needed to do to fix it. Most importantly, each error included a link to  a help article specific to the error you received . . Errors were a big step, but still weren’t a great experience. We wanted to prevent errors before they occurred. We created the  GitHub Pages Health Check  and silently ran automated checks for common DNS misconfigurations on each build. If your site’s DNS wasn’t optimally configured, such as being pointed to a deprecated IP address, we’d let you know before it became a problem. . Finally, we wanted to level up our documentation to prevent the misconfiguration in the first place. In addition to overhauling all our GitHub Pages help documentation, we reimagined  pages.github.com  as a tutorial quick-start, lowering the barrier for getting started with GitHub Pages from hours to minutes, and published  a list of dependencies , and what version was being used in production. . This meant that every time you got a communication from us, be it an error, a warning, or just a question, you’d immediately know what to do next. . While GitHub Pages is used for all sorts of crazy things, the service is all about creating  beautiful user, organization, and project pages to showcase your open source efforts on GitHub . Lots of users were doing just that, but ironically, it used to be really difficult to do so. For example, to list your open source projects on an organization site, you’d have to make dozens of client-side API calls, and hope your visitor didn’t hit the API limit, or leave the site while they waited for it to load. . We exposed  repository and organization metadata to the page build process , not because it was the most commonly used feature, but because it was at the core of the product’s use case. We wanted to make it easier to do the right thing — to create great software, and to tell the world about it. And we’ve seen a steady increase in open source marketing and showcase sites as a result. . If we did our job right, you didn’t notice a thing, but the GitHub Pages backend has been completely replaced. Whereas before, each build would occur in the same environment as part of a worker queue, today, each build occurs in its own Docker-backed sandbox. This ensured greater consistency (and security) between builds. . Getting there required a cross-team effort between the GitHub Pages, Importer, and Security teams to create  Hoosegow , a Ruby Gem for executing untrusted Ruby code in a disposable Docker sandbox. No one team could have created it alone, nor would the solution have been as robust without the vastly different use cases, but both products and the end user experience are better as a result. . Expectations are a powerful force. Everywhere on GitHub you can expect @mentions and emoji to “just work”. For historical reasons, that wasn’t the case with GitHub Pages, and we got many confused support requests as a result. Rather than embark on an education campaign or otherwise go against user expectations, we implemented  emoji  and  @mention support  within Jekyll, ensuring an expectation-consistent experience regardless of what part of GitHub you were on. . The only thing better than meeting expectations is exceeding them. Traditionally, users expected about a ten to fifteen minute lag between the time a change was pushed and when that change would be published. Through our improvements, we were able to significantly speed up page builds internally, and by sending a purge request to our third-party CDN on each build, users could see changes reflected in under ten seconds in most cases. . Jekyll may have been originally created to power GitHub Pages, but since then, it has become its own independent open source project with its own priorities. GitHubbers have always been part of the Jekyll community, but if you look at the most recent activity, you’ll notice  a sharp uptick in contributions , and  many new contributors from GitHub . . If you use open source, whether it’s the core of your product or a component that you didn’t have to write yourself, it’s in your best interest to play an active role in supporting the open source community, ensuring the project has the resources it needs, and shaping its future. We’ve started “open source Fridays” here at GitHub, where the entire company takes a break from the day-to-day to give back to the open source community that makes GitHub possible. Today, despite their beginnings, GitHub Pages needs Jekyll, not the other way around. . Throughout all these improvements, the number of GitHub Pages sites has grown exponential, with just shy of a three-quarters of a million user, organization, and project sites being hosted by GitHub Pages today. .   . But the number of sites tells only half the story. Day-to-day use of GitHub Pages has also seen similar exponential growth over the past three years, with about 20,000 successful site builds completing each day as users continuously push updates to their site’s content. .   . Last, you’ll notice that when we introduced page build warnings in mid-2014, to proactively warn users about potential misconfigurations, users took the opportunity to improve their sites, with the percentage of failed builds (and number of builds generating warnings) decreasing as we enter 2015. . GitHub Pages is a small but powerful service tied to every repository on GitHub. Deceivingly simple, I encourage you to  create your first GitHub Pages site today , or if you’re already a GitHub Pages expert,  tune in this Saturday  to level up your GitHub Pages game. . Happy publishing! ", "date": "April 27, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe GitHub Engineering Blog\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2015-05-19-the-github-engineering-blog/", "abstract": " We are happy to introduce GitHub’s  Engineering Blog  to the world. Starting today, you can read details about our infrastructure, learn about our development practices, and hear about the knowledge we’ve gained while running the world’s largest code collaboration platform. . You can also get updates by following our Engineering Twitter account  @GitHubEng . . Happy reading! ", "date": "May 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tHello World\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2015-05-19-hello-world/", "abstract": " What does it take to run and build GitHub? On our new Engineering Blog we’ll show you how it’s done. . Through the writings of our engineers you’ll gain insight into the practices we use, the software we build, and the engineering principles we follow. We believe sharing the knowledge we’ve gained over time is an important contribution to our awesome  community. . Thanks for stopping by, we’re happy you’re here. ", "date": "May 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tWorkload Analysis with MySQL’s Performance Schema\t\t", "author": ["\n\t\tNatalie Siskin\t"], "link": "https://github.blog/2015-05-19-using-mysql-performance-schema-for-workload-analysis/", "abstract": " Earlier this spring, we  upgraded our database cluster to MySQL 5.6 . Along with  many other improvements , 5.6 added some exciting new features to the performance schema. . MySQL’s  performance schema  is a set of tables that MySQL maintains to track internal performance metrics. These tables give us a window into what’s going on in the database—for example, what queries are running, IO wait statistics, and historical performance data. . One of the tables added to the performance schema in 5.6 is  table_io_waits_summary_by_index . It collects statistics per index, on how many rows are accessed via the storage engine handler layer. This table already gives us  useful insights  into query performance and index use. We also import this data into our metrics system, and displaying it over time has helped us track down sources of replication delay. For example, our top 10  most deviant  tables: .   . MySQL 5.6.5  features  another summary table:  events_statements_summary_by_digest . This table tracks unique queries, how often they’re executed, and how much time is spent executing each one. Instead of  SELECT id FROM users WHERE login = 'zerowidth' , the queries are stored in a normalized form:  SELECT `id` FROM `users` WHERE `login` = ? , so it’s easy to group queries by how they look than by the raw queries themselves. These query summaries and counts can answer questions like “what are the most frequent UPDATES?” and “What SELECTs take the most time per query?”. . When we started looking at data from this table, several queries stood out. As an example, a single UPDATE was responsible for more than 25% of all updates on one of our larger and most active tables,  repositories :   UPDATE `repositories` SET `health_status` = ? WHERE `repositories` . `id` = ? . This column was being updated every time a health status check ran on a repository, and the code responsible looked something like this: . Just to be sure, we used  scientist  to measure how often the column needed to be updated (had the status changed?) versus how often it was currently being touched: .   . The measurements showed what we had expected: the column needed to be updated less than 5% of the time. With a simple code change: . The updates from this query now represent less than 2% of all updates to the  repositories  table. Not bad for a two-line fix. Here’s a graph from  VividCortex {: data-proofer-ignore=”true”}, which shows query count data graphically: .   . GitHub is a 7-year-old rails app, and unanticipated hot spots and bottlenecks have appeared as the workload’s changed over time. The performance schema has been a valuable tool for us, and we can’t encourage you enough to check it out for your app too. You might be surprised at the simple things you can change to reduce the load on your database! . Here’s an example query, to show the 10 most frequent  UPDATE  queries: ", "date": "May 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tBrowser Monitoring for GitHub.com\t\t", "author": ["\n\t\tEmily Nakashima\t"], "link": "https://github.blog/2015-05-19-browser-monitoring-for-github-com/", "abstract": " Most large-scale web applications incorporate at least some browser monitoring, collecting metrics about the user experience with JavaScript in the browser, but, as a community, we don’t talk much about what’s working here and what’s not. At GitHub, we’ve taken a slightly different approach than many other companies our size, so we’d like to share an overview of how our browser monitoring setup works. . Browser monitoring can present an optimization problem: if you care enough to want to monitor your users’ in-browser experience, you probably also are the sort of company who cares deeply about security, privacy, and performance. Browser monitoring can be an amazing window onto your users’ experience, but it also typically presents tradeoffs on all three fronts, since it often relies on loading external scripts and sending user data to a third-party application for collection and display. . For GitHub.com, we already know many common third-party solutions aren’t a fit because of our security settings. GitHub.com sets fairly restrictive Content Security Policy (CSP) headers as a preventative measure against Cross-Site Scripting (XSS) attacks. These headers prevent inline JavaScript (including code inside HTML  &lt;script&gt;  tags) from running in modern browsers that support CSP. We also whitelist specific domains that remote scripts can load from, like our assets CDN, and try to keep that list of allowed script sources to a minimum. This automatically rules out some common real-user monitoring (RUM)/browser monitoring tools that work by writing request-specific data (like server response times) into each HTML page in a  &lt;script&gt;  tag, and it makes other solutions that rely on third-party scripts less appealing, since we’d need to add another script source and trust that the script poses a low enough risk to performance and security. . As application engineers, we’re lucky to work alongside an Infrastructure team that is also obsessed with monitoring and metrics — and has built a flexible graphing, monitoring and alerting toolchain on top of open source technologies like Graphite, Nagios, and D3. While none of our tools were built with client-side monitoring in mind, they were built in a way that was flexible enough that we can add browser monitoring with only slighty more effort than it takes to add backend stats — we already collect stats from the main GitHub.com Rails application, so collecting client-side stats just requires adding JavaScript to collect that data and post it back to the Rails app, plus Rails code to clean and filter unexpected values before sending the rest to our stats infrastructure. . Like most other sites of our size, we want to understand how fast our content is loading for end users, so we measure every browser page load. While some third-party tools keep things simple and sum up page load performance in one number (usually time to window load) or sometimes two numbers (usually one number for backend application and network time, and one number for browser rendering and subresource loading time), we prefer a bit more granularity. Because we write our own metrics collection JavaScript, it’s fairly easy to move beyond collecting one or two headline numbers, and instead use the browser’s  Navigation Timing API  to collect data from most points in the page load process: .   . We take these timings and graph the deltas between them in a stacked graph. We have an overall graph (below), and we also segment the stats by a few other metrics, like browser (Safari, Firefox, Internet Explorer) and page (the repositories#show page, or the blob#edit page). .   . While this means collecting a lot more data than just time to window load, we benefit by being able to make a graph that helps diagnose performance problems, rather than just identify that there  is  a performance problem. The graph shows the timings stacked from earliest to latest, and we can often narrow down the source of performance problems by looking for the first layer that starts to increase in size. For example, a slow backend application could show up in the “request” or “response” layer, but a slow image CDN would show up in the “domComplete” layer, and a performance regression in the initial execution time of a JS framework might show up in the “domInteractive” layer. . We still support some older browsers that don’t implement the Navigation Timing API, so we also collect a simulated time-to-window-load number, or “cross-browser load time.” This is calculated in JavaScript — we save a “simulated navigation start” timestamp to the browser’s sessionStorage on  pagehide  (when a user navigates away from or closes a page). On the next page load, we calculate the time between the start and the window  load  event. This gives us our “one big number” that we can put on dashboards and reports if we want to, and it also helps us track that the experience in browsers without Navigation Timing support isn’t too different than in browsers with support. . While GitHub.com isn’t a single-page app, we load a lot of content with  Pjax , a jQuery plugin that uses AJAX and  https://developer.mozilla.org/en-US/docs/Web/Guide/API/DOM/Manipulating_the_browser_history#The_pushState()_method “&gt;pushState to reload only a portion of the page when a user clicks a link. Pjax takes the place of a standard browser navigation, but it’s usually about 1000ms faster. From a user’s perspective, a pjax load  is  a page load, but from the browser’s perspective it’s just another ajax request, which presents an issue. The goal of pjax is to improve user-perceived performance, but we can’t collect our normal page load metrics to make sure performance is actually better. . To measure pjax timing, we use a similar strategy to our “simulated navigation start” measurement. Pjax makes this easy by firing the same events at the start and end of each navigation:  'pjax:start'  and  'pjax:end' . When we hear the first event, we record a start timestamp. When we hear the second event, we wait a tick with a call to  setImmediate  and then record an end timestamp. Since user-facing feature code might also be listening for the  'pjax:end'  event to do DOM manipulation or data processing, waiting until the next tick lets us include that time in our measure and end up with a number that’s more analogous to the time to the browser’s  domContentLoaded  event. . There are some downsides to replacing browser navigations with pjax loads — there’s no default loading indicator, unlike a standard browser navigation where you’d see a loading indicator in the nav bar, and if a pjax request times out or throws an error, we fall back to a standard full-page navigation, so users who trigger that behavior get stuck with a extra-slow page load. This means that being able to compare pjax and standard page loads for the same content is important — if we can quantify how much time we’re saving users, it can help us confirm that we’re really improving the experience by switching more links over to pjax behavior. .   . Another major focus of our browser monitoring is JavaScript errors. Our deployment process favors small, frequent deploys over big-bang releases, and for small changes or urgent fixes, the time between committing changes and having them deployed to production can be only a few minutes. Deploying small sets of changes helps reduce the risk associated with each deployment, but the higher frequency means that we’re unlikely to do exhaustive cross-browser regression testing before each deploy, so it helps to have monitoring to detect when things go wrong. . GitHub maintains an internal application called Haystack, an error reporting tool that we use to view GitHub.com application-level errors. Haystack wasn’t built with JavaScript errors in mind, but since it’s already displaying errors from the GitHub Rails application, if we can send JS errors back to the Rails app, we can send JavaScript errors to Haystack as well. . The client-side JavaScript for collecting the errors is fairly simple. We have a single global event handler for uncaught JS errors, and most of the information we report comes from the Error object itself or the Event that triggered the error, including the error message, file name, line number, stack trace, and event target. . JavaScript error reports are always a bit noisy — browser extension code can trigger errors that look legitimate; nightly or experimental browser builds may have broken APIs, etc. An important part of keeping our JavaScript error reports actionable has been to filter out any errors we don’t think are related to our code as soon as these errors get to the Rails app. Most of the time, the number of errors we filter is larger than the number we keep and send to Haystack. We filter anything with a browser extension file in the stack trace, since it’s likely related to the extension behavior, and anything that matches a blacklist of specific errors we’ve tracked down (e.g. a particular popup blocker extension). There are always a handful of one-off errors that may not be worth digging into, but we can usually spot real problems by looking for errors happening multiple times per hour to more than one user. . At this point, some might ask, “Why not just write browser tests, so you can catch errors and regressions before you deploy them?” It’s a great question — I definitely don’t see client-side monitoring as a replacement for integration or acceptance tests. But tests almost never capture the variety in real production data or the variety in real users’ browser versions. Browser monitoring captures all of these by default. Plus, tests only capture errors  when you run them , which is usually when you’re committing code changes, but the environment around front end code changes all the time. We always want to know about new errors our users are running into, regardless of whether they come from us deploying new code or from the Google Chrome team rolling out a new browser version. . While the overhead of maintaining stats collection and graphing infrastructure may not make sense for every company, for us, there have been some neat benefits to our approach. For one, the fact that our browser monitoring uses the same tools that other teams use for backend and network stats means that there’s a low barrier to getting other people in the company interested in front end metrics. As someone who writes front end code, I want to collect network-related Navigation Timing metrics like  domainLookupStart ,  domainLookupEnd ,  connectStart  and  connectEnd  so I can confirm that front end performance issues really are browser code issues, not network issues, but once these stats show up in Graphite, they’re there for everyone in the company to look at. That means it’s trivial for the Infrastructure team to start showing any of these metrics on their dashboards if they, for example, decide these metrics could be helpful for monitoring the impact of DDoS attacks on end users. . Another benefit to rolling our own browser monitoring code is that it allows us to collect new kinds of information fairly easily, if we can format it like existing stats. In the past year, engineers on the team have added a handful of new kinds of metrics as they’ve thought of them, usually in under 100 lines of code. We’ve started tracking support for new browser features (like support for the User Timing API or support for natively displaying emoji) with code similar to our performance metrics code, and we’ve started tracking accessibility issues by throwing special types of JavaScript errors for them and using our existing JavaScript error reporting to display them in Haystack alongside our other JavaScript errors. . While our browser monitoring tools give us fairly good visibility into our users’ experience on GitHub.com, they’re only effective when we look at them regularly — there’s more we could do to catch front end issues faster. Alerts are the next step I’d like to add on our path to more operable front end code. The GitHub Infrastructure team has done some amazing work to allow engineers to get alerts from different parts of our infrastructure in different channels. I’d like to be able to hook our browser metrics into the alerting tools we use for our backend metrics, so we could get a notification in chat if front end performance seems to be slightly slower or get paged if it seems like there’s been a major performance regression. ", "date": "May 19, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tRearchitecting GitHub Pages\t\t", "author": ["\n\t\tCharlie Somerville\t"], "link": "https://github.blog/2015-05-27-rearchitecting-github-pages/", "abstract": "  GitHub Pages , our static site hosting service, has always had a very simple architecture. From launch up until around the beginning of 2015, the entire service ran on a single pair of machines (in active/standby configuration) with all user data stored across 8 DRBD backed partitions. Every 30 minutes, a cron job would run generating an  nginx map file  mapping hostnames to on-disk paths. . There were a few problems with this approach: new Pages sites did not appear until the map was regenerated (potentially up to a 30-minute wait!); cold nginx restarts would take a long time while nginx loaded the map off disk; and our storage capacity was limited by the number of SSDs we could fit in a single machine. . Despite these problems, this simple architecture worked remarkably well for us — even as Pages grew to serve thousands of requests per second to over half a million sites. . When we started approaching the storage capacity limits of a single pair of machines and began to think about what a rearchitected GitHub Pages would look like, we made sure to stick with the same ideas that made our previous architecture work so well: using simple components that we understand and avoiding prematurely solving problems that aren’t yet problems. . The new Pages infrastructure has been in production serving Pages requests since January 2015 and we thought we’d share a little bit about how it works. .   . After making it through our load balancers, incoming requests to Pages hit our frontend routing tier. This tier comprises a handful of Dell C5220s running nginx. An  ngx_lua  script looks at the incoming request and makes a decision about which fileserver to route it to. This involves querying one of our MySQL read replicas to look up which backend storage server pair a Pages site has been allocated to. . Once our Lua router has made a routing decision, we just use nginx’s stock  proxy_pass  feature to proxy back to the fileserver. This is where ngx_lua’s integration with nginx really shines, as our production nginx config is not much more complicated than: . One of the major concerns we had with querying MySQL for routing is that this introduces an availability dependency on MySQL. This means that if our MySQL cluster is down, so is GitHub Pages. The reliance on external network calls also adds extra failure modes — MySQL queries performed over the network can fail in ways that a simple in-memory hashtable lookup cannot. . This is a tradeoff we accepted, but we have mitigations in place to reduce user impact if we do have issues. If the router experiences any error during a query, it’ll retry the query a number of times, reconnecting to a different read replica each time. We also use ngx_lua’s shared memory zones to cache routing lookups on the pages-fe node for 30 seconds to reduce load on our MySQL infrastructure and also allow us to tolerate blips a little better. . Since we’re querying read replicas, we can tolerate downtime or failovers of the MySQL master. This means that existing Pages will remain online even during database maintenance windows where we have to take the rest of the site down. . We also have Fastly sitting in front of GitHub Pages caching all 200 responses. This helps minimise the availability impact of a total Pages router outage. Even in this worst case scenario, cached Pages sites are still online and unaffected. . The fileserver tier consists of pairs of Dell R720s running in active/standby configuration. Each pair is largely similar to the single pair of machines that the old Pages infrastructure ran on. In fact, we were even able to reuse large parts of our configuration and tooling for the old Pages infrastructure on these new fileserver pairs due to this similarity. . We use  DRBD  to sync Pages site data between the two machines in each pair. DRBD lets us synchronously replicate all filesystem changes from the active machine to the standby machine, ensuring that the standby machine is always up to date and ready to take over from the active at a moment’s notice — say for example if the active machine crashes or we need to take it down for maintenance. . We run a pretty simple nginx config on the fileservers too – all we do is set the document root to  $http_x_github_pages_root  (after a little bit of validation to thwart any path traversal attempts, of course) and the rest just works. . Not only are we now able to scale out our storage tier horizontally, but since the MySQL routing table is kept up to date continuously, new Pages sites are published instantly rather than 30 minutes later. This is a huge win for our customers. The fact that we’re no longer loading a massive pre-generated routing map when nginx starts also means the old infrastructure’s cold-restart problem is no longer an issue. .   . We’ve also been really pleased with how ngx_lua has worked out. Its performance has been excellent — we spend less than 3ms of each request in Lua (including time spent in external network calls) at the 98th percentile across millions of HTTP requests per hour. The ability to embed our own code into nginx’s request lifecycle has also meant that we’re able to reuse nginx’s rock-solid proxy functionality rather than reinventing that particular wheel on our own. ", "date": "May 27, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tDeploying branches to GitHub.com\t\t", "author": ["\n\t\tAman Gupta\t"], "link": "https://github.blog/2015-06-02-deploying-branches-to-github-com/", "abstract": " At GitHub, we use a variant of the  Flow  pattern to deploy changes: new code is always deployed from a pull request branch, and merged only once it has been confirmed in production.  master  is our stable release branch, so everything on  master  is considered production-ready code. If a branch deploy ships bad code containing a bug or performance regression, it is rolled back by deploying the latest  master  to production. . Using this workflow, engineers at GitHub  deploy changes to our website  several hundred times every week. . All deployments happen in chat via  Hubot commands , which ensures that everyone in the company (from development to operations to support) has visibility into changes that are being pushed into production. . Deployments are reported  to the GitHub API  and show up in the timeline on corresponding pull requests. .   . Recent deployments are also available through chat. . Over the years, we’ve built a number of deployment features into Hubot and Heaven (our Capistrano-based deployment API) to help streamline our process. Below are some of our favorites. . During peak work hours, multiple developers are often trying to deploy their changes to production. To avoid confusion and give everyone a fair chance, we can ask Hubot to add us to the deployment queue. . We can also check the status of the queue, to deploy directly if it’s empty or to find a less busy time if it’s looking particularly full. . We can also unqueue ourselves if something comes up and we have to step away from the computer. . To ensure bad code cannot make it to production, Hubot won’t let us deploy a branch until continuous integration tests have run. This prevents trigger-finger deploys while CI is still running. . Similarly, if CI completed but our branch failed some tests, Hubot will prevent us from deploying. . Since  master  is our stable release branch, we want to ensure that any branches being deployed are caught up with the latest code in  master . Before proceeding with a deployment, Hubot will detect if our branch is behind and automatically merge in  master  if required. . To ensure deployments are visible to the rest of the team, Hubot forces us to deploy from specific chat rooms. . In rare emergency situations, it is possible to override these guards using  /deploy! . . As soon as a branch is deployed, Hubot locks the environment so no other branches can be deployed to it. This prevents others from accidentally deploying while a developer is testing their branch. . Once we’ve merged our branch, Hubot will automatically unlock the environment and let the next person in the queue know they can deploy. . We can also manually unlock deploys to let someone else have a turn, when we decide not to merge our branch just yet. . Finally, during outages, attacks, and other emergency situations, we can lock deployments manually to prevent changes while we investigate problems. . In addition to the main production environment, we can deploy to staging servers that are only accessible by GitHub staff. This staging environment closely mirrors our production environment, including real-world datasets to ensure high-fidelity testing. . To find out what environments are available for deployments, we can ask Hubot for a list and see which ones are currently unlocked. . The lab, garage and other staging environments each replicate different aspects of production: frontend web workers, background job queues, CDN setup for assets, Git fileserver workers, etc. Depending on what part of the stack a branch touches, we can pick a matching staging environment to exercise the new code without affecting production user traffic. . One of these environments is a special “branch lab” which does not require locking, because it sets up an isolated sandbox for each branch. This helps avoid deploy lock contention and lets developers and designers deploy experimental UI changes as shareable URLs they can send to others in the company for feedback. . The branch lab is implemented as a single staging server which runs one unicorn worker per branch. The branches deployed there can be listed via chat, and a branch can be deleted once it’s no longer being used. If the free memory on that server starts to run out, we automatically prune the oldest branches to free up some space. . We can also manually remove branches that we’re done testing, or have shipped to production already: . Once a branch has passed automated tests, undergone code-review, and been verified in staging, it comes time to push it into production. Recall that GitHub engineers are not allowed to merge any pull request that has not yet been verified in production. Production traffic patterns and datasets often trigger edge-cases that expose bugs and performance issues which might not have been seen otherwise, and we want to ensure that our  master  branch always represents our stable production release. . To safely roll out a risky branch, we can ask Hubot to deploy it to a specific subset of servers within an environment. This limits the user impact of the change, and allows us to monitor for new exceptions or performance regressions coming from the servers that are running our branch. . A change to the Rails version for example can be deployed to one or two frontend webservers, and if things look good we can continue to deploy it to more frontends. Similarly, an upgraded version of Git could be deployed to a handful of backend fileservers. . Once we’ve gained confidence in our branch, we can deploy it to all of production and then merge it to unlock deployments for the next developer in the queue. . Our deployment chatops and workflows work so well that we use them for everything in the company. If we want to add a DNS record, we make a pull request to github/dns and use  /deploy dns . If we want to add a monitoring alert for a new service, we make a pull request to github/nagios and use  /deploy nagios . If we want to install a new software package on a specific frontend, we use  /deploy puppet/add-package-branch to prod/fe142 . We even use similar workflows to ship new versions of our  native desktop apps . . If you aren’t already, we highly recommend you try some of the techniques mentioned in this blog post. This workflow brings a ton of great benefits, including: ", "date": "June 2, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tBrubeck, a statsd-compatible metrics aggregator\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2015-06-15-brubeck/", "abstract": " One of the key points of GitHub’s engineering culture —and I believe, of any good engineering culture— is our obsession with aggressively measuring everything. . Coda Hale’s seminal talk  “Metrics, Metrics Everywhere”  has been a cornerstone of our monitoring philosophy. Since the very early days, our engineering team has been extremely performance-focused; our commitment is to ship products that perform as well as they possibly can ( “it’s not fully shipped until it’s fast” ), and the only way to accomplish this is to reliably and methodically measure, monitor and graph every single part of our production systems. We firmly believe that metrics are the most important tool we have to  keep GitHub fast . . In our quest to graph everything, we became one of the early adopters of Etsy’s   statsd  .  statsd  is a Metrics (with capital M) aggregation daemon written in Node.js. Its job is taking complex Metrics and aggregating them in a server so they can be reported to your storage of choice (in our case,  Graphite ) as individual datapoints. .  statsd  is, by and large, a good idea. Its simple design based around an UDP socket, and the very Unix-y, text-based format for reporting complex metrics makes it trivial to measure every nook and cranny of our infrastructure: from the main Rails app, where each server process reports hundreds of individual metrics for each request, to the smallest, most trivial command-line tool that we run in our datacenter, as reporting a simple metric is just one  nc -u  invocation away. . However, like most technologies, the idea behind  statsd  only stretches up to a point. Three and a half years ago, as we spun up more and more machines to keep up with GitHub’s incessant growth, weird things started to happen to our graphs. Engineers started complaining that specific metrics (particularly, new ones), were not showing up on dashboards. For some older metrics, their values were simply too low to be right. . In the spirit of aggressively monitoring everything, we used our graphs to debug the issue with our graphs.  statsd  uses a single UDP socket to receive the metrics of  all our infrastructure : the simplicity of this technical decision allowed us to build a metrics cluster quickly and without hassle, starting with a single metrics aggregator back when we had very limited resources and very limited programmer time to grow our company. But this decision had now come back to bite us in the ass. A quick look at the  /proc/net/dev -generated graphs on the machine made the situation very obvious: slowly but steadily over time, the percentage of UDP packets that were being dropped in our monitoring server was increasing. From 3% upwards to 40%. We were dropping almost half of our metrics! . Before we even considered checking our network for congestion issues, we went to the most probable source of the problem: the daemon that was actually receiving and aggregating the metrics. The original  statsd  was a Node.js application, back in the days when Node.js was still new and with limited tooling available. After some tweaking with V8, we managed to profile  statsd  and tracked down the bottleneck to a critical area of the code: the UDP socket that served as only input for the daemon. The single-thread event-based nature of Node was a rather poor fit for the single socket; its high throughput made polling essentially a waste of cycles, and the reader callback API (again, enforced by Node’s event based design) was causing unnecessary GC pressure by creating individual “message” objects for each received packet. . Node.js has always been a foreign technology at GitHub (over the years we’ve taken down the few critical Node.js services we had running, and we’ve rewritten them in languages we have more expertise working with). Because of this, rather than trying to dig into  statsd  itself, our first approach to fix the performance issues was a more pragmatic one: . Since one  statsd  process was bottlenecking on CPU, we decided to load balance the incoming metrics between several processes. It quickly became obvious, however, that load-balancing metric packets is not possible with the usual tools (e.g. HAproxy) because the consistent hashing must be performed on the metric name, not the source IP. . The whole point of a metric aggregator is to unify metrics so they get reported only once to the storage backend. With several aggregators running, balancing metrics between them based on their source IP will cause the same metric to be aggregated in more than one daemon, and hence reported more than once to the backend. We’re back to the initial problem that  statsd  was trying to solve. . To work around this, we wrote a custom load balancer that parsed the metric packets and used consistent hashing on the metric’s name to balance it between a cluster of StatsD instances. This was an improvement, but not quite the results we were looking for. .   . With 4  statsd  instances that couldn’t handle our metrics load and a load balancer that was already essentially “parsing” the metrics packets, we decided to take a more drastic  approach and design the way we were aggregating metrics from a clean slate by writing a  statsd -compatible daemon called Brubeck. . Taking an existing application and rewriting it in another language very rarely gives good results. Especially in the case of a Node.js server, you go from having an event loop written in C (libuv, the cross-platform library that powers the Node.js event framework is written in C), to having, well… an event loop written in C. . A straight port of  statsd  to C would hardly offer the performance improvement we required. Instead of micro-optimizing a straight port to squeeze performance out of it, we focused on redesigning the architecture of the application so it became  efficient , and then implemented it as simply as possible: that way, the app will run fast even with few optimizations, and the code will be less complex and hence more reliable. . The first thing we changed in Brubeck was the event-loop based approach of the original  statsd . Evented I/O on a single socket is a waste of cycles; while receiving 4  million packets per second, polling for read events will give you unsurprisingly predictable results: there is always a packet ready to be read. Because of this, we replaced the event loop with several worker threads sharing a single listen socket. . Several threads working on aggregating the same metrics means that access to the metrics table needs to be synchronized. We used a modified version of a  concurrent, read-lock-free hash table with optimistic locking on writes  optimized for applications with a high read-to-write ratios, which performs exceedingly well for our use case. . The individual metrics were synchronized for aggregation with a simple spinlock per metric, which also fit our use case rather well, as we have almost no contention on the individual metrics and perform quick operations on them. . The simple, multi-threaded design of the daemon got us very far: during the last two years, our single Brubeck instance has scaled to up to 4.3 million metrics per second (with no packet drops even at peak hours — as we’ve always intended). . On top of that, last year we added a lot more headroom to our future growth by enabling the marvelous  SO_REUSEPORT , a new flag available in Linux 3.9+ that allows several  threads to bind the same port simultaneously instead of having to race for the same socket, having the kernel round-robin between them. This decreases user-land contention incredibly, simplifies our code and sets us up for almost NIC-bound performance. The only possible next step would be skipping the kernel network stack, but we’re not quite there yet. . After three years running in our production servers (with very good results both in performance and reliability), today we’re finally  releasing Brubeck as open-source . .  Brubeck  is a thank-you gift to the monitoring community for all the insight and invaluable open-source tooling that has been developed over the last couple years, and without which bootstrapping and keeping GitHub running and monitored would have been significantly more difficult. . Please skim the technical documentation of the daemon, carefully read and acknowledge its limitations, and figure out whether it’s of any use to you. . Happy monitoring! ", "date": "June 15, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tException Monitoring and Response\t\t", "author": ["\n\t\tJustin Palmer\t"], "link": "https://github.blog/2015-06-16-exception-monitoring-and-response/", "abstract": " Like most software applications, GitHub can generate a few exceptions.  Incoming exceptions range from system-level issues including Git timeouts and missing references, to application-level issues including simple code mistakes and JavaScript errors. . We take stability and performance seriously, so we need a way to quickly identify issues as they surface, determine the best teams or individuals to ping, and ship any relevant changes as soon as possible.  Haystack helps us do that. .   . Haystack, our internal exception tracking application, is a product of 6 years of development.  We use it to monitor practically every user-facing property we build, including our desktop applications. . A typical scenario begins with deploying a change. . Once an engineer deploys via chatops,  Hubot  will ping them with a link to a firehose, which is a view that will show only exceptions that happen after their changes are live, reminding them to keep an eye on exceptions.  Meanwhile, Haystack has been continuously monitoring the health of all applications—looking for exception rates that differ from recent typical rates. While we hope engineers will keep an eye on the firehose, asking someone to “keep an eye on Haystack” is like asking them to watch grass grow via Livestream.  That’s why we need a bit of  anomaly detection. . We have a couple simple rules for anomaly detection that serve most of our needs for frontline defense against bad deploys or typical errant behavior.   We ensure the number of incoming exceptions meets a minimum threshold, since even on our best days we are generating  some  noise. We then fetch a histogram of the last hour of exception counts to determine if the last point is beyond a configured  z-score .  This doesn’t work well in cases where exceptions have been elevated for a large chunk of the hour, but at that point, we’ve likely elevated the severity of the issue and multiple people are investigating.  We want to augment a developer’s ability to responsibly monitor their changes, not offload it completely to machines. .   . If an exception elevation is detected, Hubot will ping the last deployer with some general statistics about the application’s current state.  The immediate benefit of this ping is that it implicitly sets a responsible party.  In a perfect world, only the person who might have caused the spike in exceptions should receive the ping.  We’re not in a perfect world just yet, but we’re iterating on it. . Let’s take a quick peek to see how things are looking. . Looks like there are a few Git-related issues cropping up, but we can get more detail by checking out the Haystack dashboard. .   . It looks like  GitRPC  is timing out.  GitRPC::Timeout  exceptions are raised when a Git operation on one of our fileservers takes too long to complete. As you can imagine, there are many places in our application where this could happen.  We need to quickly identify what’s happening, where it’s happening, and who we should ping. .   . We can examine this by looking at the Rollup detail.  Rollups represent a group of similar needles, usually determined by hashing some common properties between needles.  By looking at the Rollup, I can immediately see how many exceptions have been coming in over time, the users experiencing it, the repositories it’s happening the most on, the routes and servers it’s being served from, and scrolling further down I can see the individual needles coming in.  But, if you look at the top of this image, you can see these purple and blue items.  These are what we refer to “Areas of Responsibility” or AORs. .   . Areas of Responsibility help us in a couple of ways.  They define instantly recognizable high-level areas of the site, and within each of those, a set of teams generally responsible for their well being and upkeep.  Beyond that, we track exception rates for AORs in Graphite, so we can see any degradation over time. .   . Given we established earlier that we use Git in a lot of places, there are a lot of AORs attached to this rollup.  We can, however, see a bunch of identical needles coming in recently and if we click on one of them, we get more details.   In particular, we can see that something is up with the  :pulse  area of the site. It happened in the action  RepositoriesController#contributors_size  while the  unicode-3-profile  branch was deployed. This is a small fraction of the information we can get about this exception. .   . Another helpful bit of this exception page is the formatted backtrace where you can quickly see application code highlighted in orange.  We also strip away any prefix paths and link lines in the backtrace to their respective source files on GitHub.com.  Each app has a simple YAML configuration file where these rules can be defined. . Now we’re on the right track.  We know what happened, when it happened, how bad it was, which team to notify, and where to look to fix it. If we’re able to fix the problem, we can create a Pull Request or Issue and drop the link to Haystack in the body or a future comment, and a cross-reference will automatically get created in Haystack.  This allows engineers to jump back and forth between the issue and needle in addition to signaling to others who might look at the needle, that progress is happening.  Finally, if this rollup crops up again in the future, there will likely be an existing discussion about the cause and how to address it. . Haystack has existed in some form or another since ~2009. I don’t recall what the ecosystem was like then, but it wasn’t what it is today.  When we started working on updating Haystack in early 2012, we were focused on improving on our existing application.  Over the years we’ve made a lot of changes to Haystack,  many changes tied to other internal applications and the way we work, to get what you see today.  The value of Haystack comes from the deep integration into our systems so it would be difficult to open-source, but we hope you can apply similar patterns to your infrastructure for better monitoring, alerting, and debugging. ", "date": "June 16, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tSecurity vulnerability in bash addressed\t\t", "author": ["\n\t\tShawn Davenport\t"], "link": "https://github.blog/2014-09-25-security-vulnerability-in-bash-addressed/", "abstract": "  Update: 2014-09-29 23:10 UTC  .  We have published an update to the Git Shell tools for GitHub for Windows, which resolves the  bash  vulnerabilities   CVE-2014-6271 ,  CVE-2014-7169 ,  CVE-2014-7186  and  CVE-2014-7187 . If you are running GitHub for Windows, we strongly encourage you to upgrade. You can check if you are on the latest version, and upgrade if needed, by opening “Tools” -&gt; “About GitHub for Windows…”  .  Update: 2014-09-28 17:30 UTC  .  Two new  bash  vulnerabilities,  CVE-2014-7186  and  CVE-2014-7187 , have been discovered. We have now released special patches of GitHub Enterprise using the latest upstream  bash  fix for CVE-2014-7186 and CVE-2014-7187. Upgrade instructions have been sent to all GitHub Enterprise customers, and we strongly encourage all customers to upgrade their instance using this latest release.  GitHub.com  remains unaffected by this vulnerability.  .  Update: 2014-09-26 00:22 UTC  .  Security patches released yesterday for the  bash  command vulnerability identified in CVE-2014-6271 turned out to be incomplete, and a new vulnerability,  CVE-2014-7169 , was identified. We have now released special patches of GitHub Enterprise using the latest upstream  bash  fix for CVE-2014-7169. Upgrade instructions have been sent to all GitHub Enterprise customers, and we strongly encourage all customers to upgrade their instance using this latest release.  GitHub.com  remains unaffected by this vulnerability.  .  Update: 2014-09-25 15:45 UTC  .  GitHub is closely monitoring  new developments  that indicate the existing  bash  patch for CVE-2014-6271 is incomplete. The fix for this new  bash  vulnerability is still in progress, but we will be releasing a new patch for GitHub Enterprise once it has been resolved. At this time, we still strongly encourage all GitHub Enterprise customers to update their instances using the patch made available yesterday.  . This morning it was disclosed that Stephane Chazelas discovered a  critical vulnerability  in the  GNU bash  utility present on the vast majority of Unix and Linux systems. Using this vulnerability, an attacker can force the execution of arbitrary commands on an affected server. While these commands may not run with root privileges, they provide a significant vector for further exploitation of a system. . We have released special patches of GitHub Enterprise to fix this vulnerability, and have provided detailed instructions to all our Enterprise customers on how to upgrade their instance. An immediate upgrade is required. . None of the extensive penetration testing we’ve performed today has uncovered any vulnerability on  GitHub.com , including  git  over SSH. As an added precaution, however, we have patched all systems to ensure the vulnerability is addressed. ", "date": "September 25, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Pages Legacy IP Deprecation\t\t", "author": ["\n\t\tBen Balter\t"], "link": "https://github.blog/2014-11-05-github-pages-legacy-ip-deprecation/", "abstract": "  Update:  We’ve extended the deprecation deadline to  February 2, 2015  to give Pages users more time to update their DNS records. . If you use a custom domain with GitHub Pages, please verify that your domain’s DNS settings are properly configured to point to the most up-to-date GitHub IP addresses. This will ensure that your site remains available after December 1st, 2014. .  GitHub Pages  allows you to  set up a custom domain  by  adding the domain to a  CNAME  file , and pointing your domain’s DNS record to GitHub’s servers. If you don’t use this feature, for example, if your GitHub Pages site is published as  username.github.io , you don’t need to take any action at this time. Please enjoy  this animated GIF  for being awesome. . Nearly a year ago,  we announced improvements to how we serve GitHub Pages sites . Today we’re making that change permanent by deprecating our old GitHub Pages infrastructure. If your custom domain is pointed at these legacy IPs, you’ll need to update your DNS configuration immediately to keep things running smoothly. . Starting the week of  November 10th , pushing to a misconfigured site will result in a build error and you will receive an email stating that your site’s DNS is misconfigured. Your site will remain available to the public, but changes to your site will not be published until the DNS misconfiguration is resolved. . For the week of  November 17th , there will be a week-long brownout for improperly configured GitHub Pages sites. If your site is pointed to a legacy IP address, you will receive a warning message that week, in place of your site’s content. Normal operation will resume at the conclusion of the brownout. . Starting  December 1st , custom domains pointed to the deprecated IP addresses will no longer be served via GitHub Pages. No repository or Git data will be affected by the change. . If you have a GitHub Pages site pointed at one of the old IP addresses, you will receive an email from us this week letting you know that you need to make the change (and should have been receiving an email on each push for the past several months). If the suspense is killing you, there’s a few ways to check yourself: . If you’re using the GitHub Pages Gem,  update to the latest version , and run  github-pages health-check  from your site’s root directory. That’ll make sure your site’s DNS is in ship-shape. . Don’t have the GitHub Pages Gem?    If you’re on a Mac or Linux machine, simply paste this command into a terminal window, replacing  your-domain.com  with, your site’s domain.  dig your-domain.com | grep -E '(207.97.227.245|204.232.175.78|199.27.73.133)' || echo \"OK\" . If you see the word “OK”, you’re all set.   On a Windows machine, you’ll want to run  nslookup your-domain.com  and ensure that the output does not include any of the deprecated IP addresses (207.97.227.XXX, 204.232.175.XX, or 199.27.73.XXX).     . If you’re on a Mac or Linux machine, simply paste this command into a terminal window, replacing  your-domain.com  with, your site’s domain.  dig your-domain.com | grep -E '(207.97.227.245|204.232.175.78|199.27.73.133)' || echo \"OK\" . If you see the word “OK”, you’re all set. . On a Windows machine, you’ll want to run  nslookup your-domain.com  and ensure that the output does not include any of the deprecated IP addresses (207.97.227.XXX, 204.232.175.XX, or 199.27.73.XXX). . From your domain registrar’s web interface, head on over to your domain’s DNS settings. Your domain should either be a  CNAME  record to  username.github.io , an  ALIAS  record, or an  A  record pointing to an IP address that begins  192.30.252.XXX . . If one of the methods above indicate that your DNS is misconfigured, or if you just want to be sure, please follow the instructions for  setting up a custom domain with GitHub Pages . . Questions?  We’re here to help . . Happy publishing! ", "date": "November 5, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tVulnerability announced: update your Git clients\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2014-12-18-vulnerability-announced-update-your-git-clients/", "abstract": " A  critical Git security vulnerability has been announced today , affecting all versions of the official Git client and all related software that interacts with Git repositories, including GitHub for Windows and GitHub for Mac. Because this is a client-side only vulnerability,  github.com  and GitHub Enterprise are not directly affected. . The vulnerability concerns Git and Git-compatible clients that access Git repositories in a case-insensitive or case-normalizing filesystem. An attacker can craft a malicious Git tree that will cause Git to overwrite its own  .git/config  file when cloning or checking out a repository, leading to arbitrary command execution in the client machine. Git clients running on OS X (HFS+) or any version of Microsoft Windows (NTFS, FAT) are exploitable through this vulnerability. Linux clients are not affected if they run in a case-sensitive filesystem. .  We strongly encourage all users of GitHub and GitHub Enterprise to update their Git clients as soon as possible , and to be particularly careful when cloning or accessing Git repositories hosted on unsafe or untrusted hosts. . Repositories hosted on  github.com  cannot contain any of the malicious trees that trigger the vulnerability because we now verify and block these trees on push. We have also completed an automated scan of all existing content on  github.com  to look for malicious content that might have been pushed to our site before this vulnerability was discovered. This work is an extension of the data-quality checks we have always performed on repositories pushed to our servers to protect our users against malformed or malicious Git data. . Updated versions of  GitHub for Windows  and  GitHub for Mac  are available for immediate download, and both contain the security fix on the Desktop application itself  and  on the bundled version of the Git command-line client. . In addition, the following updated versions of Git address this vulnerability: . More details on the vulnerability can be found in the  official Git mailing list announcement  and on the   git-blame  blog . ", "date": "December 18, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tImproving GitHub’s SSL setup\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2014-12-23-improving-github-s-ssl-setup/", "abstract": " To keep GitHub as secure as possible for every user, we will remove RC4 support in our SSL configuration on   github.com  and in the GitHub  API  on January 5th 2015. . RC4 has a number of cryptographic weaknesses that may be exploited, impacting the security of your data. More details about these vulnerabilities are listed in the  current IETF draft . . If you are using Internet Explorer on Windows XP, you will no longer be able to access  github.com  once this change takes place. Windows XP only supports outdated SSL ciphers, is no longer supported by Microsoft, and contains a known  critical security problem  in its SSL implementation. . We strongly recommend that Windows XP users upgrade to a newer version of Windows. If this is not possible, you will need to use  Chrome  or  Firefox  to access GitHub on Windows XP. The  git  client available at  git-scm.com  still works on Windows XP. ", "date": "December 23, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow to write the perfect pull request\t\t", "author": ["\n\t\tKeavy McMinn\t"], "link": "https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/", "abstract": " As a company grows, people and projects change. To continue to nurture the culture we want at GitHub, we’ve found it useful to remind ourselves what we aim for when we communicate. We recently introduced these guidelines to help us be our best selves when we collaborate on pull requests. . These guidelines were inspired partly by Thoughtbot’s  code review  guide. . Our guidelines suit the way we work, and the culture we want to nurture. We hope you find them useful too. . Happy communicating! ", "date": "January 21, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow GitHub uses GitHub to document GitHub\t\t", "author": ["\n\t\tGaren Torikian\t"], "link": "https://github.blog/2015-01-06-how-github-uses-github-to-document-github/", "abstract": " Providing well-written documentation helps people understand, make use of, and contribute back to your project, but it’s only half of the documentation equation. The underlying system used to serve documentation can make life easier for the people writing it—whether that’s just you or the team you work with. . The hardest part about documentation should be deciding which words to use, not configuring tools or figuring out how to deploy updates. Members  of the GitHub Documentation Team come from backgrounds where crude XML-based authoring tools and complicated CMSs are the norm. We didn’t want to use those tools here so we’ve spent a good deal of time configuring our own documentation workflow and set-up. . We’ve talked before about how we use GitHub to  build  GitHub; here’s a look at how we use  GitHub Pages  to serve  our GitHub Help documentation  to millions of readers each month. . A few months ago, we migrated our Help site from a custom-built  Rails app  to a static  Jekyll site  hosted on GitHub Pages. Our previous Help site consisted of two separate repositories: . Our Rails app was hosted on a third-party service; as updates were made to the code, we deployed them  with Hubot and Chatops , as we do with the rest of GitHub. . Our typical writing workflow looked like this: . Here’s an example conversation from  @neveret  and  @bernars  showing a bit of our normal editing workflow: .   . Working with pull requests was fantastic, because it directly matched  the GitHub flow  we use across the company. And we liked writing in Markdown, because its syntax enabled us to effectively describe new features in no time. . However, our Rails implementation was a fairly complicated setup: . We knew we could do much better. . When  Jekyll 2.0 was released , we saw an opportunity to replace our existing setup with a static site. The new  Collections document type  lets you define a file structure that matches your needs. In addition, Jekyll 2.0 introduced support for  Sass  and  CoffeeScript  assets, which simplifies writing front-end code. . Open source is great because it’s, well, open. As we migrated to Jekyll,  we made several pull requests to components of Jekyll , making it a better tool for users of GitHub Pages. . Very little of our original workflow has changed. We still write in Markdown and we still open pull requests for an editorial review. When the pull request is merged, the GitHub Pages site is automatically built and deployed within seconds. . Here’s a quick rundown on how we’re using core Jekyll features and a handful of plugins to implement the help site. . We intentionally rely on core Jekyll code as much as possible, to minimize our reliance on maintaining custom plugins. . Jekyll 2.0 introduced a new plugin type called a  Converter  that transforms any markup into HTML. This frees the writer up to compose content however she chooses, and Jekyll will just serve the final HTML. For example, you can  write your posts in AsciiDoc , if that’s your thing. . To that end, we wrote  jekyll-html-pipeline , an implementation of our own open-source  html-pipeline . This ensures that the content on our Help site looks the same as content everywhere on GitHub. We also wrote  our own Markdown filter  to provide some syntax extensions that make writing documentation much easier. . With the previous Rails site, we were using an  ElasticSearch  provider that indexed our database and implemented a search system for our Help site. . Now, we use  lunr-js  to provide a faster client-side search experience. In sifting through our analytics, we found that the vast majority of our users relied on an external search provider to get to our documentation. It didn’t make sense, during or after the migration, to expend much energy on a server-side search solution. . The Docs team really wanted to use “content references,” or  conrefs , when writing documentation. A conref allows you to write a chunk of text once and reuse it throughout the site. (The idea was borrowed from  the DITA standard .) . The old Rails app wouldn’t permit us to write reusable content, but now we can with the power of Jekyll’s  data files . For example, we’ve defined a file called  conrefs.yml , and have a set of key-value strings that look something like this: . Our keys are grouped by specificity ( repositories.create_new ); the values they contain are just plain Markdown (“In the upper-right corner…”). We can now reuse this single step across several pages of content that refer to creating a new repository by writing the appropriate Liquid syntax: . As GitHub’s UI evolves, we might need to change the image or rewrite the directional pointer. With a conref, we only have to make the change in  one  location, rather than a dozen. . Another goal of the move was to be able to provide versioned Help documentation. With  the release of Enterprise 2.0.0 , we began to provide different content sets for  the previous 11.10.340  and  the current 2.0  releases. In order to do that, we build the Jekyll site with a special audience flag, and check in the generated HTML as part of our Pages repository. . For example, in our  config.yml  file, we set a key called  audience  to  11.10.340 . If a feature exists that’s available in Enterprise 2.0 but not 11.10.340, we demarcate the section using Liquid tags like this: . Again, this is just taking advantage of core features in Jekyll; we didn’t need to build or maintain any aspect of this. .    . Just because the site is static doesn’t mean that we should avoid test-driven  development  writing. . Our first line of defense for testing content has always been  html-proofer . This tool helps verify that none of our links and images are broken by quickly validating every URL in our built site. . Rubyists are familiar with using  Capybara  to simulate website interactions in their tests. Would it be crazy to implement a similar idea with our static site? Nope! Our own  @bkeepers  wrote  a blog post four years ago  talking about this very problem. With that, we were able to write stronger tests that covered our content and our site behavior. For example, we check that a referenced conref is valid (by looking up the key in the YAML file) or that our JavaScript is functioning properly. . Our Help documentation runs with CI to ensure that nothing broken ever gets in front of our readers: .   . As mentioned above, our new Pages implementation is significantly faster than the old Rails app. This is partly because the site is a bunch of static HTML files—nothing is fetched from a database. More significantly,  we’ve already spent a lot of time configuring our Pages servers to be blazing fast  for everyone. The same advantages we have, like serving assets off of a CDN, are also available to every GitHub user. .   . Documentation teams across GitHub can take advantage of the GitHub Flow, Jekyll 2.0, and GitHub Pages to produce high-quality documentation. The benefits that GitHub Pages provides to our Documentation team is already available to any user running a GitHub Pages site. . With our move to Pages, we didn’t rebuild any new components. We spent far less time building anything and more time discussing a workflow that made sense for our team and company. By committing to using the same hosting features we provide to every GitHub user, we were able to provide better documentation, faster. Our internal workflow has made us more productive, and enabled us to provide features we never could before, such as versioned content. . If you have any questions on our setup, past or present,  we’re happy to help ! ", "date": "January 6, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Security Bug Bounty program turns one\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2015-01-28-github-security-bug-bounty-program-turns-one/", "abstract": " It’s already been a year since  we launched  the  GitHub Security Bug Bounty , and, thanks to bug reports from researchers across the globe, 73 previously unknown security vulnerabilities in our applications have been identified and fixed. . Of  1,920  submissions in the past year,  869  warranted further review, helping us to identify and fix vulnerabilities fitting nine of the  OWASP top 10  vulnerability classifications.  33  unique researchers earned a cumulative  $50,100  for the  57  medium to high risk vulnerabilities they reported. .   . We also saw some incredibly involved and creative vulnerabilities reported. . Our top submitter,  @adob ,  reported  a persistent  DOM based cross-site scripting vulnerability , relying on a previously unknown  Chrome browser bug  that allowed our  Content Security Policy  to be bypassed. . Our second most prolific submitter,  @joernchen ,  reported  a complex vulnerability in the communication between two of our backend services that could allow an attacker to set arbitrary environment variables. He followed that up by finding a way to achieve arbitrary remote command execution by setting the right environment variables. . To kick off our Bug Bounty Program’s second year, we’re doubling the maximum bounty payout, from $5000 to $10000. If you’ve found a vulnerability that you’d like to submit to the GitHub security team for review,  send us the details , including the steps required to reproduce the bug. You can also follow  @GitHubSecurity  for ongoing updates about the program. . Thanks to everyone who made the first year of our Bug Bounty a success. Happy hunting in 2015! ", "date": "January 28, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tKeeping GitHub OAuth Tokens Safe\t\t", "author": ["\n\t\tNeil Matatall\t"], "link": "https://github.blog/2015-02-05-keeping-github-oauth-tokens-safe/", "abstract": " While making your source code available in a public GitHub repository is awesome, it’s important to be sure you don’t accidentally commit your passwords, secrets, or anything else that other people shouldn’t know. . Starting today you can commit more confidently, knowing that we will email you if you push one of your  OAuth Access Tokens  to any public repository with a  git push  command. As an extra bonus, we’ll also revoke your token so it can’t be used to perform any unauthorized actions on your behalf. . For more tips on keeping your account secure, see “ Keeping your SSH keys and application access tokens safe ” in GitHub Help. ", "date": "February 5, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit 2.3 has been released\t\t", "author": ["\n\t\tMichael Haggerty\t"], "link": "https://github.blog/2015-02-06-git-2-3-has-been-released/", "abstract": " The Git developers have just released a major new version of the Git command-line utility, Git 2.3.0. . As usual, this release contains many improvements, performance enhancements, and bug fixes. Full details about what’s included can be found in the  Git 2.3.0 release notes , but here’s a look at what we consider to be the coolest new features in this release. . One way to deploy a Git-based web project is to keep a checked-out working copy on your server. When a new version is ready, you log into the server and run  git pull  to fetch and deploy the new changes. While this technique has some disadvantages (see below), it is very easy to set up and use, especially if your project consists mostly of static content. . With Git 2.3, this technique has become even more convenient. Now you can push changes directly to the repository on your server. Provided no local modifications have been made on the server, any changes to the server’s current branch will be checked out automatically. Instant deploy! . To use this feature, you have to first enable it in the Git repository on your server by running . Deploying by pushing to a Git repository is quick and convenient, but it is not for everybody. For example: .  See how this feature was implemented  . Cloning a remote repository can involve transferring a lot of data over the network. But if you already have another local clone of the same repository, it probably already has most of the history that the new clone will need. Now it is easy to use those local objects rather than transferring them again: . The new  --dissociate  option tells Git to copy any objects it can from local repository  ../oldclone , retrieving the remainder from the remote repository. Afterwards, the two clones remain independent; either one can be deleted without impacting the other (unlike when  --reference  is used without  --dissociate ). .  See how this feature was implemented  . If you run  git push  without arguments, Git now uses the more conservative  simple  behavior as the default. This means that Git refuses to push anything unless you have defined an “upstream” branch for your current branch  and  the upstream branch has the same name as your current branch. For example: . The new default behavior is meant to help users avoid pushing changes to the wrong branch by accident. In the case above, the  experimental  branch started out tracking  master , but the user probably wanted to push the  experimental  branch to a new remote branch called  experimental . So the correct command would be  git push origin experimental . . The default behavior can be changed by configuring  push.default . If you want to go back to the version 1.x behavior, set it to  matching : .  See how this feature was implemented  . Git knows how to connect to a remote host via the SSH protocol, but sometimes you need to tweak exactly how it makes the connection. If so, you can now use a new shell variable,  GIT_SSH_COMMAND , to specify the command (including arguments) or even an arbitrary snippet of Shell code that Git should use to connect to the remote host. For example, if you need to use a different SSH identity file when connecting to a Git server, you could enter .  See how this feature was implemented  . When Git needs a password (e.g., to connect to a remote repository over http), it uses the  credential  subsystem to query any helpers (like the OS X Keychain helper), and then finally prompts the user on the terminal. When Git is run from an automated process like a  cron  job, there is usually no terminal available and Git will skip the prompt. However, if there  is  a terminal available, Git may hang forever, waiting for the user to type something. Scripts which do not expect user input can now set  GIT_TERMINAL_PROMPT=0  in the environment to avoid this behavior. .  See how this feature was implemented  . Some other useful tidbits: . Don’t forget: an important Git security vulnerability was  fixed last December . If you haven’t upgraded your Git client since then, we recommend that you do so as soon as possible. The new release, 2.3.0, includes the security fix, as do the maintenance releases 1.8.5.6, 1.9.5, 2.0.5, and 2.1.4, which were released in December. ", "date": "February 6, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe Game Off Returns!\t\t", "author": ["\n\t\tLee Reilly\t"], "link": "https://github.blog/2015-03-02-the-game-off-returns/", "abstract": "   . The GitHub Game Off, our very own game jam is returning next week! We’ve had some  great games submitted in previous years  and can’t wait to see what you come up with this year. . We’ll announce the theme and the details on the blog on March 13th at 9am PDT, so please stay tuned. We may even have a twist in store! . The official Twitter hashtag for the Game Off is   #ggo15  . .  \t\t Tags:   \t\t game-off   gamedev \t ", "date": "March 2, 2015"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe Ghost of Issues Past\t\t", "author": ["\n\t\tTim Pease\t"], "link": "https://github.blog/2013-12-18-the-ghost-of-issues-past/", "abstract": " The end of the year is fast approaching, and this is a good time to review open issues from long ago. A great way to find older issues and pull requests is our wonderful search system. Here are a few examples: . That last group, the ones not touched in the past year, should probably just be closed. If it’s remained untouched in 2013, it probably won’t be touched in 2014. There are  563,600  open issues across GitHub that have not been touched in the past year. . So go forth and close with impunity! ", "date": "December 18, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing Forward Secrecy and Authenticated Encryption Ciphers\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2013-12-31-introducing-forward-secrecy-and-authenticated-encryption-ciphers/", "abstract": " As of yesterday we’ve updated our SSL setup on the systems that serve traffic for GitHub. The changes introduce support for Forward Secrecy and Authenticated Encryption Ciphers. . So what is Forward Secrecy? The EFF provides a  good explanation  of what it is and why it is important.  Authenticated Encryption  means that we provide ciphers that are much less vulnerable to attacks. These are already  supported in Chrome . . Also check  SSL Labs  if you want to know more details of the setup we’ve deployed. . Since this article was published, we’ve also written a  more extensive post  on what we’ve done. ", "date": "December 31, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tImproving our SSL setup\t\t", "author": ["\n\t\tDirkjan Bussink\t"], "link": "https://github.blog/2014-01-07-improving-our-ssl-setup/", "abstract": " As we  announced previously  we’ve improved our SSL setup by deploying forward secrecy and improving the list of supported ciphers. Deploying forward secrecy and up to date cipher lists comes with a number of considerations which makes doing it properly non trivial. . This is why we thought it would be worth expanding some more on the discussions we’ve had, choices we’ve made and feedback we’ve got from people. . A lot of the internet’s traffic is still secured by TLS 1.0. This version was attacked numerous times and also doesn’t provide support for newer algorithms that you’d want to deploy. . We were glad that we were already on a recent enough OpenSSL version that supports TLS 1.1 and 1.2 as well. If you’re looking at improving your SSL setup, making sure that you can support TLS 1.2 is the first step you should take, because it makes the other improvements possible as well. TLS 1.2 is  supported in OpenSSL 1.0.0h and 1.0.1 and newer . . When the  BEAST  attack first was published, the  recommended way to mitigate  this attack vector was to switch to RC4. Since then though, additional attacks against RC4 have been devised. . This led more and more people to recommend to move away from RC4, like the people behind  SSL Labs  and  Mozilla . . Attacks against RC4 will only get better over time, and the vast majority of browsers have implemented client-side protections against BEAST. This is why we have decided to move RC4 to the bottom of our cipher prioritization, keeping it only for backwards compatibility. . The only cipher that is relatively broadly supported and that hasn’t been compromised by attacks is AES  GCM . This mode of AES doesn’t suffer from  keystream bias  like RC4 or attacks on  CBC  that resulted in BEAST and Lucky 13. . Currently AES GCM is supported in  Chrome , but it’s also in the works for other browsers like  Firefox . We’ve given priority to these ciphers and, given our usage patterns we now see a large majority of our connections being secured by this cipher. . So the recommendations on which ciphers to use are fairly straightforward. But, choosing the right ciphers is only one step to ensuring forward secrecy. There are some pitfalls that can cause you to not actually provide any additional security to customers. . In order to explain these potential problems, first we need to introduce the concept of session resumption. Session resumption is a mechanism used to significantly shorten the handshake mechanism when a new connection is opened. This means that if a client connects again to the same server, we can do a shorter setup and greatly reduce the time it takes to setup a secure connection. . There are two mechanisms for implementing these session resumption, the first is using session IDs, the second is using session tickets. . Using session IDs means that the server keeps track of state and if a client reconnects with a session ID the server has given out, it can reuse the existing state it tracked there. Let’s see how that looks when we connect to a server supporting session IDs. . What you can see here is that the server hands out a Session-ID that the client can then use to send and reconnect. The downside of this is of course that this means the server needs to keep track of this state. . This state tracking also means that if you have a site that has multiple front ends for SSL termination, you might not get the benefits that you expect. If a client ends up on a different front end the second time, that front end doesn’t know about the session ID and will have to setup a completely new connection. . SSL Session tickets are described in  RFC5077  and provide a mechanism that means we don’t have to keep the same state at the server. . How this mechanism works is that the state is encrypted by the server and handed to the client. This means the server doesn’t have to keep track of all this state in memory. It does mean however that the key used to encrypt session tickets needs to be tracked server side. This is how it looks when we connect to a server supporting session tickets. . With a session ticket key, it is possible to share this ticket across multiple front ends. This way you can have the performance benefits of session resumption even across different servers. If you don’t share this ticket key it has the same performance benefits as using session ID’s. . Not carefully considering the session resumption mechanism can lead to not getting the benefits of forward secrecy. If you keep  track of the state for too long, it can be used to decrypt prior sessions, even when deploying forward secrecy. . This is  described well  by Adam Langley on his blog. Twitter also did a technical deep dive describing how they deployed a setup with  sharing the session ticket key . . So, we had to decide whether developing a secure means of sharing ticket keys (ala Twitter) was necessary to maintain acceptable performance given our current traffic patterns. We found that clients usually end up on the same load balancer when they make a new connection shortly after a previous one. As a result, we decided that we can rely on session IDs as our resumption mechanism and still maintain a sufficient level of performance for clients. . This is also where we got tripped up. We currently use HAProxy as our SSL termination which ends up using the default OpenSSL settings if you don’t specify any additional options. This means that both session IDs and session tickets are enabled by default. . The problem here lies with session tickets being enabled. Even though we didn’t setup sharing the key across servers, it still means that HAProxy uses an in-memory key to encrypt session tickets. This encryption key is initialized once the process starts up and stays the same for the process lifetime. . This means that if we would have a HAProxy running for a long time, an attacker who who obtains the session ticket key can decrypt traffic from prior sessions whose ticket was encrypted using the session ticket key. This of course doesn’t provide the forward secrecy properties we were aiming for. . Session IDs don’t have this problem, since they have a lifetime of 5 minutes (on our platform), making the window for this attack only 5 minutes wide instead of the entire process lifetime. . Given that session tickets don’t provide any additional value for us at this point, we decided to disable them and only rely on session IDs. This way we get the benefits of forward secrecy while also maintaining an acceptable level of performance for clients. . We would like to thank  Jeff Hodges  for reaching out to us and point us at what we’ve missed in our initial setup. ", "date": "January 7, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tVideo from Passion Projects Talk #7 with Jen Myers\t\t", "author": ["\n\t\tManisha Sharma\t"], "link": "https://github.blog/2014-01-15-video-from-passion-projects-talk-7-with-jen-myers/", "abstract": "  Jen Myers  joined us in December of 2013 for the 7th installment of our  Passion Projects  talk series. Jen taught us the importance of  not  being an expert and how to be responsible for our own learning and personal and professional growth. Check out the full video of her talk and our panel discussion below. .           ", "date": "January 15, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tOptimizing large selector sets\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2014-01-16-optimizing-large-selector-sets/", "abstract": " CSS selectors are to frontend development as SQL statements are to the backend. Aside from their origin in CSS, we use them all over our JavaScript. Importantly, selectors are declarative, which makes them prime candidates for optimizations. . Browsers have a number of ways of dealing with parsing, processing, and matching large numbers of CSS selectors. Modern web apps are now using thousands of selectors in their stylesheets. In order to calculate the styles of a single element, a huge number of CSS rules need to be considered. Browsers don’t just iterate over every selector and test it. That would be way too slow. . Most browsers implement some kind of grouping data structure to sort out obvious rules that would not match. In WebKit, it’s called a  RuleSet . .  SelectorSet  is a JavaScript implementation of group technique browsers are already using. If you have a set of selectors known upfront, it makes matching and querying elements against that set of selectors much more efficient. . Selectors added to the set are quickly analyzed and indexed under a key. This key is derived from a significant part of the right most side of the selector. If the selector targets an id, the id name is used as the key. If there’s a class, the class name is used and so forth. The selector is then put into a map indexed by this key. Looking up the key is constant time. . When it’s time to match the element against the group, the element’s properties are examined for possible keys. These keys are then looked up in the mapping which returns a smaller set of selectors which then perform a full matches test against the element. . jQuery’s original  $.fn.live  function (and its modern form,  $.fn.on ) are probably the most well known delegation APIs. The main advantage of using the delegated event handler over a directly bound one is that new elements added after  DOMContentLoaded  will trigger the handler. A technique like this is essential when using a pattern such as  pjax , where the entire page never fully reloads. . Extensive usage of  document  delegated event handlers is considered controversial. This includes applications with a large number of  $(‘.foo’).live(‘click’)  or  $(document).on(‘click’, ‘.foo’)  registrations. The common performance argument is that the selector has to be matched against entire ancestor chain of the event target. On an application with large and deeply nested DOM, like github.com, this could be as deep as 15 elements. However, this is likely not the most significant factor. It is when the number of delegated selectors themselves is large. GitHub has 100+ and  Basecamp  has 300+ document delegated events. . Using the selector set technique described above, installing  this jQuery patch  could massively speed up your apps event dispatch. Here’s a fun little  jsPerf test  using real GitHub selectors and markup to demonstrate how much faster the patched jQuery is. . Both of these libraries should be unnecessary and hopefully obsoleted by browsers someday. Browsers already implement techniques like this to process CSS styles efficiently. It’s still unfortunate we have no native implementation of declarative event handlers, even though people have been doing this since  2006 . ", "date": "January 16, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tDNS Outage Post Mortem\t\t", "author": ["\n\t\tJames Fryman\t"], "link": "https://github.blog/2014-01-18-dns-outage-post-mortem/", "abstract": " Last week on Wednesday, January 8th, GitHub experienced an outage of our DNS infrastructure. As a result of this outage, our customers experienced 42 minutes of downtime of services along with an additional 1 hour and 35 minutes of downtime within a subset of repositories as we worked to restore full service. I would like apologize to our customers for the impact to your daily operations as a result of this outage. Unplanned downtime of any length is unacceptable to us. In this case we fell short of both our customers’ expectations and our own. For that, I am truly sorry. . I would like to take a moment and explain what caused the outage, what happened during the outage, and what we are doing to help prevent events like this in the future. . For some time we’ve been working to identify places in our infrastructure that are vulnerable to Distributed Denial of Service (DDoS) attacks. One of the things we specifically investigated was options for improving our defenses against DNS amplification attacks, which have become very common across the internet. In order to simplify our access control rules, we decided to reduce the number of hosts which are allowed to make DNS requests and receive DNS replies to a very small number of name servers. This change allows us to explicitly reject DNS traffic that we receive for any address that isn’t explicitly whitelisted, reducing our potential attack surface area. . In order to roll out these changes, we had prepared changes to our firewall and router configuration to update the IP addresses our name servers used to send queries and receive responses. In addition, we prepared similar changes to our DNS server configuration to allow them to use these new IP addresses. The plan was to roll out this set of changes for one of our name servers, validate the new configuration worked as expected, and proceed to make the same change to the second server. . Our rollout began on the afternoon of the 8th at 13:20 PM PST. Changes were deployed to the first DNS server, and an initial verification led us to believe the changes had been rolled out successfully. We proceeded to deploy to the second name server at 13:29 PM PST, and again performed the same verification. However, problems began manifesting nearly immediately. . We began to observe that certain DNS queries were timing out. We quickly investigated, and discovered a bug in our rollout procedure. We expected that when our change was applied, both our caching name servers and authoritative name servers would receive updated configuration – including their new IP addresses – and restart to apply this configuration. Both name servers received the appropriate configuration changes, but only the authoritative name server was restarted due to a bug in our Puppet manifests. As a result, our caching name server was requesting authoritative DNS records from an IP that was no longer serving DNS. This bug created the initial connection timeouts we observed, and began a cascade of events. . Our caching and authoritative name servers were reloaded at 13:49 PST, resolving DNS query timeouts. However, we observed that certain queries were now incorrectly returning  NXDOMAIN . Further investigation found that our DNS zone files had become corrupted due to a circular dependency between our internal provisioning service and DNS. . During the investigation of the first phase of this incident, we triggered a deployment of our DNS system, which performs an API call against our internal provisioning system and uses the result of this call to construct a zone file. However, this query requires a functioning DNS infrastructure to complete successfully. Further, the output from this API call verification was not adequately checked for sanity before being converted into a zone file. As a result, this deployment removed a significant amount of records from our name servers, causing the  NXDOMAIN  results we observed. The missing DNS records were restored by performing the API call manually, validating the output, and updating the affected zones. . Many of our servers recovered gracefully once DNS service began responding appropriately. However, we quickly noted that github.com performance had not returned to normal, and our error rates were far higher than normal. Further investigation found that a subset of our fileservers were actively refusing connections due to what we found out later was memory exhaustion, exacerbated by the spawning of a significant number of processes on during the DNS outage. .   .   . The failing fileservers began creating a back pressure in our routing layer that prevented connections to healthy fileservers. Our team began manually removing all misbehaving fileservers from the routing layer, restoring services for the fileservers that had survived the spike in processes and memory during the DNS event. . The team split up the pool of disabled fileserver, and triaged their status. Collectively, we found one of two scenarios existed to be repaired: either the node had calmed down ‘enough’ as a result of DNS service restoration to allow one of our engineers to log into the box and start forcefully killing hung processes to restore service, or the node had become so exhausted that our HA daemon kicked in to STONITH the active node and bring up our secondary node. In both of these situations, our team went in and performed checks against our low-level DRBD block devices to ensure there were no inconsistencies or errors in data replication. Full service was restored for all of our customers by 15:47 PM PST. . This small problem uncovered quite a bit about our infrastructure that we will be critically reviewing over the next few weeks. This includes: . We are investigating further decoupling of our internal and external DNS infrastructure. While the pattern of forwarding requests to an upstream DNS server is not uncommon, the tight dependency that exists between our internal name servers and our external name servers needs to be broken up to allow changes to happen independently of each other. . We are reviewing our configuration management code for other service restart bugs. In many cases, this means the improvement of our backend testing. We will be reviewing critical code for appropriate tests using  rspec-puppet , as well as looking at integration tests to ensure that service management behaves as intended. . We are reviewing the cyclic dependency between our internal provisioning system and our DNS resolvers, and have already updated the deployment procedure to verify the results returned from the API call before removing a large number of records. . We are reviewing and testing all of the designed safety release valves in our fileserver management systems and routing layers. During the failure when filesevers became so exhausted that the routing layer failed due to back pressure, we should have seen several protective measures kick in to automatically remove these servers from service. These mechanisms did not fire off as designed, and need to be revisited. . We are implementing process accounting controls to appropriately limit the resources consumed by our application processes. Specifically, we are testing Linux  cgroups  to further isolate application processes from administrative system functionality. In the event of a similar event in the future, this should allow us to restore full access much more quickly. . We are reviewing the code deployed to our fileservers to analyze for tight dependencies to DNS. We reviewed the DNS time-outs on our fileservers and found that DNS requests should have timed out after 1 second, and only retried to resolve 2 times in total. This analysis along with cgroup implementation should provide a better barrier to avoid runaway processes in the first place, and a safety valve to manage them if processing becomes unruly in the future. . We realize that GitHub is an important part of your development and workflow. Again, I would like to take a moment to apologize for the impact that this outage had to your operations. We take great pride in providing the best possible service quality to our customers. Occasionally, we run into problems as detailed above. These incidents further drive us to continually improve the quality of our own internal operations and ensure that we are living up to the trust you have placed in us. We are working diligently to provide you with a stable, fast, and pleasant GitHub experience. Thank you for your continual support of GitHub! ", "date": "January 18, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tProxying User Images\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2014-01-28-proxying-user-images/", "abstract": " A while back, we started  proxying all non-https images  to avoid mixed-content warnings using a custom node server called  camo . We’re making a small change today and proxying HTTPS images as well. . Proxying these images will help protect your privacy: your browser information won’t be leaked to other third party services. Since we’re also routing images through our CDN, you should also see faster overall load times across GitHub, as well as fewer broken images in the future. .  Related open source patches  ", "date": "January 28, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tDenial of Service Attacks\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2014-03-14-denial-of-service-attacks/", "abstract": " On Tuesday, March 11th, GitHub was largely unreachable for roughly 2 hours as the result of an evolving distributed denial of service (DDoS) attack. I know that you rely on GitHub to be available all the time, and I’m sorry we let you down. I’d like to explain what happened, how we responded to it, and what we’re doing to reduce the impact of future attacks like this. . Over the last year, we have seen a large number and variety of denial of service attacks against various parts of the GitHub infrastructure. There are two broad types of attack that we think about when we’re building our mitigation strategy: volumetric and complex. . We have designed our DDoS mitigation capabilities to allow us to respond to both volumetric and complex attacks. . Volumetric attacks are intended to exhaust some resource through the sheer weight of the attack. This type of attack has been seen with increasing frequency lately through UDP based amplification attacks using protocols like DNS, SNMP, or NTP. The only way to withstand an attack like this is to have more available network capacity than the sum of all of the attacking nodes or to filter the attack traffic before it reaches your network. . Dealing with volumetric attacks is a game of numbers. Whoever has more capacity wins. With that in mind, we have taken a few steps to allow us to defend against these types of attacks. . We operate our external network connections at very low utilization. Our internet transit circuits are able to handle almost an order of magnitude more traffic than our normal daily peak. We also continually evaluate opportunities to expand our network capacity. This helps to give us some headroom for larger attacks, especially since they tend to ramp up over a period of time to their ultimate peak throughput. . In addition to managing the capacity of our own network, we’ve contracted with a leading DDoS mitigation service provider. A simple Hubot command can reroute our traffic to their network which can handle terabits per second. They’re able to absorb the attack, filter out the malicious traffic, and forward the legitimate traffic on to us for normal processing. . Complex attacks are also designed to exhaust resources, but generally by performing expensive operations rather than saturating a network connection. Examples of these are things like SSL negotiation attacks, requests against computationally intensive parts of web applications, and the “Slowloris” attack. These kinds of attacks often require significant understanding of the application architecture to mitigate, so we prefer to handle them ourselves. This allows us to make the best decisions when choosing countermeasures and tuning them to minimize the impact on legitimate traffic. . First, we devote significant engineering effort to hardening all parts of our computing infrastructure. This involves things like tuning Linux network buffer sizes, configuring load balancers with appropriate timeouts, applying rate limiting within our application tier, and so on. Building resilience into our infrastructure is a core engineering value for us that requires continuous iteration and improvement. . We’ve also purchased and installed a software and hardware platform for detecting and mitigating complex DDoS attacks. This allows us to perform detailed inspection of our traffic so that we can apply traffic filtering and access control rules to block attack traffic. Having operational control of the platform allows us to very quickly adjust our countermeasures to deal with evolving attacks. . Our DDoS mitigation partner is also able to assist with these types of attacks, and we use them as a final line of defense. . At 21:25 UTC we began investigating reports of connectivity problems to  github.com . We opened an incident on our status site at 21:29 UTC to let customers know we were aware of the problem and working to resolve it. . As we began investigating we noticed an apparent backlog of connections at our load balancing tier. When we see this, it typically corresponds with a performance problem with some part of our backend applications. . After some investigation, we discovered that we were seeing several thousand HTTP requests per second distributed across thousands of IP addresses for a crafted URL. These requests were being sent to the non-SSL HTTP port and were then being redirected to HTTPS, which was consuming capacity in our load balancers and in our application tier. Unfortunately, we did not have a pre-configured way to block these requests and it took us a while to deploy a change to block them. . By 22:35 UTC we had blocked the malicious request and the site appeared to be operating normally. . Despite the fact that things appeared to be stabilizing, we were still seeing a very high number of SSL connections on our load balancers. After some further investigation, we determined that this was an additional vector that the attack was using in an effort to exhaust our SSL processing capacity. We were able to respond quickly using our mitigation platform, but the countermeasures required significant tuning to reduce false positives which impacted legitimate customers. This resulted in approximately 25 more minutes of downtime between 23:05-23:30 UTC. . By 23:34 UTC, the site was fully operational. The attack continued for quite some time even once we had successfully mitigated it, but there were no further customer impacts. . The vast majority of attacks that we’ve seen in the last several months have been volumetric in terms of bandwidth, and we’d grown accustomed to using throughput as a way of confirming that we were under attack. This attack did not generate significantly more bandwidth but it did generate significantly more packets per second. It didn’t look like what we had grown to expect an attack to look like and we did not have the monitoring we needed to detect it as quickly as we would have liked. . Once we had identified the problem, it took us much longer than we’d like to mitigate it. We had the ability to mitigate attacks of this nature in our load balancing tier and in our DDoS mitigation platform, but they were not configured in advance. It took us valuable minutes to configure, test, and tune these countermeasures which resulted in a longer than necessary downtime. . We’re happy that we were able to successfully mitigate the attack but we have a lot of room to improve in terms of how long the process takes. . We have already made adjustments to our monitoring to better detect and alert us of traffic pattern changes that are indicative of an attack. In addition, our robots are now able to automatically enable mitigation for the specific traffic pattern that we saw during the attack. These changes should dramatically reduce the amount of time it takes to respond to a wide variety of attacks in the future and reduce their impact on our service. . We are investigating ways that we can simulate attacks in a controlled way so that we can test our countermeasures on a regular basis to build additional confidence in both our mitigation tools and to improve our response time in bringing them to bear. . We are talking to some 3rd party security consultants to review our DDoS detection and mitigation capability. We do a good job mitigating attacks we’ve seen before, but we’d like to more proactively plan for attacks that we haven’t yet encountered. . Hubot is able to route our traffic through our mitigation partner and to apply templates to operate our mitigation platform for known attack types. We’ve leveled him up with some new templates for attacks like this one so that he can help us recover faster in the future. . This attack was painful, and even though we were able to successfully mitigate the effects of it, it took us far too long. We know that you depend on GitHub and our entire company is focused on living up to the trust you place in us. I take problems like this personally. We will do whatever it takes to improve how we respond to problems to ensure that you can rely on GitHub being available when you need us. . Thanks for your support! ", "date": "March 14, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tSecurity: Heartbleed vulnerability\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2014-04-08-security-heartbleed-vulnerability/", "abstract": " On April 7, 2014 information was released about a new vulnerability ( CVE-2014-0160 ) in  OpenSSL , the cryptography library that powers the vast majority of private communication across the Internet. This library is key for maintaining privacy between servers and clients, and confirming that Internet servers are who they say they are. . This  vulnerability , known as  Heartbleed , would allow an attacker to steal the keys that protect communication, user passwords, even the system memory of a vulnerable server. This represents a major risk to large portions of private traffic on the Internet, including github.com. .  Note: GitHub Enterprise servers are not affected by this vulnerability. They run an older OpenSSL version which is not vulnerable to the attack.  . As of right now, we have no indication that the attack has been used against github.com. That said, the nature of the attack makes it hard to detect so we’re proceeding with a high level of caution. .  UPDATE: 2014-04-08 16:00 PST – All browser sessions that were active prior to the vulnerability being addressed have been reset. See below for more info.  . We’ve completed a number of measures already and continue to work the issue. . We’ve patched all our systems using the newer, protected versions of OpenSSL. We started upgrading yesterday after the vulnerability became public and completed the roll out today. We are also working with our providers to make sure they’re upgrading their systems to minimize GitHub’s exposure. . We’ve recreated and redeployed new SSL keys and reset internal credentials. We have also revoked our older certs just to be safe. . We’ve forcibly reset all browser sessions that were active prior to the vulnerability being addressed on our servers. You may have been logged out and have to log back into GitHub. This was a proactive measure to defend against potential session hijacking attacks that may have taken place while the vulnerability was open. . Prior to this incident, GitHub made a number of enhancement to mitigate attacks like this.  We deployed Perfect Forward Secrecy  at the end of last year, which makes it impossible to use stolen encryption keys to read old encrypted communication. We are working to find more opportunities like this. . Right now, GitHub has no indication that the vulnerability has been used outside of testing scenarios. However, out of an abundance of caution, you can: .  Change your GitHub password . Be sure your password is strong; for more information, see  What is a strong password?  .  Enable Two-Factor Authentication . .  Revoke and recreate personal access and application tokens . . GitHub works hard to keep your code safe. We are continuing to respond to this vulnerability and will post updates as things progress. For more information as it’s available, keep an eye on  Twitter  or the GitHub Blog. ", "date": "April 8, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tMaking MySQL Better at GitHub\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2014-09-02-making-mysql-better-at-github/", "abstract": "  At GitHub we say, “it’s not fully shipped until it’s fast.” We’ve talked before about some of the ways we keep our  frontend experience speedy , but that’s only part of the story. Our MySQL database infrastructure dramatically affects the performance of GitHub.com. Here’s a look at how our infrastructure team seamlessly conducted a major MySQL improvement last August and made GitHub even faster.  . Last year we moved the bulk of GitHub.com’s infrastructure into a new datacenter with world-class hardware and networking. Since MySQL forms the foundation of our backend systems, we expected database performance to benefit tremendously from an improved setup. But creating a brand-new cluster with brand-new hardware in a new datacenter is no small task, so we had to plan and test carefully to ensure a smooth transition. . A major infrastructure change like this requires measurement and metrics gathering every step of the way. After installing base operating systems on our new machines, it was time to test out our new setup with various configurations. To get a realistic test workload, we used  tcpdump  to extract  SELECT  queries from the old cluster that was serving production and replayed them onto the new cluster. . MySQL tuning is very workload specific, and well-known configuration settings like  innodb_buffer_pool_size  often make the most difference in MySQL’s performance. But on a major change like this, we wanted to make sure we covered everything, so we took a look at settings like  innodb_thread_concurrency ,  innodb_io_capacity , and  innodb_buffer_pool_instances , among others. . We were careful to only make one test configuration change at a time, and to run tests for at least 12 hours. We looked for query response time changes, stalls in queries per second, and signs of reduced concurrency. We observed the output of  SHOW ENGINE INNODB STATUS , particularly the  SEMAPHORES  section, which provides information on work load contention. . Once we were relatively comfortable with configuration settings, we started migrating one of our largest tables onto an isolated cluster. This served as an early test of the process, gave us more space in the buffer pools of our core cluster and provided greater flexibility for failover and storage. This initial migration introduced an interesting application challenge, as we had to make sure we could maintain multiple connections and direct queries to the correct cluster. . In addition to all our raw hardware improvements, we also made process and topology improvements: we added delayed replicas, faster and more frequent backups, and more read replica capacity. These were all built out and ready for  go-live day . . With millions of people using GitHub.com on a daily basis, we did not want to take any chances with the actual switchover. We came up with a thorough  checklist  before the transition: .   . We also planned a maintenance window and  announced it on our blog  to give our users plenty of notice. . At 5am Pacific Time on a Saturday, the migration team assembled online in chat and the process began: .   . We put the site in maintenance mode, made an announcement on Twitter, and set out to work through the list above: .   .  13 minutes  later, we were able to confirm operations of the new cluster: .   . Then we flipped GitHub.com out of maintenance mode, and let the world know that we were in the clear. .   . Lots of up front testing and preparation meant that we kept the work we needed on  go-live day  to a minimum. . In the weeks following the migration, we closely monitored performance and response times on GitHub.com. We found that our cluster migration cut the average GitHub.com page load time by half and the 99th percentile by  two-thirds : .   . During this process we decided that moving larger tables that mostly store historic data to separate cluster was a good way to free up disk and buffer pool space. This allowed us to leave more resources for our “hot” data, splitting some connection logic to enable the application to query multiple clusters. This proved to be a big win for us and we are working to reuse this pattern. . You can never do too much acceptance and regression testing for your application. Replicating data from the old cluster to the new cluster while running acceptance tests and replaying queries were invaluable for tracing out issues and preventing surprises during the migration. . Large changes to infrastructure like this mean a lot of people need to be involved, so pull requests functioned as our primary point of coordination as a team. We had people all over the world jumping in to help. . Deploy day team map: . This created a workflow where we could open a pull request to try out changes, get real-time feedback, and see commits that fixed regressions or errors — all without phone calls or face-to-face meetings. When everything has a URL that can provide context, it’s easy to involve a diverse range of people and make it simple for them give feedback. . A full year later, we are happy to call this migration a success — MySQL performance and reliability continue to meet our expectations. And as an added bonus, the new cluster enabled us to make further improvements towards greater availability and query response times. I’ll be writing more about those improvements here soon. ", "date": "September 2, 2014"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit Merge Berlin 2013\t\t", "author": ["\n\t\tScott Chacon\t"], "link": "https://github.blog/2013-06-10-git-merge-berlin-2013/", "abstract": " Last month GitHub was proud to host the first Git Merge conference, a place for Git core developers and Git users to meet, talk about Git and share what they’ve been working on or interested in.  The first Git Merge was held in Berlin at the amazing Radisson Blu Berlin on May 9-11, 2013. .   . The Git Merge conference came out of the GitTogether meetings that several Git developers held for several years at Google’s campus directly after their Google Summer of Code Mentors Summit.  We felt that we should hold a similar conference of the Git minds in the EU to accomplish the same things – get Git developers together to meet in person, talk about interesting things they’re working on and meet some users. .   . This conference was run a little differently than most.  It was split up into three days – a Developer Day, a User Day and a Hack Day. . The first day was the developer day, limited to individuals who have made contributions to core Git or one of its implementations such as libgit2 or JGit.  About 30 developers came and had discussions ranging from an incremental merge tool, to our participation and success in the Google Summer of Code program, to fixing race conditions in the Git server code. .   . The second day was the User Day, meant to allow everyone to share tools they were working on or issues they have with Git.  The first half of the day was set up entirely in lightning talk format and over 40 talks were given, ranging in duration from a few seconds to nearly 20 minutes.  After the lightning talks were done and everyone who wanted to had spoken, we broke up into small group discussions about more specific topics – Laws on GitHub, Git migration issues, tools and tips for teaching Git and more. . The final day was the Hack Day which gave attendees a chance to sit down with people they had met the previous day or two and start working on something interesting. . Notes for the entire conference, collaborated on by attendees, can be found  here . . Recorded talks from each day can be found  here .  Some really interesting examples are Roberto Tyley’s  bfg-repo-cleaner talk , a tool to clean up bad history in git repositories, and  this talk  which covers the German federal law repository on GitHub. . Thanks to everyone who attended! ", "date": "June 10, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub Flow in the Browser\t\t", "author": ["\n\t\tCoby Chapple\t"], "link": "https://github.blog/2013-07-11-github-flow-in-the-browser/", "abstract": " Now that you can   delete files directly on GitHub   we’ve reached a very exciting milestone—the entire GitHub Flow™ is now possible  using nothing but a web browser. . A little while ago our very own  @schacon  wrote   an article  outlining the  workflow we use here at GitHub to collaborate using Git. It’s a deceptively  simple workflow, but it works brilliantly for us across a huge range of  projects, and has adapted extremely well as our team has grown from  being only a handful of people to our current size of  183 GitHubbers . . Since we use this workflow every day ourselves, we wanted to bring as many  parts of the workflow to our web-based interface as possible. Here’s a quick  outline of how the GitHub Flow works using just a browser: .  Create a branch  right from the repository. .  Create ,   edit ,  and  delete  files,   rename them, or move them around . .  Send a pull request  from your branch with your changes to kick off a discussion. . Continue making changes on your branch as needed, updating the pull request automatically. . Once the branch is ready to go, the pull request can be  merged using the big green button . . Branches can then be  tidied up  using the delete buttons in the pull request, or on the branches page. . Repeat. . Even before some of these steps were possible in the browser, this simple  workflow made it possible for us to   iterate extremely quickly ,   deploy dozens of times each day ,  and address critical issues with minimal delay.  Probably the most compelling aspect of GitHub Flow however is that  there is no complicated branching model to have to wrap your head around. . Now that this workflow is available entirely via the web interface though,  a  whole new  set of benefits start to emerge. Let’s dive into a few of them. . Perhaps the most interesting consequence of having these tools available in the  browser is that people don’t have to interact with Git or the command-line at  all—let alone  understand  them—in order to contribute to projects in meaningful ways.  This makes for a much easier learning curve for anyone new to Git and GitHub, whether  they’re non-technical team members, or experienced developers learning something new. . These web-based workflows have been especially helpful for our  training team   too. For some classes they’ve been able to have people begin learning the  basic concepts of Git and GitHub using just the browser, saving students the  complexity of installing Git and learning their way around the terminal commands until   after  they’ve got their heads around the fundamental ideas. When a browser is all  you need, hallway demos and super-short classes can also more effectively convey the  GitHub Flow to people in less time, and without the distractions of preparing  computers with all the necessary software. . Even if you are a command-line wizard, there are always times when you just need  to make a quick tweak to a project. Even if it’s just a single  type typo,  doing this in the terminal requires a  crazy  number of steps. So now, instead of: . …you can simply make the change right there without leaving your browser, saving you  time and maintaining your  zen -like frame of mind. What’s more, if you have any kind  of continuous integration server set up to integrate with our  Status API ,  you’ll even see that your typo fix built successfully and is ready to go! . When editing  Markdown   files in repositories, being able to quickly preview how your  change will look before you commit is very powerful. Combining that with things like  the ability to easily create  relative links between Markdown documents ,  and the availability of  fullscreen zen mode   for enhanced focus when composing content which isn’t code, the end result is a set  of tools which make creating and maintaining documentation a real pleasure indeed. . For projects that make use of  GitHub Pages  for their websites, tasks like  writing new blog posts, or adding pages to an existing site are incredibly simple now  that these workflows are possible in the browser. Whether you use  Jekyll   or just regular static HTML for your GitHub Pages site, these new workflows mean that  you’re able to make changes using GitHub in your browser, have your site rebuilt and deployed  automatically, and see your changes reflected live on  &lt;username&gt;.github.io  or your   custom domain   before you know it. . To learn more about building websites using GitHub Pages, you should  check out  these articles . . In using them ourselves, we’ve found that these web-based workflows have come in  handy on an increasingly regular basis, and we’re noticing real benefits from the reduced  barrier to contributing. We hope you find these workflows useful too, and we can’t wait  to see the interesting ways we know you’ll put them to good use. ", "date": "July 11, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tIP Address Changes\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2013-08-25-ip-address-changes/", "abstract": " As we continue to expand the infrastructure that powers GitHub, we want to make everyone aware of some  changes to the IP addresses that we use. Most customers won’t have to do anything as a result of these  changes. . We mentioned these new addresses   back in April  and updated the   Meta API  to reflect them. Some GitHub services have  have already been migrated to the new addresses, including: . Our next step is to begin using these IP addresses for the main GitHub site, so we’re reminding everyone  about this change. There are a few gotchas that might affect some people: . If you have explicit firewall rules in place that allow access to GitHub from your network, you’ll want to make sure that all of the IP ranges listed in  this article  are included. . If you have an entry in your  /etc/hosts  file that points github.com at a specific IP address, you should remove it and instead rely on DNS to give you the most accurate set of addresses. . If you are accessing your repositories over the SSH protocol, you will receive a warning message each time your client connects to a new IP address for github.com. As long as the IP address from the warning is in the range of IP addresses in the previously mentioned Help page, you shouldn’t be concerned. Specifically, the new addresses that are being added this time are in the range from  192.30.252.0  to  192.30.255.255 . The warning message looks like this: . Thanks for your patience and continued support as we work to make GitHub faster and more reliable! ", "date": "August 25, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tSite Maintenance August 31st 2013\t\t", "author": ["\n\t\tSam Lambert\t"], "link": "https://github.blog/2013-08-26-site-maintenance-august-31st-2013/", "abstract": " This Saturday, August 31st, 2013 at  5AM PDT  we will be upgrading a large portion of our database infrastructure in order to provide a faster and more reliable GitHub experience. . We estimate that the upgrades should take no longer than 20 minutes. In order to minimize risk we will be putting the site into maintenance mode while the upgrade is performed. Thus, HTTP, API and Git access to GitHub.com will be unavailable for the duration of the upgrade. . We will update our  status page  and  @githubstatus  at the beginning of maintenance and again at the end. ", "date": "August 26, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit Internals PDF Open Sourced\t\t", "author": ["\n\t\tScott Chacon\t"], "link": "https://github.blog/2013-09-20-git-internals-pdf-open-sourced/", "abstract": " Over 5 years ago, shortly after GitHub initially launched, Chris pointed out on one of our  earliest blog posts  this Peepcode PDF on Git internals that I had just written: .   . Well, today Pluralsight has agreed to open source the book under Creative Commons Attribution-ShareAlike license and the source is  on GitHub .  You can now download and read this book for free.  Get it on its GitHub  releases page  and maybe learn a bit about how Git works under the covers. ", "date": "September 20, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tModeling your App’s User Session\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2013-10-18-modeling-your-app-s-user-session/", "abstract": " If you’ve been keeping an eye on your cookies, you may have noticed some recent changes GitHub has made to how we track your session. You shouldn’t notice any difference in session behavior (beyond the  new ability to revoke sessions ), but we’d like to explain what prompted the change. . Replay attacks on stateless session stores have been known and documented for quite some time in  Rails  and  Django . Using signed cookies for sessions is still incredibly easy to use and scales to high traffic web apps. You just need to understand its limitations. When implementing authentication, simply storing a user ID in the session cookie leaves you open to replay attacks and provides no means for revocation. . The other option is to switch to persisted storage for sessions. Either using a database, memcache or redis. On a high traffic site this may be a performance concern since a session may be allocated even for anonymous browsing traffic. Another downside, there is no clear insight into these sessions. They are stored as serialized objects. So there’s no way to query the store to see if a user has any sessions. It’s all abstracted away by Rails. . After ruling out Rails’ built in method for DB backed sessions, we decided that the concept of user sessions ought to be treated as a first class domain concern. Something with a real application API we can query, test and extend with other app concerns. . The  UserSession  class is just a normal ActiveRecord class like any other. There’s no excess Rails abstraction layer between it. We’ve extended it with other concerns such as manual revocation,  sudo mode tracking  and data like IP and user agent to help users identify sessions on the  active sessions page . . Staying true to the restful authentication spirit,  SessionsController#create  creates a new  UserSession  and  SessionsController#destroy  deletes it. . A separate cookie called  user_session  is set referencing the record unique random key. Only signed in users allocate this record. Anonymous traffic to GitHub never creates junk data in our sessions table. . We still have our signed cookie store around as  session  in our controllers. This handles non-sensitive data like flash notices and multi-step form state. Then we have a separate  user_session  helper that references the current user’s session record. . This infrastructure change took a few months. For a month, we ran both the old session code path on this new user session path at once. This allowed users to transition over to the new cookie without noticing. . Overall, we are pretty happy with the change. It has made our authentication logic much more clear and explicit. This opens up some new potential now that we have the data on the server. ", "date": "October 18, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tDisabling old IP addresses\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2013-10-30-disabling-old-ip-addresses/", "abstract": " We’ve made some significant upgrades to the network infrastructure powering GitHub, and it’s time to turn off some of the old gear. We’ve updated DNS records to point at our new IP space, but continue to see a steady trickle of requests to IP addresses long since removed from DNS. . On Tuesday, November 5th 2013, at 12pm Pacific Time, we’ll stop serving all HTTP, Git, and SSH requests to IP addresses that aren’t returned from DNS queries for the following domains: .  This won’t affect you if you don’t have any  /etc/hosts  entries for any of the above domains.  However, if you’ve added  github.com  or any of the listed domains to  /etc/hosts  over the last few years, you’ll need to remove those entries or GitHub will stop working for you next Tuesday at noon. Take a quick look at your  /etc/hosts  and/or your Puppet/Chef manifests to make sure you’re ready to go! . Please note that our DNS servers are configured to automatically return the IP address of a random, healthy load balancer for queries for the above records. If you have an existing  /etc/hosts  entry, we highly recommend  not  replacing it, but rather removing it entirely. .  Update : If you’re on Windows, you’ll want to check  %SystemRoot%system32driversetchosts  for anything matching  github.com . If there are no entries there and you’re still seeing a warning on GitHub.com, please send your network administrator a link to this blog post! ", "date": "October 30, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tAn African hack trip\t\t", "author": ["\n\t\tLuke Hefson\t"], "link": "https://github.blog/2013-11-13-an-african-hack-trip/", "abstract": " GitHub has a long tradition of supporting developer communities throughout the world. We  throw drinkups ,  speak at and sponsor conferences , and  host training events  in most corners of the globe. . However, with the exception of a few talks and drinkups in Cape Town, we’ve not yet had much of a chance to see what’s going on in the burgeoning sub-Saharan tech scene. . We had heard that several African countries were rapidly growing new and innovative tech communities, but very little is being said about  how  they operate. So last month @nrrrdcore, @luckiestmonkey and I decided to check it out for ourselves. . It just so happened that a group of like-minded developers and designers from Europe called  The AfricaHackTrip , were also planning a very similar trip. So we reached out to them to see if we could tag along and help out in any way. We ended up sponsoring the Hackathons and BarCamps they had organised in 4 African cities and participating in a couple of them as well. . Our first stop was Kigali, Rwanda. Here we joined the AfricaHackTrip halfway into their adventures and took part in their BarCamp and Hackathon at  the Office co-working space . .   . We met a ton of awesome techies and had great discussions about topics ranging from Open Source Software schools to time travel. On the hackathon day we discovered how some Rwandan hardware hackers are using Arduinos to solve rural farming problems and that reliance on decent internet connectivity is a  big  problem for developers there. Big enough that one group created a hack project that would monitor different Rwandan wifi networks at the same time: .   . In Dar es Salaam the BarCamp and Hackathon events were held at the awesome  Buni co-working space . Across the trip we noticed how technology hubs are working together to create vibrant communities. It wasn’t uncommon to see the manager of one hub helping out at events at another. . Dar es Salaam was just as exciting as Kigali. People discussed local online payment systems and hacked on  mapping solutions for Tanzanian health initiates .  At a Git &amp; GitHub workshop that we held at the Kinu co-working space 50 people suddenly turned up – excited and ready to learn. .   . Once the AfricaHackTrips in Kigali and Dar es Salaam had finished, I then travelled on to Nairobi to check out the  DEMO Africa conference  and meet some of the inspiring startups coming out of Africa. . I also spent a day at the iHub, Nairobi’s centre for everything tech. Here I met teams from  Ushahidi  and  BRCK , as well as  Akirachix  – who are taking underprivileged women from Nairobi, teaching them how to code, and mentoring them into programming jobs. . We’d like to continue supporting developer communities throughout Africa and the developing world. If you’re putting on a conference, hackathon or meet-up and you’re in search of a sponsor, please get in touch through our  community page . Or, if you’re running an innovative tech space or developer-community project,  get in touch  and we’ll see how we can help! ", "date": "November 13, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tWeak passwords brute forced\t\t", "author": ["\n\t\tShawn Davenport\t"], "link": "https://github.blog/2013-11-20-weak-passwords-brute-forced/", "abstract": " Some GitHub user accounts with weak passwords were recently compromised due to a brute force password-guessing attack. I want to take this opportunity to talk about our response to this specific incident and account security in general. . We sent an email to users with compromised accounts letting them know what to do.  Their passwords have been reset and personal access tokens, OAuth authorizations, and SSH keys have all been revoked. Affected users will need to create a new,  strong password  and  review their account  for any suspicious activity. This investigation is ongoing and we will notify you if at any point we discover unauthorized activity relating to source code or sensitive account information. . Out of an abundance of caution, some user accounts may have been reset even if a strong password was being used.  Activity on these accounts showed logins from IP addresses involved in this incident. . The  Security History page  logs important events involving your account.  If you had a strong password or GitHub’s  two factor authentication  enabled you may have still seen attempts to access your account that have failed. . This is a great opportunity for you to  review your account , ensure that you have a  strong password  and enable  two-factor authentication . . While we aggressively rate-limit login attempts and passwords are  stored properly , this incident has involved the use of nearly 40K unique IP addresses. These addresses were used to slowly brute force weak passwords or passwords used on multiple sites. We are working on additional rate-limiting measures to address this. In addition, you will no longer be able to login to GitHub.com with commonly-used weak passwords. . If you have any questions or concerns please  let us know . ", "date": "November 20, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tJoin our Octostudy!\t\t", "author": ["\n\t\tChrissie Brodigan\t"], "link": "https://github.blog/2013-11-20-join-our-octostudy/", "abstract": " There are a lot of interesting people on GitHub today. Since we can’t meet everyone at a conference,  drinkup , or charity  dodgeball  game, we are hoping you can tell us a little more about yourself. . Please take a minute to fill out this short  survey . You’ll be helping us learn how we can make GitHub even better for you. .     . Cheers &amp; Octocats! . (Also: tell your friends.) ", "date": "November 20, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tIntroducing Boxen\t\t", "author": ["\n\t\tWill Farrington\t"], "link": "https://github.blog/2013-02-15-introducing-boxen/", "abstract": " Today we’re proud to open source  Boxen , our tool for automating and managing Macs at GitHub. .   . Boxen started nearly a year ago as a project called “The Setup” — a pipe dream to let anyone at GitHub run GitHub.com on their development machine with a single command. Now every new GitHubber’s first day starts with unboxing a new laptop and hacking on GitHub in about 30 minutes. . Boxen is a  framework  for managing almost every aspect of your Mac. We built a massive  standard library  of  Puppet  modules optimized for Boxen to manage everything from running MySQL to installing Minecraft. . We designed Boxen with teams that work like GitHub in mind. Boxen automatically updates itself every run and opens and closes GitHub Issues as problems arise and get fixed. With Boxen, we treat our development environments with the same care we give production: we test our code and rely on Continuous Integration to deploy changes. . Start here to build a boxen for your team. This repo is our recommended template for a basic web development environment. The  README  shows you how to get started, and the basic configuration  shows off some of Boxen’s standard modules for managing system services and your team’s projects. . Once you’ve built a boxen for your team, boxen-web is the easiest way to roll it out. It’s a simple Rails app that allows you to onboard folks with a single Terminal command. It’s easy to run on Heroku and uses OAuth to limit access to only members of your GitHub organization. . Now that your team uses Boxen, you probably want to add support for new services and tools. Starting a new Puppet module can be a little complex, so we’ve wrapped everything you need to write one up into this example module. It follows our own best practices about Puppet module development with tools like  cardboard ,  puppet-lint , and  rspec-puppet . . Once your team writes  project manifests  for your applications, any member of your team can run them locally with ease.  At any time, you can run a single command to get a project and all its dependencies ready to go.  If any GitHubber wants to hack on  GitHub for Mac  all they need to do is run: . All sorts of GitHubbers use and improve Boxen: shippers, HR, lawyers, designers, and developers. More than 50 people have contributed internal fixes and features, and updates ship almost every day. . We love Boxen. We hope you and your team love it too. Happy automating! ", "date": "February 15, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tTCMalloc and MySQL\t\t", "author": ["\n\t\tTed Nyman\t"], "link": "https://github.blog/2013-02-21-tcmalloc-and-mysql/", "abstract": " Over the last month or so, we noticed steady week-over-week rises in overall MySQL query time. A poorly-performing database makes for a slow site and a slow site makes for a terrible experience, so we dug in to see what was going on. . The investigation is probably more interesting than the actual solution. We started with the most basic things: taking a look at long-running queries, database size, indexes. Nothing in that was especially fruitful. . We turned to some extremely helpful open source tools from Percona to figure out what was going on. I set up  pt-stalk  to trigger alerts and dumps of diagnostic data whenever MySQL’s running thread count got inordinately high. We sent stats about frequency of  pt-stalk  triggers to our Graphite instance, and I set up an alert in our Campfire that audibly pinged me on every trigger. The constant sound of the “submarine” alert in my Campfire client was driving me to Poe-esque insanity, so I was doubly motivated to fix this. . With more investigation, we were able to rule out a number of other potential causes, including physical problems on the machine, network issues, and IO issues. Naturally, the submarine noises kept going off, and at one point I may or may not have heard Sean Connery say “Give me a ping, Vasily. One ping only, please.” . Luckily, I noticed  pt-stalk  data dumps and  SHOW ENGINE INNODB STATUS  revealing hundreds of these bad things: . Issues in MySQL 5.1 and 5.5 (we run 5.1 for now) with  kernel_mutex  around transactions have been  known for some time . They have also been known to my friend and increasingly well-regarded yoga expert @jamesgolick, who last year  wrote about the exact issue  we were encountering. . So we took a page from Golick’s work and from our own thinking on the issue, and went forward with the switch from stock malloc to  TCMalloc . . Different allocators are good at different things, and TCMalloc (available as part of  gperftools ) is particularly good at reducing lock contention in programs with a lot of threads. So @tmm1 dropped in a straight-forward  export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so\" , and with a database restart we were off to the races. . Good things happened. After the switchover, those  pt-stalk  triggers declined to about 1/7th of their original frequency, and the previously-seen contention dropped significantly. I had expected at best a stop in rising query times with this change, but we ended up with something much better — a rather shocking 30 percent improvement in query performance across the board and a general decline in spikes. . I allowed myself a small celebration by reading The Raven in silence and enjoying a non-trivial amount of Nutella. ", "date": "February 21, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub for Windows Recent Improvements\t\t", "author": ["\n\t\tPhil Haack\t"], "link": "https://github.blog/2013-02-21-github-for-windows-recent-improvements/", "abstract": " It’s been almost a year since we first released  GitHub for Windows . Today we just shipped version 1.0.38. That’s 38 updates since 1.0! . As we’ve said before,  we ship early and often . . Since we’ve been mostly quiet about the work we’ve done with GitHub for Windows, I thought I’d summarize some of the improvements we’ve made recently. . You may have heard that Microsoft announced an early preview of their  Git integration for Visual Studio . The easiest way to get a Git repository into Visual Studio is to clone it via GitHub for Windows. When you navigate to the Team Explorer, it’ll be listed there. .   . Repositories now have an indicator that shows whether they are a private, public, or non-GitHub repository. .   . If you have a URL to a Git repository, you can now simply drag and drop it onto GitHub for Windows’ dashboard to clone that repository. This is handy if you have repositories not on GitHub that you need to work with. .   . Say you have some code that’s not yet in a Git repository. Simply drag it onto the dashboard and we’ll create a new Git repository with the code. .   . If you haven’t installed GitHub for Windows,  download it from windows.github.com . It’s the easiest way to get Git on a Windows machine. ", "date": "February 21, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tUpcoming IP address changes\t\t", "author": ["\n\t\tScott J. Goldman\t"], "link": "https://github.blog/2013-04-02-upcoming-ip-address-changes/", "abstract": " As we expand our infrastructure, we are making changes to the  IP addresses currently in use. . If you are explicitly whitelisting GitHub in your firewall rules,  please make sure that you include all of the IP ranges  as described in  this article . . Soon, we will begin using additional IP addresses (A records) for GitHub.com in order to add more load balancers. . Starting Thursday, April 4, 2013, GitHub.com will also resolve to  204.232.175.90 , in addition to the current IP  ( 207.97.227.239 ). Both addresses resolve to GitHub.com via reverse DNS. . If you are using SSH, you will see this message the first time you see this IP  address: . This warning is the expected behavior of SSH clients. . Please note that in the near future, we will be adding an additional IP subnet:   192.30.252.0/22  (or as a range,  192.30.252.1 - 192.30.255.255 ). ", "date": "April 2, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tNew GitHub Pages domain: github.io\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2013-04-05-new-github-pages-domain-github-io/", "abstract": " Beginning today, all  GitHub Pages  sites are moving to a new, dedicated  domain:  github.io . This is a security measure aimed at removing potential  vectors for cross domain attacks targeting the main github.com session as well  as vectors for phishing attacks relying on the presence of the “github.com”  domain to build a false sense of trust in malicious websites. . If you’ve configured a  custom domain  for your Pages site (“yoursite.com”  instead of “yoursite.github.com”) then  you are not affected by this change   and may stop reading now. . If your Pages site was previously served from a  username.github.com  domain,  all traffic will be redirected to the new  username.github.io  location  indefinitely, so you won’t have to change any links. For example,  newmerator.github.com  now redirects to  newmerator.github.io . . From this point on, any website hosted under the  github.com  domain may be  assumed to be an official GitHub product or service. . Please  contact support  if you experience any issues due to these changes.  We’ve taken measures to prevent any serious breakage but this is a major change  and could have unexpected consequences. Do not hesitate to  contact support  for assistance. . Changes to Pages sites and custom domains: . Changes to GitHub repositories: . There are two broad categories of potential security vulnerabilities that led to  this change. . Session fixation and CSRF vulnerabilities resulting from a browser security issue  sometimes referred to as “Related Domain Cookies”. Because Pages sites  may include custom JavaScript and were hosted on github.com subdomains,  it was possible to write (but not read) github.com domain cookies in  way that could allow an attacker to deny access to github.com and/or fixate  a user’s CSRF token. . Phishing attacks relying on the presence of the “github.com” domain to  create a false sense of trust in malicious websites. For instance, an  attacker could set up a Pages site at “account-security.github.com” and ask  that users input password, billing, or other sensitive information. . We have no evidence of an account being compromised due to either type of  vulnerability and have mitigated all known attack vectors. ", "date": "April 5, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tYummy cookies across domains\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2013-04-09-yummy-cookies-across-domains/", "abstract": " Last Friday we announced and performed a migration of all GitHub Pages to their own   github.io  domain. This was a long-planned migration, with the specific goal  of mitigating phishing attacks and cross-domain cookie vulnerabilities arising from  hosting custom user content in a subdomain of our main website. . There’s been, however, some confusion regarding the implications and impact of these  cross-domain cookie attacks. We hope this technical blog post will help clear things up. . When you log in on GitHub.com, we set a session cookie through the HTTP headers  of the response. This cookie contains the session data that uniquely identifies  you: . The session cookies that GitHub sends to web browsers are set on the  default domain ( github.com ), which means they are not accessible from any  subdomain at  *.github.com . We also specify the  HttpOnly  attribute, which means they  cannot be read through the  document.cookie  JavaScript API. Lastly, we specify the   Secure  attribute, which means that they will only be transferred through  HTTPS. .  Hence, it’s never been possible to read or “steal” session cookies from a GitHub  Pages hosted site . Session cookies are simply not accessible from the user code  running in GitHub Pages, but because of the way web browsers send cookies in HTTP  requests, it was possible to  “throw”  cookies from a GitHub Pages site to the  GitHub parent domain. . When the web browser performs an HTTP request, it sends the matching cookies  for the URL in a single  Cookie:  header, as key-value pairs. Only the cookies  that match the request URL will be sent. For example, when performing a request to   github.com , a cookie set for the domain  github.io  will not be sent, but a  cookie set for  .github.com  will. . Cookie tossing issues arise from the fact that the  Cookie  header only  contains the name and value for each of the cookies, and none of the extra  information with which the cookies were set, such as the  Path  or  Domain . . The most straightforward cookie-tossing attack would have involved using the   document.cookie  JavaScript API to set a  _session  cookie on a GitHub Pages  hosted website. Given that the website was hosted under  *.github.com , this  cookie would have been sent to all requests to the parent domain, despite the  fact it was set in a subdomain. . In this example, the cookie set through JavaScript in the subdomain is sent  next to the legitimate cookie set in the parent domain,  and there is no way  to tell which one is coming from where  given that the  Domain ,  Path ,   Secure  and  HttpOnly  attributes are not sent to the server. . This is a big issue for most web servers, because  the ordering of the cookies  set in a domain and in its subdomains is not specified by RFC 6265 , and web  browsers can choose to send them in any order they please. . In the case of Rack, the web server interface that powers Rails and Sinatra,  amongst others, cookie parsing happens as follows: . If there is more than one cookie with the same name in the  Cookie:  header,  the first one will be arbitrarily assumed to be the value of the cookie. . This is a very well-known attack: A couple weeks ago, security researcher Egor  Homakov  blogged about a proof-of-concept attack  just like this  one. The impact of the vulnerability was not critical (CSRF tokens get reset  after each log-in, so they cannot be permanently fixated), but it’s a very  practical example that people could easily reproduce to log out users and be  generally annoying. This forced us to rush our migration of GitHub  Pages to their own domain, but left us with a few weeks’ gap (until the  migration was complete), during which we had to mitigate the disclosed attack  vector. . Fortunately, the style of the disclosed attack was simple enough to mitigate on the server  side. We anticipated, however, several other attacks that were either trickier  to stop, or simply impossible. Let’s take a look at them. . The first step was mitigating the attack vector of simple cooking tossing.  Again, this attack exploits the fact that web browsers will send two cookie  tokens with the same name without letting us know the domain in which they were  actually set. . We cannot see  where  each cookie is coming from, but if we skip the cookie  parsing of Rack, we can see  whether any given request has two duplicate   _session  cookies . The only possible cause for this is that somebody is  attempting to throw cookies from a subdomain, so instead of trying to guess  which cookie is legitimate and which cookie is being tossed, we simply instruct  the web browser to  drop the cookie set in the subdomain before proceeding . . To accomplish this, we craft a very specific response: we instruct the web  browser to redirect to the same URL that was just requested, but with a   Set-Cookie  header that drops the subdomain cookie. . We decided to implement this as a Rack middleware. This way the cookie check  and consequent redirect could be performed before the application code gets to  run. . When the Rack middleware triggers, the redirect will happen transparently  without the user noticing, and the second request will contain only one   _session  cookie: the legitimate one. . This “hack” is enough to mitigate the straightforward cookie tossing attack  that most people would attempt, but there are more complex attacks that we also  need to consider. . If the malicious cookie is set for a specific path which is not the root (e.g.   /notifications ) the web browser will send that cookie when the user visits   github.com/notifications , and when we try to clear it in the root path, our  header will have no effect. . The solution is pretty straightforward, albeit rather inelegant: for any given  request URL, the web browser would only send a malicious JavaScript cookie if  its  Path  matches  partially  the path of the request URL. Hence, we only  need to attempt to drop the cookie once in each component of the path: . Again, we’re blind on the server-side when it comes to cookies. Our only option  is this brute-force approach to clearing the cookies, which despite its roughness,  worked surprisingly well while we completed the  github.io  migration. . Let’s step up our game: Another attack can be performed by exploiting the fact  that RFC 6265 doesn’t specify an escaping behavior for cookies. Most web  servers/interfaces, including Rack, assume that cookie names can be URL-encoded  (which is a rather sane assumption to make, if they contain non-ASCII  characters), and hence will unescape them when generating the cookie list: . This allows a malicious user to set a cookie that the web framework will  interpret as  _session  despite the fact that its name in the web browser is   not   _session . The attack simply has to escape characters that don’t  necessarily need to be escaped: . If we try to drop the second cookie from the list of cookies that Rack  generated, our header will have no effect. We’ve lost crucial  information after Rack’s parsing: the fact that the name of the cookie was  URL-encoded to a different value than the one our web framework received. . To work around this, we had to skip Rack’s cookie parsing by disabling the  unescaping and finding all the cookie names that would match our target  after  unescaping . . This way we can actually drop the right cookie (be it either set as  _session   or as a escaped variation). With this kind of Middleware in place, we were able  to tackle all the cookie tossing attacks that  can be tackled  on the server  side. Unfortunately, we were aware of another vector which made middleware  protection useless. . If you’re having cookie problems I feel bad for you, son.  I’ve got 99 cookies and my domain’s ain’t one. . This is a slightly more advanced attack that exploits the hard limit that all  web browsers have on the number of cookies that can be set per domain. . Firefox, for example, sets this hard limit to 150 cookies, while Chrome sets it  to 180. The problem is that this limit is not defined per cookie  Domain   attribute, but by the actual domain where the cookie was set. A single HTTP  request to any page on the main domain and subdomains will send a maximum  number of cookies, and the rules for which ones are picked are, once again,  undefined. . Chrome for instance doesn’t care about the cookies of the parent domain, the  ones set through HTTP or the ones set as  Secure : it’ll send the 180 newest  ones. This makes it trivially easy to “knock out” every single cookie from the  parent domain and replace them with fake cookies, all by running JavaScript on  a subdomain: . After setting these 180 cookies in the subdomain, all the cookies from the  parent domain vanish. If now we expire the cookies we just set, also from  JavaScript, the cookie list for both the subdomain and the parent domain  becomes empty: . This allows us to perform a single request with just  one   _session  cookie:  the one we’ve crafted in JavaScript. The original  Secure  and  HttpOnly    _session  cookie is now gone, and there is no way to detect in the web server  that the cookie being sent is neither  Secure ,  HttpOnly , nor set in the  parent domain, but fully fabricated. . With only one  _session  cookie sent to the server, there is no way to know  whether the cookie is being tossed at all. Even if we could detect an invalid  cookie, the same attack can be used to simply annoy users by logging them out  of GitHub. . As we’ve seen, by overflowing the cookie jar in the web browser, we can craft  requests with evil cookies that cannot be blocked server-side. There’s nothing  particularly new here: Both Egor’s original proof of concept and the variations  exposed here have been known for a while. . As it stands right now, hosting custom user content under a subdomain is simply  a security suicide, particularly accentuated by Chrome’s current implementation choices.  While Firefox handles more gracefully the distinction between Parent Domain and Subdomain  cookies (sending them in more consistent ordering, and separating their storage to prevent overflows  from a subdomain), Chrome performs no such distinction and treats session  cookies set through JavaScript the same way as  Secure HttpOnly  cookies set from the server,  leading to a very enticing playground for tossing attacks. . Regardless, the behavior of cookie transmission through HTTP headers is so ill-defined and  implementation-dependent that it’s just a matter of time until somebody comes  up with yet another way of tossing cookies across domains, independent of the targeted  web browser. . While cookie tossing attacks are not necessarily  critical  (i.e. it is not possible  to hijack user sessions, or accomplish anything anything besides phishing/annoying the  users), they are worringly straightforward to perform, and can be quite annoying. . We hope that this article will help raise awareness of the issue and the difficulties  to protect against these attacks by means that don’t involve a full domain migration:  a drastic, but ultimately necessary measure. ", "date": "April 9, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tEscape Velocity\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2013-04-17-escape-velocity/", "abstract": " We work very hard to keep GitHub fast. Ruby is not the fastest programming language, so we go to great lengths benchmarking and optimizing our large codebase: our goal is to keep bringing down response times for our website even as we add more features every day. Usually this means thinking about and implementing new features with a deep concern for performance (our motto has always been “It’s not fully shipped until it’s fast”), but sometimes optimizing means digging deep into the codebase to find old pieces of code that are not as performant as they could be. . The key to performance tuning is always profiling, but unfortunately the current situation when it comes to profiling under Ruby/MRI is not ideal. We’ve been using @tmm1’s experimental   rblineprof   for it. This little bundle of programming joy hooks into the Ruby VM and traces the stack of your process at a high frequency. This way, as your Ruby code executes,  rblineprof  can gather the accumulated time spent on each line of your codebase, and dump informative listings with the data. This is incredibly useful for finding hotspots on any Ruby application and optimizing them away. . Last week, we traced a standard request to our main Rails app trying to find bottlenecks in our view rendering code, and got some interesting results: . Surprisingly enough, the biggest hotspots in the view rendering code all had the same origin: the  escape_once  helper that performs HTML escaping for insertion into the view. Digging into the source code for that method, we saw that it was indeed not optimal: .  escape_once  performs a Regex replacement (with a rather complex regex), with table lookups in Rubyland for each replaced character. This means very expensive computation times for the regex matching, and a lot of temporary Ruby objects allocated which will have to be freed by the garbage collector later on. .  Houdini  is a set of C APIs for performing escaping for the web. This includes HTML, hrefs, JavaScript, URIs/URLs, and XML. It also performs unescaping, but we don’t talk about that because it spoils the joke on the project name. It has been designed with a focus on security (both ensuring the proper and safe escaping of all input strings, and avoiding buffer overflows or segmentation faults), but it is also highly performant. . Houdini uses different approaches for escaping and unescaping different data types: for instance, when unescaping HTML, it uses a perfect hash (generated at compile time) to match every map entity with the character it represents. When escaping HTML, it uses a lookup table to output escaped entities without branching, and so on. . We wrote Houdini as a C library so we could reuse it from the many programming languages we use internally at GitHub. The first implementation using them is @brianmario’s  EscapeUtils  gem, whose custom internal escaping functions were discarded and replaced with Houdini’s API, while keeping the well-known and stable external API. . We had been using EscapeUtils in some places of our codebase already, so it was an obvious choice to simply replace the default  escape_once  with a call to  EscapeUtils.escape_html  and see if we could reap any performance benefits. . When it comes to real-world performance in Ruby programs, EscapeUtil’s biggest advantage (besides the clearly performant C implementation behind it) is that Houdini is able to lazily escape strings. This means it will only allocate memory for the resulting string if the input contains escapable characters. Otherwise, it will flag the string as clean and return the original version, with no extra allocations and no objects to clean up by the GC. This is a massive performance win on an average Rails view render, which escapes thousands of small strings, most of which don’t need to be escaped at all. . Once the escaping method was replaced with a call to EscapeUtils, we ran our helpful  ./script/bench . This benchmarking script allows us to compare different branches of our main app and different Ruby versions or VM tweaks to see if the optimizations we are performing have any effect. It runs a specified number of GETs on any route of the Rails app, and measures the average time per request and the amount of Ruby objects allocated. . Not bad at all! Just by replacing the escaping function with a more optimized one, we’ve reduced the average request time by 45ms, and we’re allocating 20,000 less Ruby objects per request. That was a  lot  of escaped HTML right there! .  rblineprof  is still experimental, but if you’re working with Ruby, make sure to  check it out : @tmm1 has just added support for Ruby 2.0. . And for those of you not running Ruby, we are also open-sourcing the escaping implementation we’re now using in GitHub.com as a C library, so you can wrap it and use it from your language of choice. You can find it at  vmg/houdini . ", "date": "April 17, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tContent Security Policy\t\t", "author": ["\n\t\tJoshua Peek\t"], "link": "https://github.blog/2013-04-19-content-security-policy/", "abstract": " We’ve started rolling out a new security feature called “Content Security Policy” or CSP. As a user, it will better protect your account against XSS attacks. But, be aware, it may cause issues with some browser extensions and bookmarklets. .  Content Security Policy  is a new HTTP header that provides a solid safety net against XSS attacks. It does this by blocking inline scripts and limiting the domains that other scripts can be loaded from. This doesn’t mean you can forget about escaping user data on the server side, but if you screw up, CSP will give you a last layer of defense. . Activating CSP in a Rails app is trivial since it’s just a simple header. You don’t need any separate libraries; a simple before filter should do. . The header defines whitelisted urls that content can be loaded from. The  script-src  and  style-src  directives are both configured to our asset host’s (or CDNs) base URL. Then, no scripts can be loaded from hosts other than ours. Lastly,  default-src  is a catch-all for all other directives we didn’t define. For example,  image-src  and  media-src  can be used to restrict urls that images, video, and audio can loaded from. . If you want to broaden your browser support, set the same header value for  X-Content-Security-Policy  and  X-WebKit-CSP  as well. Going forward, you should only have to worry about the  Content-Security-Policy  standard. . As CSP implementations mature, this might become an out of the box feature built into Rails itself. . Turning on CSP is easy, getting your app CSP ready is the real challenge. . Unless  unsafe-inline  is set, all inline script tags are blocked. This is the main protection you’ll want against XSS. . Most of our prior inline script usage was page specific configuration. . A better place to put configuration like this would be in a relevant  data-*  attribute. . Like inline script tags, inline event handlers are now out too. . If you’ve written any JS after 2008, you’ve probably used an unobtrusive style of attaching event handlers. But you may still have some inline handlers lurking around your codebase. . Until Rails 3, Rails itself generated inline handlers for certain  link_to  and  form_tag  options. . would output . With Rails 3, it now emits a declarative data attribute. . You’ll need to be using a UJS driver like   jquery-ujs   or   rails-behaviors   for these data attributes to have any effect. . The use of  eval()  is also disabled unless  unsafe-eval  is set. . Though you may not be using  eval()  directly in your app code, if you are using any sort of client side templating library, it might be. Typically string templates are parsed and compiled into JS functions which are evaled on the client side for better performance. Take  @jeresig ‘s  classic micro-templating script  for an example. A better approach would be precompiling these templates on the server side using a library like  sstephenson/ruby-ejs . . Another gotcha is returning JavaScript from the server side via RJS or a “.js.erb” template. These would be actions using  format.js  in a  respond_to  block. Both jQuery and Prototype need to use  eval()  to run this code from the XHR response. It’s unfortunate that this doesn’t work, since your own server is white listed in the  script-src  directive. Browsers would need native support for evaluating  text/javascript  bodies in order to enforce the CSP policy correctly. . Unless  unsafe-inline  is set on  style-src , all inline style attributes are blocked. . The most common use case is to hide an element on load. . A better approach here would be using a CSS state class. . Though, there are caveats to actually using this feature. Libraries that do any sort of feature detection like jQuery or Modernizr typically generate and inject custom css into the page which sets off CSP alarms. So for now, most applications will probably need to just disable this feature. . As made clear by the CSP spec, browser bookmarklets shouldn’t be affected by CSP. . Enforcing a CSP policy should not interfere with the operation of user-supplied scripts such as third-party user-agent add-ons and JavaScript bookmarklets. .  http://www.w3.org/TR/CSP/#processing-model  . Whenever the user agent would execute script contained in a javascript URI, instead the user agent must not execute the script. (The user agent should execute script contained in “bookmarklets” even when enforcing this restriction.) .  http://www.w3.org/TR/CSP/#script-src  . But, none of the browsers get this correct. All cause CSP violations and prevent the bookmarklet from functioning. . Though its highly discouraged, you can disable CSP in Firefox as a temporary workaround. Open up  about:config  and set  security.csp.enable  to  false . . As with bookmarklets, CSP isn’t supposed to interfere with any extensions either. But in reality, this isn’t always the case. Specifically, in Chrome and Safari, where extensions are built in JS themselves, its typical to make modifications to the current page which may trigger a CSP exception. . The Chrome  LastPass  extension has some issues with CSP compatibility since it attempts to inject inline  &lt;script&gt;  tags into the current document. We’ve contacted the LastPass developers about the issue. . As part of the default CSP restrictions, inline CSS is disabled unless  unsafe-inline  is set on the  style-src  directive. At this time, only Chrome actually implements this restriction. . You can still dynamically change styles via the CSSOM. . The user agent is also not prevented from applying style from Cascading Style Sheets Object Model (CSSOM). .  http://www.w3.org/TR/CSP/#style-src  . This is pretty much a requirement if you intend to implement something like custom tooltips on your site which need to be dynamically absolutely positioned. . Though there still seems to be some bugs regarding inline style serialization. . An example of a specific bug is cloning an element with a style attribute. . Also, as noted above, libraries that do feature detection like jQuery and Modernizr are going to trigger this exception as they generate and inject custom styles to test if they work. Hopefully, these issues can be resolved in the libraries themselves. . The CSP reporting feature is actually a pretty neat idea. If an attacker found a legit XSS escaping bug on your site, victims with CSP enabled would report the violation back the server when they visit the page. This could act as sort of an XSS intrusion detection system. . However, because of the current state of bookmarklet and extension issues, most CSP violations are false positives that flood your reporting backend. Depending on the browser, the report payload can be pretty vague. You’re lucky to get a line number (without any offset) on a minified js file when a script triggers a violation. It’s usually impossible to tell if the error is happening in your JS or some extension inject code. This makes any sort of filtering impossible. . Even with these issues, we are still committing to rolling out CSP. Hopefully a wider CSP adoption helps smooth out these issues in the upcoming  CSP 1.1 draft . . Also, special thanks to  @mikewest  at Google for helping us out. ", "date": "April 19, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tHeads up: nosniff header support coming to Chrome and Firefox\t\t", "author": ["\n\t\tBen Toews\t"], "link": "https://github.blog/2013-04-24-heads-up-nosniff-header-support-coming-to-chrome-and-firefox/", "abstract": " Both GitHub and Gist offer ways to view “raw” versions of user content. Instead of viewing files in the visual context of the website, the user can see the actual text content as it was commited by the author. This can be useful if you want to select-all-and-copy a file or just see a Markdown file without having it be rendered. The key point is that this is a feature to improve the experience of our human users. . Some pesky non-human users (namely computers) have taken to “hotlinking” assets via the raw view feature — using the raw URL as the  src  for a  &lt;script&gt;  or  &lt;img&gt;  tag. The problem is that these are not static assets. The raw file view, like any other view in a Rails app, must be rendered before being returned to the user. This quickly adds up to a big toll on performance. In the past we’ve been forced to block popular content served this way because it put excessive strain on our servers. . We added the  X-Content-Type-Options: nosniff  header to our raw URL responses way back in 2011 as a first step in combating hotlinking. This has the effect of forcing the browser to treat content in accordance with the  Content-Type  header. That means that when we set  Content-Type: text/plain  for raw views of files, the browser will refuse to treat that file as JavaScript or CSS. . Until recently, Internet Explorer has been the only browser to respect this header, so this method of hotlinking prevention has not been effective for many users. We’re happy to report that the good people at Google and Mozilla are moving towards adoption as well. As  nosniff  support is added to Chrome and Firefox, hotlinking will stop working in those browsers, and we wanted our beloved users, human and otherwise, to know why. ", "date": "April 24, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tHey Judy, don’t make it bad\t\t", "author": ["\n\t\tVicent Martí\t"], "link": "https://github.blog/2013-05-01-hey-judy-don-t-make-it-bad/", "abstract": " Last week we explained how  we greatly reduced the rendering time of our web views  by switching our escaping routines from Ruby to C. This speed-up was two-fold: the C code for escaping HTML was significantly faster than its Ruby equivalent, and on top of that, the C code was generating a lot fewer objects on the Ruby heap, which meant that subsequent garbage collection runs would run faster. . When working with a mark and sweep garbage collector (like the one in MRI), the amount of objects in the Heap at any given moment of time matters a lot. The more objects, the longer each GC pause will take (all the objects must be traversed during the Mark phase!), and since MRI’s garbage collector is also “stop the world”, while GC is running Ruby code cannot be executing, and hence web requests cannot be served. . In Ruby 1.9 and 2.0, the  ObjectSpace  module contains useful metadata regarding the current state of the Garbage collector and the Ruby Heap. Probably the most useful method provided by this module is  count_objects , which returns the amount of objects allocated in the Ruby heap, separated by type: this offers a very insightful birds-eye view of the current state of the heap. . We tried running  count_objects  on a fresh instance of our main Rails application, as soon as all the libraries and dependencies were loaded: . Whelp! More than 600k Ruby objects allocated just after boot! That’s a lotta heap, like we say in my country. The obvious question now is whether all those objects on the heap are actually necessary, and whether we can free or simply prevent from allocating some of them to reduce our garbage collection times. . This question, however, is rather hard to answer by using only the  ObjectSpace  module. Although it offers an  ObjectSpace#each_object  method to enumerate all the objects that have been allocated, this enumeration is of very little use because we cannot tell where each object was allocated and why. . Fortunately, @tmm1 had a master plan one more time. With  a few lines of code , he added a  __sourcefile__  and  __sourceline__  method to every single object in the Kernel, which kept track of the file and line in which the object was allocated. This is priceless: we are now able to iterate through every single object in the Ruby Heap and pinpoint and aggregate its source of allocation. . Oh boy, let’s take a look at this in more detail. Clearly, there are allocation sources which we can do nothing about (the Rails core libraries, for example), but the biggest offender here looks very interesting. Psych is the YAML parser that ships with Ruby 1.9+, so apparently something is parsing  a lot  of YAML and keeping it in memory at all times. Who could this be? .  Linguist  is an open-source Ruby gem which we developed to power our language statistics for GitHub.com. . People push  a lot  of code to GitHub, and we needed a reliable way to identify and classify all the text files which we display on our web interface. Are they actually source code? What language are they written in? Do they need to be highlighted? Are they auto-generated? . The first versions of Linguist took a pretty straightforward approach towards solving these problems: definitions for all languages we know of were stored in a  YAML file , with metadata such as the file extensions for such language, the type of language, the lexer for syntax highlighting and so on. . However, this approach fails in many important corner cases. What’s in a file extension? that which we call  .h  by any other extension would take just as long to compile. It could be C, or it could be C++, or it could be Objective-C. We needed a more reliable way to separate these cases, and hundreds of other ambiguous situations in which file extensions are related to more than one programming language, or source files do not even have an extension. . That’s why we decided to augment Linguist with a very simple classifier: Armed with a pocket-size Library of Babel of Code Samples (that is, a collection of source code files from different languages hosted in GitHub) we attempted to perform a weighted classification of all the new source code files we encounter. . The idea is simple: when faced with a source code file which we cannot recognize, we tokenize it, and then use a weighted classifier to find out the likehood that those tokens in the file belong to a given programming language. For example, an  #include  token is very likely to belong to a C or a C++ file, and not to a Ruby file. A  class  token can very well belong to a C++ file or a Ruby file, but if we find both an  #include  and a  class  token on the same file, then the answer is most definitely C++. . Of course, to perform this classification, we need to keep in memory a large list of tokens for every programming language that is hosted on GitHub, and their respective probabilities. It was this collection of tokens which was topping our allocation meters for the Ruby Garbage collector. For the classifier to be accurate, it needs to be trained with a large dataset –the bigger the better–, and although 36000 token samples are barely enough for training a classifier, they are  a lot  for the poor Ruby heap. . We had a very obvious plan to fix this issue: move the massive token dataset out of the Ruby Heap and into native C-land, where it doesn’t need to be garbage collected, and keep it as compact as possible in memory. . For this, we decided to store the tokens in a  Judy Array , a trie-like data structure that acts as an associative array or key-value store with some very interesting performance characteristics. . As opposed to traditional trie-like data structures storing strings, branches happen at the bit-level (i.e. the Judy Array acts as a 256-ary trie), and their nodes are highly compressed: the claim is that thanks to this compression, Judy Arrays can be packed extremely tightly in cache lines, minimizing the amount of cache misses per lookup. The supposed result of this are lookup times that can compete against a hash table, even though the algorithmic complexity of Judy Arrays is  O(log n) , like any other trie-like structure. . Of course, there is no real-world silver bullet when it comes to algorithmic performance, and Judy Arrays are no exception. Despite the claims in Judy’s original whitepaper, cache misses in modern CPU architectures do not fetch data stored in the Prison of Azkaban; they fetch it from the the L2 cache, which happens to be oh-not-that-far-away. . In practice, this means that the constant loss of time caused by a few (certainly not many) cache misses in a hash table lookups ( O(1) ) is not enough to offset the lookup time in a Judy array ( O(log n) ), no matter how tightly packed it is. On top of that, on hash tables with linear probing and a small step size, the point of reduced cache misses becomes moot, as most of the time collisions can be resolved in the same cache line where they happened. These practical results have been proven  over  and  over  again in real-world tests. At the end of the day, a properly tuned hash table will always be faster than a Judy Array. . Why did we choose Judy arrays for the implementation, then? For starters, our goal right now is not related to performance (classification is usually not a performance critical operation), but to maximizing the size of the training dataset while minimizing its memory usage. Judy Arrays, thanks to their remarkable compression techniques, store the keys of our dataset in a much smaller chunk of memory and with much less redundancy than a hash table. . Furthermore, we are pitting Judy Arrays against MRI’s Hash Table implementation, which is known to be not particularly performant. With some thought on the way the dataset is stored in memory, it becomes feasible to beat Ruby’s hash tables at their own game, even if we are performing logarithmic lookups. . The main design constraint for this problem is that the tokens in the dataset need to be  separated by language . The YAML file we load in memory takes the straightforward approach of creating one hash table per language, containing all of its tokens. We can do better using a trie structure, however: we can store all the tokens in the same Judy Array, but prefixing them with an unique 2-byte prefix that identifies their language. This creates independent subtrees of tokens inside the same global data structure for each different language, which increases cache locality and reduces the logarithmic cost of lookups. .   . For the average query behavior of the dataset (burst lookups of thousands of tokens of the same language in a row), having these subtrees means keeping the cache permanently warm, and minimzing the amount of traversals around the Array, since the internal Judy cursor never leaves the subtree for a language between queries. . The results of this optimization are much more positive than what we’d expect from benchmarking a logarithmic time structure against one which allegedly performs lookups in constant time: .   . In this benchmark where we have disabled MRI’s garbage collector, we can see how the lookup of 3.5 million tokens on the database stays more than 50% faster against the Hash Table, even as we artificially increase the dataset with random tokens. Thanks to the locality of the token subtrees per language, lookup times remain mostly constant and don’t exhibit a logarithmic behavior. . Things get even better for Judy Arrays when we enable the garbage collector and GC cycles start being triggered between lookups: .   . Here we can see how the massive size of the data structures in the Ruby Heap cause the garbage collector to go bananas, with huge spikes in lookup times as the dataset increases and GC runs are triggered. The Judy Array (stored outside the Ruby Heap) remains completely unfazed by it, and what’s more, manages to maintain its constant lookup time while Hash Table lookups become more and more expensive because of the higher garbage collection times. . The cherry on top comes from graphing the RSS usage of our Ruby process as we increase the size of our dataset: .   . Once again (and this time as anticipated), Judy Arrays throw MRI’s Hash Table implementation under a bus. Their growth remains very much linear and increases extremely slowly, while we can appreciate considerable bumps and very fast growth as hash tables get resized. . With the new storage engine for tokens on Linguist’s classifier, we are now able to dramatically expand our sampling dataset. A bigger dataset means more accurate classification of programming languages and more accurate language graphs on all repositories; this makes GitHub more awesome. . The elephant in the room still lives on in the shape of MRI’s garbage collector, however. Without a generational GC capable of finding and marking roots of the Heap that are very unlikely to be freed (if at all), we must keep permanent attention to the amount of objects we allocate on our main app. More objects not only mean higher memory usage: they also mean higher garbage collection times and slower requests. . The good news is that Koichi Sasada has recently  proposed a Generational Garbage Collector for inclusion in MRI 2.1 . This prototype is remarkable because it allows a subset of generational garbage collection to happen while maintaining compatibility with MRI’s current C extension API, which in its current iteration has several trade-offs (for the sake of simplicity when writing extensions) that make memory management for internal objects extremely difficult. . This compatibility with older versions, of course, comes at a price. Objects in the heap now need to be separated between “shady” and “sunny”, depending on whether they have write barriers or not, and hence whether they can be generationally collected. This enforces an overly complicated implementation of the GC interfaces (several Ruby C APIs must drop the write barrier from objects when they are used), and the additional bookkeeping needed to separate the different kind of objects creates performance regressions under lighter GC loads. On top of that, this new garbage collector is also forced to run expensive Mark &amp; Sweep phases for the young generation (as opposed to e.g. a copying phase) because of the design choices that make the current C API support only conservative garbage collection. . Despite the best efforts of Koichi and other contributors, Ruby Core’s concern with backwards compatibility (particularly regarding the C Extension API) keeps MRI lagging more than a decade behind Ruby implementations like  Rubinius  and  JRuby  which already have precise, generational and incremental garbage collectors. . It is unclear at the moment whether this new GC on its current state will make it into the next version of MRI, and whether it will be a case of “too little, too late” given the many handicaps of the current implementation. The only thing we can do for now is wait and see…  Or more like wait and  C . HAH. Amirite guys? Amirite? ", "date": "May 1, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tMantle: a Model Framework for Objective-C\t\t", "author": ["\n\t\tJustin Spahr-Summers\t"], "link": "https://github.blog/2012-10-22-mantle-a-model-framework-for-objective-c/", "abstract": " Lately, we’ve been shipping more in GitHub for Mac than ever before. Now that   username  autocompletion   and  Notification Center  support  are out the  door, we’re releasing the two frameworks that helped make it happen. . This post talks about   Mantle  , our framework  that makes it dead simple to create a flexible and easy-to-use model layer in  Cocoa or Cocoa Touch. In our  next blog  post ,  we’ll talk about  Rebel , our framework for  improving AppKit. . First, let’s explore why you would even want such a framework. What’s wrong with  the way model objects are usually written in Objective-C? . Let’s use the  GitHub API  for demonstration. How  would one typically represent a  GitHub  issue  in  Objective-C? . Whew, that’s a lot of boilerplate for something so simple! And, even then, there  are some problems that this example doesn’t address: . Core Data solves certain problems very well. If you need to execute complex  queries across your data, handle a huge object graph with lots of relationships,  or support undo and redo, Core Data is an excellent fit. . It does, however, come with some pain points: . If you’re just trying to access some JSON objects, Core Data can be a lot of  work for little gain. . Enter    MTLModel  .  This is what  GHIssue  looks like inheriting from  MTLModel : . Notably absent from this version are implementations of  &lt;NSCoding&gt; ,   &lt;NSCopying&gt; ,  -isEqual: , and  -hash . By inspecting the  @property   declarations you have in your subclass,  MTLModel  can provide default  implementations for all these methods. . The problems with the original example all happen to be fixed as well: . The URL transformer we used (included in Mantle) returns  nil  if given a  nil   string. .  MTLModel  has an extensible  -mergeValuesForKeysFromModel:  method, which makes  it easy to specify how new model data should be integrated. .  Both  of these issues are solved by using reversible transformers.   -[GHIssue externalRepresentation]  will return a JSON dictionary, which is also  what gets encoded in  -encodeWithCoder: . No saving fragile enum values! .  MTLModel  automatically saves the version of the model object that was used for  archival. When unarchiving,  +migrateExternalRepresentation:fromVersion:  will  be invoked if migration is needed, giving you a convenient hook to upgrade old  data. . Mantle also comes with miscellaneous cross-platform extensions meant to  make your life easier, including: . There will assuredly be more, as we run into other common pain points! . Mantle is still new and moving fast, so we may make breaking changes from  time-to-time, but it has excellent unit test coverage and is already being used  in GitHub for Mac’s production code. . We heartily encourage you to  check it out   and file any issues that you find. If you’d like to contribute code, take a look  at the  README . . Enjoy! ", "date": "October 22, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tRebel: a Framework for Improving AppKit\t\t", "author": ["\n\t\tJustin Spahr-Summers\t"], "link": "https://github.blog/2012-10-23-rebel-a-framework-for-improving-appkit/", "abstract": " In our  last blog  post , we  revealed  Mantle , our Cocoa model framework.  Today, we’re announcing   Rebel  ,  a framework for improving AppKit. . Since you may recall  our original TwUI  announcement , the decision to  start using AppKit again bears some explanation. . For a while now, we’ve been collaborators on Twitter’s   TwUI , a popular UI framework for the Mac. TwUI  made it easy to build a modern layer-based application for OS X. . However, the AppKit improvements in Lion and Mountain Lion include substantial  fixes for layer-backed views. On Snow Leopard, layer-backed  NSTextFields  and   NSTextViews  were almost unusable – now, most standard views behave sanely.   NSScrollView , in particular, no longer consumes an absurd amount of memory or  performs asynchronous tiling (so content no longer fades in while scrolling). . These fixes make TwUI less necessary, so we’re slowly migrating GitHub for Mac  back to be 100% AppKit, freeing up our development time to work on GitHub for  Mac instead of maintaining an entire UI framework alongside it. . As we move away from using TwUI, we will also become less active in its  development. We want to leave the framework in good hands, though, so if you’re  interested in helping maintain TwUI, please  open an  issue  and explain why you think you’d be  a good fit. . Still, AppKit isn’t perfect. .  Some significant  improvements   are only available on Mountain Lion. Even then, there are still some bugs  – silly things like horizontally scrolling  NSTextFields  ending up on half  pixels, or  NSScrollView  being unbearably slow. . Not to mention that many of its APIs are often difficult to use: . This is where   Rebel   comes in. Rebel aims  to solve the above problems, and whatever else we may run into. . There are fixes to the  NSTextField  blurriness   and  NSScrollView  performance . There are   iOS-like    resizable  images .  Let Rebel  figure out whether you’re animating or  not . . Have you seen the username autocompletion popover? .   . That’s   RBLPopover  at work! . We want to make AppKit easy and enjoyable to use  without  rewriting it from the  ground up. . Rebel is currently  alpha quality . We’re already using it in GitHub for Mac,  but we may still make breaking changes occasionally. . So,  check it out , enjoy, and please file any  issues that you find! ", "date": "October 23, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\thtml-pipeline: Chainable Content Filters\t\t", "author": ["\n\t\tJerry Cheung\t"], "link": "https://github.blog/2012-11-27-html-pipeline-chainable-content-filters/", "abstract": " Ever wondered how to get emoji, syntax highlighting, custom linking, and markdown to play nice together?  HTML::Pipeline  is the answer. .   . We’ve extracted several HTML utilities that we use internally in GitHub and packaged them into a gem called  html-pipeline . Here’s a short list of things you can do with it: . The basic unit for building a pipeline is a filter. A filter lets you take user input, do something with it, and spit out transformed markup. For example, if you wanted to translate Markdown into HTML, you can use the  MarkdownFilter : . Translating Markdown is useful, but what if you also wanted to syntax highlight the output HTML? A pipeline object lets you can chain different filters together so that the output of one filter flows in as the input of the next filter. So after we convert our Markdown text to HTML, we can pipe that HTML into another filter to handle the syntax highlighting: . There are pre-defined filters for autolinking urls, adding emoji, markdown and textile compilation, syntax highlighting, and more. It’s also easy to build your own filters to add into your pipelines for more customization. Check out the  project page  for a full reference. ", "date": "November 27, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tNetwork problems last Friday\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-05-network-problems-last-friday/", "abstract": " On Friday, November 30th, GitHub had a rough day. We experienced 18  minutes of complete unavailability along with sporadic bursts of slow  responses and intermittent errors for the entire day. I’m very sorry  this happened and I want to take some time to explain what happened,  how we responded, and what we’re doing to help prevent a similar  problem in the future. . Note: I initially forgot to mention that we had a single fileserver pair offline  for a large part of the day affecting a small percentage of repositories.  This was a side effect of the network problems and their impact on the  high-availability clustering between the fileserver nodes. My apologies  for missing this on the initial writeup. . To understand the problem on Friday, you first need to understand how  our network is constructed. GitHub has grown incredibly quickly over  the past few years. A consequence of that growth is that our infrastructure  has, at times, struggled to keep up with the growth. . Most recently, we’ve been seeing some significant problems with network  performance throughout our network. Actions that should respond in  under a millisecond were taking several times that long with occasional  spikes to hundreds of times that long. Services that we’ve wanted to  roll out have been blocked by scalability concerns and we’ve had a  number of brief outages that have been the result of the network  straining beyond the breaking point. . The most pressing problem was with the way our network switches were  interconnected. Conceptually, each of our switches were connected to  the switches in the neighboring racks. Any data that had to travel from  a server on one end of the network to a server on the other end had to  pass through all of the switches in between. This design often put  a very large strain on the switches in the middle of the chain and those  links became saturated, slowing down any data that had to pass through  them. . To solve this problem, we purchased additional switches to build what’s  called an aggregation network, which is more of a tree structure.  Network switches at the top of the tree (aggregation swtiches) are  directly connected to switches in each server cabinet (access switches).  This topology assures that data never has to move between more than  3 tiers: The switch in the originating cabinet, the aggregation switches,  and the switch in the destination cabinet. This allows the links between  switches to be much more efficiently used. . Last week the new aggregation switches finally arrived and were installed in  our datacenter. Due to the lack of available ports in our access  switches, we needed to disconnect access switches, change the  configuration to support the aggregation design, and then reconnect them to  the aggregation switches. Fortunately, we’ve built our network with  redundant switches in each server cabinet and each server is  connected to both of these switches. We generally refer to these as “A”  and “B” switches. . Our plan was to perform this operation on the B switches and observe  the behavior before transitioning to the A switches and completing the  migration. On Thursday, November 29th we made these changes on the B  devices and despite a few small hiccups the process went essentially  according to plan. We were initially encouraged by the data we were  collecting and planned to make similar changes to the A switches the  following morning. . On Friday morning, we began making the changes to bring the A switches  into the new network. We moved one device at a time and the maintenance  proceeded exactly as planned until we reached the final switch. As we  connected the final A switch, we lost connectivity with the B switch in  the same cabinet. Investigating further, we discovered a  misconfiguration on this pair of switches that caused what’s called a   “bridge loop”  in the  network. The switches are specifically configured to detect this sort  of problem and to protect the network by disabling links where  they detect an issue, and that’s what happened in this case. . We were able to quickly resolve the initial problem and return the  affected B switch to service, completing the migration. Unfortunately,  we were not seeing the performance levels we expected. As we  dug deeper we saw that all of the connections between the access  switches and the aggregation switches were completely  saturated. We initially diagnosed this as a  “broadcast  storm”  which is one  possible consequence of a bridge loop that goes undetected. . We spent most of the day auditing our switch  configurations again, going through every port trying to locate what  we believed to be a loop. As part of that process we decided to  disconnect individual links between the access and aggregation switches  and observe behavior to see if we could narrow the scope of the problem  further. When we did this, we discovered another problem: The moment we  disconnected one of the access/aggregation links in a redundant pair,  the access switch would disable its redundant link as well. This was  unexpected and meant that we did not have the ability to withstand a  failure of one of our aggregation switches. . We escalated this problem to our switch vendor and worked with them to  identify a misconfiguration. We had a setting that was intended to  detect partial link failure between two links. Essentially it would  monitor to try and ensure that both the transmit and receive functions  were functioning correctly. Unfortunately, this feature is not  supported between the aggregation and access switch models. When we  shut down an individual link, this watchdog process would erroneously  trigger and force all the links to be disabled. The 18 minute period of  hard downtime we had was during this troubleshooting process when we  lost connectivity to multiple switches simultaneously. . Once we removed the misconfigured setting on our access switches we  were able to continue testing links and our failover functioned as  expected. We were able to remove any single switch at either the  aggregation or access layer without impacting the underlying servers.  This allowed us to continue moving through individual links in the hunt  for what we still believed was a loop induced broadcast storm. . After a couple more hours of troubleshooting we were unable to track  down any problems with the configuration and again escalated to our  network vendor. They immediately began troubleshooting the problem with  us and escalated it to their highest severity level. We spent five  hours Friday night troubleshooting the problem and eventually  discovered a bug in the aggregation switches was to blame. . When a network switch receives an ethernet frame, it inspects the  contents of that frame to determine the destination MAC address. It  then looks up the MAC address in an internal  MAC address  table  to determine which port  the destination device is connected to. If it finds a match for the MAC  address in its table, it forwards the frame to that port. If, however,  it does not have the destination MAC address in its table it is forced  to “flood” that frame to all of its ports with the exception of the  port that it was received from. . In the course of our troubleshooting we discovered that our aggregation  switches were missing a number of MAC addresses from their tables, and  thus were flooding any traffic that was sent to those devices across  all of their ports. Because of these missing addresses, a large  percentage of our traffic was being sent to every access switch and not  just the switch that the destination devices was connected to. During  normal operation, the switch should “learn” which port each MAC address  is connected through as it processes traffic. For some reason, our  switches were unable to learn a significant percentage of our MAC  addresses and this aggregate traffic was enough to saturate all of the  links between the access and aggregation switches, causing the poor  performance we saw throughout the day. . We worked with the vendor until late on Friday night to formulate a  mitigation plan and to collect data for their engineering team to  review. Once we had a mitigation plan, we scheduled a network  maintenance window on Saturday morning at 0600 Pacific to attempt to  work around the problem. The workaround involved restarting some core  processes on the aggregation switches in order to attempt to allow them  to learn MAC addresses again. This workaround was successful and  traffic and performance returned to normal levels. .   . We have worked with our network vendor to provide diagnostic  information which led them to discover the root cause for the MAC  learning issues. We expect a final fix for this issue within the next  week or so and will be deploying a software update to our switches at  that time. In the mean time we are closely monitoring our aggregation  to access layer capacity and have a workaround process if the problem  comes up again. . We designed this maintenance so that it would have no impact on  customers, but we clearly failed. With this in mind, we are  planning to invest in a duplicate of our network stack from our routers  all the way through our access layer switches to be used in a staging  environment. This will allow us to more fully test these kinds of  changes in the future, and hopefully detect bugs like the one that  caused the problems on Friday. . We are working on adding additional automated monitoring to our  network to alert us sooner if we have similar issues. . We need to be more mindful of tunnel-vision during incident  response. We fixated for a very long time on the idea of a bridge loop  and it blinded us to other possible causes. We hope to begin doing more  scheduled incident response exercises in the coming months and will  build scenarios that reinforce this. . The very positive experience we had with our network vendor’s  support staff has caused us to change the way we think  about engaging support. In the future, we will contact their support  team at the first sign of trouble in the network. . We know you depend on GitHub and we’re going to continue to work hard  to live up to the trust you place in us. Incidents like the one we  experienced on Friday aren’t fun for anyone, but we always strive to  use them as a learning opportunity and a way to improve our craft. We  have many infrastructure improvements planned for the coming year and  the lessons we learned from this outage will only help us as we plan  them. . Finally, I’d like to personally thank the entire GitHub community  for your patience and kind words while we were working through these  problems on Friday. ", "date": "December 5, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tScheduled Maintenance Windows\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-20-scheduled-maintenance-windows/", "abstract": " As our infrastructure continues to grow and evolve, it’s sometimes  necessary to perform system maintenance that may cause downtime. We  have a number of projects queued up over the coming months to take  our infrastructure to the next level, so we are announcing a scheduled  maintenance window on Saturday mornings beginning at   0500 Pacific . . We do not intend to perform maintenance every Saturday, and even when we  do, most of them will not be disruptive to customers. We are using  these windows only in cases where the tasks we’re performing have a  higher than normal level of risk of impacting the site. . We will always update our  status site   before we begin and again when we’re done. In cases where we expect  there to be more than a few minutes of disruption we will also make  an announcement on the  GitHub Blog  by the  preceding Friday. . To get things started on the right foot, we will be performing an  upgrade of the software on some of our network switches this Saturday  during the new maintenance window. We do not expect this to cause any  visible disruption. ", "date": "December 20, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tDowntime last Saturday\t\t", "author": ["\n\t\tMark Imbriaco\t"], "link": "https://github.blog/2012-12-26-downtime-last-saturday/", "abstract": " On Saturday, December 22nd we had a significant outage and we want to take  the time to explain what happened. This was one of the worst outages in  the history of GitHub, and it’s not at all acceptable to us. I’m very  sorry that it happened and our entire team is working hard to prevent  similar problems in the future. . We had a scheduled maintenance window Saturday morning to perform  software updates on our aggregation switches. This software update was  recommended by our network vendor and was expected to address the  problems that we encountered in an   earlier outage . We  had tested this upgrade on a number of similar devices without  incident, so we had a good deal of confidence. Still, performing an  update like this is always a risky proposition so we scheduled a  maintenance window and had support personnel from our vendor  on the phone during the upgrade in case of unforseen problems. . In our network, each of our access switches, which our servers are  connected to, are also connected to a pair of aggregation switches.  These aggregation switches are installed in pairs and use a feature  called  MLAG  to appear as a  single switch to the access switches for the purposes of   link aggregation ,   spanning tree ,  and other layer 2 protocols that expect to have a single master device.  This allows us to perform maintenance tasks on one aggregation switch  without impacting the partner switch or the connectivity for the access  switches. We have used this feature successfully many times. . Our plan involved upgrading the aggregation switches one at a time, a  process called in-service software upgrade. You upload new software to  one switch, configure the switch to reboot on the new version, and issue  a reload command. The remaining switch detects that its peer is no longer  connected and begins a failover process to take control over the resources  that the MLAG pair jointly managed. . We ran into some unexpected snags after the upgrade that caused 20-30  minutes of instability while we attempted to work around them within  the maintenance window. Disabling the links between half of the  aggregation switches and the access switches allowed us to mitigate  the problems while we continued to work with our network vendor to  understand the cause of the instability. This wasn’t ideal since  it compromised our redundancy and only allowed us to operate at half  of our uplink capacity, but our traffic was low enough at the time  that it didn’t pose any real problems. At 1100 PST we made the decision  to revert the software update and return to a redundant state at 1300 PST if  we did not have a plan for resolving the issues we were experiencing  with the new version. . Beginning at 1215 PST, our network vendor began gathering some final  forensic information from our switches so that they could  attempt to discover the root cause for the issues we’d been seeing.  Most of this information gathering was isolated to collecting  log files and retrieving the current hardware status of various parts  of the switches. As a final step, they wanted to gather the state of  one of the agents running on a switch. This involves terminating the  process and causing it to write its state in a way that can be analyzed  later. Since we were performing this on the switch that had its  connections to the access switches disabled they didn’t expect there  to be any impact. We have performed this type of action, which is  very similar to rebooting one switch in the MLAG pair, many times in  the past without incident. . This is where things began going poorly. When the agent on one of the  switches is terminated, the peer has a 5 second timeout period where it  waits to hear from it again. If it does not hear from the peer, but  still sees active links between them, it assumes that the other switch  is still running but in an inconsistent state. In this situation it  is not able to safely takeover the shared resources so it defaults back  to behaving as a standalone switch for purposes of link aggregation,  spanning-tree, and other layer two protocols. . Normally, this isn’t a problem because the switches also watch for the  links between peers to go down. When this happens they wait 2 seconds  for the link to come back up. If the links do not recover, the  switch assumes that its peer has died entirely and performs a stateful  takeover of the MLAG resources. This type of takeover does not trigger  any layer two changes. . When the agent was terminated on the first switch, the links between  peers did not go down since the agent is unable to instruct the hardware  to reset the links. They do not reset until the agent restarts and is  again able to issue commands to the underlying switching hardware.  With unlucky timing and the extra time that is required for the agent  to record its running state for analysis, the link remained active  long enough for the peer switch to detect a lack of heartbeat messages  while still seeing an active link and failover using the more disruptive  method. . When this happened it caused a great deal of churn within the network as  all of our aggregated links had to be re-established, leader election for  spanning-tree had to take place, and all of the links in the network had  to go through a spanning-tree reconvergence. This effectively caused  all traffic between access switches to be blocked for roughly a minute  and a half. . Our fileserver architecture consists of a number of active/passive  fileserver pairs which use   Pacemaker ,   Heartbeat  and   DRBD  to manage high-availability. We use DRBD from the  active node in each pair to transfer a copy of any data that changes on  disk to the standby node in the pair. Heartbeat and Pacemaker work  together to help manage this process and to failover in the event of  problems on the active node. . With DRBD, it’s important to make sure that the data volumes are only  actively mounted on one node in the cluster. DRBD helps protect against  having the data mounted on both nodes by making the receiving side of  the connection read-only. In addition to this, we use a   STONITH  (Shoot The Other Node  In The Head) process to shut power down to the active node before failing  over to the standby. We want to be certain that we don’t wind up in a  “split-brain” situation where data is written to both nodes  simultaneously since this could result in potentially unrecoverable  data corruption. . When the network froze, many of our fileservers which are intentionally  located in different racks for redundancy, exceeded their heartbeat  timeouts and decided that they needed to take control of the fileserver  resources. They issued STONITH commands to their partner nodes and  attempted to take control of resources, however some of those  commands were not delivered due to the compromised network. When the  network recovered and the cluster messaging between nodes came back, a  number of pairs were in a state where both nodes expected to be active for  the same resource. This resulted in a race where the nodes terminated  one another and we wound up with both nodes stopped for a number of  our fileserver pairs. . Once we discovered this had happened, we took a number of steps  immediately: . We put GitHub.com into maintenance mode. . We paged the entire operations team to assist with the recovery. . We downgraded both aggregation switches to the previous software  version. . We developed a plan to restore service. . We monitored the network for roughly thirty minutes to ensure that it  was stable before beginning recovery. . When both nodes are stopped in this way it’s important that the node  that was active before the failure is active again when brought back  online, since it has the most up to date view of what the current state  of the filesystem should be. In most cases it was straightforward for  us to determine which node was the active node when the fileserver pair  went down by reviewing our centralized log data. In some cases, though,  the log information was inconclusive and we had to boot up one node in  the pair without starting the fileserver resources, examine its local  log files, and make a determination about which node should be active. . This recovery was a very time consuming process and we made  the decision to leave the site in maintenance mode until we had  recovered every fileserver pair. That process took over five hours to  complete because of how widespread the problem was; we had to  restart a large percentage of the the entire GitHub file storage  infrastructure, validate that things were working as expected, and  make sure that all of the pairs were properly replicating between  themselves again. This process, proceeded without incident and  we returned the site to service at 20:23 PST. . We worked closely with our network vendor to identify and understand  the problems that led to the failure of MLAG to failover in the way that  we expected. While it behaved as designed, our vendor plans to revisit  the respective timeouts so that more time is given for link failure to  be detected to guard against this type of event. . We are postponing any software upgrades to the aggregation network  until we have a functional duplicate of our production environment in  staging to test against. This work was already underway. In the mean  time, we will continue to monitor for the MAC address learning problems  that we discussed in our   previous report   and apply a workaround as necessary. . From now on, we will place our fileservers high availability  software into maintenance mode before we perform any network changes,  no matter how minor, at the switching level. This allows the servers to  continue functioning but will not take any automated failover actions. . The fact that the cluster communication between fileserver nodes relies  on any network infrastructure has been a known problem for some time. We’re  actively working with our hosting provider to address this. . We are reviewing all of our high availability configurations with fresh  eyes to make sure that the failover behavior is appropriate. . I couldn’t be more sorry about the downtime and the impact  that downtime had on our customers. We always use problems like this as  an opportunity for us to improve, and this will be no exception. Thank  you for your continued support of GitHub, we are working hard and  making significant investments to make sure we live up to the trust  you’ve placed in us. ", "date": "December 26, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tA More Transparent Clipboard Button\t\t", "author": ["\n\t\tJon Rohan\t"], "link": "https://github.blog/2013-01-02-a-more-transparent-clipboard-button/", "abstract": " Copying long lines of text and shas to your clipboard has been just a click away  for a few years now . Today we’re putting a new face on that click-to-copy feature, making it easier to integrate with the rest of the site. .   . Today we’re upgrading all the clipboard buttons to  ZeroClipboard . . With  ZeroClipboard  we can glue the flash object (currently the only reliable way to put data in the clipboard) to any dom element we want, leaving the styling up to us. . Here are some examples: .   .   .   .  “Copy and Paste is so Yesterday”  ", "date": "January 2, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tReleasing Make Me\t\t", "author": ["\n\t\tMike Skalnik\t"], "link": "https://github.blog/2013-01-03-releasing-make-me/", "abstract": " A few months ago, GitHub HQ 2.0 got a  MakerBot Replicator 2 . GitHubbers  started printing almost immediately due to the easy setup but having to leave a  laptop connected was painful. We quickly learned how to print from the SD card  but then people without a way to write SD cards were out of luck. . What we needed was for  Hubot  to handle talking to the printer for us.  We bundled up some open source projects on GitHub, specifically MakerBot’s fork  of  s3g , MakerBot’s  MiracleGrue , and   @sshirokov’s   stltwalker  and put a small API on top.  Today, we’re releasing that as  make-me ! .  Make-me  makes it easy for anyone to print, primarily controlled by  your favorite  Hubot  instance. The HTTP API only allows for a single  print at a time and requires a manual unlock to help prevent others printing  while another print hasn’t been removed from the build platform yet. In addition  to this, it uses  imagesnap  to take pictures via web cam to give  you an idea of how the print is going. . We’ve been using make-me to power all of our 3D printing needs including  decorating our office with various prints and making useful trinkets. . Our setup at the GitHub HQ is still evolving. Right now, it’s connected to an  old MacBook Air, so we can use the web cam to see how prints are going remotely. .   . Once you have your 3D printer plugged into a computer running OS X you can clone   make-me  and run the bootstrap script: . You can send STL files directly to the printer via  make : . You can pass some options to MiracleGrue, which you can read about in  the make-me  README . . Make-me ships with an HTTP API via Sinatra, runnable with  script/server . It  takes advantage of the CLI interface, along with  stltwalker , to  give you the ability to scale, print multiple STLs, change infill, generate  supports, and more. Want to print  Mr. Jaws  with the default settings? . You can easily setup the Hubot script to ask Hubot to accomplish these tasks for you: .   . Make-me is still rough around the edges. It started out as a quick project to  get something working and has evolved many new features from there. If you want to  help out check out the  issues  and send a pull request! . We hope this encourages more folks to dabble with 3D printing and automate some  inefficiency. ", "date": "January 3, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tSecrets in the code\t\t", "author": ["\n\t\tBrian Doll\t"], "link": "https://github.blog/2013-01-25-secrets-in-the-code/", "abstract": " Programming often involves keeping a bunch of secrets around. You’ve got account passwords,  OAuth tokens, SSL and SSH private keys. The best way to keep a secret is, well, to keep it secret. . Sometimes in a forgetful moment, however, those secrets get shared with the whole world. . Once a secret is out, it’s out. There are no  partially  compromised secrets. If you’ve pushed  sensitive information to a public repository, there’s a good chance that it’s been indexed by  Google and can be searched. And with GitHub’s new Search feature, it’s now more easily searchable  on our site. . Our  help page on removing sensitive data   reminds us that once the commit has been pushed to a public repository, you should consider the  data to be compromised. If you think you may have accidentally shared private information in a repository,  we urge you to change that information (password, API key, SSH key, etc.) immediately and   purge that secret data from your repositories . . I also want to clarify that our code search results being unavailable is unrelated to this issue. Our operations team has been working on repairing and tuning the code search cluster.  We will continue to update  our status site  with updates on our progress.  We will also be publishing a detailed post-mortem on the code search availability issues next week. ", "date": "January 25, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tRecent Code Search Outages\t\t", "author": ["\n\t\tWill Farrington\t"], "link": "https://github.blog/2013-02-04-recent-code-search-outages/", "abstract": " Last week, between Thursday, January 24 and Friday, January 25 we experienced a critical outage surrounding our newly-launched Code Search service. As always, we strive to provide detailed, transparent post-mortems about these incidents. We’ll do our best to explain what happened and how we’ve mitigated the problems to prevent the cause of this outage from occurring again. . But first, I’d like to apologize on behalf of GitHub for this outage. While it did not affect the availability of any component but Code Search, the severity and the length of the outage are both completely unacceptable to us. I’m very sorry this happened, especially so soon after the launch of a feature we’ve been working on for a very long time. . Our previous search implementation used a technology called Solr. With the launch of our new and improved search, we had finally finished migrating all search results served by GitHub to multiple new search clusters built on elasticsearch. . Since the code search index is quite large, we have a cluster dedicated to it. The cluster currently consists of 26 storage nodes and 8 client nodes. The storage nodes are responsible for holding the data that comprises the search index, while the client nodes are responsible for coordinating query activity. Each of the storage nodes has 2TB of SSD based storage. . At the time of the outage, we were storing roughly 17TB of code in this cluster. The data is sharded across the cluster and each shard has a single replica on another node for redundancy, bringing for a total of around 34TB of space in use. This put the total storage utilization of the cluster at around 67%. This Code Search cluster operated on Java 6 and elasticsearch 0.19.9, and had been running without problem for several months while we backfilled all the code into the index. . On Thursday, January 17 we were preparing to launch our Code Search service to complete the rollout of our new, unified search implementation. Prior to doing so, we noted that elasticsearch had since released version 0.20.2 which contained a number of fixes and some performance improvements. . We decided that delaying the Code Search launch to upgrade our elasticsearch cluster from version 0.19.9 to 0.20.2 before launching it publicly would help ensure a smooth launch. . We were able to complete this upgrade successfully on Thursday, January 17. All nodes in the cluster were successfully online and recovering the cluster state. . Since this upgrade, we have experienced two outages in the Code Search cluster. . Unlike some other search services that use massive, single indexes to store data, elasticsearch uses a sharding pattern to divide data up so it can be easily distributed around the cluster in manageable chunks. Each of these shards is itself a Lucene index, and elasticsearch aggregates search queries across these shards using Lucene merge indexes. . The first outage occurred roughly 2 hours after the upgrade, during the recovery process that takes place as part of a cluster restart. We found error messages in the index logs indicating that some shards were unable to assign or allocate to certain nodes. Upon further inspection, we discovered that while some of these data shards had their segment cache files corrupted, others were missing on disk. elasticsearch was able to recover any shards with corrupted segment cache files and shards where only one of the replicas was missing, but 7 shards (out of 510) were missing both the primary copy and the replica. . We reviewed the circumstances of the outage and determined at the time that the problems we saw stemmed from the high load during the cluster recovery. Our research into this problem did not demonstrate other elasticsearch users encountering these sorts of problems. The cluster has happy and healthy over the weekend, and so we decided to send it out to the world. . The second outage began on Thursday, January 24. We first noticed problems as our exception tracking and monitoring systems detected a large spike in exceptions. Further review indicated that the majority of these exceptions were coming from timeouts in code search queries and from the background jobs that update our code search indexes with data from new pushes. . At this time, we began to examine both the overall state of all members of the cluster and elasticsearch’s logs. We were able to identify massive levels of load on a seemingly random subset of storage nodes. While most nodes were using single digit percentages of CPU, several were consuming nearly 100% of all of the available CPU cores. We were able to eliminate system-induced load and IO-induced load as culprits: the only thing contributing to the massive load on these servers was the java process elasticsearch was running in. With the search and index timeouts still occurring, we also noticed in the logs that a number of nodes were being rapidly elected to and later removed from the master role in the cluster. In order to mitigate potential problems resulting from this rapid exchanging of master role around the cluster, we determined that the best course of action was to full-stop the cluster and bring it back up in “maintenance mode”, which disables allocation and rebalancing of shards. . We were able to bring the cluster back online this way, but we noted a number of problems in the elasticsearch logs. . After the cluster restart, we noticed that some nodes were completely unable to rejoin the cluster, and some data shards were trying to double-allocate to the same node. At this point, we reached out to Shay and Drew from elasticsearch, the company that develops and supports elasticsearch. . We were able to confirm with Shay and Drew that these un-allocatable shards (23 primaries plus replicas) had all suffered data loss. In addition to the data loss, the cluster spent a great deal of time trying to recover the remaining shards. During the course of this recovery, we had to restart the cluster several times as we rolled out further upgrades and configuration changes, which resulted in having to verify and recover shards again. This ended up being the most time consuming part of the outage as loading 17TB of indexed data off of disk multiple times is a slow process. . With Shay and Drew, we were able to discover some areas where our cluster was either misconfigured or the configuration required further tuning for optimal performance. They were also able to identify two bugs in elasticsearch itself (see these  two   commits  for further details on those bugs) based on the problems we encountered and within a few hours released a new version with fixes included. Lastly, we were running a version of Java 6 that was released in early 2009. This contains multiple critical bugs that affect both elasticsearch and Lucene as well as problems with large memory allocation which can lead to high load. . Based on their suggestions, we immediately rolled out upgrades for Java and elasticsearch, and updated our configuration with their recommendations. This was done by creating a topic branch and environment on our Puppetmaster for these specific changes, and running Puppet on each of these nodes in that environment. . While these audits increased the length of the outage by a few hours, we believe that the time was well spent garnering the feedback from experts in large elasticsearch deployments. . With the updated configuration, new elasticsearch version with the fixes for the bugs we encountered, and the performance improvements in Java 7, we have not been able to reproduce any of the erratic load or rapid master election problems we witnessed in the two outages discussed so far. . We suffered an additional outage Monday, January 28 to our Code Search cluster. This outage was unrelated to any of the previous incidents and was the result of human error. . An engineer was merging the feature branch containing the Java and elasticsearch upgrades back into our production environment. In the process, the engineer rolled the Puppet environment on the Code Search nodes back to the production environment before deploying the merged code. This resulted in elasticsearch being restarted on nodes as Puppet was running on them. We recognized immediately the source of the problem and stopped the cluster to prevent any problems caused by running multiple versions of Java and elasticsearch in the same cluster. Once the merged code was deployed, we ran Puppet on all the Code Search nodes again and brought the cluster back online. Rather than enabling Code Search indexing and querying while the cluster was in a degraded state, we opted to wait for full recovery. Once the cluster finished recovering, we turned Code Search back on. . We did not sufficiently test the 0.20.2 release of elasticsearch on our infrastructure prior to rolling this upgrade out to our code search cluster, nor had we tested it on any other clusters beforehand. A contributing factor to this was the lack of a proper staging environment for the code search cluster. We are in the process of provisioning a staging environment for the code search cluster so we can better test infrastructure changes surrounding it. . The bug fixes included in elasticsearch 0.20.3 do make us confident that we won’t encounter the particular problems they caused again. We’re also running a Java version now that is actively tested by the elasticsearch team and is known to be more stable and performant running elasticsearch. Additionally, our code search cluster configuration has been audited by the team at elasticsearch with future audits scheduled to ensure it remains optimal for our use case. . As for Monday’s outage, we are currently working on automation to make the a Puppet run in a given environment impossible in cases where the branch on GitHub is ahead of the environment on the Puppetmaster. . Finally, there are some specific notes from the elasticsearch team regarding our configuration that we’d like to share in hopes of helping others who may be running large clusters: . Set the ES_HEAP_SIZE environment variable so that the JVM uses the same value for minimum and maximum memory. Configuring the JVM to have different minimum and maximum values means that each time the JVM needs additional memory (up to the maximum), it will block the Java process to allocate it. Combined with the old Java version, this explains the pauses that our nodes exhibited when introduced to higher load and continuous memory allocation when they were opened up to public searches. The elasticsearch team recommends a setting of 50% of system RAM. . Our cluster was configured with a recover_after_time set to 30 minutes. The elasticsearch team recommended a change so that recovery would begin immediately rather than after a timed period. . We did not have minimum_master_nodes configured, so the cluster became unstable when nodes experienced long pauses as subsets of nodes would attempt to form their own clusters. . During the initial recovery, some of our nodes ran out of disk space. It’s unclear why this happened since our cluster was only operating at 67% utilization before the initial event, but it’s believed this is related to the high load and old Java version. The elasticsearch team continues to investigate to understand the exact circumstances. . I’m terribly sorry about the availability problems of our Code Search feature since its launch. It has not been up to our standards, and we are taking each and every one of the lessons these outages have taught us to heart. We can and will do better. Thank you for supporting us at GitHub, especially during the difficult times like this. ", "date": "February 4, 2013"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe Making of Octicons\t\t", "author": ["\n\t\tCameron McEfee\t"], "link": "https://github.blog/2012-05-09-the-making-of-octicons/", "abstract": "   . In our  last post  we announced  Octicons , our new icon font. We put a lot of work into the font and gained a lot of knowledge in the process. With five different designers working to make it happen, this was one of our bigger collaborations. We thought we’d detail how we built Octicons and what we learned along the way. . In most cases, a designer would begin to work in Illustrator to create vector icons, but we chose Photoshop as our start place. From the outset we knew we wanted to design icons for specific sizes, so optimizing for those pre-defined pixels was paramount. With the recent release of Photoshop CS6, Photoshop has become a fairly powerful vector tool for pixel projects. . One handy feature that is a little hard to come across, but is great for creating icons is a 2-up view of a file. With the application frame visible  Window &gt; Application Frame  you can create a second instance of your working document  Window &gt; Arrange &gt; New Window For …psd . After that you can have the windows sit side-by-side with  Window &gt; Arrange &gt; 2-up Vertical . Each window is a view of the same document, but can be moved and zoomed independently. Best of all, when working with paths in one, the path outlines and handles don’t appear in the other. This lets you work zoomed into one and at 100% to proof your icon in the other. .   . In Photoshop CS6, Adobe introduced pixel snapping vectors. This is an amazing feature if you work with paths a lot. From time to time, however, it’s necessary to turn this feature off. There isn’t a hotkey that does it, but it is possible to record an action for turning this setting on and off which can be hotkeyed. .   . With Photoshop set up for maximum icon awesomeness, we began to audit all the icons on our site. We created new Octicons for each, as well as adding a few extras we thought might be useful down the road. .   . Design is in the details. With all our icons designed, it was time for us to create our font. We decided we needed two sizes of each icon. One size, 16px, would be optimized for its exact size. At 16px the details are limited so every pixel was important. Since the icons were designed for such a small space, they don’t really scale well. To take care of that our second size, 32px, would be designed with more detail so that it could be scaled up for many purposes. . Having planned out or icon strategy, we embarked on the adventure that was putting together our font. For this, we used an application called  Glyphs . . To begin, we needed to set up our font file so that it would work with our specific sizes. Since a font technically only has one size, we decided to set it up to be optimized for 32px, with the 16px icons actually scaled down from larger versions. . In the case of our icon font, the font’s metrics include a em width, cap height, and grid. This roughly translates to the idea of width, height, and resolution. . We initially tried setting our cap height, x-height, and em width to 32 to make a 32×32 unit square. This technically worked, but we found when exporting the font any nodes not placed on an exact integer unit would be shifted to the nearest unit. This resulted in a lot of ugly icons. .   . With this knowledge in hand, we decided to adjust things a bit. Using a 2048×2048 unit square, we were able to ensure that when the shifts occurred during export the effect on the overall shape was minor. This was mostly caused by being able to change the ratio of the grid to the total size of the font character. When the nodes snapped to the 1 unit grid in a 32 unit square, the jump was significant. With a 1 unit grid in a 2048 unit square, the jump was far less noticeable. . Here are our final metrics in  Glyphs . .   . The flow after we got setup was as follows. .    Our icons were built in Adobe Photoshop.    Design your icons in whatever you’re comfortable with. Building it in Photoshop did add an extra step, but we preferred the control it gave us.   .  Our icons were built in Adobe Photoshop.  . Design your icons in whatever you’re comfortable with. Building it in Photoshop did add an extra step, but we preferred the control it gave us. .    We scaled in Adobe Illustrator to 2048 units.    We first opened the Photoshop file in Illustrator. When copying the paths out of Illustrator, Glyphs seemed to use Illustrator’s coordinates for positioning. Having all the icons on one sheet, this caused alignment issues when simply copying and pasting icons into Glyphs.   To resolved this, we created a second 2048×2048 document. To get Illustrator to match Glyphs’ grid, we had to shift the ruler zero point -1792pts. We then copied in a character (enclosed in a 2048×2048 square) into the new document and ran an action that scaled it up to fill the document and set it to (0,0). From here we were able to copy the vector path into Glyphs in the correct position.   This process was repeated for every character. In the end, each character fits a 32px square, sitting 4px below the baseline to line the icon up with any text it is pared with.   .  We scaled in Adobe Illustrator to 2048 units.  . We first opened the Photoshop file in Illustrator. When copying the paths out of Illustrator, Glyphs seemed to use Illustrator’s coordinates for positioning. Having all the icons on one sheet, this caused alignment issues when simply copying and pasting icons into Glyphs. . To resolved this, we created a second 2048×2048 document. To get Illustrator to match Glyphs’ grid, we had to shift the ruler zero point -1792pts. We then copied in a character (enclosed in a 2048×2048 square) into the new document and ran an action that scaled it up to fill the document and set it to (0,0). From here we were able to copy the vector path into Glyphs in the correct position. . This process was repeated for every character. In the end, each character fits a 32px square, sitting 4px below the baseline to line the icon up with any text it is pared with. .    We exported our font as an  OTF .        You may ask yourself why we chose 2048 as our magic number. We did quite a few tests at other sizes like 512 and 1024. What we found is that the size of the characters didn’t have much of an effect on the file size. What did change it was the grid size ratio. The larger the grid size (fewer grid squares), the smaller the file size got. At 2048 units, we had much more flexibility to adjust the grid scale without impacting the shapes of the font. In the end we chose a grid of 32px, which resulted in a 20% smaller file on export than a grid of 0 units. Larger grids like 64 units and 128 units, produced files that, while slightly smaller, didn’t reduce the file size enough to make it worth the loss of vector quality.   .  We exported our font as an  OTF .  .   . You may ask yourself why we chose 2048 as our magic number. We did quite a few tests at other sizes like 512 and 1024. What we found is that the size of the characters didn’t have much of an effect on the file size. What did change it was the grid size ratio. The larger the grid size (fewer grid squares), the smaller the file size got. At 2048 units, we had much more flexibility to adjust the grid scale without impacting the shapes of the font. In the end we chose a grid of 32px, which resulted in a 20% smaller file on export than a grid of 0 units. Larger grids like 64 units and 128 units, produced files that, while slightly smaller, didn’t reduce the file size enough to make it worth the loss of vector quality. .    We uploaded the exported  .otf  file to the  Font Squirrel @font-face generator .    Since we didn’t want any of the standard characters and used our own special unicode characters, we specified our own subset range, and turned pretty much everything off. You can grab our config settings  here .   .  We uploaded the exported  .otf  file to the  Font Squirrel @font-face generator .  . Since we didn’t want any of the standard characters and used our own special unicode characters, we specified our own subset range, and turned pretty much everything off. You can grab our config settings  here . . With our font built and ready for the browser there were a few items left we had to polish before calling the icons complete. .   . The grid matters! Each one of the grid squares above represents a pixel in  Glyphs . This is equal to 128 units in the app (We actually used a 32 unit grid, but this helps illustrate our point). When the lines don’t line up with this grid, the font rendering engine tries to compensate. It does this by aliasing which renders the half pixels as grey pixels. The  Create New Repo  icon above is mis-aligned. The result is below. .   .   . The last thing our font needed was a little something-something to adjust how the font was aliased. WebKit’s  -webkit-font-smoothing  property was our answer. .   . Browsers use  subpixel rendering  by default. This looks great for text, but left the icons looking a little heavy as a result of the extra pixels it adds. .   . By turning off font-smoothing entirely, we could preserve every pixel perfectly. This was great for straight lines, but curved or angled lines ended up looking blocky and pixelated. .   . Antialiased text was a good middle ground. It provided both the sharp edges when the font was built correctly, and smooth, lightweight curves when they were needed. . While only a feature of WebKit at this point, we’ll update the icons to support font-smoothing in other browsers as it becomes available. . We’re pretty proud of these icons. They are the result of a lot of hard work from @bryanveloso, @jonrohan, @jasoncostello, @kneath, and @cameronmcefee. We hope you’ve found this breakdown of the process interesting. ", "date": "May 9, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tDesigning GitHub for Windows\t\t", "author": ["\n\t\tTimothy Clem\t"], "link": "https://github.blog/2012-06-07-designing-github-for-windows/", "abstract": "  This article hasn’t been updated in a while. For the most current information, please refer to  the official Help documentation  and the  Desktop website .  . Today, I thought it would be fun to give some insight into how we do  native application design at GitHub. Many people are surprised to hear  the breadth of design and code that happens at GitHub on a daily basis.  We love Ruby and Rails, but we are far from a single language dev shop.  We write code in Ruby, Python, JavaScript, CoffeeScript, Objective-C,  C#, C, C++, Java, and shell of various flavors from Bash to PowerShell.  On the design side of things we obviously build web and native  applications, but we also do  print ,   animation  and  motion graphics . We design   booths ,  stamps ,  whisky glasses ,   dodgeball uniforms ,  business cards  and many  many  octocats . Most of our designers work in Adobe products  but we did branch out a bit as a part of creating GitHub for Windows. . Even though we shipped  GitHub for Mac  almost a year before   GitHub for Windows , we’ve known for a long time that we wanted  to build platform-specific native applications. In fact, both  applications were conceived at the same time and share some core design  principals. They also share a lot of low level code in the form of   libgit2 , but by and large they are entirely separate  entities. We specifically made the decision to write each application in  a language native to the platform, and this has turned out to be hugely  beneficial to us. Because of this separation we’ve been free to tackle  the problems that are most pressing for each platform and work in the  best possible tools instead of being constrained to the lowest common  denominator. . So how does an application like GitHub for Windows get built from a  design standpoint? It all starts with an idea and convincing someone  else to work on it. . In the end,  Metro  was my secret weapon in convincing  Cameron  McEfee  to work on this project with  me. Cameron was coming from a design background in print media and the  layout and typography of Metro really caught his eye. The rigid grid  system and Swiss design principals, along with a very modern and clean  feel made the prospect of designing a native GitHub client in this style  very exciting. After sharing a few sketches and some initial prototyping  work I had done, Cameron was hooked and the application started to take  shape. . Despite the existence of incredible mocking tools, I still like to start  my design work with a pencil and my notebook. Paper is still a very  powerful medium for concept work. .       . This is my original sketch of the dashboard in GitHub for Windows. You  can see one of my first XAML mocks kept some similar ideas. .       . Here is one of Cameron’s first takes: .       . At this point, we were still discovering what is useful to display on  this view and trying to understand how this would actually be  implemented in WPF. At GitHub, we work in a very fast, iterative design  process. In this particular case, Cameron and I would trade mocks and do  design reviews dozens of times a day. We try to get design work out as  early as possible for peer review and iteration. It can initially be  intimidating to get feedback on incomplete work, but once you learn to  take things in stride it’s hard to imagine working any other way. . A key part of this iterative cycle is that a single person generally   owns  the design. We don’t create A, B, C versions and present them  to the CEO. We don’t have dedicated product managers (or managers of any  sort). Instead, we do work in small self-forming teams and those teams  generally end up naturally surfacing someone who owns the design.  The result is consistent, but opinionated design. . Just for reference, here is how the dashboard ended up in the final  release. We slowly refined our idea of Metro and began to add in more  GitHub elements while actually moving away from how applications like  Zune interpret Metro. .       . In the end you can see that the design is clean – living on the strict  Metro grid system and definitely focusing on content over chrome.  However, we have very specifically made this native app have some of the  feel of GitHub.com. A side-by-side perspective of the app and the  website provides an interesting example of the affinity. .       . One of the wonderful things about WPF applications is that you can  basically build anything that you can create in a Photoshop or  Illustrator mock-up. Cameron tended to work largely in Photoshop and I  would come in to tweak things and then re-create them in XAML (with  little or no underlying functionality). We would keep doing design as new  visual elements of the application came up. So, for instance, we didn’t  actually design a progress bar until I had need of one in the  application. . As we got further along, I started committing completely to  XAML and used the Photoshop mocks merely as reference points. I tend to  work half in Blend and half in Visual Studio. If you’ve never used one  of the XAML designers before I have to tell you that one of my favorite  things is how you can write markup and watch the design change, or use  the color palette and watch the markup change. It is like changing HTML  and CSS on the fly in the Chrome inspector except you are actually  editing the real code and various other assets in a full fidelity editor.  Practically, this means that you can really quickly tweak colors, alignment, and even basic  components of a control without a large debug or refresh cycle. And in  the end, you check in a version-able document to your repository so that  when I change the foreground color of a text field, it is an obvious diff  on GitHub.com. Try doing that with a Photoshop document! .       . The other amazing thing about doing design in XAML is that you have  control over not just how your controls are styled, but also the basic  graphic components of a control. Standard buttons in WPF look like this: .       . But we redefined what a button looks like entirely. This can be done in  a way where buttons have to opt into the style/template, but it can also  be done in a way where you override every button. This is subtly  powerful. Here is how we define one of our buttons: .       . Along with doing basic graphic design, Blend also lets you do  interaction design. In Photoshop, you might have to manually lay out a  series of buttons that represent what a button looks like in the various  states of hover, pressed, normal, disabled, focused, etc. In XAML  this is natively supported through something called the VisualStateManager,  and from a design-time perspective you actually get to ‘record’ what  each of those states look like and then play with the behavior of the  object. You can even control how the movement between states happens by  animating properties as part of state transition. . One of the most striking parts of GitHub for Windows is that it ditches  the traditional Windows chrome almost entirely. We have a small control  box for the traditional min/max/close buttons, but otherwise we killed  all the rest of the chrome. Here is how we did that. . Our borderless Window is implemented as a behavior that can be attached  to any Window in XAML like so: . The min/restore/close buttons are just custom XAML buttons where we’ve  written the appropriate code to make them behave as you would expect.  The meat of the BorderlessWindowsBehavior happens in the callback where  we handle a few specific window messages to control the display of the  window. We got a lot of help and inspiration from  MahApps  which has a  similar  BorderlessWindowBehavior . . I don’t know if killing the chrome is going to be widely adopted on  Windows, but I hope that it is. For me, those 5px borders and  the massive title area on each window are simply artifacts of a past  life. They are distracting, largely unnecessary, and take up space  without providing functionality. Plus, even the standard window chrome  on to-be-released Windows 8 makes it look like your computer is 20 years  old. . This doesn’t necessarily mean that every application should or needs to  be a metro application, but it has been fun to see even Visual Studio  move towards this style. I think this is very welcome change. .       . We leverage vector graphics as much as possible in GitHub for Windows.  This makes the application resolution independent and makes it really  easy to move designs from Photoshop/Illustrator to WPF. For  instance, the maximize button is implemented with this path: .       . There was a lot of criticism in the early days of WPF about performance  and some of it was warranted in the initial versions of the framework.  These days, the tools are at your disposal to make great WPF  applications. The only major thing we’ve noticed is if you are running  old hardware and old operating systems (ahem, XP, I’m looking at you),  then WPF has to fall back to software rendering and this tends to be a  bit more memory and processor intensive. To the credit of the framework  we were able to release a single application that supports a  huge   range of operating systems from a single code base with almost zero  platform specific code. . Some notes about performance. XAML files (an XML format) are actually  compiled down to a binary format (BAML) which makes them wicked fast to  load at runtime (sort of how nibs/xibs work on Mac/iOS). What’s most important  to GUI applications is perceived performance and a responsive UI, which we achieve using Rx and making sure that we don’t perform blocking operations on the UI thread.  This is the same kind of thing you have to watch out for  when writing a node application, for instance. In our case, Visual  Studio also comes with some great tools for measuring what threads are  doing work and what code is causing the UI to block (and even what code  eventually caused it to unblock). .       . What this is showing is all the threads in my application and the state  that they are in. Generally you want that main UI thread to be green, and  all those places where you see red rectangles is where the UI thread is  waiting on another thread. When the UI thread blocks like that the  application is frozen for the user, so you want to look at what code is  running that is blocking and then what code eventually ran to unblock  the thread. If you want to use this tool yourself, make sure to follow   Paul’s  instructions to get real stack traces. . So far, we’ve really focused on the visual and constructional aspects of  the design, but a lot of our effort in creating GitHub for Windows went  into understanding how to make Git and the distributed collaborative  nature of GitHub accessible to people who have no desire to use a  command line tool. From a design standpoint this means we thought hard  about everything from how you download/install the application to how we  present version control concepts like making a commit or rolling back  changes. We even went so far as to package our own distribution of   msysGit  and a curated command-line experience as part of the  application. Design is often most important for the things that you  don’t see. ", "date": "June 7, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tSurviving the SSHpocolypse\t\t", "author": ["\n\t\tScott J. Goldman\t"], "link": "https://github.blog/2012-07-26-surviving-the-sshpocolypse/", "abstract": " Over the past few days, we have had some issues with our SSH infrastructure affecting a small number of Git SSH operations. We apologize for the inconvenience, and are happy to report that we’ve completed one round of architectural changes in order to make sure our SSH servers keep their sparkle. :sparkles: . As we’ve said before, we use GitHub to build GitHub, so the recent intermittent SSH connection failures have been affecting us as well. . Before today, every Git operation over SSH would open its own connection to our MySQL database during the authentication step. In the past this wasn’t a problem, however, we’ve started seeing sporadic issues as our SSH traffic has grown. . Realizing we were potentially on the cusp of a more serious situation, we patched our SSH servers to increase timeouts, retry connections to the database, and verbosely log failures. After this initial pass of incremental changes aimed to pinpoint the source of the problem, we realized this piece of our infrastructure wasn’t as easily modified as we would have liked. We decided to take a more drastic approach. . Starting on Tuesday, I worked with @jnewland to retire our 4+ year-old SSH patches and rewrite them all from scratch. Rather than opening a database connection for each SSH client, we call out to a shared library plugin (written in C) that lives in our Rails app. The library uses an HTTP endpoint exposed by our Rails app in order to check for authorized public keys. The Rails app is backed by a web server with persistent database connections, which keeps us from creating unbounded database connections, as we were doing previously. This is pretty neat because, like all code that lives in the GitHub Rails app, we can redeploy it near-instantly at any time. This gives us tremendous flexibility in continuing to scale our SSH services. . @jnewland deployed the changes around 9:20am Thursday and things seem to be in much better shape now. Below is a graph that shows connections to the mysql database. You can see a drastic reduction in the number of database connections: .   . You can also observe an overall smaller number of SSH server processes (they’re not all stuck because of contention on the database server anymore): .   . Of course, we are also exploring additional scalability improvements in this area. . Anywho, sorry for the mess. As always, please ping our support team if you see any further issues on github.com where Git over SSH hangs up randomly. ", "date": "July 26, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tOptimizing Sales for Happiness\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2012-08-28-optimizing-sales-for-happiness/", "abstract": "  Looking for GitHub’s sales team? Please  Contact Us .  .  10/17/14 Update from @pjhyett: Building products that people love paired with a customer-focused sales team hasn’t changed, but the specifics of our approach have evolved over time as with all things at GitHub. For engineering-centric startups looking to build a sales team, this post is for you.  . We’ve spoken about how we try to  optimize for happiness  at GitHub. You’re also likely hear the term  first principles  thrown around if you spend a few days at our office. We insist on taking the time to research and discuss various solutions when faced with a new challenge rather than blindly accepting the status quo. How we approach sales at GitHub is no different. . We sure do, but figuring out what sales means within a very developer-centric company wasn’t all  unicorns  and  octocats . The siren song of big revenue gains can easily disrupt your team’s ultimate goals and culture if you’re not careful. It wasn’t difficult to spot the slippery slope after hiring even one dedicated salesperson, so how do we optimize sales for happiness? .  By putting our product and people first.  If that means turning down revenue opportunities to remain true to this philosophy, so be it. Making gobs of money has never been our main focus, but we can operate this way  and  enjoy massive revenue growth across all of our products. . The first (and most important) thing we had going for us was a product that sells itself. With an unwavering focus on building the best products possible, we’re in a wonderful position that 99% of our customers require very little handholding. People just want to use GitHub, so we make it easy to pay for it, and essentially get out of their way. . The remaining 1% of customers is where sales comes into play. Much in the same way support guides folks through technical questions, we needed people to guide customers through business questions. Not only that, developers within larger organizations sometimes need help convincing the people with the purchasing authority to buy the products they really, really want to use. . We still call this role sales, but our team likes to think of themselves as developer liaisons. The role is akin to a sales engineer or technical account manager. It’s definitely not your prototypical sales person that’s cold-calling people all day to reach their quota. . In fact, all of our sales people have technical backgrounds, they just happen to also love the support and business side of things. They enjoy speaking with customers, building long-term relationships, and are comfortable navigating the inner workings of organizations to ultimately find a way to make sure those developers get to use the products they’re asking us for. . Another traditional sales tool that doesn’t really makes sense for us is paying commissions. Commissions are an incentive to churn and burn through orders as fast as possible, regardless of the consequences. They also introduce massive overhead and logistic problems amongst the salespeople and the company as a whole, so we’re happy to avoid all of that crap. . We want our sales people compensated just like everyone else in our organization: great salaries, great benefits, and stock options. We expect everyone to work hard, but we’re all in this together and no one should feel marginalized because sales is reaping the rewards from years of hard work done by others. . At the end of the day, the key to sales at GitHub is the key to GitHub’s success in general: build an awesome product and treat people well. We don’t claim to have unlocked some huge business secret with this idea, but we’re excited about our sales team at GitHub because we feel that doing it this way is best for our company and our customers. We’re in this for the long haul! ", "date": "August 28, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tDeploying at GitHub\t\t", "author": ["\n\t\tJake Douglas\t"], "link": "https://github.blog/2012-08-29-deploying-at-github/", "abstract": " Deploying is a big part of the lives of most GitHub employees. We don’t have a release manager and there are no set weekly deploys. Developers and designers are responsible for shipping new stuff themselves as soon as it’s ready. This means that deploying needs to be as smooth and safe a process as possible. . The best system we’ve found so far to provide this flexibility is to have people deploy branches. Changes never get merged to master until they have been verified to work in production from a branch. This means that master is always stable; a safe point that we can roll back to if there’s a problem. . The basic workflow goes like this: . Not too long ago, however, this system wasn’t very smart. A branch could accidentally be deployed before the build finished, or even if the build failed. Employees could mistakenly deploy over each other. As the company has grown, we’ve needed to add some checks and balances to help us prevent these kinds of mistakes. . The first thing we do now, when someone tries to deploy, is make a call to  Janky  to determine whether the current CI build is green. If it hasn’t finished yet or has failed, we’ll tell the deployer to fix the situation and try again. . Next we check whether the application is currently “locked”. The lock indicates that a particular branch is being deployed in production and that no other deploys of the application should proceed for the moment. Successful builds on the master branch would otherwise get deployed automatically, so we don’t want those going out while a branch is being tested. We also don’t want another developer to accidentally deploy something while the branch is out. . The last step is to make sure that the branch we’re deploying contains the latest commit on master that has made it into production. Once a commit on master has been deployed to production, it should never be “removed” from production by deploying a branch that doesn’t have that commit in it yet. . We use the GitHub API to verify this requirement. An endpoint on the github.com application exposes the SHA1 that is currently running in production. We submit this to the GitHub compare API to obtain the “merge base”, or the common ancestor, of master and the production SHA1. We can then compare this to the branch that we’re attempting to deploy to check that the branch is caught up. By using the common ancestor of master and production, code that only exists on a branch can be removed from production, and changes that have landed on master but haven’t been deployed yet won’t require branches to merge them in before deploying. . If it turns out the branch is behind, master gets merged into it automatically. We do this using the new :sparkles: Merging API :sparkles: that we’re making available today. This merge starts a new CI build like any other push-style event, which starts a deploy when it passes. . At this point the code actually gets deployed to our servers. We usually deploy to all servers for consistency, but a subset of servers can be specified if necessary. This subset can be by functional role — front-end, file server, worker, search, etc. — or we can specify an individual machine by name, e.g, ‘fe7’. . What now? It depends on the situation, but as a rule of thumb, small to moderate changes should be observed running correctly in production for at least 15 minutes before they can be considered reasonably stable. During this time we monitor exceptions, performance, tweets, and do any extra verification that might be required. If non-critical tweaks need to be made, changes can be pushed to the branch and will be deployed automatically. In the event that something bad happens, rolling back to master only takes 30 seconds. . If everything goes well, it’s time to merge the changes. At GitHub,  we use Pull Requests  for almost all of our development, so merging typically happens through the pull request page. We detect when the branch gets merged into master and unlock the application. The next deployer can now step up and ship something awesome. . Most of the magic is handled by an internal deployment service called Heaven. At its core, Heaven is a catalog of Capistrano recipes wrapped up in a Sinatra application with a JSON API. Many of our applications are deployed using generic recipes, but more complicated apps can define their own to specify additional deployment steps. Wiring it up to Janky, along with clever use of post-receive hooks and the GitHub API, lets us hack on the niceties over time. Hubot is the central interface to both Janky and Heaven, giving everyone in Campfire great visibility into what’s happening all of the time. As of this writing, 75 individual applications are deployed by Heaven. . Not all projects at GitHub use this workflow, which is core to github.com development, but here are a couple of stats for build and deploy activity company-wide so far in 2012: .     ", "date": "August 29, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we keep GitHub fast\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2012-09-05-how-we-keep-github-fast/", "abstract": " The most important factor in web application design is  responsiveness . And the first step toward responsiveness is speed. But speed within a web application is complicated. . Our strategy for keeping GitHub fast begins with powerful internal tools that expose and explain performance metrics. With this data, we can more easily understand a complex production environment and remove bottlenecks to keep GitHub fast and responsive. . Response time as a simple average isn’t very useful in a complex application. But what number is useful? The performance dashboard attempts to give an answer to this question. Powered by data from Graphite, it displays an overview of response times throughout github.com. .   . We split response times by the kind of request we’re serving. For the ambiguous items: . Clicking one of the rows allows you to dive in and see the mean, 98th percentile, and 99.9th percentile response times. . The performance dashboard shows performance information, but it doesn’t  explain . We needed something more fine-grained and detailed. . GitHub staff can browse the site in  staff mode . This mode is activated via a keyboard shortcut and provides access to staff-only features, including our Mission control bar. When it’s showing, we see staff-only features and have the ability to moderate the site. When it’s hidden, we’re just regular users. .  Spoiler alert:  you might notice a few things in this screenshot that haven’t fully shipped yet. .   . The left-hand side shows which branch is currently deployed and the total time it took to serve and render the page. For some browsers (like Chrome), we show a detailed breakdown of the various time periods that make up a rendered page. This is massively useful in understanding  where  slowness comes from: the network, the browser, or the application. .   . The right-hand side is a collection of various application metrics for the given page. We show the current compressed javascript &amp; css size, background job queue, and various data source times. For the ambiguous items: . When we’re ready to make a page  fast  we can dive into some of these numbers by clicking on them. We’ve hijacked many features from  rack-bug  and  query-reviewer  to produce these breakdowns. .   .   .   . It goes without saying that we use many other tools like  New Relic ,  Graphite , and plain old UNIX-foo to aid in our performance investigations as well. . A lot of the numbers in this post are much slower than I’d like them to be, but we’re hoping with better transparency we’ll be able to deliver the fastest web application that’s ever existed. . As @tnm says:  it’s not fully shipped until it’s fast . ", "date": "September 5, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tGitHub availability this week\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2012-09-14-github-availability-this-week/", "abstract": " GitHub.com suffered two outages early this week that resulted in one hour and 46 minutes of downtime and another hour of significantly degraded performance. This is far below our standard of quality, and for that I am truly sorry. I want to explain what happened and give you some insight into what we’re doing to prevent it from happening again. . During a  maintenance window  in mid-August our operations team replaced our aging pair of DRBD-backed MySQL servers with a 3-node cluster. The servers collectively present two virtual IPs to our application: one that’s read/write and one that’s read-only. These virtual IPs are managed by Pacemaker and Heartbeat, a high availability cluster management stack that we use heavily in our infrastructure. Coordination of MySQL replication to move ‘active’ (a MySQL master that accepts reads and writes) and ‘standby’ (a read-only MySQL slave) roles around the cluster is handled by Percona Replication Manager, a resource agent for Pacemaker. The application primarily uses the ‘active’ role for both reads and writes. . This new setup provides, among other things, more efficient failovers than our old DRBD setup. In our previous architecture, failing over from one database to another required a cold start of MySQL. In the new infrastructure, MySQL is running on all nodes at all times; a failover simply moves the appropriate virtual IP between nodes after flushing transactions and appropriately changing the  read_only  MySQL variable. . The events that led up to Monday’s downtime began with a rather innocuous database migration. We use a two-pass migration system to allow for zero-downtime MySQL schema migration. This has been a relatively recent addition, but we’ve used it a handful of times without any issue. . Monday’s migration caused higher load on the database than our operations team has previously seen during these sorts of migrations. So high, in fact, that they caused Percona Replication Manager’s health checks to fail on the master. In response to the failed master health check, Percona Replication manager moved the ‘active’ role and the master database to another server in the cluster and stopped MySQL on the node it perceived as failed. . At the time of this failover, the new database selected for the ‘active’ role had a cold InnoDB buffer pool and performed rather poorly. The system load generated by the site’s query load on a cold cache soon caused Percona Replication Manager’s health checks to fail again, and the ‘active’ role failed back to the server it was on originally. . At this point, I decided to disable all health checks by enabling Pacemaker’s  maintenance-mode ; an operating mode in which no health checks or automatic failover actions are performed. Performance on the site slowly recovered as the buffer pool slowly reached normal levels. . The following morning, our operations team was notified by a developer of incorrect query results returning from the node providing the ‘standby’ role. I investigated the situation and determined that when the cluster was placed into  maintenance-mode  the day before, actions that should have caused the node elected to serve the ‘standby’ role to change its replication master and start replicating were prevented from occurring. I determined that the best course of action was to disable  maintenance-mode  to allow Pacemaker and the Persona Replication Manager to rectify the situation. . Upon attempting to disable  maintenance-mode , a Pacemaker segfault occurred that resulted in a cluster state partition. After this update, two nodes (I’ll call them ‘a’ and ‘b’) rejected most messages from the third node (‘c’), while the third node rejected most messages from the other two. Despite having configured the cluster to require a majority of machines to agree on the state of the cluster before taking action, two simultaneous master election decisions were attempted without proper coordination. In the first cluster, master election was interrupted by messages from the second cluster and MySQL was stopped. . In the second, single-node cluster, node ‘c’ was elected at 8:19 AM, and any subsequent messages from the other two-node cluster were discarded. As luck would have it, the ‘c’ node was the node that our operations team previously determined to be out of date. We detected this fact and powered off this out-of-date node at 8:26 AM to end the partition and prevent further data drift, taking down all production database access and thus all access to github.com. . As a result of this data drift, inconsistencies between MySQL and other data stores in our infrastructure were possible. We use Redis to query dashboard event stream entries and repository routes from automatically generated MySQL ids. In situations where the id MySQL generated for a record is used to query data in Redis, the cross-data-store foreign key relationships became out of sync for records created during this window. . Consequentially, some events created during this window appeared on the wrong users’ dashboards. Also, some repositories created during this window were incorrectly routed. We’ve removed all of the leaked events, and performed an audit of all repositories incorrectly routed during this window. 16 of these repositories were private, and for seven minutes from 8:19 AM to 8:26 AM PDT on Tuesday, Sept 11th, were accessible to people outside of the repository’s list of collaborators or team members. We’ve contacted all of the owners of these repositories directly. If you haven’t received a message from us, your repository was not affected. . After confirming that the out-of-date database node was properly terminated, our operations team began to recover the state of the cluster on the ‘a’ and ‘b’ nodes. The original attempt to disable  maintenance-mode  was not reflected in the cluster state at this time, and subsequent attempts to make changes to the cluster state were unsuccessful. After tactical evaluation, we team determined that a Pacemaker restart was necessary to obtain a clean state. . At this point, all Pacemaker and Heartbeat processes were stopped on both nodes, then started on the ‘a’ node. MySQL was successfully started on the ‘a’ node and assumed the ‘active’ role. Performance on the site slowly recovered as the buffer pool slowly reached normal levels. . In summary, three primary events contributed to the downtime of the past few days. First, several failovers of the ‘active’ database role happened when they shouldn’t have. Second, a cluster partition occurred that resulted in incorrect actions being performed by our cluster management software. Finally, the failovers triggered by these first two events impacted performance and availability more than they should have. . The automated failover of our main production database could be described as the root cause of both of these downtime events. In each situation in which that occurred, if any member of our operations team had been asked if the failover should have been performed, the answer would have been a resounding  no . There are many situations in which automated failover is an excellent strategy for ensuring the availability of a service. After careful consideration, we’ve determined that ensuring the availability of our primary production database is not one of these situations. To this end, we’ve made changes to our Pacemaker configuration to ensure failover of the ‘active’ database role will only occur when initiated by a member of our operations team. . We’re also investigating solutions to ensure that these failovers don’t impact performance when they must be performed, either in an emergency situation or as a part of scheduled maintenance. There are various facilities for warming the InnoDB buffer pool of slave databases that we’re investigating and testing for this purpose. . Finally, our operations team is performing a full audit of our Pacemaker and Heartbeat stack focusing on the code path that triggered the segfault on Tuesday. We’re also performing a strenuous round of hardware testing on the server on which the segfault occurred out of an abundance of caution. . We host our status site on Heroku to ensure its availability during an  outage. However, during our downtime on Tuesday our  status  site  experienced some availability issues. . As traffic to the status site began to ramp up, we increased the number of   dynos  running from 8 to 64 and  finally 90. This had a negative effect since we were running an old  development database addon (shared database). The number of dynos maxed out  the available connections to the database causing additional processes to  crash. . We worked with Heroku Support to bring a production database online that  would be able to handle the traffic the site was receiving. Once this database  was online we saw an immediate improvement to the availability of the status  site. . Since the outage we’ve added a database slave to improve our availability  options for unforeseen future events. . The recent changes made to our database stack were carefully made specifically with high availability in mind, and I can’t apologize enough that they had the opposite result in this case. Our entire operations team is dedicated to providing a fast, stable GitHub experience, and we’ll continue to refine our infrastructure, software, and methodologies to ensure this is the case. ", "date": "September 14, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we ship GitHub for Windows\t\t", "author": ["\n\t\tAdam Roben\t"], "link": "https://github.blog/2012-09-24-how-we-ship-github-for-windows/", "abstract": " We’ve shipped 25 updates to  GitHub for Windows  in the 4 months since we  first launched . That’s more than one release per week, for 17 weeks straight! Here’s how we do it. . At GitHub,  we ship a lot . We think it’s so important to ship often, we even have a  shipping mascot  and matching emoji (:shipit:). Why do we do this? . Shipping rapidly is important to us for so many reasons. For example, the more frequently we ship the less likely it is for large numbers of changes to build up between releases, and shipping a small release is almost always less risky than shipping a big one. Shipping frequently also requires that we make it downright simple to ship by automating it, so anyone can do it. This raises our  bus factor , and also democratizes the process of shipping so there aren’t “gatekeepers” who must sign off before any software goes out the door. And by shipping updates so often, there is less anxiety about getting a particular feature ready for a particular release. If your pull request isn’t ready to be merged in time for today’s release, relax. There will be another one soon, so make that code shine! . We really take this to heart. GitHub.com gets  deployed hundreds of times each day . But we don’t think shipping often should be limited to web apps. We want all these same benefits for our native apps, too! . Let’s see how we’ve been doing with GitHub for Windows. . I said above that we’ve shipped 25 updates in 4 months. That comes out to about one release every 5 days, on average, and the median time between releases is only half that, 2.5 days. A full 75% of our updates so far shipped in less than 7 days. . The story is largely the same if you look at releases in terms of the number of commits that went into each release. We’ve made 1,176 commits on our  master  branch since our first release. That’s 47 commits per release on average, with a median of 41 commits per release. 72% of GitHub for Windows updates so far have contained fewer than 50 commits. . Here’s a graph that shows all of our updates to GitHub for Windows thus far, with days since the previous release on the X axis and commits since the previous release on the Y axis: .   . That cluster of releases down in the bottom-left corner near the origin is exactly what we want. We work hard to make our updates as small as possible and to release them quickly, and I think the numbers bear that out. . So, how do we do it? . As I said earlier, shipping often requires that shipping be automated. When I joined GitHub back in March, shipping GitHub for Windows to our private beta group was a completely manual process, and  Paul  was the only one who knew how to do it. In fact, Paul was away my first week, so we weren’t able to ship an update we had all ready to go! . So Paul opened this issue in our GitHub for Windows repository: .     . Even now, looking at that release process makes me cry. . I, the bright-eyed, bushy-tailed newbie, got to work automating the process. Here’s how we deploy a release now: .     . One command,  .scriptDeploy.ps1 production , is all it takes to build, package, and deploy a new GitHub for Windows release. We can also deploy just to GitHub staff, which we do for testing changes before releasing them to the public, by running  .scriptDeploy.ps1 staff . Lowering the barrier to shipping means that anyone can ship an update, not just Paul, and that we can do it frequently, even multiple times a day. . As  Deploy.ps1  runs, it posts status to one of our Campfire chat rooms. This lets the rest of the team see what’s going on, saves a record of the deploy in Campfire’s logs for posterity, and lets us cheer each other on. .     . In addition to deploying our installer,  Deploy.ps1  uploads debug symbols (PDB files) for each release to our  symbol server , a  custom proxy  backed by Amazon S3. This makes it possible to debug  crash dumps  from users. We even have some other  PowerShell  scripts to automate that so you don’t have to be a crash dump expert just to see what went wrong. . Every release contains a change log that details the most important fixes and improvements contained in that release. The change log is stored in a JSON file in the GitHub for Windows repository, and uploaded to S3 with each deploy. GitHub for Windows downloads the change log from S3 to show you what’s new in the update it’s about to install, and  http://windows.github.com/  uses it to show release notes from all our past versions. When we decide it’s time to make a new release, we just ask  Hubot  what’s changed since our last release: .     . Hubot provides a link to a compare view on GitHub.com that shows exactly what commits and pull requests haven’t yet been deployed, making writing a change log a breeze. .     . We also use Hubot to monitor releases after we ship them. For instance, whenever GitHub for Windows throws an exception or crashes, it sends information about the exception or crash to Haystack, our internal exception aggregation app. We can ask Hubot to show us data from Haystack to determine how many exceptions and crashes our users are seeing, which is particularly useful just after an update has been released: .     . If we find that exceptions or crashes are increasing, we can go to Haystack to find out what’s going wrong. . We can even use Hubot to get an idea of how quickly users are updating to the latest version. Speaking of which… . Shipping so often would be for naught if our users weren’t actually installing our updates. So we’ve made downloading and installing updates to GitHub for Windows completely automated, too. GitHub for Windows checks for updates periodically, and when it finds one, it installs it in the background. The next time you relaunch the app, you’re automatically running the latest and greatest version, hot off our servers. . This results in our updates getting adopted by users remarkably quickly. Here’s a graph of all the GitHub for Windows releases we’ve made. On the X axis is time. On the Y axis is the percentage of total API calls to GitHub.com that are being made by each version of GitHub for Windows, stacked. .     . For example, on July 26 th  you can see that around 70% of API calls were coming from version 1.0.17.7, an additional 10% were coming from version 1.0.16.1, and older versions made up the remaining 20%. . The steep line when each release first appears gives you a good idea of how quickly releases start being used. But to really see just how quickly updates make it onto users’ machines, let’s zoom in on one week in early September, when we shipped our 1.0.24.3 update: .     . Within an hour, 25% of API calls were coming from the version we had just deployed. In 18 hours, it was over 50%. Within a week, it was over 70%. . This is really incredible. Even Google Chrome, which in many ways pioneered smooth, automatic updates for desktop software, only sees  about 33% adoption in the first week after an update . By making the update process so smooth, we’re able to get bug fixes and new features into the hands of users just hours after deploying. Our users seem to like it, too; in 4 months we’ve gained over 125,000 users, and there’s no sign of it slowing down yet: .   . It makes me incredibly happy to be able to ship GitHub for Windows so often and get it in users’ hands so quickly. But there’s still more to do. We’re always thinking about ways to make the download size of updates even smaller to help users on slow connections, and to make updates more reliable for users behind proxies. We want to make deploying even easier and more consistent with our other products by integrating with our  Heaven Sinatra app . And we want to remove the few remaining manual steps to further reduce the chance of mistakes and to encourage shipping more often. . The more quickly and easily we can ship, the better GitHub for Windows will be, the happier we will be, and the happier our users will be. ", "date": "September 24, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe GitHub hiring experience\t\t", "author": ["\n\t\tCoby Chapple\t"], "link": "https://github.blog/2012-09-24-the-github-hiring-experience/", "abstract": " Crafting experiences is central to what we do here at GitHub, and our interviewing, hiring, and on-boarding experiences are no exception. Having recently been through this process first-hand, I’d like to share a little bit about what it’s like while it’s still fresh in my mind. .   . My story all started with a single email. I found myself kicking back with a beer one night reading some articles online, and I came across one of @kneath’s posts about how things are done at GitHub. I had read plenty of posts by various GitHubbers in the past, and I had always been very impressed by what I’d seen of their approach to business, technology, and other aspects of life. I had no idea if they were hiring or not, but a day or two after deciding to send Kyle an email to introduce myself, I was amazed to be chatting to him directly on Skype. . Every candidate’s first contact with GitHub is slightly different, but it’s almost always with a person who works in a role close to what the candidate’s would be—be it developer, designer, supportocat, ops, or something else. This conversation is a chance for us to get an initial sense of what the person is like to interact with, and to begin discussing the possibility of having them join our team. . My first chat with Kyle was very relaxed. We talked about my experiences, my thoughts about GitHub as a company, my typical design process, and how I approach my work in general. It was also very much a two-way conversation—Kyle answered all my questions and shared interesting insights into the company as we were talking. It didn’t feel like a typical interview, and it was far from being an adversarial, pressure-filled encounter. I didn’t know it at the time, but this vibe was set to continue throughout my hiring and on-boarding experience. . The reason we approach people’s first contact this way is simple. We believe it’s critical to ensure candidates have an initial discussion with someone who  thoroughly understands the work they do . It gives us a sense of whether the person is likely to be a good fit for GitHub in terms of both skills and culture, but more importantly it sets the tone for the rest of the hiring experience. We hope skipping the initial paperwork-based screening process makes it clear to the candidate that we’re not playing games—that we’re  genuinely interested  in them. . For people we believe have a high probability of being a good fit for the company, the next step is to bring them into the office for face-to-face interviews. . Hiring good people is one of the most critical activities we do as a company, so we go to a lot of trouble to make sure interviewees feel important. By this stage we’ve got a good sense that they’ll make a great addition to the team, so it’s well worth investing in their experience to maximize candidates’ desires to work with us. . For all interviewees, we fly them to San Francisco from wherever they live—even if it’s  quite literally  the other side of the world, like it was in my case. There is a driver with a sign waiting at the airport to take them to a comfortable hotel—which was an absolute godsend for me. I was stumbling out of the airport like a zombie after nearly 20 hours of flights from Australia, and seeing a sign with my name on it lifted my mood  immediately . We do these things for interviewees to convey the message that  GitHub values them from the outset —and that’s certainly how it came across for me. . Valuable people deserve a bespoke hiring experience, so we go to great lengths to work around interviewees’ existing commitments and schedules, or where people have families to take care of—a little flexibility goes a long way. I was juggling existing freelance work, holiday plans with my partner, as well as assignments and exam study for my last two subjects of my university degree—so to have GitHub’s Spirit Guide™  David  be so flexible and helpful when arranging my trip was truly amazing. . People’s time is valuable too, so we make a point of moving quickly through the process. In my case, the time between contacting Kyle and having flights booked was only a couple of days. One other recent hire went from being a candidate to having a signed offer in  just four days . By the time we’re flying someone out, we mean business. . On my interview day, I was told to arrive at the office around 10 or so, where I was greeted by David (and a couple of dogs). David whisked me away to the kitchen for a beverage, and before long, the interviewing began. . I probably spoke with around 10 or 12 people over the course of the day, mostly two at a time, and everyone was amazingly friendly. My day also included pairing with @bleikamp on a design and some CSS, working through a production front-end issue with @jakeboxer, and also chatting one-on-one with @mojombo over a few beers going through some of my previous freelance work, talking him through my typical process, and generally getting to know each other. The whole experience was welcoming, open, and laid back; and I got the distinct impression that it was all about everyone simply getting to know me as a person, which turned out to be  exactly  what GitHub aims for. . My experience was a fairly typical one for candidates at GitHub. Generally speaking, you will have talked to a dozen or so people by the end of the day, paired on some real work, played some pool, seen around the office, and there’s also a good chance you’ll have been introduced to  Slow Merge™ , our very own GitHub-branded whiskey. We hope that by the time you leave we know you well enough to say that we want you to join the team, and that you think we’re awesome and want to work with us. . When anyone joins the GitHub team, we fly them back to San Francisco to spend their first week going through our on-boarding process. Each new hire is paired with a buddy that stays with them throughout the week, and I was lucky enough to have @jonrohan as mine. Buddies help new hires with things like setting up all their user accounts for the various services we use; introducing them to  The Setup™  to get a development environment installed on their new laptop, and showing newbies how to go about shipping awesomeness to the world in whatever way they do best. I found it really reassuring to have a dedicated buddy to help me through my first week. Digesting the stupendous amount of information new Hubbers are exposed was made much smoother having someone to walk me through how it all works. . A typical induction week flies by in a blur of lunches, games of pool or  insane more-than-two-player table tennis ,  beverages , :sparkles::tropical_fish::cherries:emoji:penguin::strawberry::star2:, getting to know  Hubot , and shipping (of course)—so there’s plenty of time for people to settle in to the company and get to know their fellow Hubbernauts. By the end of the week the transformation from newbie to fully-fledged GitHubber is complete, and they’ll be ready to head out on their own to kick butt. . All this experiential craftsmanship is the result of hard work by a number of people, and I’d like to take this opportunity to personally thank David, Melissa, Tom, Heather, Jenifer, Emma, and  everyone else  who plays a part in making this process such a pleasant one to go through for recent additions like myself and those to follow. . I believe that the best thing about the way we do hiring here at GitHub is that every new hire comes out  knowing  they are a valued and trusted part of the company. Ultimately, this means our business and products will be better as a result, and that the people who depend on our products every day to get work done will have better experiences. .   . Speaking of hiring, we’re looking for a couple of people to join our team at the moment. Check out  our listings on the GitHub job board , and if you think you’d be a good fit we’d love to hear from you. ", "date": "September 24, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe GitHub:Training Web Site is a Thing\t\t", "author": ["\n\t\tTim Berglund\t"], "link": "https://github.blog/2012-10-05-the-github-training-web-site-is-a-thing/", "abstract": " The past six months have seen tremendous growth in GitHub’s training organization. We’ve  added   people , we’ve  added materials , and we’ve been to more places on the planet to help make it easy for humans to use Git and GitHub. With a mere two million accounts on GitHub.com, we’ve got plenty more humans to talk to, but we’re up for the challenge. . Of course all of this work is going to require us to refine our web presence a bit. I’m very happy to present to you the new training site: .     . The new site is your online connection to everything the training team is up to—and that’s a lot: . To support the increased training demand, we’ve enlisted the help of a developer and a designer inside GitHub. The designer,  Coby Chapple , is largely responsible for the beautiful new training site we’ve just launched. The developer,  Zachary Kaplan , is helping us build internal applications to streamline our proposal and client management processes. Look for more awesomeness from Zachary and Coby in the future. . We agree with our  CEO’s famous dictum  that just about everything we do should be open-sourced. To that end, we’ve begun to release our  very own training materials  for you to use, modify, and share. We’ve got all of our open-source materials up on our beta  teach.github.com  site. Take a look, fork the repo, and send us a pull request! . If you’re interested in a private training session or in having a GitHubber speak at a conference you’re organizing, please  get in touch with us . We travel all over the world helping people know and love Git and GitHub better, so there’s a good chance we can help you too. We are waiting to serve you. ", "date": "October 5, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tRecent Services Interruptions\t\t", "author": ["\n\t\tCorey Donohoe\t"], "link": "https://github.blog/2011-01-06-recent-services-interruptions/", "abstract": " Here’s a summary of the outages we encountered this week and what we’re doing  to prevent this from happening again. . Monday marked the first “real” workday for most people in 2011.  Our wonderful  users all hopped online and got back to hacking.  As North American work hours  came around our Pacemaker application failed over one of our xen machines which  happened to host our primary load balancer.  This is something that happens  really rarely and most of our users notice because the load balancer is the  machine that everyone hits when accessing GitHub.  This exposed a few problems in our  infrastructure that we’ll be addressing. . Our internal routing had issues that we hadn’t experienced before due to our  growing internal network.  We specifically had problems with internal DNS  resolution after the failover as well as routing certain traffic to some of our  frontend machines. .  New Relic  was great in helping us diagnose this issue. .   . Something was taking WAY too long compared to how things normally look.  Everything was essentially timing out. . Unfortunately it took us a little while to figure out the real issues were with  networking.  We know this can happen now and the team has a much better  understanding over the networking overall. . We’re now aware that under our current configuration, certain services on our load balancers must be located on different hosts to prevent this particular routing issue.  We have a plan in place to reconfigure that part of our networking setup to remove the issue.  In the meantime, we’re also setting up a third load balancer to restore our n+1 redundancy. . During all of the networking insanity we had a fileserver, fs7, failover during  this bumpy outage.  We use a high availability setup for the fileservers, and  they fail over a lot more often than you’d think.  We kind of chalked it up to  general insanity inside the cluster and our trusty sysadmin, Tim, went off to  make sure we didn’t have another day like Monday. . We had intermittent service between 8:30AM PST and about 3PM PST. . Around 7AM PST on Tuesday we started to notice high load and an abnormally high  number of http connections.  By 8AM fs7, the same machine with problems the  previous day, had failed over.  The failover machine is usually online within a  few minutes but due to the high load it hobbled along for a little over an  hour.  Shortly after that it kernel panicked which required Tim to spend some  quality time with it.  We realized that the kernel the failed fileserver was  running was older than most of the rest of our fileservers so we decided to  upgrade it.  This took us a little bit and service was restored on fs7 by 3PM  PST.  Keep in mind that this only impacted a subset of our customers but a  second shaky day obviously isn’t what we want for our users. . Everything was back to normal but two straight days of issues impacting one  fileserver left us a little spooked and focusing hard on what was wrong with  fs7 specifically.  Everything seemed to corrolate around north american  business hours starting in EST, so we camped out and waited for wednesday  morning. . Wednesday we saw the heightened load start around 5AM PST and resulted in a  bumpy two hours.  The system went in and out of swap before swapping itself to  death shortly after 7am PST. . You can see it die off in collectd graphs. .   . Once again fs7 failed over and this time it had a lot of queued requests to  handle when the failover was promoted.  As the failover came up its load stayed  extremely high but started to settle after 20-30 minutes of hammering it.  We  were unhappy that it happened again, but we were glad that we’d avoided  another prolonged outage. . Around 8:30AM PST we saw another burst of activity on the fileserver, luckily  we were watching the system closely and kept the system in check.  You can see  the memory start to rise here. .   . We noticed something happening on the system that never should though, dozens  of ‘git pack-objects’ calls running.  Normally Librato keeps these processes in  check but something seemed to be ignoring this.  We made it through the second  onslaught and had time to really dig into what might be causing the issue. . We started looking into what networks were on the fileserver, I’m sure you  recognize a few of them. . We were investigating whether or not this specific fileserver might be  overloaded due to popular projects when something else popped up.  Joe from   Librato  pointed us to some really awkward behavior we were seeing in system  resource usage on the server.  Something that we weren’t managing with Librato  really grew out of control during the times we saw service interruptions and  high load. .   . Memory grew linearly from around 3PM PST the day before until 5am where it  maxed out and eventually lead to the box swapping itself to death. . You can also see the virtual memory follow a similar trend here. .   . With this information we were able to quickly identify that the git-http  service that’s running on the fs servers was not under Librato’s policy  management.  We’ve been slowly pushing more and more people to use git-http by  default and we hadn’t experienced such a spike in traffic as we’ve seen over  the past few days.  We put git-http into a Librato container and we had to wait  for Thursday morning to really test it. . This morning went smoothly.  Librato kept all of our git-http processes in  check despite another morning of enormous git-http traffic.  We’re excited to  get back to work on making GitHub better, not keeping GitHub running.  We’re  really sorry for any inconvenience our users experienced due to the insanity  over the past few days.  We hope this run down of the events gives our users  some insight into how we handle problems.  Having metrics around as many things  as possible really helped us identify a difficult problem to diagnose. . A big thanks go out to  Saj  from Anchor for waking up in the middle of the night for three days straight to help us out with systems issues.  Thanks to  Joseph Ruscio  for the  Librato  insight that revealed the real fix. ", "date": "January 6, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tScheduled Maintenance Tonight at 22:00 PST\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2011-01-24-scheduled-maintenance-tonight-at-22-00-pst/", "abstract": " We will be performing scheduled maintenance tonight from  22:00 to 22:20 PST . During this window GitHub and all Git access will be entirely unavailable for a short period while we perform MySQL and Redis maintenance. GitHub Pages and GitHub Jobs will be unaffected. ", "date": "January 24, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tReply to Comments from Email\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2011-03-10-reply-to-comments-from-email/", "abstract": " You should notice a small change to the From address on your email notifications now: they’re no longer from  no-reply@github.com . .   . We’re now accepting replies from most email notifications that you’ll receive: . The biggest change in all this is how these replies are displayed. We figured out early on in testing that we couldn’t expect people to write Markdown. People are going to be dumping code or stacktraces, and will expect them to look nicely. Also, we need to accomodate the various top posters and bottom posters among our users (while holding judgement). .   . The new email comment formatting still has some quirks, but we think that the power to reply to emails was too much to hold back. Try it out, and let us know  in support  if you have problems. . A few caveats: ", "date": "March 10, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tThose are some big numbers\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2011-04-20-those-are-some-big-numbers/", "abstract": " Every night, our friendly  Hubot  pops into one of our Campfire rooms and posts some numbers. Turns out we passed some pretty significant numbers in the past couple days. And numbers are fun, so we thought we’d share them with you. .   . What’s even more staggering is that  70% have been created in the past year . We’re getting around 4,500 new GitHub projects a day. . Many people assume GitHub is filled with Ruby and Javascript projects. Let’s look at the numbers. .   . The most prominent project language on GitHub? Everything else. Remember that our languages page counts the  amount of code  — the chart above counts the  number of projects with a primary language . . Anyhow — these are some pretty crazy numbers to me. See you guys next time. .   ", "date": "April 20, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tGist creation in 5 lines with Java v3 API\t\t", "author": ["\n\t\tKevin Sawicki\t"], "link": "https://github.blog/2011-05-18-gist-creation-in-5-lines-with-java-v3-api/", "abstract": " As part of the  Mylyn connector announcement  last week a new stand-alone Java library was released for accessing  GitHub API v3 . . Below is a 5 line example showing how to create a new private Gist that has a single file containing a simple one line Java example: .  Your browser does not support IFrames  . Running the above example would create the following Gist: .  Your browser does not support IFrames  . Many more examples can be found within the  GitHub Mylyn connector  source base which uses it exclusively for all API calls.  The library currently supports creating and fetching issues, labels, milestones, and gists.  More services will be added to the library as they become available through the v3 API. . The  org.eclipse.egit.github.core  library requires  Google Gson  1.6+ and  Apache HttpComponents  4.1+ and the latest builds can be found on the  GitHub Eclipse Hudson page . . Have you created something interesting with the GitHub Java API library? Let us know! ", "date": "May 18, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tAll of the Hooks\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2011-10-19-all-of-the-hooks/", "abstract": " Over three years ago, @pjhyett  launched GitHub Services  with  just four services : Campfire, IRC, Lighthouse, and Twitter. Since then,  124 other people  contributed to a total of  68 third-party services . We and many others depend on these services to power our workflow and ship awesome features. . There have been two main requests for Hooks: an API to manage the Hooks, and support for other GitHub events besides just push. . Today, we’re announcing two APIs for Hooks and eleven new possible events, so you can build tighter integrations around GitHub. .  . In addition to an  HTTP Hook API  that’s consistent with the rest of API v3, we also support a  PubSubHubbub endpoint  to manage Hooks. The core  Web hook  already accepts hooks from all twelve events. The Campfire and IRC hooks support the new pull request and issue events as well. These changes are available in the API only, so eager developers can take a crack at them. Expect a future update to the Service Hook admin in the near future once these changes have been proven in battle. ", "date": "October 19, 2011"},
{"website": "Github-Engineering", "title": "\n\t\t\tFileserver Maintenance Wednesday Night\t\t", "author": ["\n\t\tJesse Newland\t"], "link": "https://github.blog/2012-04-24-fileserver-maintenance-wednesday-night/", "abstract": " We’ll be performing some scheduled maintenance on a fileserver pair this Wednesday (April 25th, 2012) at  9PM PDT . A small percentage of repositories will be placed in maintenance mode for around 30 minutes during this process, impacting access to these repos via HTTP, Git, and SSH. . Update: The maintenance is complete. Thanks for your patience. ", "date": "April 24, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tAkavache is now open source\t\t", "author": ["\n\t\tPaul Betts\t"], "link": "https://github.blog/2012-04-28-akavache-is-now-open-source/", "abstract": " Today, we’re open-sourcing a library that we have been using at GitHub:  Akavache . . Akavache is an  asynchronous ,  persistent  key-value cache created for writing native desktop and mobile applications in C#. Think of it like memcached for desktop apps. . Downloading and caching remote data from the internet while still keeping the UI responsive is a task that nearly every modern native application needs to do. However, many applications that don’t take the consideration of caching into the design from the start often end up with inconsistent, duplicated code for caching different types of objects. .  Akavache  is a library that makes common app patterns easy, and unifies caching of different object types (i.e. HTTP responses vs. JSON objects vs. images). . It’s built on a core key-value byte array store (conceptually similar to a  Dictionary&lt;string, byte[]&gt; ), and on top of that store, extensions are added to support: .   . When you open Twitter for Mac, you immediately see content, even before the application finishes talking to Twitter. This content is cached, but the pattern of when to refresh the data might be different. For the tweets themselves, the logic might be something like,  “Load the cached data, but always fetch the latest data” . . However, for the avatar images, you might have logic like,  “Always load the cached image, but if the avatar is older than six hours, refresh the image.”  . In Akavache, the former might be: . And the latter might be something like: . Akavache is non-blocking, via a library called the  Reactive Extensions  – any operation that could delay the UI returns an  Observable , which represents a future result. . Akavache also solves several difficult concurrency problems that simplify UI programming. For example, in the image above, a naive cache implementation could end up loading the GitHub avatar icon multiple times if the requests were issued at the same time (which is easy to do if you try to load the avatar for each tweet). Akavache ensures that exactly  one  network request is issued. . Check out the  GitHub Repository  for more examples, and install the package via  NuGet . Akavache is also used extensively in  Play for Windows , a desktop client for  Play , your employee-powered office DJ. ", "date": "April 28, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow we use Pull Requests to build GitHub\t\t", "author": ["\n\t\tBen Bleikamp\t"], "link": "https://github.blog/2012-05-02-how-we-use-pull-requests-to-build-github/", "abstract": " We recently shipped a new About section. It has all sorts of stuff like  high resolution logos, pictures of the GitHub team, a little bit about our story, recent press mentions and maybe most importantly  positions  we’re hiring for . It’s awesome. . But that’s not the point of this post. Instead, let’s take  a look at how we used a massive Pull Request to ship this feature. . We talk a lot about how GitHub works in  blog  posts  and   talks  and this is a great  example of how we use Pull Requests. . Here is what the  PR  looked like for the new  About page: .   . You’re looking at 130 commits and 91 comments from 10 different people over a two  month timespan. The discussion ranged from the original idea and HTML mock-up, to  Skitch mock ups from developers, to content strategy. There are designs posted  for review at various points. And of course, every commit to the branch is  tracked and ready for code review. . If you’ve ever talked to a GitHubber you’ll know we think Pull Requests are the  greatest thing ever. And not just because we invented them. . They are a great way to generate discussion around new ideas and recruit people to help out. Because we don’t have major organizational decisions, Pull Requests let people see what’s being worked on and they hop in where they think they’ll add the most value. It works a lot like an Open Source project. . Some tricks to make Pull Requests more awesome for your project: .  Open a Pull Request as early as possible  . Pull Requests are a great way to start a conversation of a feature, so start  one as soon as possible- even before you are finished with the code. Your  team can comment on the feature as it evolves, instead of providing all  their feedback at the very end. .  Pull Requests work branch to branch  . No one has a fork of  github/github . We make Pull Requests in the same repository by opening Pull Requests for branches. .  A Pull Request doesn’t have to be merged  . Pull Requests are easy to make and a great way to get feedback and track  progress on a branch. But some ideas don’t make it. It’s okay to close a  Pull Request without merging; we do it all the time. ", "date": "May 2, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tReactiveCocoa for a better world\t\t", "author": ["\n\t\tJosh Abernathy\t"], "link": "https://github.blog/2012-05-04-reactivecocoa-for-a-better-world/", "abstract": " Native apps spend a lot of time waiting and then reacting. We wait for the user to do something in the UI. Wait for a network call to respond. Wait for an asynchronous operation to complete. Wait for some dependent value to change. And then they react. . But all those things—all that waiting and reacting—is usually handled in many disparate ways. That makes it hard for us to reason about them, chain them, or compose them in any uniform, high-level way. We can do better. . That’s why we’ve open-sourced a piece of the magic behind  GitHub for Mac :  ReactiveCocoa  (RAC). . RAC is a framework for  composing and transforming sequences of values . . Let’s get more concrete. ReactiveCocoa gives us a lot of cool stuff: . The ability to compose operations on future data. . An approach to minimize state and mutability. . A declarative way to define behaviors and the relationships between properties. . A unified, high-level interface for asynchronous operations. . A lovely API on top of KVO. . Those all might seem a little random until you realize that RAC is all about handling these cases where we’re waiting for some new value and then reacting. . The real beauty of RAC is that it can adapt to a lot of different, commonly-encountered scenarios. . Enough talk. Let’s see what it actually looks like. . RAC can piggyback on  KVO (key-value observing)  to give us a sequence of values from a  KVO-compliant  property. For example, we can watch for changes to our  username  property: . That’s cool, but it’s really just a nicer API around KVO. The really cool stuff happens when we  compose sequences to express complex behavior . . Let’s suppose we want to check if the user entered a specific username, but only if it’s within the first three values they entered: . We watch  username  for changes, filter out non-distinct changes, take only the first three non-distinct values, and then if the new value is “joshaber”, we print out a nice welcome. . Think about what we’d have to do to implement that without RAC. We’d have to: . RAC lets us do the same thing with  less state, less boilerplate, better code locality, and better expression of our intent . . We can combine sequences: . Any time our  password  or  passwordConfirmation  properties change, we combine the latest values from both and reduce them to a BOOL of whether or not they matched. Then we enable or disable the create button with that result. . We can adapt RAC to give us powerful bindings with conditions and transformations: . That binds our help label’s text to our  help  property when the  help  property isn’t nil and after uppercasing the string (because users love being YELLED AT). . RAC also fits quite nicely with async operations. . For example, we can call a block once multiple concurrent operations have completed: . Or chain async operations: . That will login, load the cached messages, then fetch the remote messages, and then print “Fetched all messages.” . Or we can trivially move work to a background queue: . Or easily deal with potential race conditions. For example, we could update a property with the result of an asynchronous call, but only if the property doesn’t change before the async call completes: . RAC is fundamentally pretty simple. It’s all signals all the way down. _( Until you reach turtles. )_ . Subscribers subscribe to signals. Signals send their subscribers ‘next’, ‘error’, and ‘completed’ events. So if it’s all just signals sending events, the key question becomes  when do those events get sent?  . Signals define their own behavior with respect to if and when events are sent. We can create our own signals using  +[RACSignal createSignal:] : . The block we give to  +[RACSignal createSignal:]  is called whenever the signal gets a new subscriber. The new subscriber is passed into the block so that we can then send it events. In the above example, we created a signal that sends “Hello, “, and then “world!”, and then completes. . We could then create another signal based off our  helloWorld  signal: . Now we have a  joiner  signal. When someone subscribes to  joiner , it subscribes to our  helloWorld  signal. It adds all the values it receives from  helloWorld  and then when  helloWorld  completes, it joins all the strings it received into a single string, sends that, and completes. . In this way, we can build signals on each other to express complex behaviors. . RAC implements a set of  operations  on  RACSignal  that do exactly that. They take the source signal and return a new signal with some defined behavior. .  ReactiveCocoa  works on both Mac and iOS. See the  README  for more info and check out the  Mac  demo project for some practical examples. . For .NET developers, this all might sound eerily familiar. ReactiveCocoa essentially an Objective-C version of .NET’s  Reactive Extensions  (Rx). . Most of the principles of Rx apply to RAC as well. There are some really good Rx resources out there: ", "date": "May 4, 2012"},
{"website": "Github-Engineering", "title": "\n\t\t\tRackspace Move Scheduled for Sunday, September 27th at 5PM Pacific Time\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2009-09-22-rackspace-move-scheduled-for-sunday-september-27th-at-5pm-pacific-time/", "abstract": " On Sunday, September 27th at 5PM Pacific time  (see it in your timezone)  we will begin the final move to Rackspace. We are aiming to have less than an hour of website and push unavailability if everything goes according to plan. During this period, public and private clone, fetch, and pull operations will continue to function normally. Once the final data synchronization is complete and the new installation passes a final inspection, we will switch the DNS entries and you’ll be able to start using a new and improved GitHub! . The user impact from the switch should be minimal, but there are a few things to note: .  Your browser does not support IFrames  . We appreciate your patience during this procedure and look forward to all the benefits that the new hardware and architecture will make possible! ", "date": "September 22, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tUnicorn!\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-10-09-unicorn/", "abstract": " We’ve been running  Unicorn  for more than a month. Time to talk about it. . Unicorn is an HTTP server for Ruby, similar to Mongrel or Thin. It uses Mongrel’s Ragel HTTP parser but has a dramatically different architecture and philosophy. . In the classic setup you have nginx sending requests to a pool of mongrels using a smart balancer or a simple round robin. . Eventually you want better visibility and reliability out of your load balancing situation, so you throw haproxy into the mix: .   . Which works great. We ran this setup for a long time and were very happy with it. However, there are a few problems. . When actions take longer than 60s to complete, Mongrel will try to kill the thread. This has proven unreliable due to Ruby’s threading. Mongrels will often get into a “stuck” stage and need to be killed by some external process (e.g. god or monit). . Yes, this is a problem with our application. No action should ever take 60s. But we have a complicated application with many moving parts and things go wrong. Our production environment needs to handle errors and failures gracefully. . We restart mongrels that hit a certain memory threshhold. This is often a problem with parts of our application. Engine Yard has a great post on  memory bloat  and how to deal with it. . Like slow actions, however, it happens. You need to be prepared for things to not always be perfect, and so does your production environment. We don’t kill app servers often due to memory bloat, but it happens. . When your server’s CPU is pegged, restarting 9 mongrels hurts. Each one has to load all of Rails, all your gems, all your libraries, and your app into memory before it can start serving requests. They’re all doing the exact same thing but fighting each other for resources. . During that time, you’ve killed your old mongrels so any users hitting your site have to wait for the mongrels to be fully started. If you’re really overloaded, this can result in 10s+ waits. Ouch. . There are some complicated solutions that automate “rolling restarts” with multiple haproxy setups and restarting mongrels in different pools. But, as I said, they’re complicated and not foolproof. . As with the deploys, any time a mongrel is killed due to memory growth or timeout problems it will take multiple seconds until it’s ready to serve requests again. During peak load this can have a noticeable impact on the site’s responsiveness. . With most popular load balancing solutions, requests are handed to a load balancer who decides which mongrel will service it. The better the load balancer, the smarter it is about knowing who is ready. . This is typically why you’d graduate from an nginx-based load balancing solution to haproxy: haproxy is better at queueing up requests and handing them to mongrels who can actually serve them. . At the end of the day, though, the load balancer is still pushing requests to the mongrels. You run the risk of pushing a request to a mongrel who may not be the best candidate for serving a request at that time. .   .  Unicorn  has a slightly different architecture. Instead of the nginx =&gt; haproxy =&gt; mongrel cluster setup you end up with something like: .   . nginx sends requests directly to the Unicorn worker pool over a Unix Domain Socket (or TCP, if you prefer). The Unicorn master manages the workers while the OS handles balancing, which we’ll talk about in a second. The master itself never sees any requests. . Here’s the only difference between our nginx =&gt; haproxy and nginx =&gt; unicorn configs: . When the Unicorn master starts, it loads our app into memory. As soon as it’s ready to serve requests it forks 16 workers. Those workers then select() on the socket, only serving requests they’re capable of handling. In this way the kernel handles the load balancing for us. . The Unicorn master process knows exactly how long each worker has been processing a request. If a worker takes longer than 30s (we lowered it from mongrel’s default of 60s) to respond, the master immediately kills the worker and forks a new one. The new worker is instantly able to serve a new request – no multi-second startup penalty. . When this happens the client is sent a 502 error page. You may have seen  ours  and wondered what it meant. Usually it means your request was killed before it completed. . When a worker is using too much memory, god or monit can send it a QUIT signal. This tells the worker to die after finishing the current request. As soon as the worker dies, the master forks a new one which is instantly able to serve requests. In this way we don’t have to kill your connection mid-request or take a startup penalty. . Our deploys are ridiculous now. Combined with our  custom Capistrano recipes , they’re very fast. Here’s what we do. . First we send the existing Unicorn master a USR2 signal. This tells it to begin starting a new master process, reloading all our app code. When the new master is fully loaded it forks all the workers it needs. The first worker forked notices there is still an old master and sends it a QUIT signal. . When the old master receives the QUIT, it starts gracefully shutting down its workers. Once all the workers have finished serving requests, it dies. We now have a fresh version of our app, fully loaded and ready to receive requests, without any downtime: the old and new workers all share the Unix Domain Socket so nginx doesn’t have to even care about the transition. . We can also use this process to upgrade Unicorn itself. . What about migrations? Simple: just throw up a “The site is temporarily down for maintenance” page, run the migration, restart Unicorn, then remove the downtime page. Same as it ever was. . As mentioned above, restarts are only slow when the master has to start. Workers can be killed and re-fork() incredibly fast. . When we are doing a full restart, only one process is ever loading all the app code: the master. There are no wasted cycles. . Instead of being pushed requests, workers pull requests.  Ryan Tomayko  has a great article on the nitty gritties of this process titled  I like Unicorn because it’s Unix . . Basically, a worker asks for a request when it’s ready to serve one. Simple. . So, you want to migrate from thin or mongrel cluster to Unicorn? If you’re running an nginx =&gt; haproxy =&gt; cluster setup it’s pretty easy. Instead of changing any settings, you can simply tell the Unicorn workers to listen on a TCP port when they are forked. These ports can match the ports of your current mongrels. . Check out the  Configurator documentation  for an example of this method. Specifically this part: . This tells each worker to start listening on a port equal to their worker # + 9293 forever – they’ll keep trying to bind until the port is available. . Using this trick you can start up a pool of Unicorn workers, then shut down your existing pool of mongrel or thin app servers when the Unicorns are ready. The workers will bind to the ports as soon as possible and start serving requests. . It’s a good way to get familiar with Unicorn without touching your haproxy or nginx configs. . (For fun, try running “kill -9” on a worker then doing a “ps aux”. You probably won’t even notice it was gone.) . Once you’re comfortable with Unicorn and have your deploy scripts ready, you can modify nginx’s upstream to use Unix Domain Sockets then stop opening ports in the Unicorn workers. Also, no more haproxy. . Here’s our  Unicorn config  in all its glory. . I recommend making the  SIGNALS  documentation your new home page and reading all the other pages available at the  Unicorn site . It’s very well documented and Eric is focusing on improving it every day. . Honestly, I don’t care. I want a production environment that can gracefully handle chaos more than I want something that’s screaming fast. I want stability and reliability over raw speed. . Luckily, Unicorn seems to offer both. . Here are Tom’s benchmarks on our Rackspace bare metal hardware. We ran GitHub on one machine and the benchmarks on a separate machine. The servers are 8 core 16GB boxes connected via gigabit ethernet. . What we’re testing is a single Rails action rendering a simple string. This means each request goes through the entire Rails routing process and all that jazz. . Mongrel has haproxy in front of it. unicorn-tcp is using a port opened by the master, unicorn-unix with a 1024 backlog is the master opening a unix domain socket with the default “listen” backlog, and the 2048 backlog is the same setup with an increased “listen” backlog. . These benchmarks examine as many requests as we were able to push through before getting any 502 or 500 errors. Each test uses 8 workers. . Passenger is awesome.  Mongrel  is awesome.  Thin  is awesome. . Use what works best for you. Decide what you need and evaluate the available options based on those needs. Don’t pick a tool because GitHub uses it, pick a tool because it solves the problems you have. . We use Thin to serve the  GitHub Services  and I use Passenger for many of my side projects. Unicorn isn’t for every app. . But it’s working great for us. .  Edit:  Tweaked a diagram and clarified the Unicorn master’s role based on feedback from Eric. ", "date": "October 9, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tunicorn.god\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-10-12-unicorn-god/", "abstract": "   . Some people have been asking for our  Unicorn   god  config. . Here it is: . That’s for starting and stopping the master. It’s important to note that god only knows about the master – not the workers. The memory limit condition, then, only applies to the master (and is probably never hit). . To watch the workers we use a cute hack @mojombo came up with (though he promises first class support in future versions of code): we start a thread and periodically check the memory usage of workers. If a worker is gobbling up more than 300mb of RSS, we send it a QUIT. The QUIT tells it to die once it finishes processing the current request. Once that happens the master will spawn a new worker – we should hardly notice. . That’s it! Don’t forget the  Unicorn Signals  page when working with Unicorn. ", "date": "October 12, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tHow We Made GitHub Fast\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2009-10-20-how-we-made-github-fast/", "abstract": " Now that things have settled down from the move to Rackspace, I wanted to take some time to go over the architectural changes that we’ve made in order to bring you a speedier, more scalable GitHub. . In my first draft of this article I spent a lot of time explaining why we made each of the technology choices that we did. After a while, however, it became difficult to separate the architecture from the discourse and the whole thing became confusing. So I’ve decided to simply explain the architecture and then write a series of follow up posts with more detailed analyses of exactly why we made the choices we did. . There are many ways to scale modern web applications. What I will be describing here is the method that we chose. This should by no means be considered the only way to scale an application. Consider it a case study of what worked for us given our unique requirements. . We expose three primary protocols to end users of GitHub: HTTP, SSH, and Git. When browsing the site with your favorite browser, you’re using HTTP. When you clone, pull, or push to a private URL like  git@github.com:mojombo/jekyll.git  you’re doing so via SSH. When you clone or pull from a public repository via a URL like  git://github.com/mojombo/jekyll.git  you’re using the Git protocol. . The easiest way to understand the architecture is by tracing how each of these requests propagates through the system. . For this example I’ll show you how a request for a tree page such as  http://github.com/mojombo/jekyll  happens. . The first thing your request hits after coming down from the internet is the active load balancer. For this task we use a pair of Xen instances running  ldirectord . These are called  lb1a  and  lb1b . At any given time one of these is active and the other is waiting to take over in case of a failure in the master. The load balancer doesn’t do anything fancy. It forwards TCP packets to various servers based on the requested IP and port and can remove misbehaving servers from the balance pool if necessary. In the event that no servers are available for a given pool it can serve a simple static site instead of refusing connections. . For requests to the main website, the load balancer ships your request off to one of the four frontend machines. Each of these is an 8 core, 16GB RAM bare metal server. Their names are  fe1 , …,  fe4 .  Nginx  accepts the connection and sends it to a Unix domain socket upon which sixteen  Unicorn  worker processes are selecting. One of these workers grabs the request and runs the  Rails  code necessary to fulfill it. . Many pages require database lookups. Our MySQL database runs on two 8 core, 32GB RAM bare metal servers with 15k RPM SAS drives. Their names are  db1a  and  db1b . At any given time, one of them is master and one is slave. MySQL replication is accomplished via  DRBD . . If the page requires information about a Git repository and that data is not cached, then it will use our  Grit  library to retrieve the data. In order to accommodate our Rackspace setup, we’ve modified Grit to do something special. We start by abstracting out every call that needs access to the filesystem into the Grit::Git object. We then replace Grit::Git with a stub that makes RPC calls to our Smoke service. Smoke has direct disk access to the repositories and essentially presents Grit::Git as a service. It’s called Smoke because Smoke is just Grit in the cloud. Get it? . The stubbed Grit makes RPC calls to  smoke  which is a load balanced hostname that maps back to the  fe  machines. Each frontend runs four  ProxyMachine  instances behind  HAProxy  that act as routing proxies for Smoke calls. ProxyMachine is my content aware (layer 7) TCP routing proxy that lets us write the routing logic in Ruby. The proxy examines the request and extracts the username of the repository that has been specified. We then use a proprietary library called Chimney (it routes the smoke!) to lookup the route for that user. A user’s route is simply the hostname of the file server on which that user’s repositories are kept. . Chimney finds the route by making a call to  Redis . Redis runs on the database servers. We use Redis as a persistent key/value store for the routing information and a variety of other data. . Once the Smoke proxy has determined the user’s route, it establishes a transparent proxy to the proper file server. We have four pairs of fileservers. Their names are  fs1a ,  fs1b , …,  fs4a ,  fs4b . These are 8 core, 16GB RAM bare metal servers, each with six 300GB 15K RPM SAS drives arranged in RAID 10. At any given time one server in each pair is active and the other is waiting to take over should there be a fatal failure in the master. All repository data is constantly replicated from the master to the slave via DRBD. . Every file server runs two  Ernie  RPC servers behind HAProxy. Each Ernie spawns 15 Ruby workers. These workers take the RPC call and reconstitute and perform the Grit call. The response is sent back through the Smoke proxy to the Rails app where the Grit stub returns the expected Grit response. . When Unicorn is finished with the Rails action, the response is sent back through Nginx and directly to the client (outgoing responses do not go back through the load balancer). . Finally, you see a pretty web page! . The above flow is what happens when there are no cache hits. In many cases the Rails code uses Evan Weaver’s Ruby  memcached  client to query the  Memcache  servers that run on each slave file server. Since these machines are otherwise idle, we place 12GB of Memcache on each. These servers are aliased as  memcache1 , …,  memcache4 . . For our data serialization and RPC protocol we are using BERT and BERT-RPC. You haven’t heard of them before because they’re brand new. I invented them because I was not satisfied with any of the available options that I evaluated, and I wanted to experiment with an idea that I’ve had for a while. Before you freak out about NIH syndrome (or to help you refine your freak out), please read my accompanying article  Introducing BERT and BERT-RPC  about how these technologies came to be and what I intend for them to solve. . If you’d rather just check out the spec, head over to  http://bert-rpc.org . . For the code hungry, check out my Ruby BERT serialization library  BERT , my Ruby BERT-RPC client  BERTRPC , and my Erlang/Ruby hybrid BERT-RPC server  Ernie . These are the exact libraries we use at GitHub to serve up all repository data. . Git uses SSH for encrypted communications between you and the server. In order to understand how our architecture deals with SSH connections, it is first important to understand how this works in a simpler setup. . Git relies on the fact that SSH allows you to execute commands on a remote server. For instance, the command  ssh tom@frost ls -al  runs  ls -al  in the home directory of my user on the  frost  server. I get the output of the command on my local terminal. SSH is essentially hooking up the STDIN, STDOUT, and STDERR of the remote machine to my local terminal. . If you run a command like  git clone tom@frost:mojombo/bert , what Git is doing behind the scenes is SSHing to  frost , authenticating as the  tom  user, and then remotely executing  git upload-pack mojombo/bert . Now your client can talk to that process on the remote server by simply reading and writing over the SSH connection. Neat, huh? . Of course, allowing arbitrary execution of commands is unsafe, so SSH includes the ability to restrict what commands can be executed. In a very simple case, you can restrict execution to  git-shell  which is included with Git. All this script does is check the command that you’re trying to execute and ensure that it’s one of  git upload-pack ,  git receive-pack , or  git upload-archive . If it is indeed one of those, it uses  exec(3)  to replace the current process with that new process. After that, it’s as if you had just executed that command directly. . So, now that you know how Git’s SSH operations work in a simple case, let me show you how we handle this in GitHub’s architecture. . First, your Git client initiates an SSH session. The connection comes down off the internet and hits our load balancer. . From there, the connection is sent to one of the frontends where  SSHD  accepts it. We have patched our SSH daemon to perform public key lookups from our MySQL database. Your key identifies your GitHub user and this information is sent along with the original command and arguments to our proprietary script called Gerve (Git sERVE). Think of Gerve as a super smart version of  git-shell . . Gerve verifies that your user has access to the repository specified in the arguments. If you are the owner of the repository, no database lookups need to be performed, otherwise several SQL queries are made to determine permissions. . Once access has been verified, Gerve uses Chimney to look up the route for the owner of the repository. The goal now is to execute your original command on the proper file server and hook your local machine up to that process. What better way to do this than with another remote SSH execution! . I know it sounds crazy but it works great. Gerve simply uses  exec(3)  to replace itself with a call to ssh git@&lt;route&gt; &lt;command&gt; &lt;arg&gt; . After this call, your client is hooked up to a process on a frontend machine which is, in turn, hooked up to a process on a file server. . Think of it this way: after determining permissions and the location of the repository, the frontend becomes a transparent proxy for the rest of the session. The only drawback to this approach is that the internal SSH is unnecessarily encumbered by the overhead of encryption/decryption when none is strictly required. It’s possible we may replace this this internal SSH call with something more efficient, but this approach is just too damn simple (and still very fast) to make me worry about it very much. . Performing public clones and pulls via Git is similar to how the SSH method works. Instead of using SSH for authentication and encryption, however, it relies on a server side  Git Daemon . This daemon accepts connections, verifies the command to be run, and then uses  fork(2)  and  exec(3)  to spawn a worker that then becomes the command process. . With this in mind, I’ll show you how a public clone operation works. . First, your Git client issues a  request  containing the command and repository name you wish to clone. This request enters our system on the load balancer. . From there, the request is sent to one of the frontends. Each frontend runs four ProxyMachine instances behind HAProxy that act as routing proxies for the Git protocol. The proxy inspects the request and extracts the username (or gist name) of the repo. It then uses Chimney to lookup the route. If there is no route or any other error is encountered, the proxy speaks the Git protocol and sends back an appropriate messages to the client. Once the route is known, the repo name (e.g.  mojombo/bert ) is translated into its path on disk (e.g.  a/a8/e2/95/mojombo/bert.git ). On our old setup that had no proxies, we had to use a modified daemon that could convert the user/repo into the correct filepath. By doing this step in the proxy, we can now use an unmodified daemon, allowing for a much easier upgrade path. . Next, the Git proxy establishes a transparent proxy with the proper file server and sends the modified request (with the converted repository path). Each file server runs two Git Daemon processes behind HAProxy. The daemon speaks the pack file protocol and streams data back through the Git proxy and directly to your Git client. . Once your client has all the data, you’ve cloned the repository and can get to work! . In addition to the primary web application and Git hosting systems, we also run a variety of other sub-systems and side-systems. Sub-systems include the job queue, archive downloads, billing, mirroring, and the svn importer. Side-systems include GitHub Pages, Gist, gem server, and a bunch of internal tools. You can look forward to explanations of how some of these work within the new architecture, and what new technologies we’ve created to help our application run more smoothly. . The architecture outlined here has allowed us to properly scale the site and resulted in massive performance increases across the entire site. Our average Rails response time on our previous setup was anywhere from 500ms to several seconds depending on how loaded the slices were. Moving to bare metal and federated storage on Rackspace has brought our average Rails response time to consistently under 100ms. In addition, the job queue now has no problem keeping up with the 280,000 background jobs we process every day. We still have plenty of headroom to grow with the current set of hardware, and when the time comes to add more machines, we can add new servers on any tier with ease. I’m very pleased with how well everything is working, and if you’re like me, you’re enjoying the new and improved GitHub every day! ", "date": "October 20, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tMultiple file gist improvements\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2009-11-19-multiple-file-gist-improvements/", "abstract": "   . We’ve always had the ability to embed multiple file gists: . Your browser does not support IFrames . But today we added the ability to embed specific files in a multi-file gist! . Your browser does not support IFrames . We also added permalinks next to each file name so you can hard link to specific files  like so  .   . Enjoy! ", "date": "November 19, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tOptimizing asset bundling and serving with Rails\t\t", "author": ["\n\t\tKyle Neath\t"], "link": "https://github.blog/2009-11-19-optimizing-asset-bundling-and-serving-with-rails/", "abstract": "   . We spend a lot of time optimizing the front end experience at GitHub.  With that said, our asset (css, javascript, images) packaging and serving has evolved to be the best setup I’ve seen out of any web application I’ve worked on in my life. . Originally, I was going to package what we have up into a plugin, but realized that much of our asset packaging is specific our particular app architecture and  choice of deployment strategy .  If you haven’t read up on our deployment recipe  _read it now _.  I cannot stress enough how awesome it is to have 14 second no downtime deploys. In any case, you can find the relevant asset bundling code in  this gist  . Our asset bundling is comprised of several different pieces: . A particular css &amp; js file structure . Rails helpers to include css &amp; js bundles in production and the corresponding files in development. . A rake task to bundle and minify css &amp; javascript as well as the accompanying changes to deploy.rb to make it happen on deploy . Tweaks to our Rails environment to use smart ASSET_ID and asset servers . Our file layout for CSS &amp; JS is detailed in the  README  for Javascript, but roughly resembles something like this: . I like this layout because: . Some might say that relying on including everything is bad practice — but remember that web-based javascript is almost exclusively onDOMReady or later. That means that there is no dependency order problems. If you run into dependency order issues, you’re writing javascript wrong. . To help with this new bundle strategy, I’ve created some Rails helpers to replace your standard  stylesheet_link_tag  and  javascript_include_tag .  Because of the way we bundle files, it was necessary to use custom helpers. As an added benefit, these helpers are much more robust than the standard Rails helpers. . Here’s the code: . Our  application.html.erb  now looks something like this: . This includes jQuery and all javascript files under  public/javascripts/common  and  public/javascripts/github  (recursively).  Super simple and we probably won’t need to change this for a very long time.  We just add files to the relevant directories and they get included magically. . For pages that have heavy javascript load, you can still use the regular  javascript_include_tag  to include these files (we keep them under the  public/javascripts/rogue  directory). . The  javascript_bundle  and  stylesheet_bundle  helpers both assume that in production mode, there’ll be a corresponding bundle file.  Since we are proactively generating these files, you need to create these manually on each deploy. . Throw this into  lib/tasks/bundle.rake   _and the corresponding YUI &amp; Closure jars _ and then run  rake bundle:all  to generate your javascript.  You can customize this to use the minifying package of your choice. . To make sure this gets run on deploy, you can add this to your deploy.rb: . The last step in optimizing your asset bundling for deploys is to tweak your production.rb config file to make asset serving a bit smarter.  The relevant bits in our file are: . There’s three important things going on here. .  First—  If you hit a page using SSL, we serve all assets through SSL.  If you’re on Safari, we send all CSS &amp; images non-ssl since Safari doesn’t have a mixed content warning. . It is of note that many people  suggest serving CSS &amp; images non-ssl to Firefox .  This was good practice when Firefox 2.0 was standard, but now that Firefox 3.0 is standard (and obeys cache-control:public as it should) there is no need for this hack.  Firefox does have a mixed content warning (albeit not as prominent as IE), so I choose to use SSL. .  Second—  We’re serving assets out of 4 different servers.  This fakes browsers into downloading things faster and is generally good practice. .  Third—  We’re hitting the git repo on the server (note our deployment setup) and getting a sha of the last changes to the  public/stylesheets  and  public/javascripts  directory.  We use that sha as the ASSET_ID (the bit that gets tacked on after css/js files as ?sha-here). . This means that if we deploy a change that only affects  app/application.rb  we don’t interrupt our user’s cache of the javascripts and stylesheets. . What all of this adds up to is that our deploys have almost no frontend consequence unless they intend to (changing css/js). This is  huge  for a site that does dozens of deploys a day. All browser caches remain the same and there isn’t any downtime while we bundle up assets.  It also means we’re not afraid to deploy changes that may only affect one line of code and some minor feature. . All of this is not to say there isn’t room for improvement in our stack.  I’m still tracking down some SSL bugs, and always trying to cut down on the total CSS, javascript and image load we deliver on every page. ", "date": "November 19, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tNew Languages Highlighted\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2010-02-24-new-languages-highlighted/", "abstract": "   ", "date": "February 24, 2010"},
{"website": "Github-Engineering", "title": "\n\t\t\tTracking Deploys with Compare View\t\t", "author": ["\n\t\tRyan Tomayko\t"], "link": "https://github.blog/2010-03-09-tracking-deploys-with-compare-view/", "abstract": "   . We log a message to  Campfire  anytime someone  deploys code to staging or production. It looks like this: .   . Recently, we added the link pointing to a  Compare View  where you  can review the commits that were shipped out along with a full diff of  changes: .   . This makes it really easy for everyone to keep tabs on what’s being  deployed and encourages  on the fly  code review. And because Campfire  keeps transcripts when you’re offline, it doubles as a kind of  deployment log. I typically start my day by catching up on the Campfire  backlog, hitting the Compare View links as I go. When I’m done, I have a  bunch of tabs queued up in my browser for review. . The most important piece of this trick is generating the Compare View  URL. The example in the screen cap above is truncated, so I’ll reproduce  it here in full here: . Here,  defunkt/github  is the repository with the code we’re deploying.  Just in case it isn’t obvious: you’ll need to change that to your own  repository. . The last part of the URL is the commit range. It’s important to use  commit SHA1s as the starting and ending points. We could have used the  name of the branch being deployed as the ending point, but doing so  would cause the Compare View to change when commits are pushed to the  branch in the future. By using the commit SHA1s, we’re guaranteed that  the Compare View will never change. . Here’s the  Capistrano   recipe we use to generate the Compare View URL and send the Campfire  notification ( config/deploy/notify.rb ): . This isn’t something you can drop into an existing project unmodified,  but it should serve as a good starting point. A bit more detail on  what’s happening in there: . And that’s it, really. . This technique could easily be adapted for other deployment strategies ( Heroku  — where you at) or notification systems like email, IRC, or Twitter. ", "date": "March 9, 2010"},
{"website": "Github-Engineering", "title": "\n\t\t\tScala Projects Classified Properly\t\t", "author": ["\n\t\tRisk Olson\t"], "link": "https://github.blog/2010-04-20-scala-projects-classified-properly/", "abstract": "   . We just fixed an issue where some Scala projects were being misclassified as Java projects.  Now, recent projects like Twitter’s  FlockDB  and  Gizzard  show up in the  Scala language dashboard  as they should. . You can also use Scala syntax highlighting in Gist now.  It’s bonus. ", "date": "April 20, 2010"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe Tree Slider\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2010-12-07-the-tree-slider/", "abstract": " Those of you running recent versions of Safari, Chrome, or Firefox 4  may have noticed  some changes to  tree browsing  on GitHub. . The new  HTML5 History API  (which really has nothing to do with HTML — it’s a JavaScript API) allows us to manage the URL changes while  CSS3 transitions  handle the sliding. Permalinks are always maintained, your back button works as expected, and it’s much faster than waiting for a full page load. . Basically we intercept your click, call  pushState()  to change the browser’s URL, load in data with Ajax, then slide over to it. .  Your browser does not support IFrames  . When you hit the back button, an  onpopstate  handler is fired after the URL changes, making it easy to send you “back”. .  Your browser does not support IFrames  . Want more? Check out the  HTML History API Demo  and MDC’s  Manipulating the browser history  documentation. Facebook has  blogged about  their use of this stuff, and Flickr has been doing it for months on their  lightbox view . . There’s also some hot  replaceState()  action over on our new  Features page  and the  Pull Requests dashboard . . We’re still getting all the kinks out of the Tree Slider, but we hope you like it! ", "date": "December 7, 2010"},
{"website": "Github-Engineering", "title": "\n\t\t\tDirty Git State in Your Prompt\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-15-dirty-git-state-in-your-prompt/", "abstract": "  Henrik  has a great article explaining  why and how to display Git’s dirty state status  (along with the branch, of course) in your bash prompt. .  topfunky  prefers a  skull and bones  for his dirty state indicator. . Thanks guys! ", "date": "January 15, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tGist for Greasemonkey\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2009-01-19-gist-for-greasemonkey/", "abstract": " We’re now appending the gist name at the end of its raw url. That means it’s dead-simple to serve  greasemonkey  (or  greasekit ) scripts directly from  gist.github.com . I was able to write my first script and install it in less than five minutes: .  Your browser does not support IFrames  . If you have greasemonkey installed and click the “view raw” link in the embedded gist above, your browser will ask you if you want to install the script. . This of course comes with a strong word of caution that you pay attention to the scripts you’re installing. Your browser  will not  execute the javascript, we serve it as plain/text, so feel free to hit cancel when the install dialog appears in order read over it first. .  Note:  You won’t see the new raw url until your gist cache is updated, so you can either wait until it falls out of the cache, or just make a simple change to your gist to update it immediately. ", "date": "January 19, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tCompojure: Clojure Web Framework\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-20-compojure-clojure-web-framework/", "abstract": "  Compojure  is a Clojure web framework similar to  web.py  or  Sinatra . .  Your browser does not support IFrames  . The project also has a  Wikibook  and  mailing list . Looks cool. ", "date": "January 20, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tScripting Bioclipse\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-01-23-scripting-bioclipse/", "abstract": "  Bioclipse  (a Java-based, open source, visual platform for chemo- and bioinformatics) has scripting support and the community has developed a great method for sharing those scripts:  Gist ! . They create Gists then tag them  on delicious  as bioclipse+gist. . For example, here’s one that downloads itself: .  Your browser does not support IFrames  . Check  their blog post  for more info. Nicely done, all! . (Seems like we could add a few features to Gist to make this sort of thing even easier.) ", "date": "January 23, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tEasy Git!\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-02-03-easy-git/", "abstract": "  eg  is a nifty piece of work. Are you meeting resistance trying to move your coworkers or friends to Git? (“SVN is good enough.”) Know someone who would love to use GitHub but can’t seem to find the time to learn Git? eg is your answer. . Start with the  Easy Git for SVN Users  chart. .   . Then move to the  eg cheat sheet : . Your browser does not support IFrames . Committing works similar to SVN but tries to educate you on the idea of the staging area. . Install it: . Your browser does not support IFrames .  Download eg . . It’s not Subversion, but it’s a step in the Git direction. ", "date": "February 3, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tPHP in Erlang\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-02-16-php-in-erlang/", "abstract": " You heard me right.  php_app   manages a pool of persistent PHP processes and provides a simple API to evaluate PHP code from Erlang. . The  blog post  gives a quick overview and some examples. .  Your browser does not support IFrames  . Thanks for sharing, @skeltoac! ", "date": "February 16, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tKeeping GoogleBot Happy\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2009-03-04-keeping-googlebot-happy/", "abstract": " One of the interesting side effects I hadn’t considered when we rolled out some fairly significant caching updates on GitHub in the beginning of January was how much Google’s crawler would take full advantage of the speed increase. .   . The graph lays it all out pretty clearly what happens when your site is more responsive. The number of pages the bot is able to index goes up dramatically when the time spent downloading the page is reduced dramatically. More pages indexed on Google inevitably means more revenue for us as our traffic grows, so this is a solid win beyond making sure existing GitHubbers are happy. ", "date": "March 4, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tGit as a Data Store in Python (and Ruby)\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-03-10-git-as-a-data-store-in-python-and-ruby/", "abstract": " I recently stumbled across an older article titled  Using Git as a versioned data store in Python  by @jwiegley. .  Your browser does not support IFrames  . Basically, it’s a module similar to Python’s  shelve  which stores your data in a Git repository. Nicely done. . The repository is part of his git-issues project at  http://github.com/jwiegley/git-issues . . (There’s also a similar project in Ruby, GitStore  http://github.com/georgi/git_store , worth looking into.) ", "date": "March 10, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tSmart JS Polling\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-07-30-smart-js-polling/", "abstract": " While  Comet  may be all the rage, some of us are still stuck in web 2.0. And those of us that are use Ajax polling to see if there’s anything new on the server. . Here at GitHub we normally do this with memcached. The web browser polls a URL which checks a memcached key. If there’s no data cached, the request returns and polls again in a few seconds. If there is data, the request returns with it and the browser merrily goes about its business. On the other end our background workers stick the goods in memcached when they’re ready. . In this way we use memcached as a poor man’s message bus. . Yet there’s a problem with this: if after a few Ajax polls there’s no data, there probably won’t be for a while. Maybe the site is overloaded or the queue is backed up. In those circumstances the continued polling adds additional unwanted strain to the site. What to do? . The solution is to increment the amount of time you wait in between each poll. Really, it’s that simple. We wrote a little jQuery plugin to make this pattern even easier in our own JS. Here it is, from us to you: .  Your browser does not support IFrames  . Any time you see “Loading commit data…” or “Hardcore Archiving Action,” you’re seeing smart polling. Enjoy! ", "date": "July 30, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tDeployment Script Spring Cleaning\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2009-08-05-deployment-script-spring-cleaning/", "abstract": " Better late than never, right? As we get ready to upgrade our servers I thought it’d be a good time to upgrade our deployment process. Currently pushing out a new version of GitHub takes upwards of 15 minutes. Ouch. My goal: one minute deploys (excluding server restart time). . We currently use  Capistrano  with a 400 line  deploy.rb  file. Engine Yard provides a handful of useful Cap tasks (in gem form) that we use along with many of the built-in features. We also use the  fast_remote_cache  deployment strategy and have written a handful (400 lines or so) of our own tasks to manage things like our  service hooks  or SVN importer. . As you may know, Capistrano keeps a  releases  directory where it creates timestamped versions of your app. All your daemons and processes then assume your app lives under a directory called  current  which is actually a symlink to the latest timestamped version of your app in  releases . When you deploy a new version of your app, it’s put into a new timestamped directory under  releases . After all the heavy lifting is done the  current  symlink is switched to it. . Which was really great. Before Git. So I went digging. .     . First I investigated  Vlad the Deployer , the Capistrano alternative in Ruby. I like that it’s built on Rake but it seems to make the same assumptions as Capistrano. Basically both of these tools are modular and built in such a way that they work the same whether you’re using Subversion, Perforce, or Git. Which is great if you’re using SVN but unfortunate if you’re using Git. . For example, this is from Vlad’s included Git deployment strategy: . Your browser does not support IFrames . When you deploy a new copy of your app, Vlad removes the existing copy and does a full clone to get a new version. Capistrano does something similar by default but has a bundled “remote_cache” strategy that is a bit smarter: it caches the Git repo and does a  fetch  then a  reset . It still has to then copy the updated version of your app into a timestamped directory and switch the symlink, but it’s able to cut down on time spent pulling redundant objects. It even knows about the  depth  option. .     . The next thing I looked at was Heroku’s  rush . It lets you drive servers (even clusters of them) using Ruby over SSH, which looked very promising. Maybe I’d write a little  git-deploy  script based on it. . Unfortunately for me Rush needs to be installed on every server you’re managing. It also needs a running instance of  rushd . Which makes sense – it’s a super powerful library – but that wouldn’t work for deploying GitHub. .     .  Fabric  is a library I first heard about back in February. It’s like Capistrano or Vlad but with more emphasis on being a framework/tool for remote management of servers. Easy deployment scripts are just a side effect of that mentality. . It’s very powerful and after playing with it for a while I was extremely pleased. I’ll definitely be using it in all my Python projects. However, I wasn’t looking forward to porting all our custom Capistrano tasks to Python. Also, though I love Python, we’re mostly a Ruby shop and everyone needs to be able to add, debug, and modify our deploy scripts with ease. . Playing with Fabric did inspire me, though. Capistrano is basically a tool for remote server management, too, if you think about it. We may have outgrown its ideas about deployment but I can always write my own deployment code using Capistrano’s ssh and clustering capabilities. So I did. .     . It turned out to be pretty easy. First I created a  config/deploy  directory and started splitting up the  deploy.rb  into smaller chunks: . Then I pulled them in. Careful here: Capistrano override both  load  and  require  so it’s probably best to just use  load . . Your browser does not support IFrames . This separation kept the  deploy.rb  and each specific file small and focused. . Next I thought about how I’d do Git-based deployment. Not too different from Capistrano’s remote_cache, really. Just get rid of all the timestamp directories and have the  current  directory contain our clone of the Git repo. Do a  fetch  then  reset  to deploy. Rollback? No problem. . The best part is that because Engine Yard’s gemified tasks and our own code both call standard Capistrano tasks like  deploy  and  deploy:update , we can just replace them and not change the dependent code. . Here’s what our new  deploy.rb  looks like. Well, the meat of it at least: . Your browser does not support IFrames . Great. I like this – very Gitty and simple. But copying and removing directories wasn’t the only slow part of our deploy process. . Every Capistrano task you run adds a bit of overhead. I don’t know exactly why, but I imagine each task opens a fresh SSH connection to the necessary servers. Maybe. Either way, the less tasks you run the better. . We were running about eight symlink related tasks during each deploy. Config files and cache directories that only live on the server need to be symlinked into the app’s directory structure after the  reset . Cutting these actions down to a single task made everything much, much faster. . Here’s our  symlinks.rb : . Your browser does not support IFrames . Finally, bundling CSS and JavaScript. I’d like to move us to  Sprockets  but we’re not using it yet and this adventure is all about speeding up our existing setup. . Since the early days we’ve been using Uladzislau Latynski’s jsmin.rb to minimize our JavaScript. Our Cap task looked something like this: . Your browser does not support IFrames . Spot the problem? We’re minimizing the JS locally, on every deploy, then uploading it to each server individually. We also do this same process for Gist’s JavaScript and the CSS (using YUI’s CSS compressor). So with N servers, this is basically happening 3N times on each deploy. Yowza. . Solution? Do the minimizing and bundling on the servers. The beefy, beefy servers: . Your browser does not support IFrames . As long as the  bundle  Rake tasks don’t need to load the Rails environment (which ours don’t), this is much faster. . We moved to a more Git-like deployment setup, cut down the number of tasks we run, and moved bundling and minimizing JS and CSS from our localhost to the server. Did it help? . As I said before, a GitHub deploy can take 15 minutes (not counting server restarts). My goal was to drop it down to 1 minute. How’d we do? . 15 minutes down to 14 seconds. Not bad. ", "date": "August 5, 2009"},
{"website": "Github-Engineering", "title": "\n\t\t\tDowntime Tonight\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-03-07-downtime-tonight/", "abstract": " We’ll have an hour or two of downtime tonight around midnight PST while the awesome dudes at  Engine Yard  upgrade our disk capacity.  Thanks, see you on the flip side. ", "date": "March 7, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tSupercharged Ruby-Git\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2008-07-09-supercharged-ruby-git/", "abstract": " One of the slowest things you can do in Ruby is shell out to the operating system.  As a contrived example, let’s open an empty file 1,000 times: . The difference is clear – the very act of shelling out is expensive.  And while 1,000 may seem high, we have plenty of content on GitHub with 30+ shell calls per page.  It starts to add up. . Our Grit library was written as an API to the @git@ binary using, you guessed it, shell calls.  In the past few weeks, as the site became slower and less stable, we knew we had to begin rewriting parts of our infrastructure.  Response times and memory usage were both spiking.  We began seeing weird out of memory errors and @git@ segfaults. .  Scott Chacon  had been working on a pure Ruby implementation of Git for some time, which we’d been watching with interest.  Instead of shelling out and asking the @git@ binary for information, Scott’s library understands the layout of @.git@ directories and uses methods like @File.read@ to procure the requested information . Over the past few weeks we’ve been working with Scott to integrate his library into GitHub while he adds features and improves performance.  Last night we rolled out a near-finished version of Scott’s library. . The result?  Sweet, sweet speed. . Yep, we cut our average response time in half.  (Lower numbers are better.) . Scott will soon be merging the changes he made for us into his  Grit fork .  As a result, expect to see other Ruby-based Git hosting sites speed up in the next few weeks as they integrate the code we wrote. . We’re interested in funding the development of other Git related open source projects.  If you’re working on something awesome that will drive Git adoption, please send us an email. . We’re still working to improve our architecture.  As we roll out more changes, you’ll see them here.  Everyone loves scaling. ", "date": "July 9, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tSupercharged git-daemon\t\t", "author": ["\n\t\tTom Preston-Werner\t"], "link": "https://github.blog/2008-07-14-supercharged-git-daemon/", "abstract": " Over the past several weeks I’ve been working on a secret Erlang project that will allow us to grow GitHub in new and novel ways. The project is called egitd and is a replacement for the stock git-daemon that ships with git. If you’re not familiar, git-daemon is what has, until today, served all anonymous requests for git repositories. Any time you used a command like  git clone git://github.com/user/repo.git  you were being served those files by git-daemon. . The reason we need egitd is for flexibility and power. We need the flexibility to map the repo name that you specify on the command line to any disk location we choose. git-daemon is very strict about the file system mappings that you are allowed to do. We also need it so that we can distinguish and log first-time clones of repos. Keep an eye out (down the road) for statistics that show you exactly how many times your public repo has been cloned! . Another benefit of coding our own git server is enhanced error messages. I can’t even begin to tell you how many people have come to us complaining about the following error which is caused by trying to  push  to the public clone address: . With egitd we can inject reasonable error responses into the response instead of just closing the connection and leaving the user bewildered. Behold! . Still a little crufty, but until we can get something useful into core git, it’s the best we can do and should help many people as they learn git and get over some of the confusing aspects. ", "date": "July 14, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tScaling Lesson #23742\t\t", "author": ["\n\t\tPJ Hyett\t"], "link": "https://github.blog/2008-07-17-scaling-lesson-23742/", "abstract": " GitHub was created as a side project, but it seems to have struck a nerve and gained traction quickly. As such, a lot of the infrastructure decisions were made not figuring on this sort of growth: .   . One of the major pieces of the infrastructure is how we store the repositories. The way it was originally setup worked great for a while, but it wasn’t sustainable. . As an example, lets take my  github-services  repository. Here’s where it was stored prior to yesterday: . Straight forward and simple, as well as having the added benefit of the repo being easily locatable in the file system if we needed to debug an issue. . That works well unless you have thousands of folders sitting in the same directory.  GFS  tried as best as it could, but with the amount of IO we do at GitHub writing to and reading from the file system, a change had to be made quickly. . After migrating last night, taking the same repository, this is where it’s currently stored: . Instead of every user sitting in one directory, we’ve sharded the repositories based on an MD5 of the username. A large change to be sure, but with some number crunching by our very own Tom Preston-Werner, he told me everyone on the planet can sign up twice and we still won’t have to change the way we shard our repositories after this. . Another interesting point worth mentioning is the first directory, ‘5’, was setup specifically so we could add multiple GFS mounts (we currently use just one) using a simple numbering system to help scale the data when we start bumping up against that wall again. . Now, the question you may all be asking is why we didn’t do this from the beginning. The simple answer is it would have taken more time and prevented us from launching when we did. We could have spent a couple of extra weeks in the beginning figuring out and preventing bottlenecks, but the site may not have taken off and then we would have built a scalable site that three people use. . Truth be told, it’s a great problem to have, and the site is humming along smoothly now. Now we can get back to doing fun things like building new features for you guys and gals. Keep an eye out for the big one we’re launching next week! ", "date": "July 17, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tThe New Queue\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-10-30-the-new-queue/", "abstract": " Yesterday we moved to a new queue,  Shopify’s   delayed_job  (or dj). . After trying a few different solutions in the early days, we settled on Ara Howard’s  Bj . It was fine for quite a while, but some of the design decisions haven’t been working out for us lately. Bj allows you to spawn exactly one worker per machine – we want a machine dedicated to workers. Bj loads a new Rails environment for every job submitted – we want to load a new Rails environment one time only. Both of these decisions carry performance implications. . If we were to run one Bj per machine, we’d only have four workers running as GitHub consists of four, ultra-beefy app slices. Unlike most contemporary web apps, the fewer the slices we have the better – it means less machines connected to our networked file system, and less machines create less network chatter and lock contention. As some of the jobs take a while to run (60+ seconds), four workers is a very low number. We want something like 20, but we’d settle for as few as 8. . We did hack Bj to allow multiple instances to run on a machine, but that ended up being counterproductive due to design decision #2: loading a new Rails environment for each job. . See, Rails takes a while to start up. Not only do you have to load all the associated libraries, but each @require@ statement needs to look through the entire, massive load path – a load path that includes the Rails app, Rubygems, the Rails source code, and all of our plugins. Doing this over and over, multiple times a minute, burns a lot of CPU and takes a lot of time. In some cases, the Rails load time is 99% of the entire background job’s lifetime. Spawning a whole bunch of Bjs on a single machine meant we effectively DoS’d the poor CPU. . I started working on a solution, but it was at this point we realized we were doing something wrong. These are not flaws in Bj, they are design decisions – these two ideas make Bj a pleasure to work with and perfect for simple sites. It’s working great on  FamSpam . We had simply outgrown it, and hacking Bj would have been error prone and time consuming. Luckily, we had seen people praising Dj in the past and a solid recommendation from  technoweenie  was all we needed. . The transition took about an hour and a half – from installing the plugin to successfully running Dj on the production site, complete with local and staging trial runs (and bug fixes). Because we had changed queues so many times in the past, we were using a simple interface to submitting a job. .    .  . RockQueue meant we didn’t have to change any application code, just infrastructure. I highly recommend an abstraction like this for vendor-specific APIs that would normally be littered all throughout your app, as changing vendors can become a major pain. . Anyway, Dj lets us spawn as many workers on a machine as we want. They’re just rake tasks running a @loop@, after all. It deals with locking and retries in a simple way, and works much like Bj. The queue is much faster now that we don’t have to pay the Rails startup tax. . We now have a single machine dedicated to running background tasks. We’re running 20 Dj workers on it with great success. There is no science behind this number. . Since people have already started asking “why didn’t you use queue X” or “you should use queue Y,” it seems reasonable to address that: we were very happy with Bj and wanted a similar system, albeit with a few different opinions. Dj is that system. It is simple, required no research beyond the short README, works wonderfully with Rails, is fast, is hackable, solves both the queue and the worker problems, and has no external dependecies. Also, it’s hosted on GitHub! . Dj is our 5th queue. In the past we’ve used SQS, ActiveMQ, Starling, and Bj. Dj is so far my favorite. . In a future post I’ll discuss the ways in which we use (and abuse) our queue. Count on it. ", "date": "October 30, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tSpeedy Queries\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-10-31-speedy-queries/", "abstract": " First our  new queue , and now this: . The site should be much faster – but it’s still not fast enough. We’re hard at work making things like git clones, tree browsing, and commit viewing much faster. As always, we’ll keep you in the loop. ", "date": "October 31, 2008"},
{"website": "Github-Engineering", "title": "\n\t\t\tdj.god\t\t", "author": ["\n\t\tChris Wanstrath\t"], "link": "https://github.blog/2008-11-18-dj-god/", "abstract": " People have asked for our  delayed_job   god  config. . Welp, here it is: .    ", "date": "November 18, 2008"}
]